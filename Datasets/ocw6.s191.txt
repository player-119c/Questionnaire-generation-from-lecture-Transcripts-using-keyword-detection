Good afternoon everyone!
Thank you all for joining today.  My name is Alexander Amini and I'll be one of your 
course organizers this year along with Ava -- and   together we're super excited to introduce you 
all to Introduction to Deep Learning. Now MIT  Intro to Deep Learning is a really really fun 
exciting and fast-paced program here at MIT   and let me start by just first of all giving 
you a little bit of background into what we   do and what you're going to learn about this year.
So this week of Intro to Deep Learning we're going   to cover a ton of material in just one week.
You'll learn the foundations of this really   really fascinating and exciting field of 
deep learning and artificial intelligence   and more importantly you're going to get hands-on 
experience actually reinforcing what you learn in   the lectures as part of hands-oOn software labs.
Now over the past decade AI and deep learning   have really had a huge resurgence and many 
incredible successes and a lot of problems   that even just a decade ago we thought were not 
really even solvable in the near future now we're   solving with deep learning with Incredible ease.
Now this past year in particular of 2022 has been   an incredible year for a deep learning progress 
and I like to say that actually this past year   in particular has been the year of generative 
deep learning using deep learning to generate   brand new types of data that I've never been 
seen before and never existed in reality in   fact I want to start this class by actually 
showing you how we started this class several   years ago which was by playing this video that 
I'll play in a second now this video actually   was an introductory video for the class it kind 
of exemplifies this idea that I'm talking about.  So let me just stop there and 
play this video first of all Hi everybody and welcome to MIT 6.S191 
-- the official introductory course on   deep learning taught here at MIT.
Deep Learning is revolutionizing   so many fields: from robotics to 
medicine and everything in between.  You'll learn the fundamentals of 
this field and how you can build   some of these incredible algorithms.
In fact, this entire speech and video   are not real and were created using deep 
learning and artificial intelligence.  And in this class you'll learn how.   It has been an honor to speak with you 
today and I hope you enjoy the course. so in case you couldn't tell this video and 
its entire audio was actually not real it was   synthetically generated by a deep learning 
algorithm and when we introduced this class   A few years ago this video was created several 
years ago right but even several years ago when   we introduced this and put it on YouTube it went 
somewhat viral right people really loved this   video they were intrigued by how real the video 
and audio felt and looked uh entirely generated   by an algorithm by a computer and people were 
shocked with the power and the realism of these   types of approaches and this was a few years ago 
now fast forward to today and the state of deep   learning today we have have seen deep learning 
accelerating at a rate faster than we've ever   seen before in fact we can use deep learning 
now to generate not just images of faces but   generate full synthetic environments where we can 
train autonomous vehicles entirely in simulation   and deploy them on full-scale vehicles in the 
real world seamlessly the videos here you see   are actually from a data driven simulator from 
neural networks generated called Vista that we   actually built here at MIT and have open sourced 
to the public so all of you can actually train and   build the future of autonomy and self-driving cars 
and of course it goes far beyond this as well deep   learning can be used to generate content directly 
from how we speak and the language that we convey   to it from prompts that we say deep learning can 
reason about the prompts in natural language and   English for example and then guide and control 
what is generated according to what we specify   we've seen examples of where we can generate for 
example things that again have never existed in   reality we can ask a neural network to generate 
a photo of a astronaut riding a horse and it   actually can imagine hallucinate what this might 
look like even though of course this photo not   only this photo has never occurred before but 
I don't think any photo of an astronaut riding   a horse has ever occurred before so there's 
not really even training data that you could   go off in this case and my personal favorite 
is actually how we can not only build software   that can generate images and videos but build 
software that can generate software as well we   can also have algorithms that can take language 
prompts for example a prompt like this write   code and tensorflow to generate or to train 
a neural network and not only will it write   the code and create that neural network but it 
will have the ability to reason about the code   that it's generated and walk you through step by 
step explaining the process and procedure all the   way from the ground up to you so that you can 
actually learn how to do this process as well   now I think some of these examples really just 
highlight how far deep learning and these methods   have come in the past six years since we started 
this course and you saw that example just a few   years ago from that introductory video but now 
we're seeing such incredible advances and the   most amazing part of this course in my opinion is 
actually that within this one week we're going to   take you through from the ground up starting 
from today all of the foundational building   blocks that will allow you to understand and 
make all of this amazing Advance as possible   so with that hopefully now you're all super 
excited about what this class will teach and I   want to basically now just start by taking a step 
back and introducing some of these terminologies   that I've kind of been throwing around so far 
the Deep learning artificial intelligence what   do these things actually mean so first of 
all I want to maybe just take a second to   speak a little bit about intelligence and 
what intelligence means at its core so to   me intelligence is simply the ability to process 
information such that we can use it to inform some   future decision or action that we take now the 
field of artificial intelligence is simply the   ability for us to build algorithms artificial 
algorithms that can do exactly this process   information to inform some future decision 
now machine learning is simply a subset of AI   which focuses specifically on how we can build 
a machine to or teach a machine how to do this   from some experiences or data for example now deep 
learning goes One Step Beyond this and is a subset   of machine learning which focuses explicitly on 
what are called neural networks and how we can   build neural networks that can extract features in 
the data these are basically what you can think of   as patterns that occur within the data so that 
it can learn to complete these tasks as well   now that's exactly what this class is really all 
about at its core we're going to try and teach   you and give you the foundational understanding 
and how we can build and teach computers to learn   tasks many different type of tasks directly from 
raw data and that's really what this class spoils   down to at it's it's most simple form and we'll 
provide a very solid foundation for you both on   the technical side through the lectures which will 
happen in two parts throughout the class the first   lecture and the second lecture each one about one 
hour long followed by a software lab which will   immediately follow the lectures which will try to 
reinforce a lot of what we cover in the in the in   the technical part of the class and you know give 
you hands-on experience implementing those ideas   so this program is split between these two pieces 
the technical lectures and the software Labs we   have several new updates this year in specific 
especially in many of the later lectures the   first lecture will cover the foundations of 
deep learning which is going to be right now   and finally we'll conclude the course with 
some very exciting guest lectures from both   Academia and Industry who are really leading 
and driving forward the state of AI and deep   learning and of course we have many awesome 
prizes that go with all of the software labs   and the project competition at the end of the 
course so maybe quickly to go through these   each day like I said we'll have dedicated 
software Labs that couple with the lectures   starting today with lab one you'll actually 
build a neural network keeping with this   theme of generative AI you'll build a neural 
network that can learn listen to a lot of   music and actually learn how to generate 
brand new songs in that genre of music   at the end at the next level of the class on 
Friday we'll host a project pitch competition   where either you individually or as part of a 
group can participate and present an idea a novel   deep learning idea to all of us it'll be roughly 
three minutes in length and we will focus not as   much because this is a one week program we're 
not going to focus so much on the results of   your pitch but rather The Innovation and the idea 
and the novelty of what you're trying to propose   the prices here are quite significant already 
where first price is going to get an Nvidia   GPU which is really a key piece of Hardware that 
is instrumental if you want to actually build a   deep learning project and train these neural 
networks which can be very large and require   a lot of compute these prices will give you 
the compute to do so and finally this year   we'll be awarding a grand prize for labs two and 
three combined which will occur on Tuesday and   Wednesday focused on what I believe is actually 
solving some of the most exciting problems in this   field of deep learning and how specifically how 
we can build models that can be robust not only   accurate but robust and trustworthy and safe when 
they're deployed as well and you'll actually get   experience developing those types of solutions 
that can actually Advance the state of the art   and AI now all of these Labs that I mentioned and 
competitions here are going to be due on Thursday   night at 11 PM right before the last day of 
class and we'll be helping you all along the   way this this Prize or this competition in 
particular has very significant prizes so I   encourage all of you to really enter this prize 
and try to try to get a chance to win the prize   and of course like I said we're going to 
be helping you all along the way who are   many available resources throughout this class to 
help you achieve this please post to Piazza if you   have any questions and of course this program 
has an incredible team that you can reach out   to at any point in case you have any issues or 
questions on the materials myself and Ava will   be your two main lectures for the first part 
of the class we'll also be hearing like I said   in the later part of the class from some guest 
lectures who will share some really cutting edge   state-of-the-art developments in deep learning 
and of course I want to give a huge shout out and   thanks to all of our sponsors who without their 
support this program wouldn't have been possible   at first yet again another year so thank you all   okay so now with that let's really dive into 
the really fun stuff of today's lecture which   is you know the the technical part and I think I 
want to start this part by asking all of you and   having yourselves ask yourself you know having 
you ask yourselves this question of you know why   are all of you here first of all why do you 
care about this topic in the first place now   I think to answer this question we have to take a 
step back and think about you know the history of   machine learning and what machine learning is and 
what deep learning brings to the table on top of   machine learning now traditional machine learning 
algorithms typically Define what are called these   set of features in the data you can think of these 
as certain patterns in the data and then usually   these features are hand engineered so probably 
a human will come into the data set and with a   lot of domain knowledge and experience can try to 
uncover what these features might be now the key   idea of deep learning and this is really Central 
to this class is that instead of having a human   Define these features what if we could have a 
machine look at all of this data and actually   try to extract and uncover what are the core 
patterns in the data so that it can use those   when it sees new data to make some decisions 
so for example if we wanted to detect faces   in an image a deep neural network algorithm might 
actually learn that in order to detect a face it   first has to detect things like edges in the image 
lines and edges and when you combine those lines   and edges you can actually create compositions 
of features like corners and curves which when   you create those when you combine those you can 
create more high level features for example eyes   and noses and ears and then those are the features 
that allow you to ultimately detect what you care   about detecting which is the face but all of these 
come from what are called kind of a hierarchical   learning of features and you can actually see some 
examples of these these are real features learned   by a neural network and how they're combined 
defines this progression of information but   in fact what I just described this underlying and 
fundamental building block of neural networks and   deep learning have actually existed for decades 
now why are we studying all of this now and today   in this class with all of this great enthusiasm 
to learn this right well for one there have been   several key advances that have occurred in the 
past decade number one is that data is so much   more pervasive than it has ever been before in our 
lifetimes these models are hungry for more data   and we're living in the age of Big Data more data 
is available to these models than ever before and   they Thrive off of that secondly these algorithms 
are massively parallelizable they require a lot of   compute and we're also at a unique time in history 
where we have the ability to train these extremely   large-scale algorithms and techniques that have 
existed for a very long time but we can now   train them due to the hardware advances that have 
been made and finally due to open source toolbox   access and software platforms like tensorflow 
for example which all of you will get a lot of   experience on in this class training and building 
the code for these neural networks has never been   easier so that from the software point of view 
as well there have been incredible advances   to open source you know the the underlying 
fundamentals of what you're going to learn   so let me start now with just building up from 
the ground up the fundamental building block of   every single neural network that you're going 
to learn in this class and that's going to be   just a single neuron right and in neural network 
language a single neuron is called a perceptron   so what is the perceptron a perceptron 
is like I said a single neuron and it's   actually I'm going to say it's very 
very simple idea so I want to make   sure that everyone in the audience understands 
exactly what a perceptron is and how it works   so let's start by first defining a perceptron 
as taking it as input a set of inputs right so   on the left hand side you can see this perceptron 
takes M different inputs 1 to M right these are   the blue circles we're denoting these inputs as 
X's each of these numbers each of these inputs   is then multiplied by a corresponding weight which 
we can call W right so X1 will be multiplied by W1   and we'll add the result of all of these 
multiplications together now we take that   single number after the addition and we pass it 
through this non-linear what we call a non-linear   activation function and that produces our final 
output of the perceptron which we can call Y   now this is actually not entirely accurate of 
the picture of a perceptron there's one step   that I forgot to mention here so in addition 
to multiplying all of these inputs with their   corresponding weights we're also now going to add 
what's called a bias term here denoted as this w0   which is just a scalar weight and you can think 
of it coming with a input of just one so that's   going to allow the network to basically shift 
its nonlinear activation function uh you know   non-linearly right as it sees its inputs now 
on the right hand side you can see this diagram   mathematically formulated right as a single 
equation we can now rewrite this linear this this   equation with linear algebra terms of vectors and 
Dot products right so for example we can Define   our entire inputs X1 to XM as a large Vector 
X right that large Vector X can be multiplied   by or taking a DOT excuse me Matrix multiplied 
with our weights W this again another Vector of   our weights W1 to WN taking their dot product 
not only multiplies them but it also adds the   resulting terms together adding a bias like 
we said before and applying this non-linearity   now you might be wondering what is this non-linear 
function I've mentioned it a few times already   well I said it is a function right that's passed 
that we pass the outputs of the neural network   through before we return it you know to the next 
neuron in the in the pipeline right so one common   example of a nonlinear function that's very 
popular in deep neural networks is called the   sigmoid function you can think of this as kind of 
a continuous version of a threshold function right   it goes from zero to one and it's having it can 
take us input any real number on the real number   line and you can see an example of it Illustrated 
on the bottom right hand now in fact there are   many types of nonlinear activation functions that 
are popular in deep neural networks and here are   some common ones and throughout this presentation 
you'll actually see some examples of these code   snippets on the bottom of the slides where we'll 
try and actually tie in some of what you're   learning in the lectures to actual software and 
how you can Implement these pieces which will help   you a lot for your software Labs explicitly so 
the sigmoid activation on the left is very popular   since it's a function that outputs you know 
between zero and one so especially when you want   to deal with probability distributions for example 
this is very important because probabilities live   between 0 and 1. in modern deep neural networks 
though the relu function which you can see on the   far right hand is a very popular activation 
function because it's piecewise linear it's   extremely efficient to compute especially when 
Computing its derivatives right its derivatives   are constants except for one non-linear idiot 
zero now I hope actually all of you are probably   asking this question to yourself of why do we 
even need this nonlinear activation function   it seems like it kind of just complicates this 
whole picture when we didn't really need it in   the first place and I want to just spend a moment 
on answering this because the point of a nonlinear   activation function is of course number one is to 
introduce non-linearities to our data right if we   think about our data almost all data that we care 
about all real world data is highly non-linear   now this is important because if we want to be 
able to deal with those types of data sets we   need models that are also nonlinear so they can 
capture those same types of patterns so imagine   I told you to separate for example I gave you this 
data set red points from greenpoints and I ask you   to try and separate those two types of data points 
now you might think that this is easy but what if   I could only if I told you that you could only 
use a single line to do so well now it becomes   a very complicated problem in fact you can't 
really Solve IT effectively with a single line   and in fact if you introduce nonlinear activation 
functions to your Solution that's exactly what   allows you to you know deal with these types of 
problems nonlinear activation functions allow   you to deal with non-linear types of data now 
and that's what exactly makes neural networks   so powerful at their core so let's understand 
this maybe with a very simple example walking   through this diagram of a perceptron one 
more time imagine I give you this trained   neural network with weights now not W1 W2 I'm 
going to actually give you numbers at these   locations right so the trained weights w0 will 
be 1 and W will be a vector of 3 and negative 2.   so this neural network has two inputs like we 
said before it has input X1 it has input X2 if   we want to get the output of it this is also 
the main thing I want all of you to take away   from this lecture today is that to get the output 
of a perceptron there are three steps we need to   take right from this stage we first compute the 
multiplication of our inputs with our weights   sorry yeah multiply them together add 
their result and compute a non-linearity   it's these three steps that Define the forward 
propagation of information through a perceptron   so let's take a look at how that exactly 
works right so if we plug in these numbers   to the to those equations we can see that 
everything inside of our non-linearity   here the nonlinearity is G right that function G 
which could be a sigmoid we saw a previous slide   that component inside of our nonlinearity is 
in fact just a two-dimensional line it has two   inputs and if we consider the space of all of 
the possible inputs that this neural network   could see we can actually plot this on a decision 
boundary right we can plot this two-dimensional   line as as a a decision boundary as a plane 
separating these two components of our space   in fact not only is it a single plane there's a 
directionality component depending on which side   of the plane that we live on if we see an input 
for example here negative one two we actually   know that it lives on one side of the plane and 
it will have a certain type of output in this case   that output is going to be positive right because 
in this case when we plug those components into   our equation we'll get a positive number that 
passes through the nonlinear component and that   gets propagated through as well of course if 
you're on the other side of the space you're   going to have the opposite result right and that 
thresholding function is going to essentially live   at this decision boundary so depending on which 
side of the space you live on that thresholding   function that sigmoid function is going to then 
control how you move to one side or the other   now in this particular example this is very 
convenient right because we can actually   visualize and I can draw this exact full space 
for you on this slide it's only a two-dimensional   space so it's very easy for us to visualize 
but of course for almost all problems that we   care about our data points are not going to 
be two-dimensional right if you think about   an image the dimensionality of an image is going 
to be the number of pixels that you have in the   image right so these are going to be thousands 
of Dimensions millions of Dimensions or even   more and then drawing these types of plots like 
you see here is simply not feasible right so we   can't always do this but hopefully this gives 
you some intuition to understand kind of as we   build up into more complex models so now that we 
have an idea of the perceptron let's see how we   can actually take this single neuron and start 
to build it up into something more complicated a   full neural network and build a model from that 
so let's revisit again this previous diagram of   the perceptron if again just to reiterate one more 
time this core piece of information that I want   all of you to take away from this class is how a 
perceptron works and how it propagates information   to its decision there are three steps first is the 
dot product second is the bias and third is the   non-linearity and you keep repeating this process 
for every single perceptron in your neural network   let's simplify the diagram a little bit I'll get 
rid of the weights and you can assume that every   line here now basically has an Associated weight 
scaler that's associated with it every line also   has it corresponds to the input that's coming 
in it has a weight that's coming in also at the   on the line itself and I've also removed the bias 
just for a sake of Simplicity but it's still there   so now the result is that Z which let's call 
that the result of our DOT product plus the   bias is going and that's what we pass into 
our non-linear function that piece is going   to be applied to that activation function 
now the final output here is simply going   to be G which is our activation function of 
Z right Z is going to be basically what you   can think of the state of this neuron it's 
the result of that dot product plus bias   now if we want to Define and build up a 
multi-layered output neural network if we   want two outputs to this function for example 
it's a very simple procedure we just have now   two neurons two perceptrons each perceptron will 
control the output for its Associated piece right   so now we have two outputs each one is a normal 
perceptron it takes all of the inputs so they   both take the same inputs but amazingly now 
with this mathematical understanding we can   start to build our first neural network entirely 
from scratch so what does that look like so we   can start by firstly initializing these two 
components the first component that we saw   was the weight Matrix excuse me the weight 
Vector it's a vector of Weights in this case   and the second component is the the bias Vector 
that we're going to multiply with the dot product   of all of our inputs by our weights right so the 
only remaining step now after we've defined these   parameters of our layer is to now Define you know 
how does forward propagation of information works   and that's exactly those three main components 
that I've been stressing to so we can create this   call function to do exactly that to Define this 
forward propagation of information and the story   here is exactly the same as we've been seeing it 
right Matrix multiply our inputs with our weights   Right add a bias and then apply a non-linearity 
and return the result and that literally this code   will run this will Define a full net a full neural 
network layer that you can then take like this   and of course actually luckily for all 
of you all of that code which wasn't much   code that's been abstracted away by these 
libraries like tensorflow you can simply   call functions like this which will actually 
you know replicate exactly that piece of code   so you don't need to necessarily copy all of 
that code down you just you can just call it   and with that understanding you know we just saw 
how you could build a single layer but of course   now you can actually start to think about how 
you can stack these layers as well so since we   now have this transformation essentially from 
our inputs to a hidden output you can think   of this as basically how we can Define some 
way of transforming those inputs right into   some new dimensional space right perhaps closer 
to the value that we want to predict and that   transformation is going to be eventually learned 
to know how to transform those inputs into our   desired outputs and we'll get to that later but 
for now the piece that I want to really focus on   is if we have these more complex neural networks 
I want to really distill down that this is nothing   more complex than what we've already seen if we 
focus on just one neuron in this diagram take is   here for example Z2 right Z2 is this neuron that's 
highlighted in the middle layer it's just the same   perceptron that we've been seeing so far in this 
class it was a its output is obtained by taking   a DOT product adding a bias and then applying 
that non-linearity between all of its inputs   if we look at a different node for example Z3 
which is the one right below it it's the exact   same story again it sees all of the same inputs 
but it has a different set of weight Matrix that   it's going to apply to those inputs so we'll have 
a different output but the mathematical equations   are exactly the same so from now on I'm just 
going to kind of simplify all of these lines and   diagrams just to show these icons in the middle 
just to demonstrate that this means everything   is going to fully connect it to everything and 
defined by those mathematical equations that we've   been covering but there's no extra complexity in 
these models from what you've already seen now if   you want to Stack these types of Solutions on top 
of each other these layers on top of each other   you can not only Define one layer very easily but 
you can actually create what are called sequential   models these sequential models you can Define one 
layer after another and they define basically the   forward propagation of information not just 
from the neuron level but now from the layer   level every layer will be fully connected to the 
next layer and the inputs of the secondary layer   will be all of the outputs of the prior layer 
now of course if you want to create a very deep   neural network all the Deep neural network is is 
we just keep stacking these layers on top of each   other there's nothing else to this story that's 
really as simple as it is once so these layers are   basically all they are is just layers where the 
final output is computed right by going deeper and   deeper into this progression of different layers 
right and you just keep stacking them until you   get to the last layer which is your output layer 
it's your final prediction that you want to Output   right we can create a deep neural network to do 
all of this by stacking these layers and creating   these more hierarchical models like we saw very 
early in the beginning of today's lecture one   where the final output is really computed by you 
know just going deeper and deeper into this system   okay so that's awesome so we've now seen how 
we can go from a single neuron to a layer to   all the way to a deep neural network right 
building off of these foundational principles   let's take a look at how exactly we can use these 
uh you know principles that we've just discussed   to solve a very real problem that I think all 
of you are probably very concerned about uh   this morning when you when you woke up so that 
problem is how we can build a neural network to   answer this question which is will I how will 
I pass this class and if I will or will I not   so to answer this question let's see if we can 
train a neural network to solve this problem okay   so to do this let's start with a very simple 
neural network right we'll train this model with   two inputs just two inputs one input is going to 
be the number of lectures that you attend over the   course of this one week and the second input is 
going to be how many hours that you spend on your   final project or your competition okay so what 
we're going to do is firstly go out and collect   a lot of data from all of the past years that 
we've taught this course and we can plot all of   this data because it's only two input space we can 
plot this data on a two-dimensional feature space   right we can actually look at all of the students 
before you that have passed the class and failed   the class and see where they lived in this space 
for the amount of hours that they've spent the   number of lectures that they've attended and so 
on greenpoints are the people who have passed red   or those who have failed now and here's you right 
you're right here four or five is your coordinate   space you fall right there and you've attended 
four lectures you've spent five hours on your   final project we want to build a neural network 
to answer the question of will you pass the class   although you failed the class so let's do it we 
have two inputs one is four one is five these   are two numbers we can feed them through a neural 
network that we've just seen how we can build that   and we feed that into a single layered neural 
network three hidden units in this example but   we could make it larger if we wanted to be more 
expressive and more powerful and we see here   that the probability of you passing this class 
is 0.1 it's pretty visible so why would this   be the case right what did we do wrong because I 
don't think it's correct right when we looked at   the space it looked like actually you were a good 
candidate to pass the class but why is the neural   network saying that there's only a 10 likelihood 
that you should pass does anyone have any ideas exactly exactly so this neural network is just uh 
like it was just born right it has no information   about the the world or this class it doesn't 
know what four and five mean or what the notion   of passing or failing means right so exactly right 
this neural network has not been trained you can   think of it kind of as a baby it hasn't learned 
anything yet so our job firstly is to train it   and part of that understanding is we first need 
to tell the neural network when it makes mistakes   right so mathematically we should now think 
about how we can answer this question which is   does did my neural network make a mistake and if 
it made a mistake how can I tell it how big of a   mistake it was so that the next time it sees this 
data point can it do better minimize that mistake   so in neural network language those mistakes 
are called losses right and specifically you   want to Define what's called a loss function 
which is going to take as input your prediction   and the true prediction right and how 
far away your prediction is from the   true prediction tells you how big of 
a loss there is right so for example   let's say we want to build a neural 
network to do classification of   or sorry actually even before that I want to 
maybe give you some terminology so there are   multiple different ways of saying the same thing 
in neural networks and deep learning so what I   just described as a loss function is also commonly 
referred to as an objective function empirical   risk a cost function these are all exactly the 
same thing they're all a way for us to train the   neural network to teach the neural network when it 
makes mistakes and what we really ultimately want   to do is over the course of an entire data set not 
just one data point of mistakes we won't say over   the entire data set we want to minimize all of the 
mistakes on average that this neural network makes   so if we look at the problem like I said of 
binary classification will I pass this class   or will I not there's a yes or no answer that 
means binary classification now we can use what's   called a loss function of the softmax Cross 
entropy loss and for those of you who aren't   familiar this notion of cross entropy is actually 
developed here at MIT by Sean Sean Excuse me yes   Claude Shannon who is a Visionary he did his 
Masters here over 50 years ago he introduced   this notion of cross-entropy and that was you 
know pivotal in in the ability for us to train   these types of neural networks even now into the 
future so let's start by instead of predicting   a binary cross-entropy output what if we wanted 
to predict a final grade of your class score for   example that's no longer a binary output yes or 
no it's actually a continuous variable right it's   the grade let's say out of 100 points what is the 
value of your score in the class project right for   this type of loss we can use what's called a mean 
squared error loss you can think of this literally   as just subtracting your predicted grade from 
the true grade and minimizing that distance apart   foreign so I think now we're ready to really put 
all of this information together and Tackle this   problem of training a neural network right to not 
just identify how erroneous it is how large its   loss is but more importantly minimize that loss 
as a function of seeing all of this training data   that it observes so we know that we want to find 
this neural network like we mentioned before that   minimizes this empirical risk or this empirical 
loss averaged across our entire data set now   this means that we want to find mathematically 
these W's right that minimize J of w JFW is our   loss function average over our entire data set 
and W is our weight so we want to find the set   of Weights that on average is going to give 
us the minimum the smallest loss as possible   now remember that W here is just a list basically 
it's just a group of all of the weights in our   neural network you may have hundreds of weights 
and a very very small neural network or in today's   neural networks you may have billions or trillions 
of weights and you want to find what is the value   of every single one of these weights that's 
going to result in the smallest loss as possible   now how can you do this remember that our loss 
function J of w is just a function of our weights   right so for any instantiation of our weights 
we can compute a scalar value of you know how   how erroneous would our neural network be for 
this instantiation of our weights so let's try   and visualize for example in a very simple example 
of a two-dimensional space where we have only two   weights extremely simple neural network here very 
small two weight neural network and we want to   find what are the optimal weights that would train 
this neural network we can plot basically the loss   how erroneous the neural network is for every 
single instantiation of these two weights right   this is a huge space it's an infinite space but 
still we can try to we can have a function that   evaluates at every point in this space now what 
we ultimately want to do is again we want to find   which set of W's will give us the smallest loss 
possible that means basically the lowest point   on this landscape that you can see here where 
is the W's that bring us to that lowest point   the way that we do this is actually just by 
firstly starting at a random place we have no idea   where to start so pick a random place to start in 
this space and let's start there at this location   let's evaluate our neural network we can compute 
the loss at this specific location and on top   of that we can actually compute how the loss is 
changing we can compute the gradient of the loss   because our loss function is a continuous function 
right so we can actually compute derivatives of   our function across the space of our weights and 
the gradient tells us the direction of the highest   point right so from where we stand the gradient 
tells us where we should go to increase our loss   now of course we don't want to increase our loss 
we want to decrease our loss so we negate our   gradient and we take a step in the opposite 
direction of the gradient that brings us one   step closer to the bottom of the landscape and 
we just keep repeating this process right over   and over again we evaluate the neural network 
at this new location compute its gradient and   step in that new direction we keep traversing 
this landscape until we converge to the minimum   we can really summarize this algorithm which 
is known formally as gradient descent right so   gradient descent simply can be written like this 
we initialize all of our weights right this can   be two weights like you saw in the previous 
example it can be billions of Weights like   in real neural networks we compute this gradient 
of the partial derivative with of our loss with   respect to the weights and then we can update our 
weights in the opposite direction of this gradient   so essentially we just take this small 
amount small step you can think of it   which here is denoted as Ada and we refer 
to this small step right this is commonly   referred to as what's known as the learning 
rate it's like how much we want to trust that   gradient and step in the direction of that 
gradient we'll talk more about this later   but just to give you some sense of code this this 
algorithm is very well translatable to real code   as well for every line on the pseudocode you can 
see on the left you can see corresponding real   code on the right that is runnable and directly 
implementable by all of you in your labs but now   let's take a look specifically at this term here 
this is the gradient we touched very briefly on   this in the visual example this explains like I 
said how the loss is changing as a function of the   weights right so as the weights move around will 
my loss increase or decrease and that will tell   the neural network if it needs to move the weights 
in a certain direction or not but I never actually   told you how to compute this right and I think 
that's an extremely important part because if you   don't know that then you can't uh well you can't 
train your neural network right this is a critical   part of training neural networks and that process 
of computing this line This gradient line is known   as back propagation so let's do a very quick 
intro to back propagation and how it works so   again let's start with the simplest neural network 
in existence this neural network has one input one   output and only one neuron right this is as simple 
as it gets we want to compute the gradient of our   loss with respect to our weight in this case let's 
compute it with respect to W2 the second weight   so this derivative is going to tell us how much a 
small change in this weight will affect our loss   if if a small change if we change our weight a 
little bit in One Direction we'll increase our   loss or decrease our loss so to compute that we 
can write out this derivative we can start with   applying the chain rule backwards from the loss 
function through the output specifically what   we can do is we can actually just decompose this 
derivative into two components the first component   is the derivative of our loss with respect to 
our output multiplied by the derivative of our   output with respect to W2 right this is just a 
standard um uh instantiation of the chain rule   with this original derivative that we had on the 
left hand side let's suppose we wanted to compute   the gradients of the weight before that which in 
this case are not W1 but W excuse me not W2 but W1   well all we do is replace W2 with W1 and that 
chain Rule still holds right that same equation   holds but now you can see on the red component 
that last component of the chain rule we have to   once again recursively apply one more chain rule 
because that's again another derivative that we   can't directly evaluate we can expand that 
once more with another instantiation of the   chain Rule and now all of these components we 
can directly propagate these gradients through   the hidden units right in our neural network all 
the way back to the weight that we're interested   in in this example right so we first computed 
the derivative with respect to W2 then we can   back propagate that and use that information 
also with W1 that's why we really call it   back propagation because this process occurs 
from the output all the way back to the input   now we repeat this process essentially many many 
times over the course of training by propagating   these gradients over and over again through 
the network all the way from the output to   the inputs to determine for every single weight 
answering this question which is how much does   a small change in these weights affect our loss 
function if it increases it or decreases and how   we can use that to improve the loss ultimately 
because that's our final goal in this class   foreign so that's the back propagation algorithm 
that's that's the core of training neural networks   in theory it's very simple it's it's really 
just an instantiation of the chain rule   but let's touch on some insights that make 
training neural networks actually extremely   complicated in practice even though the algorithm 
of back propagation is simple and you know many   decades old in practice though optimization of 
neural networks looks something like this it   looks nothing like that picture that I showed you 
before there are ways that we can visualize very   large deep neural networks and you can think 
of the landscape of these models looking like   something like this this is an illustration from 
a paper that came out several years ago where   they tried to actually visualize the landscape 
a very very deep neural networks and that's what   this landscape actually looks like that's what 
you're trying to deal with and find the minimum   in this space and you can imagine the challenges 
that come with that so to cover the challenges   let's first think of and recall that update 
equation defined in gradient descent right so   I didn't talk too much about this parameter Ada 
but now let's spend a bit of time thinking about   this this is called The Learning rate like we saw 
before it determines basically how big of a step   we need to take in the direction of our gradient 
on every single iteration of back propagation   in practice even setting the learning rate 
can be very challenging you as you as the   designer of the neural network have to set this 
value this learning rate and how do you pick   this value right so that can actually be quite 
difficult it has really uh large consequences   when building a neural network so for example 
if we set the learning rate too low then we   learn very slowly so let's assume we start on 
the right hand side here at that initial guess   if our learning rate is not large enough 
not only do we converge slowly we actually   don't even converge to the global minimum right 
because we kind of get stuck in a local minimum   now what if we set our learning rate too high 
right what can actually happen is we overshoot and   we can actually start to diverge from the solution 
the gradients can actually explode very bad things   happen and then the neural network doesn't trade 
so that's also not good in reality there's a very   happy medium between setting it too small setting 
it too large where you set it just large enough to   kind of overshoot some of these local Minima 
put you into a reasonable part of the search   space where then you can actually Converge on the 
solutions that you care most about but actually   how do you set these learning rates in practice 
right how do you pick what is the ideal learning   rate one option and this is actually a very common 
option in practice is to simply try out a bunch of   learning rates and see what works the best right 
so try out let's say a whole grid of different   learning rates and you know train all of these 
neural networks see which one works the best   but I think we can do something a lot smarter 
right so what are some more intelligent ways   that we could do this instead of exhaustively 
trying out a whole bunch of different learning   rates can we design a learning rate algorithm 
that actually adapts to our neural network and   adapts to its landscape so that it's a bit 
more intelligent than that previous idea   so this really ultimately means that the learning 
rate the speed at which the algorithm is trusting   the gradients that it sees is going to depend 
on how large the gradient is in that location   and how fast we're learning how many other 
options uh and sorry and many other options   that we might have as part of training in 
neural networks right so it's not only how   quickly we're learning you may judge it on many 
different factors in the learning landscape   in fact we've all been these different algorithms 
that I'm talking about these adaptive learning   rate algorithms have been very widely studied in 
practice there is a very thriving community in   the Deep learning research community that 
focuses on developing and designing new   algorithms for learning rate adaptation and faster 
optimization of large neural networks like these   and during your Labs you'll actually get the 
opportunity to not only try out a lot of these   different adaptive algorithms which you can see 
here but also try to uncover what are kind of the   patterns and benefits of One Versus the other 
and that's going to be something that I think   you'll you'll find very insightful as part of your 
labs so another key component of your Labs that   you'll see is how you can actually put all of this 
information that we've covered today into a single   picture that looks roughly something like this 
which defines your model at the first at the top   here that's where you define your model we talked 
about this in the beginning part of the lecture   for every piece in your model you're now going 
to need to Define this Optimizer which we've just   talked about this Optimizer is defined together 
with a learning rate right how quickly you want   to optimize your lost landscape and over many 
Loops you're going to pass over all of the   examples in your data set and observe essentially 
how to improve your network that's the gradient   and then actually improve the network in those 
directions and keep doing that over and over   and over again until eventually your neural 
network converges to some sort of solution so I want to very quickly briefly in the 
remaining time that we have continue to talk   about tips for training these neural networks 
in practice and focus on this very powerful   idea of batching your data into well what are 
called mini batches of smaller pieces of data   to do this let's revisit that gradient descent 
algorithm right so here this gradient that we   talked about before is actually extraordinarily 
computationally expensive to compute because it's   computed as a summation across all of the pieces 
in your data set right and in most real life or   real world problems you know it's simply not 
feasible to compute a gradient over your entire   data set data sets are just too large these days 
so in you know there are some Alternatives right   what are the Alternatives instead of computing 
the derivative or the gradients across your entire   data set what if you instead computed the gradient 
over just a single example in your data set just   one example well of course this this estimate of 
your gradient is going to be exactly that it's   an estimate it's going to be very noisy it may 
roughly reflect the trends of your entire data set   but because it's a very it's only one example in 
fact of your entire data set it may be very noisy   right well the advantage of this though is 
that it's much faster to compute obviously   the gradient over a single example because 
it's one example so computationally this   has huge advantages but the downside is that it's 
extremely stochastic right that's the reason why   this algorithm is not called gradient descent 
it's called stochastic gradient descent now   now what's the middle ground right instead of 
computing it with respect to one example in   your data set what if we computed what's called a 
mini batch of examples a small batch of examples   that we can compute the gradients over and when we 
take these gradients they're still computationally   efficient to compute because it's a mini batch 
it's not too large maybe we're talking on the   order of tens or hundreds of examples in our data 
set but more importantly because we've expanded   from a single example to maybe 100 examples 
the stochasticity is significantly reduced and   the accuracy of our gradient is much improved so 
normally we're thinking of batch sizes many batch   sizes roughly on the order of 100 data points 
tens or hundreds of data points this is much   faster obviously to compute than gradient descent 
and much more accurate to compute compared to   stochastic gradient descent which is that single 
single point example so this increase in gradient   accuracy allows us to essentially converge to 
our solution much quicker than it could have   been possible in practice due to gradient descent 
limitations it also means that we can increase our   learning rate because we can trust each of those 
gradients much more efficiently right we're now   averaging over a batch it's going to be much 
more accurate than the stochastic version so we   can increase that learning rate and actually 
learn faster as well this allows us to also   massively parallelize this entire algorithm in 
computation right we can split up batches onto   separate workers and Achieve even more significant 
speed UPS of this entire problem using gpus the   last topic that I very very briefly want to cover 
in today's lecture is this topic of overfitting   right when we're optimizing a neural network with 
stochastic gradient descent we have this challenge   of what's called overfitting overfitting I looks 
like this roughly right so on the left hand side   we want to build a neural network or let's say 
in general we want to build a machine learning   model that can accurately describe some patterns 
in our data but remember we're ultimately we don't   want to describe the patterns in our training data 
ideally we want to define the patterns in our test   data of course we don't observe test data we only 
observe training data so we have this challenge of   extracting patterns from training data and hoping 
that they generalize to our test data so set in   one different way we want to build models that can 
learn representations from our training data that   can still generalize even when we show them brand 
new unseen pieces of test data so assume that you   want to build a line that can describe or find 
the patterns in these points that you can see on   the slide right if you have a very simple neural 
network which is just a single line straight line   you can describe this data sub-optimally right 
because the data here is non-linear you're not   going to accurately capture all of the nuances 
and subtleties in this data set that's on the   left hand side if you move to the right hand 
side you can see a much more complicated model   but here you're actually over expressive you're 
too expressive and you're capturing kind of the   nuances the spurious nuances in your training 
data that are actually not representative of   your test data ideally you want to end up with the 
model in the middle which is basically the middle   ground right it's not too complex and it's not too 
simple it still gives you what you want to perform   well and even when you give it brand new data so 
to address this problem let's briefly talk about   what's called regularization regularization 
is a technique that you can introduce to your   training pipeline to discourage complex models 
from being learned now as we've seen before   this is really critical because neural networks 
are extremely large models they are extremely   prone to overfitting right so regularization 
and having techniques for regularization has   extreme implications towards the success of 
neural networks and having them generalize   Beyond training data far into our testing domain 
the most popular technique for regularization in   deep learning is called Dropout and the idea of 
Dropout is is actually very simple it's let's   revisit it by drawing this picture of deep neural 
networks that we saw earlier in today's lecture in   Dropout during training we essentially randomly 
select some subset of the neurons in this neural   network and we try to prune them out with some 
random probabilities so for example we can select   this subset of neural of neurons we can randomly 
select them with a probability of 50 percent and   with that probability we randomly turn them off 
or on on different iterations of our training   so this is essentially forcing the neural network 
to learn you can think of an ensemble of different   models on every iteration it's going to be exposed 
to kind of a different model internally than the   one it had on the last iteration so it has 
to learn how to build internal Pathways to   process the same information and it can't rely on 
information that it learned on previous iterations   right so it forces it to kind of capture some 
deeper meaning within the pathways of the neural   network and this can be extremely powerful 
because number one it lowers the capacity   of the neural network significantly right you're 
lowering it by roughly 50 percent in this example   but also because it makes them easier to 
train because the number of Weights that   have gradients in this case is also reduced so 
it's actually much faster to train them as well   now like I mentioned on every iteration we 
randomly drop out a different set of neurons right   and that helps the data generalize better and the 
second regularization techniques which is actually   a very broad regularization technique far beyond 
neural networks is simply called early stopping   now we know the the definition of overfitting 
is simply when our model starts to represent   basically the training data more than the 
testing data that's really what overfitting   comes down to at its core if we set aside some of 
the training data to use separately that we don't   train on it we can use it as kind of a testing 
data set synthetic testing data set in some ways   we can monitor how our network is learning on 
this unseen portion of data so for example we   can over the course of training we can basically 
plot the performance of our Network on both the   training set as well as our held out test set and 
as the network is trained we're going to see that   first of all these both decrease but there's 
going to be a point where the loss plateaus and   starts to increase the training loss will actually 
start to increase this is exactly the point where   you start to overfit right because now you're 
starting to have sorry that was the test loss the   test loss actually starts to increase because now 
you're starting to overfit on your training data   this pattern basically continues for the rest 
of training and this is the point that I want   you to focus on right this Middle Point 
is where we need to stop training because   after this point assuming that this test set 
is a valid representation of the true test   set this is the place where the accuracy 
of the model will only get worse right so   this is where we would want to early stop 
our model and regularize the performance   and we can see that stopping anytime before 
this point is also not good we're going to   produce an underfit model where we could 
have had a better model on the test data   but it's this trade-off right you can't stop 
too late and you can't stop too early as well   so I'll conclude this lecture by just summarizing 
these three key points that we've covered in   today's lecture so far so we've first covered 
these fundamental building blocks of all neural   networks which is the single neuron the perceptron 
we've built these up into larger neural layers and   then from their neural networks and deep neural 
networks we've learned how we can train these   apply them to data sets back propagate through 
them and we've seen some trips tips and tricks   for optimizing these systems end to end in 
the next lecture we'll hear from Ava on deep   sequence modeling using rnns and specifically 
this very exciting new type of model called the   Transformer architecture and attention mechanisms 
so maybe let's resume the class in about five   minutes after we have a chance to swap speakers 
and thank you so much for all of your attention thank you 

Hello everyone! I hope you enjoyed Alexander's 
first lecture. I'm Ava and in this second lecture,   Lecture 2, we're going to focus on this 
question of sequence modeling -- how   we can build neural networks that can 
handle and learn from sequential data. So in Alexander's first lecture he 
introduced the essentials of neural   networks starting with perceptrons building 
up to feed forward models and how you can   actually train these models and start 
to think about deploying them forward. Now we're going to turn our attention to 
specific types of problems that involve   sequential processing of data and we'll 
realize why these types of problems require   a different way of implementing and building 
neural networks from what we've seen so far. And I think some of the components in 
this lecture traditionally can be a bit   confusing or daunting at first but what I 
really really want to do is to build this   understanding up from the foundations walking 
through step by step developing intuition   all the way to understanding the math and the 
operations behind how these networks operate. Okay so let's let's get started to to 
begin I to begin I first want to motivate   what exactly we mean when we talk about 
sequential data or sequential modeling. So we're going to begin with a really simple 
intuitive example let's say we have this   picture of a ball and your task is to predict 
where this ball is going to travel to next. Now if you don't have any prior information about   the trajectory of the ball it's motion 
it's history any guess or prediction   about its next position is going 
to be exactly that a random guess. If however in addition to the current 
location of the ball I gave you some   information about where it was moving in the 
past now the problem becomes much easier and   I think hopefully we can all agree that most 
likely or most likely next prediction is that   this ball is going to move forward 
to the right in in the next frame. So this is a really you know reduced down 
bare bones intuitive example but the truth   is that beyond this sequential 
data is really all around us. As I'm speaking the words coming out of 
my mouth form a sequence of sound waves   that define audio which we can split up 
to think about in this sequential manner   similarly text language can be split up into a 
sequence of characters or a sequence of words   and there are many many more examples in which 
sequential processing sequential data is present   right from medical signals like EKGs to financial 
markets and projecting stock prices to biological   sequences encoded in DNA to patterns in the 
climate to patterns of motion and many more   and so already hopefully you're getting 
a sense of what these types of questions   and problems may look like and where 
they are relevant in the real world   when we consider applications of sequential 
modeling in the real world we can think about   a number of different kind of problem definitions 
that we can have in our Arsenal and work with   in the first lecture Alexander introduced the 
Notions of classification and the notion of   regression where he talked about and we learned 
about feed forward models that can operate one   to one in this fixed and static setting right 
given a single input predict a single output   the binary classification example of 
will you succeed or pass this class   here there's there's no notion of sequence 
there's no notion of time now if we introduce   this idea of a sequential component we can 
handle inputs that may be defined temporally   and potentially also produce a sequential 
or temporal output so for as one example we   can consider text language and maybe we want to 
generate one prediction given a sequence of text   classifying whether a message is a 
positive sentiment or a negative sentiment   conversely we could have a single input let's say 
an image and our goal may be now to generate text   or a sequential description of this image right 
given this image of a baseball player throwing a   ball can we build a neural network that generates 
that as a language caption finally we can also   consider applications and problems where we have 
sequence in sequence L for example if we want to   translate between two languages and indeed this 
type of thinking in this type of Architecture   is what powers the task of machine translation 
in your phones in Google Translate and and many   other examples so hopefully right this has given 
you a picture of what sequential data looks like   what these types of problem definitions may look 
like and from this we're going to start and build   up our understanding of what neural networks we 
can build and train for these types of problems   so first we're going to begin with the notion 
of recurrence and build up from that to Define   recurrent neural networks and in the last 
portion of the lecture we'll talk about the   underlying mechanisms underlying the Transformer 
architectures that are very very very powerful in   terms of handling sequential data but as I said 
at the beginning right the theme of this lecture   is building up that understanding step by step 
starting with the fundamentals and the intuition   so to do that we're going to go back revisit 
the perceptron and move forward from there   right so as Alexander introduced where we 
studied the perception perceptron in lecture one   the perceptron is defined by this single 
neural operation where we have some set of   inputs let's say X1 through XM and each of these 
numbers are multiplied by a corresponding weight   pass through a non-linear activation function 
that then generates a predicted output y hat   here we can have multiple inputs 
coming in to generate our output   but still these inputs are not thought of as 
points in a sequence or time steps in a sequence   even if we scale this perceptron and start 
to stack multiple perceptrons together to   Define these feed forward neural networks we still 
don't have this notion of temporal processing or   sequential information even though we are able to 
translate and convert multiple inputs apply these   weight operations apply this non-linearity 
to then Define multiple predicted outputs   so taking a look at this diagram right on the left 
in blue you have inputs on the right in purple you   have these outputs and the green defines the 
neural the single neural network layer that's   transforming these inputs to the outputs Next 
Step I'm going to just simplify this diagram I'm   going to collapse down those stack perceptrons 
together and depict this with this green block   still it's the same operation going 
on right we have an input Vector being   being transformed to predict this output 
vector now what I've introduced here which   you may notice is this new variable T right 
which I'm using to denote a single time step   we are considering an input at a single time 
step and using our neural network to generate   a single output corresponding to that how could 
we start to extend and build off this to now   think about multiple time steps and how we could 
potentially process a sequence of information   well what if we took this diagram all I've done 
is just rotated it 90 degrees where we still have   this input vector and being fed in producing an 
output vector and what if we can make a copy of   this network right and just do this operation 
multiple times to try to handle inputs that are   fed in corresponding to different times right 
we have an individual time step starting with   t0 and we can do the same thing the same operation 
for the next time step again treating that as an   isolated instance and keep doing this repeatedly 
and what you'll notice hopefully is all these   models are simply copies of each other just with 
different inputs at each of these different time   steps and we can make this concrete right in terms 
of what this functional transformation is doing   the predicted output at a particular time step 
y hat of T is a function of the input at that   time step X of T and that function is what is 
learned and defined by our neural network weights   okay so I've told you that our goal here is 
Right trying to understand sequential data   do sequential modeling but what could 
be the issue with what this diagram is   showing and what I've shown you 
here well yeah go ahead [Music]   exactly that's exactly right so the student's 
answer was that X1 or it could be related to X   naught and you have this temporal dependence but 
these isolated replicas don't capture that at all   and that's exactly answers the question perfectly 
right here a predicted output at a later time   step could depend precisely on inputs at previous 
time steps if this is truly a sequential problem   with this temporal dependence so how could we 
start to reason about this how could we Define   a relation that links the Network's computations 
at a particular time step to Prior history and   memory from previous time steps well what if we 
did exactly that right what if we simply linked   the computation and the information understood 
by the network to these other replicas via what   we call a recurrence relation what this means is 
that something about what the network is Computing   at a particular time is passed on to those 
later time steps and we Define that according   to this variable H which we call this internal 
state or you can think of it as a memory term   that's maintained by the neurons and the 
network and it's this state that's being   passed time set to time step as we read in 
and and process this sequential information   what this means is that the Network's output 
its predictions its computations is not only   a function of the input data X but also we have 
this other variable H which captures this notion   of State captions captures this notion of memory 
that's being computed by the network and passed on   over time specifically right to walk through this 
our predicted output y hat of T depends not only   on the input at a time but also this past memory 
this past state and it is this linkage of temporal   dependence and recurrence that defines this idea 
of a recurrent neural unit what I've shown is this   this connection that's being unrolled over time 
but we can also depict this relationship according   to a loop this computation to this internal State 
variable h of T is being iteratively updated over   time and that's fed back into the neuron the 
neurons computation in this recurrence relation   this is how we Define these recurrent cells 
that comprise recurrent neural networks or   and the key here is that we have this this idea of 
this recurrence relation that captures the cyclic   temporal dependency and indeed it's this idea 
that is really the intuitive Foundation behind   recurrent neural networks or rnns and so let's 
continue to build up our understanding from here   and move forward into how we can actually Define 
the RNN operations mathematically and in code   so all we're going to do is formalize this 
relationship a little bit more the key idea   here is that the RNN is maintaining the state 
and it's updating the state at each of these   time steps as the sequence is is processed we 
Define this by applying this recurrence relation   and what the recurrence relation captures is how 
we're actually updating that internal State h of t   specifically that state update is exactly like any 
other neural network operator operation that we've   introduced so far where again we're learning 
a function defined by a set of Weights w we're   using that function to update the cell State h 
of t and the additional component the newness   here is that that function depends both on the 
input and the prior time step h of T minus one   and what you'll know is that this function f sub 
W is defined by a set of weights and it's the same   set of Weights the same set of parameters 
that are used time step to time step as the   recurrent neural network processes this temporal 
information the sequential data okay so the key   idea here hopefully is coming coming through is 
that this RNN stay update operation takes this   state and updates it each time a sequence is 
processed we can also translate this to how we   can think about implementing rnns in Python code 
or rather pseudocode hopefully getting a better   understanding and intuition behind how these 
networks work so what we do is we just start by   defining an RNN for now this is abstracted away 
and we start we initialize its hidden State and   we have some sentence right let's say this is 
our input of Interest where we're interested in   predicting maybe the next word that's occurring in 
this sentence what we can do is Loop through these   individual words in the sentence that Define our 
temporal input and at each step as We're looping   through each word in that sentence is fed into 
the RNN model along with the previous hidden state   and this is what generates a prediction for 
the next word and updates the RNN state in turn   finally our prediction for the final word in 
the sentence the word that we're missing is   simply the rnn's output after all the prior 
words have been fed in through the model   so this is really breaking down how the RNN Works 
how it's processing the sequential information   and what you've noticed is that the 
RNN computation includes both this   update to the hidden State as well 
as generating some predicted output   at the end that is our ultimate 
goal that we're interested in   and so to walk through this how we're actually 
generating the output prediction itself   what the RNN computes is given some input vector 
it then performs this update to the hidden state   and this update to the head and state is just 
a standard neural network operation just like   we saw in the first lecture where it consists of 
taking a weight Matrix multiplying that by the   previous hidden State taking another weight Matrix 
multiplying that by the input at a time step and   applying a non-linearity and in this case right 
because we have these two input streams the input   data X of T and the previous state H we have these 
two separate weight matrices that the network is   learning over the course of its training that 
comes together we apply the non-linearity and   then we can generate an output at a given 
time step by just modifying the hidden state   using a separate weight Matrix to update this 
value and then generate a predicted output   and that's what there is to it right 
that's how the RNN in its single   operation updates both the hidden State 
and also generates a predicted output   okay so now this gives you the internal working 
of how the RNN computation occurs at a particular   time step let's next think about how this looks 
like over time and Define the computational graph   of the RNN as being unrolled or expanded acrost 
across time so so far the dominant way I've been   showing the rnns is according to this loop-like 
diagram on the Left Right feeding back in on   itself another way we can visualize and think 
about rnns is as kind of unrolling this recurrence   over time over the individual time steps in our 
sequence what this means is that we can take   the network at our first time step and continue 
to iteratively unroll it across the time steps   going on forward all the way until we process all 
the time steps in our input now we can formalize   this diagram a little bit more by defining the 
weight matrices that connect the inputs to the   hidden State update and the weight matrices that 
are used to update the internal State across time   and finally the weight matrices that Define 
the the update to generate a predicted output   now recall that in all these cases right for all 
these three weight matrices add all these time   steps we are simply reusing the same weight 
matrices right so it's one set of parameters   one set of weight matrices that just process this 
information sequentially now you may be thinking   okay so how do we actually start to be thinking 
about how to train the RNN how to define the loss   given that we have this temporal processing in 
this temporal dependence well a prediction at   an individual time step will simply amount to 
a computed loss at that particular time step   so now we can compare those predictions time step 
by time step to the true label and generate a loss   value for those timestamps and finally we can get 
our total loss by taking all these individual loss   terms together and summing them defining the 
total loss for a particular input to the RNN   if we can walk through an example of how we 
implement this RNN in tensorflow starting from   scratch the RNN can be defined as a layer 
operation and a layer class that Alexander   introduced in the first lecture and so we can 
Define it according to an initialization of weight   matrices initialization of a hidden state which 
commonly amounts to initializing these two to zero   next we can Define how we can actually pass 
forward through the RNN Network to process a given   input X and what you'll notice is in this forward 
operation the computations are exactly like we   just walked through we first update the hidden 
state according to that equation we introduced   earlier and then generate a predicted output that 
is a transformed version of that hidden state   and finally at each time step we return it 
both the output and the updated hidden State   as this is what is necessary to be stored 
to continue this RNN operation over time   what is very convenient is that although you 
could Define your RNN Network and your RNN   layer completely from scratch is that tensorflow 
abstracts this operation away for you so you can   simply Define a simple RNN according to 
uh this this call that you're seeing here   um which yeah makes all the the computations 
very efficient and and very easy   and you'll actually get practice implementing and 
working with with rnns in today's software lab   okay so that gives us the understanding of 
rnns and going back to what I what I described   as kind of the problem setups or the problem 
definitions at the beginning of this lecture   I just want to remind you of the types of sequence 
modeling problems on which we can apply rnns right   we can think about taking a sequence of 
inputs producing one predicted output   at the end of the sequence we can think 
about taking a static single input and   trying to generate text according 
to according to that single input   and finally we can think about taking a sequence 
of inputs producing a prediction at every time   step in that sequence and then doing this sequence 
to sequence type of prediction and translation okay so yeah so so this will 
be the the foundation for   um the software lab today which will focus 
on this problem of of many to many processing   and many to many sequential modeling 
taking a sequence going to a sequence   what is common and what is universal across 
all these types of problems and tasks that we   may want to consider with rnns is what I like 
to think about what type of design criteria we   need to build a robust and reliable Network for 
processing these sequential modeling problems what   I mean by that is what are the characteristics 
what are the the design requirements that the   RNN needs to fulfill in order to be able 
to handle sequential data effectively   the first is that sequences can be of different 
lengths right they may be short they may be   long we want our RNN model or our neural network 
model in general to be able to handle sequences of   variable lengths secondly and really importantly 
is as we were discussing earlier that the whole   point of thinking about things through the lens of 
sequence is to try to track and learn dependencies   in the data that are related over time so 
our model really needs to be able to handle   those different dependencies which may occur at 
times that are very very distant from each other next right sequence is all about order right 
there's some notion of how current inputs depend   on prior inputs and the specific order of the 
observations we see makes a big effect on what   prediction we may want to generate at the end 
and finally in order to be able to process this   information effectively our Network needs to be 
able to do what we call parameter sharing meaning   that given one set of Weights that set of weights 
should be able to apply to different time steps   in the sequence and still result in a meaningful 
prediction and so today we're going to focus on   how recurrent neural networks meet these design 
criteria and how these design criteria motivate   the need for even more powerful architectures 
that can outperform rnns in sequence modeling   so to understand these criteria very concretely 
we're going to consider a sequence modeling   problem where given some series of words our task 
is just to predict the next word in that sentence   so let's say we have this sentence this morning I 
took my cat for a walk and our task is to predict   the last word in the sentence given the prior 
words this morning I took my cap for a blank   our goal is to take our RNN Define it and put 
it to test on this task what is our first step   to doing this well the very very first step before 
we even think about defining the RNN is how we can   actually represent this information to the network 
in a way that it can process and understand if we have a model that is processing this data 
processing this text-based data and wanting to   generate text as the output our problem can arise 
in that the neural network itself is not equipped   to handle language explicitly right remember 
that neural networks are simply functional   operators they're just mathematical operations 
and so we can't expect it right it doesn't have   an understanding from the start of what a word is 
or what language means which means that we need a   way to represent language numerically so that 
it can be passed in to the network to process   so what we do is that we need to define 
a way to translate this text this this   language information into a numerical 
encoding a vector an array of numbers   that can then be fed in to our neural network and 
generating a a vector of numbers as its output so now right this raises the question 
of how do we actually Define this   transformation how can we transform 
language into this numerical encoding   the key solution and the key way that a 
lot of these networks work is this notion   and concept of embedding what that means 
is it it's some transformation that takes   indices or something that can be represented as 
an index into a numerical Vector of a given size   so if we think about how this idea 
of embedding works for language data   let's consider a vocabulary of words that we can 
possibly have in our language and our goal is to   be able to map these individual words in our 
vocabulary to a numerical Vector of fixed size   one way we could do this is by defining all the 
possible words that could occur in this vocabulary   and then indexing them assigning a index 
label to each of these distinct words   a corresponds to index one cat responds to index 
two so on and so forth and this indexing Maps   these individual words to numbers unique indices 
what these indices can then Define is what we   call a embedding vector which is a fixed length 
encoding where we've simply indicated a one value   at the index for that word when we observe 
that word and this is called a one-hot   embedding where we have this fixed length 
Vector of the size of our vocabulary and   each instance of the vocabulary corresponds 
to a one-hot one at the corresponding index   this is a very sparse way to do 
this and it's simply based on   purely purely count the count index there's 
no notion of semantic information meaning   that's captured in this vector-based encoding 
alternatively what is very commonly done is to   actually use a neural network to learn in encoding 
to learn in embedding and the goal here is that we   can learn a neural network that then captures 
some inherent meaning or inherent semantics   in our input data and Maps related words or 
related inputs closer together in this embedding   space meaning that they'll have numerical 
vectors that are more similar to each other   this concept is really really foundational to 
how these sequence modeling networks work and   how neural networks work in general okay so with 
that in hand we can go back to our design criteria   thinking about the capabilities that we desire 
first we need to be able to handle variable   length sequences if we again want to predict 
the next word in the sequence we can have short   sequences we can have long sequences we can have 
even longer sentences and our key task is that we   want to be able to track dependencies across 
all these different lengths and what we need   what we mean by dependencies is that there could 
be information very very early on in a sequence   but uh that may not be relevant or come up 
late until very much later in the sequence   and we need to be able to track these dependencies 
and maintain this information in our Network   dependencies relate to order and sequences are 
defined by their order and we know that same words   in a completely different order have completely 
different meanings right so our model needs to   be able to handle these differences in order and 
the differences in length that could result in   different predicted outputs okay so hopefully 
that example going through the example in text   motivates how we can think about transforming 
input data into a numerical encoding that can   be passed into the RNN and also what are 
the key criteria that we want to meet in   handling these these types of problems so so 
far we've painted the picture of rnn's how they   work intuition their mathematical operations and 
what are the key criteria that they need to meet   the final piece to this is how we actually train 
and learn the weights in the RNN and that's done   through back propagation algorithm with a bit 
of a Twist to just handle sequential information if we go back and think about how we train feed 
forward neural network models the steps break down   in thinking through starting with an input where 
we first take this input and make a forward pass   through the network going from input to Output the 
key to back propagation that Alexander introduced   was this idea of taking the prediction and back 
propagating gradients back through the network   and using this operation to then Define and 
update the loss with respect to each of the   parameters in the network in order to gradually 
adjust the parameters the weights of the network   in order to minimize the overall loss now with 
rnns as we walked through earlier we have this   temporal unrolling which means that we have these 
individual losses across the individual steps   in our sequence that sum together to comprise 
the overall loss what this means is that when   we do back propagation we have to now instead of 
back propagating errors through a single Network   back propagate the loss through 
each of these individual time steps   and after we back propagate loss through each 
of the individual time steps we then do that   across all time steps all the way from our current 
time time T back to the beginning of the sequence   and this is the why this is why this algorithm is 
called back propagation Through Time right because   as you can see the data and the the predictions 
and the resulting errors are fed back in time   all the way from where we are currently to 
the very beginning of the input data sequence so the back propagations through time is actually 
a very tricky algorithm to implement uh in   practice and the reason for this is if we take a 
close look looking at how gradients flow across   the RNN what this algorithm involves is that many 
many repeated computations and multiplications of   these weight matrices repeatedly against each 
other in order to compute the gradient with   respect to the very first time step we have to 
make many of these multiplicative repeats of   the weight Matrix why might this be problematic 
well if this weight Matrix W is very very big   what this can result in is what they call what 
we call the exploding gradient problem where our   gradients that we're trying to use to optimize our 
Network do exactly that they blow up they explode   and they get really big and makes it infeasible 
and not possible to train the network stably what   we do to mitigate this is a pretty simple solution 
called gradient clipping which effectively scales   back these very big gradients to try to 
constrain them more in a more restricted way   conversely we can have the instance where the 
weight matrices are very very small and if these   weight matrices are very very small we end up with 
a very very small value at the end as a result of   these repeated weight Matrix computations and 
these repeated um multiplications and this is   a very real problem in rnns in particular where 
we can lead into this funnel called a Vanishing   gradient where now your gradient has just dropped 
down close to zero and again you can't train the   network stably now there are particular tools that 
we can use to implement that we can Implement to   try to mitigate the Spanish ingredient problem 
and we'll touch on each of these three solutions   briefly first being how we can Define the 
activation function in our Network and how we   can change the network architecture itself to try 
to better handle this Vanishing gradient problem   before we do that I want to take just 
one step back to give you a little more   intuition about why Vanishing gradients can 
be a real issue for recurrent neural networks   Point I've kept trying to reiterate is this notion 
of dependency in the sequential data and what   it means to track those dependencies well if the 
dependencies are very constrained in a small space   not separated out that much by time this repeated 
gradient computation and the repeated weight   matrix multiplication is not so much of a problem 
if we have a very short sequence where the words   are very closely related to each other and it's 
pretty obvious what our next output is going to be   the RNN can use the immediately passed 
information to make a prediction   and so there are not going to be that many uh 
that much of a requirement to learn effective   weights if the related information 
is close to to each other temporally   conversely now if we have a sentence 
where we have a more long-term dependency   what this means is that we need information from 
way further back in the sequence to make our   prediction at the end and that gap between what's 
relevant and where we are at currently becomes   exceedingly large and therefore the vanishing 
gradient problem is increasingly exacerbated   meaning that we really need to um the RNN becomes 
unable to connect the dots and establish this   long-term dependency all because of this Vanishing 
gradient issue so the ways that we can imply the   ways and modifications that we can make to our 
Network to try to alleviate this problem threefold   the first is that we can simply change 
the activation functions in each of our   neural network layers to be such that they can 
effectively try to mitigate and Safeguard from   gradients in instances where from shrinking the 
gradients in instances where the data is greater   than zero and this is in particular true for the 
relu activation function and the reason is that in   all instances where X is greater than zero with 
the relu function the derivative is one and so   that is not less than one and therefore it helps 
in mitigating The Vanishing gradient problem   another trick is how we initialize the parameters 
in the network itself to prevent them from   shrinking to zero too rapidly and there are there 
are mathematical ways that we can do this namely   by initializing our weights to Identity matrices 
and this effectively helps in practice to prevent   the weight updates to shrink too rapidly to zero 
however the most robust solution to the vanishing   gradient problem is by introducing a slightly 
more complicated uh version of the recurrent   neural unit to be able to more effectively track 
and handle long-term dependencies in the data   and this is this idea of gating and what the 
idea is is by controlling selectively the flow   of information into the neural unit to be able to 
filter out what's not important while maintaining   what is important and the key and the most popular 
type of recurrent unit that achieves this gated   computation is called the lstm or long short term 
memory Network today we're not going to go into   detail on lstn's their mathematical details their 
operations and so on but I just want to convey the   key idea and intuitive idea about why these lstms 
are effective at tracking long-term dependencies   the core is that the lstm is able to um control 
the flow of information through these gates   to be able to more effectively filter out the 
unimportant things and store the important things   what you can do is Implement Implement lstms 
in tensorflow just as you would in RNN but   the core concept that I want you to take away when 
thinking about the lstm is this idea of controlled   information flow through Gates very briefly the 
way that lstm operates is by maintaining a cell   State just like a standard RNN and that cell state 
is independent from what is directly outputted   the way the cell state is updated is according to 
these Gates that control the flow of information   for getting and eliminating what is irrelevant 
storing the information that is relevant   updating the cell state in turn and then 
filtering this this updated cell state to   produce the predicted output just like the 
standard RNN and again we can train the lstm   using the back propagation Through Time algorithm 
but the mathematics of how the lstm is defined   allows for a completely uninterrupted flow of the 
gradients which completely eliminates the well   largely eliminates the The Vanishing 
gradient problem that I introduced earlier   again we're not if you're if you're interested 
in learning more about the mathematics and the   details of lstms please come and discuss 
with us after the lectures but again just   emphasizing the core concept and the 
intuition behind how the lstm operates   okay so so far where we've out where we've been at 
we've covered a lot of ground we've gone through   the fundamental workings of rnns the architecture 
the training the type of problems that they've   been applied to and I'd like to close this 
part by considering some concrete examples   of how you're going to use rnns in your software 
lab and that is going to be in the task of Music   generation where you're going to work to build an 
RNN that can predict the next musical note in a   sequence and use it to generate brand new musical 
sequences that have never been realized before   so to give you an example of just the quality 
and and type of output that you can try to aim   towards a few years ago there was a work that 
trained in RNN on a corpus of classical music   data and famously there's this composer 
Schubert who uh wrote a famous unfinished   Symphony that consisted of two movements but 
he was unable to finish his uh his Symphony   before he died so he died and then he left the 
third movement unfinished so a few years ago a   group trained a RNN based model to actually try to 
generate the third movement to Schubert's famous   unfinished Symphony given the prior to movements 
so I'm going to play the result quite right now [Music] okay I I paused it I interrupted it quite 
abruptly there but if there are any classical   music aficionados out there hopefully you get 
a appreciation for kind of the quality that was   generated uh in in terms of the music quality and 
this was already from a few years ago and as we'll   see in the next lectures the and continuing with 
this theme of generative AI the power of these   algorithms has advanced tremendously since 
we first played this example um particularly   in you know a whole range of domains which I'm 
excited to talk about but not for now okay so   you'll tackle this problem head on in today's 
lab RNN music generation foreign we can think   about the the simple example of input sequence 
to a single output with sentiment classification   where we can think about for example text like 
tweets and assigning positive or negative labels   to these these text examples based on the 
content that that is learned by the network   okay so this kind of concludes the portion on rnns 
and I think it's quite remarkable that using all   the foundational Concepts and operations 
that we've talked about so far we've been   able to try to build up networks that handle 
this complex problem of sequential modeling   but like any technology right and RNN is not 
without limitations so what are some of those   limitations and what are some potential issues 
that can arise with using rnns or even lstms   the first is this idea of encoding and and 
dependency in terms of the the temporal separation   of data that we're trying to process while rnns 
require is that the sequential information is fed   in and processed time step by time step what that 
imposes is what we call an encoding bottleneck   right where we have we're trying to encode a lot 
of content for example a very large body of text   many different words into a single output that 
may be just at the very last time step how do   we ensure that all that information leading up to 
that time step was properly maintained and encoded   and learned by the network in practice this is 
very very challenging and a lot of information   can be lost another limitation is that by 
doing this time step by time step processing   rnns can be quite slow there is not really 
an easy way to parallelize that computation   and finally together these components of 
the encoding bottleneck the requirement to   process this data step by step imposes the biggest 
problem which is when we talk about long memory   the capacity of the RNN and the lstm is really not 
that long we can't really handle data of tens of   thousands or hundreds of thousands or even Beyond 
sequential information that effectively to learn   the complete amount of information and patterns 
that are present within such a rich data source   and so because of this very recently there's been 
a lot of attention in how we can move Beyond this   notion of step-by-step recurrent processing 
to build even more powerful architectures for   processing sequential data to understand how we 
do how we can start to do this let's take a big   step back right think about the high level goal 
of sequence modeling that I introduced at the   very beginning given some input a sequence of data 
we want to build a feature encoding and use our   neural network to learn that and then transform 
that feature encoding into a predicted output   what we saw is that rnns use this notion 
of recurrence to maintain order information   processing information time step by time step 
but as I just mentioned we had these key three   bottlenecks to rnns what we really want to achieve 
is to go beyond these bottlenecks and Achieve even   higher capabilities in terms of the power of 
these models rather than having an encoding   bottleneck ideally we want to process information 
continuously as a continuous stream of information   rather than being slow we want to be able to 
parallelize computations to speed up processing   and finally of course our main goal is 
to really try to establish long memory   that can build nuanced and Rich 
understanding of sequential data   the limitation of rnns that's linked to all 
these problems and issues in our inability   to achieve these capabilities is that they 
require this time step by time step processing   so what if we could move beyond that what if we 
could eliminate this need for recurrence entirely   and not have to process the data time set by time 
step well a first and naive approach would be   to just squash all the data all the time steps 
together to create a vector that's effectively   concatenated right the time steps are eliminated 
there's just one one stream where we have now one   vector input with the data from all time points 
that's then fed into the model it calculates some   feature vector and then generates some output 
which hopefully makes sense and because we've   squashed all these time steps together we 
could simply think about maybe building a   feed forward Network that could that could do this 
computation well with that we'd eliminate the need   for recurrence but we still have the issues that 
it's not scalable because the dense feed forward   Network would have to be immensely large defined 
by many many different connections and critically   we've completely lost our in order information by 
just squashing everything together blindly there's   no temporal dependence and we're then stuck in 
our ability to try to establish long-term memory so what if instead we could still think 
about bringing these time steps together   but be a bit more clever about how we try 
to extract information from this input data   the key idea is this idea of being able to 
identify and attend to what is important in   a potentially sequential stream of information and 
this is the notion of attention or self-attention   which is an extremely extremely powerful Concept 
in modern deep learning and AI I cannot understate   or I don't know understand overstate I I cannot 
emphasize enough how powerful this concept is   attention is the foundational mechanism of the 
Transformer architecture which many of you may   have heard about and it's the the the notion 
of a transformer can often be very daunting   because sometimes they're presented with these 
really complex diagrams or deployed in complex   applications and you may think okay how 
do I even start to make sense of this   at its core though attention the key operation 
is a very intuitive idea and we're going to in   the last portion of this lecture break 
it down step by step to see why it's so   powerful and how we can use that as part of 
a larger neural network like a Transformer   specifically we're going to be talking and 
focusing on this idea of self-attention   attending to the most important parts of an 
input example so let's consider an image I   think it's most intuitive to consider an image 
first this is a picture of Iron Man and if our   goal is to try to extract information from this 
image of what's important what we could do maybe   is using our eyes naively scan over this image 
pixel by pixel right just going across the image   however our brains maybe maybe internally they're 
doing some type of computation like this but you   and I we can simply look at this image and 
be able to attend to the important parts   we can see that it's Iron Man coming at you 
right in the image and then we can focus in   a little further and say okay what are the 
details about Iron Man that may be important   what is key what you're doing is your brain 
is identifying which parts are attending   to to attend to and then extracting those 
features that deserve the highest attention   the first part of this problem is really 
the most interesting and challenging one   and it's very similar to the concept of search 
effectively that's what search is doing taking   some larger body of information and trying 
to extract and identify the important parts   so let's go there next how does search work you're 
thinking you're in this class how can I learn more   about neural networks well in this day and age one 
thing you may do besides coming here and joining   us is going to the internet having all the videos 
out there trying to find something that matches   doing a search operation so you have a giant 
database like YouTube you want to find a video   you enter in your query deep learning and 
what comes out are some possible outputs   right for every video in the database there is 
going to be some key information related to the   interview to that to that video let's say the 
title now to do the search what the task is to   find the overlaps between your query and each 
of these titles right the keys in the database   what we want to compute is a metric of similarity 
and relevance between the query and these keys how   similar are they to our desired query and we can 
do this step by step let's say this first option   of a video about the elegant giant sea turtles 
not that similar to our query about deep learning   our second option introduction to deep learning 
the first introductory lecture on this class yes   highly relevant the third option a video about 
the late and great Kobe Bryant not that relevant   the key operation here is that there is this 
similarity computation bringing the query   and the key together the final step is now that 
we've identified what key is relevant extracting   the relevant information what we want to pay 
attention to and that's the video itself we   call this the value and because the searches 
is implemented well right we've successfully   identified the relevant video on deep learning 
that you are going to want to pay attention to   and it's this this idea this intuition of 
giving a query trying to find similarity   trying to extract the related values 
that form the basis of self-attention   and how it works in neural networks like 
Transformers so to go concretely into this   right let's go back now to our text our language 
example with the sentence our goal is to identify   and attend to features in this input that are 
relevant to the semantic meaning of the sentence   now first step we have sequence we have order 
we've eliminated recurrence right we're feeding in   all the time steps all at once we still need a way 
to encode and capture this information about order   and this positional dependence how this is done is 
this idea of possession positional encoding which   captures some inherent order information present 
in the sequence I'm just going to touch on this   very briefly but the idea is related to this 
idea of embeddings which I introduced earlier   what is done is a neural network layer is 
used to encode positional information that   captures the relative relationships in 
terms of order within within this text   that's the high level concept right we're still 
being able to process these time steps all at once   there is no notion of time step rather the data 
is singular but still we learned this encoding   that captures the positional order information now 
our next step is to take this encoding and figure   out what to attend to exactly like that search 
operation that I introduced with the YouTube   example extracting a query extracting a key 
extracting a value and relating them to each other   so we use neural network layers to do exactly 
this given this positional encoding what   attention does is applies a neural network layer 
transforming that first generating the query   we do this again using a separate neural network 
layer and this is a different set of Weights a   different set of parameters that then transform 
that positional embedding in a different way   generating a second output the key and finally 
this repeat this operation is repeated with a   third layer a third set of Weights generating the 
value now with these three in hand the key the   the query the key and the value we can compare 
them to each other to try to figure out where   in that self-input the network should attend 
to what is important and that's the key idea   behind this similarity metric or what you can 
think of as an attention score what we're doing   is we're Computing a similarity score between a 
query and the key and remember that these query   and Qui key values are just arrays of numbers 
we can Define them as arrays of numbers which   you can think of as vectors in space the query 
Vector the query values are some Vector the key   the key values are some other vector and 
mathematically the way that we can compare these   two vectors to understand how similar they are is 
by taking the dot product and scaling it captures   how similar these vectors are how whether or not 
they're pointing in the same direction right this   is the similarity metric and if you are familiar 
with a little bit of linear algebra this is also   known as the cosine similarity operation functions 
exactly the same way for matrices if we apply this   dot product operation to our query in key matrices 
key matrices we get this similarity metric out   now this is very very key in defining our next 
step Computing the attention waiting in terms of   what the network should actually attend to within 
this input this operation gives us a score which   defines how how the components of the input data 
are related to each other so given a sentence   right when we compute this similarity score metric 
we can then begin to think of Weights that Define   the relationship between the sequential the 
components of the sequential data to each other   so for example in the this example with a text 
sentence he tossed the tennis ball to serve   the goal with the score is that words in the 
sequence that are related to each other should   have high attention weights ball related to 
toss related to tennis and this metric itself   is our attention waiting what we have done is 
passed that similarity score through a soft Max   function which all it does is it constrains 
those values to be between 0 and 1. and so   you can think of these as relative scores of 
relative attention weights finally now that we   have this metric that can captures this notion of 
similarity and these internal self-relationships   we can finally use this metric to extract 
features that are deserving of high attention   and that's the exact final step in this 
self-attention mechanism in that we take that   attention waiting Matrix multiply it by the value 
and get a transformed transformation of of the   initial data as our output which in turn reflects 
the features that correspond to high attention all right let's take a breath let's 
recap what we have just covered so far   the goal with this idea of self-attention 
the backbone of Transformers is to eliminate   recurrence attend to the most important features 
in in the input data in an architecture how   this is actually deployed is first we take our 
input data we compute these positional encodings   the neural network layers are applied three-fold 
to transform the positional encoding into each   of the key query and value matrices we can 
then compute the self-attention weight score   according to the up the dot product operation 
that we went through prior and then self-attend   to these features to these uh information to 
extract features that deserve High attention what is so powerful about this approach in taking 
this attention wait putting it together with the   value to extract High attention features is that 
this operation the scheme that I'm showing on   the right defines a single self-attention head 
and multiple of these self-attention heads can   be linked together to form larger Network 
architectures where you can think about   these different heads trying to extract different 
information different relevant parts of the input   to now put together a very very rich encoding and 
representation of the data that we're working with   intuitively back to our Ironman example what 
this idea of multiple self-attention heads   can amount to is that different Salient features 
and Salient information in the data is extracted   first maybe you consider Iron Man attention had 
one and you may have additional attention heads   that are picking out other relevant parts of 
the data which maybe we did not realize before   for example the building or the spaceship 
in the background that's chasing iron   man and so this is a key building block of many 
many many many powerful architectures that are out   there today today I again cannot emphasize 
how enough how powerful this mechanism is   and indeed this this backbone idea of 
self-attention that you just built up   understanding of is the key operation of some 
of the most powerful neural networks and deep   learning models out there today ranging from the 
very powerful language models like gpt3 which are   capable of synthesizing natural language in a 
very human-like fashion digesting large bodies   of text information to understand relationships 
in text to models that are being deployed for   extremely impactful applications in biology 
and Medicine such as Alpha full 2 which uses   this notion of self-attention to look at data 
of protein sequences and be able to predict the   three-dimensional structure of a protein just 
given sequence information alone and all the   way even now to computer vision which will be the 
topic of our next lecture tomorrow where the same   idea of attention that was initially developed in 
sequential data applications has now transformed   the field of computer vision and again using 
this key concept of attending to the important   features in an input to build these very rich 
representations of complex High dimensional data   okay so that concludes lectures for today I 
know we have covered a lot of territory in a   pretty short amount of time but that is what this 
boot camp program is all about so hopefully today   you've gotten a sense of the foundations of neural 
networks in the lecture with Alexander we talked   about rnns how they're well suited for sequential 
data how we can train them using back propagation   how we can deploy them for different applications 
and finally how we can move Beyond recurrence to   build this idea of self-attention for 
building increasingly powerful models   for deep learning in sequence modeling 
all right hopefully you enjoyed we have   um about 45 minutes left for the for the 
lab portion and open Office hours in which   we welcome you to ask us questions uh of us 
and the Tas and to start work on the labs   the information for the labs is is up there 
thank you so much for your attention foreign 

Hi everyone and welcome back 
to Intro to Deep Learning! We had a really awesome kickoff day 
yesterday so we're looking to keep   that same momentum all throughout 
the week and starting with today. Today we're really excited to be talking 
about actually one of my favorite topics in   this course which is how we can build computers 
that can achieve the sense of sight and vision. Now I believe that sight and 
specifically like I said vision   is one of the most important 
human senses that we all have. In fact sighted people rely on vision 
quite a lot in our day-to-day lives   from everything from walking around navigating the   world interacting and sensing other 
emotions in our colleagues and peers. And today we're going to learn about how we 
can use deep learning and machine learning   to build powerful vision systems that can both   see and predict what is where by 
only looking at raw visual inputs. I like to think of that phrase as a 
very concise and sweet definition of   what it really means to achieve vision but at its   core vision is actually so much more 
than just understanding what is where. It also goes much deeper takes the scene for 
example we can build computer vision systems   that can identify of course all of the objects 
in this environment starting first with the   yellow taxi or the van parked on the side of 
the road but we also need to understand each   of these objects at a much deeper level not just 
where they are but actually predicting the future   predicting what may happen in the scene next for 
example that the yellow taxi is more likely to   be moving and dynamic into the future because 
it's in the middle of the lane compared to the   white van which is parked on the side of the road 
even though you're just looking at a single image   your brain can infer all of these very subtle cues 
and it goes all the way to the pedestrians on the   road and even these even more subtle cues in the 
traffic lights and the rest of the scene as well   now accounting for all of these details in the 
scene is an extraordinary challenge but we as   humans do this so seamlessly within a split second 
I probably put that frame up on the slide and all   of you within a split stepping could reason 
about many of those subtle details without me   even pointing them out but the question of today's 
class is how we can build machine learning and   deep learning algorithms that can achieve that 
same type and subtle understanding of our world   and deep learning in particular is really leading 
this revolution of computer vision and achieving   sight of computers for example allowing robots 
to keep pick up on these key visual cues in their   environment critical for really navigating 
the world together with us as humans these   algorithms that you're going to learn about today 
have become so mainstreamed in fact that they're   fitting on all of your smartphones and your 
pockets processing every single image that you   take enhancing those images detecting faces and 
so on and so forth and we're seeing some exciting   advances ranging all the way from biology and 
Medicine which we'll talk about a bit later today   to autonomous driving and accessibility as well 
and like I said deep learning has taken this field   as a whole by storm in the over the past decade 
or so because of its ability critically like we   were talking about yesterday its ability to learn 
directly from raw data and those raw image inputs   in what it sees in its environment and learn 
explicitly how to perform like we talked about   yesterday what is called feature extraction of 
those images in the environment and one example of   that is through facial detection and recognition 
which all of you are going to get practice with in   today's and tomorrow's Labs as part of the grand 
final competition of this class another really   go-to example of computer vision is in autonomous 
driving and self-driving Vehicles where we can   take an image as input or maybe potentially a 
video as input multiple images and process all   of that data so that we can train a car to learn 
how to steer the wheel or command a throttle or   actuate a breaking command this entire control 
system the steering the throttle the braking   of a car can be executed end to end by taking 
as input the images and the sensing modalities   of the vehicle and learning how to predict those 
actuation commands now actually this end-to-end   approach having a single neural network do all of 
this is actually radically different than the vast   majority of autonomous vehicle companies like if 
you look at waymo for example that's a radically   different approach but we'll talk about those 
approaches in today's class and in fact this is   one of our vehicles that we've been building at 
MIT in my lab in CSL just a few floors above this   room and we'll again and share some of the details 
on this incredible work but of course it doesn't   stop here with autonomous driving these algorithms 
directly the same algorithms that you'll learn   about in today's class can be extended all 
the way to impact Healthcare medical decision   making and finally even in these accessibility 
applications where we're seeing computer vision   algorithms helping the visually impaired so for 
example in this project researchers have built   deep learning enabled devices that could detect 
Trails so that visually impaired Runners could   be provided audible feedback so that they too 
could you know navigate when they go out for runs   and like I said we often take many of these tasks 
that we're going to talk about in today's lecture   for granted because we do them so seamlessly in 
our day-to-day lives but the question of today's   class is going to be at its core how we can build 
a computer to do these same types of incredible   things that all of us take for granted day to day 
and specifically we'll start with this question of   how does a computer really see and even more 
detailed than that is how does a computer   process an image if we think of you know site 
as coming to computers through images then how   can a computer even start to process those images 
well to a computer images are just numbers right   and suppose for example we have a picture here 
of Abraham Lincoln okay this picture is made up   of what are called pixels every pixel is just a 
dot in this image and since this is a grayscale   image each of these pixels is just a single 
number now we can represent our image now as   this two-dimensional Matrix of numbers and because 
like I said this is a grayscale image every pixel   is corresponding to just one number added that 
Matrix location now assume for example we didn't   have a grayscale image we had a color image that 
would be an RGB image right so now every pixel   is going to be composed not just of one number 
but of three numbers so you can think of that as   kind of a 3D Matrix instead of a 2d Matrix where 
you almost have three two-dimensional Matrix that   are stat stacked on top of each other so now with 
this basis of basically numerical representations   of images we can start to think about how we can 
uh or what types of computer vision algorithms   we can build that can take these systems as input 
and what they can perform right so the first thing   that I want to talk to you about is what kind of 
tasks do we even want to train these systems to   complete with images and broadly speaking there 
are two broad categories of tasks we touched   on this a little bit in yesterday's lecture but 
just to be a bit more Concrete in today's lecture   those two tasks are either classification or 
regression now in regression your prediction   value is going to take a continuous value right 
that could be any real number on the number line   but in classification your prediction could take 
one of let's say k or n different classes right   these are discrete different classes so let's 
consider first the task of image classification   in this test we want to predict an individual 
label for every single image and this label   that we predict is going to be one of n different 
possible labels that could be considered so for   example let's say we have a bunch of images of U.S 
precedence and we want to build a classification   pipeline to tell us which President is in this 
particular image that you see on the screen now   the goal of our model in this case is going 
to be basically to Output a probability score   a probability of this image containing one of 
these different precedents right and the maximum   score is going to be ultimately the one that we 
infer to be the correct precedent in the image   so in order to correctly perform this task and 
correctly classify these images our pipeline our   computer vision model needs the ability to be able 
to tell us what is unique about this particular   image of Abraham Lincoln for example versus a 
different picture of George Washington versus a   different picture of Obama for example now another 
way to think about this whole problem of image   classification or image processing at its high 
level is in terms of features or think of these   as almost patterns in your data or characteristics 
of a particular class and classification then is   simply done by detecting all of these different 
patterns in your data and identifying when certain   patterns occur over other patterns so for example 
if the features of a particular class are present   in an image then you might infer that that image 
is of that class so for example if you want to   detect cars you might look for patterns in your 
data like Wheels license plates or headlights   and if those things are present in your image 
then you can say with fairly high confidence   that your images of a car versus one of these 
other categories so if we're building a computer   vision pipeline we have two main steps really to 
consider the first step is that we need to know   what features or what patterns we're looking 
for in our data and the second step is we need   to then detect those patterns once we detect 
them we can then infer which class we're in   now one way to solve this is to leverage knowledge 
about our particular Fields right so we if we know   something about our field for example about human 
faces we can use that knowledge to Define our   features right what makes up a face we know faces 
are made up of eyes noses and ears for example we   can Define what each of those components look like 
in defining our features but there's a big problem   with this approach and remember that images are 
just these three-dimensional arrays of numbers   right they can have a lot of variation even within 
the same type of object these variations can   include really anything ranging from occlusions 
to variations in lighting rotations translations   into a class variation and the problem here 
is that our classification pipeline needs the   ability to handle and be invariant to all of these 
different types of variations while still being   sensitive to all of the inter-class variations the 
variations that occur between different classes   now even though our pipeline could use features 
that we as humans you know Define manually Define   based on some of our prior knowledge the problem 
really breaks down in that these features become   very non-robust when considering all of these 
vast amounts of different variations that images   take in the real world so in practice like I said 
your algorithms need to be able to withstand all   of those different types of variations and then 
the natural question is that how can we build a   computer vision algorithm to do that and still 
maintain that level of robustness what we want   is a way to extract features that can both detect 
those features right those patterns in the data   and do so in a hierarchical fashion right so going 
all the way from the ground up from the pixel   level to something with semantic meaning like 
for example the eyes or the noses in a human face   now we learned in the last class that we can 
use neural networks exactly for this type of   problem right neural networks are capable 
of learning features directly from data   and learn most importantly a hierarchical 
set of features building on top of previous   features that it's learned to build 
more and more complex set of features now we're going to see exactly how neural networks 
can do this in the image domain as part of this   lecture but specifically neural networks will 
allow us to learn these visual features from   visual data if we construct them cleverly and the 
key Point here is that actually the models and the   architectures that we learned about in yesterday's 
lecture and so far in this course we'll see how   they're actually not suitable or extensible to 
today's uh you know problem domain of images and   how we can build and construct neural networks 
a bit more cleverly to overcome those issues so   maybe let's start by revisiting what we talked 
about in lecture one which was where we learned   about fully connected networks now these were 
networks that you know have multiple hidden   layers and each neuron in a given hidden layer is 
connected to every neuron in its prior layer right   so it receives all of the previous layers inputs 
as a function of these fully connected layers now   let's say that we want to directly without any 
modifications use a fully connected Network like   we learned about in lecture one with an image 
processing pipeline so directly taking an image   and feeding it to a fully connected Network could 
we do something like that actually in this case we   could the way we would have to do it is remember 
that because our image is a two-dimensional array   the first thing that we would have to do is 
collapse that to a one-dimensional sequence   of numbers right because it's a fully connected 
network is not taking in a two-dimensional array   it's taking in a one-dimensional sequence so 
the first thing that we have to do is flatten   that two-dimensional array to a vector of 
pixel values and feed that to our Network   in this case every neuron in our first layer 
is connected to all neurons in that input layer   right so in that original image flattened down we 
feed all of those pixels to the first layer and   here you should already appreciate the very 
important notion that every single piece of   spatial information that really defined our image 
that makes an image and image is totally lost   already before we've even started this problem 
because we've flattened that two-dimensional image   into a one-dimensional array we've completely 
destroyed all notion of spatial information   and in addition we really have a enormous number 
of parameters because this system is fully   connected take for example in a very very small 
image which is even 100 by 100 pixels that's an   incredibly small image in today's standards but 
that's going to take 10 000 neurons just in the   first layer which will be connected to let's say 
10 000 neurons in the second layer the number of   parameters that you'll have just in that one layer 
alone is going to be 10 000 squared parameters   it's going to be highly inefficient you can 
imagine if you wanted to scale this network   to even a reasonably sized image that we have to 
deal with today so not feasible in practice but   instead we need to ask ourselves how we can build 
and maintain some of that spatial structure that's   very unique about images here into our input and 
here into our model most importantly so to do   this let's represent our 2D image as its original 
form as a two-dimensional array of numbers one way   that we can use spatial structure here inherent to 
our input is to connect what are called basically   these patches of our input to neurons in the 
hidden layer so for example let's say that each   neuron in the hidden layer that you can see here 
only is going to see or respond to a certain uh   set or a certain patch of neurons in the previous 
layer right so you could also think of this as   almost a receptive field or what the single neuron 
in your next layer can attend to in the previous   layer it's not the entire image but rather a 
small receptive field from your previous image   now notice here how the region of the input layer 
right which you can see on the left hand side here   influences that single neuron on the right hand 
side and that's just one neuron in the next layer   but of course you can imagine basically defining 
these connections across the whole input right   each time you have the single patch on your input 
that corresponds to a single neuron output on the   other layer and we can apply the same principle 
of connecting these patches across the entire   image to single neurons in the subsequent layer 
and we do this by essentially sliding that patch   pixel by pixel across the input image and we'll 
be responding with you know another image on our   output layer in this way we essentially preserve 
all of that very key and Rich spatial information   inherent to our input but remember that the 
ultimate task here is not only to just preserve   that spatial information we want to ultimately 
learn features learn those patterns so that we   can detect and classify these images and we can 
do this by waiting right waving the connections   between the patches of our input and and in order 
to detect you know what those certain features are   let me give a practical example here and so in 
practice this operation that I'm describing this   patching and sliding operation that I'm describing 
is actually a mathematical operation formerly   known as convolution we'll first think about this 
as a high level supposing that we have what's   called a four by four pixel patch right so you can 
see this 4x4 pixel patch represented in red as a   red box on the left hand side and let's suppose 
for example since we have a 4x4 patch this is   going to consist of 16 different weights in this 
layer we're going to apply this same four by four   let's call this not a patch anymore let's use the 
terminology filter we'll apply the same 4x4 filter   in the input and use the result of that operation 
to define the state of the neuron in the next   layer right and now we're going to shift our 
filter by let's say two pixels to the right   and that's going to define the next neuron in 
the adjacent location in the future layer right   and we keep doing this and you can see that on 
the right hand side you're sliding over not only   the input image but you're also sliding 
over the output neurons in the secondary   layer and this is how we can start to think 
about convolution at a very very high level   but you're probably wondering right not just how 
the convolution operation works but I think the   main thing here to really narrow down on is how 
convolution allows us to learn these features   these patterns in the data that we were talking 
about because ultimately that's our final goal   that's our real goal for this class is to extract 
those patterns so let's make this very concrete   by walking through maybe a concrete example 
right so suppose for example we want to build a   convolutional algorithm to detect or classify an X 
in an image right this is the letter X in an image   and we hear for Simplicity let's just say we have 
only black and white images right so every pixel   in this image will be represented by either 
a zero or a one for Simplicity there's no   grayscale in this image right and actually here 
so we're representing black as negative one and   white as positive one so to classify we simply 
cannot you know compare the left hand side to   the right hand side right because these are both 
X's but you can see that because the one on the   right hand side is slightly rotated to some degree 
it's not going to directly align with the X on the   left hand side even though it is an X we want to 
detect x's in both of these image so we need to   think about how we can detect those features that 
define an x a bit more cleverly so let's see how   we can use convolutions to do that so in this 
case for example instead we want our model to   compare images of this x piece by piece or patch 
by patch right and the important patches that we   look for are exactly these features that 
will Define our X so if our model can find   these rough feature patches roughly in the same 
positions in our input then we can determine or   we can infer that these two images are of the 
same type or the same letter right it can get   a lot better than simply measuring the similarity 
between these two images because we're operating   at the patch level so think of each patch 
almost like a miniature image right a small   two-dimensional array of values and we can use 
filters to pick up on when these small patches   or small images occur so in the case of x's these 
filters may represent semantic things for example   the diagonal lines or the crossings that capture 
all of the important characteristics of the X   so we'll probably capture these features in the 
arms and the center of our letter right in any   image of an X regardless of how that image is you 
know translated or rotated or so on and note that   even in these smaller matrices right these 
are filters of weights right these are also   just numerical values of each pixel in these mini 
patches is simply just a numerical value they're   also images in some effect right and all that's 
really left in this problem and in this idea that   we're discussing is to Define that operation that 
can take these miniature patches and try to pick   up you know detect when those patches occur 
in your image and when they maybe don't occur   and that brings us right back to this notion of 
convolution right so convolution is exactly that   operation that will solve that problem convolution 
preserves all of that spatial information in our   input by learning image features in those smaller 
squares of regions that preserve our input data so   just to give another concrete example to perform 
this operation we need to do an element wise   multiplication between the filter Matrix those 
miniature patches as well as the patch of our   input image right so you have basically think 
of two patches you have the weight Matrix patch   the thing that you want to detect which you can 
see on the top left hand here and you also have   the secondary patch which is the thing that you 
are looking to compare it against in your input   image and the question is how how similar are 
these two patches that you observe between them   so for example there was this results in a 
three by three Matrix because you're doing   an element-wise multiplication between two 
small three by three matrices you're going   to be left with another three by three Matrix in 
this case all of the Matrix all of the elements   of this resulting Matrix you can see here are 
ones right because in every location in the   filter and every location in the image patch 
we are perfectly matching so when we do that   element-wise multiplication we get ones everywhere 
the last step is that we need to sum up the result   of that Matrix or that element-wise multiplication 
and the result is let's say 9 in this case right   everything was a one it's a three by three Matrix 
so the result is nine now let's consider one more   example right now we have this image in green 
and we want to detect this filter in yellow   suppose we want to compute the convolution of 
this five by five image with this three by three   filter to do this we need to cover basically the 
entirety of our image by sliding over this filter   piece by piece and comparing the similarity or the 
convolution of this filter across the entire image   and we do that again through the same mechanism 
at every location we compute an element-wise   multiplication of that patch with that location 
on the image add up all of the resulting entries   and pass that to our next layer so let's walk 
through it first let's start off in the upper   left hand corner we place our filter over the 
upper left hand corner of our image we element   wise multiply we add up all the results and we 
get four and that 4 is going to be placed into   the next layer right this next layer again is 
another image right but it's determined as the   result of our convolution operation we slide 
over that filter to the next location the next   location provides the next value in our image 
and we keep repeating this process over and   over and over again until we've covered our filter 
over the entire image and as a result we've also   completely filled out the result of our output 
feature map the output feature map is basically   what you can think of as how closely aligned our 
filter is to every location in our input image   so now that we've kind of gone through the 
mechanism that defines this operation of   convolution let's see how different filters 
could be used to detect different types of   patterns in our data so for example let's take 
this picture of a woman's face and the output   of applying three different types of filters to 
this picture right so you can see the exact filter   this is they're all three by three filters so the 
exact filters you can see on the bottom right hand   corner of the corresponding face and by applying 
these three different filters you can see how we   can achieve drastically different results and 
simply by changing the weights that are present   in these three by three matrices you can see the 
variability of different types of features that   we can detect so for example we can design filters 
that can sharpen an image make the edges sharper   in the image we can design filters that will 
extract edges we can do stronger Edge detection   by again modifying the weights in all of those 
filters so I hope now that all of you can kind   of appreciate the power of you know number one is 
these filtering operations and how we can Define   them you know mathematically in the form of these 
smaller patch-based operations and matrices that   we can then slide over an image and these concepts 
are so powerful because number one they preserve   the spatial information of our original input 
while still performing this feature extraction   now you can think of instead of defining those 
filters like we said on the previous slide what   if we tried to learn them and remember again that 
those filters are kind of proxies for important   patterns in our data so our neural network 
could try to learn those elements of those   small patch filters as weights in the neural 
network and learning those would essentially   equate to picking up and learning the patterns 
that Define one class versus another class   and now that we've gotten this operation and this 
understanding under our belt we can take this   one step further right we can take this singular 
convolution operation and start to think about how   we can build entire layers convolutional layers 
out of this operation so that we can start to   even imagine convolutional networks and neural 
networks and first we'll take a look at you know   what are called well what you ultimately create 
by creating convolutional layers and convolutional   networks is what's called a CNN a convolutional 
neural network and that's going to be the core   architecture of today's class so let's consider 
a very simple CNN that was designed for image   classification the task here again is to learn the 
features directly from the raw data and use these   learn features for classification towards some 
task of object detection that we want to perform   now there are three main operations to a CNN and 
we'll go through them step by step here but then   go deeper into each of them in the remainder 
remainder of this class so the first step is   convolutions which we've already seen a lot of 
in today's class already convolutions are used   to generate these feature Maps so they take as 
input both the previous image as well as some   filter that they want to detect and they output a 
feature map of how this filter is related to the   original image the second step is like yesterday 
applying a non-linearity to the result of these   feature maps that injects some non-linear 
activations to our neural networks allows it to   deal with non-linear data third step is pooling 
which is essentially a down sampling operation   to allow our images or allow our networks to 
deal with larger and larger scale images by   progressively down scaling their size so that our 
filters can progressively grow in receptive field   and finally feeding all of these resulting 
features to some neural network to infer the   class scores now by the time that we get to this 
fully connected layer remember that we've already   extracted our features and essentially you can 
think of this no longer being a two-dimensional   image we can now use the methods that we 
learned about in lecture one to directly   take those learned features that the neural 
network has detected and infer based on those   learned features and based on what if they were 
detected or if they were not what class we're in   so now let's basically just go through each of 
these operations one by one in a bit more detail   and see how we could even build up this very 
basic architecture of a CNN so first let's go   back and consider one more time the convolution 
operation that Central a central core to the CNN   and as before each neuron in this hidden layer 
is going to be computed as a weighted sum of   its inputs applying a bias and activating with a 
non-linearity should sound very similar to lecture   one in yesterday's class but except now when 
we're going to do that first step instead of   just doing a DOT product with our weights we're 
going to apply a convolution with our weights   which is simply that element-wise multiplication 
and addition right and that sliding operation   now what's really special here and what I really 
want to stress is the local connectivity every   single neuron in this hidden layer only sees a 
certain patch of inputs in its previous layer   so if I point at just this one neuron in 
the output layer this neuron only sees the   inputs at this red square it doesn't see any 
of the other inputs in the rest of the image   and that's really important to be able to scale 
these models to very large scale images now you   can imagine that as you go deeper and deeper 
into your network eventually because the next   layer you're going to attend to a larger 
patch right and that will include data from   not only this red square but effectively a much 
larger Red Square that you could imagine there now let's define this actual computation that's 
going on for a neuron in a hidden layer its inputs   are those neurons that fell within its patch in 
the previous layer we can apply this Matrix of   Weights here denoted as a 4x4 filter that you can 
see on the left hand side and in this case we do   an element-wise multiplication we add the outputs 
we apply a bias and we add that non-linearity   it's the the core steps that we take and 
really all of these neural networks that   you're learning about in today's 
and this week's class to be honest   now remember that this element-wise multiplication 
and addition operation that sliding operation   that's called convolution and that's the basis 
of these layers so that defines how neurons in   convolutional layers are connected how they're 
mathematically formulated but within a single   convolutional layer it's also really important 
to understand that a single layer could actually   try to detect multiple sets of filters right maybe 
you want to detect in one image multiple features   not just one feature but you know in if you're 
detecting faces you don't only want to detect eyes   you want to detect you know eyes noses mouths ears 
right all of those things are critical patterns   that Define a face and can help you classify 
a face so what we need to think of is actually   convolution operations that can output a volume of 
different images right every slice of this volume   effectively denotes a different filter that can 
be identified in our original input and each of   those filters is going to basically correspond to 
a specific pattern or feature in our image as well   think of the connections in these neurons in terms 
of you know their receptive field once again right   the locations within the input of that node that 
they were connected to in the previous layer these   parameters really Define what I like to think 
of as the spatial arrangement of information   that propagates throughout the network and 
throughout the convolutional layers in particular   now I think just to summarize what we've seen 
and how Connections in these types of neural   networks are defined and let's say how the how 
the output of a convolutional network is a volume   we are well on our way to really understanding you 
know convolutional neural networks and defining   them right that's the what we just covered is 
really the main component of cnns right that's   the convolutional operation that defines these 
convolutional layers the remaining steps are   very critical as well but I want to maybe pause 
for a second and make sure that everyone's on the   same page with the convolutional operation 
and the definition of convolutional layers awesome okay so the next step here is to 
take those resulting feature maps that our   convolutional layers extract and apply a 
non-linearity to the output volume of the   convolutional layer so as we discussed in the 
first lecture applying these non-linearities   is really critical because it allows us 
to deal with non-linear data and because   image data in particular is extremely 
non-linear that's a you know a critical   component of what makes convolutional neural 
networks actually operational in practice   in particular for convolutional neural networks 
the activation function that is really really   common for these models is the relu activation 
function we talked a little bit about this in   lecture one and two yesterday the relative 
activation function you can see it on the   right hand side think of this function as a pixel 
by pixel operation that replaces basically all   negative values with zero it keeps all positive 
values the same it's the identity function when   a value is positive but when it's negative it 
basically squashes everything back up to zero   think of this almost as a thresholding function 
right thresholds is everything at zero anything   less than zero comes back up to zero so negative 
values here indicate uh basically a negative   detection in convolution that you may want to 
just say was no detection right and you can think   of that as kind of an intuitive mechanism for 
understanding why the relative activation function   is so popular in convolutional neural networks 
the other common the other uh popular belief   is that relu activation functions well it's not 
a belief they are extremely easy to compute and   they're very easy and computationally efficient 
their gradients are very cleanly defined they're   constants except for a piecewise non-nonlinearity 
so that makes them very popular for these domains now the next key operation in a CNN is that 
of pooling now pooling is an operation that   is at its core it serves one purpose and that 
is to reduce the dimensionality of the image   progressively as you go deeper and deeper through 
your convolutional layers now you can really start   to reason about this is that when you decrease 
the dimensionality of your features you're   effectively increasing the dimensionality of your 
filters right now because every filter that you   slide over a smaller image is capturing a larger 
receptive field that occurred previously in that   Network so a very common technique for pooling 
is what's called maximum pooling or Max pooling   for short Max pooling is exactly you know what it 
sounds like so it basically operates with these   small patches again that slide over an image but 
instead of doing this convolution operation what   these patches will do is simply take the maximum 
of that patch location so I think of this as kind   of activating the maximum value that comes from 
that location and propagating only the maximums   I encourage all of you actually to think of maybe 
brainstorm other ways that we could perform even   better pooling operations than Max pooling there 
are many common ways but you could think of some   for example or mean pooling or average pooling 
right maybe you don't want to just take the   maximum you could collapse basically the 
average of all of these pixels into your   into your single value in the result but these 
are the key operations of convolutional neural   networks at their core and now we're ready to 
really start to put them together and form and   construct a CNN all the way from the ground 
up and with cnns we can layer these operations   one after the other right starting first with 
convolutions non-linearities and then pooling   and repeating these over and over again to learn 
these hierarchies of features and that's exactly   how we obtained pictures like this which we 
started yesterday's lecture with and learning   these hierarchical decompositions of features by 
progressively stacking and stacking these filters   on top of each other each filter could then use 
all of the previous filters that it had learned   so a CNN built for image classification can be 
really broken down into two parts first is the   feature learning pipeline which we learn 
the features that we want to detect and   then the second part is actually detecting 
those features and doing the classification now the convolutional and pooling layers output 
from the first part of that model the goal of   those convolutional pooling layers is to Output 
the high level features that are extracted from   our input but the next step is to actually 
use those features and detect their presence   in order to classify the image so we can feed 
these outputted features into the com the fully   connected layers that we learned about in lecture 
one because these are now just a one-dimensional   array of features and we can use those to detect 
you know what class we're in and we can do this   by using a function called a softmax function 
you can think of a softmax function as simply   a normalizing function whose output represents 
that of a categorical probability distribution   so another way to think of this is basically 
if you have an array of numbers you want to   collapse and those numbers could take any real 
number form you want to collapse that into some   probability distribution a probability 
distribution has several properties name   that all of its values have to sum to one it 
always has to be between zero and one as well   so maintaining those two properties is what a soft 
Max operation does you can see its equation right   here it effectively just makes everything positive 
and then it normalizes the result across each   other and that maintains those two properties 
that I just mentioned great so let's put all   of this together and actually see how we could 
program our first convolutional neural network   end to end entirely from scratch so let's start 
by firstly defining our feature extraction head   which starts with a convolutional layer and here 
32 filters or 32 features you can imagine that   this first layer the result of this first layer 
is to learn not one filter not one pattern in   our image but 32 patterns okay so those 32 results 
are going to then be passed to a pooling layer and   then passed on to the next set of convolutional 
operations the next set of convolutional   operations now will contain 64 features we'll 
keep progressively growing and expanding our   set of patterns that we're identifying in this 
image next we can finally flatten those resulting   features that we've identified and feed all of 
this through our dense layers our fully connected   layers that we learned about in lecture one these 
will allow us to predict those final let's say 10   classes if we have 10 different final possible 
classes in our image this layer will account   for that and allow us to Output using softmax the 
probability distribution across those 10 classes   so so far we've talked about right how we 
can let's say use cnns to perform image   classification tasks but in reality one thing I 
really want to stress in today's class especially   towards the end is that this same architecture 
and same building blocks that we've talked about   so far are extensible and they extend to so 
many different applications and model types   that we can imagine so for example when we 
considered the CNN for classification we saw   that it really had two parts right the first 
part being feature extraction learning what   features to look for and the second part being the 
classification the detection of those features now   what makes a convolutional neural network really 
really powerful is exactly the observation that   the feature learning part this first part of the 
neural network is extremely flexible you can take   that first part of the neural network chop off 
what comes after it and put a bunch of different   heads into the part that comes after the goal of 
the first part is to extract those features what   you do with the features is entirely up to you 
but you can still Leverage The flexibility and the   power of the first part to learn all of those core 
features so for example that portion will look for   you know all of the different image classification 
domains that future portion after you've extracted   the features or we could also introduce new 
architectures that take those features and   maybe perform tasks like segmentation or image 
captioning like we saw in yesterday's lecture   so in the case of classification for example just 
to tie up the the classification story there's   a significant impact in domains like healthcare 
medical decision making where deep learning models   are being applied to the analysis of medical scans 
across a whole host of different medical imagery   now classification tells us basically a discrete 
prediction of what our image contains but we can   actually go much deeper into this problem 
as well so for example imagine that we're   not trying to only identify that this image is 
an image of a taxi which you can see here but   also more importantly maybe we want our neural 
network to tell us not only that this is a taxi   but identify and draw a specific bounding 
box over this location of the taxi so this   is kind of a two-phase problem number one is 
that we need to draw a box and number two is   we need to classify what was in that box right 
so it's both a regression problem where is the   box right that's a continuous problem as well as 
a classification problem is what is in that box   now that's a much much harder problem than what 
we've covered so far in the lecture today because   potentially there are many objects in our scene 
not just one object right so we have to account   for this fact that maybe our scene could contain 
arbitrarily many objects now our Network needs to   be flexible to that degree right it needs to be 
able to infer a dynamic number of objects in the   scene and if the scene is only of a taxi then it 
should only output you know that one bounding box   but on the other hand if the image has many 
objects right potentially even of different   classes we need a model that can draw a bounding 
box for each of these different examples as well   as associate their predicted classification 
labels to each one independently now this is   actually quite complicated in practice because 
those boxes can be anywhere in the image right   there's no constraints on where the boxes can be 
and they can also be of different sizes they can   be also different ratios right some can be tall 
some can be wide let's consider a very naive   way of doing this first let's take our image and 
start by placing a random box somewhere on that   image for example we just pick a random location a 
random size we'll place a box right there this box   like I said has a random location random size then 
we can take that box and only feed that random box   through our convolutional neural network which is 
trained to do classification just classification   and this neural network can detect well number one 
is there a class of object in that box or not and   if so what class is it and then what we can do is 
we could just keep repeating this process over and   over again for all of these random boxes in our 
image you know many many instances of random boxes   we keep sampling a new box feed it through our 
convolutional neural network and ask this question   what was in the box if there was something in 
there then what what is it right and we keep   moving on until we kind of have exhausted all of 
the the boxes in the image but the problem here   is that there are just way too many potential 
inputs that we would have to deal with this   would be totally impractical to run in a real-time 
system for example with today's compute it results   in way too many scales especially for the types 
of resolutions of images that we deal with today   so instead of picking random boxes let's 
try and use a very simple heuristic right   to identify maybe some places with lots 
of variability in the image where there   is high likelihood of having an object might 
be present right these might have meaningful   insights or meaningful objects that could be 
available in our image and we can use those   to basically just feed in those High attention 
locations to our convolutional neural network   and then we can basically speed up that first 
part of the pipeline a lot because now we're   not just picking random boxes maybe we use 
like some simple heuristic to identify where   interesting parts of the image might be but still 
this is actually very slow in practice we have to   feed in each region independently to the model 
and plus it's very brittle because ultimately   the part of the model that is looking at where 
potential objects might be is detached from the   part that's doing the detection of those objects 
ideally we want one model that is able to both   you know figure out where to attend to 
and do that classification afterwards   so there have been many variants that have been 
proposed in this field of object detection but I   want to just for the purpose of today's class 
introduce you to one of the most popular ones   now this is a point or this is a model called rcnn 
or faster rcnn which actually attempts to learn   not only how to classify these boxes but learned 
how to propose those where those boxes might be   in the first place so that you could learn how 
to feed or where to feed into the downstream   neural network now this means that we can feed 
in the image to what are called these region   proposal networks the goal of these networks is 
to propose certain regions in the image that you   should attend to and then feed just those regions 
into the downstream cnns so the goal here is to   directly try to learn or extract all of those key 
regions and process them through the later part of   the model each of these regions are processed 
with their own independent feature extractors   and then a classifier can be used to aggregate 
them all and perform feature detection as well as   object detection now the beautiful thing about 
this is that this requires only a single pass   through the network so it's extraordinarily 
fast it can easily Run in real time and it's   very commonly used in many industry applications 
as well even it can even run on your smartphone   so in classification we just saw how we can 
predict you know not only a single image per   or sorry a single object per image we saw 
an object detection potentially inferring   multiple objects with bounding boxes in your 
image there's also one more type of task which   I want to point out which is called segmentation 
segmentation is the task of classification but   now done at every single Pixel this takes 
the idea of object detection which bounding   boxes to the extreme now instead of drawing 
boxes we're not even going to consider boxes   we're going to learn how to classify every 
single Pixel in this image in isolation right   so it's a huge number of classifications 
that we're going to do and we'll do this   well first let me show this example so on the 
left hand side what this looks like is you're   feeding an original RGB image the goal of the 
right hand side is to learn for every pixel in   the left hand side what was the class of that 
pixel right so this is kind of in contrast to   just determining you know boxes over our image 
now we're looking at every pixel in isolation   and you can see for example you know this pixels 
of uh the cow are clearly differentiated from the   pixels of the sky or the pixels of the the grass 
right and that's a Cree a key critical component   of semantic segmentation networks the output here 
is created by again using these convolutional   operations followed by pooling operations which 
learn an encoder which you can think of on the   left hand side these are learning the features 
from our RGB image learning how to put them into a   space so that it can reconstruct into a new space 
of semantic labels so you can imagine kind of a   Down scaling and then Progressive upscaling into 
the semantic space but when you do that upscaling   it's important of course you can't be pulling down 
that information you need to kind of invert all of   those operations so instead of doing convolutions 
with pooling you can now do convolutions with   basically reverse pulling or expansions right 
you can grow your feature sets at every labels   and here's an example on the bottom of just a code 
piece that actually defines these layers you can   plug these layers combine them with convolutional 
layers and you can build these fully convolutional   networks that can accomplish this type of task 
now of course this can be applied in many other   applications in healthcare as well especially 
for segmenting out let's say cancerous regions   or even identifying parts of the blood which are 
infected with malaria for example and one final   example here of self-driving cars let's say that 
we want to build a neural network for autonomous   navigation specifically building a model let's say 
that can take as input an image as well as let's   say some very coarse maps of where it thinks it 
is think of this as basically a screenshot of you   know the Google Maps essentially to the neural 
network right it's the GPS location of the map   and it wants to directly inferred not a 
classification or a semantic classification   of the scene but now directly infer the actuation 
how to drive and steer this car into into the   future right now this is a full probability 
distribution over the entire space of control   commands right it's a very large continuous 
probability space and the question is how can   we build a neural network to learn this function 
and the key point that I'm stressing with all of   these different types of architectures here 
is that all of these architectures use the   exact same encoder we haven't changed anything 
when going from classification to detection to   semantic segmentation and now to here all of them 
are using the same underlying building blocks   of convolutions non-linearities and pooling the 
only difference is that after we ex perform those   feature extractions how do we take those features 
and learn our ultimate tasks so for example in the   case of probabilistic control commands we would 
want to take those learned features and understand   how to predict you know the parameters of a 
full continuous probability distribution like   you can see on the right hand side as well 
as the deterministic control of our desired   destination and again like we talked about at 
the very beginning of this class this model   which goes directly from images all the way to 
steering wheel angles essentially of the car is   a single model it's learned entirely end to end we 
never told the car for example what a lane marker   is or you know the rules of the road it was able 
to observe a lot of human driving data extract   these patterns these features from what makes 
a good human driver different from a bad human   driver and learn how to imitate those same types 
of actions that are occurring so that without any   you know human intervention or human rules that 
we impose on these systems they can simply watch   all of this data and learn how to drive entirely 
from scratch so a human for example can actually   enter the car input a desired dust Nation and this 
end-to-end CNN will actually actuate the control   commands to bring them to their destination now 
I'll conclude today's lecture with just saying   that the applications of cnns we've touched on a 
few of them today but the applications of cnms are   enormous Right Far Beyond these examples that 
I've provided today they all tie back to this   core concept of feature extraction and detection 
and after you do that feature extraction you can   really crop off the rest of your network 
and apply it to many different heads for   many different tasks and applications that you 
might care about we've touched on a few today   but there are really so so many in different 
domains and with that I'll conclude and very   shortly we'll just be talking about generative 
modeling which is a really Central class really   central part of today's and this week's lectures 
series and after that later on we'll have the   software lab which I'm excited for all of you 
to to start participating in and yeah we can   take a short five minute break and continue 
the lectures from there thank you [Applause] 

foreign I'm really really excited about this lecture because as Alexander introduced to yesterday right now we're in this tremendous age of generative Ai and today we're going to learn the foundations of deep generative modeling where we're going to talk about Building Systems that can not only look for patterns in data but can actually go A Step Beyond this to generate brand new data instances based on those learned patterns this is an incredibly complex and Powerful idea and as I mentioned it's a particular subset of deep learning that has actually really exploded in the past couple of years and this year in particular so to start and to demonstrate how powerful these algorithms are let me show you these three different faces I want you to take a minute think think about which face you think is real raise your hand if you think it's face a okay you see a couple of people face B many more people face C about second place well the truth is that all of you are wrong all three of these faces are fake these people do not exist these images were synthesized by Deep generative models trained on data of human faces and asked to produce new instances now I think that this demonstration kind of demonstrates the power of these ideas and the power of this notion of generative modeling so let's get a little more concrete about how we can formalize this so far in this course we've been looking at what we call problems of supervised learning meaning that we're given data and associated with that data is a set of labels our goal is to learn a function that maps that data to the labels now we're in a course on deep learning so we've been concerned with functional mappings that are defined by Deep neural networks but really that function could be anything neural networks are powerful but we could use other techniques as well in contrast there's another class of problems in machine learning that we refer to as unsupervised learning where we take data but now we're given only data no labels and our goal is to try to build some method that can understand the hidden underlying structure of that data what this allows us to do is it gives us new insights into the foundational representation of the data and as we'll see later actually enables us to generate new data instances now this class of problems this definition of unsupervised learning captures the types of models that we're going to talk about today in the focus on generative modeling which is an example of unsupervised learning and is United by this goal of the problem where we're given only samples from a training set and we want to learn a model that represents the distribution of the data that the model is seeing generative modeling takes two general forms first density estimation and second sample generation in density estimation the task is given some data examples our goal is to train a model that learns a underlying probability distribution that describes the where the data came from with sample generation the idea is similar but the focus is more on actually generating new instances our goal with sample generation is to again learn this model of this underlying probability distribution but then use that model to sample from it and generate new instances that are similar to the data that we've seen approximately falling along ideally that same real data distribution now in both these cases of density estimation and Sample generation the underlying question is the same our learning task is to try to build a model that learns this probability distribution that is as close as possible to the true data distribution okay so with this definition and this concept of generative modeling what are some ways that we can actually deploy generative modeling forward in the real world for high impact applications well part of the reason that generative models is are so powerful is that they have this ability to uncover the underlying features in a data set and encode it in an efficient way so for example if we're considering the problem of facial detection and we're given a data set with many many different faces starting out without inspecting this data we may not know what the distribution of Faces in this data set is with respect to Features we may be caring about for example the pose of the head clothing glasses skin tone Hair Etc and it can be the case that our training data may be very very biased towards particular features without us even realizing this using generative models we can actually identify the distributions of these underlying features in a completely automatic way without any labeling in order to understand what features may be overrepresented in the data what features may be underrepresented in the data and this is the focus of today and tomorrow's software Labs which are going to be part of the software lab competition developing generative models that can do this task and using it to uncover and diagnose biases that can exist within facial detection models another really powerful example is in the case of outlier detection identifying rare events so let's consider the example of self-driving autonomous cars with an autonomous car let's say it's driving out in the real world we really really want to make sure that that car can be able to handle all the possible scenarios and all the possible cases it may encounter including edge cases like a deer coming in front of the car or some unexpected rare events not just you know the typical straight freeway driving that it may see the majority of the time with generative models we can use this idea of density estimation to be able to identify rare and anomalous events within the training data and as they're occurring as the model sees them for the first time so hopefully this paints this picture of what generative modeling the underlying concept is and a couple of different ways in which we can actually deploy these ideas for powerful and impactful real world applications yeah in today's lecture we're going to focus on a broad class of generative models that we call latent variable models and specifically distilled down into two subtypes of latent variable models first things first I've introduced this term latent variable but I haven't told you or described to you what that actually is I think a great example and one of my favorite examples throughout this entire course that gets at this idea of the latent variable is this little story from Plato's Republic which is known as the myth of the cave in this myth there is a group of prisoners and as part of their punishment they're constrained to face a wall now the only things the prisoners can observe are shadows of objects that are passing in front of a fire that's behind them and they're observing the casting of the Shadows on the wall of this cave to the prisoners those Shadows are the only things they see their observations they can measure them they can give them names because to them that's their reality but they're unable to directly see the underlying objects the true factors themselves that are casting those Shadows those objects here are like latent variables in machine learning they're not directly observable but they're the true underlying features or explanatory factors that create the observed differences and variables that we can see and observe and this gets out the goal of generative modeling which is to find ways that we can actually learn these hidden features these underlying latent variables even when we're only given observations of The observed data so let's start by discussing a very simple generative model that tries to do this through the idea of encoding the data input the models we're going to talk about are called autoencoders and to take a look at how an auto encoder works we'll go through step by step starting with the first step of taking some raw input data and passing it through a series of neural network layers now the output of this of this first step is what we refer to as a low dimensional latent space it's an encoded representation of those underlying features and that's our goal in trying to train this model and predict those features the reason a model like this is called an encoder an autoencoder is that it's mapping the data X into this Vector of latent variables Z now let's ask ourselves the question let's pause for a moment why maybe we care about having this latent variable Vector Z be in a low dimensional space anyone have any ideas all right maybe there are some ideas as yes the suggestion was that it's more efficient yes that's that's gets at it the heart of the of the question the idea of having that low dimensional latent space is that it's a very efficient compact encoding of the rich High dimensional data that we may start with as you pointed out right what this means is that we're able to compress data into this small feature representation a vector that captures this compactness and richness without requiring so much memory or so much storage so how do we actually train the network to learn this latent variable vector since we don't have training data we can't explicitly observe these latent variables Z we need to do something more clever what the auto encoder does is it builds a way to decode this latent variable Vector back up to the original data space trying to reconstruct their original image from that compressed efficient latent encoding and once again we can use a series of neural network layers such as convolutional layers fully connected layers but now to map back from that lower dimensional space back upwards to the input space this generates a reconstructed output which we can denote as X hat since it's an imperfect reconstruction of our original input data to train this network all we have to do is compare the outputted Reconstruction and the original input data and say how do we make these as similar as possible we can minimize the distance between that input and our reconstructed output so for example for an image we can compare the pixel wise difference between the input data and the reconstructed output just subtracting the images from one another and squaring that difference to capture the pixel wise Divergence between the input and the reconstruction what I hope you'll notice and appreciate is in that definition of the loss it doesn't require any labels the only components of that loss are the original input data X and the reconstructed output X hat so I've simplified now this diagram by abstracting away those individual neural network layers in both the encoder and decoder components of this and again this idea of not requiring any labels gets back to the idea of unsupervised learning since what we've done is we've been able to learn a encoded quantity our latent variables that we cannot observe without any explicit labels all we started from was the raw data itself it turns out that as as the question and answer got at that dimensionality of the latent space has a huge impact on the quality of the generated reconstructions and how compressed that information bottleneck is Auto encoding is a form of compression and so the lower the dimensionality of the latent space the less good our reconstructions are going to be but the higher the dimensionality the more the less efficient that encoding is going to be so to summarize this this first part this idea of an autoencoder is using this bottlenecked compressed hidden latent layer to try to bring the network down to learn a compact efficient representation of the data we don't require any labels this is completely unsupervised and so in this way we're able to automatically encode information within the data itself to learn this latent space Auto encoding information Auto encoding data now this is of the pretty simple model and it turns out that in practice this idea of self-encoding or Auto encoding has a bit of a Twist on it to allow us to actually generate new examples that are not only reconstructions of the input data itself and this leads us to the concept of variational autoencoders or vaes with the traditional autoencoder that we just saw if we pay closer attention to the latent layer right which is shown in that orange salmon color that latent layer is just a normal layer in the neural network it's completely deterministic what that means is once we've trained the network once the weights are set anytime we pass a given input in and go back through the latent layer decode back out we're going to get the same exact reconstruction the weights aren't changing it's deterministic in contrast variational autoencoders vaes introduce a element of Randomness a probabilistic Twist on this idea of Auto encoding what this will allow us to do is to actually generate new images similar to the or new data instances that are similar to the input data but not forced to be strict reconstructions in practice with the variational autoencoder we've replaced that single deterministic layer with a random sampling operation now instead of learning just the latent variables directly themselves for each latent variable we Define a mean and a standard deviation that captures a probability distribution over that latent variable what we've done is we've gone from a single Vector of latent variable Z to a vector of means mu and a vector of standard deviations Sigma that parametrize the probability distributions around those latent variables what this will allow us to do is Now sample using this element of Randomness this element of probability to then obtain a probabilistic representation of the latent space itself as you hopefully can tell right this is very very very similar to the autoencoder itself but we've just added this probabilistic twist where we can sample in that intermediate space to get these samples of latent variables foreign now to get a little more into the depth of how this is actually learned how this is actually trained with defining the vae we've eliminated this deterministic nature to now have these encoders and decoders that are probabilistic the encoder is Computing a probability distribution of the latent variable Z given input data X while the decoder is doing the inverse trying to learn a probability distribution back in the input data space given the latent variables Z and we Define separate sets of weights Phi and Theta to define the network weights for the encoder and decoder components of the vae all right so when we get now to how we actually optimize and learn the network weights in the vae first step is to define a loss function right that's the core element to training a neural network our loss is going to be a function of the data and a function of the neural network weights just like before but we have these two components these two terms that Define our vae loss first we see the Reconstruction loss just like before where the goal is to capture the difference between our input data and the reconstructed output and now for the vae we've introduced a second term to the loss what we call the regularization term often you'll maybe even see this referred to as a vae loss and we'll go into we'll go into describing what this regular regularization term means and what it's doing to do that and to understand remember and and keep in mind that in all neural network operations our goal is to try to optimize the network weights with respect to the data with respect to minimizing this objective loss and so here we're concerned with the network weights Phi and Theta that Define the weights of the encoder and the decoder we consider these two terms first the Reconstruction loss again the Reconstruction loss is very very similar same as before you can think of it as the error or the likelihood that effectively captures the difference between your input and your outputs and again we can trade this in an unsupervised way not requiring any labels to force the latent space and the network to learn how to effectively reconstruct the input data the second term the regularization term is now where things get a bit more interesting so let's go on on into this in a little bit more detail because we have this probability distribution and we're trying to compute this encoding and then decode back up as part of regular regularizing we want to take that inference over the latent distribution and constrain it to behave nicely if you will the way we do that is we place what we call a prior on the latent distribution and what this is is some initial hypothesis or guess about what that latent variable space may look like this helps us and helps the network to enforce a latent space that roughly tries to follow this prior distribution and this prior is denoted as P of Z right that term d That's effectively the regularization term it's capturing a distance between our encoding of the latent variables and our prior hypothesis about what the structure of that latent space should look like so over the course of training we're trying to enforce that each of those latent variables adapts a problem adopts a probability distribution that's similar to that prior a common Choice when training va's and developing these models is to enforce the latent variables to be roughly standard normal gaussian distributions meaning that they are centered around mean zero and they have a standard deviation of one what this allows us to do is to encourage the encoder to put the latent variables roughly around a centered space Distributing the encoding smoothly so that we don't get too much Divergence away from that smooth space which can occur if the network tries to cheat and try to Simply memorize the data by placing the gaussian standard normal prior on the latent space we can define a concrete mathematical term that captures the the distance the Divergence between our encoded latent variables and this prior and this is called the KL Divergence when our prior is a standard normal the KL Divergence takes the form of the equation that I'm showing up on the screen but what I want you to really get away Come Away with is that the concept of trying to smooth things out and to capture this Divergence and this difference between the prior and the latent encoding is all this KL term is trying to capture so it's a bit of math and I I acknowledge that but what I want to next go into is really what is the intuition behind this regularization operation why do we do this and why does the normal prior in particular work effectively for vaes so let's consider what properties we want our latent space to adopt and for this regularization to achieve the first is this goal of continuity we don't me and what we mean by continuity is that if there are points in the latent space that are close together ideally after decoding we should recover two reconstructions that are similar in content that make sense that they're close together the second key property is this idea of completeness we don't want there to be gaps in the lane space we want to be able to decode and sample from the latent space in a way that is smooth and a way that is connected to get more concrete what let's ask what could be the consequences of not regularizing our latent space at all well if we don't regularize we can end up with instances where there are points that are close in the latent space but don't end up with similar decodings or similar reconstructions similarly we could have points that don't lead to meaningful reconstructions at all they're somehow encoded but we can't decode effectively regularization allows us to realize points that end up close in the latent space and also are similarly reconstructed and meaningfully reconstructed okay so continuing with this example the example that I showed there and I didn't get into details was showing these shapes these shapes of different colors and that we're trying to be encoded in some lower dimensional space with regularization we are able to achieve this by Trying to minimize that regularization term it's not sufficient to just employ the Reconstruction loss alone to achieve this continuity and this completeness because of the fact that without regularization just encoding and reconstructing does not guarantee the properties of continuity and completeness we overcome this these issues of having potentially pointed distributions having discontinuities having disparate means that could end up in the latent space without the effect of regularization we overcome this by now regularizing the mean and the variance of the encoded latent distributions according to that normal prior what this allows is for the Learned distributions of those latent variables to effectively overlap in the latent space because everything is regularized to have according to this prior of mean zero standard deviation one and that centers the means regularizes the variances for each of those independent latent variable distributions together the effect of this regularization in net is that we can achieve continuity and completeness in the latent space points and distances that are close should correspond to similar reconstructions that we get out so hopefully this this gets at some of the intuition behind the idea of the vae behind the idea of the regularization and trying to enforce the structured normal prior on the latent space with this in hand with the two components of our loss function reconstructing the inputs regularizing learning to try to achieve continuity and completeness we can now think about how we Define a forward pass through the network going from an input example and being able to decode and sample from the latent variables to look at new examples our last critical step is how the actual back propagation training algorithm is defined and how we achieve this the key as I introduce with vaes is this notion of randomness of sampling that we have introduced by defining these probability distributions over each of the latent variables the problem this gives us is that we cannot back propagate directly through anything that has an element of sampling anything that has an element of randomness back propagation requires completely deterministic nodes deterministic layers to be able to successfully apply gradient descent and the back propagation algorithm the Breakthrough idea that enabled vaes to be trained completely end to end was this idea of re-parametrization within that sampling layer and I'll give you the key idea about how this operation works it's actually really quite quite clever so as I said when we have a notion of randomness of probability we can't sample directly through that layer instead with re-parametrization what we do is we redefine how a latent variable Vector is sampled as a sum of a fixed deterministic mean mu a fixed Vector of standard deviation Sigma and now the trick is that we divert all the randomness all the sampling to a random constant Epsilon that's drawn from a normal distribution so mean itself is fixed standard deviation is fixed all the randomness and the sampling occurs according to that Epsilon constant we can then scale the mean and standard deviation by that random constant to re-achieve the sampling operation within the latent variables themselves what this actually looks like and an illustration that breaks down this concept of re-parametrization and Divergence is as follows so look looking here right what I've shown is these completely deterministic steps in blue and the sampling random steps in Orange originally if our latent variables are what effectively are capturing the randomness the sampling themselves we have this problem in that we can't back propagate we can't train directly through anything that has stochasticity that has randomness what reparametrization allows us to do is it shifts this diagram where now we've completely diverted that sampling operation off to the side to this constant Epsilon which is drawn from a normal prior and now when we look back at our latent variable it is deterministic with respect to that sampling operation what this means is that we can back propagate to update our Network weights completely end to end without having to worry about direct Randomness direct stochasticity within those latent variables C this trick is really really powerful because it enabled the ability to train these va's completely end to end in a in through back propagation algorithm all right so at this point we've gone through the core architecture of vais we've introduced these two terms of the loss we've seen how we can train it end to end now let's consider what these latent variables are actually capturing and what they represent when we impose this distributional prior what it allows us to do is to sample effectively from the latent space and actually slowly perturb the value of single latent variables keeping the other ones fixed and what you can observe and what you can see here is that by doing that perturbation that tuning of the value of the latent variables we can run the decoder of the vae every time reconstruct the output every time we do that tuning and what you'll see hopefully with this example with the face is that an individual latent variable is capturing something semantically informative something meaningful and we see that by this perturbation by this tuning in this example the face as you hopefully can appreciate is Shifting the pose is Shifting and all this is driven by is the perturbation of a single latent variable tuning the value of that latent variable and seeing how that affects the decoded reconstruction the network is actually able to learn these different encoded features these different latent variables such that by perturbing the values of them individually we can interpret and make sense of what those latent variables mean and what they represent to make this more concrete right we can consider even multiple latent variables simultaneously compare one against the other and ideally we want those latent features to be as independent as possible in order to get at the most compact and richest representation and compact encoding so here again in this example of faces we're walking along two axes head pose on the x-axis and what appears to be kind of a notion of a smile on the y-axis and you can see that with these reconstructions we can actually perturb these features to be able to perturb the end effect in the reconstructed space and so ultimately with with the vae our goal is to try to enforce as much information to be captured in that encoding as possible we want these latent features to be independent and ideally disentangled it turns out that there is a very uh clever and simple way to try to encourage this Independence and this disentanglement while this may look a little complicated with with the math and and a bit scary I will break this down with the idea of how a very simple concept enforces this independent latent encoding and this disentanglement all this term is showing is those two components of the loss the Reconstruction term the regularization term that's what I want you to focus on the idea of latent space disentanglement really arose with this concept of beta beta vaes what beta vas do is they introduce this parameter beta and what it is it's a weighting constant the weighting constant controls how powerful that regularization term is in the overall loss of the vae and it turns out that by increasing the value of beta you can try to encourage greater disentanglement more efficient encoding to enforce these latent variables to be uncorrelated with each other now if you're interested in mathematically why beta vas enforce this disentanglement there are many papers in the literature and proofs and discussions as to why this occurs and we can point you in those directions but to get a sense of what this actually affects Downstream when we look at face reconstruction as a task of Interest with the standard vae no beta term or rather a beta of one you can hopefully appreciate that the features of the rotation of the head the pose and the the rotation of the head is also actually ends up being correlated with smile and the facial the mouth expression in the mouth position in that as the head pose is changing the apparent smile or the position of the mouth is also changing but with beta vaes empirically we can observe that with imposing these beta values much much much greater than one we can try to enforce greater disentanglement where now we can consider only a single latent variable head pose and the smile the position of the mouth in these images is more constant compared to the standard vae all right so this is really all the core math the core operations the core architecture of the A's that we're going to cover in today's lecture and in this class in general to close this section and as a final note I want to remind you back to the motivating example that I introduced at the beginning of this lecture facial detection where now hopefully you've understood this concept of latent variable learning and encoding and how this may be useful for a task like facial detection where we may want to learn those distributions of the underlying features in the data and indeed you're going to get Hands-On practice in the software labs to build variational autoencoders that can automatically uncover features underlying facial detection data sets and use this to actually understand underlying and hidden biases that may exist with those data and with those models and it doesn't just stop there tomorrow we'll have a very very exciting guest lecture on robust and trustworthy deep learning which will take this concept A step further to realize how we can use this idea of generative models and latent variable learning to not only uncover and diagnose biases but actually solve and mitigate some of those harmful effects of those biases in neural networks for facial detection and other applications all right so to summarize quickly the key points of vais we've gone through how they're able to compress data into this compact encoded representation from this representation we can generate reconstructions of the input in a completely unsupervised fashion we can train them end to end using the repair maturization trick we can understand the semantic uh interpretation of individual latent variables by perturbing their values and finally we can sample from the latent space to generate new examples by passing back up through the decoder so vaes are looking at this idea of latent variable encoding and density estimation as their core problem what if now we only focus on the quality of the generated samples and that's the task that we care more about for that we're going to transition to a new type of generative model called a generative adversarial Network or Gam where with cans our goal is really that we care more about how well we generate new instances that are similar to the existing data meaning that we want to try to sample from a potentially very complex distribution that the model is trying to approximate it can be extremely extremely difficult to learn that distribution directly because it's complex it's high dimensional and we want to be able to get around that complexity what Gans do is they say okay what if we start from something super super simple as simple as it can get completely random noise could we build a neural network architecture that can learn to generate synthetic examples from complete random noise and this is the underlying concept of Gans where the goal is to train this generator Network that learns a transformation from noise to the training data distribution with the goal of making the generated examples as close to the real deal as possible with scans the Breakthrough idea here was to interface these two neural networks together one being a generator and one being a discriminator and these two components the generator and discriminator are at War at competition with each other specifically the goal of the generator network is to look at random noise and try to produce an imitation of the data that's as close to real as possible the discriminator that then takes the output of the generator as well as some real data examples and tries to learn a classification classification decision distinguishing real from fake and effectively in the Gan these two components are going back and forth competing each other trying to force the discriminator to better learn this distinction between real and fake while the generator is trying to fool and outperform the ability of the discriminator to make that classification so that's the overlying concept but what I'm really excited about is the next example which is one of my absolute favorite illustrations and walkthroughs in this class and it gets at the intuition behind Gans how they work and the underlying concept okay we're going to look at a 1D example points on a line right that's the data that we're working with and again the generator starts from random noise produces some fake data they're going to fall somewhere on this one-dimensional line now the next step is the discriminator then sees these points and it also sees some real data the goal of the discriminator is to be trained to Output a probability that a instance it sees is real or fake and initially in the beginning before training it's not trained right so its predictions may not be very good but over the course of training you're going to train it and it hopefully will start increasing the probability for those examples that are real and decreasing the probability for those examples that are fake overall goal is to predict what is real until eventually the discriminator reaches this point where it has a perfect separation perfect classification of real versus fake so at this point the discriminator thinks okay I've done my job now we go back to the generator and it sees the examples of where the real data lie and it can be forced to start moving its generated fake data closer and closer increasingly closer to the real data we can then go back to the discriminator which receives these newly synthesized examples from the generator and repeats that same process of estimating the probability that any given point is real and learning to increase the probability of the true real examples decrease the probability of the fake points adjusting adjusting over the course of its training and finally we can go back and repeat to the generator again one last time the generator starts moving those fake points closer closer and closer to the real data such that the fake data is almost following the distribution of the real data at this point it becomes very very hard for the discriminator to distinguish between what is real and what is fake while the generator will continue to try to create fake data points to fool the discriminator this is really the key concept the underlying intuition behind how the components of the Gan are essentially competing with each other going back and forth between the generator and the discriminator and in fact this is the this intuitive concept is how the Gan is trained in practice where the generator first tries to synthesize new examples synthetic examples to fool the discriminator and the goal of the discriminator is to take both the fake examples and the real data to try to identify the synthesized instances in training what this means is that the objective the loss for the generator and discriminator have to be at odds with each other they're adversarial and that is what gives rise to the component of adversarial ingenerative adversarial Network these adversarial objectives are then put together to then Define what it means to arrive at a stable Global Optimum where the generator is capable of producing the true data distribution that would completely fool the discriminator concretely this can be defined mathematically in terms of a loss objective and again though I'm I'm showing math I can we can distill this down and go through what each of these terms reflect in terms of that core intuitive idea and conceptual idea that hopefully that 1D example conveyed so we'll first consider the perspective of the discriminator D its goal is to maximize probability that its decisions uh in its decisions that real data are classified real Faith data classified as fake so here the first term G of Z is the generator's output and D of G of Z is the discriminator's estimate of that generated output as being fake D of x x is the real data and so D of X is the estimate of the probability that a real instance is fake 1 minus D of X is the estimate that that real instance is real so here in both these cases the discriminator is producing a decision about fake data real data and together it wants to try to maximize the probability that it's getting answers correct right now with the generator we have those same exact terms but keep in mind the generator is never able to affect anything the the discriminator's decision is actually doing besides generating new data examples so for the generator its objective is simply to minimize the probability that the generated data is identified as fake together we want to then put this together to Define what it means for the generator to synthesize fake images that hopefully fool the discriminator all in all right this term besides the math besides the particularities of this definition what I want you to get away from this from this section on Gans is that we have this dual competing objective where the generator is trying to synthesize these synthetic examples that ideally fool the best discriminator possible and in doing so the goal is to build up a network via this adversarial training this adversarial competition to use the generator to create new data that best mimics the true data distribution and is completely synthetic new instances foreign what this amounts to in practice is that after the training process you can look exclusively at the generator component and use it to then create new data instances all this is done by starting from random noise and trying to learn a model that goes from random noise to the real data distribution and effectively what Gans are doing is learning a function that transforms that distribution of random noise to some Target what this mapping does is it allows us to take a particular observation of noise in that noise space and map it to some output a particular output in our Target data space and in turn if we consider some other random sample of noise if we feed it through the generator again it's going to produce a completely new instance falling somewhere else on that true data distribution manifold and indeed what we can actually do is interpolate and Traverse between trajectories in the noise space that then map to traversals and and interpolations in the Target data space and this is really really cool because now you can think about an initial point and a Target point and all the steps that are going to take you to synthesize and and go between those images in that Target data distribution so hopefully this gets gives a sense of this concept of generative modeling for the purpose of creating new data instances and that notion of interpolation and data transformation leads very nicely to some of the recent advances and applications of Gans where one particularly commonly employed idea is to try to iteratively grow the Gan to get more and more detailed image Generations progressively adding layers over the course of training to then refine the examples generated by the generator and this is the approach that was used to generate those synthetic those images of those synthetic faces that I showed at the beginning of this lecture this idea of using again that is refined iteratively to produce higher resolution images another way we can extend this concept is to extend the Gan architecture to consider particular tasks and impose further structure on the networkers itself one particular idea is to say okay what if we have a particular label or some factor that we want to condition the generation on we call this C and it's supplied to both the generator and the discriminator what this will allow us to achieve is paired translation between different types of data so for example we can have images of a street view and we can have images of the segmentation of that street view and we can build a gan that can directly translate between the street view and the segmentation let's make this more concrete by considering some particular examples so what I just described was going from a segmentation label to a street scene we can also translate between a satellite view aerial satellite image to what is the road map equivalent of that aerial satellite image or a particular annotation or labels of the image of a building to the actual visual realization and visual facade of that building we can translate between different lighting conditions day to night black and white to color outlines to a colored photo all these cases and I think in particular the the most interesting and impactful to me is this translation between street view and aerial view and this is used to consider for example if you have data from Google Maps how you can go between a street view of the map to the aerial image of that finally again cons extending the same concept of translation bit between one domain to another idea is that of completely unpaired translation and this uses a particular Gan architecture called cyclogam and so in this video that I'm showing here the model takes as input a bunch of images in one domain and it doesn't necessarily have to have a corresponding image in another Target domain but it is trained to try to generate examples in that Target domain that roughly correspond to the source domain transferring the style of the source onto the Target and vice versa so this example is showing the translation of images in horse domain to zebra domain the concept here is this cyclic dependency right you have two Gans that are connected together via this cyclic loss transforming between one domain and another and really like all the examples that we've seen so far in this lecture the intuition is this idea of distribution transformation normally with again you're going from noise to some Target with the cycle Gan you're trying to go from some Source distribution some data manifold X to a target distribution another data manifold why and this is really really not only cool but also powerful in thinking about how we can translate across these different distributions flexibly and in fact this is a allows us to do Transformations not only to images but to speech and audio as well so in the case of speech and audio it turns out that you can take sound waves represent it compactly in a spectrogram image and use a cycle Gan to then translate and transform speech from one person's voice in one domain to another person's voice in another domain right these are two independent data distributions that we Define maybe you're getting a sense of where I'm hinting at maybe not but in fact this was exactly how we developed the model to synthesize the audio behind Obama's voice that we saw in yesterday's introductory lecture what we did was we trained a cycle Gan to take data in Alexander's voice and transform it into Data in the manifold of Obama's voice so we can visualize how that spectrogram waveform looks like for Alexander's Voice versus Obama's voice that was completely synthesized using this cyclegan approach hi everybody and welcome to my food sickness191 official introductory course here at NYC hi everybody I replayed it okay but basically what we did was Alexander spoke that exact phrase that was played yesterday and we had the Train Cycle Gan model and we can deploy it then on that exact audio to transform it from the domain of Alexander's voice to Obama's voice generating the synthetic audio that was played for that video clip all right okay before I accidentally uh played again I jump now to the summary slide so today in this lecture we've learned deep generative models specifically talking mostly about latent variable models autoencoders variational Auto encoders where our goal is to learn this low dimensional latent encoding of the data as well as generative adversarial networks where we have these competing generator and discriminator components that are trying to synthesize synthetic examples we've talked about these core foundational generative methods but it turns out as I alluded to in the beginning of the lecture that in this past year in particular we've seen truly truly tremendous advances in generative modeling many of which have not been from those two methods those two foundational methods that we described but rather a new approach called diffusion modeling diffusion models are driving are the driving tools behind the tremendous advances in generative AI that we've seen in this past year in particular viez Gans they're learning these Transformations these encodings but they're largely restricted to generating examples that fall similar to the data space that they've seen before diffusion models have this ability to now hallucinate and envision and imagine completely new objects and instances which we as humans may not have seen or even thought about right parts of the design space that are not covered by the training data so an example here is this AI generated art which art if you will right which was created by a diffusion model and I think not only does this get at some of the limits and capabilities of these powerful models but also questions about what does it mean to create new instances what are the limits and Bounds of these models and how do they how can we think about their advances with respect to human capabilities and human intelligence and so I'm I'm really excited that on Thursday in lecture seven on New Frontiers in deep learning we're going to take a really deep dive into diffusion models talk about their fundamentals talk about not only applications to images but other fields as well in which we're seeing these models really start to make a transformative advances because they are indeed at the very Cutting Edge and very much the New Frontier of generative AI today all right so with that tease and and and um hopefully set the stage for lecture seven on Thursday and conclude and remind you all that we have now about an hour for open Office hour time for you to work on your software Labs come to us ask any questions you may have as well as the Tas who will be here as well thank you so much [Applause] 

foreign I'm really excited especially for this lecture which is a very special lecture on robust and trustworthy deep learning by one of our sponsors of this amazing course themus AI and as you'll see today themus AI is a startup actually locally based here in Cambridge our mission is to design advance and deploy the future of AI and trustworthy AI specifically I'm especially excited about today's lecture because I co-founded Themis right here at MIT right here in this very building in fact this all stemmed from really the incredible scientific innovation and advances that we created right here just a few floors higher than where you're sitting today and because of our background in really cutting edge scientific innovation stemming from MIT themus is very rooted deeply in science and like I said innovation we really aim to advance the future of deep learning and Ai and much of our technology has already grown from published research that we've published a top tier peer review conferences in the AI venues around the world and our work has been covered by high profile International media Outlets this scientific innovation with the scientific innovation themus we are tackling some of the biggest challenges in safety critical AI that exists today and really that stems from the fact that we want to take all of these amazing advances that you're learning as part of this course and actually achieve them in reality as part of our daily lives and we're working together with leading Global industry Partners across many different disciplines ranging from robotics autonomy Health Care and More to develop a line of products that will guarantee safe and trustworthy Ai and we drive this really deeply with our technical engineering and machine learning team and our focus is very much on the engineering very flexible and very modular platforms to scale algorithms towards robust and trustworthy AI this really enables this deployment towards Grand challenges that our society faces with AI today specifically the ability for AI Solutions today are not very trustworthy at all even if they may be very high performance on some of the tasks that we study as part of this course so it's an incredibly exciting time for Themis and specific right now where VC backed we're located our offices are right here in Cambridge so we're local and we have just closed around the funding so we're actively hiring the best and the brightest Engineers like all of you to realize the future of safe and trustworthy Ai and we hope that really today's lecture inspires you to join us on this mission to build the future of AI and with that it's my great pleasure to introduce sadhana sadhana is a machine learning scientist at Themis she's also the lead TA of this course intro to deep learning at MIT her research at Themis focuses specifically on how we can build very modular and flexible methods for AI and building what we call a safe and trustworthy Ai and today she'll be teaching us more about specifically the bias and the uncertainty Realms of AI algorithms which are really two key or critical components towards achieving this Mission or this vision of safe and trustworthy deployment of AI all around us so thank you and please give a big warm Round of Applause for sadhana [Applause] I'm a machine learning scientist here at Themis Ai and the lead TA of the course this year and today I'm super excited to talk to you all about robust and trustworthy deep learning on behalf of Themis so over the past decade we've seen some tremendous growth in artificial intelligence across safety critical domains in the Spheres of autonomy and Robotics we now have models that can make critical decisions about things like self-driving at a second's notice and these are Paving the way for fully autonomous vehicles and robots and that's not where this stops in the Spheres of medicine and Healthcare robots are now equipped to conduct life-saving surgery we have algorithms that generate predictions for critical drugs that may cure diseases that we previously thought were incurable and we have models that can automatically diagnose diseases without intervention from any Health Care Professionals at all these advances are revolutionary and they have the potential to change Life as we know it today but there's another question that we need to ask which is where are these models in real life a lot of these Technologies were innovated five ten years ago but you and I don't see them in our daily lives so what is what's the Gap here between Innovation and deployment the reason why you and I can't go buy self-driving cars or robots don't typically assist in operating rooms is this these are some headlines about the failures of AI from the last few years alone in addition to these incredible Advantage advances we've also seen catastrophic failures and every single one of the safety critical domains I just mentioned these problems range from crashing autonomous vehicles to healthcare algorithms that don't actually work for everyone even though they're deployed out in the real world so everyone can use them now at a first glance this seems really demoralizing if these are all of the things wrong with artificial intelligence how are we ever going to achieve that vision of having our AI integrated into the fabric of our daily lives in terms of safety critical deployment but at them is this is exactly the type of problem that we solve we want to bring these advances to the real world and the way we do this is by innovating in the Spheres of safe and trustworthy artificial intelligence in order to bring the things that were developed in research labs around the world to customers like you and me and we do this by our core ideology is that we believe that all of the problems on this slide are underlaid by two key Notions the first is bias bias is what happens when machine learning models do better on some demographics than others this results in things like facial detection systems not being able to detect certain faces with high accuracy Siri not being able to recognize voices with accents or algorithms that are trained on imbalanced data sets so what the algorithm believes is a good solution doesn't actually work for everyone in the real world and the second notion that underlies a lot of these problems today is unmitigated and uncommunicated uncertainty this is when models don't know when they can or can't be trusted and this results in scenarios such as self-driving cars continuing to operate in environments when they're not 100 confident instead of giving control to users or robots being moving around in environments that they've never been in before and have high unfamiliarity with and a lot of the problems in modern day AI are the result of a combination of unmitigated bias and uncertainty so today in this lecture we're going to focus on investigating the root causes of all of these problems these two big challenges to robust deep learning we'll also talk about solutions for them that can improve the robustness and safety of all of these algorithms for everyone and we'll start by talking about bias bias is a word that we've all heard outside the context of deep learning but in the context of machine learning it can be Quantified and mathematically defined today we'll talk about how to do this and methods for mitigation of this bias algorithmically and how Themis is innovating in these areas in order to bring new algorithms in this space to Industries around the world afterwards we'll talk about uncertainty which is can we teach a model when it does or doesn't know the answer to us to its given task and we'll talk about the ramifications for this for real world AI so what exactly does bias mean and where is it present in the artificial intelligence life cycle the most intuitive form of bias comes from data we have two different two main types of bias here the first is sampling bias which is when we over sample from some regions of our input data distribution and under sample from others a good example of this is a lot of clinical data sets where they often contain fewer examples of diseased patients than healthy patients because it's much easier to acquire data for healthy patients than their disease counterparts in addition we also have selection bias at the data portion of the AI lifecycle think about Apple's series voice recognition algorithm this model is trained largely on Flawless American English but it's deployed across the real world to be able to recognize voices with accents from all over the world the distribution of the model's training data doesn't match the distribution of this type of of language in the real world because American English is highly overrepresented as opposed to other demographics but that's not where uncertain that's not where bias and data stops these biases can be propagated towards models training Cycles themselves which is what we'll focus on in the second half of this lecture and then once the model is actually deployed which means it's actually put out into the real world and customers or users can actually get the predictions from it we may see um further biases perpetuated that we haven't seen before the first of these is distribution shifts let's say I have a model that I trained on the past 20 years of data and then I deploy it into the real world in 2023 this model will probably do fine because the data input distribution is quite similar to data in the training distribution but what would happen to this model in 2033 it's it probably would not work as well because the distribution that the data is coming from would shift significantly across this decade and if we don't continue to update our models with this input stream of data we're going to have Obsolete and incorrect predictions and finally after deployment there is the evaluation aspect so think back to the Apple Siri example that we've been talking about um if the evaluation metric or the evaluation data set that Siri was evaluated on was also mostly comprised of American English then to anybody this model will look like it does extremely well right it can detect it can recognize American English voices with extremely high accuracy and therefore is deployed into the real world but what about its accuracy on subgroups on accented voices on people who for whom English is not their first language if we don't also test on subgroups in our evaluation metrics we're going to face evaluation bias so now let's talk about another example in the real world of how bias can perpetuate throughout the course of this artificial intelligence life cycle commercial facial detection systems are everywhere you actually played around with some of them in lab two when you trained your vae on a facial detection data set in addition to the lock screens on your cell phones um facial detection systems are also present in the automatic filters that your phone cameras apply whenever you try to take a picture and they're also used in criminal investigations these are three commercial facial detection systems that were deployed and um we'll analyze the biases that might have been present in all of them for in the in the next few minutes so the first thing you may notice is that there is a huge accuracy gap between two different demographics in this um plot this accuracy Gap can get up to 34 keep in mind that this facial detection is a binary classification task everything is either a face or it's not a face this means that a randomly initialized model would be expected to have an accuracy of 50 because it's going to randomly assign whether or not something is a face or not some of these facial detection classifiers do only barely better than random on these underrepresented um data on these underrepresented samples in this population so how did this happen why is there such a blatant Gap in accuracy between these different demographic groups and how did these ever these models ever get deployed in the first place what types of biases were present in these models so a lot of facial detection systems exhibit very clear selection bias this model was likely trained mostly on lighter skin faces and therefore learned those much more effectively than it learned to classify darker skin faces but that's not the only bias that was present the second bias that's often um very present in facial detection systems is evaluation bias because originally this data set that you see on the screen is not the data set that these models were evaluated on they were evaluated on one big bulk data set without any classification into subgroups at all and therefore you can imagine if the data set was also comprised mostly of lighter skin faces these accuracy metrics would be incredibly inflated and therefore would cause unnecessary confidence and we could deploy them into the real world in fact the biases in these models were only uncovered once an independent study actually constructed a data set that is specifically designed to uncover these sorts of biases by balancing across race and gender however there are other ways that data sets can be biased that we haven't yet talked about so so far we've assumed a pretty key assumption in our data set which is that the number of faces in our data is the exact same as the number of non-faces in our data but you can imagine especially if you're looking at things like security feeds this might not always be the case you might be faced with many more negative samples than positive samples in your data set in the most so what what's the problem here in the most extreme case we may assign the label non-face to every item in the data set because the model sees items that are labeled as faces so infrequently that it isn't able to learn um an accurate class boundary between the two SIM between the two classes so how can we mitigate this this is a really big problem and it's very common across a lot of different types of machine learning tasks and data sets and the first way that we can try to mitigate class imbalance is using sample re-weighting which is when instead of uniformly sampling from our data set at a rate um we instead sample at a rate that is inversely proportional to the incidence of a class in our data set so in the previous example if the likelihood if faces were much if the number of faces was much lower than the number of non-faces in our data set we would sample the faces with a higher probability than the negatives so that the model sees both classes equally the second example the second way we can mitigate class and balance is through loss re-weighting which is when instead of having every single mistake that the model makes contribute equally to our total loss function we re-weight the samples such that samples from underrepresented classes contribute more to the loss function so instead of the model assigning every single input face to a as a negative it'll be highly penalized if it does so because the loss of the faces would contribute more to the total loss function than the loss of the negatives and the final way that we can mitigate class imbalance is through batch selection which is when we choose randomly from classes so that every single batch has an equal number of data points per class so is everything solved like clearly there are other forms of bias that exist even when the classes are completely balanced because the thing that we haven't thought about yet is latent features so if you remember from Lab 2 and the last lecture latent features are the actual represent is the actual representation of this image according to the model and so far we've mitigated the problem of when we know that we have underrepresented classes but we haven't mitigated the problem of when we have a lot of variability within the same class let's say we have an equal number of faces and negative examples in our data set what happens if the majority of the faces are from a certain demographic or they have a certain set of features can we still apply the techniques that we just learned about the answer is that we cannot do this and the problem is that the bias present right now is in our latent features all of these images are labeled with the exact same label so according to the as the model all we know is that they're all faces so we have no information about any of these features only from the label therefore we can't apply any of the previous approaches that we used to mitigate class imbalance because our classes are balanced but we have feature imbalance now however we can adapt the previous methods to account for bias in latent features which we'll do in just a few slides so let's unpack this a little bit further we have our potentially biased data set and we're trying to build and deploy a model that classifies the faces in a traditional training pipeline this is what that pipeline would look like we would train our classifier and we would deploy it into the real world but this training pipeline doesn't de-bias our inputs in any way so one thing we could do is label our biased features and then apply resampling so let's say in reality that this data set was biased on hair color most of the data set is made up of people with blonde hair with faces with black hair and red hair underrepresented if we knew this information we could label the hair color of every single person in this data set and we could apply either sample re-weading or loss relating just as we did previously wait does anyone want to tell me what the problem is here there are a couple problems here and that's definitely one of them the first is how do we know that hair color is a biased feature in this data set unless we visually inspect every single sample in this data set we're not going to know what the biased features are and the second thing is exactly what you said which is once we have our bias features going through and annotating every image with this feature is an extremely labor-intensive task that is infeasible in the real world so now the question is what if we had a way to automatically learn latent features and use this learn feature representation to dbias a model so what we want is a way to learn the features of this data set and then automatically determine that samples with the highest feature bias and the samples with the lowest feature bias we've already learned a method of doing this in the generative modeling lecture you all learned about variational autoencoders which are models that learn the latent features of a data set as a recap variational autoencoders work by probabilistically sampling from a learn latent space and then they decode this new latent Vector into back into the original input space measure the Reconstruction loss between the inputs and the outputs and continue to update their representation of the latent space and the reason why we care so much about this latent space is that we want samples that are similar to each other in the input to decode to latent vectors that are very close to each other in this latent space and samples that are far from each other or samples that are dissimilar to each other in the input should decode to should encode to latent vectors that are far from each other in the latent space so now we'll walk through step by step a de-biasing algorithm that automatically uses the latent features learned by a variational autoencoder to under sample and oversample from regions in our data set um before I start I want to point out that this debiasing model is actually the foundation of themis's work this work comes out of a paper that we published a few years ago that has been demonstrated to debias commercial facial detection algorithms and um it was so impactful that we decided to make it available and work with companies and industries and that's how themus was started so let's first start by training a vae on this data set the Z shown here in this diagram ends up being our latent space and the latent space automatically captures features that were important for classification so here's an example latent feature that this model captured this is the facial position of an input face and something that's really crucial here is that we never told the model to calculate the to encode the feature Vector of um the facial position of a given face it learned this automatically because this feature is important for the model to develop a good representation of what a face actually is so now that we have our latent structure we can use it to calculate a distribution of the inputs across every latent variable and we can estimate a probability distribution depending on that's based on the features of every item in this data set essentially what this means is that we can calculate the probability that a certain combination of features appears in our data set based on the latent space that we just learned and then we can over sample denser or sparser areas of this data set and under sample from denser areas of this data set so let's say our distribution looks something like this this is an oversimplification but for visualization purposes and the denser portions of this data set we would expect to have a homogeneous skin color and pose in hair color and very good lighting and then in this parser portions of this data set we would expect to see diverse skin color pose and illumination so now that we have this distribution and we know what areas of our distribution are dense and which areas are sparse we want to under sample areas from the under sample samples that fall in the denser areas of this distribution an oversample data points that fall in the sparser areas of this distribution so for example we would probably under sample points with the very common skin colors hair colors and good lighting that is extremely present in this data set and oversample the diverse images that we saw on the last slide and this allows us to train in a fair and unbiased manner to dig in a little bit more into the math behind how this resampling works this approach basically approximates the latent space via a joint histogram over the individual latent variables so we have a histogram for every latent variable Z sub I and what the histogram essentially does is it discretizes the continuous distribution so that we can calculate probabilities more easily then we multiply the probabilities together across all of the latent distributions and then after that we can develop a an understanding of the joint distribution of all of the samples in our latent space based on this we can Define the adjusted probability for sampling for a particular data point as follows the probability of selecting a sample data point x will be based on the latent space of X such that it is the inverse of the joint approximated distribution we have a parameter Alpha here which is a divising parameter and as Alpha increases this probability will tend to the uniform distribution and if Alpha decreases we tend to de-bias more strongly and this gives us the final weight of the sample in our data set that we can calculate on the Fly and use it to adaptively resample while training and so once we apply these this debiasing we have pretty remarkable results this is the original graph that shows the accuracy gap between the darker Mills and the lighter Mills in this data set once we apply the devising algorithm where as Alpha gets smaller we're devising more and more as we just talked about the this accuracy Gap decreases significantly and that's because we tend to over sample samples with darker skin color and therefore the model learns them better and tends to do better on them keep this algorithm in mind because you're going to need it for the lab 3 competition which I'll talk more about towards the end of this lecture so so far we've been focusing mainly on facial recognition systems and a couple of other systems as canonical examples of bias however bias is actually far more widespread in machine learning consider the example of autonomous driving many data sets are comprised mainly of cars driving down straight and sunny roads in really good weather conditions with very high visibility and this is because the data for these cars are for these algorithms is actually just collected by cars driving down roads however in some specific cases you're going to face adverse weather bad um bad visibility near Collision scenarios and these are actually the samples that are the most important for the model to learn because they're the hardest samples and they're the samples where the model is most likely to fail but in a traditional autonomous driving pipeline these samples are often extremely low have extremely low representation so this is an example where using the unsupervised latent debiasing that we just talked about we would be able to upsample these important data points and under sample the data points of driving down straight and sunny roads similarly consider the example of large language models um an extremely famous paper a couple years ago showed that if you put terms that imply female or women into a large language model powered job search engine you're going to get roles such as artists or things in the humanities but if you help input similar things but of the male counterpart you put things like mail into the the search engine you'll end up with roles for scientists and engineers so this type of bias also occurs regardless of the task at hand for a specific model and finally let's talk about Healthcare recommendation algorithms these recommendation algorithms tend to amplify racial biases a paper from a couple years ago showed that black patients need to be significantly sicker than their white counterparts to get the same level of care and that's because of inherent bias in the data set of this model and so in all of these examples we can use the above algorithmic bias mitigation method to try and solve these problems and more so we just went through how to mitigate some forms of bias in artificial intelligence and where these Solutions may be applied and we talked about a foundational algorithm that Themis uses that UL will also be developing today and for the next part of the lecture we'll focus on uncertainty or when a model does not know the answer we'll talk about why uncertainty is important and how we can estimate it and also the applications of uncertainty estimation so to start with what is uncertainty and why is it necessary to compute let's look at the following example this is a binary classifier that is trained on images of cats and dogs for every single input it will output a probability distribution over these two classes now let's say I give this model an image of a horse it's never seen a horse before the horse is clearly neither a cat nor a dog however the model has no choice but to Output a probability distribution because that's how this model is structured however what if in addition to this prediction we also achieved a confidence estimate in this case the model would should be able to say I've never seen anything like this before and I have very low confidence in this prediction so you as the user should not trust my prediction on this model and that's the core idea behind uncertainty estimation so in the real world uncertainty estimation is useful for scenarios like this this is an example of a Tesla car driving behind a horse-drawn buggy which are very common in some parts of the United States it has no idea what this horse-drawn buggy is it first thinks it's a truck and then a car and then a person and um it continues to Output predictions even though it is very clear that the model does not know what this image is and now you might be asking okay so what's the big deal it didn't recognize the horse-drawn buggy but it seems to drive successfully anyway however the exact same problem that resulted in that video has also resulted in numerous autonomous car crashes so let's go through why something like this might have happened there are multiple different types of uncertainty in neural networks which may cause incidents like the ones that we just saw we'll go through a simple example that illustrates the two main types of uncertainty that we'll focus on in this lecture so let's say I'm trying to estimate the curve Y equals X cubed as part of a regression task the input here x is some real number and we want it to Output f of x which is should be ideally X cubed so right away you might notice that there are some issues in this data set assume the red points in this image are your training samples so the boxed area of this image shows data points in our data set where we have really high noise these points do not follow the curve Y equals X cubed in fact they don't really seem to follow any distribution at all and um the model won't be able to um compute outputs for the air um points in this region accurately because very similar inputs have extremely different outputs which is the definition of data uncertainty we also have regions in this data set where we have no data so if we queried the model for a prediction in this part of in this region of the data set we should not really expect to see an accurate result because the model's never seen anything like this before and this is what is called Model uncertainty when the model hasn't seen enough data points or cannot estimate that area of the input distribution accurately enough to Output a correct prediction so what would happen if I added the following blue training points to um the areas of the data set with high model uncertainty do you think the model uncertainty would decrease raise your hand does anyone think it would not change okay so yeah most of you were correct model uncertainty can typically be reduced by adding in data into any region but specifically regions with high model uncertainty and now what happens if we we add these blue data points into this data set would anyone expect the data uncertainty to decrease you can raise your hand that's correct so data uncertainty is irreducible in the real world the blue points and the noisy red points on this image correspond to things like robot sensors let's say I'm I have a robot that's trained to um that has a sensor that is making measurements of uh depth if the sensor has noise in it there's no way that I can add any more data into the system to reduce that noise unless I replaced my sensor entirely so now let's assign some names to the types of uncertainty that we just talked about the blue area or the area of high data uncertainty is known as aliatoric uncertainty it is irreducible as we just mentioned and it can be directly learned from data which we'll talk about in a little bit the green areas of this just the green boxes that we talked about which were Model uncertainty are known as epistemic uncertainty and this cannot be learned directly from the data however it can be reduced by adding more data into our systems into these regions okay so first let's go through aliatoric uncertainty so the goal of out estimating alliatoric uncertainty is to learn a set of variances that correspond to the input keep in mind that we are not looking at a data distribution and we are as humans are not estimating the variance we're training the model to do this task and so what that means is typically when we train a model we give it an input X and we expect an output y hat which is the prediction of the model now we also predict an additional Sigma squared so we add another layer to our model we have the same output size that predicts a variance for every output so the reason why we do this is that we expect that areas in our data set with high data and certainty are going to have higher variance and The crucial thing to remember here is that this variance is not constant it depends on the value of x we typically tend to think of variance as a single number that parameterizes an entire distribution however in this case we may have areas of our input distribution with really high variance and we may have areas with very low variance so our variance cannot be independent of the input and it depends on our input X so now that we have this model we have an extra layer attached to it in addition to predicting y hat we also predict a sigma squared how do we train this model our current loss function does not take into account variance at any point this is your typical mean squared error loss function that is used to train regression models and there's no way training from this loss function that we can learn whether or not the variance that we're estimating is accurate so in addition to adding another layer to estimate alliatoric uncertainty correctly we also have to change our loss function so the mean squared error actually um learns a multivariate gaussian with a mean y i and constant variance and we want to generalize this loss function to when we don't have constant variance and the way we do this is by changing the loss function to the negative log likelihood we can think about this for now as a generalization of the mean squared error loss to non-constant variances so now that we have a sigma squared term in our loss function we can determine how accurately the sigma and the Y that we're predicting parametrize the distribution that is our input so now that we know how to estimate aliatoric uncertainty let's look at a real world example for this task we'll focus on semantic segmentation which is when we label every pixel of an image with its corresponding class we do this for scene understanding and because it is more fine-grained than a typical object detection algorithm so the inputs of this data to this data set are known as it's from a data set called cityscapes and the inputs are RGB images of scenes the labels are pixel wise annotations of this entire image of which label every pixel belongs to and the outputs try to mimic the labels they're also predicted pixel wise masks so why would we expect that this data set has high natural alliatoric uncertainty and which parts of this data set do you think would have aliatoric uncertainty because labeling every single Pixel of an image is such a labor-intensive task and it's also very hard to do accurately we would expect that the boundaries between um between objects in this image have high alliatoric uncertainty and that's exactly what we see if you train a model to predict aliatoric uncertainty on this data set corners and boundaries have the highest aliatoric uncertainty because even if your pixels are like one row off or one column off that introduces noise into the model the model can still learn in the face of this noise but it does exist and it can't be reduced so now that we know about data uncertainty or aliatoric uncertainty let's move on to learning about epistemic uncertainty as a recap epistemic uncertainty can best be described as uncertainty in the model itself and it is reducible by adding data to the model so with epistemic uncertainty essentially what we're trying to ask is is the model unconfident about a prediction so a really simple and very smart way to do this is let's say I train the same network multiple times with random initializations and I ask it to predict the exact I call it on the same input so let's say I give model one the exact same input and the blue X is the output of this model and then I do the same thing again with model 2. and then again with model 3 and again with model 4. these models all have the exact same hyper parameters the exact same architecture and they're trained in the same way the only difference between them is that their weights are all randomly initialized so they're where they start from is different and the reason why we can use this to determine epistemic uncertainty is because we would expect that with familiar inputs in our Network our networks should all converge to around the same answer and we should see very little variance in the um the logits or the outputs that we're predicting however if a model has never seen a specific input before or that input is very hard to learn all of these models should predict slightly different answers and the variance of them should be higher than if they were predicting a similar input so creating an ensemble of networks is quite similar it's quite simple you start out with defining the number of ensembles you want you create them all the exact same way and then you fit them all on the same training data and training data and then afterwards when at inference time we call ever all of the models every model in The Ensemble on our specific input and then we can treat um our new prediction as the average of all of the ensembles this results in a usually more robust and accurate prediction and we can treat the uncertainty as the variance of all of these predictions keep in again remember that if we saw familiar inputs or inputs with low epistemic uncertainty we should expect to have very little variance and if we had a very unfamiliar input or something that was out of distribution or something the model hasn't seen before we should have very high epistemic uncertainty or variance so what's the problem with this can anyone raise their hand and tell me what a problem with training an ensemble of networks is so training an ensemble of networks is really compute expensive even if your model is not very large training five copies of it or 10 copies of it tends to it takes up compute and time and that's just not really feasible when we're training um if on specific tasks however the key insight for ensembles is that by introducing some method of Randomness or stochasticity into our networks we're able to estimate epistemic uncertainty so another way that we've seen about um introducing stochasticity into networks is by using Dropout layers we've seen Dropout layers as a method of reducing overfitting because we randomly drop out different nodes in our in our layer and then we continue to propagate information through them and it prevents models from memorizing data however in the um case of epistemic uncertainty we can add Dropout layers after every single layer in our model and in addition we can keep these Dropout layers enabled at test time usually we don't keep Dropout layers enabled at test time because we don't want to lose any information about the the Network's process or any weights when we're at inference time however when we're estimating epistemic uncertainty we do want to keep Dropout enabled at test time because that's how we can introduce Randomness at inference time as well so what we do here is we have one model it's the same model the entire way through we add Dropout layers with a specific probability and then we run multiple forward passes and at every forward pass different layers get dropped different nodes in a layer get dropped out so we have that that measure of Randomness and stochasticity so again in order to implement this what we have is a model with the exact one model and then when we're running our forward passes we can simply run T forward passes where T is usually a number like 20. um we keep Dropout enabled at test time and then we use the mean of these samples as the new prediction and the variance of these samples as a measure of epistemic uncertainty so both of the methods we talked about just now involve sampling and sampling is expensive ensembling is very expensive but even if you have a pretty large model um having or introducing Dropout layers and calling 24 word passes might also be something that's pretty infeasible and at Themis we're dedicated to developing Innovative methods of estimating epistemic uncertainty that don't rely on things like sampling so that they're more generalizable and they're usable by more Industries and people so a method that we've developed to estimate a method that we've studied to estimate epistemic uncertainty is by using generative modeling so we've talked about vaes a couple times now but let's say I trained a vae on the exact same data set we were talking about earlier which is only dogs and cats the leading space of this model would be comprised of features that relate to dogs and cats and if I give it a prototypical dog it should be able to generate a pretty good representation of this dog and it should have pretty low reconstruction loss now if I gave the same example of the horse to this vae the latency the latent Vector that this horse would be decoded to would be incomprehensible to the decoder of this network the decoder wouldn't be able to know how to project the latent Vector back into the original input space and therefore we should expect to see a much worse reconstruction here and we should see that the Reconstruction loss is much higher than if we gave the model A Familiar input or something that it was used to seeing so now let's move on to what I think is the most exciting method of estimating epistemic uncertainty that we'll talk about today so in both of the examples before sampling is compute intensive but generative modeling can also be compute intensive let's say you don't actually need a variational autoencoder for your task then you're training an entire decoder for no reason and other than to estimate the epistemic uncertainty so what if we had a method that did not rely on generative modeling or sampling in order to estimate the epistemic uncertainty that's exactly what a method method that we've developed here at Themis does so we view learning as an evidence-based process so if you remember from earlier when we were training The Ensemble and calling multiple ensembles on the same input we received multiple predictions and we calculated that variance now the way we frame evidential learning is what if we assume that those data points those predictions were actually drawn from a distribution themselves if we could estimate the parameters of this higher order evidential distribution we would be able to learn this variance or this measure of epistemic uncertainty automatically without doing any sampling or generative modeling and that's exactly what evidential uncertainty does so now that we have many methods in our toolbox for episode for estimating epistemic uncertainty let's go back to our real world example let's say the again the input is the same as before it's a RGB image of some scene in a city and the output again is a pixel level mask of what every pixel in this image belongs to which class it belongs to which parts of the data set would you expect to have high epistemic uncertainty in this example take a look at the output of the model itself the model does mostly well on semantic segmentation however it gets the sidewalk wrong um it assigns some of the sidewalk to the road and other parts of the sidewalk are labeled incorrectly and we can using epistemic uncertainty we can see why this is the areas of the sidewalk that are discolored have high levels of epistemic uncertainty maybe this is because the model has never seen an example of a sidewalk with multiple different colors in it before or maybe it hasn't been trained on examples with sidewalks generally either way epistemic uncertainty has isolated this specific area of the image as an area of high uncertainty so today we've gone through two major challenges for robust steep learning we've talked about bias which is what happens when models are skewed by sensitive feature inputs and uncertainty which is when we can measure a level of confidence of a certain model now we'll talk about how themus uses these Concepts to build products that transform models to make them more risk aware and how we're changing the AI landscape in terms of safe and trustworthy AI so at Themis we believe that uncertainty and bias mitigation unlock a um a host of new solutions to solving these problems with safe and responsible AI we can use bias and uncertainty to mitigate risk in every part of the AI life cycle let's start with labeling data today we talked about aliatoric uncertainty which is a method to detect mislabeled samples to highlight label noise and to generally maybe tell labelers to re relabel images or samples that they've gotten that may be wrong in the second part of this cycle we have analyzing the data before a model is even trained on any data we can analyze the bias that is present in this data set and tell the creators whether or not they should add more samples which demographics which areas of the data set are underrepresented in the current data set before we even train a model on them and then let's go to training the model once we're actually training a model if it's already been trained on a bias data set we can de-bias it adaptively during training using the methods that we talked about today and afterwards we can also verify or certify deployed machine learning models making sure that models that are actually out there are as safe and unbiased as they claim they are and the way we can do this is by leveraging epistemic uncertainty or bias in order to calculate the samples or data points that the model will do the worst on the model has the high the most trouble learning or data set samples that are the most underrepresented in a model's data set if we can test the model on these samples specifically the hardest samples for the model and it does well then we know that the model has probably been trained in a fair and unbiased manner that mitigates uncertainty and lastly we can think about we're developing a product at Themis called AI guardian and that's essentially a layer between the artificial intelligence algorithm and the user and the way this works is this is the type of algorithm that if you're driving an autonomous vehicle would say hey the model doesn't actually know what is happening in the world around it right now as the user you should take control of this autonomous vehicle and we can apply this to spheres outside autonomy as well so you'll notice that I skipped one part of the cycle I skipped the part about building the model and that's because today we're going to focus a little bit on um themes ai's product called capsa which is a model agnostic framework for risk estimation so capsa is an open source Library you all will actually use it in your lab today that transforms models so that they're risk aware so this is a typical training pipeline you've seen this many times in the course by now we have our data we have the model and it's fed into the training algorithm and we get a trained model at the end that outputs a prediction for every input but with capsa what we can do is by adding a single line into any training workflow we can turn this model into a risk-aware variant that essentially calculates biases uncertainty and label noise for you because today as you're probably as you've heard by now there are so many methods of estimating uncertainty and bias and sometimes certain methods are better than others it's really hard to determine what kind of uncertainty you're trying to estimate and how to do so so capsa takes care of this for you by inserting one line into your training workflow you can achieve a risk aware model that you can then further analyze and so this is the one line that I've been talking about um after you build your model you can just create a wrapper or you can call a wrapper that capsa has a an extensive library of and then in addition to achieving prediction or receiving predictions from your model you can also receive whatever bias or uncertainty metric that you're trying to estimate and the way capsule works is it does this by wrapping models for every uncertainty metric that we want to estimate we can apply and create the minimal model modifications as necessary while preserving the initial architecture and predictive capabilities in the case of aliatoric uncertainty this could be adding a new layer in the case of a variational autoencoder this could be creating and training the decoder and calculating the Reconstruction loss on the Fly and this is an example of capsa working on one of the data sets that we talked about today which was the cubic data set with added noise in it and also another simple classification task and the reason why I wanted to show this image is to show that using capsa we can achieve all of these uncertainty estimates with very little additional added work so using all of the products that I just talked about today and using capsa Themis is unlocking the key to deploy deep learning models safely across fields we can now answer a lot of the questions that the headlines were raising earlier which is when should a human take control of an autonomous vehicle what types of data are underrepresented in commercial autonomous driving pipelines we now have educated answers to these questions due to products that Themis is developing and in spheres such as medicine and health care we can now answer questions such as when is a model uncertain about a life-threatening diagnosis one should this diagnosis be passed to a medical professional before this information is conveyed to a patient or what types of patients might drug Discovery algorithms be biased against and today the the application that you guys will focus on is on facial detection you'll use capsa in today's lab to thoroughly analyze a common facial detection data set that we've perturbed in some ways for you so that you can discover them on your own and we we highly encourage you to compete in the competition which the details are described in the lab but basically it's about analyzing this data set creating risk-aware models that mitigate bias and uncertainty in the specific training pipeline and so at Themis our goal is to design advance and deploy a trustworthy AI across Industries and around the world um we're passionate about scientific innovation we release open source tools like the ones you'll use today and our products transform Ai workflows and make artificial intelligence safer for everyone we partner with Industries around the globe and we're hiring for the upcoming summer and for full-time roles so if you're interested please send an email to careers themesai.io or apply by submitting your resume to the Deep learning resume drop and we'll see those resumes and get back to you thank you foreign 

foreign hi everyone welcome back today I think that these two lectures today are really exciting because they start to move Beyond you know a lot of what we've talked about in the class so far which is focusing a lot on really static data sets and specifically in today in this lecture right now I'm going to start to talk about how we can learn about this very long-standing field of how we can specifically marry two topics the first topic being reinforcement learning which has existed for many many decades together with a lot of the very recent advances in deep learning which you've already started learning about as part of this course now this marriage of these two Fields is actually really fascinating to me particularly because like I said it moves away from this whole Paradigm of uh or really this whole Paradigm that we've been exposed to in the class thus far and that Paradigm is really how we can build a deep learning model using some data set but that data set is typically fixed in in our world right we collect we go out and go collect that data set we deploy it on our machine learning or deep learning algorithm and then we can evaluate on a brand new data set but that is very different than the way things work in the real world in the real world you have your deep learning model actually deployed together with the data together out into reality exploring interacting with its environment and trying out a whole bunch of different actions and different things in that environment in order to be able to learn how to best perform any particular tasks that it may need to accomplish and typically you want to be able to do this without explicit human supervision right this is the key motivation of reinforcement learning you're going to try and learn through reinforcement making mistakes in your world and then collecting data on those mistakes to learn how to improve now this is obviously a huge field in or a huge Topic in the field of Robotics and autonomy you can think of self-driving cars and robot manipulation but also very recently we've started seeing incredible advances of deep reinforcement learning specifically also on the side of gameplay and strategy making as well so one really cool thing is that now you can even imagine right this this combination of Robotics together with gameplay right now training robots to play against us in the real world and I'll just play this very short video on Starcraft and deepmind perfect information and is played in real time it also requires long-term planning and the ability to choose what action to take from millions and millions of possibilities I'm hoping for a 5-0 not to lose any games but I think the realistic goal would be four and one in my favor I think he looks more confident than Taylor Taylor was quite nervous before the room was much more tense this time really didn't know what to expect playing Starcraft pretty much since he's five I wasn't expecting the AI to be that good everything that he did was proper it was calculated and it was done well I thought I'm learning something it's not very unexpected I would consider myself a good player right but I lost every single one of five games we're way ahead of why right so let's take maybe a start and take a step back first of all and think about how reinforcement learning fits into this whole Paradigm of all of the different topics that you've been exposed to in this class so far so as a whole I think that we've really covered two different types of learning in this course to date right up until now we've really started focusing in the beginning part of the lectures firstly on what we called supervised learning right supervised learning is in this domain where we're given data in the form of x's our inputs and our labels y right and our goal here is to learn a function or a neural network that can learn to predict why given our inputs X so for example if you consider this example of an apple right observing a bunch of images of apples we want to detect you know in the future if we see a new image of an apple to detect that this is indeed an apple now the second class of learning approaches that we've discovered yesterday in yesterday's lecture was that of unsupervised learning and in these algorithms you have only access to the data there's no notion of labels right this is what we learned about yesterday in these types of algorithms you're not trying to predict a label but you're trying to uncover some of the underlying structure what we were calling basically these latent variables these hidden features in your data so for example in this apple example right using unsupervised learning the analogous example would basically be to build a model that could understand and cluster certain certain parts of these images together and maybe it doesn't have to understand that necessarily this is an image of an apple but it needs to understand that you know this image of the red apple is similar it has the same latent features and same semantic meaning as this black and white outline sketch of the Apple now in today's lecture we're going to talk about yet another type of learning algorithms right in reinforcement learning we're going to be only given data in the form of what are called State action pairs right now states are observations right this is what the the agent let's call it the neural network is going to observe it's what it sees the actions are the behaviors that this agent takes in those particular States so the goal of reinforcement learning is to build an agent that can learn how to maximize what are called rewards right this is the third component that is specific to reinforcement learning and you want to maximize all of those rewards over many many time steps in the future so again in this apple example we might now see that the agent doesn't necessarily learn that okay this is an apple or it looks like these other apples now it has to learn to let's say eat the apple take an action eat that Apple because it has learned that eating that Apple makes it live longer or survived because it doesn't starve so in today right like I said we're going to be focusing exclusively on this third type of learning Paradigm which is reinforcement learning and before we go any further I just want to start by building up some very key terminology and like basically background for all of you so that we're all on the same page when we start discussing some of the more complex components of today's lecture so let's start by building up you know some of this terminology the first main piece of terminology is that of an agent right an agent is a a being basically that can take actions for example you can think of an agent as as a machine right that is let's say an autonomous drone that is making a delivery or for example in a game it could be Super Mario that's navigating inside of your video game the algorithm itself it's important to remember that the algorithm is the agent right we're trying to build an agent that can do these tasks and the algorithm is that agent so in life for example all of you are agents in life the environment is the other kind of contrary approach or the contrary perspective to the agent the environment is simply the world where that agent lives and where it operates right it where it exists and it moves around in the agent can send commands to that environment in the form of what are called actions right you can take actions in that environment and let's call first notation purposes let's say the possible set of all actions that it could take is let's say a set of capital a right now it should be noted that agents at any point in time could choose amongst this let's say list of possible actions but of course in some situations your action space does not necessarily need to be a finite space right maybe you could take actions in a continuous space for example when you're driving a car you're taking actions on a continuous angle space of what angle you want to steer that car it's not necessarily just going right or left or straight you may stare at any continuous degree observations is essentially how the environment responds back to the agent right the environment can tell the agent you know what it should be seeing based on those actions that it just took and it responds in the form of what is called a state a state is simply a concrete and immediate situation that the agent finds itself in at that particular moment now it's important to remember that unlike other types of learning that we've covered in this course reinforcement learning is a bit unique because it has one more component here in addition to these other components which is called the reward now the reward is a feedback by which we measure or we can try to measure the success of a particular agent in its environment so for example in a video game when Mario grabs a coin for example he wins points right so from a given State an agent can send out any form of actions to take some decisions and those actions may or may not result in rewards being collected and accumulated over time now it's also very important to remember that not all actions result in immediate rewards you may take some actions that will result in a reward in a delayed fashion maybe in a few time steps down the future or maybe in life maybe years you may take an action today that results in a reward many uh some time from now right and but essentially all of these try to effectively evaluate some way of measuring the success of a particular action that an agent takes so for example when we look at the total reward that an agent accumulates over the course of its lifetime we can simply sum up all of the rewards that an agent gets after a certain time T right so this capital r of T is the sum of all rewards from that point on into the future into Infinity and that can be expanded to look exactly like this it's reward at time t plus the reward time t plus one plus t plus two and so on and so forth often it's actually very useful for all of us to consider not only the sum of all of these rewards but instead What's called the discounted sum so you can see here I've added this gamma factor in front of all of the rewards and and that discounting factor is essentially multiplied by every future reward that the agent sees and it's discovered by the agent and the reason that we want to do this is actually this dampening factor is designed to make future rewards essentially worth less than rewards that we might see at this instant right at this moment right now now you can think of this as basically enforcing some kind of short-term uh a greediness in the algorithm right so for example if I offered you a reward of five dollars today or a reward of five dollars in 10 years from now I think all of you would prefer that five dollars today simply because we have that same discounting factor applied to this to this processing right we have that factor that that five dollars is not worth as much to us if it's given to us 10 years in the future and that's exactly how this is captured here as well mathematically this discounting factor is like multiple like I said multiplied at every single future award exponentially and it's important to understand that also typically this discounting factor is you know between zero and one there are some exceptional cases where maybe you want some Strange Behaviors and have a discounting factor greater than one but in general that's not something we're going to be talking about today now finally it's very important in reinforcement learning this special function called The Q function which ties in a lot of these different components that I've just shared with you all together now let's look at what this Q function is right so we already covered this R of T function right R of T is the discounted sum of rewards from time T all the way into the future into time Infinity but remember that this R of T right it's discounted number one and number two we're going to try and build a q function a function that captures the the maximum or the best action that we could take that will maximize this reward so let me say that one more time in a different way the Q function takes as input two different things the first is the state that you're currently in and the second is a possible action that you could execute in this particular state so here s of T is that state at time t a of T is that action that you may want to take at time T and the Q function of these two pieces is going to denote or capture what the expected total return would be of that agent if it took that action in that particular state now one thing that I think maybe we should all be asking ourselves now is this seems like a really powerful function right if you had access to this type of a function this Q function I think you could actually perform a lot of tasks right off the bat right so if you wanted to for example understand how to what actions to take in a particular State and let's suppose I gave you this magical Q function does anyone have any ideas of how you could transform that Q function to directly infer what action should be taken yep given a state you can look at your possible action space and pick the one that gives you the highest Q values exactly so that's exactly right so just to repeat that one more time the queue function tells us for any possible action right what is the expected reward for that action to be taken so if we wanted to take a specific action given in a specific State ultimately we need to you know figure out which action is the best action the way we do that from a q function is simply to pick the action that will maximize our future reward and we can simply try out number one if we have a discrete action space we can simply try out all possible actions compute their Q value for every single possible action based on the state that we currently find ourselves in and then we pick the action that is going to result in the highest Q value if we have a continuous action space Maybe we do something a bit more intelligent maybe following the gradients along this Q value curve and maximizing it as part of an optimization procedure but generally in this lecture what I want to focus on is actually how we can obtain this Q function to start with right I I kind of skipped a lot of steps in that last slide where I just said let's suppose I give you this magical Q function how can you determine what action to take but in reality we're not given that Q function we have to learn that Q function using deep learning and that's what today's lecture is going to be talking about primarily is first of all how can we construct and learn that Q function from data and then of course the final step is use that Q function to you know take some actions in the real world and broadly speaking there are two classes of reinforcement learning algorithms that we're going to briefly touch on as part of today's lecture the first class is what's going to be called value learning and that's exactly this process that we've just talked about value learning tries to estimate our Q function right so to find that Q function Q given our state and our action and then use that Q function to you know optimize for the best action to take given a particular state that we find our cell then the second class of algorithms which we'll touch on right at the end of today's lecture is kind of a different framing of the same approach but instead of first optimizing the Q function and finding the Q value and then using that Q function to optimize our actions what if we just try to directly optimize our policy which is what action to take based on a particular state that we find ourselves in if we do that if we can obtain this function right then we can directly sample from that policy distribution to obtain the optimal action we'll talk more details about that later in the lecture but first let's cover this first class of approaches which is Q learning approaches and we'll build up that intuition and that knowledge onto the second part of policy learning so maybe let's start by just digging a bit deeper into the Q function specifically just to start to understand you know how we could estimate this in the beginning so first let me introduce this game maybe some of you recognize this is the game of called Atari Breakout the the game here is essentially one where the agent is able to move left to right this paddle on the bottom left or right and the objective is to move it in a way that this ball that's coming down towards the bottom of the screen can be you know bounced off of your pedal reflected back up and essentially you want to break out right reflect that ball back up to the top of the screen towards the rainbow portion and keep breaking off every time you hit a pixel on the top of the screen you break off that pixel the objective of the game is to basically eliminate all of those rainbow pixels right so we want to keep hitting that ball against the top of the screen until you remove all the pixels now the Q function tells us you know the expected total return or the total reward that we can expect based on a given State an action pair that we may find ourselves in this game now the first point I want to make here is that sometimes even for us as humans to understand what the Q value should be is sometimes quite unintuitive right so here's one example let's say we find these two State action pairs right here is a and b two different options that we can be presented with in this game a the ball is coming straight down towards us that's our state our action is to do nothing and simply reflect that ball back up uh vertically up the second situation the state is basically that the ball is coming slightly at an angle we're not quite underneath it yet and we need to move towards it and actually hit that ball in a way that you know will will make it and not miss it hopefully right so hopefully that ball doesn't pass below us then the game would be over can you imagine you know which of these two options might have a higher Q value for the network which one would result in a greater reward for the neural network or for the agent so how many people believe a would result in a higher return okay how about B okay how about someone who picked B can you tell me why B agency you're actually doing something okay yeah how about more for a you only have like the maximum you can take off is like one because after you reflect your automatically coming back down but then be you can bounce around and there's more than one at least exactly and actually there's a very interesting thing so when I first saw this actually it's uh uh it was very unintuitive for me why a is actually working much worse than D but in general this very conservative action of B you're kind of exactly like you said the two answers were implying is that a is a very conservative action you're kind of only going up and down it will achieve a good reward it will solve the game right it's in fact it solves the game exactly like this right here you can see in general this action is going to be quite conservative it's just bouncing up hitting one point at a time from the top and breaking off very slowly the board that you can see here but in general you see the part of the board that's being broken off is towards the center of the board right not much on the edges of the board if you look at B now with B you're kind of having agency like one of the answers said you're coming towards the ball and what that implies is that you're sometimes going to actually hit the corner of your paddle and have a very extreme angle on your paddle and hit the sides of the board as well and it turns out that the algorithm the agent can actually learn that hitting the side of the board can have some kind of unexpected consequences that look like this so here you see it trying to enact that policy it's targeting the sides of the board but once it reaches a breakout on the side of the board it found this hack in the solution where now it's breaking off a ton of points so that was a kind of a trick that this neural network learned it was a way that it even moves away from the ball as it's coming down just so it could move back towards it just to hit it on the corner and execute on those those Corner parts of the board and break out a lot of pieces for free almost so now that we can see that sometimes obtaining the Q function can be a little bit unintuitive but the key Point here is that if we have the Q function we can directly use it to determine you know what is the best action that we can take in any given state that we find ourselves in so now the question naturally is how can we train a neural network that can indeed learn this Q function so the type of the neural network here naturally because we have a function that takes us input two things let's imagine our neural network will also take as input these two objects as well one object is going to be the state of the board you can think of this as simply the pixels that are on the screen describing that board so it's an image of the board at a particular time maybe you want to even provide two or three images to give it some sense of temporal information and some past history as well but all of that information can be combined together and provided to the network in the form of a state and in addition to that you may also want to provided some actions as well right so in this case the actions that a neural network or an agent could take in this game is to move to the right to the left to stay still right and those could be three different actions that could be provided and parametrized to the input of a neural network the goal here is to you know estimate the single number output that measures what is the expected value or the expected Q value of this neural network at this particular state State action pair now oftentimes what you'll see is that if you wanted to evaluate let's suppose a very large action space it's going to be very inefficient to try the approach on the left with the with a very large action space because what it would mean is that you'd have to run your neural network forward many different times one time for every single element of Your Action space so what if instead you only provided it an input of your state and as output you gave it let's say all n different Q values one Q value for every single possible action that way you only need to run your neural network once for the given state that you're in and then that neural network will tell you for all possible actions what's the maximum it simply then look at that output and pick the action that has the Chi's Q value now what would happen right so actually the question I want to pose here is really you know we want to train one of these two networks let's stick with the network on the right for Simplicity just since it's a much more efficient version of the network on the left and the question is you know how do we actually train that Network on the right and specifically I want all of you to think about really the best case scenario just to start with how an agent would perform ideally in a particular situation or what would happen right if an agent took all of the ideal actions at any given State this would mean that essentially the target return right the the predicted or the the value that we're trying to predict the target is going to always be maximized right and this can serve as essentially the ground truth to the agent now for example to do this we want to formulate a loss function that's going to essentially represent our expected return if we're able to take all of the best actions right so for example if we select an initial reward plus selecting some action in our action space that maximizes our expected return then for the next future State we need to apply that discounting factor and and recursively apply the same equation and that simply turns into our Target right now we can ask basically what does our neural network predict right so that's our Target and we recall from previous lectures if we have a Target value in this case our Q value is a continuous variable we have also a predicted variable that is going to come as part of the output of every single one of these potential actions that could be taken we can Define what's called a q loss which is essentially just a very simple mean squared error loss between these two continuous variables we minimize their distance over two over many many different iterations of buying our neural network in this environment observing actions and observing not only the actions but most importantly after the action is committed or executed we can see exactly the ground truth expected return right so we have the ground truth labels to train and supervise this model directly from the actions that were executed as part of random selection for example now let me just stop right there and maybe summarize the whole process one more time and maybe a bit different terminology just to give everyone kind of a different perspective on this same problem so our deep neural network that we're trying to train looks like this right it takes us input a state is trying to Output n different numbers those n different numbers correspond to the Q value Associated to n different actions one Q value per action here the actions in Atari Breakout for example should be three actions we can either go left we can go right or we can do nothing we can stay where we are right so the next step from this we saw if we have this Q value output what we can do with it is we can make an action or we can even let me be more formal about it we can develop what's called a policy function a policy function is a function that given a state it determines what is the best action so that's different than the Q function right the Q function tells us given a state what is the best or what is the value the return of every action that we could take the policy function tells us one step more than that given it given a state what is the best action right so it's a very end-to-end way of thinking about you know the agent's decision making process based on what I see right now what is the action that I should take and we can determine that policy function directly from the Q function itself simply by maximizing and optimizing all of the different Q values for all of the different actions that we see here so for example here we can see that given this state the Q function has the results of these three different values has a q value of 20 if it goes to the left has a q value of 3 if it stays in the same place and it has a q value of zero it's going to basically die after this iteration if it moves to the right because you can see that the ball is coming to the left of it if it moves to the right the game is over right so it needs to move to the left in order to do that in order to continue the game and the Q value reflects that the optimal action here is simply going to be the maximum of these three Q values in this case it's going to be 20 and then the action is going to be the corresponding action that comes from that 20 which is moving left now we can send this action back to the environment in the form of the game to execute the next step right and as the agent moves through this environment it's going to be responded with not only by new pixels that come from the game but more importantly some reward signal now it's very important to remember that the reward signals in pong or sorry in in Atari Breakout are very sparse right you get a reward not necessarily based on the action that you take at this exact moment it usually takes a few time steps for that ball to travel back up to the top of the screen so usually your rewards will be quite delayed maybe at least by several time steps sometimes even more if you're bouncing off of the corners of the screen now one very uh popular or very famous approach that showed this was presented by deepmind Google deepmind several years ago where they showed that you could train a q value Network and you can see the input on the left hand side is simply the raw pixels coming from the screen all the way to the actions of a controller on the right hand side and you could train this one network for a variety of different tasks all across the Atari Breakout ecosystem of games and for each of these tasks the really fascinating thing that they showed was for this very simple algorithm which really relies on random choice of selection of actions and then you know learning from you know actions that don't do very well that you discourage them and trying to do actions that did perform well more frequently very simple algorithm but what they found was even with that type of algorithm they were able to surpass human level performance on over half of the game there were some games that you can see here were still below human level performance but as we'll see this was really like a such an exciting Advance because of the Simplicity of the algorithm and how you know clean the formulation of the training was you only needed a very little amount of prior knowledge to impose onto this neural network for it to be able to learn how to play these games you never had to teach any of the rules of the game right you only had to let it explore its environment play the game many many times against itself and learn directly from that data now there are several very important downsides of Q learning and hopefully these are going to motivate the second part of today's lecture which we'll talk about but the first one that I want to really convey to everyone here is that you learning is naturally um applicable to discrete action spaces right because you can think of this output space that we're providing it's kind of like one number per action that could be taken now if we have a continuous action space we have to think about clever ways to work around that in fact there are now more recently there are some solutions to achieve queue learning and continuous action spaces but for the most part Q learning is very well suited for discrete action spaces and we'll talk about ways of overcoming that with other approaches a bit later and the second component here is that the policy that we're learning right the Q function is giving rise to that policy which is the thing that we're actually using to determine what action to take given any state that policy is determined by you know deterministically optimizing that Q function we simply look at the results from the Q function and apply our or we we look at the results of the Q function and we pick the action that has the best or the highest Q value that is very dangerous in many cases because of the fact that it's always going to pick the best value for a given State there's no stochasticity in that pipeline so you can very frequently get caught in situations where you keep repeating the same actions and you don't learn to explore potentially different options that you may be thinking of so to address these very important challenges that's hopefully going to motivate now the next part of today's lecture which is going to be focused on policy learning which is a different class of reinforcement learning algorithms that are different than Q learning algorithms and like I said those are called policy gradient algorithms and policy gradient algorithms the main difference is that instead of trying to infer the policy from the Q function we're just going to build a neural Network that will directly learn that policy function from the data right so it kind of Skips one step and we'll see how we can train those Networks so before we get there let me just revisit one more time the queue function illustration that we're looking at right queue function we are trying to build a neural network outputs these Q values one value per action and we determine the policy by looking over this state of Q values picking the value that has the highest and looking at its corresponding action now with policy networks the idea that we want to keep here is that instead of predicting the Q values themselves let's directly try to optimize this policy function here we're calling the policy function Pi of s right so Pi is the policy s is our state so it's a it's a function that takes as input only the state and it's going to directly output the action so the outputs here give us the desired action that we should take in any given state that we find ourselves in that represents not only the best action that we should take but let's denote this as basically the probability that selecting that action would result in a very desirable outcome for our Network so not necessarily the value of that that action but rather the probability that selecting that action would be the highest value right so you don't care exactly about what is the numerical value that selecting this action takes or gives rise to rather but rather what is the likelihood that selecting this action will give you the best performing value that you could expect exact value itself doesn't matter you only care about if selecting this action is going to give you with high likelihood the best one so we can see that if these predicted probabilities here right in this example of Atari right going left has the probability of being the highest value action with 90 percent staying in the center that's a probability of uh 10 going right is zero percent so ideally what our neural networks should do in this case is 90 of the time in this situation go to the left 10 of the time it could still try staying at where it is but never it should go to the right now note that this now is a probability distribution this is very different than a q function a q function has actually no uh structure right the Q values themselves can take any real number right but here the policy network has a very formulated output all of the numbers here in the output have to sum to one because this is a probability distribution right and that gives it a very rigorous version of how we can train this model that makes it a bit easier to train than Hue functions as well so one other very important advantage of having an output that is a probability distribution is actually going to tie back to this other issue of Q functions and Q neural networks that we saw before and that is the fact that Hue functions are naturally suited towards discrete action spaces now when we're looking at this policy Network we're outputting a distribution right and remember those distributions can also take continuous forms in fact we've seen this in the last two lectures right in the generative lecture we saw how vaes could be used to predict gaussian distributions over their latent space in the last lecture we also saw how we could learn to predict uncertainties which are continuous probability distributions using data and just like that we could also use this same formulation to move Beyond discrete action spaces like you can see here which are one possible action a probability Associates at one possible action in a discrete set of possible actions now we may have a space which is not what action should I take go left right or send Center but rather how quickly should I move and in what direction should I move right that is a continuous variable as opposed to a discrete variable and you could say that now the answer should look like this right moving very fast to the right versus very slow to the or excuse me very fast to the left versus very slow to the left has this continuous spectrum that we may want to model now when we plot this entire distribution of taking an action giving a state you can see basically a very simple illustration of that right here this this distribution has most of its mass over or sorry it has all of its mass over the entire real number line first of all it has most of its mass right in the optimal action space that we want to take so if we want to determine the best action to take we would simply take the mode of this distribution right the highest point that would be the speed at which we should move and the direction that we should move in if we wanted to also you know try out different things and explore our space we could sample from this distribution and still obtain some stochasticity now let's look at an example of how we can actually model these continuous distributions and actually we've already seen some examples of this in the previous two lectures like I mentioned but let's take a look specifically in the context of reinforcement learning and policy gradient learning so instead of predicting this probability of taking an action giving all possible states which in this case there is now an infinite number of because we're in the continuous domain we can't simply predict a single probability for every possible action because there is an infinite number of them so instead what if we parametrized our action Space by a distribution right so let's take for example the gaussian distribution to parametrize a gaussian distribution we only need two outputs right we need a mean and a variance given a mean and a variance we can actually have a probability mass and we can compute a probability over any possible action that we may want to take just from those two numbers so for example in this image here we may want to Output a gaussian that looks like this right its mean is centered at uh let's see negative 0.8 indicating that we should move basically left with a speed of 0.8 meters per second for example and again we can see that because this is a probability distribution because of the format of policy networks right we're enforcing that this is a probability distribution that means that the integral now of this of this outputs right by definition of it being a gaussian must also integrate to one okay great so now let's maybe take a look at how policy gradient networks can be trained and you know step through that process as well as we look at a very concrete example and maybe let's start by just revisiting this reinforcement learning Loop that we've started this class with now let's specifically consider the example of training an autonomous vehicle since I think that this is a particularly very intuitive example that we can walk through the agent here is the vehicle right the state could be obtained through many sensors that could be mounted on the vehicle itself so for example autonomous vehicles are typically equipped with sensors like cameras lidars Radars Etc all of these are giving observational inputs to the to the vehicle the action that we could take is a steering wheel angle this is not a discrete variable this is a continuous variable It's actually an angle that could take any real number and finally the reward in this very simplistic example is the distance that we travel before we crash okay so now let's take a look at how we could train a policy gradient neural network to solve this task of self-driving cars as a concrete example so we start by initializing our agents right remember that we have no training data right so we have to think about actually reinforcement learning is almost like a data acquisition plus learning pipeline combined together so the first part of that data acquisition pipeline is first to initialize our agent to go out and collect some data so we start our vehicle our agent and in the beginning of course it knows nothing about driving it's never it's been exposed to any of these rules of the environment or the observation before so it runs its policy which right now is untrained entirely until it terminates right until it goes outside of some bounds that we Define we measure basically the reward as the distance that we traveled before it terminated and we record all of the states all of the actions and the final reward that it obtained until that termination right this becomes our mini data set that we'll use for the first round of training let's take those data sets and now we'll do one step of training the first step of training that we'll do is to take excuse me to take the later half of our of our trajectory that our agent ran and decrease the probability of actions that resulted in low rewards now because the vehicle we know the vehicle terminated we can assume that all of the actions that occurred in the later half of this trajectory were probably not very good actions because they came very close to termination right so let's decrease the probability of all of those things happening again in the future and we'll take all of the things that happened in the beginning half of our training episode and we will increase their probabilities now again there's no reason why there shouldn't necessarily be a good action that we took in the first half of this trajectory and a bad action in the later half but it's simply because actions that are in the later half were closer to a failure and closer determination that we can assume for example that these were probably sub-optimal actions but it's very possible that these are noisy rewards as well because it's such a sparse signal it's very possible that you had some good actions at the end and you were actually trying to recover your car but you were just too late now repeat this process again re-initialize the agent one more time and run it until completion now the agent goes a bit farther right because you've decreased the probabilities at the ends increase the probabilities of the future and you keep repeating this over and over again until you notice that the agent learns to perform better and better every time until it finally converges and at the end the agent is able to basically follow Lanes usually swerving a bit side to side while it does that without crashing and this is actually really fascinating because this is a self-driving car that we never taught anything about what a lane marker means or what are the rules of the road anything about that right this was a car that learned entirely just by going out crashing a lot and you know trying to figure out what to do to not keep doing that in the future right and the remaining question is actually how we can update you know that policy as part of this algorithm that I'm showing you on the right on the left hand side right like how can we basically formulate that same algorithm and specifically the update equation steps four and five right here these are the two really important steps right of how we can use those two steps to train our policy and decrease the probability of bad events while promoting these likelihoods of all these good events so let's assume the let let's look at the loss function first of all the loss function for a policy gradient neural network uh looks like this and then we'll start by dissecting it to understand why this works the way it does so here we can see that the loss consists of two terms the first term is this term in green which is called the log likelihood of selecting a particular action the second term is something that all of you are very familiar with already this is simply the return at a specific time right so that's the expected return on rewards that you would get after this time point now let's assume that we got a lot of reward for a particular action that had a high log probability or a high probability right if we got a lot of reward for a particular action that had high probability that means that we want to increase that probability even further so we do it even more or even more likelihood we sample that action again into the future on the other hand if we selected or let's say if we obtained a reward that was very low for an action that had high likelihood we want the inverse effect right we never want to sample that action again in the future because it resulted in a low reward right and you'll notice that this loss function right here by including this negative we're going to minimize the likelihood of achieving any action that had love Awards in this trajectory now in our simplified example on the the car example all the things that had low rewards were exactly those actions that came closest to the termination part of the neural of the vehicle right all the things that had high rewards were the things that came in the beginning that's just the assumption that we make when defining our reward structure now we can plug this into the the loss of gradient descent algorithm to train our neural network when we see you know this policy gradient algorithm which you can see highlighted here this gradient is exactly of the policy part of the neural network that's the probability of selecting an action given a specific State and if you remember before when we defined you know what does it mean to be a policy function that's exactly what it means right given a particular state that you find yourself in what is the probability of selecting a particular action with the highest likelihood and that's you know exactly where this method gets its name from this policy gradient piece here that you can see here now I want to take maybe just a very brief second towards the end of the class here just to talk about you know some of the the challenges and keeping in line with the first lecture today some of the challenges of deploying these types of algorithms in the context of the real world right what do you think when you look at this training algorithm that you can see here right what do you think are the shortcomings of this training algorithm and which Step I guess specifically if we wanted to deploy this approach into reality yeah exactly so it's step two right if you wanted to do this in reality right that essentially means that you want to go out collect your car crashing it a bunch of times just to learn how to not crash it right and that's you know that's simply not feasible right number one it's also you know very dangerous number two um so there are ways around this right the number one way around this is that people try to train these types of models in simulation right simulation is very safe because you know if we're not going to actually be damaging anything real it's still very inefficient because we have to run these algorithms a bunch of times and crash them a bunch of times just learn how not to crash but at least now at least from a safety point of view it's much safer but you know the problem is that modern simulation engines for reinforcement learning and generally very broadly speaking modern simulators for vision specifically do not at all capture reality very accurately in fact uh there's a very famous notion called The Sim to real gap which is a gap that exists when you train algorithms in simulation and they don't extend to a lot of the phenomena that we see and the patterns that we see in reality and one really cool result that I want to just highlight here is that when we're training reinforcement learning algorithms we ultimately want them to be you know not operating in simulation we want them to be in reality and as part of our lab here at MIT we've been developing this very very cool brand new photorealistic simulation engine that goes beyond basically the Paradigm of how simulators work today which is basically defining a model of their environment and trying to you know synthesize that that model essentially these simulators are like glorified game engines right they all look very game-like when you look at them but one thing that we've done is taken a data-driven approach using real data of the real world can we build up synthetic environments that are super photorealistic and look like this right so this is a cool result that we created here at MIT developing this photorealistic simulation engine this is actually an autonomous agent not a real car driving through our virtual simulator in a bunch of different types of different scenarios so this simulator is called Vista it allows us to basically use real data that we do collect in the real world but then re-simulate those same real roads so for example let's say you take your car you drive out on Mass Ave you collect data of Mass Ave you can now drop a virtual agent into that same simulated environment observing new viewpoints of what that scene might have looked like from different types of perturbations or or types of angles that it might be exposed to and that allows us to train these agents now entirely using reinforcement learning no human labels but importantly allow them to be transferred into reality because there's no sim to real Gap anymore so in fact we we did exactly this we placed agents into our simulator we trained them using the exact algorithms that you learned about in today's lecture these policy gradient algorithms and all of the training was done entirely in simulation then we took these policies and we deployed them on board our full-scale autonomous vehicle this is now in the real world no longer in simulation and on the left hand side you can see basically this car driving through this environment completely autonomous in the real world no transfer learning is is done here there is no augmentation of data from Real World data this is entirely trained using simulation and this represented actually the first time ever that reinforcement learning was used to train a policy end to end for an autonomous vehicle that could be deployed in reality so that was something really cool that uh we we created here at MIT but now that we covered you know all of this foundations of reinforcement learning and policy learning I want to touch on some other maybe very exciting applications that we're seeing and one very popular application that a lot of people will tell you about and talk about is the game of Go so here reinforcement learning agents could be actually tried to put against the test against you know Grand Master Level go players and you know at the time achieved incredibly impressive results so for those of you who are not familiar with the game of go the go game of Go is played on a 19 by 19 board the rough objective of go is to claim basically more board pieces than your opponent right and through the grid of uh sorry through the grid that you can see here this 19 by 19 grid and while the game itself the The Logical rules are actually quite simple the number of possible action spaces and possible states that this board could be placed into is greater than the number of atoms in the universe right so this this game even though the rules are very simple in their logical definitions is an extraordinarily complex game for an artificial algorithm to try and master so the objective here was to build a reinforcement learning algorithm to master the game of Go not only beating you know these gold standard softwares but also what was at the time like an amazing result was to beat the Grand Master Level player so the number one player in the world of go was a human a human Champion obviously so Google deepmind Rose to this challenge they created a couple years ago developing this solution which is very much based in the exact same algorithms that you learned about in today's lecture combining both the value part of this network with residual layers which we'll cover in the next lecture tomorrow and using reinforcement learning pipeline they were able to defeat the grand champion human players and the idea at its core was actually very simple the first step is that you train a neural network to basically watch human level experts right so this is not using reinforcement learning this is using supervised learning using the techniques that we covered in lectures one two and three and from this first step the goal is to build like a policy that would imitate some of the rough patterns that a human type of player or a human Grandmaster would take based on a given board State the type of actions that they might execute but then given this pre-trained model essentially you could use it to bootstrap in reinforcement learning algorithm that would play against itself in order to learn how to improve even beyond the human levels right so it would take its human understandings try to imitate the humans first of all but then from that imitation they would pin these two neural networks against themselves play a game against themselves and the winners would be receiving a reward the losers would try to negate all of the actions that they may have acquired from their human counterparts and try to actually learn new types of rules and new types of actions basically that might be very beneficial to achieving superhuman performance and one of the very important auxiliary tricks that brought this idea to be possible was the usage of this second Network this auxiliary Network which took as input the state of the board and tried to predict you know what are all of the different possible uh board states that might emerge from this particular State and what would their values be what would their potential returns and their outcomes be so this network was an auxiliary Network that was almost hallucinating right different board states that it could take from this particular State and using those predicted values to guide its planning of you know what action should it take into the future and finally very much more recently they extended this algorithm and showed that they could not even use the human Grand Masters in the beginning to imitate from in the beginning and bootstrap these algorithms what if they just started entirely from scratch and just had two neural networks never trained before they started pinning themselves against each other and you could actually see that you could without any human supervision at all have a neural network learn to not only outperform the solution that or outperform the humans but also outperform the solution that was created which was bootstrapped by humans as well so with that I'll summarized very quickly what we've learned today and and conclude for the day so we've talked a lot about really the foundational algorithms underlying reinforcement learning we saw two different types of reinforcement learning approaches of how we could optimize these Solutions first being Q learning where we're trying to actually estimate given a state you know what is the value that we might expect for any possible action and the second way was to take a much more end-to-end approach and say how given a state that we see ourselves in what is the likelihood that I should take any given action to maximize the potential that I I have in this particular State and I hope that all of this was very exciting to you today we have a very exciting lab and kickoff for the competition and the deadline for these competitions will be well it was originally set to be Thursday which is uh tomorrow at 11 pm thank you foreign 

foreign this next lecture is my absolute favorite lecture in introduction to deep learning focusing on the limitations of deep learning methods as they exist today and how those limitations and outstanding challenges really motivate new research at The Cutting Edge and the New Frontiers of deep learning and AI before we dive in we have a couple of logistical things to to discuss and and go through starting with perhaps one of the most important aspects of this of this course we have a tradition of Designing and giving t-shirts as part of this course and hopefully you can all see them we have them here today right at the front so we're going to do the distribution of the t-shirts at the end of today's program and so please please stay if you wish to pick up a t-shirt all right so where we are right now we have this lecture that I'm going to be giving on deep learning limitations in New Frontiers and we're going to have three more lectures following that continuing our series of guest lectures for this year importantly we still have the competitions surrounding both the software lab and the project pitch proposal for the project pitch proposal we ask that you please upload your slides by tonight we have all the detailed instructions for that on the course syllabus and if you are interested in submitting to the lab competitions the deadline for that has been extended to tomorrow afternoon at 1 pm so please submit the labs for the labs as well motivation hopefully is not only to is mostly to exercise your skills in deep learning and to build knowledge but we also have these amazing competitions and prizes for each of the labs as well as the project pitch proposal competition so to remind you for the first lab on designing neural networks for music generation we have audio related prizes that are up up in the air the competition is wide open it can be anyone's game so please please please submit your entries tomorrow we'll have the project pitch proposal competition every year it's a highlight of this program we hear your amazing ideas about new deep learning algorithms applications in a quick Shark Tank style three minute pitch it's really fun not only to be up here and have the chance to present to everyone but also to hear the Amazing Ideas of your course mates and your peers and colleagues again some of the logistical information for how to submit your slides up to the Google slide deck are included on the syllabus and finally as we introduced in the first lecture and hopefully you've realized we have a exciting grand prize competition surrounding the lab on trustworthy deep learning robustness uncertainty and bias and again emphasizing the competition is wide open right now please submit your entries should be very very exciting and we look forward to receiving your submissions foreign okay so in addition to that those technical components of the course we have three remaining guest lectures to round out our lecture series we heard an amazing talk Yesterday by Saturn from themus AI on robust and trustworthy deep learning today we're going to hear from Ramin Hassani from Vanguard who's going to talk about the new age of statistics and what that can mean for deep learning algorithms tomorrow we'll have two awesome guest lectures from dilip Krishnan from Google and to round it out from Daniela Roos the director of mitc sale herself yes some we know that there are many fans of Danielle in the audience us included so please attend these should be really awesome um talks and you'll get to hear more about The Cutting Edge of research in deep learning okay so that rounds out all the logistical and program announcements that I wanted to make now we can really dive into the fun stuff the technical content for this class so so far an introduction to deep learning you've learned about the foundations of neural network algorithms and also gotten a taste for how deep learning has already started to make an impact across many different research areas and applications from advances in autonomous vehicles to think about to thinking about applications in medicine and health care to reinforcement learning that is changing the way that we think about games and play to new generative modeling advances to robotics to many many other applications like natural language processing Finance security and more what we really hope you come away with from this course is a concrete understanding of how deep neural networks work and how these foundational algorithms are really enabling these advances across this multitude of disciplines and you've seen in this class and in this program that we've dealt with neural networks as a way as an algorithmic way to think about going from input data in the form of signals or measurements that we can derive from sensors of our world to directly produce some sort of decision that could be a prediction like a class label or a numerical value or it could be an action itself like in the case of reinforcement learning we've also seen the inverse where we can think about now building neural network algorithms that can go from a desired prediction or a desired action to try to generate new data instances as is the case with generative modeling now taking a step back right in both these cases whether we're going from data to decision or the reverse neural networks can be thought of as very powerful function approximators what that means and to get at this in more detail we have to go back to a really famous theorem in computer science and in the theory of neural networks that was this theorem that was presented in 1989 and it generated quite the stir in the field this theorem is called the universal approximation theorem and what it states is that if we start with a neural network with just a single hidden layer that single layer neural network is sufficient to approximate any function now in this class we've been thinking about not single layer neural networks but rather deep models that stack multiple of these hidden layers together but this theorem says oh you don't even need that all you need is a single layer and if you believe that any problem can be simply reduced to this idea of mapping an input to an output through some function that neural network can exist the universal approximate approximation theorem states that there is some neural network with a sufficient number of neurons that can approximate any arbitrary function now this is an incredibly powerful result but if we look more closely at this and dig a little further you can start to see that there are a few caveats that we have to keep in mind first of all this theorem makes no guarantees on how many units how many neurons in that layer you need to solve such a problem furthermore it says okay how do it doesn't answer the question of how we can actually Define that neural network how do we find the ways to support that architecture all it claims is that such a neural network exists but we know that using gradient descent and some of the tools that we learned about in this class that finding those weights is not always a straightforward problem it can be very complex very non-linear very non-convex and thinking about how we actually achieve that training of the neural network is a very very hard problem secondly what is really important about this theorem is that it does not make any guarantees on how well that Network would generalize to new tasks all it says is that given this desired mapping from input to Output we can find some neural network that exists no guarantees on its performance in other tasks or in other settings and I think that this theorem this idea of the universal approximation theorem and its underlying caveats is a perfect example of what can be thought of as the possible effects of the overhype in artificial intelligence and in deep learning and now here you've become part of this community right that's interested in advancing the state of deep learning and AI and I think collectively we need to be extremely careful in how we think about these algorithms how we Market them how we advertise them how we use them in the problems that we care about and while Universal approximation tells us and that neural networks can be very very powerful and generates a lot of excitement around this idea at a time historically it actually provided false hope to the computer science and AI community that neural networks could solve any problem of any complexity in the real world now I think it goes without saying that such over hype can be extremely dangerous and in fact it's not only the effect is not only in the course of research but also potentially at Society at Large so this is why that today for the rest of this lecture I really want to focus on some of the limitations of deep learning algorithms that you've learned about in this over the course of this class and I want to not only stop there I want to extend beyond that to see how these limitations motivate new opportunities for new research that can be aimed at addressing some of those problems and overcoming them so to start one of my favorite examples of a potential danger of deep neural networks come from this paper comes from this paper that says understanding deep neural networks requires rethinking generalization this paper proposes a really elegant and simple experiment that highlights this notion of generalization and its limitations with deep learning models so what they do in this paper is that they took images from this large data set called imagenet and each of these images is associated with a particular label dog banana dog tree as you can see here then what the what the authors of this paper did was that they considered each of these image examples and for every image in the data they took a k-sided die where K is the number of possible class labels in this data set and use that die to randomly assign new labels to each of these instances now all of the labels have been completely scrambled right they're doing this random sampling assigning these brand new labels to these images and what this means is that the labels that are associated with an image as you see are completely random with respect to what is actually in that image then what they did and if you know here is you can have instances because you have overlaps of multiple instances corresponding to the same class but now when you introduce Randomness you can have two instances of the same class that now end up with completely different labels dog here maps to banana in one case and two Tree in another case so literally they're completely completely randomizing the labels entirely with that in hand what they did was they tried to fit a deep neural network model to the sampled data from this data set imagenet and they vary the degree of Randomness ranging from the preserved original labels to completely random and then they took the resulting model looked at its performance on the test set an independent data set where now we're given new images and the task of the network is to predict the associated label and as you can expect the accuracy of the model on this independent test set decreases as you introduce more and more Randomness into the label process what was really interesting however was what they saw when they now looked not at the test set but at the training set and this is what they found that no matter how much they randomized these labels the neural network model when trained on that resulting data was able to get 100 accuracy on the training set what this means is that these neural network models can really do this perfect fitting this very powerful function approximation and I think that this is a very powerful example because it shows once again like the universal approximation theorem that deep neural Nets can perfectly fit to any function even if that function is defined by entirely random labels now you'll note that this difference between the training set performance and the test set performance captures this idea of generalization what is the limit of the neural network with respect to function fitting and how it actually performs on unseen data and to drive this point home even further again we can really understand neural networks simply as functional approximators and what the universal approximation theorem is telling us is that neural networks are just really really really good at doing this job so if we consider this example here where I've shown these data points in this 2D space we can always train a neural network to learn what is the most likely position of the data within this space within this realm that it has seen examples before such that if we get an if we give the model a new data point here in purple we can expect that the neural network is going to generate a reasonable prediction of the estimate for that data point now the challenge becomes what if you now start to extend beyond this landscape beyond the region that you have information that you have data examples well we have no guarantees on what the training data looks like in these regions and this is in fact one of the large limitations that exist in modern deep neural networks that they're very very effective at functional approximation in regions when we have training data but we can't make guarantees on their performance out of those regions and so this raises this question which again ties back to sada's lecture yesterday of how can we derive methods that tell us when the network doesn't know when it needs more information needs more examples building off this idea a little further I think there's often this common conception which can indeed be inflated by the media that deep learning is basically this magic solution Alchemy it can be the be-all and all solution to any problem but this spawns the belief that if we take some data examples and apply some training architecture to it train the resulting Model turn the crank on the Deep learning algorithm that that will spit out some beautiful excellent results solving our problem but this is simply not how deep Learning Works there's this common idea of garbage in garbage out if your data is noisy if you have insufficient data if you try to build this very large neural network model to operate on the data you're not going to guarantee performant results at the outset and this motivates one of the most pertinent failure modes of modern deep neural Nets and it highlights just how much they depend on the underlying data that they're trained with so let's say we have this image of a dog and we're going to pass it into a CNN architecture and our task is now to try to colorize this image black and white image of a dog and produce a colored output this can be the result look closely at this resulting image anyone notice anything unusual about the dog yes the ear is green awesome observation anything else yes yeah the chin is pink or purple and so two different instances pointed out by two different people of things that don't really align why could this be the case in particular if we look at the data that the that this model was trained with amongst the images of the dogs many of those images are going to be probably of the dogs sticking their tongues out or having some grass in the background and as a result the CNN model may have mapped that region around the chin to be pink in color or around the ears to be green in color and what this example really highlights in thinking about this contrast between what the training data looks like and what the predictive outputs can be is that deep learning models all they're doing is that they're building up this representation based on the the data that they have seen over the course of training and that raises this question of how do neural networks effectively handle those instances where they may not have seen enough information they may be highly uncertain and exactly as sadhana motivated in yesterday's lecture and laid out beautifully This is highly relevant to real world safety critical scenarios for example in the case of autonomous driving where cars that are on autopilot can or in autonomous mode can end up crashing with the results often being fatal or having very significant significant consequences in this particular instance to really highlight this further from a few years ago there was the case of a autonomous vehicle that resulted in a fatal accident where it it crashed into a pylon a construction pylon that was present on the road and it turned out that in when they looked back at the data that was used to train the resulting neural network that was used to control the car that in those training examples the Google street view images of that particular region of the road did not have those construction barriers and construction pylons that resulted in the car crashing in the end and so again this idea of how disparities and issues with the training data can lead to these Downstream consequences is a really prominent failure mode with neural network systems and it's exactly these types of failure modes that motivated the need for understanding uncertainty in neural network systems and this was what you learned about in yesterday's lecture and hopefully have gotten in-depth experience with through the software lab highlighting the importance of developing robust methods to understand these metrics of uncertainty and risk and how they can be really important for safety critical applications from autonomy to medicine to facial recognition as you're exploring and how these Downstream consequences are linked fundamentally to issues of data imbalance feature imbalance and Noise so overall I think that what these instances and these considerations point us to is the susceptibilities of neural networks to succumb to these failure modes the final failure mode that I'd like to consider and highlight is this idea of adversarial examples the intuition and the key idea here is that we can take a data instance for example an image which if deployed and inputted into a standard CNN is going to be predicted to contain a temple with 97 probability what we can do is now apply some tiny perturbation in the form of random noise Choice random noise to that original image that then results in a perturbed image which to us humans appears visually largely the same but if you pass that resulting image back to that same neural network it produces a prediction that completely does not align with what is actually in the image predicting ostrich label of ostrich which with 98 probability and this is this notion of an adversarial attack or an adversarial perturbation what exactly is this perturbation doing remember that when we train neural networks using gradient descent the task is to optimize or minimize some loss function an objective and the goal in standard gradient descent is to say okay how can I change the weights of the neural network to decrease the loss to optimize that objective we are specifically concerned with changing those weights W in a way to minimize our loss if you look closer here we're considering only the changes of the weights with respect to the input data and the corresponding label X and Y in contrast in thinking about adversarial attacks we now ask a different question how can we modify that input data for example that image in order to increase the error in the Network's prediction concretely how does a small change a tiny perturbation in the input data X result in a maximal increase in the loss what that means is that we can fix the weights keep the network the same and look at how we can perturb and change and manipulate that input data to try to increase that loss function an extension of this idea was developed and presented by a group of students right here at MIT where they took this idea of adversarial perturbation and created an algorithm that could synthesize not only adversarial realizations in 2D but actually in 3D using a set of different Transformations like rotations color changes other types of perturbations then using those learned perturbations they took those information and actually physically synthesized objects using 3D printing that were designed to be adversarial examples for a neural network and so they printed 3D printed a bunch of these examples of turtles 3D physical objects such that images of those would be completely fooling a neural network that looked at that image and tried to classify the correct label and so this shows that this notion of adversarial examples and perturbation can extend to different domains in the real world where now we can think about constructing these synthetic examples that are designed to probe at the weaknesses of neural networks and expose their vulnerabilities finally and we've discussed a lot about this idea uh yesterday as well is this notion of algorithmic bias the fact that as AI systems become more broadly deployed in society that they're susceptible to significant biases resulting from many of those issues that I highlighted earlier and these can lead to very real detrimental consequences as we've been exploring throughout this course in both labs and lectures so the examples that I covered so far are certainly not a exhaustive list of all the limitations of neural networks but I'd like to think of these not strictly as limitations but rather an invitation and a motivation for new innovation Creative Solutions that are aimed at trying to tackle some of these questions which are indeed open problems in Ai and deep learning research today specifically we've tackled and already thought about ways of by to address issues of bias and uncertainty in neural networks and now for the remainder of this talk I want to focus on some of the really exciting New Frontiers of deep learning tackling some of these additional limitations that are introduced the first being this idea that stems from this idea of the size and scale of neural networks they rely heavily on data they're massive and as a result it can be difficult to understand what is the underlying structure if any that the neural network is picking up on and so there's a very very exciting line of work now that is focused on introducing structure and more information from prior human knowledge into neural network architectures to enable them to become much smaller more compact more expressive more efficient and we're going to see a bit of this in today's lecture but also in our following guest lecture by Ramin where he will talk about this idea of structural encoding and how that can result in highly expressive and efficient neural network architectures the second issue that I'm going to talk about and discuss and is very related to this uh problem of scale and structure is how we can encourage neural networks to now extrapolate more effectively Beyond data and this will tie very neatly into some of the ideas we've been discussing around generative Ai and will be the last topic that I'm going to discuss in the remainder of today's lecture okay so let's focus on this first first concept of how we can leverage more of our human domain knowledge to encode greater structure into deep learning algorithms turns out we've already seen examples of this in the course so far particularly highlighted in cnns used for spatial processing and visual data as we saw cnns were introduced to be a highly efficient architecture for capturing spatial dependencies in data and the core idea behind CNN's was this notion of convolution how it would relate spatially similar and adjacent portions of an input image to each other through this efficient convolution operation and we saw that convolution was able to be effective at extracting local features from visual data and then using this local feature extraction to then enable Downstream predictive tasks like classification object detection and so forth what about now instead of images if we consider more irregular data structures that can be encoded in different ways Beyond 2D grids of pixels graphs are a particularly powerful way of encoding structural information and it can be in many cases that the notion of a graph or a network is very important to the problem that we may be considering graphs as a structure and as an example for representing data are really all around us across many different applications ranging from social networks defining how we are all connected to each other to State machines that describe how a process can transition from one state to another to metrics and models of human Mobility Urban transport to chemical molecules and biological Networks and the motivation across all these examples is that so many instances of real world data fall into naturally this idea of a network or graph structure but that this structure cannot be readily incorporated or captured by standard data encodings or geometries and so we're going to talk a little bit about graphs as a structure and how we can try to represent this information encode it in a graph to build neural networks that can capable are capable of handling these data types to see how we can do this and and approach this problem let's go back for a moment to the CNN in cnns we saw that we have this rectangular 2D kernel and what it effectively does is it slides across the image paying attention and picking up to features that exist across this 2D grid and all this function is doing is that element wise multiplication that defines the convolution operation and intuitively we can think about convolution as this visualization of sliding this kernel iteratively patch by patch across the image continuing on to try to pick up on informative features that are captured in this 2D pixel space this core idea of convolution and sliding this this feature extraction filter is at the heart of now how we can extend this idea to graph based data this motivates a very recent type of neural network architecture called graph convolutional networks where the idea is very similar to standard cnns we have some type of weight kernel and all it is is a matrix defined by neural network weights as before and now rather than sliding that kernel across a 2d grid of pixels the kernel goes around to different parts of the graph defined by particular nodes and it looks at what the neighbors of the of that node is how it's connected to adjacent nodes and the kernel is used to extract features from that local neighborhood within the graph to then capture the relevant information within the structure so we can visualize this concretely here where now instead of seeing the 2D kernel looking at a particular patch of an image what I'm highlighting is how the kernel is paying attention to node and its local Neighbors and effectively the graph convolution operation what it's doing is picking up on the local connectivity of a particular node and learning weights that are associated with those edges defining that local connectivity the kernel is just then applied iteratively to each node in the graph trying to extract information about the local connectivity such that we can go around and around and at the end start to bring all this information together in aggregation to then encode and update the weights according to what the local observations were the key idea is very very similar to The Standard convolution operation where this weight kernel all it's doing is a feature extraction procedure that's now defined by nodes and edges in this graph rather than a 2d grid of pixels in an image this idea of graph encoding and graph convolution is very exciting and very powerful in terms of now how we can extend this to real world settings that are defined by data sets that naturally lend themselves to this graph like structure so to highlight some of those applications across a variety of domains the first example I'd like to draw attention to is in the chemical and biological sciences where now if we think about the structure of a molecule right it's defined by atoms and those atoms are individually connected to each other according to molecular bonds and it turns out that we can design graph neural networks that can effectively build representations of molecules in the same operation of this graph convolution to build up an informative representation of chemical structures this very same idea of graph convolution was recently applied to the problem of drug Discovery and in fact out of work here at MIT it turned out that we can Define graph neural network architectures that can look at data of small molecule drugs and then look at new data sets to try to predict and discover new therapeutic compounds that have potent activity such as killing bacteria and functioning as antibiotics Beyond this example another recent application of graph neural networks has been in traffic prediction where the goal is to look at streets and intersections as nodes and edges defined by a graph and apply a graph neural network to that structure and to patterns of Mobility to learn to predict what resulting traffic patterns could be and indeed in work from Google and deepmind this same modeling approach was able to result in significant improvements in the prediction of ETA estimated time of arrival in Google Maps so when you're looking at Google Maps on your phone and predicting when you're going to arrive at some location what it's doing on the back end is applying this graph neural network architecture to look at data of traffic and make more informative predictions about how those patterns are changing over time a final and another recent example was in forecasting the spread of covid-19 where again building off of this same idea of Mobility patterns and contacts graph neural networks were employed to look at not only spatial distributions of covid spread but also incorporate a temporal Dimension so that researchers could effectively forecast what were the most likely areas to be affected by covid-19 and what time scales those effects were likely to occur on so hopefully this this highlights this idea of how simple ideas about data structure can then be translated into new neural network architectures that are specifically designed for particular problems in relevant application areas and this also lends very naturally to how we can extend not Beyond 2D data to graphs to now three-dimensional data which is often referred to as 3D Point clouds and these are unordered sets of data and you can think about them as being in scattered in 3D space and it turns out that you can extend graph neural networks very naturally to operate on 3D data sets like Point clouds where the core idea is that you can dynamically compute meshes present in this three-dimensional space using the same idea of graph convolution and graph neural networks okay so hopefully with that run through of graph structure graph neural networks you've started to get a little bit of idea about how inspection of the underlying structure of data and incorporation of our prior knowledge can be used to inform new neural network architectures that are very well suited for particular tasks on that I want to spend the remaining of time of this lecture on our second New Frontier focusing on generative AI so as we first introduced in the first lecture of this course and throughout I think that today we're really at this inflection point in AI where we're seeing tremendous new capabilities with generative models in particular enabling new advances in a variety of fields and I think we're just at the at the cusp of this inflection point in that in the coming years we're going to see generative AI radically transform the landscape of our world the landscape of our society so today in the remaining portion of the New Frontiers lecture we're going to focus specifically on a new class of generative models diffusion models that are powering some of these latest advances in generative AI all right to start let's think back to the lecture on generative modeling where we focus primarily on two classes of generative models vas and Gans what we didn't have time to go into into depth was what are some of the underlying limitations of these models vaes and Gans turns out that there are three primary limitations the first is what we call mode collapse meaning that in the generative process va's and Gans can kind of collapse down to this mode this phase where they're generating a lot of predictions a lot of new samples that are very very similar to each other we often kind of think about this as regression to the average value or the most common value the second key limitation as I kind of alluded to in our generative modeling lecture was that these models really struggle to generate radically new instances that are not similar to the training data that are more diverse finally it turns out that Gans in particular NBA's as well in practice can be very difficult to train they're unstable inefficient and this leads to a lot of problems in practice when thinking about how to scale these models these limitations then motivate a concrete set of challenges or criteria that we really want to meet when thinking about generative models we want our generative model to be stable efficient to train we wanted to generate high quality samples that are synthetic and novel diverse different from what the model has seen before in its training examples today and tomorrow we're going to focus on two very very exciting new classes of generative models that tackle these challenges head on today I'm going to specifically focus on diffusion models provide the intuition behind how they work where you may have seen them before and what are some of the advances that they're enabling and tomorrow in the guest lecture from Google we're going to hear from dilip on another generative modeling approach that's focused specifically on this task of text to image generation okay so let's get into it four diffusion models I think it first helps us to compare back again to the models we've seen the architectures we've seen and know about with va's and Gans that we talked about in lecture four the task is to generate examples synthetic examples let's say an image in a single shot by taking some compressed or noisy latent space and then trying to decode or generate from that to produce now a new instance in our original data space diffusion models work fundamentally differently from this approach rather than doing this One-Shot prediction what diffusion models do is that they generate new samples iteratively by starting from purely random noise and learning a process that can iteratively remove small increments of noise from that complete random State all the way up back to being able to generate a synthetic example in the original data landscape that we started off with in caring about the intuition here is really clever and really I think powerful in that what diffusion models allow us to do is to capture maximum variability maximum amount of information by starting from a completely random state diffusion models can be broken down into two key aspects the first is what we call the forward noising process and at the core of this is this idea of how we build up data for the diffusion model to look at and then learn how to denoise and generate from the key step here is that we start with training data let's say examples of images and over the course of this forward noising diffusion process what we do is we progressively add increasing increments of noise such that we are slowly wiping out the details in the image corrupting the information destroying that information until we arrive at a state that is Pure Noise then what we actually build our neural network to do is to learn a mapping that goes from that completely noise State back up to the original data space what we call the reverse process learning and mapping that denoises going from noise back up to data and the core idea here is that denoising is iteratively recovering back more and more information from a completely random state these are the two core components the forward noising process the reverse denoising process and we're going to look at how each one works distilled down to the core intuition underlying this is the forward noising the first step where we're given an image and we want to arrive at a random sample of noise importantly this process does not require any neural network learning any training what we are able to do is Define a fixed way a determined way to go from that input image to the noise the core idea is really simple all we do is we have some noising function and starting at the first time step our initial time step t0 where we have 100 image no noise added all we do is progressively add more and more noise in an iterative way over the course of these individual time steps such that the result is increasingly more noisy first we go from all image less image less image more noise more noise all noise in this defined forward process no training no learning all we have is a noising function now this gives us a bunch of examples right if we have a bunch of instances of instances in the data set we apply this noising to every one of them and so we have those slices at each of these different noising time steps our goal now in learning the neural network is to then learn how to denoise in this reverse process starting from data in the input space what the noising process led us to is this time series of these iteratively more noised examples now our task is given an image can we given a given one of these slices at these time steps let's say t all we're asking is can we learn the next less next most denoised example from that time step making this more concrete walk through it a particular image add a slice let's call it t what we ask the neural network to learn is what is the estimated image at that prior step T minus one making this very concrete let's say we have this noised image at time three the neural network is then trying to predict what was the denoise result at just one step before comparing these two let's take a step back all these two images differ by is that noise function and so the question that's posed to the neural network in the course of training is how can we train how can we learn this difference if our goal is to learn this iterative denoising process and we have all these consecutive steps going from more noise to less noise I pose the question to you all how could you think about defining a loss defining a training objective for the neural network to learn any ideas yes can you expand a little further on that it gets so the idea was can we look at the same concept of trying to encode information from an image to maybe a reduced latent space and try to learn that it's getting it's getting that's um very related to what the network is actually doing but something even simpler thinking about what how can we just compare these two images yes simple just think really simply yes exactly the idea was how many of the pixels are the same all we need to do it turns out is look at the pixel wise difference between these two steps and the cleverness behind diffusion models is defining that exact uh residual noise as what defines the loss the training objective for such a model all you do is compare these consecutive time steps of iterative denoi of iterative noising and ask what does that mean squared error what is that pixel wise difference between those two and it turns out that this works very effectively in practice that we can train a neural network to do this iterative denoising by this very um intuitive idea of the loss all right so hopefully this gives you the space of how the diffusion model builds up this understanding of the denoising process and is able to go in learning to iteratively denoise examples now the task is how can we actually sample something brand new how can we generate a new instance well it's very related to that exact reverse process the denoising process that we just walked through what we do is we take a brand new instance of completely random noise and do take our trained diffusion model and ask it okay just predict that residual noise difference that will get us to something that is slightly less noisy and this is all that is done repeatedly at these iterative time steps such that we can go from Pure Noise to something less noisy repeatedly and as this process occurs hopefully you can see the emergence of something that reflects image related content time step by time step iteratively doing this sampling procedure over many generations and many time steps such that we can get back to the point of this dog that's peeking out through us what I want you to take away with is that what the diffusion model enables is that going from a completely random noise state where we have this maximum notion of variability by defining it and breaking it down to this iterative process at each of those steps there is a prediction that's made there's a generation that's made and so that maximum variability is translated into maximum diversity in the generated samples such that when we get to the end State we have arrived at an image a synthetic sample that's back in our completely Noise free data space so at its core this is the summary of how a diffusion model works and how they're able to go from completely random noise to very faithful diverse newly generated samples as I've kind of been alluding to what I think is so powerful about this approach and this idea is the fact that we can start from complete randomness it encapsulates maximum variability such that now diffusion models are able to produce generated samples that are very very diverse from noise samples that are fundamentally different to each other what this means is now in this one instance we go from some noise sample to this image but equivalent equivalently and what is striking is if we consider now a completely different instance of random noise they're very variable from each other right random noise random noise maximum variability internally and in comparison to each other the result as a result of that denoising generation is going to be a different sampled image that's highly diverse highly different to what we saw before and that's really the power of diffusion models in thinking about how we can generate new samples that are very different and diverse quick question yes that's a fantastic question so the question was how does the model know when to stop when you build these models you define a set number of time steps and this is a parameter that you can play around with when building and training the diffusion model and there are different studies looking at what works and and what doesn't work but the core ideas by more time steps you're going to result in better resolution effectively of generations but it's a trade-off as well in terms of the stability of the model and how efficiently it can train and learn you know it sounds like an earth so so the question was if explaining why there could be some missing details or incorrect details in the results of the diffusion models and the particular example was hands in particular I don't know if there's a specific reason for why hands seem to to cause issues I personally haven't seen reports or examples or literature discussing that but I think um the the general point is yes there is there are going to be imperfections right it's not going to be 100 percent perfect or accurate in terms of uh faithfulness to the to the true example um and I think a lot of advances are now like thinking about what are modifications to the underlying architecture that could try to alleviate some of those details or those issues but for the sake of lecture I can discuss further with you about that particular example afterwards as well okay so so yeah indeed right I think that the power and um the power of these models is is very very significant and going back right to the example that we showed at lecture one right this ability to generate now a synthetic image from a language prompt photo of an astronaut riding a horse in fact it is a diffusion model that is underlying this uh this approach and what the diffusion model is doing is it's able to take a embedding that translates between text and language and then run a diffusion process between the two such that image generation can be guided by the particular language prompt more examples of this idea of text to image generation have I think really taken the internet kind of by storm but I think what is so powerful about this idea is that we can guide the generative process according to constraints that we specify through language which is very very powerful everything from something that's highly photorealistic or generated with the specific architect artistic style as in the examples that I'm showing here so so far in both today's portion on General on diffusion models and our discussion of generative models more broadly we've largely focused on examples in images but what about other data modalities other applications can we design new generative models leveraging these ideas to design new synthetic instances in other real world application areas to me and I'm definitely biased in saying this but one of the most exciting aspects about thinking about this idea is in the context of molecular design and how we can relate to chemistry the life sciences and environmental science as well so for example in the in the landscape of chemistry and molec and small molecules now instead of thinking about images and pixels we can think about atoms and molecules in three-dimensional space and there's been a lot of recent work in building now diffusion models that can look at the 3D coordinates of atoms defining a molecule and go from a completely random state in that 3D space to again perform this same iterative denoising procedure to now generate molecular structures that are well defined and could be used for applications in drug Discovery or therapeutic design beyond that and in my research specifically we are building new diffusion models that can generate biological molecules or biological sequences like those of proteins which are the actuators and executors of all of biological function in human life and across all the domains of life and specifically in my work and in my research team we've developed a new diffusion model that is capable of generating brand new protein structures I'm going to share a little bit about how that model works and our core idea behind it to kind of close out this section the motivation that is really driving this work across the field at large is this goal of trying to design new biological entities like proteins that could have therapeutic functions or expand the functional space of biology at Large and there are many many efforts across generative AI that are now aiming at this problem because of the tremendous potential impact that it can have so in our work in specific we considered this problem of protein design by going back to the biology and drawing inspiration from it and if you consider how protein function is determined and encoded all it is distilled down to is the structure of that protein in three-dimensional space and how that structure informs and encodes a particular biological function in turn proteins don't always start out adopting a particular 3D structure they go through this process of what we call protein folding that defines how the chain of atoms in the protein wiggle around in three-dimensional space to adopt a final defined 3D structure that is highly specific and highly linked to a particular biological function so we asked and we looked at this question of protein folding and how protein folding leads to structure to inspire a new diffusion model approach for this protein structure design problem where we said okay if a protein is in a completely unfolded State you can think about that as equivalent to the random noise state in an image it's in a completely floppy configuration it doesn't have a defined structure it's unfolded and what we did is we took that notion of protein protein structure being unfolded as the noisy state for a diffusion model and we designed a diffusion model that we trained to go from that unfolded state of the protein to then produce a prediction a generated prediction about a new structure that would define a specific 3D structure and we can visualize how this algorithm works and we call it folding diffusion in this video that I'm going to show here where at the start in this initial frame we have a random noise protein chain completely random configuration uh unfolded and and unfolded in 3D space now what it's going to show this video is the iterative denoising steps that go from that noise random state to the final generated structure and as you can see right it's this process very similar to the concept that I introduce with images where we're trying to go from something noisy to something structured arriving now at a final generated protein structure so our work was really focused on introducing a new and foundational algorithm for this protein structure design problem via this via diffusion models but it turns out that in just um the very short time from when we and others introduced these algorithms that it precipitated really a rise in scaling and extending these diffusion models for very very specific controlled programmable protein design where now we are seeing large-scale efforts that use these diffusion model algorithms to generate protein designs and actually realize them experimentally in the physical world so in this image I'm showing on the left the colored image is the pre is the generated output by the diffusion model and the black and white image is what if you take that result synthesize it in the biological lab and look and take a picture of the resulting protein and assess its structure we see that there is a strong correspondence meaning that we're at the ability to leverage these powerful diffusion models to now generate new proteins that can be realized in the physical world it doesn't stop there we can now think about designing proteins for a very specific therapeutic applications so for example in this visualization this is designed as a novel protein binder that's designed to block to bind to and block the activity of the covid spike receptor so what you're seeing again is that visualization of the diffusion process and arriving at a final design that binds to and blocks the top of the covet Spike receptor so hopefully you know this kind of paints a bit of the landscape of where we are today with generative AI and I think that this example in in protein science and in biology more broadly really highlights the fact that generative AI is now at the ability to enable truly impactful applications not just for creating cool images or what we can think of as AI synthetic art generative AI is now enabling us to think about designed solutions for real world problems and I think there are so many open questions in understanding the capabilities the limitations the applications of generative AI approaches and how they can Empower our society so to conclude I want to end on a higher level thought and that's the fact that this idea of generative design kind of raises this higher level concept of what it means to understand and I think it's captured perfectly in this quote by the physicist Richard Feynman who said what I cannot create I cannot understand in order to create something we as humans really have to understand how it works and conversely in order to understand something fully I'd argue that we'd have to create it engineer design generative Ai and generative intelligence is at the core of this idea of what I think about as concept learning being able to design a distill a very complex problem down to its core roots and then build up from that Foundation to design and to create when this course began on on Monday Alexander introduced the idea of what it means to be intelligent Loosely speaking right the ability to take information use it to inform a future decision human learning is not only restricted to the ability to solve specific distinct tasks it's foundational and founded by the idea of being able to understand Concepts and using those Concepts to be creative to imagine to inspire to dream I think now we're at the point where we have to really think about what this notion of intelligence really means and how it relates to the connections and distinctions between artificial intelligence and what we as humans can do and create so with that I'm going to leave you with that and I hope that inspires you to think further on what we've talked about in the course and intelligence AI deep deep learning more broadly so thank you very much for for your attention we're going to take a very brief pause since where I know we're running a little bit late and then have our fantastic lecture from Rami next thank you so much [Applause] 

foreign for the invite so yeah I'm I'm a research scientist at Google research just across the street and I want to talk today about a paper that we just put up on archives which is a new model for text to image generation via must generative Transformers and as you can guess from these figures that the name of the model is Muse so this is a work that I've done with an awesome set of colleagues at Google research the paper and a web pages online so many of you must be familiar with text to image generation which has uh really advanced in the last year or two and a question might be why texture image generation I think it's because text is a very natural control mechanism for Generation we are able to express our thoughts creative ideas through the use of text and then that allows non-experts non-artists to generate compelling images and then be able to iterate them through editing tools to create your own personal art and ideas uh also very importantly deep learning requires lots of data and it is much more feasible to collect large-scale paired image text Data an example is the lion 5 billion data set on Which models such as stable diffusion have been trained so we should recognize that various biases exist in these data sets and bias mitigation is an important research problem and lastly these models can exploit uh pre-trained large language models which are very powerful and they allow for extremely fine-grained understanding of text parts of speech you know nouns verbs adjectives and then be able to translate those semantic Concepts to Output images and these llms importantly can be pre-trained on various text tasks with orders of magnitude larger Text data so you can pre-train with text only data and then use paired data to do the text to image translation training so what's the state of the art you must be familiar with many of these so Dali 2 from openai was one of the first models which is a diffusion model that was uh that was built on pre-trained clip representations uh I won't have time to go into each of these terminologies so I'm sorry if some of it is not familiar uh imagine from Google is also a diffusion model that was built on a pre-trained large language model uh party which is another large-scale model from Google is an auto regressive model on latent token space and stable or latent diffusion is a model released from stability AI which is a diffusion model on latent embeddings so so here's an example that is showing comparing dalitu imagine and the Muse model on this particular text prompt um and I'll go revisit this example and point out some pros and cons of the different models later uh some image editing applications that have been built on these models one example is personalization via a tool called dream boot which was a paper written by some of my colleagues so here the idea is that instead of generating for example an image of a cat you can generate an image of your cat by fine-tuning the model on some images of your own cat and then doing text guided generation of your cat so that's actually proven extremely popular a couple of apps built on the dreamboot tool have gone to the top of the App Store and many people have used it to generate their own avatars for you know on social media another example is a fine tuning for instruction following so that one can do various kinds of flexible mask free editing for example here on this top right image we say add fireworks to the sky and then the model has an after fine tuning has an understanding of Concepts such as the sky and fireworks and it's able to do a reasonably good job of following the instruction foreign so how's Muse different from the the prior models uh that I listed so firstly it's neither a diffusion model nor is it auto regressive although it has some connections to both diffusion and auto regressive family of models uh it is extremely fast so for example a 512 by 512 image is generated in 1.3 seconds instead of 10 seconds for Imogen or party and about four seconds for stable diffusion on the same Hardware and on quantitative evals such as clip score and FID which are measures of how well the text prompt and the image line up with each other that's the clip score and FID is a measure of the image quality itself the diversity and Fidelity the model performs very well um so so it has similar semantic performance and quality as these much larger models but significantly faster inference and it has significantly better performance than stable diffusion all of these statements just hold true at this point in time all of these models keep improving so um there could be a new model that does even better you know next week and some applications that are enabled by the way we train the model are zero shot uh editing um so I'll show some examples of that musk free editing in painting to remove objects and cropping to expand beyond the boundaries of an image so I'll give a quick overview of the architecture here and then go into the individual components one by one Muse is mostly a Transformer based architecture for both the text and image parts of the network but we also use cnns and and we also use vector quantization and we also use Gan so we're using a lot of the toolkits in the modern CNN modern deep network uh you know from the toolbox we use image tokens that are in the quantized latent space of a CNN vqan and we train it with a masking loss which is similar to the masking loss used in large language models so the model is basically learning how to reconstruct masked tokens you pass in a set of tokens mask a bunch of them at a random and then learn to predict that and just by doing that with variable masking ratios enables it to get very good quality generation uh we have two models a base model that generates uh two to six by 256 size images and then a super resolution model that upscales that to five and two by five one two okay so the first uh major component is this pre-trained large language model so we use a a language model train at Google called the T5 XXL model which has about five billion parameters which was trained on many text tasks text to text tasks such as translation question answering and classification and when a text prompt is provided uh it's passed in through the text encoder and we get a sequence of vectors of Dimension 4096 which are then projected down to another lower dimensional sequence and those set of text tokens are then passed into the rest of the network and then we use cross attention from the text to the image tokens to guide the generation process foreign the next component is a the vector quantized latent space so for this we built first a vqan which is simply a a form of autoencoder with some Vector quantization built into the latent space and um the reason we use a quantize set of tokens about in most of the models we use 8 000 tokens 8192 is that this is amenable to a cross entropy classification type loss so when you want to predict what token is missing you just say which of the 8192 tokens does it need to be and that's a classification loss and we find that this works much more effectively than a regression loss where you might try to predict the exact pixel values of the missing token the wiki Gan has an entirely convolutional structure and encoder decoder structure which we found performs better than either Transformer based with q-gans or hybrid approaches that mix Transformers and cnns we use a down sampling ratio of 16 which generates latent so so when you go through the encoder layers of the CNN you get a latents that are of size 16 by 16 from a256 by 256 input and the super resolution model uses another vqan with latents of size 64 by 64. so those are larger latents because we want more information in each of those super resolution latents um and I this this um the um weakugan is built on work uh from some from from this paper called musket which is also from our group so a key component of our masking of our training is this idea of variable ratio masking so given a set of tokens you have an image it goes through the VQ tokenizer and then you get a sequence of say 196 tokens what we do is to drop a variable fraction of those tokens to then pass into the network to learn to predict um so unlike llms where you usually have a fixed ratio of tokens that's dropped here we use a variable distribution which is biased towards a very high value of about 64 percent of the tokens being dropped and we find that this makes the network much more amenable to editing applications like in painting and uncropping and since it's variable at uh inference time you can then pass in masks of different sizes and since it has done that during training it's able to do that during inference time okay so here's the base model which has which is producing 256 by 256 size images so this is a a Transformer based model the the base Transformer that gets in the mass tokens and also the token the text tokens so these these mask tokens are the image tokens and then we have the text tokens so we have cross attention from the text tokens to image and also self attention between the image tokens during training all the tokens are predicted in parallel with the cross entropy loss but during inference we find that it is better to do an iterative schedule where we predict a subset of the tokens first we choose it we choose tokens with the highest confidence pass those back in as unmasked tokens and repeat this process for about 12 or 24 steps until all of the tokens are unmasked we find that this significantly increases the quality of the result foreign and then the super resolution model up samples uh the 256 by 256 image 2512 by 512 importantly this does this in token Space by transforming the latents from 16 by 16 to 64 by 64. and we use cross attention from the text embedding as well as the low res tokens and pass it into the high-res Transformer so that's we Mask The high-res Tokens but the low res are left unmasked the text embedding is left unmasked and the supervised model is trained to predict the Mast high-res tokens so you can see the effect here especially on the text the the low res output of 256 by 256 it is hard to see the text and then once it's corrected in token space you can you can read the text the facial features of the hamster are much more clear many details are are reconstructed in token space what we found is that a Cascade of models is actually very important if you directly try to train a 512 by 512 model it tends to focus the model tends to focus too much on the details uh whereas if you first train a low-res model you get the overall semantics or the layout of the scene right somewhat like what an artist might do where you you get the scene right first and then start filling in the details uh so one of the things we are looking at is how to increase uh maybe increase the depth of this Cascade go from 128 to 256 to 512 see if that improves further the quality um so here you can see um so so here's a comparison of our token based super resolution comparing to a diffusion based pixel based super resolution so here on the left the prompt the text prompt here is a rabbit playing the violin and this is the two these are two samples from the model um of 256 by 256 resolution and on the top right is the result of our token based super resolution output so you can see that the musical notes the details on the violin all of these are much more clear but if we just took this and did a diffusion based super resolution which is just pixel hallucination that doesn't look so good so our token super resolution plays well with our token based approach um so another important kind of a heuristic or a trick is something called classifier free guidance which we found is crucial at inference time to trade off diversity and quality um what this um this trick does is to Simply trade off sorry um push the logits away from unconditional generation and towards text conditional generation so you can think of the scale as the amount by which you're pushing away from unconditional generation and here's a comparison the image on the left the two images on the left were generated by with the guidance scale of 0.5 so there's more diversity in the poses of the cat and the backgrounds but it's it's not as there are there are more errors for example here on the mouth of the cat compared to the two images on the right which are generated with a much larger guidance scale so during training um we just dropped some tokens with a probability of 10 I'm sorry we dropped conditioning with a probability of ten percent uh but then at inference time we choose a particular guidance scale um another trick so many tricks here to make the images look good is something called negative prompting so the idea here is that there are Concepts which one cannot say in the text prompt itself for example if you would like to generate an image but not have trees in it it's somewhat cumbersome to say that in the text prompt so so the classifier free guidance allows us to say generate this scene but don't have trees in it and we do that by pushing away from the negative prompt so here's an example um a text the text prompt says cyberpunk Forest by Salvador Dali so that generated these are two examples um that were generated using that prompt and then I added a negative prompt where I said I don't want trees I don't want green and blurry that was the text prompt that I provided and then it generates these other kinds of styles which seem to respect the negative prompt so that is a very useful trick um for for generating images closer to the things you're thinking of in your head this is um yeah so this is the iterative decoding I mentioned earlier at inference time and you can see that uh decoding tokens in multiple steps is very helpful for high quality generation so here's a sequence of unmasking steps which are generating tokens up to 18 steps and there's a steady Improvement in the quality of the output our base model uses 24 steps and the super resolution model uses eight steps but these number of steps are significantly lower than the 50 or 1000 steps that are required by diffusion models and this is actually one of the most important reasons for the speed of our approach so we get like a 10x lower number of forward props that we need to push through the model however these days there's these ideas of progressive distillation which can potentially reduce the number of steps for diffusion models and we hope to exploit that to further reduce our sampling steps okay so we did some so now on to evalse that that was a quick tour of the entire model so we did some qualitative evals where we took a set of 1650 prompts from the party paper and generated images from our model and the stable diffusion model and sent it to raters to answer the question which of the two images one from our model one from stable diffusion which of them matches the prompt better so the Raiders preferred our model 70 of the time compared to 25 percent of the time for stable diffusion and for the remaining a few percent they were indifferent so we removed those from this plot um we also did some qualitative evals on various properties of the model how well does it respect things like cardinality so here so here it's about you know counting so we have three elephants standing on top of each other we get three elephants uh four wine bottles so one two three four a tiny football in front of three yellow tennis balls so it seems to respect all of those however when the num the the count goes up Beyond six or seven the model tends to start making mistakes uh here we are looking at different styles so here's a portrait of A well-dressed raccoon oil painting in the style of Rembrandt uh pop art style and a Chinese ink and wash painting style other evals are about composition geometric composition three small yellow boxes on a large Blue Box and a large present with the red ribbon to the left of a Christmas tree Etc we find that it's if you have want to render text then it does a reasonable job if there's not too many words in the text so one word or two words the model is able to render them well and we're also able to provide um very long detailed uh prompts so here's an example an art gallery displaying Monet paintings the art gallery is flooded robots are going around the art gallery using paddle boards so a lot of this power comes from the language model itself which is really really powerful and able to represent each of those Concepts in the embedding space and what we're able to do is to map that to pixels it's still mind-blowing though it's still amazing that we can get these kinds of outputs from text prompts here are some failure cases like I mentioned um here we asked the model to render a long a number of words and it did not do such a good job 10 wine bottles and it stopped at one two three four five six seven and so on here's a subjective comparison to other state-of-the-art models so um one thing to say is that in this space the evaluations are in my opinion not very robust uh because by definition often the text prompts and the Styles we ask for are not natural we want various mix and match of styles and so in important open research question in my mind is how do you evaluate that model A is better than model B other than just looking at some results I think that's a very interesting and open uh question so here so here I'll just um point out a couple of things with the uh the example at the bottom which is a rainbow colored penguin and dalitu is generating Penguins but doesn't do such a good job with the colors whereas the Imagine and Muse models seem to be able to respect both and we think this is probably because the model is relying on a clip embedding which might lose some of these details um we did some quantitative eval on the metrics that we have which is FID and clip on a data set called cc3m and compared to a number of other models both in diffusion and autoregressive type models um here for FID score lower is better and for clip higher is better so overall we seem to be scoring the best on both of these metrics and here's a the eval on Coco comparing to many of the state-of-the-art models like Dali Dali 2 imagine party and um we are almost as good as the party 20 billion model um uh just slightly worse in FID score but doing significantly better on clip score um so so that's good which means that we are able to respect the semantics of the text prompt better than those models if one was to believe the clip score and finally a runtime on TPU B4 Hardware where here's the model the resolution that is generated and the time wall clock time that it took so most of the compute goes into the super resolution the base model takes 0.5 seconds and then it takes another 0.8 seconds to do the super resolution for a total of 1.3 seconds foreign so um now here are some examples of the editing that are enabled by the model on the left is a real input image and we've drawn a mask over one part of the image and then ask the model to fill that in with a text guided prompt so a funny big inflatable yellow duck hot air balloons and futuristic streamline modern building so all that came just zero shot we didn't do any fine tuning of the model but the fact that we trained it with variable masking allows it to do this out of the box so another example where we do out painting so the mask here is the rest we only want to keep the um this person and replace the rest of the image and the text guided prompt was provided to fill in the rest of the region so the London Skyline a wildflower Bloom at Mount Rainier on the Ring of Saturn uh we can also do other kinds of editing uh where we took a real image with there was a negative prompt use a man wearing a t-shirt and then these were the positive prompts to to do various kinds of style transfer uh on the on the on the T-shirt and here are some examples of mask free editing where on the top row are input images and on the bottom are the transformed outputs where it just relies on the text and attention between the text and images to make various changes for example here we could we say ashiba Inu and that the model converts that cat to a Shiba Inu dog a dog holding a football in its mouth so here the dog itself changes and then this ball changes to a football a basket of oranges where the model is able to keep the general composition of the scene and just change the apples to oranges the basket texture has also changed a little bit but mostly the composition is is kept similar uh here it's able to change the just um make the cat yawn without changing the composition of the scene and one of the things we are exploring is how we can have further control of the model uh you know really be able to adjust specific parts of the image without affecting the rest um yeah and here's a an example of iterative editing where uh the the the image at the top was provided and the top right and then on the bottom right is the output across for a croissant next to a latte with a flower latte art and these are we we ran the editing for a hundred steps uh progressively adjusting the tokens and these are the results of the different iterations of of train of inference so you can see that it starts with the cake and the latte and then progressively transforms the cake to something that looks in between the cake and the croissant and then finally it looks like the croissant and similarly the latte art changes from a heart to a flower uh you know kind of um some kind of an interpolation in some latent space so because of the speed of the model there's some possibility of interactive editing so I'll just play this short clip which shows how we might be able to do interactive work with the model so that's the real time as in it it's not sped up or slowed down it's not perfect but you can see the idea okay so um next steps for us are improving the resolution quality handling of details such as rendered text probing the cross attention between text and images to enable more control exploring applications um so yeah the paper and web page are listed here and I'm happy to take questions thank you foreign thank you so much maybe I have one question just to get things started um I'm curious in your opinion what are the most important contributions that Muse made specifically to achieve the very impressive speed up results right because in comparison to the Past methods it's really like a huge gap so I'm curious what like what across the many contributions led to that in your opinion yeah it was primarily the parallel decoding so in Auto regressive models by definition you decode one token at a time so you have to go let's say the image consists of 196 tokens then you're doing 196 steps because you unmask one token pass it back in get number two and so on in diffusion models what you have to do is to denoise step by step yeah you start with Pure Noise pass it through the network get something out then pass it back in and repeat this process and that process needs to be fairly slow otherwise it breaks down a lot of the research is about how to speed that up so that takes thousands of steps so if you have a comparable model there's just a fundamental number of forward props that need to be done so what we did instead was to go for parallel decoding and they instead of one token at a time you just do end tokens at a time and then if it's fixed and you just need 196 by n steps and the idea is that if you use high confidence tokens then they are potentially conditionally independent of each other at each step uh and we can each of them can be predicted without affecting the other so that seems to hold up very interesting [Music] so the prompt is allowing you to navigate the latent space of the image itself have you done any analysis or looked at what is the relationship between the prompting and the directions or velocity or any other sort of metric that you can look at in that latent representation exploration yeah it's a good question we haven't yet done a very thorough investigation uh what we looked at was uh and I don't have it here but we looked at some cross attention Maps uh between the text embeddings and the and the image the generated image and what you can see is that um you know um the uh it does what you might expect which is that uh there's cross attention between the different nouns and the objects in the image when you have a verb that connects two objects it seems to then highlight both those objects uh sort of paying attention to that so that's one level of exploration but I think we need to do more about like as we walk around in latent space what's the resulting image if we move between two text prompts what happens we we don't know yet if the latent space is smooth or it has abrupt jumps the editing suggests some smoothness here as you iterate but this is with a fixed prompt not with the changing prompt um so I think we need to do more any other questions foreign [Music] yeah I had a question about like the cardinality portion of the results um like one of the failure cases show that like the model can't really handle if you give it more than like six or seven of the same item but sometimes when you don't specify anything in the prompt it automatically generates like more than that of the number of items do you know why it breaks down when you give it like six or seven or eight I think it's my feeling and again we haven't analyzed this is that it's just that fewer num small numbers are more common in the data we just have a few two three four uh and then the model has just never seen uh 10 or 20 and then there's also not the way we train it there's not this concept of graceful degradation from 10 to many uh where it might just say Okay a crowd of people and then you generate what looks like 30 people I think we need more there have been papers that are trying to overcome this problem uh I think explicitly through just enriching the data I think more clever Solutions are needed thank you [Music] when you do not specify a background of a request how does it happen that is there a limited uh amount of backgrounds that it has to choose from randomly or uh is there some sort of a generator that it just how does it work when you do not specify the background for example at all yeah it's a great question so one thing we did was just type in nonsense into the text prompt and see what happens and it seems to generate just random scenes like of of you know Mountains and the beach and so on it doesn't generate nonsense so we think that the code book in the latent space is dominated by these kinds of backgrounds and somehow that's what gets fed in when you go through the decoder so um yeah I don't have a better answer than that hi thanks for the talk on kind of the Mask free and painting super interesting um a lot of the prompts you showed had like the the correction was a small change from maybe the next slide I think um one more sorry I'm looking for the one with the dog with the football in his mouth yeah yeah right many of these are kind of small changes from what's in there right you're going like give it the input as a basket with apples in it you're saying oranges have you tried if the like editing text is completely different like you go from Dog With A basketball to a cat bowling or something like that yep that's even something crazy like a city that doesn't work so we've tried that uh it doesn't uh so I think something like the instruct picks to picks uh that it works for that because they explicitly train it with large edits part of the problem could be the way we do the editing which is based on these small back prop steps which just allow you to do local changes so we don't know if it's a limitation of the model or a limitation of the gradient this the the SGD steps we do when we're doing the editing so what happens with the editing is you start with the original image then take the text prompt and just keep back propping till you know it settles down converges um say 100 steps and so each of those steps like I showed here is small changes and if you want something like what you described you need to have the ability to kind of jump faster 300 steps maybe if you did it for a million steps you could have that change we haven't yet looked at that yeah it's almost like hard to imagine even what that middle ground would look like yes like do you have to pass through some Valley of unrealistic images to get to this very large change here each of those sub steps look like believable things so it could be an optimization problem thank you maybe extension on that same question I'm curious for the case where the not the entire image is changing but maybe at the Image level you're changing like the style for example right like the main content is maintained but the style is being changed and modified is that have you tried these type of things before um uh yes I think so like I don't have example maybe something like the uh maybe something like this um I think um it seems much harder to do realistic pose changes like the kinds um he was asking about compared to these Global style changes because I think the global style is controlled by maybe one or two elements of the code book or something like that whereas to to change pose or make drastic geometry changes might require much more interaction among the different tokens yeah good question I thank you for that great talk I was wondering how does the model determine which picture is more accurate to the description or which is which has a better resolution like what part of the model determines that without needing to needing the human oversight so it doesn't in practice well are you talking about training or inference uh with the inference so we what we do is just use random seeds and generate a whole bunch of images and then we just pick out the one we like so often what happens is that if you have eight images or 16 three or four of them would be really nice and then a few of them would look really bad and we still don't have a self-correcting way or an automated way to say this image matches the prompt better um so yeah so so in general the hope is that the the latent space has been trained in a sensible way so that it will generate plausible images okay uh but for example you might generate a cat with you know the legs all in the wrong position and then it's still using the right elements of the codebook but it arranged them wrongly so we do see those kinds of mistakes okay so for your next step in improving the resolution do you imagine um you'd go about it the same way yeah okay yeah we'd have to some okay thank you those issues okay one more question um so I kind of have two questions in one so my first question is uh I'm not sure what are the limitations on the size of the Corpus for the text and for the images um but say uh there's like a new artist that hasn't come out yet or like will in the next two months um if I were to ask in the prompt say I want you to draw this and the style of this thing that isn't invented yet how would the model react to that change and um a separate question is with these especially in the example with the masked images the top row and the bottom row how much new information are we actually getting from these uh images that the model spits out so it's a it's a great um great questions so for the for the data clearly the data is biased towards famous artists and that's why we can say things like you know in the style of Rembrandt or Monet and it has seen many examples of those because this is data scraped from the web if you had a new uh like the style of a new artist the only current way to do it would be through fine tuning where we take those images of the person pair it with some text and then kind of train the model to generate that this is kind of what the dream boot approach tries to do although that's specific to you know objects rather than the style but you could imagine using something like this for the style of a new artist um it's not yet zero shot where you just present these examples and say can you generate something based on this text prompt but in that style so that's something we would like to work towards regarding the masking I didn't fully follow the question so you said something about the top and bottom rows um yeah I think it was uh one of the slides way past this one but it was a pretty general question it was how much new information are we actually gaining from these I guess pictures that haven't like no one has seen before like with the bearer on the bicycle Oh you mean yeah are you talking about memorization versus like is it actually is do you mean if this is already in the data set the training data set um yeah yeah so this is actually a great question and I think I don't think we still have very good answers to this um you know is it is a large language model just hallucinating some things it's seen before just mix and match probably uh it's probably this is also doing something like that where it's seen Bears before and bikes and has been able to put them together in a plausible way it's unlikely that the exact same image was in the data set but again we don't have really good tools yet to go in like like we could try to search based on some kind of embedding clip embedding or something to look for similar images uh uh but at the scale at which we are training hundreds of millions of images we haven't actually gone in and looked to see how much this memorization versus new combination of Concepts I do think it's the latter because it seems unlikely that you know these kinds of images would be in the training data set okay thank you thank you very much let's give tulip one more round of applause thank you 

foreign thank you hi everyone it's Alexander for the introduction all right very excited to talk about these modern era of Statistics so you've heard throughout the lectures probably a lot about the you know deep learning and the technologies that enables this but what I want to talk about I want to say why it works okay so and give you some intuitions about where we are standing in terms of the theoretical analysis of this system because a lot of people think that machine learning is an ad hoc field and deep learning is extremely ad-hoc just change a couple of hyper parameters and everything starts working but that's not actually the case so we have some cues like what's what's the whole thing is about okay all right so to motivate I want to tell you about this I think it's a very common photo these days that you can see that uh the Morse doubling law got broken after 2012 and we got into kind of a new type of models that as we growed in size on the y-axis here what you see is the energy consumption of these models it could be even accuracy you know the accuracy of these models or generalization ability of these models goes higher and higher as uh we scale them okay and that's the kind of observation that we had so far and that's what I mean by modern era because we are exhaustively increasing their size as this photo can show you like some scale of these large language models that are out there you can see that we have models up to 540 billion parameters where uh you know it's beyond our understanding how these models perform representation learning and um it's not only in the realm of language modeling but it's also across like different fields for example in Time series modeling in medical diagnosis in financial time series we have new technologies that are getting better and better with size and this seems to be the case across different data modalities and perhaps the one that Ava was presenting before generative modeling we went from in 2014 generative models like that two generative models like this right so the quality drastically improved not only because of the size but also the underlying structures that enabled us to scale the small networks because we already knew that if you have two stack two layers of neural networks next to each other you have a universal approximator right so then why couldn't we scale it now we had to find the right structure in order to do that for example diffusion was one of those structures that I've actually presented so it seems like bigger seems better but why okay let's find out the reason so you all um I assume that you would all know how to solve these two equations so any questions requires an unknown right so how many of you you know still how to solve this kind of thing oh my God yeah I think it's pretty but then the crazy thing about deep learning is that it comes to says that yeah make the number of these X and Y larger and larger excessively for these two equations and things be things would start working even better and what does it mean even better when you have like two equations and you have like multiple a lot of unknowns how does that even make sense you know let's do an analysis a numerical analysis on the MS data set you you've all seen this data set before it's a handwritten handwritten digit and as you see like it has 60k points it has Dimension out of 28 by 28 grayscale images and then today's models they have millions of parameters to model 60k images so the performance on this data set actually keeps improving as we scale the neural networks and how does this make sense like where is the information basically that we're learning from 60 image 60k images with millions of data parameters what are we learning now uh we know that generalization error or test error has this kind of proportion from the theory that uh it's proportional to the number of parameters of the system over the square root of number of parameters of the system over the data set size that means if I increase the number of parameters of my system then I would expect at some point that generalization or the error was high so then why is it that we make them larger and larger but they work in case of imagenet another larger scale data set a very common in machine learning we have like 1.4 million uh images of size 256 2563 and then you have models with hundreds of millions of parameters that you fit this 1.4 million images in NLP as we showed before we have data points of few billions and then models with 100 billions and then perhaps like the best and my favorite kind of illustration of this size improving performance in this generative AI when you have a prompt this is like a generative AI that receives text as input and generates output images so the prompt or the input to this system was a portrait photo of a kangaroo wearing an orange hoodie and blue sunglasses standing on the grass in front of the Sydney Opera House holding a sign on the chest that says welcome friends right and as we see that on the top of each image we have the size of the model and as we improve the quality of the uh the size of the models the quality improves and we're getting closer and closer to that description that we provided as an input to the system right like the first image actually 350. it's still like has all the components of that input but it's not as good as the last one right it misses on a couple of things all right so this happens and let's now figure out a way to explain this okay how many of you have heard about an a phenomenon called double descent one two okay good all right so all right so uh this is not a learning curve okay first x-axis shows the model size you have an image classification problem c510 you know and these are 10 classes you want to classify these images and now on the x-axis you are increasing the size of the networks and on the y-axis we see the test error okay let's just concentrate on the um on the purple uh side of the basically a process because that's at the end of the training process okay if you're looking at the test error at the end of the process so classical statistics was telling us that as you improve as you increase the size of neural networks up to a certain point you find a nominal point where the performance of model the generalization that you have is optimal and from that moment on if you increase the size of the network you're overfitting what that means that means like the the kind of accuracy basically that Bell the first Bell shape that starts going up as you see that part was going and and the project they projected that that thing is going to go uh up as we actually scale the models but then the phenomena that we observed is that there is a second descent as we scale the size so this is called a double descent phenomena so the first descent we knew already in the modern era we figured out that as we scale these neural networks the size of the models and this is this has been observed across many different architectures okay so as we scale it could be a resnet architecture it could be a convolutional neural network it could be an lstm or multi-lay perceptron or whatever that you you know but as we increase the size that's the kind of phenomena that we see now let me uh make it a little bit easy easier to understand this was classical statistics in terms of accuracy now in the previous one we were seeing errors and obviously accuracy we go high up to a certain point and then as we increase the size of the models we start overfitting now up to a certain point now if you have the new kind of experimental results showed that that's actually the case where up to a certain point we go up and then again there's an improve Improvement in performance this regime is called an over parametrization regime I'll tell you more and I'll give you more substance why we call them over parametrized over parametrization regime I mean it should be pretty obvious but I'll put it in the theoretical and the technical kind of language of uh machine learning all right so one of the things that we can observe from this image is that the accuracy at the very end of this over parametrization regime you see that it's a slightly better than the first accuracy that you get from a model that is reasonably sized okay it's not that much better the accuracy but then the discovery of deep learning was that when we're in the over parameterization regime we get a new type of behavior starts emerging what kind of behavior the characteristics that emerge in that over parameterization regime is something that I'm going to talk about next so one of the things is um as we go larger and larger in networks sizes they learn Concepts that they can pass on to different tasks that means they can generalize across different tasks that means those Concepts that they learn they can be transferred to actually perform more tasks not even a single task that they have been trained on okay so it seems like you're giving them more ability to generalize and getting closer to those kind of a kind of General representation of AI I'm afraid to say AGI yet okay but yeah but that's the kind of thing that we observe the kind of Concepts that they learn they are being able to um perform across different tasks across different domains they get better and better another observation that we had is that the scale improves robustness what does it mean for uh a deep learning model to be robust that means if I change the input a little bit if I perturbate with noise on the output it doesn't completely go crash so the change that the input is proportional to the output changes so robust means that you are being able to control the error and the output if you have a little bit of error or deviation at the input so I this curve actually shows that as we scale these neural networks we see that the size of the the robustness actually improved okay I'm going to talk about this in much more detail in because that's the part where we have the theory for from a deep learning perspective now not everything improves by uh by um growing uh in size for example the problems of bias and accountability and and minor accuracy on minority samples as we scale they get worsen so that's we have evidence for those uh kind of cases as well so you need to be careful when you're deploying this in our society now another very important part of intelligence which is the reasoning the ability to logic logically talk about kind of different phenomena is basically reasoning and um and reasoning is basically stays unchanged if you just scale a model unless you provide a system with a simulation engine let's say you want to do physical reasoning say there are two cubes they're both moving in X on x-axis one of them is faster than the other one so one thing that you have as a human being in your head is a simulation engine that you can basically simulate that kind of reality so if you provide that kind of reality to a language model the results of that simulation to language model you st you would see again increasing reasoning so that part is also extremely important now the part that we are like all these results that I'm showing here they are very experimental so there has been like uh large corporations involved to actually perform this kind of analysis to actually to see the behavior of these large models but do we have an actual theory that fundamentally is true about these models or this behavior that we see let me focus on the robustness okay this is from uh this kind of graph is from Alexander Madrid's group from here MIT where they're studying robustness in neural network what they do I mean it doesn't matter what the three lines are what they do they try to attack the input images and they and then compute the accuracy of models as we scale again on the x-axis we see the scale and accuracy as we start increasing the capacity of the networks in image classification Ms classification here we see that there's a jump in robustness that means the attacks that they were doing were perturbing the input images with something called an attack is called projected Dragon descent it was basically like you you got into a really good accuracy after you actually increased the size of the network up to a certain point and there was this transition this there was a shift uh in in performance and that has been confirmed as I said by experiments and we said all right so scale the conclusions of those kind of results was that scale improves robustness but then and the best paper award at the the conference on neural information processing system new ribs and 2021 came and said scale is a law of robustness okay what that means okay that means like let's formulate how scale is basically contributing to robustness okay so I'm going to talk a little bit more technical about this thing hopefully you can you can all follow but you can ask questions like even now if you uh have questions let me just finish this one and then we'll take some questions so um let's say fix any reasonable reasonable function what is a reasonable function it's basically a function that has a smooth Behavior okay like a sigmoid function okay that's a very reasonable function okay a crazy function would be like if you have like jumps across like different modes you know like you have a reasonable function okay and it's basically uh it has P it's parameterized okay like a sigmoid function that you can parametrize it and then it's poly size like it has also reasonable size in parameters and it's not a common graph Arnold type Network what does that mean that means um I'm showing you a representation that was discovered by Arnold and and Karl McGruff that shows that there are these this this this composition that we see can approximate any continuous function multivariate continuous function okay so these are basically two non-linearities and the sum operator is basically applying uh doing the the inner kind of processes but one problem with this uh kind of functions is that they're not as smooth and uh as uh Tommy pojo actually mentions the they show wild Behavior like they could be anything the function in inner function could be anything so let's let's rule out those kind of uh functions let's talk about only neural networks okay like very reasonable functions and really function as well now let's sample n data points for the case of mnis data set we have 60k data points truly High dimensional I mean mnis is like 28 by 28 times 1 so it's like 728 right so it's not that truly High dimensional but let's say image net is 256 times 256 times 3. so it's like truly High dimensional and then um add label noise so why why do we want to add noise to the labels what does that mean even that means we want to make the problem harder for the neural network that means it should not be the mapping between the input High dimensional input than the output classes should not be trivial okay so we add noise a little bit of labeled noise to basically create this um you know a complex problem okay so that means like if you have all your labels are random the problem is really complex right so you want to add a little bit of random noise as well and then then to memorize this data set what does memorization mean that means if I have a function and I'm fitting a parametric model like a neural network to it I want to be completely fitting every single data point okay in during training that means I want to achieve a zero loss on my training set okay that means like memorizing so definition of memorization during learning now optimize the training error below the uh noise level and below the noise level what does that mean again that's like another definition that you guys have to know is um as I said we want to complicate the process so that it's not a trivial mapping so you add that noise and then when your accuracy is a little bit higher than the amount of noise that you injected for example for the amnes basically the hard part of the training of the name status let's say if you all kind of machine learning models can get up to 92 91 accuracy on the endless data set but then what's the hard part the hard part of it is the last two percent to five percent to reach the hundred percent so that's the hard part that's what we call below the noise level okay that means if you can learn this process really like would be really good accuracy now and to do this robustly in the sense of lip who knows what is a lip sheet function good five six okay all right so lipstickness so good that I put this in here so so as I said like so you're moving your your inputs with Epsilon okay A little bit of perturbation at the input of a function then if the output is also moving with Epsilon or proportional linearly proportional to that Epsilon that means this function is lip shaped so you have a controlled kind of process changing the input do not dramatically change the output of a function that's called the leap sheets function now yes so you memorize the data and you want to do this memorization robustly so if you want to do that it is necessary it's absolutely necessary to parameterize your neural network at least equal to n n is basically the dimensionality of your data set which is 60k times d d is the dimensionality of your uh of every input sample let's say if your input is an mnis data set I mean I'm gonna give you an example later on but let's say the input is 728 in the order of 10 to the power 3. and the number of data set is 60k right then the minimum size of an Eminence data set has to be 10 to the power 8. that would be the size of a neural network model 10 to the power 9 to robustly learn mnis dataset that's a huge neural network but this is one of the fundamental explanations very recent that we have about the theory of deep learning and why is it called uh dramatic over parametrization because intuitively as we showed before memorizing n data points we require in parameters and we had a Eric palm and David Hessler they were showing that two two layer neural network with threshold activation function they showed it in 1988 that you would need T is the number of parameters you would need almost equivalent order of number of data set data points that you have that would actually would be enough theoretically and then they showed it recently for the relu networks and uh we even have that in the neural tangent kernels so how many of you are familiar with neural tangent kernels good three out of three one two yeah so neural tangent kernels is that so imagine your the process of training a neural network with gradient descent the whole process from beginning of training to the end of training is a dynamical process that means you're updating the weights of the system as you go further given a data set given a neural network and given the parameters of your optimizer you can this learning Dynamics can be converted if I can actually be modeled by a dynamical process by differential equation that explains how the updates of gradient Descent of a neural network would work now if I increase the size of the neural network to infinite width if I increase the size of the neural network to infinite waste then this Dynamic of learning has a closed form solution that means it has actually a solution which is a kernel method so basically you will end up having a kernel and that kernel explains the entire behavior and dynamics of your learning process in the infinite way this is called the neural tangent kernel Theory and you have a PhD student it's actually sitting over there that is focusing on that and this is actually his PhD topic that he's working on in uh Danielle's lab okay so now let's let's move from this uh Theory and let's let's give an example so we have the mnist data set it has around and 10 to the power five number of parameters the dimensionality of each data point is 10 to the power 3 is 728 so it's just the scale and then we saw the transition in the uh kind of robustness that means like at least you need to have one minute 10 to the power six parameters to robustly fit the mnist data set but then there are a couple of notes that we want to have the notion of robustness in the in the theory paper was a little bit different than the notion of robustness and lip shitness that we showed in the uh in the theory of law of robustness and then another point is that the law seems to be contradicted because if you multiply we just said the law is like you know n times d right like that's the minimum number of samples that you can you have to transition so but here if you do that 10 times 10 to the power 5 times 10 to the power 3 is 10 to the power 8 and it's much larger than that 10 to the power 6 that was observed in reality but one of the things that you have to also notice is that there's something about data sets it's called effective dimensionality and effective dimensionality means that when I show you an image there are principal components of an image so finding the principal components of an image that how small that image what's the information the information is not the same size as the pixel itself the information is much smaller so the effective dimensionality of it's hard to actually determine what's the effective dimensionality of a given data set but still for the Eminence data set it's 10 to the power 1 and now we have 10 to the power 5 n and if the effective dimensionality is basically at 10 to the power 1 then you would have basically the same you can confirm with experiments the theoretical results that was observed with the law of robustness now the noisy labels as I said it's basically learning the difficult part of the training and then in regard to imagenet is basically shows the law of robustness predicts because we haven't yet trained super large neural networks it seems like the networks that we have trained so far they're actually small you know we have trained like really large neural networks still but they have to be on the order of uh 10 to the power 12 or 10 to the power 10 you know and this is like the prediction of the law of robustness okay so now that's get back to this image so all these networks that I was talking about and law of robustness is basically in that regime in that regime I showed you that we have great generalization you get more robots provably more robustness and then reasoning still we have questions about how to achieve it and then bias and fairness which is very important energy consumption and accountability of the models there is a way and that has been the focus of the research that we have been doing at Daniela Russo's lab with Alexander and a couple of other graduate students of ours is to find out how can we get out of that over parametricization regime while addressing all these social technical challenges and how we did that we went back to Source okay basically we looked into brains okay we looked into how we can get inspiration from brains to improve over architectural biases that we have in neural networks in order to break the law of robustness now and then we invented something called liquid neural networks with the direct inspiration from how neurons and synapses interact with each other in the brain okay that's how uh we we started our focus and the second half of my talk I'm just going to talk about liquid neural networks and their implications in this realm of modern era of statistics so as I said we started with nervous systems then in order to understand nervous systems you want to go down and you you can look into neural circuits so neural circuits are circuits that are composed of neurons then they can get sensory inputs and generate some some sort of uh outputs motor outputs and then we went even deeper than that and we looked into how neurons individual neurons communicate with each other two cells receiving information between each other and then we arrived at an equation that or a formulation for the interaction of neurons and synapses that is abstract enough so that we can perform computation efficient computation but at the same time has more details of how the actual computation happens in in in in brains and not just the threshold or activate it's very kind of the sigmoid function that we always see so let's get in more deeper into that and we show that if you really get into more biologically plausible representation of neural networks you can gain more expressivity you can handle memory in a much uh more uh natural way you gain some properties such as causality and which was basically something that we haven't discussed and what's the cause and effect of a task can we actually learn that you can get to robustness and you don't need to be crazy like large be out of the that regime you can perform a generative modeling which was uh what I described before and you can even do extrapolation that means you can you can even go beyond the the kind of data that you have been trained on that means you can you can go out of distribution so that's something that we couldn't do and the like the the area that we were focused on was robotics real world you know like we call it mixed Horizon decision making that means if you have a robot interacting in an environment in a real setting now how to bring these kind of networks inside uh uh reward and that's the focus of our research um yeah so what are the building blocks of this type of neural networks I said the building blocks are basically interaction of two neurons or two cells with each other one of some observation about this process the first observation is that the kind of interaction between two two neurons in the brains is a continuous time process so that's something to account for so we we're not talking about discrete processes another thing is that synaptic release right now in neural networks when you connect two nodes with each other you connect them with a scalar rate right now what if I tell you that when you when two neurons interact with each other it's not only a scalar rate but there is actually a probability distribution that how many kind of neurotransmitters generated from one cell is going to bind to the channels of the other cell and how it can actually activate uh the other cells so the communication between the nodes is very sophisticated and it's one of the fundamental building blocks of intelligence which we have actually abstracted away in artificial neural network to await to scalar rate now another point is that we have massive parallelization and massive recurrence and feedback and memory and sparsity kind of uh structures in brains and that's something that is missing from um uh not entirely but but somehow it's missing from the artificial neural network so we deviated a little bit now what I want to argue is that if we incorporate all these building blocks we might get into better representation learning more flexible models and robust and at the same time be able to interpret those results and this and for because of the first reason that the whole process was built on top of continuous time processes let's get get into this continuous time processes and where we are in terms of neural networks with continuous time processes so here I'm showing you an equation so f is a neural network it could be a five layer neural network six layer neural network fully connected but this F that has n layers and with K and then it has certain activation function it has it's a function that means it receives input it receives recurrent connections from the other cells it receives exogenous input as well like this I and it is parameterized by Theta now this neural network is parameterizing the derivatives of the Hidden State and not hidden State itself that builds a continuous time neural network now if you have a continuous time process that means like the updates or the outputs of a neural network is actually generating the updates of your derivative of the Hidden State and not hidden State itself what kind of you can give rise to continuous time Dynamics with neural networks that we haven't explored before now what does this mean in terms of like neural networks let's look at this um His Image how many of you have heard about residual Networks one okay residual networks are deep neural networks that have a kind of Escape connections okay like from one layer to the other one you have basically skip connections and that skip connection is modeled by this equation HD plus one equal to HT plus F of whatever that HPT is basically uh resembling basically a skip connection and now here on the y-axis what we see we see the depths of neural network and each dot each black dot here shows a computation that happened in the system so when you have a neural network that is layer wise like let's call it layer wise because the photo is actually showing the layers in the vertical axis so if you look at the vertical axis you see that computation happen at each layer okay because as the input is computed from the first layer to the second one and then the next one next one but if you have a continuous process continuous process equivalent to this process you have the ability to compute at an arbitrarily point in a vector field so if you if you know what are differential equations and um how how do they basically change the entire space into a vector field how can you basically go uh you know you can do adaptive computation okay and that's one of the massive benefits of continuous time processes now in terms of things that you have seen before standard recurrent neural networks you had the lecture here so a neural network f is basically computes the next step of the Hidden state so it's a discretized kind of process of updating the next step and neural ode obsesses states with a neural network like that uh in a continuous time fashion and then there is a better uh more stable version of this differential equation that is called continuous time or ctrnens recurrent neural networks there are continuous time recurrental networks where they have a damping Factor so that differential if it is a linear ode ordinary differential equation that describes basically the Dynamics of a of a neural network hidden state of neural network now in terms of what kind of app what kind of benefits will you get let's look at the top one these are reconstruction plots from uh data data corresponding on to a spiral Dynamics if you have a discretized neural network this spiral Dynamics is being able to you know it's the the kind of interpolation that you have is kind of edgy but if you have a continuous time process it actually can compute very smoothly and also it can even generalize to the Unseen area of this which is the red part of this and as we see here the normal recurrental Network misses out on on this kind of spiral Dynamics so with the continuous time process it is believed that you can get better and better representation learning but then the problem here is that um if you if you actually bring these models in practice a simple lstm network works better than this type of network so basically you you can actually outperform everything that we showed here within lstm Network so far so what's the point of basically creating such a complex architectures and stuff so this was the place where we thought that uh The Continuous time processes that we bring from nature can actually help us build better inductive devices so we introduced a neural network called liquid time constant networks on short ltcs ltcs are constructed by neurons and synapses so a Neuron model is a continuous process like that it's a differential equation so linear differential equation that receives the synaptic input s it's a linear differential equation neurons and synapses now we have a synapse Model S of T is a function that has a neural network multiplied by a term called a difference of potential that non-linearity resembles the non-linear Behavior between the synapses in real kind of neurons if you have synaptic connections and they are basically uh designed by non-linearity because that's actually the reality of the thing and then if you if you plug in this s inside this linear equation you will end up having a differential equation which is looking very sophisticated but you will see the implications of this differential equation it has a neural network as a coefficient of x of T which is this x of T here that neural network is input dependent that means the inputs to the neural network defines how the behavior of this differential equation is going to be so it sets the behavior of the differential equation in a way that when you're deploying this system on board it can be adaptable to inputs so this neural network this system if you have it in practice it's input dependent and as you change the input as the result of inputs the behavior of this differential equation changes now in terms of connectivity structures it has if you look at the range of possible connectivity structures that you have in a standard neural network you would have um sigmoid basically activation functions you might have reciprocal connections between two nodes you might have external inputs and you might have recurrent connections for a standard neural network now for a liquid neural network instead each node in the system is a differential equation x x j and x i they have Dynamics and then they're connected by synapses and then there are process non-linear processes that controls the interaction of synapses so now in some sense you can think about liquid neural networks as being um being processes that have um non-linearity of the system of synapses instead of the neurons so in neural networks we have the activation functions which are the non-linearity of the system now you can think about non-linearity of the system being on the synapses or the weights of the system now in terms of practice let's look at this application here we train an artificial neural network that can control a car doesn't drive the car in this environment and what we are showing on the sides of this middle image is the activity of two neurons that are inside the standard neural network and a liquid neural network on the x-axis we see the time constant or sensitivity of the behavior of the output Behavior basically controlling the vehicles as steering angle the sensitivity of that control on the x-axis and on the y-axis we see this steering angle the color also represents the output which is mapping between the steering angle and the outputs of the neural network okay so now as we see we added like with liquid neural network we have an additional degree of Freedom that we could that these networks can set its sensitivity a liquid neural network depending on whether we are turning whether we are going straight if you're going straight the uh the time constant you want to be more cautious when you're taking turns right so your neural network you want to you want to be faster so to be able to actually control during those kind of uh kind of events where you're actually turning so that's the kind of degree that you can add even at the new Runner level for interpreting these systems now let's look at the cases study of liquid neural networks so usually you saw a deep neural network for an autonomous driving application we have a combo a stack of convolution on all networks they receive camera inputs and they can output the kind of a steering angle at its output these kind of uh neural network well one of the things that we can do first of all they have like a lot of parameters in this system what we want to do we want to take out the fully connected layers of this neural network and replace it with recurrent neural network processes one of those recurrent processes that we replace it with its uh liquid neural network and the LT and lstms and the normal continuous time neural networks now if I replace this I would end up having like four different variants these four variants one of them is called an NCP which is a neural circuit policy that has a four layer architecture each node of this system is defined by an LTC neuron that the equations that I was showing you and it is a sparsely connected to the other parts of the network so we have a sparse neural network architecture here we have then one neural network that has lstm as the control signal it receives the perception with a convolutional Networks and then we have ctrnens and we have a convolutional neural networks now I want to show you the driving performance of these different types of networks and what kind of characteristics is added to this systems so first let's look at this dashboard where I'm showing you a convolutional neural network followed by a fully connected layers a normal very standard deep Learning System that receives those camera inputs and has learned to control this car okay so those camera inputs comes in and the output decisions are basically driving decisions now on the bottom left what we see we see the decision making process of this system the inputs if you realize the inputs has a little bit of noise on top of them so I added some noise at the input so at so that we can see the robustness in decision making of this process as we see actually there is the the kind of decision making the brighter regions here is where the neural network is paying attention when it's taking a driving decision and we see that the attention is basically scattered with a little bit of noise on top of the system now if you do the same thing and add noise we then replace that fully connected layer of neural network with 19 liquid neurons you can actually get to a performance of Lane keeping on the same environment while having the attention of the system being basically focused on the road's Horizon like the way that we actually drive right so and the fact that the parameters that are needed for performing this tasks basically 19 neurons with a very small convolutional neural network as perception module that's the fascinating part that you get from that inductive bias that we put inside neural networks from brains okay so if we model that so you you have like an uh a set of neurons that you can really go through their Dynamics you can analyze them you can understand that process of that system and you get benefits like real world benefits for example if on the x-axis as I increase the noise I increase the noise the amount of noise that I add at the input and on the y-axis I compute the output crashes number of crashes that actually happens when the drive when the network wants to drive the car outside we see that these are the other networks and here we see um a liquid neural network where it actually keeps the level extremely low ltc's if you look into the attention map of different those four different network variants how do they make driving decisions when they are deployed in the environment we see that the consistency of attention is different across different networks while where we have a liquid neural network actually maintains its focus and uh that's one of the nice thing about this but then we ask why why is it that the liquid neural network can focus on the actual task which is Lane keeping here there is no this is driving and it's just a simple driving example where you have just you know like a road and you want to stay in the road right then now what as I said like the representation that learned by a liquid neural network is much more causal so that means they can they can really focus and find out the essence of the task and then if you look into the machine learning modeling statistical modeling is like the least form of causal modeling is basically you just extract uh these are like these are the taxonomies of the causal models you have on the bottom of this axis you have the statistical models that they can learn from data but they cannot actually establish the cause or relationship between the input and outputs the best type of models that we could have is physical models that describes the exact dynamics of a process then you have basically a causal model and in the middle you have a kind of a spectrum of different types of causal models so one thing that we realize is um basically liquid neural networks are something that is called Dynamic causal models Dynamic causal models are models that can adapt their Dynamics to so that they can extract the the essence of a task and and really find out the the the relationship of an input output relationship of a task based on a mechanism that uh is ruled out by this differential equation here so it has parameters a b and c that they account for internal interventions to the system that means if I change something in the middle of the system there are mechanisms that can control that processes if if something that comes from outside an intervention inside the process of a system then you would actually get into uh you can control those kind of processes with the dynamic causal model now a little bit I wanted to go through the causal modeling process of a neural network basically a differential equation can form a causal structure was what does that mean that means it can predict from the current situation we can predict the future uh one step in the future of a process that's temporal causation and another thing is that if I change something or intervene in a system how can I uh can I actually control that intervention so or can I account for that intervention that I had in the system so this would construct these two points being able to account for future uh evolution of a system and interventions and being able to account for interventions in the systems if you have these two models then you have a causal structure so so for that I'm just skipping over over this part I just want to tell you that I mean I wanted to show you like more about uh how you're driving this uh connection between liquid neural networks and causal models but I share this slide so you can see and also Professor Roots tomorrow is going to give a lecture on the topic that hopefully she can cover these parts and um just a couple of uh remarks on on on the performance of the network and what's the implication of having a causal model now let's look at this environment we train some neural networks that these neural networks are basically um learned to fly towards a Target in an unstructured environment so we collected some data we trained neural networks instances and then we tested this then we take this neural networks that we train by moving towards like basically these are scenes that are a drone is inside the forest and the Drone is navigating towards a Target okay now we collect this data from a couple of traces okay that we simulate then we take this data we train neural networks we bring the neural networks back on the Drone and we test them on the drama to see to see whether they can learn to navigate this task without even programming uh uh what is the objective of the task basically the objective of the task has to be distilled from the data itself and as we saw here on the right we see the decision making process of these networks that liquid networks learned to pay attention to the Target as the target becomes visible that means it actually figured out the most important part of this process was really focusing on the target even if there is an unstructured kind of uh kind of input data to the system now if we compare the attention map of these neural circuit policies which is the first the second column compared to the other attention Maps we see that the only network type of network that learned from this data to pay attention to this target is liquid neural networks and that's the kind of implication that you have so you learn the cause and effect of a task using a liquid neural network that has dramatically less number of parameters compared to other types of neural networks now as I told you the the relation between two neurons can be defined by a differential equation by neuron equation and a synapse equation we recently have solved this differential equation also in closed form and then this gave rise to a new type of neural network which is again a close form continuous time neural networks you call them CFC these are the closed form liquid networks they have the same behavior but they are basically defined in closed form now what does that mean that means if I have a differential equation you see the ode on the top if I simulate this ode okay the closed form solution would actually give you the very same behavior of the differential equation itself but it's it does not require any numerical solvers so it does not require any kind of you know complex numerical solvers so as a result you could scale liquid neural networks much larger to much larger instances if you want to scale the networks in terms of performance in modeling Dynamics liquid neural networks we see here a series of different types of advanced recurrent neural networks and we see variants of uh closed form liquid neural networks and ltcs themselves that are performing remarkably better than the other systems in modeling physical Dynamics and the last thing I want to show is the difference between this understanding causality and understanding the cause and effect of a task and not being able to understand that now here you see a drone which is an agent that wants to navigate towards a Target okay there's a Target in the environment now we collected traces of a drone navigating towards this Target in this Forest then what we did we collected this data then we train neural networks and now we brought back these neural networks on the Drone to see whether they learned to navigate towards that Target or not now here I'm showing the performance of an lstm neural network so as we see the lstm is basically completely like looking around and it cannot really control the Drone is actually if you look at the attention map of the system actually looks uh all around the the place it did not really realize what's the objective of the task from data so it could not really associate anything or learn something meaningful from the data and then here is a liquid neural network on the same test look at the attention map here as the Drone is going towards that Target and that's the kind of uh flexible flexibility that you can learn their actual uh cause and effect of a task from neural networks now okay so I'm going to conclude now I showed you this plot I showed you that there is over parametrization regime and we have an idea about what kind of uh benefits you would get in that regime and what kind of intuitive understand theoretical understanding do we have for neural networks and then I showed you that there are neural networks like liquid neural networks that have inductive biases from brains that can actually solve resolve a couple of problems for example the robustness while being significantly smaller than over parameterized networks so it is not that you always have to dramatically over parameterize neural networks but there you can have inductive biases to actually gain good performance to sum up the law of robustness is real so that's something that is came out of a theory is um number of parameters it's it's necessarily has to be large if you want to have with effective dimensionality so one of the ideas that we have I'll talk about effective dimensionality here the over parametrization definitely improves generalization and robustness but it has some socio-technical challenges so inductive biases or architectural motive why do we have different types of architectures and why studying the brain is actually a good thing it's because we can actually get smarter in designing neural networks and not just blindly over parameters over parameterizing them so we can really put incorporating some of some ideas to really get what's going on and liquid neural networks they can enable robust representation learning outside of over parametization regime because of their causal mechanisms they reduce networks per se so that's the speculation that I have I have to we have to actually think about the theoretical implication here but I what I think is that the reason why liquid neural networks can pass or break the the universal law of robustness is because they can extract a better or more minimum uh kind of effective dimensionality out of data so you have data sets so if the if the effective dimensionality get reduced by a parametrized neural network then the law of robustness still holds okay given the number of data and that's the end of my presentation I'm just putting here at the end a couple of resources if you want to get Hands-On with liquid neural networks we have put together like really good documentations and I think you would enjoy to play around with these systems for your own applications thank you for your attention thank you [Applause] 

Robotics is a really cool and 
important direction for the future. I really believe that we are 
moving towards a world where   so many routine tasks are taken off your plate. Fresh produce turns up at your doorsteps 
delivered by drones garbage bins take   themselves out smart infrastructure 
ensures that the garbage gets removed.   Robots help with recycling with 
shelving with cleaning windows. Robots can do so many physical things for 
us. And by the way can you count how many   robots there are in this in this image? 
Anybody wants to want to take a guess   how many robots do I have in this image? Okay that's so that's that's really close! 
It turns out it's 19. So here are all the   robots we have flying robots we have 
cars we have shopping cart robots we   have robots that carry we have robots 
that shelf we have robots that clean. I really believe that in the future we will have 
AI assistance whether they are embodied or not   to act as our guardian angels to provide advice to 
ensure that we maximize and optimize our lives to   live well and work effectively and these agents 
will help us with cognitive and physical work.   And so today we can say that with AI we we will 
see such a wide breadth of applications for   instance these technologies have the potential to 
reduce and eliminate car accidents they have the   potential to better diagnose monitor and treat 
disease as you have seen in some of the previous   lectures we these technologies have the potential 
to keep your information private and safe to   transport people and goods more effectively and 
faster and cheaper to really make it easier to   communicate globally by providing instantaneous 
translations to to develop education to everyone   to allow human workers to focus on big picture 
tasks with machines taking on the routine tasks   and so this future is really enabled by three 
interconnected fields and on one hand we have   robots now robots I like to think of robots as 
as the machines that put computing in motion   and give give our our machines in the world the 
ability to navigate and to manipulate the world   we have artificial intelligence which enables 
machines to see to hear and to communicate and   to make decisions like humans and then we have 
machine learning and to me machine learning is   about learning from and making predictions on data 
and this uh this kind of application of machine   learning is Broad it it applies to cognitive tasks 
it applies to physical tasks but regardless of the   task we can characterize how machine learning 
works as using data to answer questions that   are either descriptive predictive or prescriptive 
so you can look at data to see what happened or   to see what is this you can look at data to see 
what will happen in the future and what the world   might look like in the future or you can use 
data to ask where should I go what should I do So when we think about these questions 
in the context of a robot we have to we   have to kind of get on the same 
page about what a robot is and   um and so think of a robot as a programmable 
mechanical device that takes input uh with its   sensors reasons about this input and then 
generates an action in the physical world   and robots are made of a body and the Brain the 
body consisting of actuators and sensors determine   the range of tasks that the robot can do so the 
robot can only do what its body is capable of   doing a robot on wheels will not be able to do the 
task of climbing stairs so we have to think about   that body and we have at T Cell we have a lot of 
machine learning based research that allows us to   um that that examines how to design 
optimally a robot body for a particular task   now in order for the body to do what it's meant to 
do we need the brain we need the machine learning   and the reasoning and the decision making engine 
and this is what we are going to talk about today   now in the context of robots we have three types 
of learning and you have seen different aspects   of these methodologies throughout the course we 
have supervised learning and so in this uh in   this method of learning we use data to have 
to find the relationship between input and   output we have unsupervised learning and in 
the context of unsupervised learning we can   use data to have patterns to find patterns 
and classifications of the data and then   we have reinforcement learning which is about 
learning to perform a task by maximizing reward   so for robots we end up with a cycle that 
most often consists of three steps we have   perception a perception step we have a planning 
and reasoning step and we have an action step   and so this is what we are going to 
talk about today so let me start with   some examples of how we can use machine learning 
to enhance the perception capability of robots   and so this is addressing the question what 
is this and this question is really important   because for instance uh training robot cars to 
recognize all objects on roads including Ducks   cars people is really critical uh for autonomous 
driving and um so how does this work well let me   let me give you a high level view of how a 
robot car can actually recognize the scene so in order to in order to use deep learning 
for the perception task of robots we use data   this is manually labeled data that gets fed 
into a convolutional neural network and then   the labels are used to classify what the data 
is so for instance for this image we may have   classifications like car duck Road and we do this 
so that when the system when the car sees a new   image for example this one the car could say oh 
this is a this is ducks on road now in order to   in order to actually provide solutions for this 
object classification problem we have to we   have to employ multiple algorithms and the first 
algorithm that we have to employ is called image   segmentation in image segmentation we take as we 
take inputs as input images and we group together   the pick pixels that belong to the same object in 
the image and this is a kind of a lexical step and   then we need to label and recognize these images 
this is a semantic step and so the exciting thing   is that we already have very good algorithms 
that that can segment images very fast so we   can we can do this we can do this so we can we can 
take an image and we can find the object in the   image very fast we don't know what the objects 
are so in order to know what the objects are   well you know what we do right so we employ 
thousands of people to label the objects that   we have and so this is exciting but it but 
labeling is a very labor intensive activity   and a significant challenge to machine 
learning let's keep this thought for later   now the part the most popular Benchmark for 
measuring the accuracy of image classification   is imagenet and here we see the leaderboard 
of imagenet and we see performance of various   variations of of image classification algorithms 
that perform well into 90 90 accuracy and this is   really quite exciting it's exciting but in the 
but if those algorithms were to run on a car   that's not good enough because because the 
car is a safety critical system and in fact we   cannot afford to have any errors in how images get 
recognized in the car on the car here's an example   of of an object object detector that is running 
on a on a car and we we see the image from three   cameras there is a camera pointing forward there 
is a camera pointing to the left there is a camera   pointing to the right and so you can see that the 
car takes this this input and it does pretty well   see there is a blue car there are some bicyclists 
there are there is a bicyclist here's another car   so the system does pretty well and manages to find 
the Gap in the road to make a turn but it's not a   hundred percent incorrect so in particular there 
is this moving truck and when this moving and when   the image of the moving truck is passed through 
the object recognition piece of the system we   um we end up with a lot of interesting things the 
writing is recognized as a fence and so these are   these are the kinds of extreme errors um or um the 
way um we we denote them Corner cases that we need   to pay attention to when we train machine learning 
for safety critical applications like driving   but it turns out that in fact if you use a deep 
neural network solutions for image classification   um the the solutions work really well because 
they are trained on a huge data set imagenet   but the solutions capture more than the essence of 
the object the solutions also capture the context   in which the object appears and so MIT researchers 
led by George Tannenbaum and Boris Katz did an   experiment a few years ago where they took regular 
objects and they put them in a different context   so for instance they took your shoes and put them 
on a bed they took the pots and pans and put them   in the bathroom and with this significant change 
in context the performance of the top performing   imagenet algorithms dropped by as much as 40 
to 50 percent which is really extraordinary   and I'm sharing this with you not to discourage 
you from using these algorithms but to point   out that when you when you deploy an algorithm and 
especially when you deploy it in a safety critical   application it's important to understand the scope 
of the algorithm it's important to understand what   works and what doesn't work when you can apply the 
algorithm and you when you shouldn't apply the the   algorithm and so um so keep this in mind as you 
think about deploying or building and deploying   deep neural network Solutions there's another 
thing um that is very critical for for autonomous   driving and and for robots you have heard a 
beautiful lecture on adversarial attacks well it   turns out you can attack very easily the images 
that get fed from the camera streams of cars   um to the decision-making engine of the car and 
in fact it's it's quite easy to take a stop sign   and perturb it a little bit perturb it in uh in 
such a way that you can't even tell with a with a   naked eye that there is a difference between the 
two images and with all small perturbations you   can turn the stop sign into a yield sign and you 
can imagine what kind of chaos this would create   on a on a physical Road so machine learning is 
very powerful for building perception systems   for robots but as we employ machine learning in 
the context of robots it's important to keep in   mind the scope when they work when they don't work 
and and then it's important to think about what my   what kind of guard rails we might put in place at 
the decision time so that we have robust Behavior   so what could we do about the possibility of 
adversarial perturbations on the stop sign   well let's talk a little bit about decision 
making and let's talk about how the car figures   out what to do given input reinforcement learning 
is causing a huge revolution in robotics and so um   and and why is that well the reason reinforcement 
learning is causing a huge revolution in robotics   is because we have built fast simulation systems 
and simulation methodologies that allow us to run   thousands of simulations in parallel in order 
to train a reinforcement learning policy and   we are also uh decreasing the gap between the 
hardware platforms and the simulation engines   so you have seen reinforcement learning earlier in 
the in the boot camp and so reinforcement learning   is concerned with how intelligent agents ought 
to take action in an environment in order to   maximize the notion of a cumulative reward and 
so um reinforcement learning is really about   learning to act and this differs from supervised 
learning and not needing a labeled input or in   not needing labeled input output pairs so in this 
example the agent has to learn to avoid fire and   it's very simple it gets a negative reward if it 
goes to Fire and it gets a positive reward when   it gets to water and that's essentially what what 
this approach is like you you do trial and error   and and eventually the positive rewards dominate 
the negative rewards and that that directs the the   agent towards the the best action and so here is 
an example where we have a reinforcement learning   agent that is trying to drive on a race track and 
you can see that it starts off and it initially   it makes mistakes but eventually it learns 
how to how to take the turns at high speeds   and interestingly you can do that you can take 
this idea and you can run it in parallel so you   can take thousands of cars and you can you can put 
them on the same track and in the beginning they   all make mistakes but eventually they get the 
solution right and eventually that joint policy   ends up being the policy that reliably controls 
the the vehicles so it's very it's really a very   exciting area reinforcement learning much like 
deep learning has been invented decades ago   but it works well now because of the Advent of 
of computation we have so much more compute today   than we had 40 years ago we have so much more 
data for deep annual networks today than we had   40 years ago and so these techniques that did not 
do so well back then all of a sudden are creating   extraordinary possibilities and capabilities 
in our agents now this is a simple simulation   in order to get the simulation to drive a 
real robot we actually need to think about   the Dynamics of the robot and so in other words 
we have to take into account what the vehicle   looks like what are its kinematics what are 
its Dynamics and so here is a vehicle that   is running the policy learned in simulation so 
it's really cool because really we are now able   to train in simulation and if the model that we 
have for the vehicle Dynamics is good enough we   can take the policy that was learned very fast in 
simulation and we can deploy it on the vehicle and   here are two vehicles racing with each other 
the white car and the green car so check out   what the white car is doing see how it snuck wow 
it's really great I wish I could race like this   so this is really a really exciting now in this 
case the vehicles have limited field of view and   they get the position of the other vehicles on the 
track but only only so they get this position from   an external localization system but they only 
know where the vehicles within their field of   view are so look at this that's great so okay 
so what can we do with with these methodologies   um so we've seen how we can use deep learning to 
understand the um the view of the vehicle from   cameras uh we've seen an example of learning uh to 
steer what can we do well I think that these these   advancements in robotics are really enabling the 
possibility that you saw in the first Slide the   possibility of creating many robots that can do 
many tasks and much more complicated tasks than   what we see here and so what I want to talk about 
uh next is the autopilot how do we take these   pieces together to enable the autopilot meaning to 
enable a self-driving vehicle I don't mean so this   autopilot is not the Tesla autopilot is it's just 
the idea that you have a full self-driving vehicle   now in order to do this we need to advance 
the brain more we need to do more about   the learning reasoning and planning part of 
the car so let me uh let me ask you do you   know when was the first autonomous Coast to 
Coast Drive in the United States any guesses not you I know you know sorry two two thousand interesting uh interesting 
well actually it was in 1995. in 1995 A Carnegie   Mellon project called nav lab um built a car 
that um that actually was was driven by a machine   learning engine called Alvin and Alvin drove this 
car all the way from Washington DC to Los Angeles   and the car was in autonomous mode for a large 
part of the highway driving but there was always   a student there right ready to um to take 
control and the car did not did not drive   in autonomous mode when there were when it was 
raining or when there was a lot of congestion or   when the car had to had to take exits so this is 
what the car did it went from Washington DC all   the way to Los Angeles now 1995 is a long time 
ago right I mean it's before many of you were   born so it's really extraordinary to think about 
what is needed uh in terms of of advancement in   terms of progress in order to get from where 
we were back then to the point where we can   actually see deployed autonomous vehicles and 
by the way you should come and check out the   MIT autonomous vehicles which Alexander has built 
over the past five years which are very powerful   and can drive in our neighborhood and we'll 
talk about how they drive but interestingly   this was not the first time when we had cars 
racing in autonomous modes on on highways in   fact do you know when was the first autonomous 
Highway Drive in the world anywhere in the world all right so it was in 1986. and in 1986 German 
engineer Ernst Dickman started thinking about how   he could turn his van into an autonomous vehicle 
and so he put computers and cameras on the van and   began running tests on an empty section of the 
German Autobahn which had not been open for for   um for for public driving and he was able to 
actually get his van to drive on that empty road   but uh interestingly when he 
started developing this work   um computers needed about 10 minutes to analyze 
an image can you imagine okay so how do you   how do you go from that to enabling an autonomous 
vehicle to drive at 90 kilometers an hour   well um what they did was they they developed some 
very fast solutions for paring down the image to   only the the the aspects that they needed to look 
at and they assumed that there were no obstacles   in the world which made the problem much easier 
because all the car had to do was to stay on on   the road so it's really super interesting to think 
about how visual processing improved from one   frame per 10 minutes to 100 frames per second and 
this has been a game changer for autonomous cars   and we're getting back to the connection between 
hardware and software we need both in order to get   good solutions for real problems so um the other 
thing that happened in autonomous driving was that   the lidar sensors decreased the uncertainty and 
increased safety and today we have many companies   and groups that are deploying um self-driving cars 
this is an example from Singapore it's uh it's a   vehicle we deployed and in fact we had the public 
ride our vehicle in 2014 we have vehicles at MIT   we have a lot of other groups that are developing 
these vehicles now before we had lidar we had   sonar and nothing worked when we had sonar because 
when you when you work with sonar the sonar beams   just kind of go forward and then they bounce and 
if the angle is about plus minus seven degrees   you will hear the Ping back but if the angle so 
if you're if if the sonar bounces on a Surface   that's more than seven degrees angled from 
from the direction of the sonar that sonar   ping will bounce off and it will bounce on other 
objects and walls and you will get wrong direction   measurements wrong distance measurements so with 
lidar that problem went away so all of a sudden   a powerful accurate sensor made a huge difference 
all the algorithms that were developed on Sonar   and didn't work started working when the 
later was introduced it's really exciting um okay now when we think about autonomous 
driving there are several key parameters that   emerge as we think about what the capabilities 
of these systems are one one question how complex   is the environment where the car t Road like in 
the German case then the problem is much easier   then we have to ask ourselves how how complex 
are the interactions between the car and the   environment and we also have to think about 
how complex is the reasoning of the car how   fast is the car going and underlying all these 
questions is a fundamental question and this   fundamental question is how does the car cope 
with uncertainties now you have seen that machine   learning has uncertainty associated with it so as 
you consider deploying machine learning on safety   critical applications it is super important to 
consider the connection between um between your   your context the uncertainties of the models that 
you're deploying and what the actual application   requires I will tell you that today we have very 
effective and Deployable solutions for robot cars   that move safely in Easy environments where 
there aren't many static nor moving obstacles   and you can you can see from this example this is 
this is an example of the MIT car and it's you can   see this this car operating autonomously without 
any issues at Fort Devens where there aren't too   many obstacles and the car is perfectly capable 
of avoiding the obstacle by the way that's my car   so I'm very glad that the car is very capable 
of uh of avoiding obstacles and in fact I was   so convinced I said okay we can use my car as 
the obstacle but the sensors don't work well   in weather and the uncertainty of the perception 
system increases significantly if it rains hard or   it snows and the uncertainty of the vehicle 
prior also increases in the case of extreme   congestion where you have erratic driving with 
vehicles with people with scooters even with cows   on the road and this is a video I took during 
a taxi ride in Bangalore there come the cows so um there are so many important preconditions 
and many of these preconditions revolve around   certainty in perception planning learning 
reasoning and execution before we can get   to Robo taxi but we can have many other robot 
solutions that are much that that can happen today   and so I want to tell you that many companies 
and research teams are deploying and developing   self-driving cars and many of them follow a 
very simple solution which you can adopt and   turn your car into a self-driving 
car so here's what you have to do   you take your car you extend it to drive 
by wires so that your computer can talk to   um to the steering and the acceleration the 
throttle controls so then you'll further extend   this car with sensors and most of the sensors 
we use are cameras and lidars and then there   are Suite of software models modules and this 
includes a perception module that provides support   for making maps and for detecting a static and 
dynamic obstacles and then we have an estimation   module that identifies where the robot is located 
and it does so by comparing the what the what the   perception system sees now against a map that 
was created by looking at the infrastructure   and finally there is a learning planning and 
control system that figures out what the car   should do based on where it is so this is it this 
is the recipe you can take your vehicle and turn   it into an autonomous vehicle and as you do so 
you really have to think about foundationally   what are the computational units that 
you have to make you you have to create   so you have to process the sensor data you have 
to detect obstacles you have to localize the   vehicle you have to plan and then you have 
to move and so there are so many works that   address each of these subtasks that are involved 
in autonomous navigation and and some of these   works are model based and some of the works 
are machine learning based but what's really   interesting is that in this autonomous driving 
pipeline the classical autonomous driving pipeline   there are a lot of parameters so every for every 
solution of each of these individual problems   you you have to hand engineer parameters for any 
type of Road situation that the car will encounter   and then you will have to think about how the 
modules get stitched together you need to Define   parameters that connect the modules together and 
this is very tough to do in a robust way and it   brings brittleness to these Solutions in fact you 
have to really think about what are the parameters   if you have nighttime driving or if you'd have 
rainy weather or you're you're on in the country   on a country road or in the city on a city 
road or you're on a road with no Lane markings   so these are these are really challenging things 
that that the first solutions for autonomous   driving had to had to reason through now in 
Alexander's PhD thesis his idea was to utilize   a large data set to learn a representation 
of what humans did in similar situations and   develop autonomous driving solutions that drive 
more like humans than than the the traditional   pipeline which is much more roboticy if you 
if so um so then the question is how can we   use machine learning to go directly from sensors 
to actuation in other words can we compress all   the stuff in the middle and use learning 
to connect directly perception and action so the solutions that we employed build on things 
we have already talked about we can use deep   learning and reinforcement learning to take us 
from from images of ofroads onto steering and and   throttle onto what to do so this is really great 
because you can train on certain kinds of Roads   and and you can then take your vehicle and 
put the vehicle in completely different   driving environments and driving situations 
and you don't need new parameters you don't   need retraining you can go exactly 
directly to what the car has to do   so in other words we can learn a model to go from 
raw perception and here you can think of this as   pixels from a camera and the other thing we feed 
the vehicle is noisy Street View maps so these   are not the high definition maps that are usually 
created by autonomous driving labs and companies   and so you can do this to directly infer a 
full continuous probability distribution over   the space of all control and here the red lines 
indicates the inferred trajectories of the vehicle   projected out onto the image frame and the opacity 
represents the large density function by our model   and so this is done by training a deep 
learning model and the Deep learning model   can output the parameters of this conditional 
distribution directly from Human driving data okay so um more precisely the input to our 
Learning System consists of camera feeds from   three cameras camera that looks forward and two 
cameras that look sideways and also the the street   view maps and from this from this data this data 
is processed and it's from this data we can learn   to maximize the likelihood of particular control 
signals for particular situations and amazingly   the solution also allows us to to localize 
the the vehicle so it's really super exciting okay so we can we can we can get this human-like 
control but assuming light control requires a   lot of data and I've told you at the beginning 
that we have to be careful with the data because   there are a lot of corner cases there are a 
lot of typical near accident situations for   which it's difficult to generate real data um so 
for instance let's see for instance if you have   if you want to ensure that the car will not be 
responsible for an accident and will know what   to do when it comes to Road situation like this 
it'd be pretty expensive to take a car and crash   that car in order to generate the data so instead 
what we do is we do the training in simulation   and so Alexander developed the Vista 
simulator and the Vista simulator can   model multiple agents multiple types of sensors 
and multiple types of agent to agent interaction   and and so the the Vista simulator 
has been recently open sourced you   can get the code from vista.csel.mit.edu and 
a lot of people are already using the system   so what we get from Vista are is the ability to 
simulate different physical sensing modalities   that means including 2D cameras 3D lidar event 
cameras and and so forth and then you get the   possibility to to simulate different types of 
environmental situations and perturbations you   can simulate weather you can simulate different 
lighting you can simulate simulate different types   of roads and you can also simulate different types 
of of interactions so here's how we use Vista we can take one high quality a data set taken from 
a human-driven vehicle we can take this very high   definition data set and then in simulation we can 
turn it into anything we want we can turn it into   erratic driving we can turn it into near accident 
simulation situation we can turn it into anything   anything you want so for instance here you can 
see our original data and you can see how this   original data can be mapped in simulation in 
a way that looks very realistically into a new   simulated trajectory that is erratic and that now 
exists that's part of our training set in Vista   and so we can we can do this and we can use 
this data and then we can learn a policy we can   evaluate this policy offline and ultimately 
deploy it on the vehicle and this works by   um first updating the state of all agents based 
on the vehicle Dynamics and interactions and then   by recreating the world from the new viewpoints 
of the agents once you move them the the world   will look different to the agent than than 
in the original driving data set and finally   we can we can then render the image 
from the different agents viewpoints   and we can perform the the control so um so 
this is really cool there are several other   simulation engines and there are simulation 
engines that rely on on imitation learning   um or on domain randomization or there's car 
lab that's very effective that seemed to real   now however our solution works better than all the 
other Solutions and here you can see the results   of comparing what happens in Vista with the um 
with what happens in the existing simulators in   the state of the art so the Top Line shows crash 
locations in red and the bottom line shows mean   trajectory variation in color and you can you can 
see that our solution really does the best and in   fact the solution is able to do things that other 
simulation based learning based control cannot do   for instance in our solution we are able to 
recover from orientations that point the vehicle   off the road or we are able to to recover 
from being in the wrong in the wrong lane   so here's the vehicle that is executing the 
learning based control and here's Alexander with   his vehicle that was trained using data from Urban 
driving and now he's driving to the soccer field   and you can see that he he's able to drive to 
to get this car to drive him to the soccer field   without doing any training without ever having 
seen this road and explicitly providing data   about this road so this is pretty cool right all 
right so um okay I'm gonna open the hood for you   and so I'm going to show you what happens inside 
the decision engine of this solution so let me   Orient you in this image bottom right you see the 
map of the environment top left you see the camera   input stream bottom left you'll see the attention 
map of the vehicle and then in in the middle you   see the decision engine the decision engine has 
about a hundred thousand neurons and about a half   a million parameters and I will challenge you to 
um to figure out if there are any patterns that   associate the state of neurons with the behavior 
of the vehicle it's really hard to see because   there are so many of them there's just so much 
stuff that is happening in Peril at at the same   time and then have a look at the attention map so 
it turns out this vehicle is looking at the bushes   on the roads in order on the road in order to make 
decisions still it seems to do a pretty good job   but we asked ourselves can we do better can we 
have more reliable uh learning based Solutions   and and so yesterday Ramin introduced liquid 
networks and introduced neural circuit policies   and so I just want to drill down a little bit 
more into this area because you can now compare   you cannot understand how the the original engine 
worked and you can compare that against what we   get from liquid Networks and so look at this we 
have 19 neurons and now it's much easier to look   at patterns of activation of these neurons and 
Associate them with the behavior of the vehicle   and we the attention map is so much cleaner right 
the vehicle is looking at the road Horizon and at   the sides of the road which is what we all do 
when we drive a vehicle now remember how um so   remember that Ramin told us that this um this 
model called liquid time constant network is a   continuous time Network and this this model uh 
changes what the neuron computes and in particular   we start with a well-behaved state-space model 
to increase the neuron stability during learning   and then we have non-linearities over the synaptic 
inputs to increase the explosivity of the model   and to also increase the model State 
during training and inference and by   plugging these equations into each other 
we can see the equation of the LTC neuron   where here the function is where the function 
here determines not only the state of the neuron   but also this function can be con is controlled 
by new input at at the time of of execution of   inference so what's really cool about this model 
is that it is able to dynamically adapt after   training based on the inputs that it sees and this 
is something very powerful about liquid Networks   now in addition to changing the neuron equation we 
also change the wiring and this new type of wiring   essentially gives function to the neurons in a 
deep neural network every neuron is the same in   our architecture we have input neurons we have we 
have control neurons we have interneurons and they   each do different things and so with this in mind 
we can look again at the beautiful solution that   is enabled by liquid networks and the solution 
keeps the car on on the roads and only requires 19   neurons to deliver that kind of function and you 
can see here that the attention of the solution is   extremely focused as compared to other models like 
CNN or ctrnn or lstm which are much more noisy so   I hope you now have a better understanding of how 
liquid networks work and what their properties are   now we can take this model and apply it to many 
other problems and so here is a problem we call   Canyon Run where we have taken a liquid Network 
and we have implemented it on a task of flying a   plane with one degree of Freedom where the plane 
can only go up and down but it has to hit these   obstacles which are at locations that the the 
plane does not know and the plane also does not   know what the environment looks like and so in 
particular when you have we have uh when when   you have the when you implement the task with one 
degree of Freedom control for the plane all you   need is 11 liquid neurons if you want to control 
all the degrees of freedom of the plane then you   need 24 neurons it's still much smaller than 
the huge models that we're talking about today   here's another task we call drone dodgeball 
where the objective is to keep the a drone at   a specified location and the Drone has to protect 
itself when um when balls come its way and you   can see a two degree of Freedom solution to drone 
dodgeball and that's the network you can see how   it how all the neurons fire and you can really 
associate the the function of of this controller   of this learning based control with activation 
patterns of the neurons and so very excited   because in fact we're able to extract decision 
trees from these kinds of solutions and these   decision trees provide human understandable of 
human understandable explanations and so this is   really important for safety critical systems all 
right um so um let's see um Ramin told you that   these liquid networks are Dynamic causal models 
and I want to show you some more examples that   um that explain how these models are Dynamic 
ozone models so here you can see that so here   we're studying the task of finding an object 
inside a wooded environment and the object and   here here are some examples of the data that 
we have used to train this test so essentially   we've had a human pilot drive a drone to um to to 
accomplish the task now this is the data so check   this out we have then used a standard deep neural 
network and we have asked this network to solve   this problem and the attention map of the network 
is really all over the place you can see that the   network the the Deep neural network solution is 
very confused but check out something else the   data that we collected was summertime data and now 
it's fall so the background is no longer green we   have we don't have as many leaves on trees and so 
the context for this task has completely changed by comparison the the liquid network is able 
to focus on the task is not confused and is   able to go directly to the object that it needs 
to find and look how clean the attention map of   this of this example is so this is very exciting 
because it really shows that with liquid networks   we have a solution that is that is able to 
in some sense abstract out the context of   training and that means we can get zero shots 
transfers from one environment to another   and so moreover we actually have done the same 
task in the middle of the winter when we no longer   have leaves we have black tree lines and the 
environment looks much much different than than   the environment where we trained and this kind of 
this kind of ability to transfer from one set of   training data to completely different environments 
is is truly transformational for the capabilities   of machine learning well we've done more than that 
so we've taken our our trained solution and we   deployed it in the lab so um so here is um uh here 
is uh macrame who worked on this problem and he is   um and look at the attention map I mean he's the 
environment is not not even the woods uh it's a   it's a it's in it's an office it's an indoor 
environment and we see other examples where   we take our solution and we deploy it to find 
the same object the chair just outside of the   stata building and this is uh the the Deep neural 
network solution that gets completely confused and   here is the liquid Network solution that has the 
exact same input and has no problem uh going to   um to the the going to the robot let's see 
a few more examples here where we are doing   um uh hop by hop we're actually 
searching the object and doing   multi-step Solutions and in fact in fact 
we can if I can get to my next video   I'm sorry so um I am the next one is it shows you 
that we can actually do this forever so here is   an infinite hop demo that was done just outside 
on the baseball field and we we placed uh three   of the same objects that we trained on and we 
placed them at unknown locations and we added   the the search feature to our machine learning 
solution and so we can the system can go on and   on and on hopping from one to the other the final 
example I will show you is in is on the patio of   the stata building where we have put a number of 
chairs we have put our favorite chair but also   we have put a lot of other other similar chairs 
and we can see that liquid networks generalize   very well whereas if we take an lstm solution 
it gets confused and goes to the wrong object   so all of these ideas come together to really 
point to a new type of machine learning that   yields models that generalize to unseen scenarios 
essentially addressing a challenge with today's   neural networks that do not generalize well to 
unseen test scenarios because the models are so   fast and and compact you can train them online you 
can train them on edge devices and you can really   see that they are beginning to understand the 
tasks that they're given so you can see that we're   really beginning to get at the semantics of what 
these systems what these systems have to do so   what does this have to do with the future well I 
I think it's so exciting to use machine learning   to study nature and to begin to understand the 
nature of intelligence and we in in our lab here   at C cell we have one project that is looking 
at whether we can understand the lives of whales   and so what do I mean by this so here is an 
example where we have used a robotic drone to   um to go very sorry this is uh this is very loud we have used the robotic drone to find whales 
and look at what they do and track them   and here is some Imaging and here's some some 
clips from what this what the system is able to   do we have used machine learning to identify 
the whales and then once you have identified   the whale we can actually Servo to the center of 
the whale essentially tracking the whale along   the way and here is how how the system works you 
can um you can see a bunch of a group of whales   and you can see our robot servoing and following 
the whales as they move along now um we this is a   very exciting project because whales are such 
Majestic intelligent and mysterious creatures   and so if we can use our Technologies to get 
better insights into their lives we will be able   to to understand more about about other animals 
and other other creatures we share this beautiful   planet with so we can study these whales from 
above from the air we can also study the whales   from from within from from inside the uh the Ocean 
and here's a here's Sophie our soft robotic fish   um which Joseph who is with us today has 
participated in in building and here is   this beautiful beautiful very natural uh moving 
robot that can get close to aquatic creatures   that can move in the same way aquatic creatures 
do without without disturbing them when you put   thruster-based robots in Ocean environments they 
behave differently than than the fish do and they   they tend to scare the fish so if you're curious 
um the the tail is made out of silicone and there   is a pump that can that can pump water in two of 
its Chambers so you see it has two ripped Chambers   and you can move water from one chamber to the 
other and depending on how much water we move   and in what proportions you can get the fish 
to move forward so turn left or to turn right   so we can observe the motion of animals 
using robotic Technologies but we can do   more we can also listen in on what oops 
actually I need sound here I forgot about   this we can observe the the whales and we 
can observe what they say to each other foreign so this is a sperm whale   and you have heard the vocalization of sperm 
whales we believe that they're talking that   the sperm whale is talking to its family and 
friends and we would like to know what it's saying we have no idea but we can use 
machine learning to make progress   and the way we can do that is by is by by using 
data using the kind of the kind of data you have   just heard to look for to look for the presence 
of language which is a major sign of intelligence   we can look at whether we have discrete compounds 
we can look at whether there might be grammar we   can look at whether we have long range 
dependencies we can look at whether we   have other properties that human language has 
and so um basically our project is very much   work in progress I can't tell you today what 
the whales are saying to each other but I can   tell you that we have made progress I can tell 
you that we are beginning to find which parts of   their calls carry information we can use machine 
learning to differentiate the clicks that allow   the whales to echolocate from the clicks that seem 
to be vocalization and information carrying clicks   we can begin to look at what the protocols 
for information exchange are how do they   engage in dialogue and we can begin to ask what 
is the information that they say to one another   so with our project we are trying 
to understand the phonetics   the semantics and the syntax and the discourse 
for whales so we have a big data set consisting   of about 22 000 clicks uh the clicks get grouped 
into codas the codas are like the phonemes   and using machine learning we can identify 
coded types we can identify patterns for   Coda exchanges and we can we can begin to really 
ask ourselves how is it that that Wales exchange   information and if you're interested in this 
problem please come see us because we have   a lot of projects that are very very exciting 
and important towards reverse engineering what   this really extraordinary and majestic animal is 
capable of doing so let me close by saying that   in this class you have looked at a number of 
really exciting machine learning algorithms but   you have also looked at what some of the technical 
challenges with a machine learning algorithms are   including data availability including data 
quality including the amount of computation   required the model size and the ability of that 
model to run on edge devices or on huge devices   uh you have seen that many of our Solutions are 
Black Box Solutions and sometimes we have brittle   function we have we have easily attackable models 
you have also seen some alternative models like   liquid networks which attempt to address some of 
these questions there is so much opportunity for   developing improved machine learning using 
existing models and inventing new models   and if we can do this we can create an exciting 
work world where machines will really Empower us   will really augment us and and enhance us in our 
cognitive abilities and in our physical abilities   so just imagine waking up enabled by your 
personal assistant that figures out the   optimal time and helps you organize all the items 
that you need for the day and then brings them to   you so you don't have to think about whether 
your outfit matches and as you walk by a store   the image in the store window displays your 
picture with the latest fashion on your body   and inside the store maybe you want to buy a 
certain shoe an AI system can analyze how you walk   can analyze your your dimensions and can create 
a bespoke shoe a bespoke model just for you and   then all the all the clothing all the items in our 
environment can kind of awaken our clothing could   become robots and so our clothing could become 
monitoring devices but they could also become   programmable for instance in this case you can see 
the ability of a sweater to change color so that   the girl can match her friend now this is actually 
not far-fetched we have a group on campus that   is delivering these programmable fibers that can 
change color and that they can do some computation   at work inside the intelligent boardroom the 
temperature could get adjusted automatically   by monitoring people's comfort and gesture and 
just-in-time Holograms could be used to make the   virtual world much more much more realistic much 
more connected and so here they're discussing the   the design of a new flying car and let's say we 
have these flying cars and then we can integrate   these cars with the it infrastructure and the cars 
will know your needs so that they can tell you for   instance that you can buy the plants you have 
been wanting nearby by Computing a small detour   and back at home you can take a first ride 
on a bike and the bike itself becomes a robot   with adaptable wheels that appear and disappear 
according to your skill level you can have robots   that help with planting you have you can have 
delivery robots and there's the garbage ban the   garbage bin that takes itself out and after a good 
day when it's time for a bedtime story you can   begin to enter the story and control the flow and 
begin to interact with the characters in the story   these are some possibilities for the kind 
of future that machine learning artificial   intelligence and robots are enabling and I'm 
personally very excited about this future with   robots helping us with cognitive and physical 
work but this future is really dependent on very   important new advancements that will come from all 
of you and so I'm so excited to see what you'll   be doing in the next years in the years ahead so 
thank you very much and uh come come work with us 

okay good afternoon everyone and thank you all for joining today i'm super excited to welcome you all to mit 6s191 introduction to deep learning my name is alexander amini and i'm going to be your instructor this year along with ava soleimani now 6s191 is a really fun and fast-paced class and for those of you who are not really familiar i'll start by giving you a bit of background on on what deep learning is and what this class is all about just because i think we're going to cover a ton of material in today's class and only one week this class is in total and in just that one week you're going to learn about the foundations of this really remarkable field of deep learning and get hands-on experience and practical knowledge and practical guides through these software labs using tensorflow now i like to tell people that 6s 191 is like a one week boot camp in deep learning and that's because of the amount of information that you're going to learn over the course of this one week and i'll start by just asking a very simple question and what is deep learning right so instead of giving you some boring technical answer and description of what deep learning is and the power of deep learning and why this class is so amazing i'll start by actually showing you a video of someone else doing that instead so let's take a look at this first hi everybody and welcome to mip fitness 191 the official introductory course on deep learning taught here at mit reflecting is revolutionizing so many views from robotics to medicine and everything in between you'll learn the fundamentals of this field and how you can build some of these incredible algorithms in fact this entire speech and video are not real and were created using deep learning and artificial intelligence and in this class you'll learn how it has been an honor to speak with you today and i hope you enjoy the course so in case you can tell that video was actually not real at all that was not real video or real audio and in fact the audio you heard was actually even purposely degraded even further just by us to make it look and sound not as real and avoid any potential misuse now this is really a testament to the power of deep learning uh to create such high quality and highly realistic videos and quality models for generating those videos so even with this purposely degraded audio that intro that we always show that intro and we always get a ton of really exciting feedback from our students and how excited they are to learn about the techniques and the algorithms that drive forward that type of progress and the progress in deep learning is really remarkable especially in the past few years the ability of deep learning to generate these very realistic uh data and data sets extends far beyond generating realistic videos of people like you saw in this example now we can use deep learning to generate full simulated environments of the real world so here's a bunch of examples of fully simulated virtual worlds generated using real data and the power and powered by deep learning and computer vision so this simulator is actually fully data driven we call it and within these virtual worlds you can actually place virtual simulated cars for training autonomous vehicles for example this simulator was actually designed here at mit and when we created it we actually showed the first occurrence of using a technique called end-to-end training using reinforcement learning and training a autonomous vehicle entirely in simulation using reinforcement learning and having that vehicle controller deployed directly onto the real world on real roads on a full-scale autonomous car now we're actually releasing this simulator open source this week so all of you as students in 191 will have first access to not only use this type of simulator as part of your software labs and generate these types of environments but also to train your own autonomous controllers to drive in these types of environments that can be directly transferred to the real world and in fact in software lab three you'll get the ability to do exactly this and this is super exciting addition to success one nine this year because all of you as students will be able to actually enter this competition where you can propose or submit your best deep learning models to drive in these simulated environments and the winners will actually be invited and given the opportunity to deploy their models on board a full-scale self-driving car in the real world so we're really excited about this and i'll talk more about that in the software lab section so now hopefully all of you are super excited about what this class will teach you so hopefully let's start now by taking a step back and answering or defining some of these terminologies that you've probably been hearing a lot about so i'll start with the word intelligence intelligence is the ability to process information take as input a bunch of information and make some informed future decision or prediction so the field of artificial intelligence is simply the ability for computers to do that to take as input a bunch of information and use that information to inform some future situations or decision making now machine learning is a subset of ai or artificial intelligence specifically focused on teaching a computer or teaching an algorithm how to learn from experiences how to learn from data without being explicitly programmed how to process that input information now deep learning is simply a subset of machine learning as a whole specifically focused on the use of neural networks which you're going to learn about in this class to automatically extract useful features and patterns in the raw data and use those patterns or features to inform the learning tasks so to inform those decisions you're going to try to first learn the features and learn the inputs that determine how to complete that task and that's really what this class is all about it's how we can teach algorithms teach computers how to learn a task directly from raw data so just be giving a data set of a bunch of examples how can we teach a computer to also complete that task like the like we see in the data set now this course is split between technical lectures and software labs and we'll have several new updates in this year in this year's edition of the class especially in some of the later lectures in this first lecture we'll cover the foundations of deep learning and neural networks starting with the building blocks of of neural networks which is just a single neuron and finally we'll conclude with some really exciting guest lectures were and student projects from all of you and as part of the final prize competition that you'll be eligible to win a bunch of exciting prizes and awards so for those of you who are taking this class for credit you'll have two options to fulfill your credit requirement the first option is a project proposal where you'll get to work either individually or in groups of up to four people and develop some cool new deep learning idea doing so will make you eligible for some of these uh awesome sponsored prizes now we realize that one week is a super short and condensed amount of time to make any tangible code progress on a deep learning progress so what we're actually going to be judging you here on is not your results but other rather the novelty of your ideas and the ability that we believe that you could actually execute these ideas in practice given the the state of the art today now on the last day of class we'll give you all a three-minute presentation where your group can present your idea and uh win an award potentially and there's actually an art i think to presenting an idea in such a short amount of time that we're also going to be kind of judging you on to see how quickly and effectively you can convey those ideas now the second option to fill your grade requirement is just to write a one-page essay on a review of any deep learning paper and this will be due on the last thursday of the class now in addition to the final project prizes we'll also be awarding prizes for the top lab submissions for each of the three labs and like i mentioned before this year we're also holding a special prize for lab 3 where students will be able to deploy their results onto a full-scale self-driving car in the real world for support in this class please post all of your questions to piazza check out the course website for announcements the course canvas also for announcements and digital recordings of the lectures and labs will be available on canvas shortly after each of the each of the classes so this course has an incredible team that you can reach out to if you ever have any questions either through canvas or through the email list at the bottom of the slide feel free to reach out and we really want to give a huge shout out and thanks to all of our sponsors who without this who without their support this class would not be possible this is our fifth year teaching the class and we're super excited to be back again and teaching such a remarkable field and exciting content so now let's start with some of the exciting stuff now that we've covered all of the logistics of the class right so let's start by asking ourselves a question why do we care about this and why did all of you sign up to take this class why do you care about deep learning well traditional machine learning algorithms typically operate by defining a set of rules or features in the environment in the data right so usually these are hand engineered right so a human will look at the data and try to extract some hand engineered features from the data now in deep learning we're actually trying to do something a little bit different the key idea of deep learning is that these features are going to be learned directly from the data itself in a hierarchical manner so this means that given a data set let's say a task to detect faces for example can we train a deep learning model to take as input a face and start to detect the face by first detecting edges for example very low level features building up those edges to build eyes and noses and mouths and then building up some of those smaller components of faces into larger facial structure features so as you go deeper and deeper into a neural network architecture you'll actually see its ability to capture these types of hierarchical features and that's the goal of deep learning compared to machine learning is actually the ability to learn and extract these features to perform machine learning on them now actually the fundamental building blocks of deep learning and their underlying algorithms have actually existed for decades so why are we studying this now well for one data has become much more prevalent so data is really the driving power of a lot of these algorithms and today we're living in the world of big data where we have more data than ever before now second these models and these algorithms neural networks are extremely and massively parallelizable they can benefit tremendously from and they have benefited tremendously from modern advances in gpu architectures that we have experienced over the past decade right and these these advances these types of gpu architecture simply did not exist when we think about when these algorithms were detected in and created excuse me in for example the neuron the idea for the foundational neuron was created in almost 1960. so when you think back to 1960 we simply did not have the compute that we have today and finally due to amazing open source toolboxes like tensorflow we're able to actually build and deploy these algorithms and these models have become extremely streamlined so let's start with the fundamental building block of a neural network and that is just a single neuron now the idea of a single neuron or let's call this a perceptron is actually extremely intuitive let's start by defining how a single neuron takes as input information and it outputs a prediction okay so just looking at its forward pass it's forward prediction call from inputs on the left to outputs on the right so we define a set of inputs let's call them x1 to xm now each of these numbers on the left in the blue circles are multiplied by their corresponding weight and then added all together we take this single number that comes out of this edition and pass it through a nonlinear activation function we call this the activation function and we'll see why in a few slides and the output of that function is going to give us our our prediction y well this is actually not entirely correct i forgot one piece of detail here we also have a bias term which here i'm calling w0 sometimes you also see it as the letter b and the bias term allows us to shift the input to our activation function to the left or to the right now on the right side here you can actually see this diagram on the left illustrated and written out in mathematical equation form as a single equation and we can actually rewrite this equation using linear algebra in terms of vectors and dot products so let's do that here now we're going to collapse x1 to xm into a single vector called capital x and capital w will denote the vector of the corresponding weights w1 to wm the output here is obtained by taking their dot product adding a bias and applying this non-linearity and that's our output y so now you might be wondering the only missing piece here is what is this activation function right well i said it's a nonlinear function but what does that actually mean here's an example of one common function that people use as an activation function on the bottom right this is called the sigmoid function and it's defined mathematically above its plot here in fact there are many different types of nonlinear activation functions used in neural networks here are some common ones and throughout this entire presentation you'll also see what these tensorflow code blocks on the bottom part of the screen just to briefly illustrate how you can take the concepts the technical concepts that you're learning as part of this lecture and extend it into practical software right so these tensorflow code blocks are going to be extremely helpful for some of your software labs to kind of show the connection and bridge the connection between the foundation set up for the lectures and the practical side with the labs now the sigmoid activation function which you can see on the left hand side is popular like i said largely because it's the it's one of the few functions in deep learning that outputs values between zero and one right so this makes it extremely suitable for modeling things like probabilities because probabilities are also existing in the range between zero and one so if we want the output of probability we can simply pass it through a sigmoid function and that will give us something that resembles the probability that we can use to train with now in modern deep learning neural networks it's also very common to use what's called the relu function and you can see an example of this on the right and this is extremely popular it's a piecewise function with a single non-linearity at x equals 0. now i hope all of you are kind of asking this question to yourselves why do you even need activation functions what's the point what's the importance of an activation function why can't we just directly pass our linear combination of their inputs with our weights through to the output well the point of an activation function is to introduce a non-linearity into our system now imagine i told you to separate the green points from the red points and that's the thing that you want to train and you only have access to one line it's an it's not non-linear so you only have access to a line how can you do this well it's an extremely hard problem then right and in fact if you can only use a linear activation function in your network no matter how many neurons you have or how deep is the network you will only be able to produce a result that is one line because when you add a line to a line you still get a line output non-linearities allow us to approximate arbitrarily complex functions and that's what makes neural networks extremely powerful let's understand this with a simple example so imagine i give you a trained network now here i'm giving you the weights and the weights w are on the top right so w0 is going to be set to 1 that's our bias and the w vector the weights of our input dimension is going to be a vector with the values 3 and negative 2. this network only has two inputs right x1 and x2 and if we want to get the output of it we simply do the same step as before and i want to keep drilling in this message to get the output all we have to do is take our inputs multiply them by our corresponding weights w add the bias and apply a non-linearity it's that simple but let's take a look at what's actually inside that non-linearity when i do that multiplication and addition what comes out it's simply a weighted combination of the inputs in the form of a 2d line right so we take our inputs x of t x transpose excuse me multiply it as a dot product with our weights add a bias and if we look at what's inside this parentheses here what is getting passed to g this is simply a two dimensional line because all right we have two inputs x1 and x2 so we can actually plot this line in feature space or input space we'll call it because this is along the x-axis is x1 and along the y-axis is x2 and we can plot the the decision boundary we call it of the input to this um class to this activation function this is actually the line that defines our perceptron neuron now if i give you a new data point let's say x equals negative 1 2 we can plot this data point in this space in this two-dimensional space and we can also see where it falls with respect to that line now if i want to compute its weighted combination i simply follow the perceptron equation to get 1 minus 3 minus 4 which equals minus 6. and when i put that into a sigmoid activation function we get a final output of approximately 0.002 now why is that the case so assume we have this input negative 1 negative 2 and this is just going through the math again negative 1 and 2. we pass that through our our equations and we get this output from g let's dive in a little bit more to this feature graph well remember if i if the sigmoid function is defined in the standard way it's actually outputting values between 0 and 1 and the middle is actually at 0.5 right so anything on the left hand side of this feature space of this line is going to correspond to the input being less than 0 and the output being greater than 0.5 or excuse me less than 0.5 and on the other side is the opposite that's corresponding to our activation z being greater than 0 and our output y being greater than 0.5 right so this is just following all of the sigmoid math but illustrating it in pictorial form and schematics and in practice neural networks don't have just two weights w1 w2 they're composed of millions and millions of weights in practice so you can't really draw these types of plots for the types of neural networks that you'll be creating but this is to give you an example of a single neuron with a very small number of weights and we can actually visualize these type of things to gain some more intuition about what's going on under the hood so now that we have an idea about the perceptron let's start by building neural networks from this foundational building block and seeing how all of this story starts to come together so let's revisit our previous diagram of the perceptron if there's a few things i want you to take away from this class in this lecture today i want it to be this thing here so i want you to remember how a perceptron works and i want to remember three steps the first step is dot product your inputs with your weights dot product add a bias and apply a non-linearity and that defines your entire perceptron forward propagation all the way down into these three operations now let's simplify the diagram a little bit now that we got the foundations down i'll remove all of the weight labels so now it's assumed that every line every arrow has a corresponding weight associated to it now i'll remove the bias term for simplicity as well here you can see right here and note that z the result of our dot product plus our bias is before we apply the non-linearity right so g of z is our output our prediction of the perceptron our final output is simply our activation function g taking as input that state z if we want to define a multi-output neural network so now we don't have one output y let's say we have two outputs y one and y two we simply add another perceptron to this diagram now we have two outputs each one is a normal perceptron just like we saw before each one is taking inputs from x1 to xm from the x's multiplying them by the weights and they have two different sets of weights because they're two different neurons right they're two different perceptrons they're going to add their own biases and then they're going to apply the activation function so you'll get two different outputs because the weights are different for each of these neurons if we want to define let's say this entire system from scratch now using tensorflow we can do this very very simply just by following the operations that i outlined in the previous slide so our neuron let's start by a single dense layer a dense layer just corresponds to a layer of these neurons so not just one neuron or two neurons but an arbitrary number let's say n neurons in our dense layer we're going to have two sets of variables one is the weight vector and one is the bias so we can define both of these types of variables and weights as part of our layer the next step is to find what is the forward pass right and remember we talked about the operations that defined this forward pass of a perceptron and of a dense layer now it's composed of the steps that we talked about first we compute matrix multiplication of our inputs with our weight matrix our weight vector so inputs multiplied by w add the bias plus b and feed it through our activation function here i'm choosing a sigmoid activation function and then we return the output and that defines a dense layer of a neural network now we have this dense layer we can implement it from scratch like we see in the previous slide but we're pretty lucky because tensorflow has already implemented this dense layer for us so we don't have to do that and write that additional code instead let's just call it here we can see an example of calling a dense layer with the number of output units set equal to 2. now let's dive a little bit deeper and see how we can make now a full single layered neural network not just a single layer but also an output layer as well this is called a single hidden layered neural network and we call this a hidden layer because these states in the middle with these red states are not directly observable or enforceable like the inputs which we feed into the model and the outputs which we know what we want to predict right so since we now have this transformation from the inputs to the hidden layer and from the hidden layer to the output layer we need now two sets of weight matrices w1 for the input layer and w2 for the output layer now if we look at a single unit in this hidden layer let's take this second unit for example z2 it's just the same perceptron that we've been seeing over and over in this lecture already so we saw before that it's obtaining its output by taking a dot product with those x's its inputs multiplying multiplying them via the dot product adding a bias and then passing that through through the form of z2 if we took a different hidden node like z3 for example it would have a different output value just because the weights leading to z3 are probably going to be different than the weights leading to z2 and we we basically start them to be different so we have diversity in the neurons now this picture looks a little bit messy so let me clean it up a little bit more and from now on i'll just use this symbol in the middle to denote what we're calling a dense layer dense is called dense because every input is connected to every output like in a fully connected way so sometimes you also call this a fully connected layer to define this fully connected network or dense network in tensorflow you can simply stack your dense layers one after another in what's called a sequential model a sequential model is something that feeds your inputs sequentially from inputs to outputs so here we have two layers the heightened layer first defined with n hidden units and our output layer with two output units and if we want to create a deep neural network it's the same thing we just keep stacking these hidden layers on top of each other in a sequential model and we can create more and more hierarchical networks and this network for example is one where the final output in purple is actually computed by going deeper and deeper into the layers of this network and if we want to create a deep neural network in software all we need to do is stack those software blocks over and over and create more hierarchical models okay so this is awesome now we have an idea and we've seen an example of how we can take a very simple and intuitive mechanism of a single neuron a single perceptron and build that and build that all into the form of layers and complete complex neural networks let's take a look at how we can apply them in a very real and practical problem that maybe some of you have thought about before coming today's to today's class now here's the problem that i want to train an ai to to solve if i was a student in this class so will i pass this class that's the problem that we're going to ask our machine or a deep learning algorithm to answer for us and to do that let's start by defining some inputs and outputs or sorry input features excuse me to the to the ai to the ai model one feature that's let's use to learn from is the number of lectures that you attend as part of today as part of this course and the second feature is the number of hours that you're going to spend developing your final project and we can collect a bunch of data because this is our fifth year teaching this amazing class we can collect a bunch of data from past years on how previous students performed here so each dot corresponds to a student who took this class we can plot each student in this two-dimensional feature space where on the x-axis is the number of lectures they attended and on the y-axis is the number of hours that they spent on the final project the green points are the students who pass and the red points are those who failed and then there's you you lie right here right here at the point four five so you've attended four lectures and you've spent five hours on your final project you want to build now a neural network to determine given everyone else's standing in the class will i pass or fail this class now let's do it so we have these two inputs one is four one is five this is your inputs and we're going to feed these into a single layered neural network with three hidden units and we'll see that when we feed it through we get a predicted value of probability of you passing this class as 0.1 or 10 percent so that's pretty bad because well you're not going to fail the class you're actually going to succeed so the actual value here is going to be one you do pass the class so why did the network get this answer incorrectly well to start with the network was never trained right so all it did was we just started the network it has no idea what success 191 is how it occurs for a student to pass or fail a class or what these inputs four and five mean right so it has no idea it's never been trained it's basically like a baby that's never seen anything before and you're feeding some random data to it and we have no reason to expect why it's going to get this answer correctly that's because we never told it how to train itself how to update itself so that it can learn how to predict such a such an outcome or to predict such a task of passing or failing a class now to do this we have to actually define to the network what it means to get a wrong prediction or what it means to incur some error now the closer our prediction is to our actual value the lower this error or our loss function will be and the farther apart they are the uh the farther the part they are the more error we will incur the closer they are together the less error that we will occur now let's assume we have data not just from one student but for many students now we care about how the model did on average across all of the students in our data set and this is called the empirical loss function it's just simply the mean of all of the individual loss functions from our data set and when training a network to to solve this problem we want to minimize the empirical law so we want to minimize the loss that the network incurs on the data set that it has access to between our predictions and our outputs so if we look at the problem of binary classification for example passing or failing a class we can use something a loss function called for example the softmax cross-entropy loss and we'll go into more detail and you'll get some experience implementing this loss function as part of your software labs but i'll just give it as a a quick aside right now as part of this slide now let's suppose instead of predicting pass or fail a binary classification output let's suppose i want to predict a numeric output for example the grade that i'm going to get in this class now that's going to be any real number now we might want to use a different loss function because we're not doing a classification problem anymore now we might want to use something like a mean squared error loss function or maybe something else that takes as input continuous real valued numbers okay so now that we have this loss function we're able to tell our network when it makes a mistake now we've got to put that together with the actual model that we defined in the last part to actually see now how we can train our model to update and optimize itself given that error function so how can it minimize the error given a data set so remember that we want the objective here is that we want to identify a set of weights let's call them w star that will give us the minimum loss function on average throughout this entire data sets that's the gold standard of what we want to accomplish here in training a neural network right so the whole goal of this class really is how can we identify w star right so how can we train our the weights all of the weights in our network such that the loss that we get as an output is as small as it can possibly be right so that means that we want to find the w's that minimize j of w so that's our empirical loss our average empirical loss remember that w is just a group of all of the ws from our from every layer in the model right so we just concatenate them all together and we want to minimize the we want to find the weights that give us the lowest loss and remember that our loss function is just a is a function right that takes us input all of our weights so given some set of weights our loss function will output a single value right that's the error if we only have two weights for example we might have a loss function that looks like this we can actually plot the loss function because it's it's relatively low dimensional we can visualize it right so on the x on the horizontal axis x and y axis we're having the two weights w0 and w1 and on the vertical axis we're having the loss so higher loss is worse and we want to find the weights w0 and w1 that will bring us the lowest part to the lowest part of this lost landscape so how do we do that this a process called optimization and we're going to start by picking an initial w0 and w1 start anywhere you want on this graph and we're going to compute the gradient remember our loss function is simply a mathematical function so we can compute the derivatives and compute the gradients of this function and the gradient tells us the direction that we need to go to maximize j of w to maximize our loss so let's take a small step now in the opposite direction right because we want to find the lowest loss for a given set of weights so we're going to step in the opposite direction of our gradient and we're going to keep repeating this process we're going to compute gradients again at the new point and keep stepping and stepping and stepping until we converge to a local minima eventually the gradients will converge and we'll stop at the bottom it may not be the global bottom but we'll find some bottom of our lost landscape so we can summarize this whole algorithm known as gradient descent using the gradients to descend into our loss function in pseudocode so here's the algorithm written out as pseudocode we're going to start by initializing weights randomly and we're going to repeat the two steps until we convert so first we're going to compute our gradients and then we're going to step in the opposite direction a small step in the opposite direction of our gradients to update our weights right now the amount that we step here eta this is the the n character next to our gradients determines the the magnitude of the step that we take in the direction of our gradients and we're going to talk about that later that's a very important part of this problem but before i do that i just want to show you also kind of the analog side of this algorithm written out in tensorflow again which may be helpful for your software labs right so this whole algorithm can be replicated using automatic differentiation using platforms like tensorflow so tensorflow with tensorflow you can actually randomly initialize your weights and you can actually compute the gradients and do these differentiations automatically so it will actually take care of the definitions of all of these gradients using automatic differentiation and it will return the gradients that you can directly use to step with and optimize and train your weights but now let's take a look at this term here the gradient so i mentioned to you that tensorflow and your software packages will compute this for you but how does it actually do that i think it's important for you to understand how the gradient is computed for every single weight in your neural network so this is actually a process called back propagation in deep learning and neural networks and we'll start with a very simple network and this is probably the simplest network in existence because it only contains one hidden neuron right so it's the smallest possible neural network now the goal here is that we're going to try and do back propagation manually ourselves by hand so we're going to try and compute the gradient of our loss j of w with respect to our weight w2 for example this tells us how much a small change in w2 will affect our loss function right so if i change and perturb w2 a little bit how does my error change as a result so if we write this out as a derivative we start by applying the chain rule and use we start by applying the chain rule backwards from the loss function through the output okay so we start with the loss function here and we specifically decompose dj dw2 into two terms we're going to decompose that into dj dy multiplied by d y d w two right so we're just applying the chain rule to decompose the left hand side into two gradients that we do have access to now this is possible because y is only dependent on the previous layer now let's suppose we want to compute the gradients of the weight before w2 which in this case is w1 well now we've replaced w2 with w1 on the left hand side and then we need to apply the chain rule one more time recursively right so we take this equation again and we need to apply the chain rule to the right hand side on the the red highlighted portion and split that part into two parts again so now we propagate our gradient our old gradient through the hidden unit now all the way back to the weight that we're interested in which in this case is w1 right so remember again this is called back propagation and we repeat this process for every single weight in our neural network and if we repeat this process of propagating gradients all the way back to the input then we can determine how every single weight in our neural network needs to change and how they need to change in order to decrease our loss on the next iteration so then we can apply those small little changes so that our losses a little bit better on the next trial and that's the backpropagation algorithm in theory it's a very simple algorithm just compute the gradients and step in the opposite direction of your gradient but now let's touch on some insights from training these networks in practice which is very different than the simple example that i gave before so optimizing neural networks in practice can be extremely difficult it does not look like the loss function landscape that i gave you before in practice it might look something like this where your lost landscape is super convex uh super non-convex and very complex right so here's an example of the paper that came out a year ago where authors tried to actually visualize what deep learn deep neural network architecture landscapes actually look like and recall this update equation that we defined during gradient descent i didn't talk much about this parameter i alluded to it it's called the learning rates and in practice it determines a lot about how much step we take and how much trust we take in our gradients so if we set our learning rate to be very slow then we're model we're having a model that may get stuck in local minima right because we're only taking small steps towards our gradient so we're going to converge very slowly we may even get stuck if it's too small if the learning rate is too large we might follow the gradient again but we might overshoot and actually diverge and our training may kind of explode and it's not a stable training process so in reality we want to use learning rates that are neither not small not too small not too large to avoid these local minima and still converge right so we want to kind of use medium-sized learning rates and what medium means is totally arbitrary you're going to see that later on just kind of skip over these local minima and and still find global or hopefully more global optimums in our lost landscape so how do we actually find our learning rate well you set this as the define as a definition of your learning algorithm so you have to actually input your learning rate and one way to do it is you could try a bunch of different learning rates and see which one works the best that's actually a very common technique in practice even though it sounds very unsatisfying another idea is maybe we could do something a little bit smarter and use what are called adaptive learning rates so these are learning rates that can kind of observe its landscape and adapt itself to kind of tackle some of these challenges and maybe escape some local minima or speed up when it's on a on a local minima so this means that the learning rate because it's adaptive it may increase or decrease depending on how large our gradient is and how fast we're learning or many other options right so in fact these have been widely explored in deep learning literature and heavily published on as part of also software packages like tensorflow as well so during your labs we encourage you to try out some of these different types of of uh optimizers and algorithms and how they they can actually adapt their own learning rates to stabilize training much better now let's put all of this together now that we've learned how to create the model how to define the loss function and how to actually perform back propagation using an optimization algorithm and it looks like this so we define our model on the top we define our optimizer here you can try out a bunch of different of the tensorflow optimizers we feed the output of our model grab its gradient and apply its gradient to the optimizer so we can update our weight so in the next iteration we're having a better prediction now i want to continue to talk about tips for training these networks in practice very briefly towards the end of this lecture and because this is a very powerful idea of batching your data into mini batches to stabilize your training even further and to do this let's first revisit our gradient descent algorithm the gradient is actually very very computationally expensive to compute because it's computed as a summation over your entire data set now imagine your data set is huge right it's not going to be feasible in many real life problems to compute on every training iteration let's define a new gradient function that instead of computing it on the entire data set it just computes it on a single random example from our data set so this is going to be a very noisy estimate of our gradient right so just from one example we can compute an estimate it's not going to be the true gradient but an estimate and this is much easier to compute because it's it's very small so just one data point is used to compute it but it's also very noisy and stochastic since it was used also with this one example right so what's the middle ground instead of computing it from the whole data set and instead of computing it from just one example let's pick a random set of a small subset of b examples we'll call this a batch of examples and we'll feed this batch through our model and compute the gradient with respect to this batch this gives us a much better estimate in practice than using a single gradient it's still an estimate because it's not the full data set but still it's much more computationally attractive for computers to do this on a small batch usually we're talking about batches of maybe 32 or up to 100 sometimes people use larger with larger neural networks and larger gpus but even using something smaller like 32 can have a drastic improvement on your performance now the increase in gradient accuracy estimation actually allows us to converge much quicker in practice so it allows us to more smoothly and accurately estimate our gradients and ultimately that leads to faster training and more parallelizable computation because over each of the elements in our batch we can kind of parallelize the gradients and then take the average of all of the gradients now this last topic i want to address is that of overfitting this is also a problem that is very very general to all of machine learning not just deep learning but especially in deep learning which is why i want to talk about it in today's lecture it's a fundamental problem and challenge of machine learning and ideally in machine learning we're given a data set like these red dots and we want to learn a model like the blue line that can approximate our data right said differently we want to build models that learn representations of our data that can generalize to new data so assume we want to build this line to fit our red dots we can do this by using a single linear line on the left hand side but this is not going to really well capture all of the intricacies of our red points and of our data or we can go on the other far extreme and overfit we can really capture all the details but this one on the far right is not going to generalize to a new data point that it sees from a test set for example ideally we want to wind up with something in the middle that is still small enough to maintain some of those generalization capabilities and large enough to capture the overall trends so to address this problem we can employ what's called a technique called regularization regularization is simply a method for in that you can introduce into your training to discourage complex models so to encourage these more simple types of models to be learned and as we've seen before it's actually critical and crucial for our models to be able to generalize past our training data right so we can fit our models to our training data but actually we can minimize our loss to almost zero in most cases but that's not what we really care about we always want to train on a training set but then have that model be deployed and generalized to a test set which we don't have access to so the most popular regularization technique for deep learning is a very simple idea of dropout and let's revisit this picture of a neural network that we started with in the beginning of this class and in dropout during training what we're going to do is we're going to randomly drop and set some of the activations in this neural network in the hidden layer to zero with some probability let's say we drop out 50 of the neurons we randomly pick 50 of neurons that means that their activations now are all set to zero and we force the network to not rely on those neurons too much so this forces the model to kind of identify different types of pathways through the network on this iteration we pick some random 50 to drop out and on the next iteration we may pick a different random percent and this is going to encourage these different pathways and encourage the network to identify different forms of processing its information to accomplish its decision making capabilities another regularization technique is a technique called early stopping now the idea here is that we all know the definition of overfitting is when our training set is or sorry when our model starts to have very bad performance on our test set we don't have a test set but we can kind of create a example test set using our training set so we can split up our training set into two parts one that we'll use for training and one that will not show to the training algorithm but we can use to start to identify when we start to overfit a little bit so on the x-axis we can actually see training iterations and as we start to train we can see that both the training loss and the testing loss go down and they keep going down until they start to converge and this pattern of divergence actually continues for the rest of training and what we want to do here is actually identify the place where the testing accuracy or the testing loss is minimized and that's going to be the model that we're going to use and that's going to be the best kind of model in terms of generalization that we can use for deployment so when we actually have a brand new test data set that's going to be the model that we're going to use so we're going to employ this technique called early stopping to identify it and as we can see anything that kind of falls on the left side of this line is are models that are under fitting and anything on the right side of this line are going to be models that are considered to be overfit right because this divergence has occurred now i'll conclude this lecture by first summarizing the three main points that we've covered so far so first we learned about the fundamental building blocks of neural networks the perceptron a single neuron we learned about stacking and composing these types of neurons together to form layers and full networks and then finally we learned about how to actually complete the whole puzzle and train these neural networks and to end using some loss function and using gradient descent and back propagation so in the next lecture we'll hear from ava on a very exciting topic taking a step forward and actually doing deep sequence modeling so not just one input but now a series a sequence of inputs over time using rnns and also a really new and exciting type of model called the transformer and attention mechanism so let's resume the class in about five minutes once we have a chance for ava to just get set up and bring up her presentation so thank you very much 

all right hi everyone and welcome back my name is ava and before we dive into lecture two of success 191 which is going to be on deep sequence modeling i'll just note that as you probably noticed we're running a little bit late so we're going to proceed with the lecture you know in full and in completion and at the time it ends then we'll transition to the software lab portion of the course just immediately after at the time that this lecture ends and i'll make a note about the structure and how we're going to run the software labs at the end of my lecture okay so in alexander's first lecture we learned about really the essentials of neural networks and feed-forward models and how to construct them so now we're going to turn our attention to applying neural networks to tasks that involve modeling sequences of data and we'll see why these sorts of tasks require a fundamentally different type of network architecture from what we've seen so far and to build up to that point we're going to walk through step by step building up intuition about why modeling sequences is different and important and start back with our fundamentals of feed forward networks to build up to the models we'll introduce in this lecture all right so let's dive into it let's first motivate the need for sequence modeling and what we mean in terms of sequential data with a super intuitive and simple example so suppose we have this picture of a ball and our task is to predict where this ball is going to travel to next now if i don't give you any prior information on the ball's history any guess on its next position is just going to be that a random guess but now instead if in addition to the current location of the ball i also gave you some information about its previous locations now our problem becomes much easier and i think we can all agree that we have a sense of where this ball is going to next and beyond this simple example the fact of the matter is that sequential data is all around us for example audio like the waveform of my voice speaking to you can be split up into a sequence of sound waves while text can be split up into a sequence of characters or a sequence of words and beyond these two examples there are many many more cases in which sequential processing may be useful from medical signals like ekgs to stock prices to dna sequences and beyond so now that we've gotten a sense of what sequential data looks like let's consider applications of sequential modeling in the real world in alexander's first lecture we learned about this notion of feed-forward models that operate sort of on this one-to-one fixed setting right a single input to a single output and he gave the very simple example of a binary classification task predicting whether you as a student will pass or fail this class of course we all hope you will pass but in this example there's no real component of time or sequence right in contrast with sequence modeling we can now handle a vast variety of different types of problems where for example we have a sequence of temporal inputs and potentially a sequential output so let's consider one example right where we have a natural language processing task where we have a tweet and we want to classify the emotion or the sentiment associated with that tweet mapping a sequence of words to a positive or negative label we can also have a case where our input initially may not have a time dimension so for example we have this image of a baseball player throwing a ball but instead the output that we want to generate has a temporal or sequential component where we now want to caption that image with some associated text and finally we can have a final case where we have a sequential input and we want to map it to a sequential output for example in the case of translating text from one language to another and so sometimes it can be really challenging to kind of wrap your head around and get the idea about how we can add a new temporal dimension to our models and so to achieve this understanding what i want to do is really start from the fundamentals and revisit the concept of the perceptron that alexander introduced and go step by step from that foundation to develop an understanding of what changes we need to make to be able to handle sequential data so let's recall the architecture and the the diagram of the perceptron which we studied in the first lecture we defined a set of inputs and we have these weights that are associated with connecting those inputs to an internal node and we can apply those weights apply a non-linearity and get this output and we can extend this now to a layer of individual neurons a layer of perceptrons to yield a multi-dimensional output and in this example we have a single layer of perceptrons shown in green taking three inputs shown in blue predicting four outputs in purple but does this notion does this have a notion of time or of sequence not yet let's simplify that diagram right what i've done here is just i've collapsed that layer of those four perceptrons into the single green box and i've collapsed those nodes of the input and the output into these single circles that are represented as vectors so our inputs x are some vectors of a length m and our outputs are vectors of another length n still here what we're considering is an input at a specific time denoted by t nothing different from what we saw in the first lecture and we're passing it through a feed-forward model to get some output what we could do is we could have fed in a sequence to this model by simply applying the same model that same series of operations over and over again once for each time step in our sequence and this is how we can handle these individual inputs which occur at individual time steps so first let's just rotate the same diagram i've taken it from a horizontal view to a vertical view we have this input vector at some time sub t we feed it into our network get our output and since we're interested in sequential data let's assume we don't just have a single time step we now have multiple individual time steps starting from t equals zero our first time step in our sequence and extending forward right again now we're treating the individual time steps as isolated time steps right we don't yet have a notion of the relationship between time step zero and time step one time step two and so on and so forth and what we know from the first lecture is that our output vector at a particular time step is just going to be a function of the input at that time step what could be the issue here right well we have this transformation yet but this is inherently sequential data and it's probably in a sequence for some important reason and we don't yet have any sort of interdependence or notion of interconnectedness across time steps here and so if we consider the output at our last time step right the fundamental point is that that output is related to the inputs at the previous time steps how can we capture this interdependence what we need is a way to relate the network's computations at a particular time step to its prior history and its memory of the computations from those prior time steps passing information forward propagating it through time and what we consider doing is actually linking the information and the computation of the network at different time steps to each other via what we call a recurrence relation and specifically the way we do this in neural recurrent recurrent models is by having what we call an internal memory or a state which we're going to denote as h of t and this value h of t is maintained time set to time step and it can be passed forward across time and the idea and the intuition here is we want the state to try to capture some notion of memory and what this means for the network's computation its output is that now our output is dependent not only on the input at a particular time step but also this notion of the state of the memory that's going to be passed forward from the prior time step right and so this output just to make this very explicit this output at a particular time step t depends both on the input as well as the past memory and that past memory is going to capture the prior history of what the what has occurred previously in the sequence and because this output y of t is a function of both current input past memory what this means is we can define and describe these types of neurons in terms of a recurrence relation and so on the right you can see how we visualize these individual time steps as sort of being unrolled extended across time but we could also depict this same relationship via a cycle which i've shown on the left which shows and highlights this concept of a recurrence relation all right so hopefully this builds up some intuition about this notion of recurrence and why it can help us in sequential modeling tasks and this intuition that we've built up from starting with the feed forward model is really the key to the recurrent neural network or rnns and we're going to continue to build up from this foundation and build up our understanding of how this recurrence relation defines the behavior of an rnn so let's formalize this just a bit more right the key idea that i mentioned and i'm going to keep driving home is that the rnn maintains this internal state h of t which is going to be updated at each time step as the sequence is processed and we do this by applying this recurrence relation at every time step where our cell state is now a function yeah our cells ourselves say h of t is now a function of the current input x of t as well as the prior state h of t minus 1. and importantly this function is parametrized by a set of weights w and this set of weights is what we're actually going to be learning through our network over the course of training as the model is being learned and as these weights are being updated right and the key point here is that this set of weights w is the same across all time steps that are being considered in the sequence and this function that computes this hidden state is also the same we can also step through this intuition behind the rnn algorithm in sort of pseudo code to get a better sense of how these networks work so we can begin by initializing our rnn right what does it take to initialize it well first we have to initialize some first hidden state which we're going to do with a vector of zeros and we're going to consider a sentence that's going to serve as our input sequence to the model and our task here is to try to predict the next word that's going to come at the end of the sentence and our recurrence relation is captured by this loop where we're going to iterate through the words in the sentence and at each step we're going to feed both the current word being considered as well as the previous hidden state into our rnn model and that's going to output a prediction for what the likely next word is and also update its internal computation of the hidden state and finally our last our token prediction that we're interested in at the end is the rnn's output after all the words all the time points in the sequence have been considered and that generates our prediction for the likely next word and so that's that hopefully provides more intuition about how this rnn algorithm is working and if you notice the internal computation of the rnn includes both this internal state output as well as ultimately trying to output the prediction that we're interested in our output vector y of t so to walk through how this we actually derive this output vector let's step through this what we do is given an input our input vector we pass that in to compute the rnn's internal state computation and breaking this function down what it's doing is just a standard neural net operation just like we saw in the first lecture right it consists of multiplication by weight matrices right donated as w and in this case we're going to multiply both the past hidden state by a weight matrix w as well as the current input x of t by another wave matrix and then we're going to add them together and apply a non-linearity and you'll notice as i just mentioned right because we have these two inputs to the state update equation we have these two independent weight matrices and the final step is to actually generate the output for a given time step which we do by taking that internal state and simply modifying it following a multiplication by another weight matrix and then using this as our generated output and that's it that's how the rnn updates its hidden state and also produces an output at a given time step so so far right we've seen the rnns depicted largely as having these loops that feed back in on themselves and as we as we built up from we can also represent this loop as being unrolled across time where effectively starting from the first time step we have this unrolled network that we can continue to unroll across time from time step 0 to our n time step time sub t and in this diagram let's now formalize things a little bit more we can also make the weight matrices that compute that are applied to the input very explicit and we can also annotate our diagram with the way matrices that relate the prior hidden state to the current hidden state and finally our predictions at individual time steps are um are generated by a a separate weight matrix matrices okay so as i as i mentioned right the key point is that these weight matrices are reused across all of the individual time steps now our next step that you may be thinking of is okay this is all great we figured out how to update the hidden state we figured out how to generate the output how do we actually train this thing right well we'll need a loss because as alexander mentioned the way we train neural networks is through this optimization this iterative optimization of a loss function or an objective function and as you may may predict right we can generate an individual loss for each of these individual time steps according to what the output at that time step is and we can generate a total sum loss by taking these time steps and summing them all together and when we make a forward pass through our network this is exactly what we do right we generate our output predictions and we sum sum the loss uh functions across individual time steps to get the total loss now let's walk through let's next walk through an example of how we can implement an rnn from scratch the previous code block that i showed you was kind of an intuitive pseudo code example and here now we're going to get into things in a little bit more detail and build up the rnn from scratch our rnn is going to be defined as a neural network layer and we can build it up by inheriting from the neural network layer class that alexander introduced in the first lecture and as before we are going to start by initializing our weight matrices and also initializing the hidden state to zero our next step which is really the important step is defining the call function which is what actually defines the forward pass through our rnn model and within this call function the key operations are as follows we first have a update of the hidden state right according to that same equation we saw earlier incorporating the previous hidden state incorporating the input summing them passing them through a non-linearity we can then compute the output transforming the hidden state and finally at each time step we return both the current output and our hidden state that's it that's how you can code up an rnn line by line and define the forward pass but thankfully tensorflow has very very conveniently summarized this already and implemented these types of rnn cells for us in what they wrap into the simple rnn layer and you're going to get some practice using this um this class of neural network layer in today's software lab all right so to recap right we've we've seen how uh we've seen the the function and the computation of rnns by first moving from the one-to-one computation of a traditional feed-forward vanilla or vanilla neural network excuse me and seeing how that breaks down when considering sequence modeling problems and as i mentioned right we can we can apply this idea of sequence modeling and of rnns to many different types of tasks for example taking a sequential input and mapping it to one output taking a static input that's not resolved over time and generating a sequence of outputs for example a text associated with an image or translating a sequence of inputs to a sequence of outputs which can be done in machine translation natural language processing and also in generation so for example in composing new musical scores entirely using recurrent neural network models and this is what you're going to get your hands-on experience with in today's software lab and beyond this right you know you all come from a variety of different backgrounds and interests and disciplinary domains so i'm sure you can think of a variety of other applications where this type of architecture may be very useful okay so to wrap up this section right this simple example of rnns kind of motivates a set of concrete design criteria that i would like you to keep in mind when thinking about sequence modeling problems specifically whatever model we design needs to be able to handle sequences of variable length to track long-term dependencies in the data to be able to map something that appears very early on in the sequence to something related later on in the sequence to be able to preserve and reason and maintain information about order and finally to share parameters across the sequence to be able to keep track of these dependencies and so most of today's lecture is going to focus on recurrent neural networks as a workhorse neural network architecture for sequence modeling criteria design problems but we'll also get into a new and emerging type of architecture called transformers later on in the lecture which i think you'll find really exciting and really interesting as well before we get into that i'd like to spend a bit of time thinking about the these design criteria that i enumerated and why they're so important in the context of sequence modeling and use that to move forward into some concrete applications of rnns and sequence models in general so let's consider a very simple sequence modeling problem suppose we have this sentence right this morning i took my cat for a walk and our task here is to use some prior information in the sentence to predict the next word in the sequence right this morning i took my cat 4a predict the next work walk how can we actually go about doing this right i've introduced the intuition and the diagrams and everything about the recurrent neural network models but we really haven't started to think about okay how can we even represent language to a neural network how can we encode that information so that it can actually be passed in and operated on mathematically so that's our first consideration right let's suppose we have a model we're inputting a word and we want to use our neural network to predict the next work word what are considerations here right remember the neural network all it is is it's just a functional operator they execute some functional mathematical operation on an input they can't just take a word as a string or as as a as a language as a sequence of language characters passed in as is that's simply not going to work right instead we need a way to represent these elements these words numerically to be fed in to our neural network as a vector or a matrix or an array of numbers such that we can operate on it mathematically and get a vector or array of numbers out this is going to work for us so how can we actually encode language transform it into this vector representation the solution is this concept of what we call an embedding and the idea is that we're going to transform a set of indices which effectively are just identifiers for objects into some vector of fixed size so let's think through how this embedding operation could work for language data for example for this sequence that we've been considering right we want to be able to map any word that could appear in our body of language our corpus into a fixed sized vector and so the first step to doing this is to think about the vocabulary right what's the overall space of unique words in your corpus in your language from this vocabulary we can then index by mapping individual words to numerical unique indices and then these indices can then be mapped to an embedding which is just a fixed length vector one way to do this is by taking a vector right that's length is just going to equal the total number of unique words in our vocabulary and then we can indicate what word that vector corresponds to by making this a sparse vector that's just binary so it's just zeros and ones and at the index that corresponds to that word we're going to indicate the identity of that word with a one right and so in this example our word is cat and we're going to index it at the second index and what this is referred to is a one hot embedding it's a very very popular embed choice of embedding which you will encounter across many many different domains another option to generating and embedding is to actually use some sort of machine learning model it can be a neural network to learn in embedding and so the idea here is from taking an input of words that are going to be indexed numerically we can learn an embedding of those words in some lower dimensional space and the motivation here is that by introducing some sort of machine learning operation we can map the meaning of the words to an encoding that is more informative more representative such that similar words that are semantically similar in meaning will have similar embeddings and this will also get us our fixed length encoding vector and this idea of a learned embedding is a super super powerful concept that is very pervasive in modern deep learning today and it also motivates a whole nother class of problems called representation learning which is focused on how we can take some input and learn use neural networks to learn a meaningful meaningful encoding of that of that input for our problem of choice okay so going back to our design criteria right we're first going to be able to try to handle variable sequence lengths we can consider again this this problem of trying to predict the next word we can have a short sequence we can have a longer sequence or an even longer sequence right but the whole point is that we need to be able to handle these variable length inputs and feed forward networks are simply not able to do this because they have inputs of fixed dimensionality but because with rnns we're unrolling across time we're able to handle these variable sequence lengths our next our next criteria is that we need to be able to capture and model long-term dependencies in the data so you can imagine an example like this where information from early on in the sentence is needed to make a accurately make a prediction later on in the sentence and so we need to be able to capture this longer term information in our model and finally we need to be able to retain some sense of order right that could result in differences in the overall contact or meaning of a sentence so in this example these two sentences have the exact same words repeated the exact same number of times but the semantic meaning is completely different because the words are in different orders and so hopefully this example shows a very concrete and common example of sequential data right language and motivates how these different design considerations uh play into this general problem of sequence modeling and so these points are something that i really like for you to take away from this class and keep in mind as you go forward implementing these types of models in practice our next step as we as we walk through this lecture on sequence modeling is to actually go through very briefly on the algorithm that's used to actually train recurrent neural network models and that algorithm is called backpropagation through time and it's very very related to the backpropagation algorithm that alexander introduced in the first lecture so if you recall the way we train feedforward models is go from input and make a forward pass through the network going from input to output and then back propagate our gradients back downwards through the network taking the derivative of the loss with respect to the weights learned by our model and then shifting and adjusting the parameters of these weights in order to try to minimize the loss over the course of training and as we saw earlier for rnns they have a little bit of a different scenario here because our forward pass through the network consists of going forward across time computing these individual loss values at the individual time steps and then summing them together to back propagate instead of back propagating errors through a single feed forward network now what we have to do is back propagate error individually across each time step and then across all the time steps all the way from where we currently are in this sequence to the beginning of the sequence and this is the reason why this algorithm is called backpropagation through time because as you can see errors flow backwards in time to the beginning of our data sequence and so taking a closer look at how these gradients flow across this rnn chain what you can see is that between each time step we need to perform these individual matrix multiplications right which means that computing the gradient that is taking the loss with respect to an internal state and the weights of that internal state requires many many matrix in multiplications involving this weight matrix as well as repeated gradient computation so why might this be problematic well if we have many of these weight values or gradient values that are much much much larger than one we could have a problem where during training our gradients effectively explode and the idea behind this is the gradients are becoming extremely large due to this repeated multiplication operation and we can't really do optimization and so a simple solution to this is called gradient clipping just trimming the gradient values to scale back bigger gradients into a smaller value we can also have the opposite problem where now our weight values are very very small and this leads to what is called the vanishing gradient problem and is also very problematic for training recurrent neural models and we're going to touch briefly on three ways that we can mitigate this vanishing gradient problem in recurrent models first choosing our choice of activation function initially initializing the weights in our model intelligently and also designing our architecture of our network to try to mitigate this issue altogether the reason why before we do that to take a step back right the reason why vanishing gradients can be so problematic is that they can completely sabotage this goal we have of trying to model long-term dependencies because we are multiplying many many small numbers together what this effectively biases the model to do is to try to preferentially focus on short-term dependencies and ignore the long-term dependencies that may exist and while this may be okay for simple sentences like the clouds are in the blank it really breaks down in longer sentences or longer sequences where where information from earlier on in the sequence is very important for making a prediction later on in the case of this example here so how can we alleviate this our first strategy is a very simple trick that we can employ when designing our networks we can choose our activation function to prevent the gradient from shrinking too dramatically and the relu activation function is a good choice for doing this because in instances where our input x is greater than zero it automatically boosts the value of the activation function to one whereas other activation functions don't do do that right another trick is to be smart in how we actually initialize the parameters in our model what we can do is initialize the weights that we set to the identity matrix which prevents them from shrinking to zero too rapidly during back propagation and the final and most robust solution is to use a more complex recurrent unit that can effectively track long-term dependencies in the data and the idea here is that we're going to introduce this computational infrastructure called the gate which functions to selectively add or remove information to the state of the rnn and this is done by you know standard operations that we see in neural networks for example sigmoid activation functions pointwise matrix multiplications and the idea behind these gates is that they can effectively control what information passes through the recurrent cell and today we're going to touch very briefly on one type of gated cell called a lstm a long short term memory network and they're fairly good at using this gating mechanism to selectively control information over many time steps and so i'm not going to get into the details right because we have more interesting things to uh touch on in our on our limited time but the key idea behind the lstms is they have that same chain-like structure as a standard rnn but now the internal computation is a little bit more complex right we have these different gates that are effectively interacting with each other to try to control information flow and you would implement the lstm in tensorflow just as you would a standard rnn and while that diagram i just like blew past that right the gated structure we could spend some time talking about the mathematics of that but really what i want you to take away from this lecture is the key concepts behind what the lstm is doing internally so to break that down the lstm like a standard rnn is just maintaining this notion of self-state but it has these additional gates which control the flow of information functioning to effectively eliminate irrelevant information from the past keeping what's relevant keeping what's important from the current input using that important information to update the internal cell state and then outputting a filtered version of that cell state as the predictive output and what's key is that because we incorporate this great gated structure in practice our backpropagation through time algorithm actually becomes much more stable and we can mitigate against the vanishing gradient problem by having fewer repeated matrix multiplications that allow for a smooth flow of gradients across our model okay so now we've gone through the fundamentals of rnns in terms of their architecture and training and i'd next like to consider a couple of concrete examples of how we can use recurrent role models the first is let's imagine we're trying to use a recurrent model to predict the next musical note in a sequence and use this to generate brand new musical sequences what we can do is we can treat this as a next next input predict sorry a next time step prediction problem where you input a sequence of notes and the output at each time step is the most likely next note in the sequence and so for example it turns out that this very famous classical composer named franz schubert had what he called a very famous unfinished symphony and that was left as the name suggests partially undone and he did not get a chance to actually finish composing the symphony before he died and a few years ago some researchers trained a neural network model on this uh on the prior movements of that symphony to try to actually generate new music that would be similar to schubert's music to effectively finish the symphony and compose two new movements so we can actually take a listen to what that result turned out like [Music] so hopefully you you were able to hear that and and appreciate the point that maybe there are some classical music aficionados out there who can recognize it as being stylistically similar hopefully to schubert's music and you'll get practice with this exact same task in today's lab where you'll be training a recurrent model to generate brand new irish folk songs that have never been heard before as another cool example which i kind of motivated at the beginning of the lecture we can also do a classification task where we take an input sequence and try to predict a single output associated with that sequence for example taking a sequence of words and assigning an emotion or a sentiment associated with that sequence and one use case for this type of task is in tweet sentiment classification so training a model on a bunch of tweets from twitter and using it to predict a sentiment associated with given tweets so for example we can take we can train a model like this with a bunch of tweets hopefully we can train an rnn to predict that this first tweet about our course has a positive sentiment but that this other tweet about winter weather is actually just having a negative sentiment okay so at this point you know we focus exclusively on recurrent models and it's actually fairly remarkable that with this type of architecture we can do things that seem so complex as generating brand new classical music but let's take a step back right with any technology they're going to be strengths and there are going to be limitations what could be potential issues of using recurrent models to perform sequence modeling problems the first key limitation is that these network architectures fundamentally have what we like to think of as an encoding bottleneck we need to take a lot of content which may be a very long body of text many different words and condense it into a representation that can be predicted on and information can be lost in this actual encoding operation another big limitation is that recurrent neurons and recurrent models are not efficient at all right they require sequentially processing information taking time steps individually and this sequential nature makes them very very inefficient on the modern gpu hardware that we use to train these types of models and finally and perhaps most importantly while we've been emphasizing this point about long-term memory the fact is that recurrent models don't really have that high of memory capacity to begin with while they can handle sequences of length on the order of tens or even hundreds with lstms they don't really scale well to sequences that are of length of thousands or ten thousands of time steps how can we do better and how can we overcome this so to understand how to do this right let's go back to what our general task is with sequence modeling we're given some sequence of inputs we're trying to compute some sort of features associated with those inputs and use that to generate some output prediction and with rnns as we saw we're using this recurrence relation to try to model sequence dependencies but as i mentioned these rnns have these three key bottlenecks what is the opposite of these these three limitations if we had any capability that we desired what could we imagine the capabilities that we'd really like to achieve with sequential models is to have a continuous stream of information that overcomes the encoding bottleneck we'd like our model to be really fast to be paralyzable rather than being slow and dependent on each of the individual time steps and finally we wanted to be able to have long memory the main limitation of rnn's when it comes to these capabilities is that they process these individual time steps individually due to the recurrence relation so what if we could eliminate the recurrence relation entirely do away with it completely one way we could do this is by simply taking our sequence and squashing everything together concatenating those individual time steps such that we have one vector of input with data from all time points we can feed it into a model calculate some feature vector and then generate an output which maybe we hope makes sense right naively a first approach to do this may be to take that squashed concatenated input pass it into a fully connected network and yay congrats we've eliminated the need for recurrence but what are issues here right this is totally not scalable right our dense network is very very densely connected right it has a lot of connections and our whole point of doing this was to try to scale to very long sequences furthermore we've completely eliminated any notion of order any notion of sequence and because of these two issues our long memory that we want all along is also made impossible and so this approach is definitely not going to work we don't have a notion of what points in our sequence is important can we be more intelligent about this and this is really the key idea behind the next concept that we're going to i'm going to introduce uh in the remaining time and that's this notion of attention right which intuitively we're we're going to think about the ability to identify and attend to parts of an input that are going to be important and this idea of attention is an extremely powerful and rapidly emerging mechanism for modern neural networks and the it it's the core foundational mechanism for this very very powerful architecture called transformers you may have heard of transformers before in popular news media what have you and the idea when you maybe want to try to look at the math and the operation of transformers can seem very daunting it was definitely daunting for me as it tends to be presented in a pretty complex and complicated way but at its core this attention mechanism which is the really key insight into transformers is actually a very elegant and intuitive idea we're going to break it down step by step so you can see how it's computed and what makes it so powerful to do that we're specifically going to be talking about this idea of self-attention and what that means is the ability to take an input and attend to the most important parts of that input i think it's easiest to build up that intuition by considering an image so let's look at this picture of our hero iron man how do we figure out what's important a naive way could just be to scan across this image pixel by pixel but as humans we don't do this our brains are able to immediately pick up what is important in this image just by looking at it that's iron man coming at us and if you think about it right what this comes down to is the ability to identify the parts that are important to attend to and be able to extract features from those regions with high attention and this first part of this problem is very very similar conceptually to search and to build up understanding of this attention mechanism we're going to start their first search how does search work so maybe sitting there listening to my lecture you're thinking wow this is so exciting but i want to learn even more about neural networks how can i do this one thing you could do is go to our friend the internet do a search and have all the videos on the internet accessible to you and you want to find something that matches your desired goal so let's consider youtube right youtube is a giant database of many many many videos and across that database the ranging of different topics how can we find and attend to a relevant video to what we're searching for right the first step you do is to input some query some query into the youtube search bar your topic is deep learning and what effectively can be done next is that for every video in this database we're going to extract some key information which we call the key it's the title maybe associated with that video and to do the search what can occur is the overlaps between your query and the keys in that database are going to be computed and as we do this at each check we make we'll ask how similar is that key the title of the video to our query deep learning our first example right this video of a turtle is not that similar our second example lecture from our course is similar and our third example kobe bryant is not similar and so this is really this idea of computing what we'll come to see as an attention mask measuring how similar each of these keys these video titles is to our query our next and final step is to actually extract the information that we care about based on this computation the video itself and we are going to call this the value and because our search was implemented with a good notion of attention gratefully we've identified the best deep learning course out there for you to watch and i'm sure all of you sitting there can hopefully relate and agree with with that assessment okay so this concept of search is very very related to how self-attention works in neural networks like transformers so let's go back from our youtube example to sequence modeling for example where we have now this sentence he tossed the tennis ball to serve and we're going to break down step by step how self-attention will work over the sequence first let's remember what we're trying to do we're trying to identify and attend to the most important features in this input without any need for processing the information time step by time step we're going to eliminate recurrence and what that means is that we need a way to preserve order information without recurrence without processing the words in the sentence individually the way we do this is by using an embedding that is going to incorporate some notion of position and i'm going to touch on this very very briefly for the sake of time but the key idea is that you compute a word embedding you take some metric that captures position information within that sequence you combine them together and you get an embedding an encoding that has this notion of position baked in you can we can talk about the math of this further if you like but this is the key intuition that i want you to come away with our next step now that we have some notion of position from our input is to actually figure out what in the input to attend to and that relates back to our search operation that i'm motivated with the youtube example we're going to try to extract the query the key and the value features and recall we're trying to learn a mechanism for self-attention which means that we're going to operate on the input itself and only on the input itself what we're going to do is we're going to create three new and unique transformations of that embedding and these are going to correspond to our query our key and our value all we do is we take our positional embedding we take a linear layer and do a matrix multiplication that generates one output a query then we can make a copy of that same positional embedding now we can take a separate and independent a different linear layer do the matrix multiplication and get another transformation of that output that's our key and similarly do this also for the value and so we have these three distinct transformations of that same positional embedding our query our key and our value our next step right is to take these three uh features right and actually compute this attention weighting figuring out how much attention to pay to and where and this is effectively thought of as the attention weighting and if you recall from our youtube example we focus on the similarity between our query and our key and in neural networks we're going to do exactly the same right so if you recall these query and key features are just numeric matrices or vectors how can we compute their similarity their overlap so let's suppose we have two vectors q and k and for vectors as you may recall from linear algebra or calculus we can compute the similarity between these vectors using a dot product and then scaling that dot product and this is going to quantify our similarity between our query and our key matrix vectors this metric is also known as the cosine similarity and we can apply the same exact operation to matrices where now we have a similarity matrix metric that captures the similarity between the query matrix and the key matrix okay let's visualize what the result of this operation could actually look like and mean remember we're trying to compute this self-attention we compute this dot product of our query and our key matrices we apply our scaling and our last step is to apply a function called the softmax which just squashes every value so that it falls between 0 and 1. and what this results in is a matrix where now the entries reflect the relationship between the components of our input to each other and so in this sentence example he tossed the tennis ball to serve what you can see with this heat map visualization is that the words that are related to each other tennis serve ball post have a higher weighting a higher attention and so this matrix is what we call the attention weighting and it captures where in our input to actually self attend to our final step is to use this weighting matrix to actually extract features with high attention and we simply do this it's super elegant by taking that attention weighting multiplying it by our value matrix and then getting a transformed version of of what was our value matrix and this is our final output this reflects the features that correspond to high attention okay that's it right i know that could be fast so let's recap it as as sort of the last thing that we're going to touch on our goal identify and attend to the most important features in the input how does this look like in an architecture right our first step was to take this positional encoding copy three times right we then pass that into three separate different linear layers computing the query the key and the value matrices and then use these values multiply them together apply multiply them together apply the scaling and apply the soft max to get this attention weighting matrix and our final step was to take that attention weighting matrix apply it to our value matrix and get this extraction of the features in our input that have high attention and so it's these core operations that form this architecture shown on the right which is called a self-attention head and we can just plug this into a larger network and it's a very very very powerful mechanism for being able to attend to important features in an input okay so that's it that's i know it's it's a lot to go through very quickly but hopefully you appreciate the intuition of this attention mechanism and how it works final note that i'll make is that we can do this multiple times right we can have multiple individual attention heads so in this example we're attending to iron man himself but we can have independent attention heads that now pay attention to different things in our input for example a background building or this little region shown on the far right which is actually an alien spaceship coming at iron man from the back all right this is the fundamentals of the attention mechanism and its application as i mentioned at the beginning of this section has been most famous and most notable in these architectures called transformers and they're very very very powerful architectures that have a variety of applications most famously perhaps is in language processing so you may have seen these examples where there are these really large language transformers that can do things like create images based on sentences for example an armchair in the shape of an avocado and many other tasks ranging from machine translation to dialogue completion and so on beyond language processing we can also apply this mechanism of self-attention to other domains for example in biology where one of the breakthroughs of last year was in protein structure structure prediction with a neural network architecture called alpha fold 2. and a key component of alpha fold 2 is this exact same self-attention mechanism and what what these authors showed was that this achieved really a breakthrough in the quality and accuracy of protein structure prediction and a final example is that this this mechanism does not just apply only to traditional sequence data we can also extend it to computer vision with an architecture known as vision transformers the idea is very similar we just need a way to encode positional information and then we can apply the attention mechanism to extract features from these images in a very powerful and very high throughput manner okay so hopefully you've gotten a sense over this course of this lecture about sequence modeling tasks and why rnns as a introductory architecture are very powerful for processing sequential data in that vein we discussed how we can model sequences using a recurrence relation how we can train rnns using back propagation through time how we can apply rnns to different types of tasks and finally we in this new component of the sequence modeling lecture we discussed how we can move beyond recurrence and recurrent neural net networks to build self-attention mechanisms that can effectively model sequences without the need for recurrence okay so that concludes the two lectures for today and again i know we're we're running a bit late on time and i apologize but i hope you enjoyed both of today's lectures in the remaining hour that we have dedicated it will focus on the software lab sessions a couple important notes for the software labs we're going to be running these labs in a hybrid format you can find the information about downloading the labs on the course website both the intro to deep learning course website as well as the canvas course website all you will need to run the software labs is a computer internet and a google account and you're going to basically walk through the labs and start executing the code blocks and fill out these to do action items that will allow you to complete the labs and execute the code and we will be holding office hours both virtually on gallertown the link for that is on the course canvas page as well as in person in mit room 10 250 for those of you who are on campus and would like to drop by for in-person office hours alexander and myself will be there so yeah with that i will i will conclude and thank you thank you again for your attention thank you for your patience and hope you enjoyed it hope to see you in the software lab sessions thank you 

okay hi everyone and welcome back to 6s191 today we're going to be taking or talking about one of my favorite subjects in this course and that's how we can give machines a sense of sight and vision now vision is one of the most important senses in sighted people now sighted people and humans rely on vision quite a lot every single day everything from navigating the physical world to manipulating objects and even interpreting very minor and expressive facial expressions i think it's safe to say that for all of us sight and vision is a huge part of our everyday lives and today we're going to be learning about how we can give machines and computers the same sense of sight and processing of of images now what i like to think about is actually how can we define what vision is and one way i think about this is actually to know what we are looking at or what is where only by looking now when we think about it though vision is actually a lot more than just detecting where something is and what it is in fact vision is much more than just that simple understanding of what something is so take this example for take the scene for example we can build a computer vision system for example to identify different objects in this scene for example this yellow taxi as well as this van parked on the side of the road and beyond that we can probably identify some very simple properties about this scene as well beyond just the aspect that there's a yellow van and a uh sorry a yellow taxi and a white fan we can actually probably infer that the white van is parked on the side of the road and the yellow taxi is moving and probably waiting for these pedestrians which are also dynamic objects in fact we can also see these other objects in the scene which present very interesting dynamical scenarios such as the red light and other cars merging in and out of traffic now accounting for all of the details in this scene is really what vision is beyond just detecting what is where and we take all of this for granted i think as humans because we do this so easily but this is an extraordinarily challenging problem for humans to or for machines to be able to also tackle uh this in a learning fashion so vision algorithms really require us to bake in all of these very very subtle details and deep learning has actually revolutionized and brought forth a huge rise in computer vision algorithms and their applications so for example everything from robots in kind of operating in the physical world to mobile computing all of you on your phones are using very advanced machine vision and computer vision algorithms that even a decade ago were training on kind of super complete computers that existed at the highest clusters of computer clusters and now we're seeing that in all of our pockets used every day in our lives we're seeing in biology and medicine computer vision being used to diagnose cancers and as well as autonomous driving where we're having actually machines operate together with physical humans in our everyday world and finally we're seeing how computer vision can help humans who are lacking a sense of sight in terms of able to increase their own accessibility as well so deep learning has really taken computer vision systems by storm because of their ability to learn directly from raw pixels and directly from data and not only to learn from the data but learn how to process that data and extract those meaningful image features only by observing this large corpus of data sets so one example is data facial detection and recognition another common example is in the context of self-driving cars where we can actually take an image of that the car is seeing in its environment and try to process the control signal that it's that it should execute at that moment in time now this entire control control system in this example is actually being processed by a single neural network which is radically different when we think about how other companies like the majority of self-driving car companies like waymo for example their approach that those companies are taking which is a very very different pipelined approach now we're seeing computer vision algorithms operate the entire robot control stack using a single neural network and this was actually work that we published as part of my lab here at csail and you'll get some practice developing some of these algorithms in your software labs as well we're seeing it in medicine biology taking the ability to diagnose radiograph scans from a from a doctor and actually make clinical decisions and finally computer vision is being widely used in accessibility applications for example to help the visually impaired so projects in this research endeavor helped build a deep learning enabled device that could actually detect trails for running and provide audible feedback to visually impaired users so that they could still go for runs in the outdoor world and like i said this is often these are all tasks that we really take for granted as humans because each of us as cited individuals have to do them routinely but now in this class we're going to talk about how we can train a computer to do this as well and in order to do that we need to kind of ask ourselves firstly how can we build a computer to quote-unquote c right and specifically how can we build a computer that can firstly process an image into something that they can understand well to a computer an image is simply a a bunch of numbers in the world now suppose we have this picture of abraham lincoln it's simply a collection of pixels and since this is a grayscale image it's simply a each of those pixels is just one single number that denotes the intensity of the pixel and we can also represent our bunch of numbers as a two-dimensional matrix of numbers one pixel for every location in our matrix and this is how a computer sees it's going to take as input this giant two-dimensional matrix of numbers and if we had a rgb image a color image it would be exactly the same story except that each pixel now we don't have just one number that denotes intensity but we're going to have three numbers to denote red green and blue channels now we have to actually have this way of representing images to computers and i think that's and i think we can actually think about what computer tasks we can now perform using this representation of an image so two common types of machine learning tasks in computer vision are that of recognition and detection and sorry recognition and classification and regression and kind of quantitative analysis on your image now for regression our output is going to take a continuous valued number and for classification our output is going to take a one of k different class outputs so you're trying to output a probability of your image being in one of k classes so let's consider firstly the task of image classification we want to predict a single label for each image for example we can say we have a bunch of images of u.s presidents and we want to build a classification pipeline that will tell us which present president is this an image of on the left hand side and on the output side on the right side we want to output the probability that this was an image coming from each of those particular precedents now in order to correctly classify these images our pipeline needs to be able to tell what is unique to each of those different present so what is unique to a picture of lincoln versus a picture of washington versus a picture of obama now another way to think about this image classification problem and how a computer might go about solving it is at a high level in terms of features that distinguish each of those different types of images and those are characteristics of those types of classes right so classification is actually done and performed by detecting those features in our given image now if the features of a given class are present then we can say or predict with pretty high confidence that our class or our image is coming from that particular class so if we're building a computer vision pipeline for example our model needs to know what the features are and then it needs to be able to detect those features in an image so for example if we want to do facial detection we might start by first trying to detect noses and eyes and mouths and then if we can detect those types of features then we can be pretty confident that we're looking at a face for example just like if we want to detect a car or a door we might look at wheels or or sorry if we want to detect the car we might look for wheels or a license plate or headlights and those are good indicators that we're looking at a car now how might we solve this problem of feature detection first of all well we need to leverage certain information about our particular field for example in human faces we need to use our understanding of human faces to say that okay a human face is usually comprised of eyes and noses and mouths and a classification pipeline or an algorithm would then try to do that exactly and try to detect those small features first and then make some determination about our overall image now of course the big problem here is that we as humans would need to define for the algorithm what those features are right so if we're looking at faces a human would actually have to say that a face is comprised of my eyes and noses and mouths and that that's what the computer should kind of look for but there's a big problem with this approach because actually a human is not very good usually at defining those types of features that are really robust to a lot of different types of variation for example scale variations deformations viewpoint variations there's so many different variations that an image or a three-dimensional object may undergo in the physical world that make it very difficult for us as humans to define what good features that our our computer algorithm may need to identify so even though our pipeline could use the features that we the human may define this manual extraction will actually break down in the detection part of this task so due to this incredible variability of image data the detection of these features is actually really difficult in practice right so because your detection algorithm will need to withstand all of those different variations so how can we do better what we want ideally is we want a way to extract features and detect their presence in images automatically just by observing a bunch of images can we detect what a human face is comprised of just by observing a lot of human faces and maybe even in a hierarchical fashion so and to do that we can use a neural network like we saw in yesterday's class so a neural network-based approach here is going to be used to learn and extract meaningful features from a bunch of data of human faces in this example and then learn a hierarchy of features that can be used to detect the presence of a face in a new image that it may see so for example after observing a lot of human faces in a big data set an algorithm may learn to identify that human faces are usually comprised of a bunch of lines and edges that come together and form mid-level features like eyes and noses and those come together and form larger pieces of your facial structure and your facial appearance now this is how neural networks are going to allow us to learn directly from visual data and extract those features if we construct them cleverly now this is where the whole part of the class gets interesting because we're going to start to talk about how to actually create neural networks that are capable of doing that first step of extraction and learning those features now in yesterday's lecture we talked about two types of architectures first we learned about fully connected layers these dense layers where every neuron is connected to every neuron in the previous layer and let's say we wanted to use this type of architecture to do our image classification task so in this case our image is a two-dimensional image sorry our input is a two-dimensional image like we saw earlier and since our fully connected layer is taking just a list of numbers the first thing we have to do is convert our two-dimensional image into a list of numbers so let's simply flatten our image into a long list of numbers and we'll feed that all into our fully connected network now here immediately i hope all of you can appreciate that the first thing that we've done here by flattening our image is we've completely destroyed all of the spatial structure in our image previously pixels that were close to each other in the two-dimensional image now may be very far apart from each other in our one-dimensional flattened version right and additionally now we're also going to have a ton of parameters because this model is fully connected every single pixel in our first layer has to be connected to every single neuron in our next layer so you can imagine even for a very small image that's only 100 by 100 pixels you're going to have a huge number of weights in this neural network just within one layer and that's a huge problem so the question i want to pose and that's going to kind of motivate this computer vision architecture that we'll talk about in today's class is how can we preserve this spatial structure that's present in images to kind of inform and detect the arc to detect features and inform the decision of how we construct this architecture to do that form of feature extraction so to do this let's keep our representation of an image as a 2d matrix so we're not going to ruin all of that nice spatial structure and one way that we can leverage that structure is now to inherit is now to actually feed our input and connect it to patches of some weight so instead of feeding it to a fully connected layer of weights we're going to feed it just to a patch of weights and that is basically another way of saying that each neuron in our hidden layer is only going to see a small patch of pixels at any given time so this will not only reduce drastically the number of parameters that our next hidden layer is going to have to learn from right because now it's only attending to a single patch at a time it's actually quite nice right because pixels that are close to each other also share a lot of information with each other right so there's a lot of correlation that things that are close to each other and images often have especially if we look kind of at the local part of an image very locally close to that patch there's often a lot of relationships so notice here how the only only one small region in this red box of the input layer influences this single neuron that we're seeing on the bottom right of the slide now to define connections across the entire input image we can simply apply this patch based operation across the entire image so we're going to take that patch and we're going to slowly slide it across the image and each time we slide it it's going to predict this next single neuron output on the bottom right right so by sliding it many times over the image we can kind of create now another two-dimensional extraction of features on the bottom right and keep in mind again that when we're sliding this patch across the input each patch that we slide is the exact same patch so we create one patch and we slide that all the way across our input we're not creating new patches for every new place in the image and that's because we want to reuse that feature that we learned and kind of extract that feature all across the image and we do this feature extraction by waving the connections between the patch that the input is applied on and the neurons that get fed out so as to detect certain features so in practice the operation that we can use to do this this sliding operation and extraction of features is called a convolution that's just the mathematical operation that is actually being performed with this small patch and our large image now i'm going to walk through a very brief example so suppose we have a 4x4 patch which we can see in red on the top left illustration that means that this patch is going to have 16 weights so there's one weight per per in per pixel in our patch and we're going to apply this same filter to a um sorry we're going to apply the same filter of uh 4x4 pack of 4x4 pixels all across our entire input okay and we're going to use the result of that operation to define the state of the neuron in the next layer so for example this red patch is going to be applied at this location and it's going to inform the value of this single neuron in the next layer this is how we can start to think of a convolution at a very high level but now you're probably wondering how exactly or mathematically very precisely how does the convolution work and how does it allow us actually to extract these features right so how are the weights determined how are the features learned and let's make this concrete by walking through a few simple examples so suppose we want to classify this image of an x right so we're given a bunch of images black and white images and we want to find out if in this image there's an x or not now here black is actually the black pixel is defined by a negative one and a white pixel is defined by a positive one right so this is a very simple image black and white and to classify it it's clearly not possible to kind of compare the two matrices uh to see if they're equal right because if we did that what we would see is that these two matrices are not exactly equal because there are some slight deformations and transformations between one and the other so we want to classify an x even if it's rotated shrink or shrunk uh deformed in any way right so we want to be resilient and robust to those types of modifications and still have a robust classification system so instead we want our model to compare the images of an x piece by piece right or patch by patch we want to identify certain features that make up an x and try to and try to detect those instead now if our model can find these rough feature matches in roughly the same places then this is probably again a good indicator that indeed this image is of an x now each feature you should think of is kind of like a mini image right so it's a small two-dimensional array of values so here are some of these different filters or features that we may learn now each of the filters on the top right is going to be designed to pick up a different type of feature in the image so in the case of x's our filters may represent diagonal lines or like in the top in the top left or it may represent a crossing type of behavior which we can see in the middle or the unoriented diagonal line in the opposite direction on the far right now note that these smaller matrices are filters of weights just like in images right so these filters on the top row they're smaller mini images but they're still two-dimensional images in some way right so they're defined by a set of weights in 2d all that's left now is to define an operation a mathematical operation that will connect these two pieces that will connect the small little patches on the top to our big image on the bottom and output a new uh image on the right so convolution is that operation convolution just like addition or multiplication is an operation that takes as input two items instead of like addition which takes as input two numbers convolution is an input that takes as or sorry it's a function that takes as input two matrices in this case or in the more general case it takes as input two functions and it will output a third function so the goal of convolution is to take as input now two images and output a third image and because of that convolution preserves the spatial relationship between pixels by learning image features in small squares of the image or input image data and to do this we can perform an element-wise multiplication of our filter or our feature with our image so we can place this filter on the top left with on top of our image on the bottom and kind of element wise multiply every pixel in our filter with every pixel in in the corresponding overlapped region of our image so for example here we take this bottom right pixel of our filter one we can multiply it by the corresponding image in our sorry corresponding pixel in our image which is also one and the result one times one is one and we can basically do this for every single pixel in our filter right so we repeat this all over our filter and we see that all of the resulting element element-wise multiplications are one we can add up all of those results we get nine in this case and that's going to be the output of the convolution at this location in our next image next time we slide it we'll have a different set of numbers that we multiply with and we'll have a different output as well so let's consider this with one more example suppose we want to compute the convolution of a five by five image and a three by three filter okay so the image is on the left the filter is on the right and to do this we need to cover the input image entirely with our filter and kind of slide it across the image and then that's going to give us a new image so each time we put the filter on top of our image we're going to perform that operation i told you about before which is element-wise multiply every pixel in our filter and image and then add up the result of all of those multiplications so let's see what this looks like so first we're going to place our filter on the top left of our image and when we element wise multiply everything and add up the result we get the value 4. and we're going to place that value 4 in the top left of our new feature map let's call it this is just the output of this operation next time we slide the filter across the image we're going to have a new set of input pixels that we're going to element wise multiply with add up the result and we'll get three and we can keep repeating this all over the image as we slide across the image and that's it once we get to the end of the image now we have a feature map on the right hand side that denotes at every single location the strength of detecting that filter or that feature at that location in the input pixel so where there's a lot of overlap you're going to see that the element wise multiplication will have a large result and where there's not a lot of overlap the element wise multiplication will have a much smaller result right so we can see kind of where this feature was detected in our image by looking at the result of our feature map and we can actually now observe how different filters can be used to produce different types of outputs or different types of feature maps right so in effect our filter is going to capture or encode that feature within it so let's take this woman of a sorry let's take this picture of a woman's face and by applying the output of three different convolutional features or filters we can obtain three different forms of the same image right so for example if we take this this filter this three by three matrix of numbers it's just three by three numbers nine numbers nine weights and we slide it across our entire image we actually get the same image back but in sharpened form so it's a much sharper version of the same image similarly we can change the weights in our filter or our feature detector and now we can detect different types of features here for example this feature is performing edge detection right so we can see that everything is kind of blacked out except for the edges that were present in the original image or we can modify that those weights once again to perform an even stronger or magnified version of edge detection right and again here you can really appreciate that the edges are the things that remain in our output detector and this is just to demonstrate that by changing the weights in our filter our model is able to learn how to identify different types of features that may be present in our image so i hope now you can kind of appreciate how convolution allows us to capitalize on spatial structure and also use this set of weights to extract local features and very easily detect different features by extracting different those filters right so we can learn a bunch of those filters now and each filter is going to capture a different feature and that's going to define different types of objects and well features that the image is possessing now these concepts of preserving spatial structure as well as the local feature extraction using convolution are at the core of neural networks that we're going to learn about today and these are the primary neural networks that are still used for computer vision tasks and i've really shattered all of the state-of-the-art algorithms so now that we've gotten to the operational hood underneath convolutional neural networks which is the convolution operation now we can actually start to think about how we can utilize this operation to build up these full scale like i call them convolutional neural networks right so these networks are essentially appropriately named because under their hood they're just utilizing the same operation of convolution kind of combined with this weight multiplication and addition formulation that we discussed in the first lecture with fully connected layers so let's take a look at how convolutional neural networks are actually structured and then we'll kind of dive a little bit deeper into the mathematics of each layer so let's again stick with this example of image classification now the goal here is to learn the features again directly from the image data and use these learn features to actually identify certain types of properties that are present in the image and use those properties to guide or inform some classification model now there are three main operations that you need to be familiar with when you're building a convolutional neural network first is obviously the convolution operation right so you have convolutional layers that simply apply convolution with your original input and a set of filters that our model is going to learn so these are going to be weights to the model that will be optimized using back propagation the second layer is just like we saw in the first lecture we're going to have to apply a non-linearity right so this is to introduce nonlinearity to our model oftentimes in convolutional neural networks we'll see that this is going to be relu because it's a very fast and efficient form of activation function and thirdly we're going to have some pooling layer which is going to allow us to down sample and downscale our features every time we down scale our features our filters are now going to attend to a larger region of our of our input space right so imagine as we progressively go deeper and deeper into our network each step kind of down scaling now our features are capable of attending to that original image which is much larger and kind of the attention field of the of the later layers becomes and grows much larger as we down sample so we'll go through each of these operations now just to break down the basic architecture of a cnn and first we'll consider the convolutional operation and how it's implemented specifically in the operating and specifically in neural networks we saw how the mathematical operation was implemented or computed or formulated now let's look at it specifically in neural networks so as before each neuron in our hidden layer will compute a weighted sum of inputs right so remember how we talked about yesterday when we were talking about three steps of a perceptron one was apply weights second add a bias and then three was applied in nonlinearity and we're going to keep the same three steps in convolutional neural networks first is we're going to compute a weighted sum of its inputs using a convolutional operation we're going to add a bias and then we're going to activate it with a non-linearity and here's an example of or here's an illustration formulation of that same exact idea written out in mathematical form so each of our weights is multiplied element-wise with our input x we add a bias we add up all the results and then we pass it through a nonlinear activation function so this defines how neurons in one layer in our input layer are connected to our output layer and how they're actually computed that output layer but within a single convolutional layer now we can have multiple filters right so just like before in the percept in the first lecture when we had a fully connected layer that can have multiple neurons and perceptrons now we're having a convolutional layer that can learn multiple filters and the output of each layer of our convolution now is not just one image but it's a volume of images right it's one image corresponding to each filter that is learned now we can also think of the connections of neurons in convolutional layers as in terms of kind of their receptive field like i mentioned before the locations of the ith input is a node in that path that it's connected to at the ith output now these parameters essentially define some spatial arrangement of the output of a convolutional layer because of that and to summarize we've seen now how connections in convolutional layers are defined and how they can output and that output is actually a volume right because we're learning the stack of filters not just one filter and for each filter we output an image so okay so we are well on our way now to understanding how a cnn or a convolutional network is working in practice there's a few more steps the next step is to apply that non-linearity like i said before and the motivation is exactly like we saw in the first two lectures as introduced previously we do this because our images and real data is highly non-linear so to capture those types of features we need to apply non-linearities to our model as well in cnns it's very common practice to apply what is called the relu activation function in this case and you can think of the relu activation as actually some form of thresholding right so when the input image on the left hand side is less than 0 nothing gets passed through and when it's greater than zero the original pixel value gets passed through so it's kind of like the identity function when you're positive and it's zero when you're negative it's a form of thresholding centered at zero and yeah so also you can think of negative numbers right so these are negative numbers that correspond to kind of inverse detection of a feature during your convolutional operation so positive numbers correspond to kind of a positive detection of that feature zero means there's no detection of the feature and if it's negative you're seeing like an inverse detection of that feature and finally once we've done this activation of our output filter map the next operation in the cnn is pooling pooling is an operation that is primarily used just to reduce the dimensionality and make this model scalable in practice while still preserving spatial invariance and spatial structure so a common technique to pooling is what's called max pooling and the idea is is very intuitive as the name suggests we're going to select patches now in our our input on the left hand side and we're going to pool down the input by taking the maximum of each of the patches so for example in this red patch the maximum is six and that value gets propagated forward to the output you can note that the output because this max pooling operation was done with a scale of two the output is half as large as the input right so it's downscaled by also a factor of two and i encourage you some different ways to think about some different ways that we could perform this down sampling or down scaling operation without using a max pooling specific equation is there some other operation for example that would also allow you to kind of down scale and preserve spatial structure without doing the pooling of of using the maximum of each of the patches and there's some interesting kind of changes that can happen when you use different forms of pooling in your network so these are some key operations to convolutional neural networks and now we're kind of ready to put them together into this full pipeline so that we can learn these hierarchical features from one convolutional layer to the next layer to the next layer and that's what we're going to do with convolutional neural networks and we can do that by essentially stacking these three steps in tandem in the form of a neural network in sequence right so on the left hand side we have our image we're going to start by applying a series of convolutional filters that are learned that extracts a feature map or sorry a feature volume right one per map we're going to apply our activation function pull down and repeat that process and we can kind of keep stacking those layers over and over again and this will eventually output a set of feature volumes right here that we can take and at this time now it's very small in terms of the spatial structure we can now at this point extract all of those features and start to feed them through a fully connected layer to perform our decision making task so the objective of the first part is to extract features from our image and the objective of our second part is to use those features and actually perform our detection now let's talk about actually putting this all together into code very tangible code to create our first end-to-end convolutional neural network so we start by defining our feature extraction head which starts with a convolutional layer here with 32 feature maps so this just means that our neural network is going to learn 32 different types of features at the output of this layer we sample spatial information in this case using a max pooling layer we're going to downscale here by a factor of 2 because our pooling side and our stride size is of two and next we're going to feed this into our next set of convolutional pooling layers now instead of 32 features we're going to learn a larger number of features because remember that we've down scaled our image so now we can afford to kind of increase the resolution of our feature dimension as we downscale so we're kind of having this inverse relationship and trade-off as we downscale and can enlarge our attention or receptive field we can also expand on the feature dimension and then we can finally flatten this spatial information into a set of features that we ultimately feed into a series of dense layers and the series of dense layers will perform our eventual decision right so this is going to do the ultimate classification that we care about so so far we've talked only about using cnns for image classification tasks in reality this architecture is extremely general and can extend to a wide number of different types of applications just by changing the second half of the architecture so the first half of the architecture that i showed you before is focusing on feature detection or feature extraction right picking up on those features from our data set and then the second part of our network can be kind of swapped out in many different ways to create a whole bunch of different models for example we can do classification in the way that we talked about earlier or if we change our loss function we can do regression we can change this to do object detection as well segmentation or probabilistic control so we can even output probability distributions all by just changing the second part of the network we keep the first part of the network the same though because we still need to do that first step of just what am i looking at what are the features that are present in this image and then the second part is okay how am i going to use those features to do my task so in the case of classification there's a significant impact right now in medicine and healthcare where deep learning models are being applied to the analysis of a whole host of different forms of medical imagery now this paper was published on how a cnn could actually outperform human expert radiologists in detecting breast cancer directly from looking at mammogram images classification tells us a binary prediction of what an object sees so for example if i feed the image on the left classification is essentially the task of the computer telling me okay i'm looking at the class of a taxi right and that's a single class label maybe it even outputs a probability that this is the taxi versus the probability that it's something else but we can also take a look and go deeper into this problem to also determine not just what this image is or that this image has a taxi but maybe also a much harder problem would be for the neural network to tell us a bounding box right for every object in this image right so it's going to look at this image it's going to detect what's in the image and then also provide a a measure of where that image is by encapsulating it in some bounding box that it has to tell us what that is now this is a super hard problem because a lot of images there may be many different objects in the scene so not only does it have to detect all of those different objects it has to localize them place boxes at their location and it has to do the task of classification it has to tell us that within this box there's a taxi so our networks needs to be extremely flexible and then for a dynamic number of objects because for example in this image i may have one primary object but maybe in a different image i might have two objects so our model needs to be capable of outputting a variable number of different detections on the other hand so here for example there's one object that we're outputting a taxi at this location what if we had an image like this now we have kind of many different objects that are very present in our image and our model needs to be able to have the ability to output that variable number of classifications now this is extremely complicated number one because the boxes can be anywhere in the image and they can also be of different sizes and scales so how can we accomplish this with convolutional neural networks let's consider firstly the very naive way of doing this let's first take our image like this and let's start by placing a random box somewhere in the image so for example here's a white box that i've placed randomly at this location i've also randomized the size of the the box as well then let's take this box and feed it through just this small box and let's feed it through a cnn and we can ask ourselves what's the class of this small box then we can repeat this basically for a bunch of different boxes in our model so we keep sampling randomly pick a box in our image and feed that box through our neural network and for each box try to detect if there's a class now the problem here this might work actually the problem is that there are way too many inputs to be able to do this there's way too many scales so can you imagine for a reasonably sized image the number of different permutations of your box that you'd have to be able to account for would just be too intractable so instead of picking random boxes let's use a simple heuristic to kind of identify where these boxes of interesting information might be okay so this is in this example not going to be a learned kind of heuristic to identify where the box is but we're going to use some algorithm to pick some boxes on our image usually this is an algorithm that looks for some signal in the image so it's going to ignore kind of things where there's not a lot of stuff going on in the image and only focus on kind of regions of the image where there is some interesting signal and it's going to feed that box to a neural network it's first going to have to shrink the box to fit a uniform size right that it's going to be amenable to a single network it's going to warp it down to that single size feed it through a classification model try to see if there's a an object in that patch if there is then we try to say to the model okay there's a there's a class there and there's the box for it as well but still this is extremely slow we still have to feed each region of our heuristic the thing that our heuristic gives us we still have to feed each of those values and boxes down to our cnn and we have to do it one by one right and check for each one of them is there a class there or not plus this is extremely brittle as well since the network part of the model is completely detached from the feature extraction or the region part of the model so extracting the regions is one heuristic and extracting the features that correspond to the region is completely separate right and ideally they should be very related right so probably we'd want to extract boxes where we can see certain features and that would inform a much better process here so there's two big problems one is that it's extremely slow and two is that it's brittle because of this disconnect now many variants have been proposed so to tackle these issues but i'd like to touch on one extremely briefly and just point you in this direction called the faster rcnn method or faster rcnn model which actually attempts to learn the regions instead of using that simple heuristic that i told you about before so now we're going to take as input the entire image and the first thing that we're going to do is feed that to what's called a region proposal network and the goal of this network is to identify proposal regions where there might be interesting boxes for us to detect and classify in the future now that regional region proposal network now is completely learned and it's part of our neural network architecture now we're going to use that to directly grab all of the regions out right and process them independently but each of the regions are going to be processed with their own feature extractor heads and then a classification model will be used to ultimately perform that object detection part now it's actually extremely fast right or much much faster than before because now that part of the model is going to be learned in tandem with the downstream classifier so in a classification model we want to predict a single we want to predict from a single image to a list of bounding boxes and that's object detection now one other type of task is where we want to predict not necessarily a fixed number or a variable number of classes and boxes but what if we want to predict for every single pixel what is the class of this pixel right this is a super high dimensional output space now it's not just one classification now we're doing one classification per pixel so imagine for a large input image you're going to have a huge amount of different predictions that you have to be able to predict for here for example you can see an example of the cow pixels on the left being classified separately from the grass pixels on the bottom and separately from the sky pixels on the top right and this output is created first again the beginning part is exactly as we saw before kind of in feature extraction model and then the second part of the model is an upscaling operation which is kind of the inverse side of the encoding part so we're going to encode all that information into a set of features and then the second part of our model is going to use those features again to learn a representation of whatever we want to output on the other side which in this case is pixel-wise classes so instead of using two-dimensional convolutions on the left we're going to now use what are called transposed convolutions on the right and effectively these are very very similar to convolutions except they're able to do this upscaling operation instead of down scaling of course this can be applied to so many other applications as well especially in healthcare where we want to segment out for example cancerous regions of medical scans and even identify parts of the blood that are affected with malaria for example let's see one final example of how we can have another type of model for a neural network and let's consider this example of self-driving cars so let's say we want to learn one neural network for autonomous control of self-driving cars specifically now we want a model that's going to go directly from our raw perception what the car sees to some inference about how the car should control itself so what's the steering wheel angle that it should take at this instance at the specific instance as it's seeing what it sees so we're trying to infer here a full not just one steering command but we're trying to infer a full probability distribution of all of the different possible steering commands that could be executed at this moment in time and the probability is going to be very high you can kind of see where the red lines are darker that's going to be where the model is saying there's a high probability that this is a good steering command to actually take right so this is again very different than classification segmentation all of those types of networks now we're outputting kind of a continuous distribution of over our outputs so how can we do that this entire model is trained end-to-end just like all the other models by passing each of the cameras through their own dedicated convolutional feature extractors so each of these cameras are going to extract some features of the environment then we're going to concatenate combine all of those features together to have now a giant set of features that encapsulates our entire environment and then predicting these pro these control parameters okay so the the loss function is really the interesting part here the top part of the model is exactly like we saw in lecture one this is just a fully connected layer or dense layer that takes this input to features and outputs the parameters of this continuous distribution and on the bottom is really the interesting part to enable learning these continuous continuous probability distributions and we can do that even though the human never took let's say all three of these actions it could take just one of these actions and we can learn to maximize that action in the future after seeing a bunch of different intersections we might see that okay there's a key feature in these intersections that is going to permit me to turn all of these different directions and let me maximize the probability of taking each of those different directions and that's an interesting way again of predicting a variable number of outputs in a continuous manner so actually in this example a human can enter the car and put a desired destination and not only will its navigate to that location it will do so entirely autonomously and end to end the impact of cnns is very wide reaching beyond these examples that i've explained here and i've also touched on many other fields in in computer vision that i'm not going to be able to talk about today for the purpose of time and i'd like to really conclude today's lecture by taking a look at what we've covered just as a summary just to summarize so first we covered the origins of computer vision and of the computer vision problem right so how can we represent images to computers and how can we define what a convolutional operation does to an image right given a set of features which is just a small weight matrix how can we extract those features from our image using convolution then we discussed the basic architecture using convolution to build that up into convolutional layers and convolutional neural networks and finally we talked a little bit about the extensions and applications of this very very general architecture and model into a whole host of different types of tasks and and different types of problems that you might face when you're building an ai system ranging from segmentation to captioning and control with that i'm very excited to go to the next lecture which is going to be focused on generative modeling and just to remind you that we are going to have the software lab and the software lab is going to tie very closely to what you just learned in this lecture with lecture 3 and convolutional neural networks and kind of a combination with the next lecture that you're going to hear about from alva which is going to be on generative modeling now with that i will pause the lecture and let's reconvene in about five minutes after we we set up with the next lecture thank you 

okay so welcome back hopefully you had a little bit of a break as we got set up so in this next lecture on deep generative modeling we're going to be talking about a very powerful concept building systems that not only look for patterns in existing data but can actually go a step beyond this to actually generate brand new data instances based on those learn patterns and this is an idea that's different from what we've been exploring so far in the first three lectures and this area of generative modeling is a particular field within deep learning that's enjoying a lot of success a lot of attention and interest right now and i'm eager to see how it continues to develop in the coming years okay let's get into it so to start take a look at these three images these three faces and i want you all to think about which face of these three you think is real unfortunately i can't see the chat right now as i'm lecturing but please mentally or submit your answers i think they may be coming in okay mentally think about it the punch line which i'll give to you right now for the sake of time is in fact that all three of these faces are in fact fake they were completely generated as you may or may not have guessed by a by a generative model trained on data sets of human faces so this shows the power and maybe inspire some caution about the impact that generative modeling could have in our world all right so to get into the technical bit so far in this course we've been primarily looking at what we call problems of supervised learning meaning that we're given data and we're given some labels for example a class label or a numerical value and we want to learn some function that maps the data to those labels and this is a course on deep learning so we've been largely concerned about building neural network models that can learn this functional mapping but at its core that function that is is performing this mapping could really be anything today we're going to step beyond this from the class of supervised learning problems to now consider problems in the domain of unsupervised learning and it's a brand new class of problems and here in this setting we're simply given data data x right and we're not necessarily given labels associated with each of those individual data instances and our goal now is to build a machine learning model that can take that raw data and learn what is the underlying structure the hidden and underlying structure that defines the distribution of this data so you may have seen some examples of of unsupervised learning in the setting of traditional machine learning for example clustering algorithms or principal component analysis for example these are all unsupervised methods but today we're going to get into using deep generative models as an example of unsupervised learning where our goal is to take some data examples data samples from our training set and those samples are going to be drawn from some general data distribution right our task is to learn a model that is capturing some representation of that distribution and we can do this in two main ways the first is what is called density estimation where we're given our samples our data samples right they're going to fall according to some probability distribution and our task is to learn an approximation of what the function of that probability distribution could be the second class of problems is in sample generation where now given some input samples right from again some data distribution we're going to try to learn a model of that data distribution and then use that process to actually now generate new instances new samples that hopefully fall in line with what the true data distribution is and in both of these cases our task overall is actually fundamentally the same we're trying to learn a probability distribution using our model and we're trying to match that probability distribution similar to the true distribution of the data and what makes this task difficult and interesting and complex is that often we're working with data types like images where the distribution is very high dimensional right it's not a simple you know normal distribution that we can predict with a known function and that's why using neural networks for this task is so powerful because we can learn these extraordinarily complex functional mappings and estimates of these high dimensional data distributions okay so why why care about generative models what could some applications be well first of all there because they're modeling this probability distribution they're actually capable of learning and uncovering what could be the underlying features in a data set in a completely unsupervised manner and where this could be useful is in applications where maybe we want to understand more about what data distributions look like in a setting where our model is being applied for some downstream task so for example in facial detection we could be given a data set of many many different faces and starting off we may not know the exact distribution of these faces with respect to features like skin tone or hair or illumination or occlusions so on and so forth and our training data that we use to build a model may actually be very homogeneous very uniform with respect to these features and we could want to be able to determine and uncover whether or not this actually is the case before we deploy a facial detection model in the real world and you'll see in today's lab and in this lecture how we can use generative models to not only uncover what the distribution of these underlying features may be but actually use this information to build more fair and representative data sets that can be used to train machine learning models that are unbiased and equitable with respect to these different underlying features another great example is in the case of outlier detection for example when in the context of autonomous driving and you want to detect rare events that may not be very well represented in the data but are very important for your model to be able to handle and effectively deal with when deployed and so generative models can be used to again estimate these probability distributions and identify those instances for example in the context of driving when pedestrian walks in or there's a really strange event like a deer walking onto the road or something like that and be able to effectively handle and deal with these outliers in the data today we're going to focus on two principal classes of generative models the first being inc auto encoders specifically auto encoders and variational auto encoders and the second being an architecture called generative adversarial networks or gans and both of these are what we like to call latent variable models and i just threw out this term of latent variable but i didn't actually tell you what a latent variable actually means and the example that i love to use to illustrate the concept of a latent variable comes from the story from the work of plato plato's republic and this story is known as the myth of the cave and in this legend there is a group of prisoners who are being held imprisoned and they're constrained as part of their punishment to face a wall and just stare at this wall observe it and the only things they can actually see are shadows of objects that are behind their heads right so these are their observations right they're not seeing the actual entities the physical objects that are casting these shadows in front of them and so to their perspective these shadows are the observed variables but in truth there are physical objects directly behind them that are casting these shadows onto the wall and so those objects here are like latent variables right they're the underlying variables that are governing some behavior but that we cannot directly observe we only see what's in front of us they're the true explanatory explanatory factors that are resulting in some behavior and our goal in generative modeling is to find ways to actually learn what these true explanatory factors these underlying latent variables can be using only observations right only given the observed data okay so we're going to start by discussing a very simple generative model that tries to do this and the idea behind this model called auto encoders is to build some encoding of the input and try to reconstruct an input directly and to take a look at the way that the auto encoder works it functions very similarly to some of the architectures that we've seen in the prior three lectures we take in as input raw data pass it through some series of deep neural network layers and now our our output is directly a low dimensional latent space a feature space which we call z and this is the this is the actual uh representation the actual variables that we're trying to predict in training this type of network so i encourage you to think about in considering this this type of architecture why we would care about trying to enforce a low dimensional set of variables z why is this important the fact is that we are able to effectively build a compression of the data by moving from the high dimensional input space to this lower dimensional latent space and we're able to get a very compact and hopefully meaningful representation of the input data okay so how can we actually do this right if our goal is to predict this vector z we don't have any sort of labels for what these variables z could actually be they're underlying they're hidden we can't directly observe them how can we train a network like this because we don't have training data what we can do is use our input data maximally to our advantage by complementing this encoding with a decoder network then now takes that latent representation that lower dimensional set of variables and goes up from it builds up from it to try to learn a reconstruction of the original input image and here the reconstructed output is what we call x-hat because it's a imperfect reconstruction of the original data and we can train this network end-to-end by looking at our reconstructed output looking at our input and simply trying to minimize the distance between them right taking the output taking the input subtracting them and squaring it and this is called a mean squared error between the the input and the reconstructed output and so in the case of images this is just the pixel by pixel difference between that reconstruction and our original input and no here right our loss function doesn't have any labels all we're doing is taking our input and taking the reconstructed output spread out spit out to us at the end of training by our network itself okay so we can we can simplify this plot a little bit by just abstracting away those individual neural layers and saying okay we have an encoder we have a decoder and we're trying to learn this reconstruction and this type of diagram where those layers are abstracted away is something that i'll use throughout the rest of this presentation and you'll probably also come across as you move forward with looking at these types of models further beyond this course to take a step back this idea of using this reconstruction is is a very very powerful idea in taking a step towards this idea of unsupervised learning we're effectively trying to capture these variables which could be very interesting without requiring any sort of labels to our data right and the fact is that because we're we're lowering the dimension dimensionality of our data into this compressed latent space the degree to which we perform this compression has a really big effect on how good our reconstructions actually turn out to be and as you may expect the smaller that bottleneck is the fewer latent variables we try to learn the poor quality of reconstruction we're going to get out right because effectively this is a form of compression and so this idea of the autoencoder is a powerful method a powerful first step for this idea of representation learning where we're trying to learn a com compressed representation of our input data without any sort of label from the start and in this way we're sort of building this automatic encoding of the data as well as self-encoding the the input data which is why this term of auto-encoder comes into play from this from this base bare bone auto encoder network we can now build a little bit more and introduce the concept of a variational autoencoder or vae which is more commonly used in actual generative modeling today to understand the difference right between the traditional auto encoder that i just introduced and what we'll see with the variational autoencoder let's take a closer look at the nature of this latent latent representation z so here with the traditional autoencoder given some input x if we pass it through after training we're always going to get the same input same output out right no matter how many times we pass in the same input one input one output that's because this encoding and decoding that we're learning is deterministic once the network is fully trained however in the case of a variational auto encoder and it and more generally we want to try to learn a better and smoother representation of the input data and actually generate new images that we weren't able to generate before with our auto encoder structure because it was purely deterministic and so vaes introduce an element of stochasticity of randomness to try to be able to now generate new images and also learn more smooth and more complete representations of the latent space and specifically what we do with a vae is we break down the latent space z into a mean and a standard deviation and the goal of the encoder portion of the network is to output a mean vector and a standard deviation vector which correspond to distributions of these latent variables z and so here as you can hopefully begin to appreciate we're now introducing some element of probability some element of randomness that will allow us to now generate new data and also build up a more meaningful and more informative latent space itself right the key that you'll see and the key here is that by introducing this notion of a probability distribution for each of those latent variables right each latent variable being defined by a mean standard deviation we will be able to sample from that latent distribution to now generate new data examples okay so now because we have introduced this element of of probability both our encoder and decoder architectures or our networks are going to be fundamentally probabilistic in their nature and what that means is that over the course of training the encoder is trying to infer a probability distribution of the latent space with respect to its input data while the decoder is trying to infer a new probability distribution over the input space given that same latent distribution and so when we train these networks we're going to learn two separate sets of weights one for the encoder which i'll denote by phi and one for the decoder which is going to be denoted by the variable theta and our loss function is now going to be a function of those weights phi and theta and what you'll see is that now our loss is no longer just constituted by the reconstruction term we've now introduced this new term which we'll call the regularization term and the idea behind the regularization term is that it's going to impose some notion of structure in this probability probabilistic space and we'll break it down step by step um in a few slides okay so just remember um that when we when we're after we define this loss and over the course of training as always we're trying to optimize the loss with respect to the weights of our network and the weights are going to iteratively being updated over the course of training the model to break down this loss term right the reconstruction loss is exactly is very related to as it was before with the auto encoder structure so in the case of images you can think about the pixel wise difference between your input and the reconstructed output what is more interesting and different here is the nature of this regularization term so we're going to discuss this in more detail what you can see is that we have this term d right and it's introducing something about a probability distribution q and something about a probability distribution p the first thing that i that i want you to know is that this term d is going to reflect a divergence a difference between these two probability probability distributions q of phi and p of z first let's look at the term q of phi of z given x this is the computation that our encoder is trying to learn it's a distribution of the latent space given the data x computed by the encoder and what we do in regularizing this network is place a prior p of z on that latent distribution and all a prior means is it's some initial hypothesis about what the distribution of these latent variables z could look like and what that means is it's going to help the network enforce some structure based on this prior such that the learned latent variables z roughly follow whatever we define this prior distribution to be and so when we introduce this regularization term d we're trying to prevent the network from going too wild or to overfitting on certain restricted parts of the latent space by imposing this enforcement that tries to effectively minimize the distance between our inferred latent distribution and some notion of of this prior and so in practice we'll see how this helps us smooth out the actual quality of the distributions we learn in the lane space what turns out to be a common choice for this prior right because i haven't told you anything about how we actually select this prior in the case of variational autoencoders a common choice for the prior is a normal gaussian distribution meaning that it is centered with a mean of 0 and has a standard deviation and variance of 1. and what this means in practice is that encourages our encoder to try to place latent variables roughly evenly around the center of this latent space and distribute its encodings quite smoothly and from this when we have we now that we have defined the the prior on the latent distribution we can actually make this divergence this regularization term explicit and with vaes what is commonly used is this function called the kublac liblar divergence or kl divergence and all it is is it's a statistical way to measure the divergence the distance between two distributions so i want you to think about this term the kl divergence as a metric of of distance between two probability distributions a lot of people myself included when introduced to va's have a question about okay you've introduced this idea said to us like we've we are defining our prior to be a normal gaussian why why it seems kind of arbitrary yes it's a very convenient function it's very commonly used but what effect does this actually have on how well our network regularizes so let's get some more intuition about this first i'd like you to think about what properties we actually want this regularization function to achieve the first is that we desire this notion of continuity meaning that if two points are close in a latent space probably they should relate to similar content that's semantically or functionally related to each other after we decode from that latent space secondly we want our latent space to be complete meaningful meaning that if we do some sampling we should get something that's reasonable and sensible and meaningful after we do the reconstruction so what what could be consequences of not meeting these two criteria in practice well if we do not have any regularization at all what this could lead to is that if two points are close in the latent space they may not end up being similarly decoded meaning that we don't have that notion of continuity and likewise if we have a point in latent space that cannot be meaningfully decoded meaning it just in this example doesn't really lead to a sensible shape then we don't have completeness our latent space is not very useful for us what regularization helps us achieve is these two criteria of continuity and completeness we want to realize points that are close in this lower dimensional space that can be meaningfully decoded and that can be reflect some some notion of continuity and of actual relatedness after decoding okay so with this intuition now i'll show you how the normal prior can actually help us achieve this type of regularization again going to our very simple example of colors and shapes simply encoding the encoding the latent variables according to a non-regularized probability distribution does not guarantee that we'll achieve both continuity and completeness specifically if we have variances rights these values sigma that are too small what this could result in is distributions that are too narrow too pointed so we don't have enough coverage of the latent space and furthermore if we say okay each latent variable should have a completely different mean we don't impose any prior on them being centered at mean zero what this means is that we can have vast discontinuities in our latent space and so it's not meaningful to traverse the latent space and try to find points that are similar and related imposing the normal prior alleviates both of these issues right by imposing the standard deviations to be one and trying to regularize the means to be zero we can ensure that our different latent variables have some degree of overlap that our distributions are not too narrow that they're have enough breath and therefore encourage our latent space to be regularized and be more complete and smoother and this yeah again reiterating that this is achieved by centering our means around zero and regularizing variances to be 1. note though that regularization can the the greater degree of regularization you impose in the network can adversely affect the quality of your reconstruction and so there's always going to be a balance in practice between having a good reconstruction and having good regularization that helps you enforce this notion of a smooth and complete latent space by imposing this normal based regularization okay so with that right now we've taken a look at both the reconstruction component and the regularization component of our loss function and we've talked about how both these encoder and the decoder are inferring and computing a probability distribution over their respective learning learning tasks but one key step that we're missing i'll just go back a bit sorry that was my error okay one key step that we're missing is how we actually in practice can train this network end to end and what you may notice is that by introducing this mean and variance term by imposing this probabilistic structure to our latent space we introduce stochasticity this is effectively a sampling operation operating over a probability distribution defined by these mu and sigma terms and what that means is that during back propagation we can't effectively back propagate gradients through this layer because it's stochastic and so in order to train using back propagation we need to do something clever the breakthrough idea that solved this problem was to actually re-parameterize the sampling layer a little bit so that you divert the stochasticity away from these mu and sigma terms and then ultimately be able to train the network end to end so as as we saw right this notion of probability distribution over mu and sigma squared does not lead to direct propagation because of this stochastic nature what we do instead is now reparametrize the value of z ever so slightly and the way we do that is by taking mu taking sigma independently trying to learn fixed values of mu fixed values of sigma and effectively diverting all the randomness of the stochasticity to this value epsilon where now epsilon is what is actually being drawn from a normal distribution and what this means is that we can learn a fixed vector of means a fixed vector of variances and scale those variances by this random constant such that we can still enforce learning over a probability distribution but diverting the the stochastic stochasticity away from those mean and sigmas that we actually want to learn during training another way to visualize this is that looking at sort of a broken down flowchart of where these gradients could actually flow through in the original form we were trying to go from inputs x through z to a mapping right and the problem we saw was that our probabilistic node z prevented us from doing back propagation what the re-parametrization does is that it diverts the probabilistic operation completely elsewhere away from the means and sigmas of our latent variables such that we can have a continuous flow of gradients through the latent variable z and actually train these networks end to end and what is super super cool about vaes is that because we have this notion of probability and of these distributions over the latent variables we can sample from our latent latent space and actually perturb and tune the values of individual latent variables keeping everything else fixed and generate data samples that are perturbed with a single feature or a single latent variable and you can see that really clearly in this example where one latent variable is being changed in the reconstructed outputs and all other variables are fixed and you can see that this is effectively functioning to tilt the pose of this person's face as a result of of that latent perturbation and these different latent variables that we're trying to learn over the course of training can effectively encode and pick up on different latent features that may be important in our data set and ideally our goal is we want to try to maximize the information that we're picking up on through these latent variables such that one latent variable is picking up on some feature and another is picking up on a disentangled or separate and uncorrelated feature and this is this idea of disentanglement so in this example right we have the head pose changing on the x-axis and something about the smile or the shape of the person's lips changing on the y-axis the way we can actually achieve and enforce this this entanglement in process is actually fairly straightforward and so if you take a look at the standard loss function for a vae again we have this reconstruction term a regularization term and with an architecture called beta vaes all they do is introduce this hyperparameter this different parameter beta that effectively controls the strength of how strictly we are regularizing and it turns out that if you enforce beta to be greater than one you can try to impose a more efficient latent encoding that encourages disentanglement such that with a standard vae looking at a value of beta equals one you can see that we can enforce the head rotation to be changing but also the smile to also change in in conjunction with this whereas now if we look at a beta bae with a much higher value of beta hopefully it's subtle but you can appreciate that the smile the shape of the lips is staying relatively the same while only the head pose the rotation of the head is changing as a function of of latent variable perturbation okay so i introduced at the beginning a potential use case of generative models in terms of trying to create more fair and de-biased machine learning models for deployment and what you will explore in today's lab is using this is practicing this very idea and it turns out that by using a latent variable model like vae because we're training these networks in a completely unsupervised fashion we can pick up automatically on the important and underlying latent variables in a data set such that we can build estimates of the distributions of our data with respect to important features like skin skin tone pose illumination head rotation so on and so forth and what this actually allows us to do is to take this information and go one step forward by using these distributions of these latent features to actually adjust and refine our data set actively during training in order to create a more representative and unbiased data set that will result in a more unbiased model and so this is the idea that you're going to explore really in depth in today's lab okay so to summarize our key points on variational autoencoders they use a compressed representation of the world to return something that's interpretable in terms of the latent features they're picking up on they allow for completely unsupervised learning via this reconstruction they we employ the re-parametrization trick to actually train these architectures and to end via back propagation we can interpret latent variables using a perturbation function and also sample from our latent space to actively generate new data samples that have never been seen before okay that being said the key problem of variational autoencoders is a concern of density estimation trying to estimate the probability distributions of these latent variables z what if we want to ignore that or pay less attention to it and focus on the generation of high quality new samples as our output for that we're going to turn and transition to a new type of generative model called gans where the goal here is really we don't want to explicitly model the probability density or distribution of our data we want to care about this implicitly but use this information mostly to sample really really realistic and really new instances of data that match our input distribution right the problem here is that our input data is incredibly complex and it's very very difficult to go from something so complex and try to generate new realistic samples directly and so the key insight and really elegant idea behind gans is what if instead we start from something super simple random noise and use the power of neural networks to learn a transformation from this very simple distribution random noise to our target data distribution where we can now sample from and this is the really the key breakthrough idea of generative adversarial networks and the way that gans do this is by actually creating a overall generative model by having two individual neural networks that are effectively adversaries they're competing with each other and specifically we're going to explore how this architecture involves these two components a generator network which is functioning and drawing from a very simple data very simple input distribution purely random noise and it's trying to use that noise and transform it into an imitation of the real data and conversely we have this adversary network and discriminator which is going to take samples generated by the generator entirely fake samples and is going to predict whether those samples are real or whether they're fake and we're going to set up a competition between these two networks such that we can try to force the discriminator to classify real and fake data and to force the generator to produce better and better fake data to try to fool the discriminator and to show you how this works we're going to go through one of my absolute favorite illustrations of this class and build up the intuition behind gans so we're going to start really simply right we're going to have data that's just one-dimensional points online and we begin by feeding the generator completely a random noise from this one-dimensional space producing some fake data right the discriminator is then going to see these points together with some real examples and its task is going to be to try to output a probability that the data it sees are real or if they're fake and initially when it starts out right the discriminator is not trained at all so its predictions may not be very good but then over the course of training the idea is that we can build up the probability of what is real versus decreasing the probability of what is fake now now that we've trained our discriminator right until we've achieved this point where we get perfect separation between what is real and what is fake we can go back to the generator and the generator is now going to see some examples of real data and as a result of this it's going to start moving the fake examples closer to the real data increasingly moving them closer such that now the discriminator comes back receives these new points and it's going to estimate these probabilities that each point is real and then iteratively learn to decrease the probability of the fake points and now we can continue to adjust the probabilities until eventually we repeat again go back to the generator and one last time the generator is going to start moving these fake points closer to the real data and increasingly increasingly iteratively closer and closer such that these fake examples are almost identical following the distribution of the real data such that now at this point at the end of training it's going to be very very hard for the discriminator to effectively distinguish what is real what is fake while the generator is going to continue to try to improve the quality of its samples that it's generating in order to fool the discriminator so this is really the intuition behind how these two components of a gan are effectively competing with each other to try to maximize the quality of these uh fake instances that the gener generator is spitting out okay so now translating that intuition back to our architecture we have our generator network synthesizing fake data instances to try to fool the discriminator the discriminator is going to try to identify the synthesized instances the fake examples from the real data and the way we train gans is by formulating an objective a loss function that's known as an adversarial objective and overall our goal is for our generator to exactly reproduce the true data distribution that would be the optimum solution but of course in practice it's not it's very difficult to try to actually achieve this global optimum but we'll take a closer look at how this loss function works the loss function while at first glance right may look a little daunting and scary it actually boils down to concepts that we've already introduced we're first considering here the objective for the discriminator network d and the goal here is that we're trying to maximize the probability of the discriminator of identifying fake data here as fake and real data as real and this term comprising uh a loss over the fake data and the real data is effectively a cross-entropy loss between the true distribution and the distribution generated by the generator network and our goal as the discriminator is to maximize this objective overall conversely for our generator we still have the same overall component this cross-entropy type term within our loss but now we're trying to minimize this objective from the perspective of the generator and because the generator cannot directly access the true data distribution d of x its focus is on minimizing minimizing the distribution and loss term d of g of z which is effectively minimizing the probability that it's generated data is identified as fake so this is our goal for the generator and overall we can put this together to try to comprise the comprise the overall loss function the overall min max objective which has both the term for the generator as well as the term for the discriminator now after we've trained our network our goal is to really use the generator network once it's fully trained focusing in on that and sample from it to create new data instances that have never been seen before and when we look at the train generator right the way it's synthesizing these new data instances is effectively going from a distribution of completely random gaussian noise and learning a function that maps a transformation from that gaussian noise towards a target data distribution right and this mapping this approximation is what the generator is learning over the course of training itself and so if we consider one point right in this distribution one point in the noise distribution is going to learn to one is going to lead to one point in the target distribution and similarly now if we consider an independent point that independent point is going to produce a new instance in the target distribution falling somewhere else on this data manifold and what is super super cool and interesting is that we can actually interpolate and traverse in the noise space to then interpolate and traverse in the target data distribution space and you can see the result of this inter interpolation this traversal in practice where in these examples we've transformed this image of a black goose or black swan on the left to a robin on the right simply by traversing this input data manifold to result in a traversal in the target data manifold and this idea of domain transformation and traversal in these complex data manifolds leads us to discuss and consider why gans are such a powerful architecture and how what the some examples of their generated data actually can look like and so one idea that has been very effective in the practice of building gans that can synthesize very realistic examples is this idea of progressive growing the idea here is to effectively add layers to each of the generator and discriminator as a function of training such that you can iteratively build up more and more detailed image generations as a result of the progression of training so you start you know with a very simple model and the outputs of as a result of this are going to have very low spatial resolution but if you iteratively add more and more network layers you can improve the quality and the spatial resolution of the generated images and this helps also speed up training and result in more stable training as well and so here are some examples of of a gan architecture using this progressive growing idea and you can see the realism the photorealism of these generated outputs another very interesting advancement was in this idea of style transfer this has been enabled by some fundamental architecture improvements in the network itself which we're not going to go into too detail about but the idea here is that we're going to actually be able to build up a progressive growing gan that can also transfer styles so features and effects from one series of images onto a series of target images and so you can see that example here where on one axis we have target images and on the other axis the horizontal axis is the style captures the style of image that we want to transfer onto our target and the result of such an architecture is really remarkable where you can see now the input target has effectively been transformed in the style of those source images that we want to draw features from and as you may have guessed the images that i showed you at the beginning of the lecture were generated by one of these types of gan architectures and these results are very very striking in terms of how realistic these images look you can also clearly extend it to other domains and other examples and i will note that while i've focused largely on image data in this lecture this general idea of generative modeling applies to other data modalities as well and in fact many of the more recent and exciting applications of generative models are in moving these types of architectures to new data modalities and new domains to formulate design problems for a variety of application areas okay so one final series of architecture improvements that i'll briefly touch on is this idea of trying to impose some more structure on the outputs itself and control a little bit better about what these outputs actually look like and the idea here is to actually impose some sort of conditioning factor a label that is additionally supplied to the gan network over the course of training to be able to impose generation in a more controlled manner one example application of this is in the instance of paired translation so here the network is considering new pairs of inputs for example a scene as well as a corresponding segmentation of that scene and the goal here is to try to train the discriminator accordingly to classify real or fake pairs of scenes and their corresponding segmentations and so this idea of pair translation can be extended to do things like moving from labels semantic labels of a scene to generating a an image of that scene that matches those labels going from an aerial view of a street to a map type output going from a label to a facade of a building day to night black and white to color edges of an image to to a filled out image and really the applications are very very wide and the results are quite impressive in being able to go back and forth and do this sort of pair translation operation for example in data from google street view shown here and i think this is a fun example which i'll briefly highlight this here is looking at um coloring from edges of of a sketch and in fact the data that were used to train the scan network were uh images of pokemon and these are results that the network was generating from simply looking at images of pokemon you can see that that training can actually extend to other types of artwork instances beyond the pokemon example shown here okay that just replaces it okay the final thing that i'm going to introduce and touch on when it comes to gan architectures is this cool idea of completely unpaired image to image translation and our goal here is to learn a transformation across domains with completely unpaired data and the architecture that was introduced a few years ago to do this is called cyclegan and the idea here is now we have two generators and two discriminators where they're effectively operating in their own data distributions and we're also learning a functional mapping to translate between these two corresponding data distributions and data manifolds and without going into i could explain and i'm happy to explain the details of of this architecture more extensively but for the sake of time i'll just highlight what the outputs of this of this architecture can look like where in this example the task is to translate from images of horses to images of zebras where you can effectively appreciate these various types of transformations that are occurring as this unpaired translation across domains is is occurring okay the reason why i highlighted this example is i think that the cycle gan highlights this idea of this concept of gans being very very powerful distribution transformers where the original example we introduced was going from gaussian noise to some target data manifold and in cycle gain our objective is to do something a little more complex going from one data manifold to another target data manifold for example horse domain to zebra domain more broadly i think it really highlights this idea that this neural network mapping that we're learning as a function of this generative model is effectively a very powerful distribution transformation and it turns out that cycle gans can also extend to other modalities right as i alluded to not just images we can effectively look at audio and sequence waveforms to transform speech by taking an audio waveform converting it into a spectrogram and then doing that same image based domain translation domain transformation learned by the cycle again to translate and transform speech in one domain to speech in another domain and you may uh may be thinking ahead but this turns out that this was the exact approach that we used to synthesize the audio of obama's voice that alexander showed at the start of the first lecture we used a cycle again architecture to take alexander's audio in his voice and convert that audio into a spectrogram waveform and then use the cyclegan to translate and transform the spectrogram waveform from his audio domain to that of obama so to remind you i'll just play this output welcome to mit 6s191 the official introductory course on deep learning here at mit again right with this sort of architecture the the applications can be very very broad and extend to other um instances and use cases beyond you know turning images of horses into images of zebras okay all right so that concludes the the core of this lecture and the core technical lectures for today um in this section in particular we touched on these two key generative models variational auto encoders and auto encoders where we're looking to build up estimates of lower dimensional probabilistic latent spaces and secondly generative adversarial networks where our goal is really to try to optimize our network to generate new data instances that closely mimic some target distribution with that we'll conclude today's lectures and just a reminder about the lab portion of the course which is going to follow immediately after this we have the open office hour sessions in 10 250 where alexander and i will be there in person as well as virtually in gather town two important reminders i sent an announcement out this morning about picking up t-shirts and other related swag we will have that in room 10 250. we're going to move there next so please be patient as we arrive there will make announcements about availability for the remaining days of the course the short answer is yes we'll be available to pick up shirts at later days as well and yeah that's that's basically it for for today's lectures and i hope to see many of you in office hours today and if not hopefully for the remainder of the week thank you so much 

okay hi everyone and welcome back to 6s191 today is a really exciting day because we'll learn about how we can actually marry two very long-standing fields in computer science and with reinforcement learning with recent advances in deep learning and the topic of today's lecture will actually be on how we can combine those two fields and into the form of what's called deep reinforcement learning now this field is really amazing to me because it moves away from this paradigm that we've been seeing in this class so far and that paradigm is really where we have a machine learning model or a deep learning model that is trained on a fixed data set okay so this is a fixed data set that we go out we collect we label and then we train our model on that data set and in rl or reinforcement learning the deep learning model is is not going to be learning on some fixed data set that's static now our algorithm is going to be placed in some dynamic environment and it's going to be able to explore and interact with that environment in different ways so that it can actually try out different actions and experiences so that it can learn how to best accomplish its task in that environment without any form of human supervision or fixed annotations from a human or any form of human guidance for example all it has to define is simply some objective that the that the algorithm should try to optimize for now this has huge obvious implications in many different fields ranging from robotics to self-driving cars and robot manipulation but also in this new and emerging field of gameplay and building strategies within games for solving and optimizing how an agent or how a player in the game can try to beat out other forms of human players now you can even imagine a combination of robotics and gameplay now where you train robots to play against humans in the real world to take from millions and millions of possibilities here's actually an example that you may have already seen in the past about a deep mind algorithm that was actually trained to play the game of uh of starcraft and it uh or sorry uh yeah starcraft and the algorithm's name was alpha star now here you're seeing it competing against some of the top human players and this was a huge endeavor playing star trek the algorithm creators and this is a huge deal when this came out and let's just watch this video for a little bit to be that good everything that we did was proper it was calculated and it was done well i thought i'm learning something it's much better than i expected it i would consider myself a good player right this is a professional player of starcraft competing against the deep learning algorithm alpha star and against this professional player who came in actually at the beginning of the video extremely confident that they would not only win but kind of win convincingly alpha star ended up defeating the human five to zero right so this is really an extraordinary achievement and we've kind of been keep seeing these type of achievements especially in the field of gameplay and strategies and i think the first thing i want to do as part of this lecture is kind of take a step back and introduce how reinforcement learning and how this this algorithm that you saw in the last slide was trained in the context of everything else that we've learned in this course so we've seen a bunch of different types of models so far in this course and we've also seen several different types of training algorithms as well but how does reinforcement learning compare to those algorithms so in the beginning of this class we saw what was called supervised learning this is an example where we have a data set of x as our input y as our output or our label for that input and our goal here is to learn some functional mapping that goes from x to y and predicts y right so for example we could give a neural network this image of an apple and the goal of our neural network is to is to label this image and say this thing is an apple right so that's an example of a supervised learning problem if we collect a bunch of images of apples we can train that type of model in a supervised way now in yesterday's lecture we also uncovered a new type of learning algorithms called unsupervised learning now here it's different than supervised learning because now we only have access to our data x there are no labels in unsupervised learning and now our only goal is to uncover some underlying structure within our data so here for example we can observe a bunch of different pictures of apples we don't need to say that these are apples and maybe there's other images of oranges right so these are two different images we don't need to give them labels we can just give all of the images to our algorithm and the goal of an unsupervised learning algorithm is simply to identify that okay this one picture of an apple is pretty similar to this other thing right it doesn't know that it's an apple it just knows that these things share similar features to each other and now in reinforcement learning we are given only data in the form of what are called state action pairs states are the observations that an agent or a player sees and actions are the behavior that that agent takes in those states so the goal now of reinforcement learning is to learn how to maximize some metric of its rewards or future rewards over many different time steps into the future so in this apple example now that we've been keeping on the bottom of the slide we might now see that the agent doesn't know that this thing is an apple and now it just learns that it should eat this thing because when it eats it it gets to survive longer because it's a healthy food right this thing will help you keep or help keep you alive right it doesn't understand anything about what it is or maybe even the fact that it's a food right but it just got this reward over time by eating an apple it was able to become healthier and stay alive longer so it learns that that's kind of an action that it should take when it sees itself in a state presented with an apple now rl or reinforcement learning this third panel is going to be the focus of our lecture today so before we go any deeper i want to really make sure that you understand all of the new terminology that is associated to the reinforcement learning field because a lot of the terminology here is actually very intuitive but it's a little bit different than what we've seen in the class so far so i want to really walk through this step by step and make sure that all of it from the foundation up is really clear to everyone so the first and most important aspect of a reinforcement learning system is going to be this agent here we call an agent is anything that will take actions for example it could be a drone that's making a delivery or it could be super mario navigating through a video game the algorithm that you have is essentially your agent so for example in life you are the agent right so you live life and you take some actions so you that makes you the agent the next main component of the system is the environment the environment is simply the world in which the agent exists and takes actions in the place that it moves and lives the agent can send commands to the environment in the form of actions right so it can take steps in the environment with actions and here we can say that a is the set of all possible actions that this agent could take in this environment capital a and an action is almost very self-explanatory but it should be noted that agents choose among a potentially discrete set of actions so for example in this case we have an agent that can choose between moving forwards backwards left or right you could also imagine cases where it's not a fixed number of actions that an agent could take maybe it could be represented using some function so it's a continuous action space and we're going to talk about those types of problems as well in today's lecture but just for simplicity we can consider kind of a finite space of actions now when the agent takes actions in an environment its environment will send it back observations right so an observation is simply how an agent interacts with its environment and you can see that it's sending back those observations in the form of what are called states now a state is just a concrete and immediate situation that the agent finds itself in at this moment in time t right so it takes some action at t it gets some state back at time t plus one and in addition to getting the state back we also get what's called a reward back from our environment now a reward is simply a feedback let's think of this as a number it's a feedback by which we can measure the success of an agent's actions right so for example in a video game when mario touches a coin a gold coin he wins points right those are examples of rewards that are distributed to mario when he touches that gold coin now from any given state an agent sends out or sends outputs in the form of actions to the environment and then the environment will return in response with those states and those rewards now if there are any rewards there are also cases where rewards may be delayed right so you may not get your reward immediately you may see the reward later on in the future and these effectively will evaluate your agent's actions in a delayed state now let's dig into this reward part a little bit more now we can also identify or kind of formulate the total reward that the agent is going to see which is just the sum of all rewards that an agent gets after any certain time t okay so here for example capital r of t is denoted as this total reward or what's also called the return and that can be expanded to look like this so it's reward at time t plus its reward at time t plus one and so on all the way on to infinity right so this is kind of like the total reward that the agent is going to achieve from here on out now it's often useful to not only consider the total reward right but the total or the total sum of rewards but also to think about what we call a discounted total reward or a discounted sum of future awards now here the discount factor you can think of is this gamma parameter so now instead of just adding up all of the rewards we're going to multiply it by some discount factor which is just a number that we can define and that's that number is chosen to effectively dampen these rewards effects on the agent's choices of an action now why would we want to do this so the discounting factor is effectively designed to make future rewards worth less than current rewards right so it enforces some form of short-term or greedy learning in the agent and this is actually a very natural form of of rewards that we can think about so let's suppose i offered you i can give you a reward of five dollars for taking this class today or a reward of five dollars in five years from now right you still take the class today but you're gonna get the reward either today or in five years from today now which reward would you prefer to take they're both five dollars right but you would prefer the reward today because you actually have some internal discounting factor for those future rewards that make it less valuable for you now the discount factor here is simply multiplied by future rewards uh as discovered by the agent as it moves through the environment as it takes actions and like i said these effectively dampen the rewards effect on the agent's choice of action now finally there's a very important function in reinforcement learning and this is going to be kind of the main part of the first half of today's lecture where we look at this function called the q function okay and let's look at how we can actually define what the q function is from what we've learned in the previous slides and all the terminology that we've built up thus far now the q function is simply a function that takes as input two things it takes as input the current state that the robot is in or sorry that the agent is in and the current action that it executes in this current state okay so it's gonna take as input the observation that the agent sees and the action that it's going to take in response to that observation and the output of our q function is going to return the expected total future sum of rewards that an agent can receive after that point given that action that it took in this particular state right so if we took a really good action in this state our q function should return a very high expected total future reward if we took a bad action though in this state we should see that our q function should reflect that and return a a very poor or a penalized future reward right so now the question is if we are given this let's say magical q function let's not say i i'm going to let's say you don't care about how you get the q function let's say i give it to you so you're going to get some black box function that you can feed in two things too the state and the action and it's going to give you this expected future return on your rewards as a return now how can we let's say we are agents in this environment how can we choose what action to take if we have access to this magical q function right so let me ask this kind of question to all of you and i hope maybe you can take a second to reflect on this let's say you're given this q function i'll just repeat the question again you're given this function it takes this input and kind of evaluates how good of an action this action is in your current state how can you use that function to determine what is the best action well ultimately we want to kind of uh infer we need to create a policy let's call it pi a policy is something that just takes as input the state and that policy the goal of the policy is to infer or output the best possible action that could be executed in this state right so think of a policy as just another function it takes as input your state and outputs what you should do in this state right so that's the ultimate goal that's what we want to compute we want to find what action do we take now that we're in this state how can we use our q function to create or that policy function well one strategy and this strategy is a is exactly the correct strategy that you would take is that you should just define your policy to choose the action that will maximize your q function right that will maximize your future rewards well how do you do that well you have some let's say finite list or finite array of possible actions you can feed each action into your q function along with your current state and each time you feed in an action you're going to get like how good of an action was that from your q function that's what your q function tells you and we just want to find the maximum of all of those different future returns of rewards right so by finding the arg max we identify the action that yielded the best or the greatest return on our future rewards as possible from this current state so we can actually define our policy our optimal policy at this time we'll call pi star which is denoting the optimal policy at this state s should just be the arg max or the max the the action that results in the maximum q value at this time now in this lecture we're going to focus primarily on two forms of reinforcement learning algorithms that can broadly be disconnected into two different categories one of which is where we try to actually learn this q function and then use it in the exact way that i just described on the previous slide right so assuming we have the q function we can solve the problem of reinforcement learning just by using this arg max over our q q function but then the question is how do we get the q function right previously i just said i'll give it to you magically but in practice you'll need to actually learn it right so how can we use reinforcement learning and deep learning to learn that q function that's going to define what we call a value learning algorithm and the second class of algorithms are what we call policy learning algorithms because they try to directly learn the policy that governs the agent and then sample actions from this policy right so you can think of almost policy learning as a much more direct way of modeling the problem instead of finding a q function and then maximizing that q function you want to just directly find your policy function and use the neural network to optimize or identify your policy function from a bunch of data and then sample actions from that policy function so first let's look at value learning and then we'll build up our intuition and then we'll extend on in the second half of today's lecture on to policy learning so let's start by digging deeper firstly onto the into this q function since the q function is actually the foundational basis of value learning now the first thing i'll introduce is the atari breakout game which you can see here on the left the objective of this game is essentially that you have this uh paddle on the bottom this paddle can move left or right that's the agent so the agent is the paddle you can move either left or right at any moment in time and you also have this ball that's coming at this moment coming towards the agent now the objective of the agent the paddle is to move in ways that it reflects the ball and can hit the ball back towards the top of the screen and every time it hits the top of the screen it kind of breaks off some of those colored blocks at the top and that's why we call this game breakout because essentially you're trying to keep breaking out as many of those top pieces as possible you lose the game when that ball passes the paddle and that's when the game is finished so you got to keep hitting the paddle up and up until you break out all of the balls if you miss the ball then you lose the game so the q function essentially tells us the expected total return that we can expect given a certain state and action pair and the first point i want to convey to all of you is that it can be sometimes extremely challenging for even humans to define what is a good state and action pair right and what is a bad state in action pair and out of i'm going to show two examples here out of these two examples a and b these are two examples of both states and actions so you can see in state a the action of the agent is to do nothing right as the ball is coming towards that agent and it's going to bounce off the ball back towards the top of the board or state b where the ball is coming towards the side and the the agent is kind of out of the direction of the ball right now but it can kind of zoom in at the last second and kind of hit the ball between these two state action pairs which do you think will have the higher future expected return on rewards maybe enter your thoughts through the chat and let's see what you guys think so and i just want to convey again that i think the first time i looked at this i was really interested in paul in the state action pair a because this was a very conservative action to take and i thought actually this would be the best action that could or this would be the state action pair that i would have a higher return on rewards and we can actually look at a policy that behaves in the manner of this agent here so now i'm going to play a video on the left hand side which actually shows kind of this strategy of the agent where it's constantly trying to get under the ball and hit the ball back up towards the middle of the screen so let's watch this agent in practice you can see it is successfully hitting off and breaking off balls or sorry breaking off these colored boxes at the top of the screen so it is winning the game but it's doing so rather slowly right so it's kind of breaking off all of the points in the middle because its strategy is kind of conservative to hit the middle of the screen now let's consider a strategy b by a different agent where the agent may even potentially purposely move away from the ball just so we can come back and hit it from an angle what does this agent look like so here's an example where you can see the agent is really targeting the edges of the screen why because the second it attacks the edges it's able to break off a ton of the balls from the top of the screen just by entering through a small door kind of that it creates in the in the in the side of the screen so this is a very desirable policy for the model but it's not a very intuitive policy that humans would think about necessarily that you need to attack those edges just for kind of unlocking this cheat code almost where you can now start to kill all of the balls or blocks from the top of the screen so we can now see that if we know the q function then we can directly use it to determine what is the best action that we should take at any moment in time or any state in our environment now the question is how can we train a neural network or a deep learning model to learn that q function right so we kind of have already answered the second part of this problem given a q function how to take an action but now we need to answer the first part of the problem which is how do we even learn that q function in the first place well there's two different ways that we could do this one way is an approach very similar to the formulation of the function itself so the function takes as input a state and an action and it outputs a value right so we can define a neural network to do exactly the same thing we can define a neural network that takes as input convolutional uh convolutional layers with an image input that defines the state just the pixels of our board or of our game at this moment in time and also simultaneously we can feed in our action as well the action that the agent should take at this given state for example move towards the right right now the output we can train our neural network to just output this q value okay that's one option that we could use for training this system with a deep neural network and i'll talk about the actual loss function in a little bit later but first i want to share also this idea of a different type of method and i want to kind of debate a little bit which one you think might be better or more efficient now instead of inputting both the state and the action we're going to input only the state and we're going to learn to output the q value for all of the different possible actions so imagine again we have a finite set of actions we could have let's say there are k actions we could have our neural network output k different q values each one corresponding to taking action one through action k now this is often much more convenient and efficient than the option shown on the left hand side why is that because now when we want to evaluate what is the best action we only need to run our network once given a state so we feed in our state we extract all of the q values at one time simultaneously and then we just find the one with the maximum q value and that's the action that we want to take let's say we find that this q value q of s a two this is the highest one out of all of the k q values so this action a two is the one that we ultimately pick and move forward with at that state whereas if we're on the left hand side we have to feed in each action independently so to find what is the best action we'd have to run this neural network k times and propagate the information k times all the way over so what happens if we take first of all uh all of the best actions and the point i want to get at here is i want to start answering this question of how we can actually train this q-valued network right and hopefully this is a question that all of you have kind of been posing in your minds thus far because we kind of talked about how to use the q value how to kind of structure your neural network in terms of inputs and outputs to find the q value but never we talked about how to actually train that neural network so let's think about the best case scenario right how an agent would likely perform in the ideal case what would happen if we took all of the best actions okay well this would mean that the target return would be maximized right and this can serve as our ground truth to train the agent right so to train the agent we're going to follow this simple ideology which is in order to maximize the target return we're going to try to sorry to in order to train the agent we will ultimately maximize our target return right so to do this we're going to first formulate our expected return as if we were going to take all of the best actions from this point onwards right so we pick some action now and then after that point we pick all of the best actions so think kind of optimistically right i'm going to take some action now and i'll take all of the best actions in the future what would that look like right that would be my initial reward at time t that i get right now by taking this current action right so i take some action my environment immediately tells me what my reward is so that's i can hold that in my memory as ground truth for my training data and then i can select the action that maximizes the expected return for the next future scenario right and of course we have one to apply this discounting factor as well uh so this is our target right so now let's let's start about thinking about estimating this this prediction value right q of s a so this is the cube given our current state and action pair that is the expected total return given our state and our action and that is exactly what our network is going to predict for every one of our different actions how can we use these two terms here to kind of formulate this loss function that will train our neural network and provide some objective that we can back propagate through so this is known as the q loss and this is how we can train deep neural networks right so we predict some value right so we pass our state interactions through our network we get some value that's on the right side that's the predicted value right here and then our target value is just going to be obtained by observing what our reward was at time t so we take that action and we actually get a reward back from our environment that's a tangible reward that we can hold in memory and that's going to define our second part of the loss function and then combine that with what we expect our expected total future return on rewards would be and that's our target value now our loss function is just simply we want to minimize the divergence between these two so we subtract them we take some normalize some norm over them like a mean squared error and that's our q loss so we're going to try to have our predictions match as closely as possible to our ground truth okay great so now let's summarize this whole process because i've kind of thrown a lot at you let's see how all of this kind of comes together and can shape up into a solid reinforcement learning algorithm that first tries to learn the q function so first we're going to have our deep neural network that takes as input our state at time t and it's going to output the q values for let's say three different possible actions in this case there are three actions that our breakout agent can take it can either go left it can go right or it can stay in the middle and take no action okay so for each of these three actions there's going to be one output so we'll have three different outputs each output is going to correspond to the q value or the expected return of taking that action in this state so here for example the actions are right stay and we can see that the q values for example are 20 for left three for stay and zero for right and we can actually this makes sense right because we can see that the ball is moving towards the left the paddle is already a bit towards the right so the paddle is definitely going to need to move towards the left otherwise it has no chance of really hitting that ball back into place and continuing the game right so our neural network is able to output these three different q values we can compute our policy which is what is the best action that we want to take in this given state by simply looking at our three q values looking at which is the highest right so in this case it's 20 which corresponds to the q value of action number one which corresponds to the action of going left and that's the action that our network should execute or our agent should execute so because that had the highest q value now our agent is going to step towards the left now when that agent steps towards the left that's an action that gets fed back to our environment and our environment will respond with a new state that new state will be again fed over to our deep neural network and we'll start this process all over again now deepmind showed how these networks which are called deep queue networks could actually be applied to solving a variety of different types of atari games not just breakout like we saw in the previous slide but a whole host of different atari games just by providing the state and oftentimes the state was in the form of pixels only on kind of the left hand side you can see how these pixels are being provided to the network pass through a series of convolutional layers like we learned about yesterday and then extracting some two-dimensional features from these images of the current state passing these to fully connected layers and then extracting or predicting what our q values should be for every single possible action that the agent could take at this moment in time so here for example the agent has a bunch of different actions that it could execute all on the right side and it's going and the network is going to output the q value for executing each of these different uh possible actions on the right now they tested this on many many games and showed that over 50 percent of the games just by applying this kind of very intuitive algorithm where the agent is just stepping in the environment trying out some actions and then maximizing its own own reward in that environment they showed that this technique of reinforcement learning was able to surpass uh human level performance just by training neural networks to accomplish and and operate in this manner now there were certain games that you can see on the right hand side that were more challenging but still given how simple and kind of clean and elegant this algorithm was it's actually to me still amazing that this works at all right now there are several downsides to q-learning and i want to talk about those and those will kind of motivate the next part of today's class so the first downside is the complexity side right so in q learning our model can only we can only kind of model scenarios right that we can define the action space in discrete and small pieces right so because and the reason for this is because we have to have our network output all of these actions as q values right so our number of outputs has to be number one has to be fixed right because we can't have our neural network output a variable number of outputs and it has to be also relatively small we can't have extremely large or infinite action spaces or continuous action spaces necessarily and that's the other downside right so we can't easily handle at least in this basic version of q learning handle continuous action spaces there have been some updates of q learning that now can handle continuous action spaces but in the in the foundational version of q learning typically we can only handle discrete or fixed amounts of actions that an agent can tackle at any moment in time the other side is the flexibility right so our policies are now determined deterministically by our q function right so we have some q function that our network outputs and we simply pick the maximum the the action that has the maximum q value right so that's a deterministic operation it cannot learn for example stochastic processes where where our environment is kind of stochastic right and may change different output outcomes in the future so to address both of these issues actually we're going to have now the second part of today's lecture where we're going to consider a new class of or a different class rather of reinforcement learning algorithms which are called policy gradient methods now just again as a recap we already saw what value learning was where we tried to first learn the q function and then extract actions based on maximizing our q function now in policy learning we're simply going to directly learn the the policy that governs our action taking steps or our ideal action taking steps now if we have that policy function that's a function that takes us input state and outputs an action we can simply sample from that function given in a state and you will return an action so essentially now we want to let's first revisit the q function the q neural networks these take us input a state and the output a expected maximum reward or return that you can expect to have if you take this action each of these actions right now in policy learning we're going to be not predicting the q values but we're going to directly optimize for pi of s so our policy its state s which is the policy distribution you can think of it directly governing how the agent should operate and act when it sees itself placed in this state so the outputs here give us the desired action in a significantly more direct way right so the outputs represent now not a expected reward that the agent can achieve in the future but now it represents the probability that this is a good action that it should take in this in this state right so it's a much more direct way of thinking about this problem for example what's the probability that this action will give us the the maximum reward so if we can predict these probabilities for example here let's say we train this policy gradient model it inputs a state and it outputs three different probabilities now for example the probability that going left is the best action is ninety percent the probability that staying is ten percent and the probability of going right is zero right we can aggregate them into pi to define what's called our our policy function and then to compute the action that we should take we will sample from this policy function right so keep in mind that now we see that our sample is action a but this is a distribution right this is defining a probability distribution and every time we sample from our pi of s our policy function ninety percent of times we'll get action a right because our weight on action is ninety percent but 10 percent of times we'll also get a different action right and that's because this is a stochastic distribution and again because this is a distribution all of the outputs of our neural network must add up to one here in order to maintain that this is a valid and well-formed probability right now what are some advantages of this of this type of formulation in comparison to q learning well the first thing is that we can now handle continuous action spaces not just situations where our actions are fixed and predefined maybe we can have a situation where instead of saying okay my actions are i go left i go right or i stay that's it now let's say i can have a continuous number or a continuous spectrum of actions which is actually an infinite set of actions ranging from i want to go really really fast to the right to really really slow to the right to really really slow to the left or really really fast to the left right so it's this full spectrum of kind of speeds that you want to move in this axis so instead of saying which direction should i move which is a kind of a classification problem now i want to say how fast should i move now when we plot the probability of our of our action space the likelihood that any action in this action space is a is a good action or an action that will return positive rewards when we plot that distribution now we can see that it has some mass over the entire number line right everywhere on the number line it's going to have some mass some probability not just at a few specific points or discrete points that are kind of predefined categories now let's look at how we can actually model these continuous actions with policy gradient learning methods so instead of predicting a probability that an action given all possible states which is actually not possible if there's an infinite number of actions in a continuous space let's assume that our output distribution now is a it's a set of parameters that define some continuous distribution okay so for example instead of outputting the mass at an infinite number of places along our number line let's define or let's have our network output a mean and a variance that defines a normal distribution which will describe how we have or how we have the mass all over the number line so for example in this image we can see the parallel needs to move towards the left so if we plot the distribution here we can see that the density of this distribution that the network predicts it has mean negative one right and it has a variance of 0.5 and when we plot it we can see okay it has a lot of mass on the side that's going fast to the left and we can also see that now when we sample from this distribution we get an action that we should travel left at a speed of 0.8 let's say meters per second or whatever the units may be right and again every time that we sample from this because this is a probability distribution we might see a slightly different answer right and again just like before because this is a probability distribution the same rules apply the mass underneath this entire density function has to integrate to one right because it's a continuous space now we use integrals instead of discrete summations but the story is still the same great so now let's look at how policy gradients work in a kind of concrete example so let's revisit firstly this reinforcement learning training loop or kind of environment loop that we saw earlier in today's lecture so let's think about how we could train for example in this in this toy problem an autonomous vehicle to drive using reinforcement learning and policy gradients algorithms so the agent here is a vehicle right so its goal is to drive as long as possible without crashing the states that it has the observations are coming from some camera maybe other sensors like lidar and so on and it can take actions in the form of the steering wheel angle that it wants to execute so it can decide the angle that the steering wheel should be turned to in order to achieve some reward which is maximizing the distance traveled before it has to crash for example so now let's look at how we can train a policy gradient network in the context of self-driving cars as a complete example and kind of walk through this step by step so first of all we're going to start with our agent right so we start by placing our agent somewhere in the world our environment right we initialize it now we run a policy now remember our policy is defined by a neural network it's outputting the actions that it should take at any moment in time and because it's not really trained this policy is going to crash pretty early on but we're going to run it until it crashes and when it crashes we will now record all of the states the actions and the rewards that it took at each point in time leading up to that crash right so this is kind of our memory of what just happened in this situation that led up to the crash then we're going to look at the the half of the state action rewards that came close to the crash and we're going to so those are all rewards that we can say kind of resulted in a low outcome or a bad outcome it's a low reward state right so for all of those actions let's decrease their probability of ever being executed again in those states right so let's try some other actions at those places and for the actions here close to the beginning part or close to the part far away from our bad rewards from our penalties from our crash let's try to increase the probability of those actions being repeated again in the future right so now the next time we repeat this process because we increase these good actions and we've decreased the bad actions now we can see the same process again we reinitialize the agent we run until completion and we update this policy and we can keep doing this over and over again and you'll see that eventually the agent learns to perform better and better because it keeps trying to optimize all of these different actions that resulted in high returns and high rewards and it minimized all of the actions that came very close to crashing so it doesn't kind of repeat those actions again in the future and eventually it starts to kind of follow the lanes without crashing and i mean this is really incredible because first of all we never taught this algorithm anything about uh lanes right we never taught anything about roads all we told it was when it crashed and when it survived right all we gave it was a reward signal about survival and it was able to learn that in order to maximize its rewards okay probably it should detect the lanes it should detect the other cars for example and we should try to avoid those types of crashes now the remaining question that needs to be seen is how we can actually update our policy on every training iteration in order to accomplish this right so in order to decrease the probability of all of the bad actions and increase the probability of all the good actions how can we kind of construct our training algorithm to promote those good likelihoods and demote the bad likelihoods now the part of this algorithm on the left-hand side that i'm talking about is steps four and five right so these are the these are the main uh kind of next pieces that i want to start to talk about and i want to kind of start by saying what is the big challenge first of all with this whole training loop and before i get to that let's say here first with when we want to think about kind of the loss function for training this type of model for training policy gradients in practice we'll dissect it first of all to kind of look at what pieces it's comprised of so the loss function is going to be composed of two different terms the first one is this log likelihood think of this almost as being like a probability or the likelihood that it should select a given action from a given state this is the output of our policy right so this is uh again just to repeat it one more time the likelihood that our agent thinks that it should execute action a given that it's in state s we multiply we put that inside a log so it's a log probability and that's that's it right now we multiply this by the total discounted reward that was achieved at this time t right now let's assume that we got a lot of reward at this time a lot of return by taking this action that had a high log likelihood this loss will be very um very large right and it will reinforce those actions because they resulted in very good returns right on the other hand if we had rewards that are very low for a specific action so those are bad rewards or kind of penalties right then those actions should not be sampled again in the future because it did not result and it's a in a desirable action so we'd want to in order to minimize our loss function we'd want to minimize the log probability the probability of sampling that action again in the future that would essentially be equivalent to minimizing the probability of mass at that specific action state given that that point or that observation in the environment and when we plug this into our gradient descent algorithm to train our neural network we can actually see that policy gradients uh kind of highlighted here in blue right and this is where this method kind of gets its name from right because when you uh take the gradient and you plug it into your your gradient descent optimizer the gradient that you're actually taking over is the gradient of your policy multiplied by the returns right and that's kind of where the connection comes into play a little bit now i want to talk a little bit as i conclude and wrap up this lecture is how we can extend this framework of reinforcement learning into real life right so in the beginning we talked a lot about gameplay and how reinforcement learning is really amazing uh shown to do amazing things in games where we have kind of like a full observation of our environment but in the real world we don't have a full observation of our environment oftentimes we don't even really know what's going to happen next a lot of time in a lot of games if i move my piece here there is a fixed number of possible outcomes that can be shown back to me right so i have some level of understanding of my future even if it's at one state there's a fixed number of possible states in games often whereas in real life that's very much not the case now we can get around this somewhat by training and simulation but then the problem is that modern simulators often do not accurately really depict the real world and they do not really transfer to reality either because of that when you deploy them so one interesting point here is where does this whole thing break down right so if we want to execute this algorithm on the left the key step that is going to break everything is step number two if we try to do this in reality why because in reality we can't just go out and crash our vehicles and let them drive on roads and just crash just so that we can teach them how not to crash right so that's not the way that we can train let's say an autonomous vehicle to operate in reality that's why simulators do come into into play a little bit and one cool result that we actually created in our lab was actually how we have been developing a brand new or photorealistic simulation engine for self-driving cars that is entirely data-driven so it overcomes this boundary of of uh this this gap and this transferability of whatever we learn in simulation cannot be applied to reality now because of this extremely photorealistic simulation that is entirely data driven the simulator is actually amenable to learning reinforcement learning policies and helping us use real data of the world to actually generate and synthesize brand new real data um that is very photorealistic and allows us to train reinforcement learning in simulation and still be transferred and deployed into reality so in fact we did exactly this and we showed that you can place agents within your simulator within this simulator train them using policy gradients the exact same algorithm that you learned about in today's lecture and all of the training was done entirely within the simulator within the simulator called vista and then we took these policies and actually put them directly on board our full-scale autonomous vehicle on real roads right and there was a direct transferability of all of the policies that came over and this represented actually the first time ever that a full-scale autonomous vehicle was capable of being trained using only reinforcement learning there was no human supervision and it was trained entirely in simulation and then deployed directly into reality so that's a really awesome result and actually in lab three we're going to well all of you will have the ability to kind of number one play around with the simulator number two train your own agents using policy gradients or whatever reinforcement learning algorithm you would like to train within simulation and design your own autonomous vehicle controllers and then kind of as part of the prices the winners will will invite you to put your policies on board the car and you can actually say that you trained uh an entire autonomous vehicle end-to-end using a single neural network and put your neural network onto the car and how to drive on in real roads so i think that's a pretty awesome result that should motivate all of you so now we've covered the fundamentals behind value learning policy learning and policy gradient approaches um very briefly i'm going to talk about some very exciting advances and applications that we're seeing all over the world right so first we turn to the game of go where reinforcement learning agents were put to the test against kind of human champions and achieved what at the time was an extremely exciting result so just very briefly a quick introduction to the game of go this is a 19 by 19 grid extremely high dimensional in terms of gameplay it's played between two players white and black and the objective of the game is to actually occupy more territory than your opponent okay so the problem of go or the game of go is actually extremely complex and in fact the full size of the board with this 19 by 19 board there are a greater number of legal board positions than atoms in the entire universe that's two times 10 to the 170 positions now the objective here is to train an ai algorithm using reinforcement learning to master the game of go not only to beat the existing gold standard software but also to beat the current world champion so google deepmind rose to this challenge a couple years ago they developed a reinforcement learning based platform that defeated the grand champion of go and the idea at its core was actually very clean and elegant so first they trained the neural network to watch human expert go players and learn how to imitate their behaviors then they used pre-trained networks to play against and a reinforcement learning policy network which actually allowed that policy to go beyond what the human experts had had done in the past but actually achieved what's called superhuman performance right and one of the tricks that really brought this algorithm to life and really to the next level was the use of an auxiliary network which took as input the states of the board and predicted how good of a state was that position and given that network this ai was kind of able to hallucinate almost how we could reach these great board positions and the steps it would have to take to reach these great board positions and use those as actions essentially to guide its predicted values and finally in recently published research of these approaches uh just over about a year or a year and a half ago called alpha zero that only uses um self-play and generalizes to three frame this board games starting from chess to shogi and go and in all of these examples the authors demonstrated that it was not only kind of possible to to learn how to really master the game but again to surpass uh human performance and surpass uh the at the time the gold standard humans so now just to wrap up today's lecture i'm going to uh first or uh just to wrap up today's lecture we talked about first the foundations of reinforcement learning in the beginning kind of what defines the reinforcement and learning environment ranging from the the agent to the environment to how they interact with each other then we talked about two different approaches for solving this reinforcement learning problem first with q learning where we want to estimate using a neural network the expect the total future expected return on rewards and then later with policy gradients and how we can train a network to directly optimize our policies directly and how we can actually extend these to continuous action spaces for example autonomous driving right so yeah so in the next lecture we're going to hear from ava on kind of the new exciting and recent advances of deep learning literature in the field and also some kind of interesting limitations about what you've been learning as part of this class and hopefully that can spark some motivation for you in the future of how you can kind of build on everything that you learned in this class and advance the field even further because there's still so much to be done so in a few minutes we'll hear from ava on that thank you very much 

hello everyone welcome back um so this is going to be our last lecture that's given by alexander and myself and traditionally this is one of my favorite lectures where we're going to discuss some of the limitations of deep learning algorithms that we've been learning about as well as some of the new and emerging research frontiers that are enjoying a lot of success in both theory and application today okay so first things first we're going to cover some logistical information given that this is the last lecture that we'll be giving prior to our guest lectures so the first point is regarding the class t-shirts thank you to everyone who came by yesterday and picked up t-shirts i will note importantly that the t-shirts we have a lot of t-shirts this year due to backlog from from last year so the t-shirts today are going to be different than those that you may have received yesterday the t-shirts today are our specific class t-shirts and we really really like these as kind of a way for you all to remember the course and you know signify your participation in it so please come by if you would like these one of these t-shirts and um cannot guarantee that they will be there remaining tomorrow or friday due to the demand so please come by today it will be in 10 250 where the in-person office hours are right after this lecture completes so to recap on where we are right now and where we are moving forward as you've seen we've you know pushed through deep learning at sort of this breakneck pace and we've done this split between our technical lectures and our hands-on project software labs so in the remaining four lectures after this one we're going to actually extend even further from some of the new research frontiers that i'll introduce today to have a series of four really exciting guest lectures from leading industry researchers and academic researchers in the field and i'll touch briefly on what they're each be talking about in a bit but please note that we're really really lucky and privileged to have these guest speakers come and participate and join our course so highly highly encourage and recommend you to join and to join synchronously specifically as far as deadlines and where things stand with the labs submissions and the final project assignments lab 3 was released today the reinforcement learning lab all three labs are due tomorrow night at midnight uploaded to canvas and there are going to be specific instructions on what you need to submit for each of the three labs for that submission and again since we've been receiving a lot of questions about this on piazza and by email the submission of the labs is not required to receive a grade to receive a passing grade this is simply for entry into each of the three project competitions what is required to receive credit for the course is either the deep learning paper review or the final project presentation we'll get into more specifics on those in a couple of slides as a reminder but these are both due friday the last day of class okay so about the labs really really exciting hopefully you've been enjoying them i think they're i mean they're awesome i think they're awesome but that's also because i'm biased um we because we made them but nevertheless right really exciting opportunity to enter these cool competitions and win some of these awesome prizes in particular again reminding you that they're going to be due tomorrow night and so for each of the labs we have selected a prize that kind of corresponds with the theme of that lab hopefully you'll appreciate that and as alexander mentioned again i'd like to highlight that for lab three focusing on reinforcement learning for autonomous vehicle control there's the additional opportunity to actually deploy your model if you are a winner on mit's full-scale self-driving autonomous vehicle okay a couple notes on the final class projects won't go too in detail because these are this is summarized on the slides as well as on the course syllabus but the critical action item for today is that if you are interested in participating in the final project proposal competition you must submit your name of your group by tonight and midnight i checked the sign up sheet right before starting this lecture and there's a lot of spaces open so plenty of opportunities for you to be eligible and to compete and to you know pitch us your ideas and be eligible for these prizes again more and more on the logistics of this on the syllabus and on this summary slide the second option as you may know is the for the for receiving credit for the course is the written report of a deep learning paper and the instructions for this summarized on the syllabus it will be due by the end of class 3 59 pm eastern time on this friday okay so that covers most of the logistics that i wanted to touch on the last and very exciting point is the amazing and fantastic lineup of guest lectures that we have slotted for tomorrow's lecture time as well as friday's lecture time briefly i'll talk about each of these amazing speaker sets and what they're going to talk about and contribute to our our exploration of deep learning so first we're going to have two speakers from this really exciting emerging self-driving car company innoviz and they're going to be talking about using a data modality called lidar to train and build deep learning models for autonomous vehicle control our second lecture will be from jasper snook from google research and google brain in cambridge he's going to be talking about the role and importance of uncertainty in deep learning models i'll introduce a little bit about this as a prelude to his lecture today next we'll have professor anima anand kumar from nvidia and caltech she's the head of ai research of all of nvidia and she's going to give a talk which i personally am super excited about on the applications of ai for science and how we can do this in a really principled manner and finally we're going to have miguel and jenny from rev ai which is a company that specializes in natural language processing and automatic speech recognition and they'll be talking about some of their recent research and product development in this line for all of these four series of lectures i highly highly encourage you to attend these synchronously and in person excuse me synchronously live virtually the reason is that we have been publishing the recordings for alexander and i's lectures on canvas but we will need to share our these these lecture recordings with our guest speakers for full company approval prior to publishing them on our course website and so this may take time so we cannot guarantee that the lectures will be shortly accessible via recording so please please please try to attend live synchronously if you can okay so that was a breakneck run through of the logistics of where we've been where we're going for the remainder of the course and as usual if you have any questions please direct them to me and alexander via piazza or by email great so now let's dive into the really really exciting part and the core technical content for this last lecture so so far right 6s191 it's a class about deep learning and why deep learning is so powerful and so awesome and we've specifically seen the rise of deep learning algorithms in a variety of different application domains and begun to understood how it has the potential to revolutionize so many different research areas and parts of society ranging from advances in autonomous vehicles and robotics to medicine biology and health care reinforcement learning generative modeling robotics finance security and the list goes on and on and hopefully now as a result of this course you have a more concrete understanding of how you can take deep learning and neural network approaches and apply them in your own research for example or in in lines of investigation that may be of interest to you and with that you've also come away with some understanding of how the foundations of these algorithms actually function and what that means for actually enabling some of these incredible advances specifically on you know taking a step back about what this learning problem even means so far we've been been seeing algorithms that can move from raw data and train a neural model to output some sort of decision like a prediction a classification taking some action and this was the case for both supervised learning examples as well as reinforcement learning examples and we've also seen the inverse right where we're trying to now instantiate and create new data instances based on some learned probability distribution of the data as was the case with unsupervised learning and generative modeling now what's common to both these directionalities is the fact that neural networks if you really abstract everything away what you can think of them as is very very powerful function approximators all they're learning all they're doing is learning a mathematical and computational mapping from data to decision or vice versa and to understand this in a bit more detail i think it's very helpful to go back to a very famous theorem that was proposed back in 1989 called the universal approximation theorem and at the time it generated quite the stir in the community because it states this very very bold and powerful statement that it then proves in theory and in math which is that a feed-forward neural network with just a single neural layer is absolutely sufficient to approximate any arbitrary function to any uh any arbitrary continuous function to any arbitrary precision and so far right in this class we've been exploring this concept of deep neural models right which constitute taking these individual neural layers and stacking them into a hierarchy but the universal approximation theorem is saying okay you don't even need to stack layers together all you need is one layer you can have as many nodes as many neurons as you want and you should be able to approximate a continuous function to some arbitrary precision so what does this really mean right what this theorem is kind of getting at is let's say you have some problem and you believe that you can reduce your problem down to a set of inputs and a set of outputs that can be related to each other via a continuous function if that's the case in theory you could build a neural net that could solve that problem that could learn that mapping that's pretty powerful but if we take a step more closely right there there are a bit of a few caveats to this theorem and what it's stating firstly it's making no claims or guarantees on the number of units the number of neurons that could be necessary to achieve you know this this continuous function prediction and furthermore it leaves open the question of how do you actually find the weights that could solve this problem and actually result in this architecture it doesn't tell you how you could go about doing this it just says the weights may exist right and finally and perhaps most importantly this theorem is making no claims about how well this neural net function would be able to generalize beyond the setting that it was trained on this theorem i think gets a a larger historical issue that was present in the computer science community which is this idea of this possible over hype about the promise of artificial intelligence and the promise of deep learning in particular and for us as a community you know you're all here clearly interested in learning more about deep learning i think we need to be extremely careful in terms of how we market and advertise these algorithms because while the universal approximation theorem makes a very powerful claim the reality is that such potential over hype that could result from either theory or some of the success that deep learning algorithms are enjoying in practice could in truth be very very dangerous and in fact historically there were two so-called ai winters where research in artificial intelligence and a neural network specifically came to an abrupt decline because of this issue of concern about you know what could be the potential um downstream consequences of ai and whether these methods would actually be robust and generalizable to real-world tasks and so in keeping with this spirit and being mindful about what are the limitations of these types of technologies in the first part of this lecture we're going to dive deeply into some of the most profound limitations of modern deep learning architectures and hopefully you'll get a sense of why these types of limitations arise and start to think about how we can actually in the future advance research to mitigate some of these these limitations the first example is one of my favorites and something that i like to highlight every year which is this idea of generalization how well will a neural network generalize to unseen examples that it may not have encountered before during training and so there was this really beautiful paper in 2017 that took a very elegant approach to exploring this problem and all they did was they took images right from this data set called imagenet very very famous data set in computer vision and in this data set these simple images right are associated with a class label an individual class label and what the authors of this paper did was they took each of these images and for each of them they flipped a coin a flip or flipped a die right a k-sided die where the number of the sides of the die were the number of possible classes that they wanted to assign a label to to each of these images and instead of taking the existing true class label for a corresponding image they used the result of that random sample to assign a brand new label to each of the images in the data set and what this meant is that now the images were no longer associated with their true class label just had a random assignment and in fact two images that could in truth belong to the same class could now be mapped to completely different classes altogether so the effect of this is that they're trying to randomize their labels entirely from this they then asked okay now we have this data set where we have images with completely random labels what happens if we train a neural network on this data set that's exactly what they did and as you may they trained their ima their model on this random label data set and then tested it on a test set where the true labels were preserved and as you could expect in the testing set the error and the accuracy of this network the accuracy of the network fell sort of exponentially as a function of the degree of random randomness imposed in the label assignment process what was really interesting though was what they observed when they now looked at the performance on the training set and what they found was that no matter how much they randomized the labels of the data the model was able to get nearly 100 accuracy on the training set what this means is that no modern neural networks can basically perfectly fit to entirely random data and it highlights this idea that the universal approximation theorem is getting at right the deep neural networks or neural networks in general are just very very powerful function approximators that can fit some arbitrary function even if it has entirely random labels to drive this point home even further again this idea that neural networks are simply excellent function approximators let's consider this 2d example right where we have some points lying on a 2d plane and we're trying to fit a function using a neural network to this data what the neural network is doing is learning a maximum likelihood estimate of the of the training data in that region where it has observations and so if we give our model a new data point shown here in purple that fall somewhere on that training distribution of data that it has seen before yeah we can expect that our neural network would be able to predict a reasonable maximum likelihood estimate for that data point but what happens now if we consider the out-of-distribution regions what is occurring what is the neural network predicting on regions to the left and to the right of these purple points well we have absolutely no guarantees on what the data could even look like in these regions and as a result we have no guarantees on how the function in truth could behave and this is a huge huge limitation that exists in modern deep neural network architectures which is this question of how do we know when our network doesn't know how can we establish guarantees on the generalization bounds of our network and how can we use this information to inform the training process the learning process and the deployment process and so a slight revision to this idea of neural networks being excellent function approximators is the idea that yes they are excellent function approximators but really only when they have training data and to build off this idea a little further i think there can be this popular conception which we've seen can be really inflated by the media in the popular press is that deep learning is basically magic alchemy it can be the be all and all solution to any problem that may be of interest right and so this spawns this belief which is incorrect that you can take some arbitrary training data apply some magical beautiful complex network architecture turn the crank on the learning algorithm and expect it to spit out excellent results but that's simply not how deep learning works and i want you to be really really mindful of this as it's perhaps the most important thing to take away if you're actually going to try to build deep neural models in practice this idea of garbage in garbage out your data is just as important if not more important than the actual architecture of the network you're trying to build so this motivates some discussion of what i think is one of the most pertinent and important failure modes of neural networks highlighting just how much they depend on the nature of the data on which they're trained so let's say we have this black and white image of this dog and we pass it into some convolutional neural network architecture and our task is to train this network to colorize this black and white image to paint it with color the result when this example was passed into some you know state-of-the-art cnn was what you see here on the right look closely and what can you notice there's this pink region under the dog's nose which i think hopefully you can all appreciate is actually the dog's fur but the network is predicting that it should be pink why could this be the case well if you consider what could be some of the examples of the data that was used to train this network probably pictures of dogs right and amongst those pictures of dogs it's very very likely that many of those images will be of dogs sticking their tongues out right because that's what dogs do and so the cnn this convolutional architecture that's trained to colorize a black and white image may have learned to effectively map the region under the dog's nose to be the color pink and so what this example really highlights is that deep learning models are powerful at building up representations based on the data that they have seen during training so this raises the question of okay yeah you've told me that you know neural networks depend on very very heavily on the distribution and the nature of the data that they're trained on but what happens if now they're deployed in the real world and they have to encounter and handle data instances where they may not have encountered before very infamously and very tragically a few years ago a car an autonomous vehicle from tesla that was operating autonomously uh ended up crashing in a major accident and resulting in the death and the killing the the death of the driver and it turned out in fact that that driver who was killed in this crash had in fact reported multiple instances over the prior weeks in which his his or her their car would um would would behave abnormally would swivel and turn towards this highway barrier which turned out to be the very same barrier into which the car ended up crashing and what was revealed was when they looked at this instance a little bit further was that it turned out as they were able to investigate from google street view images was that some years ago in the in the data on which the autonomous system uh was built on it lacked the actual physical construction of this barrier that was uh that ended up being the barrier into which the car crashed later on and so effectively what this instance highlights was that the car had encountered a real world data example that was an out of training distribution example and was unable to handle this situation effectively resulting in this accident and the death of the driver so this idea of potential failure modes have very very significant real world consequences and it's these very same types of failure modes and this notion of what could be safety critical applications that motivate the need for really being able to understand when the predictions from deep learning models can or cannot be trusted effectively when when the network's predictions are associated with a degree of uncertainty and this is a very emerging research direction in in deep learning that's important to a number of safety critical applications autonomous striving like i highlighted earlier biology and medicine facial recognition and countless other examples beyond these types of real world applications this notion of uncertainty is also very important from a more fundamental perspective where we're thinking about how to build neural models that can handle sparse limited or noisy data sets that may be that may carry imbalances with respect to the data or the features that are represented and so jasper in his guest lecture tomorrow will be focusing on this topic of uncertainty in deep learning and we'll give a really comprehensive overview of the field and talk about talk as well about some of his recent research advances in this direction and so to prepare a little bit for that and to set the stage for tomorrow's lecture i'm going to touch briefly on this notion of uncertainty in deep learning to get intuition about what uncertainty means and what are the different types of uncertainties we can encounter in deep learning models so to do that let's consider a very simple classification example where we're given some images of cats and dogs we train a model to predict an output a probability of the image containing a cat or the image containing a dog remember that importantly our model is being trained to predict probabilities over a fixed number of classes in this case two cat dog and so what could happen potentially when we now feed in a net an image that contains both a cat and a dog well because these are probabilities that are being outputted over a fixed number of classes the network is going to have to return estimates that ultimately sum to one but in truth right this image is containing both a cat and a dog so ideally we'd like to generate a high probability estimate for cat as well as a high probability estimate for dog and so this type of example highlights this case where there can be noise or variabilities in our input data such that even though we are able to use a traditional model to output a prediction probability in terms of the classification decision that classification decision is not strictly associated with a sense of confidence or a sense of uncertainty in that prediction and so this type of example where we have a noise or variability inherent to the data can result in a form of uncertainty known as aleatoric uncertainty or data uncertainty now let's suppose that instead of an image of containing both a cat and a dog we input an image of a horse right and again the network is being trained to predict dog or cat and again these probabilities will have to sum to one so yeah we can generate these probability estimates but ideally we want our network to be able to say i'm not confident in this prediction this is a high uncertainty estimate because this instance of an image of a horse is very very unlike the images of the cats and dogs seen during training and so this is an instance where the model is being tested on an out-of-distribution example this horse and so we again expected to not be very confident in the prediction to have a high uncertainty but it's a fundamentally different type of uncertainty than simple data noise or data variability here we're trying to actually capture the model's effective confidence in its prediction on a out of domain out of distribution example and this is this notion of epistemic uncertainty or also model uncertainty so these two types of uncertainties are commonly thought of as the dominant forms of uncertainty in deep neural models although i will say that there is also some hot debate in the field about whether these two data and model uncertainties capture all the types of uncertainty that could exist and so jasper is going to dive really deeply into this topic so i'm going to leave this discussion of uncertainty at that and again encourage you to please attend his lecture tomorrow because it will be very very um really really exciting a third failure mode to consider in addition to you know issues of generalization issues of extending to out-of-distribution regions and and predicting uncertainty is what you may know and have heard of as adversarial examples and the idea here is that we can synthetically construct a data instance that will function as an adversary to the network it will fool it to generate a incorrect and spurious prediction so the idea here is we can take an image and apply some degree of noisy perturbation that generates a adversarial example which to our eyes as humans looks fundamentally the same to our original input image but the difference is that this perturbation actually has a profound effect on the network's decision where with the original image the network may correctly classify this image as an image of a temple but upon applying this adversarial perturbation the resulting prediction is now completely nonsensical predicting with 98 probability that this image is actually of an ostrich and so what is really clever and really important in this idea of adversarial generation and adversarial attacks is this perturbation piece it appears to be random noise but in truth it's constructed in a very clever way so as to actually generate an example that will function effectively as an adversary to understand how this works let's recall our actual training operation when we're optimizing our neural network to according to some loss according to some objective function recall that our objective here is to apply this algorithm gradient sent to optimize an objective or loss function l or in this case j and specifically what we're trying to adjust as a function of this optimization procedure is the weights of the neural network w and we're doing this by constraining fixing our input image and its associated label our input data and its associated label and then asking how do small iterative adjustments in the network weights change the objective change the loss with respect to our input data with adversarial attacks we're now doing in many ways the opposite optimization where now we're asking how can we modify our input data by fixing the weights fixing the labels and seeking to increase the loss as a function of this perturbation on the input data and this is how we can actually train a neural network to learn this perturbation this this perturbation this perturbation entity that can then be applied to an input image or an input data instance to create an adversarial attack extension of this idea and this example that i showed in the 2d case was recently explored by a group of students here at mit in alexander madrid's research group where they devised an algorithm for synthesizing examples adversarial examples that can be robustly adversaria adversarial over a set of complex transformations like rotations color changes and so on and they extended this idea not only in the 2d case but also to the 3d case such that they were able to show that they could actually use 3d printing techniques to synthesize physical adversarial examples in the real world that could then be taken um you could then take a picture of and use that picture to feed it into a model which would then misclassify this actual 3d object based on that 2d image and so the example that they highlighted was these 3d printed turtles where many of these adversarial turtle examples were actually incorrectly classified by a network as a rifle when the network was trained to predict predict the types of you know the class label of the of the image okay so the final limitation that i'd like to very very briefly touch on is this notion of algorithmic bias and that's this idea which you've explored through our second lab that neural network models ai systems more broadly can be susceptible to significant biases such that these biases can actually have potentially very real and potentially detrimental societal consequences and so hopefully through your exploration of our second lab you have begun to understand how these types of issues may arise and what could be strategies to actually effectively mitigate algorithmic bias so these limitations that i touched on are just the the tip of the iceberg right in terms of what limitations currently exist and it's certainly not an exhaustive list that i've written up here but to highlight a little bit again in your lab you have focused and dove really deeply into this exploration of algorithmic bias in computer vision and in tomorrow's guest lecture will dive quite deeply into uncertainty and how we can develop methods for robust uncertainty estimation in deep learning and in the second second portion of this lecture today i'm going to use the remaining time to tackle and discuss two other classes of limitations and use these classes of limitations in terms of structural information and actual optimization to introduce what are some of the new and emerging research frontiers in deep learning today okay so the first that i will dive into is this idea of encoding structure into deep learning architectures imposing some sort of domain knowledge and some sort of knowledge about the problem at hand to intelligently design the network architecture itself to be better suited for the task at hand and we've already seen examples of this right perhaps most notably in our exploration and discussion of convolutional neural networks where the fundamental operation of the convolution was intricately linked to this idea of spatial structure in image data and envisioned data and we saw how we could use convolution as an operation to build up these networks that were capable of extracting features and preserving spatial invariants from spatial data but beyond images beyond sequences there can be many many different other types of data structures that exist and namely ones that are more irregular than a standard 2d image for example and one really interesting data structure modality is that of graphs which are a very powerful way to represent a vast variety of data types from social networks to abstract state machines that are from theoretical computer science to networks of human mobility and transport to biological and chemical molecules as well as networks of proteins and other biological modules that may be interacting within cells or within the body all of these types of data and all of these instances and application areas are very amenable to being thought of as a graph based structure and so this has motivated this really rapidly emerging field in deep learning today of extending neural networks beyond quote unquote standard encodings or standard data structures to cut capture more complicated data geometries and data structures such as graphs and so today we're going to talk a little bit about graphs as as a data structure and how they can inspire a new sort of network architecture that is related to convolution but also a bit different and so to discuss this i'll i'll first remind you of how convolutional neural networks operate right operating on 2d image data where as we saw yesterday the idea is to take a filter kernel and effectively iteratively slide this kernel over input images or input features and do this process over the course of the entire 2d image represented that it's a 2d matrix of numbers and do this in order to be able to extract local and global features that are present in the data in a way that is spatially invariant and preserves spatial structure again the key takeaway is that we're taking a 2d matrix a smaller filter matrix and sliding it over this 2d input this idea of taking the filter of weights and applying it sort of iteratively across a more global input is the exact idea that's implement that's implemented in graph convolutional neural networks where now we are representing our data not as a 2d matrix but rather as a set of nodes and a set of edges that connect those nodes and preserve some degree of information about the relationship of the nodes to one another and again graph convolutional neural networks function by take learning weights for a feature kernel which again is just like a weight matrix that we saw in convolutional networks and now rather than sliding that weight kernel over a 2d matrix that kernel is effectively going to traverse the graph going around to different nodes and at each instance of this traversal it's going to look at what are the neighboring nodes of of a particular node according to the edges of the graph and we're going to iteratively apply matrix multiplication operations to extract some features about the local connectivity of the graph structure and this process is applied iteratively over the course of the entire graph structure such that the learning process can pick up on weights that can extract and pick up on information about the patterns of connectivity and structure that exist to the graph and so this process repeats iteratively across all the nodes in the graph going forward such that at the end at the end of this iterative operation we can then aggregate the information that was picked up by each of these iterative node visits and used this to build a more global representation of what the feature feature space could look like for this graph example shown here so this is a very very brief very high level overview of the idea behind graph convolutional networks but hopefully you get a bit of intuition about how they work and why they can be very relevant to a variety of data types data modalities and application areas and indeed graph neural networks are now being used in a variety of different domains ranging from applications in chemistry biology looking at modeling small molecules according to a graph like structure which naturally you can think of it right as having the atoms in those small molecule be represented the atoms and the elements in that small molecule being represented as nodes in a graph and the local bond structure that connects these uh individual atoms together as vertices excuse me as as edges in the graph yeah as edges in the graph preserving some local structure about what the structure of the molecule looks like and in fact these very same types of graph convolutional networks were used a couple years ago to discover a novel antibiotic compound called halisin that had very potent antibiotic properties and was structurally completely dissimilar from traditional classes of antibiotics and so really this this idea of imposing this graph structure to model and represent small molecules has tremendous applications in drug discovery and in therapeutic design other application areas include the the context of urban mobility urban planning traffic prediction and so google maps it turns out uses graph based architectures to model the flow of traffic in cities and use these models to actually improve their estimates of estimated time of arrival for providing directions returned to the user via google maps which is a functionality that i know all of us are very likely to appreciate and finally in the in the past couple of years due to the nature of the coven 19 pandemic initially at the start of the pandemic there was a lot of excitement about applying deep learning and ai to various problems related to coping 19 from both the public health perspective as well as you know the fundamental biology perspective and diagnosis perspective and so one example using graph neural networks was in incorporating both spatial data and temporal data to perform very accurate forecasting of the likely spread of covet 19 in local neighborhoods and local communities a final example of a different type and class of data that we may encounter in addition to graphs sequences 2d images is that of 3d sets of points which often are called point clouds and they're effectively completely unordered sets of of a cloud of points where still there's some degree of spatial dependence between the points and much like as we saw with convolutional neural networks and 2d images we can perform many of the same types of prediction problems on this 3d types of data structures and it turns out that graph convolutional neural networks can be extended very naturally to these point cloud data sets and the way this is done is by actually dynamically computing graphs as as of effectively a mesh present in a 3d space where this point cloud exists and so you can think of this graph structure as being imposed on this 3d point cloud manifold where you can use the graph to effectively preserve the connectivity of the points and maintain spatial invariance really really cool stuff being done in this domain in applications like neural rendering and 3d graphics all right so the final and second sort of new frontier research direction i'm going to touch on is this idea of automated machine learning and learning to learn and over the the course of five years of teaching this course every single year one of the most popular questions and most popular areas of interest for you all has been this question of how do we actually design neural network architectures how do we choose hyper parameters how do we optimize the model design itself to achieve the best performance on our desired task and this is really one of the fundamental motivations behind this idea of automated machine learning where as you've probably seen there's there's a bit of alchemy there's a bit of mysticism behind how you actually go about building an architecture for some problem of interest and there's a degree of practice of trial and error and of expert knowledge that's involved in this process and so the motivation behind this field of automated machine learning is what if we could what if we could use ai what if we could use machine learning to actually solve this design problem in the first place and so the goal here is to try to build a learning algorithm that learns which model specified by its its hyper parameters its architecture could be most effective at solving a given problem and this is this idea of automl so the original automl framework and there have been many many extensions and efforts that have extended beyond this used a reinforcement learning setup where there were these two components the first being what they called a controller neural net architecture and the function of this controller architecture was to effectively propose a sample architecture what the model potentially could look like and the way is this this is defined is in terms of the hyper parameters of that network right the number of filters the number of layers so on and so forth and then following this effective spawning of a sample candidate architecture by the controller network that network was then in turn trained to get some accuracy some predictive accuracy on a desired task and as a result of that the feedback and the performance of that actual evaluation was then used to inform the training of the controller iteratively improving the controller algorithm itself to improve its architecture proposals for the next round of optimization and this process is iteratively repeated over the course of training many many times generating architectures testing them giving that the giving the resulting feedback to the controller to actually learn from and eventually the idea is that the controller will converge to propose architectures that achieve better accuracies on some data set of interest and will assign low low output probabilities to architectures that perform poorly so to get a little bit more sense about how these agents actually work the idea is that at each step of this iterative algorithm your controller is actually proposing a brand new network architecture based on predictions of hyperparameters of that network right so if you had a convolutional network for example these parameters could include the number of filters the the size of those filters the degree of striding you're employing so on and so forth and the next step after taking that child's network is to then take your training data from your original task of interest use that child network that was spawned by this rnn controller and generate a prediction and compute an accuracy from that prediction that could then be used to update the actual rnn controller to propose iteratively better and better architectures as a function of its training and more recently this idea of of automl has been extended from this original reinforcement learning framework to this broader idea of neural architecture search right where again we're trying to search over some design space of potential architecture designs and hyperparameter hyper parameters to try to identify optimal optimally performing models and so this has really kind of exploded and is very commonly used in modern machine learning and deep learning design pipelines particularly in industrial applications so for example designing new architectures for image recognition and what is remarkable is that this idea of automl is not just hype right it turns out that these algorithms are actually quite strong at designing new architectures that perform very very well and what you can see on this plot that i'm going to show on the right is the performance on a image object recognition task of networks that were designed by humans and what i'm now going to show you in in red is the performance of architectures that were proposed by an automl algorithm and what you can appreciate is that the neural architecture search and automl pipeline was able to produce architectures that actually achieved superior accuracy on this image recognition task with fewer parameters and and more recently and extending finally beyond this there's been a lot of interest in taking this concept of automl and extending it to this broader idea of auto ai designing entire data processing learning prediction pipelines that go end to end to um sort of beginning from data curation all the way to deployment and using ai machine learning algorithms as a way to actually optimize the components of this process itself and so i encourage you to think about what it would mean if we could actually achieve this capability of designing ai that can generate new neural networks and new machine learning model models that can be very very performant on tasks of interest of course this will reduce our troubles and our difficulty in actually designing the networks themselves but it also gets at this heart of this broader question about what it actually means to be intelligent sort of alluding back to how alexander opened the course and started this lecture series and i hope that as a result of of our course and as a result of your participation you've gained a bit more appreciation about what are the connections and distinctions between human learning human intelligence and some of these deep learning models that we've been exploring this week and so with that i'll close and conclude the lecture and finally make a final point about reminding you about our open office hour session which is going to occur from now till about 4 p.m we'll be there to answer questions about the labs about the lectures and importantly in 10 250 in person we will be distributing the class t-shirts so please come by we will be there to distribute the t-shirts right after this and with that um once again thank you all so much for your attention and your participation we really enjoyed this and doing this every year and i hope to see many of you very shortly in 10 to 50. thank you so much 

thank you very much alex so i'm very happy to be here today i and i hope we will be able to uh cover everything that we wanted there is about 100 pages here and for 45 minutes but uh we'll try okay so we're very in show in very in in very short uh innova is developing lidars for autonomous vehicles we're very active in in automotive trying to pursue autonomous driving level two level three we have several products that we offer to the market but actually here today and we're going to focus on two main topics one of them is about what our car makers are trying to achieve in developing autonomous vehicle and how we're helping them not only with the lidar but also with the perceptions of them that's something that amir will cover you might have heard on innovas through our program with bmw we have our lidar here innova is one that's our first generation that is going to be part of bmw series production for level three for highway driving it's going to be used on several models seven model series the five series the ix i'm very fortunate to be part of this program and of course many other things as i said uh today i'll cover i'll cover some topics that are coming from the lidar space uh and talk about possibly uh some standardization that is required in that space and later amir will translate some of those requirements also taking from the perceptions of them we have a white paper uh that we shared on our website some of the material that i'm going to cover here very quickly because we don't have time uh is is explained uh very in deep uh in in that document so you can find it on our website and i'm sure you'll find it interesting today most of the cars on all of the cars that are trying to do some automation of driving are at a level two meaning that the car controls either the pedals uh or the the wheel but still require the attention of the passenger uh you just probably heard about a cow er that a person that was accused in killing a person uh due to an accident in in you know automated driving basically because the car makers are still not taking liability the quantum leap between level two and level three comes from the car maker actually taking full responsibility on anything that happens when driving and does not require the person to be attentive it obviously requires them to have much higher confidence and that comes from additional sensors and capabilities in order to reach a full autonomous driving you need to have a good visibility a good prediction on what everything is going on on the road and there is an eye diagram a certain way that you need to cover the space with different types of sensors and lidar is is one of them and what you're seeing here is just one i would say one way to try to do it with existing solutions in the market uh someone who took specific sensors and tried to map how putting them one next to the other gives you the full visibility that is required by the system there are other ways uh to do it this is just one example i want to talk about explain first what is a lidar and maybe specifically how we approach this problem allied are using a laser a laser beam that we move around by using a two-axis manner uh scanning mechanism that allows us to scan the scene that light is emitted towards um an object and the light that bounces back to the system a fraction of the light is collected by the system and there is a sphere that comes from 200 meters away and you have a certain flux of photons that are collected by the system the aperture of the system is you can say it's like an antenna that's the antenna of the system it defines how many photons are eventually collected into the system those photons are collimated on a detector and of course the sensitivity of the system defines how well we are able to react to each photon you want to have the lowest uh noise figure in order to be able to detect each and every photon of course photons could come from either the object or photons that actually came from the sun the sun is like the nemesis of sliders and and that's our job to try to differentiate between them and there are ways to do it those photons that are converted to electrons through an avalanche reaction of the silicon are collected by the signal processing mechanism of course there is also self noise of the of the detector itself and it's also part of what we need to do is to uh see the difference between them eventually uh light that comes back from the system is detected by the system but by the unit and by measuring the time in which it took for the light to bounce back we know how far things are now eventually a lidar is like a communication system as you might know you can define it by a signal to noise ratio the signal to noise ratio for lidars is defined by the emission because that's the transmission you are using the aperture of the system which is the antenna the photon detection efficiency which defines the gain of the system and of course the noise the noise that either comes within from the cells from the system or the noise that comes from from the sun now we use this um equation to see how we can improve our system from one generation to the other between innovas one and innovis2 our second generation which we recently showed we improved this equation by a facto of more than 30 times okay this comes from being able to control different knobs of the system using 905 means that we are capped by the amount of laser that we are allowed to use but we there are many other measures that we do in order to improve the system significantly and today innoviz 2 is is a few orders of magnitude really better than any other lidar company any system that is available i'm showing you an exa a demo of innovis2 and this is actually also in a very early stage but already showing the very nice i would say point cloud just to get you understanding every point that you see here is a reflection of light coming back from the scene but after shooting a pulse of light you and we do that very very fast in order to cover the entire field of view and we can see very far away at very nice field of view and range and resolution and and that is how we solve the problem uh which is defined by requirements we get from automakers now autonomous driving is could be described by different applications shuttles trucks passenger vehicles those provide different requirements for example passenger vehicles on highway require a certain requirement for driving very fast but a shuttle the drives in the city with much more complex scenarios and and traffic light and complex intersections require a different set of requirements today i'm going to cover uh this area the highway driving the highway driving is what we see is the mvp of autonomous driving because it it actually simplifies and reduces the variance of different use cases that could happen on the road because highways are more alike and and it actually narrows the number of use cases that you need to catch it it shortens the validation process uh our lidar can support obviously all of those applications uh but we see that level two and level three uh the the opportunities that probably would go the fastest in the market now when a car maker is trying to develop a a level three it starts from trying to understand how fast it wants to drive because the faster the car wants to drive it it needs to be able to predict things further ahead it needs higher resolution it needs higher frame rate and and those are the interpretation of the different requirements from the features of the car into the light of space which i this is covered in the white paper and i'm inviting you to read it of course on top of it there is the design of the vehicle uh whether you want to mount it on the roof in the grill uh you know the the the window tilt they're they're the cleaning system there are many things that are from practical elements uh require some modification for the design of the lidar which we obviously need to take into account for those of you that are less familiar with gliders uh then you know obviously a lighter is needed to provide redundancy for cameras due to low light condition or or missed you can see here an example of a very simple example of just some water that is aggregated on the camera and of course every drop of water can create the same problem and and that's not it's not okay to be in this situation this is why you need to have redundancy another case is direct sun of course uh some might say that if you have sufficient processing power and collected millions of objects a camera might be enough but obviously if if you're unable to see because of limitation of the physical layer of the of the sensor it's not enough you need to have a secondary sensor that is not sensitive so um today we're talking about level three level three requirements is is defined uh by the ability to see the front of the vehicle mostly and a good system is not only there to provide safety uh to the to the person because if if all the car needs to do is to drive uh to make to bring to make you uh leave after the the car travel it can decide to break uh every five minutes and every for everything that might happen on the road it will slow down uh you will be exhausted and possibly want to throw up after such a a drive which means that the system in order to be good it has to be smooth in its driving and to have a smooth driving you need to be able to predict things uh very well and and in order to do a smooth uh acceleration uh brakes maneuvers and that really what defines uh the requirements of the sensor i will not go uh very deep in these diagrams these are part of the things that you can find on the white paper talking about the requirements of the field of view from cutting scenario analysis uh and you know whether what whether you place it on the grill or you place it on the roof uh how you manage a highway with different curvature a slope and then you have the vertical field of view that is very much affected by uh the height of the vehicle and uh and the need to support uh the ability to see the the ground very well and under drivable again i don't want to go very deep here but if you're interested to learn about those principles and how they are translated to the requirements of the liar again you can find this on on a white paper and actually there is also a lecture which i gave just a month ago about this white paper it's it's possibly another hour where you need to hear me but you know i don't do i don't want you to do that twice at least um before we go to the perception area i think this possibly something that i do want to dwell on eventually in order to have a good uh driving and smooth driving it's about predicting it's about being able to classify an object as a person knowing that a certain object is a person allows the cow to have better projection of its trajectory on the way it can actually move in space if it's a car there is a certain degree of freedom of how it can move and the same for a a bicycle and a person and its velocity the higher the resolution of the sensor is it allows you to have a better capability in doing that at a longer range because of course the longer the person is you get less pixels on it less points so the vertical resolution in this example in the horizontal resolution is key in order to allow the the sensor to have good capabilities in identifying objects this talks about uh the braking of the the frame rate also related to braking distance and i don't want to spend time here it's again another example of you know why a certain frame rate is better than the other or why this is enough i'll let you read it in the white paper uh this example is something that i do want to spend some time on sorry uh yeah here okay so this is a use case we hear a lot from car makers uh you are driving behind a vehicle and there are two vehicles next to you so you can't move in case you're seeing a problem and at some point this vehicle here identifies an object that he wants to avoid crashing into and and this use case it tries to um provide indication of how fast uh or how well the car would be able to do emergency braking assuming that you're driving at 80 miles an hour now imagine that you have an object that is very far away from you and you want to be able to make a decision that this object is non-overdriveable meaning that it's too high it's high enough to cause damage to the car and basically this is about 14 centimeter because of the suspension of the vehicle which cannot absorb a collision into an object that is too high so the vertical resolution is very important because it's not enough to make a decision on an object because of a single pixel if you're seeing a single pixel at far away you don't know if it's a reflection from the road a reflection from a bot dot cat eye or just anything else you need to have at least two pixels so you have a good clarity that you're looking at something that is uh tall and and therefore the vertical resolution is very important once you are able to collect enough frames to identify that at good capability there is the latency of the vehicle itself in terms of how slow it can eventually stop now this analysis here is trying to show you the sensitivity of of parameters of the lidar even if the lidar had twice the range capabilities the ability to see an object at twice the range would not help me because if i only get one pixel it will not help me to make a decision earlier if i have higher frame rate even once i'll see the object and start to aggregate frames to make a decision that this is something i worry about it will only save me about six meters of decision the time in which i collect start to seeing the object and collecting enough frames to make a decision it's a very short period which i will try to save if i will have double the vertical resolution i will be able to identify this obstacle 100 meters more away so just to show you the importance of the different parameters of the lidar is not very obvious but are critical to the safety of the vehicle i will let i mean take it from here and thank you thanks [Music] it took more time than i told you maybe while aamir is getting his screen set up i have a quick question omar um how do you in the company view um kind of like the evolution of solid-state lidars is this something that's also on the business radar and you want to also develop that kind of technology or or do you believe like the mechanical lidars or we have i mean our lighter is a solid state yeah it is a solid state but we are also working on a product i didn't talk about it we're also working on a product for a 360 but as such that is you know about 10 times the resolution of today available solutions i mean the best-in-class 360 solutions are 128 lines we are working on a lidar with 1280 lines at a at a fraction of the price uh we decided to step into that direction because there are still applications that leverage on a wider field of view for non-automotive or non-front-looking lighters and that's something that we announced just just a month ago and we will show later in the year very exciting thank you yeah okay so so thanks so much again so now now we'll speak about how we take this oem uh requirements and that specification and actually build a system to support this perception system so first before we dive in uh i would like to speak about um i think the most obvious but yet most important characteristics of uh of point cloud of lidar uh and that lidar is actually measured uh in 3d uh scene around it it means that that each points represents a physical object uh in in the scene uh so uh it's really easy to measure uh distance between two points uh it's easy to measure the height of object it's easier to fit planes to the ground for example um and and i want i want to take you um like through a really simple algorithm uh and and we'll see together how far uh can we go just with lighter and really simple algorithm and support uh much of the requirements on the on that i mentioned before uh so um the simple algorithm uh the essence of this simple algorithm is detecting or classifying the collision relevancy of each point in the point cloud so in this visualization you can see pink points and green points the peak points are a collision relevant points means you don't want to drop through them and the green points are actually derivable points in this case and the row the load so the most simple algorithm you can you can come up with it just take every uh each pair of points in the point line and if they are close enough so just matter and measure the height difference between these two points and if it's greater than 40 centimeters like like you can just classify these two points it's collision relevant uh so here this uh turnabout track uh is uh is easily detected as uh exclusion relevant uh and the car won't drive through this truck um so while we're talking about uh deep learning network uh it's really easy to it's really hard to generalize and deep learning networks to new examples new scenarios uh so you can have a really good deep learning network that detects cars trucks desmond whatever but then you get this santa on the road uh and it's not trivial uh to um to generalize and to understand this center uh is actually an object which you want to avoid not just just a background uh and and with point cloud like with this really really simple algorithm uh these tasks become uh really easy uh another example just is fire trucks uh maybe in ambulances and other um other cars which are not uh represented um sufficiently in the train set uh and you probably heard about accidents that might be to similar reasons uh and another but but related uh cases is completely different scenario i mean um most of the data tends to be uh from from places like uh north america or europe uh but then you can end up uh in india uh a city full of riches and you just want to make sure you never connect them um so so again with lidar and this really simple algorithm i described before this problem it still exists obviously uh but it's it's suppressed and it's under control once you can actually measure the scene so now let's look a little bit more complete example maybe some of you recognize this this video from uh from one of the tesla crashes so you can see the white tesla actually crash into this uh this turn over trucks uh so there are many reasons for for this crash some say it's lighting other maybe because uh because during training uh the the network never seen a turn of the truck so it might be a problem um but but as as alma says and as i i mentioned before with lidar uh this this whole accident would be will be avoided so you can see here um this how it would look and light up um so the truck is easily easily detected from really really far um and and the car would would never would ever actually crash this this vehicle um and and this this algorithm is the same algorithm uh as i described before it's a really simple algorithm that actually um [Music] makes all the safety criticals and others much more solid and under control so maybe some of you guys saying that uh detecting this huge truck is is easy so here's a different example uh from from our action i thought this is the end of this one look at the distance uh two i don't know if you guys can see uh a tile uh this the distance is a tile um so this same really simple algorithm just looking at two points one above the other can easily detect this tyre as uh as a collision relevant so what the car actually sees uh the car just take uh the collision relevant point and maybe project them on on xy plane like on the ground plane or something and use this information to uh to navigate through the many other obstacles uh in the city and this is that just to close up just you can see this really uh really a tire and a can actually a pallet next to the next little tire that cannot be seen from distance um but but as long as i mentioned before it's not enough i mean get a good understanding of the static object and seeing small obstacles big optical is really important and safety critical but it's not enough because eventually we want to understand where these objects are going maybe they're moving in our velocity so we don't need to do anything just be well or maybe they're going to enter our eagle lane so we need to break um so so still detecting an object as an object is is really important so let's take this example uh just splash detection this is actually a pedestrian captured by uh by the ladder so i and i think everybody everybody would agree this is this is an easy example right um it is expected from every uh average network to say uh this is actually a pedestrian and classified it with pedestrian but what about this example i mean here it's not it's not obvious anymore right uh i mean maybe you can see legs a little bit of head torso but it's it's not it's not obvious so but but still i think a good trained uh network or system can still say this is this one giving surrounding maybe more context um so so here again it's expected uh expected to to be detected and classified as pedestrian but what about these two points these two points really distant points um so now now vehicle is moving really fast and we want to be super aware of any anything even even if it's it's a high distance so what what what can we do what do we expect uh from uh from deep learning network or look at the appearance of the object um it's it's really i think everybody agrees how to say this is a pedestrian but with lidar luckily uh it's not it's not critical i mean we can still uh classify or cluster uh these two points as an object because we know they are close by and if we classified it as an object and now we have a bounding box we can keep track uh and and estimates all the other how to use like velocity shape uh position obviously all that needed uh for the car to to predict uh it's it's uh it's motion and and act accordingly so taking this simple uh really simple uh clustering algorithm and putting on put it on real real scenario like uh normal how we drive uh so so it would look uh roughly like this uh you can see many many clusters of actual cars and objects um around uh with zero uh thin semantic uh but since we don't have the systematic you you would also see also see um like uh other objects which are not relevant uh necessarily for driving was not moving uh classified uh or not classified detected as objects but if our tracking system is good enough we would say okay this is an object i don't know what is it but it's stationary so just don't drive into it but don't worry it will never cross your lane um so you can you can go really far with these two simple algorithms just uh build quality relevant and clustering you can really go far uh with uh with perception task force and who's driving but now the question is it enough i mean is it enough to really create a system which is robust and useful for upper level stack so here's an example uh where this cluster mechanism is not perfect um what what you see here you see in blue is actually deeply on an airport it's the first time showing like deep learning results so this is uh the blue is deep learning that's what detects this this whole object as a trunk but unfortunately the cluster mechanism actually split it into two different objects um and and and reported if we use just the clustering mechanism we would report it as two different objects uh see you can imagine uh this ambiguity or instability uh of the cluster mechanism actually make it a little bit harder uh for for the upper layers of the stack to get a good understanding of what it's in uh and if you're not classified as a truck so the motion model is not it's not clear um and again the upper layer of the autonomous vehicle stuff a truck autonomous vehicle stack uh can't be sure which uh how how this uh uh object would behave uh so so semantic uh is is still important and still critical uh for for this full full system um so now let's see how how we can do uh deep learning uh on on point club so first thing we need to we need to decide is how should we represent the data so now point cloud that sounds just a set of points uh so the first thing we need to uh need to understand while processing point cloud is that it's unstructured it means if we took all the points of of on this car and and order it differently in the memory it will still be the same card still with the exact same scene uh so there are actually a deep learning architecture which take advantages take advantage of this like uh pointing at the multiple class but but for sure this is not standard and i'm going to make sure we understand this before from processing data another characteristic which is important and and we need to consider is the sparsity of point cloud if you're looking at point cloud at the cartesian coordinate system and this is the relationship from the top uh so you would see that most of the points are concentrated in the beginning of the scene uh because we sampled the world in spheric coordinate system so this this again [Music] challenged some of the architecture but actually some other architectures actually can leverage from this from these characteristics and and create much more efficient networks and this efficiency with computation gonna i'm gonna speak about it through this stock because uh on autonomous driving and in general processing on the edge computational efficiency efficiency is a key element and sometimes actually defines the solution so it's really important to make sure your algorithms are efficient um another presentation now which is structured okay uh like images is front view uh so you can see the camera image just normal cameras above and below the point cloud which are projected on the on the ladder itself so it looks like uh it looks like an image uh the only difference is that each point here has some has different attributes it has the reflectivity as you can see here but it also has um it also has the the xyz position relative to the sensor um so now now the data is structured and we can apply uh many legacy networks that yes you are available or and leverage from from a lot of legacy uh but but now it's a little bit harder to exploit the 3d measurement characteristic of point cloud for instance even though we know this car and this car roughly the same size and same shape roughly and while looking at it from front view it's a little bit hard to use this advantage now we get a lot of scale per distance and this is a kind of uh increasing data set that we need we need to uh to use in order to have a good generalization but this is useful representation another representation which is also common uh while processing uh polycloud is vocalization uh so uh if we take all the volume uh we wanna process and predict uh road users estimates roses location and we split it into multiple uh smaller volumes like you can see here this is the visualization i'll try to try to give here and in each voxel uh just uh put a representation of the points in it or surrounding uh then we can get again a structured representation of the point um and in each boxer we can we can start with really simple representation like occupancy whether it has points in it or not or we can go for much more complex representations like statistics even small networks that actually estimate the best representation of each boxes and there is a lot of research about it so here is the here is an example uh for for this uh vocalization uh representation so this is the the voxelized map uh looking from the top okay so uh it's uh it's an image uh and each uh each voxel uh is uh represented by by the reflectivity of the center of the central point uh and i put here that give you just some ankles so you can associate these two pictures uh so by the way it's really course representation mostly it's much better um so you can see how the network might see but but now we lost uh maybe key uh information that that we have if we look for the point cloud from the front view now it's a little bit harder to understand and what which object includes which object for instance this break uh in the guardrail uh it's a little bit harder to understand that this break is actually due to this car um and and not just due to a break in the garden uh so in order to do this again we need to build uh a network with greater receptive field and a little bit more deeper network to get a deeper understanding of the scene and sometimes i said before one avoid want to still be as efficient as possible but once again likely with point cloud is still easy uh to get all this uh occlusion information and so what you can see here this is the example of an illusion map so all the white is non-occluded point cloud uh so if you take this clustering mechanism and just uh just color all the free space uh you would get this occlusion map so you can add this occlusion map as an extra layer for the network so it has this information in events and you don't need to to create really big fat networks in order to understand this these conclusions so now after we know how to represent data with deep learning and we picked [Music] architecture and the question is what what are the key elements what i want to achieve with the food system so if we take the the cutting use case mentioned um we want to make sure this motorcycle is not in our lane we want to make sure we never crash into this motorcycle uh so first we need to detect it with the lidar and luckily our ladder is good enough for detection of this has a large enough field of view and then you want to put the boundary box around it right so you can tell to the car where it's located where it's going you can you can track it uh but now it's really critical that this bounding box is is really tight uh on this object right because if we just missed by a few centimeters in this case to the right uh we might think that this this motorcycle is actually not in our lane there's no problem because it can just keep going and it might cross cross the motorcycle [Music] uh so we want to get really good accuracy uh with with bounding box with output detection um so if we take this uh voxel presentation uh of the field of view now now we have a problem because on one hand you want to get really dense and really fine grid uh in order to uh to be much more accurate and and to reduce the ambiguity uh between the center of the cell and the actual object in it but as i said before it is computationally extensively expensive so uh we want to still find a way to work with reduced representations uh but let's share this this uh this accuracy um so a possible solution is uh fusion between the deep learning approach and the classical approach leverage uh the best from from each approach to to create a solid uh object list for the output of the stack uh so uh this deep learning uh stream gives you the uh the semantics it can say whether roughly where the object starts where it ends um is it is it a classified object is the car is the pedestrian motorcycle um and and the clustering stream actually gives you the accuracy uh that you need in order to drive and drive the car safely uh so so this is this is an example of how it looks so we again in blue this is the deep learning and and in white this is the clustering so you can see the deep learning no this is the car and actually put a bunk box around around the car but it's not accurate enough it's a few centimeters often and these few centimeters are important uh for for the safety critical objects objects which are close by um and the clustering actually really good uh it fits really good uh the of itself so once we did this uh we actually gained one more uh one more thing which is again again important in safety critical systems and this part the clustering the clustering path and the fusion is fully interpretable path and it's really helped to get to root cause of problems and look at the system as a white box so you can understand exactly what it does and and in some cases this is this is important it's really useful that you have a safety path which is fully interpretable so this is how it all kind of uh adds up uh so this is uh the deep learning output you can see the bounding box you can see them a little bit shaky a lot of the objects are fully fully detected all the time uh this is the clustering uh output uh so you can see it's solid but you have many false positives and the object length is not predicted and you don't know which object is obviously uh and this is the fused uh diffuse output so you can kind of get the best from from everything you you have a classes uh you have boundary books which are pretty solid uh and and it's really helpful uh for again for the upper layers of the stack so i know i know i moved fast because i didn't have much time uh but this is it thank you very much thank you so much i'm trying to answer some questions in the meantime and if you want i can answer some of them or i think that maybe one thing that it was it is important for me to to add because there was a question innovate is not developing an autonomous vehicle meaning we're not developing the driving decision we are developing the lidar and the perception software which allows car makers to have a more seamless integration assume that the processing unit of the car maker has a certain input from the camera an object detection classification interface and they are just getting another one from another sensor and you can imagine that they don't really care if it's a lighter or not all they care is that that secondary interface which tells them where things are uh is is in redundant to the other and gives them higher confidence in certain conditions so we're not developing so we are not doing driving decision but we are aiding uh our customers um do you want to ask specific questions you want me to go over questions that came up and you know maybe choose one of those oh uh yeah either one is it's perfectly fine or if anyone else has other questions feel free to just jump in and ask sure someone asked me about weather condition um although it's less related to perception maybe um anyway quickly on that rain is actually not very affecting lidars because drop of water is almost meaningless in terms of how much light is reflected back when you're you know meeting a drop of water in mid-air and even if very with very uh dense rain it's a it's only reducing possibly a few percentages of range fog is like an extrapolation of rain imagine a very large volume of particles that each one of them by itself reflects light back so it creates attenuation of the light going through it doesn't blind it completely but it does reduce it uh quite significantly depends on the density of course um there was a question here let me just check um so when we someone asked about uh false positive etc or actually there is another question i prefer someone asked me um what's uh how what what makes our lidar possibly a better fit to this application compared to others so beyond of course the obvious of cost and size which i think are important for automotive um if you would follow the white paper you would see that there are really um there is a trade-off between different parameters it's very important not to fall in love with only one because just again we talked about ranging as example just seeing like doing a lidar that says one kilometer with a single laser pointer is obviously you can say you have a lidar that says one kilometer and you can probably uh spark it and and raise a lot of money but eventually it will not help autonomous vehicles so there is there are many parameters and i think what what innovate is doing uh well is that we have a system that has a very nice working point and tradable meaning that we can actually we we can trade between palmettos but the working point the overall snl that we have in our system is significantly higher which allows us to meet all of the parameters that we show in that document including resolution frame rate it's not only resolution it's also frame rate it's also field of view and of course range so it's not and of course there's the pricing so that's uh i think the white paper explains it probably better than me um there is a question here on classified classifiers i mean maybe this is for you is it possible in theory to rig the loss function of the classifier to be more or maybe that was a joke actually sorry good [Music] so let's start with the training and i think uh i think we have two major concerns uh in training uh one which is related directly to the training is and and it's um sampling uh the most beneficial samples uh for annotation uh i think uh like especially in autonomous driving especially on highway uh most of the saints especially in north america and uh and in europe most of the series is just identical uh so you won't get much uh from just uh sampling uh random random friends for the training and we actually built a system of active learning uh and i've described it in previous talks so you can lock it up tonight so so this is really like a key element and it is like uh make sure that there is there is a question here make sure make sure we have another one i'm sorry sorry yeah i think there might be a little bit of flag but yeah yeah maybe we have time for one more question if there's one more there's someone to ask me a question here about the different types of sliders fmcw and time of flight and it's actually there are there are different camps in the light of space you have the the wavelength camp 905 1550 that's kind of a big kind of discussion and then you have uh the laser modulation whether it's time of flight and fmcw and i think other than that you have the scanning mechanism like whether it's mechanical solid-state or i don't know optical phased array so those are the primarily the the three main kind of branches in in the market uh starting with the question of fmcw and in time of flight so the only benefits uh proclaimed by the fmcw is the ability to do uh direct measurement of velocity meaning it's it's you modulate the laser in a certain way uh that allows you to measure uh both range and velocity by measuring doppler very similar on how you do it with radars the only uh thing that the disadvantage just comes with the need to use 1550 and again very expensive and there is a very strong coupling between the range frame rate and field of view so the trade the working point there is quite limited so fmcw systems can reach around uh 200k samples a second innova is one is about seven mega samples per second and in obvious two it's uh even significantly higher and it means that when you need to trade between resolution number of points frame frame rate and field of view fmcw mostly is using a very very narrow periscope kind of lidars because of that limitation and eventually measuring the the velocity of the vehicle in fmcw is is only a possible in the longitude vector because you're measuring velocity in the vector of the of the light direction you cannot measure velocity in the lateral which is as important so the need to calculate velocity is there anyway with time of light you can calculate velocity very nice if you have very high resolution and high frame rate it's not less well and eventually when it comes to the trade-off between parameters definitely resolution range field of view frame rate comes on top of the requirement for velocity and seeing probably tens of rfis and rfqs in the market i haven't seen yet anyone asking for velocity really so uh the the the value there is i think very limited and comes with very high cost um excellent yeah so thank you both so much and maybe one more quick question i know car makers are probably your primary customer but i was wondering do you also send sell your sensor to or others beyond the car makers for example academia and universities doing autonomous driving research you know someone yeah sure we do and we're happy to work with teams that are trying to innovate uh and of course we can talk about it after this session of course of course i mean uh we yes of course i mean we we work with construction companies smart cities surveillance i mean look today every in every corner of the world you have a 2d camera somewhere we live in a 3d volt okay anything you might ever want to automate you would prefer to use a 3d sensor it gives you much faster capability ability to exercise an application i'm sure lidars would be in the same position in several years from today excellent thank you so much and thank you both for for your presentation today 

um yeah well thanks i'll i i figured i would take a few minutes to just set the context of why rev cares about speech recognition um and give a little bit of history overview of rev and then i'll let jenny explain the cool stuff and the technical you know aspects of our work um so at rev our founding mission has always been to create work-at-home jobs for people powered by ai um and so what rev is is a double-sided marketplace that enables people to work from anywhere in the world and they the work they do is they transcribe they caption or they subtitle media i sometimes call it uber for transcription right and so imagine before rev um the the if you wanted anything to be transcribed you'd have to go to like a website like up upwork or fiverr or maybe even craigslist you know find a transcriber make sure they're good at what they do negotiate a price and so on and you know you send your audio and you kind of hope that that they do a good job there weren't many very good website catering the single user for transcription today when you know now that we've built rev you know you you can as a single user just come to rev drop a file you know in a gui put your credit card and a few minutes later you get a transcript um and so we really set out to turn this cumbersome you know process into some more of a magical experience you know and so everything is hidden for you and you just give your audio you get a transcript back and today rev has over 170 000 customers um and more importantly we create work at a at home jobs for over 60 000 people we call them reverse and the result of you know having this amazing marketplace is that we transcribe we transcribe over 15 000 hours of media a week which if you think about it is like producing training data for asr right so if you're vaguely familiar with asr one of the most classic data set is called libre speech and it's about a thousand hours and most you know people in the research community uses you know data sets that are around a thousand two thousand you know um there are a few bigger data sets nowadays that are coming out uh but 15 000 hours of data every week is is you know like a massive amount um so you know quickly why do we care about asr at rev it should be a bit obvious but you know as a marketplace that that does caption subtitles and transcription you know um powering our reverse with ai means using speech recognition right so um our biggest customer is our internal customer we run every file that we get at rev through a speech recognition engine to produce a first draft so that the transcribers can go ahead and kind of fix it and you know work with work with that draft um but we also offer our api externally on revi um as a way for people to build their own voice applications like if you know a company called loom or the script you know both great companies powered by revi um and as now jenny's showing this this slide you know and we've accumulated a lot of data over the years uh 200 minutes is is like three 200 million minutes is is over three million hours of transcribed data so we have this massive amount of data and the question really becomes how do we use that data to create you know world-class asr models and uh i'm happy to hand it over to jenny and to show you some technical uh aspects of rev i hope you enjoy it thanks miguel um very excited to be here today i actually did my phd at mit i graduated in 2020 and i've been at rev since then and really my main project since coming to rev has been developing our new end-to-end deep learning asr model that we're very excited to have in beta release right now um so today what i'm going to be walking through first i'm just going to talk a little bit about um the performance of this model and sort of why we're so excited about it but then i'll i'll go back to the beginning and sort of talk about the development of this kind of model and the modeling choices that go into end-to-end asr and especially the big learning experience it's been for me to go from an academic setting at mit where the models i trained were just for me to use and test and ultimately you know report their performance versus um developing a model that customers are going to actually use and sort of all the extra things that go into that so this model that we're releasing now is we're calling it version two of asr at rev um so version one was what's called a hybrid architecture it had some deep neural network components in it but also a lot of other things going on based on you know probably 30 40 years of research into really specific aspects of the speech recognition problem and deep learning has really come along and blown a lot of that out of the water it's pretty incredible so we're really excited to um have this model now and and be able to share it so as i said the first thing i wanted to talk about um is um just sort of the the performance of this new model that we're releasing um these are results on a data set that we've actually released this is just test data so it's basically a benchmark that we can use to compare asr systems against each other and this data all comes from earnings calls from public companies and we think it's more representative than what's out there sort of in the academic community it's more representative of the types of data that we actually see on rev and you know what we're interested in transcribing for our customers and the y-axis of this chart is word error rate and that's really the standard metric for asr the way we measure our performance so for this case we took all of these earnings calls and we actually submitted them to rev.com to get gold standard transcripts from human transcribers and we use those as our references and then we compare the asr output to those and word error rate as a metric you can probably guess since it has error in the name is measuring how many errors that we make and so lower is better in this case we want to make as few errors as possible in our asr system so what i'm showing here is a comparison of several of our competitors that have open apis that you can run audio through and get transcripts back from and then our hybrid model from rev which was version one of our asr and then end to end our new v2 asr system so the first thing i wanted to highlight on the left hand side is the overall word error rate of our models um so because of the amazing data resources that we have at rev and i'll talk a little bit more about how we use that data in a minute but because of the really incredible resources that we have we were already getting really great performance out of the hybrid architecture that we had previously but now with end to end we're able to get you know additional significant improvements on top of the really nice performance that we already had the other thing i wanted to highlight on this chart so the rest um other than the sort of overall category the the rest of the categories shown on this chart are different types of entities that occur in our transcripts and so we label those entities and then calculate word error rate just on those words to get a sense of how well we're performing on different types of words um i wanted to highlight in particular the org entity which stands for organization so this is essentially company names and the person entities so these are both things that come up frequently in earnings calls and i would say arguably are the most important words in the transcript getting these words right is really imperative for to make it so that someone who's reading the transcript can get you know the same information out of the transcript as someone who's listening um to the audio um so what you can see is that you know we are doing a really good job relative to our competitors on these words which is exciting for us um but you can also see that these types of words are definitely some of the hardest ones to get right and our error rate is still quite high overall so i just wanted to illustrate you know speech recognition it's definitely even in english it's definitely not a solved problem and we certainly have plenty of work we can keep doing to try to improve on this and the next set of results that i wanted to show um are on a test benchmark set that we did not create but that's an open source data set designed to help um research groups understand the bias in different asr systems so here we have our end to end engine compared against just a couple of our competitors and the data has been broken out into different nationalities according to the speakers and so this is something that's really important to us and something that we make sure to track and that we're constantly trying to improve on um so here we can see we're definitely doing a good job on what i would call the maybe most common accents that you might expect to see you know in a us-based company we certainly get a lot of data from the us but also you know canada england australia our large english-speaking companies and we do well sorry not companies countries um and we do well uh on data from those nationalities um one thing that's really interesting about end-to-end models is that it can be hard to really track down and understand why the model does well on certain particular things and not well on other things so one thing that i think is interesting in this data is that it turns out we do quite well on scottish accents and very poorly on irish accents so bias is definitely something that we are constantly working on and trying to improve on and that our research team is very interesting is very interested in um so i just wanted to highlight again you know we're very excited about the performance of our models but there's always always work to do in terms of trying to improve them so now that i've talked a little bit about model performance i want to go back and start from the beginning and talk about the development of an end-to-end model for speech recognition and a little bit about the the process we went through in terms of um you know making making modeling choices and trying to get the best results that we could so i hope you've learned over the course of this week in this class that data is really the foundation of any machine learning model um without without data there's not a huge amount you can do as miguel talked about earlier luckily for rev data is something we have in abundance we're very lucky on that front and for speech recognition in particular when you think about it in comparison to other say language tasks something like language modeling or machine translation there's the aspect of learning to model how text is generated which is a difficult problem in and of itself and speech recognition we have to our models have to be able to do that but given that we have audio input the models also have sort of an extra level of difficulty in terms of having to learn what information in the audio signal is important and what information is essentially irrelevant to our speech recognition task one thing that's very different about developing models at rev or i think really anywhere in industry versus in the academic setting is that academic literature and the benchmarks that people use well actually already mentioned libra speech those models tend to cover really relatively small domains where all of the audio seen both at training and test time was you know recorded in relatively narrow conditions for us our goal is really to produce a model that can handle anything our customers want to throw at it so any audio that gets submitted to rev.com we want to be able to do a good job transcribing it and that's a much larger and more difficult problem than you tend to see in academia so miguel showed the slide earlier with sort of the the tip of the iceberg of the data that we're actually using to train our models currently versus the data that we have access to um the most important data selection we do before we train our models is that we've decided as a team at least for the moment to only use what we call verbatim transcripts for model training um the verbatim product on rev.com asks transcriptionists to transcribe literally everything that's said in an utterance so this includes um things like filler words disfluencies any errors things like that everything that gets said gets transcribed most of the data that comes through rev.com gets what's called non-verbatim transcription um where transcriptionists um have the option and the ability to sort of make small corrections um with the goal of making the transcript as readable as possible but for us for asr training it's very important that we get um everything that's said um several of our customers miguel mentioned both loom and descript actually um have as part of their product that they flag disfluencies and filler words and in some cases like in descript i believe you can automatically remove those from the audio based on um our asr output that says where they are in the audio signal um so it's very important to us that we get those correct the next piece of preparing to train a speech recognition model is to actually break the input down into segments that we can train on um our audio data that we get is often long files so it could be 20 minutes could be several hours um so long files of audio and transcripts and it's not realistic to you know feed one whole 20 minute audio and the transcript for it into a deep learning model and try to you know back propagate through that entire transcript typically what you see again in academia is data sets broken up into single sentences both at training time and test time for us at test time we don't know anything about segmentation and we have to essentially segment the audio arbitrarily we do some what's called voice activity detection to try to segment the audio around pauses but um people don't always pause where you think they will um and so we've found that actually we can get the best results from our models if at training time we split up our training data into essentially arbitrary segments of different lengths um and also including multi-speaker segments so having um having some segments that go from one speaker into another speaker that actually gets us the best results in terms of speech input to um to these models speech or i should say audio is you know one-dimensional signal that's high frequency so typically we see 16 kilohertz audio which means 16 000 samples per second where each sample is just one number it is possible to feed that to a neural network and essentially learn features from it but we find that everything is sort of simpler and easier to handle if we first do some pretty simple signal processing to turn our audio into a spectrogram like you see in the bottom image so what we use as input is a series of vectors we have one vector typically every 10 milliseconds and the vector has the energy in the different frequency bins in our signal and it is possible also to think of that signal as more of an image like you see here um and i'll talk a little bit about how some of the sort of low level processing we do in the neural network has some commonalities with what you might have seen in networks for image recognition as well the next piece of the model i think you guys might have seen this before sort of in the language modeling context so i'll go over it quickly but a really important piece of the model is deciding how we're going to break up the text data in order to have the model generate text output so one option is to produce output word by word another option would be to break everything up into characters and produce characters one at a time um if you go the words option um you tend to have sparsity problem in that you have to in that your model can only generate words that it saw during training and realistically probably can only generate words it saw multiple times during training so a lot of you know rare rare words um will just sort of be inaccessible to your model if you go that way with characters we don't have a sparsity problem there's a very small set of characters and the model will definitely learn to generate all of them um but now you have often very long output sequences and it becomes much harder for the model to learn um sort of longer range dependencies between words um so like in the in the language model case you know often it's important um when you're generating a word to look look back you know maybe several words maybe back to the beginning of a sentence and in the character case that can be very far away in terms of the length of the sequence um so what the field seems to have kind of settled on is the use of what are called subword units or wordpiece units um so here we break words up um sometimes some of our units are whole words but the words can also be broken up into units that can be single characters or longer than single characters in speech recognition we generally use the same techniques for this that are used in language modeling or like machine translation so either what's called bite pair encoding or a unigram model for this um they're both a bit heuristic i would say but they tend to work really well in practice and this is sort of what the field has settled on uh jenny before we move uh ben had a question about uh some of the design the the decision process around using mel scale male frequency scale versus other other approaches sure yeah happy to jump into that a little bit um this is definitely something that is um sort of left over from older speech recognition research um the mel scale the the way these filter banks are um are set up is designed to sort of mimic actually the way the human ear processes audio so humans actually can do a much better job of distinguishing between lower frequencies than higher frequencies and so this is intended to reflect that as i said there are some neural network models that can take a raw audio signal and essentially learn these filters as part of end-to-end model training um my understanding is that for the most part they tend to learn filters that look kind of like these ones um and i haven't seen results where that really ends up adding any extra accuracy from just doing the signal processing up front um so for us we just find it much simpler to to do it in signal processing rather than include it in the network nice is that cool thanks uh actually the just forever the question was was uh around now scale versus bark scale and i i've actually never heard of anybody using bark scale and speed tracks i i don't know exactly uh why that would be but i'll look it up after this talk yeah go for it i'm i'm also not sure i've i've heard of it but i think mel scale is definitely standard for speech recognition um it seems to be what everyone uses all right cool um so moving on to the actual deep learning modeling choices um i wanted to start with the encoder decoder model with attention again because i believe or hope that you guys might have already seen this a little bit in a previous lecture and the reason why you might have seen it before is because this is a very standard generic model that really could be applied to almost any sequence to sequence problem um and see speech recognition is no exception this works very well for speech recognition so this is um you know at a really high level what our model looks like and just really the key like thing to remember about these models um is the auto-regressive decoder um so these models produce output one unit at a time um and each output as it's produced is fed back into the model to bias what the next output will be so another way to think about it if you've been exposed to like neural language models but not this kind of model before is that essentially the decoder is a neural language model but it's also conditioned on embeddings of the input audio so for speech recognition there are just a few um basic choices to make about these architectures as i said earlier you can sort of think about our speech features as an image and we actually do a first embedding layer in our encoder that looks a lot like sort of the the low low level layers of like a vgg network for image recognition if anyone has seen seen that but basically we just do some convolutional layers to pull out low level features from our speech speech features here in the embedding we find that it's also useful to just do some down sampling as i mentioned earlier typically we we use 10 millisecond frames as our speech features um so that's 100 frames per second um so the output sequence is much much much longer sorry the input sequence is much much much longer than the output sequence um so it just makes things simpler to sort of down sample everything up front so we typically down sample by a factor of four and then if you're using a transformer layer as i'll talk about in the next part um this is where we also add relative positional encodings um and this helps the um the transformers it gives it you know more information about where in the sequence each input feature originally came from um and then as i said there's a choice about what kind of actual layer to use recurrent neural networks were very standard until really very recently it's been a very quick change towards transformers but transformers are definitely very popular and they're very effective in speech as they are in other arenas like language modeling and you know this nice feature about transformers that they're actually quite efficient at training time um is definitely a plus you know if you're training really big models on a lot of data which is what we're doing and having more efficient training is definitely nice we actually use something that's called a conformer i think i've only seen it in the speech recognition context but i wouldn't be surprised if this is something people are using for other types of problems it's pretty simple it just has you know this extra convolutional layer um sort of stuck after the self-attention layer of um of the transformer and these have been shown to be sort of a bit more efficient in terms of being able to get slightly better performance out of the same number of parameters or similar performance out of fewer parameters compared to a pure transformer model so attention-based asr models um ha work really well um definitely it's been a little while now it feels like a long time in sort of the deep learning world but really not that long since um they sort of officially surpassed the older hybrid architecture in terms of performance and that was with recurrent neural network models so with transformer models now we see even better performance out of these models um like really some really impressive numbers that i think people thought we might never hit in terms of accuracy um but there's a big problem here um and one that i was actually not really aware of i wrote my entire phd thesis on attention-based asr models but now that i'm at rev i realized that these models are just not practical for commercial applications so it's very hard to have you know a big enough model that gets good performance that's also fast enough to use for inference for us we offer two different products with our asr models we have streaming which means like live captioning so it's very important that you know the transcription process be fast enough to keep up with audio uh but even for offline speech recognition where speed is slightly less important um you know there's still trade-offs to be made related to compute costs um and whether um you know it really makes sense to offer a product at a price that customers are willing to pay um so ultimately these models are great but we actually can't use them at rev in production so um what do we do instead it turns out that there's this older algorithm called connectionist temporal classification or ctc um that's actually very well suited to asr and performs the backbone i think of most of the industry speech recognition systems that you'll see and certainly is the backbone of ours so ctc is not a generic sequences sequence algorithm it only works for certain types of problems in particular it's really best when the alignment between your input and output is monotonics so for speech recognition we definitely meet this criteria but something like translation where depending on your languages the alignments could be kind of all over the place would not work well with ctc another sort of criteria is that the output sequence should be the same length or shorter than the input sequence again this is perfect for asr our input sequences are very long relative to our output sequences but again something like translation you don't know necessarily whether the input or output sequence will be longer for any given example and ctc is um it's not a model architecture exactly it's actually sort of a loss function and a decoding algorithm that sits on top of a deep learning model so the main properties of the models that we use for ctc um is that the first one is that they are not auto regressive so unlike um the model we saw previously where our outputs are produced one at a time and that really contributed to um sort of the the slowness in the inference process um here we don't have any of that feedback feedback and the outputs are generated sort of all at once out of the model um and some of the probability calculations i'm going to talk about on the next slide are um dependent on the assumption that the outputs are conditionally independent given the inputs the other thing you need from a model to be used for ctc is a softmax output layer um so this is the same thing you'd see with an encoder decoder model with attention you have the softmax layer and that gives you um each output is a probability distribution over your output vocabulary so like your list of characters or sub units for ctc we add one extra symbol to our output vocabulary which is a blank symbol and i'll talk on the next slide about how we're going to use that so with the encoder decoder model which again i'm hoping you saw a little bit in the past but essentially you can just calculate the probability of your output sequence um by uh summing up the log probabilities that you have in each of your softmax outputs for each time step so here we can do the same thing assuming that we have an output sequence that's the same length as our model outputs our issue is that typically what we want to calculate is the probability of a label sequence that is shorter in the asr case much shorter than the output sequence so the way we do this is we say the probability of the sequence z the shorter sequence is simply the sum of the probability of all of the longer sequences that reduce to z and what i mean by reduce um so y reduces to z if y and z are the same once you remove all of the blanks and repeats from y so i have a couple examples down here um if our desired output sequence say we're using characters is three characters long it's c-a-t uh but our model outputs um are four characters long um then here are a couple examples of four character output sequences that reduce to the three character output sequence c a t so for this simple example you could actually just write out all of the different y's that reduce to this labeling um and you could calculate their probabilities individually and just add them up but as these sequences get large that's um very inefficient um there's a lot of redundancy in the different sequences and just the the length of the list of probabilities that you need to add up gets very long um so what's nice about ctc is it comes with a really elegant dynamic programming algorithm um that lets us very efficiently calculate this probability of z given x and also is can be easily modified to be used in decoding with an algorithm that's called ctc prefix beam search i'm not going to get into the details on this but i actually think this original paper is really nice and i would definitely recommend anyone who's interested go back and read it so ctc in terms of performance it's definitely not as good as our encoder decoder models um but it can be better than the hybrid models and at least in the right conditions so we're moving in the right direction um in particular recent advances in terms of transformer or conformer models and large data makes ctc work pretty well and what's really nice is that the decoding or inference is very fast so it's even faster than the hybrid model and it's much faster than the attention-based model so now we've found something that we actually could put into production um i think i'm gonna stop for a second just ask what are we thinking about timing i know we started a tiny bit late um just wanted to check in yeah that's that's i don't know how much time you have sort of in your deck but we can take another five to ten minutes or so that's okay okay great i'll try to move a little bit quickly but i think that's very doable thanks awesome um so yeah i think i think ctc is just a really good reminder that um even though you know these deep learning models are incredibly powerful and the results are can be really amazing um it's not always the best idea to just choose the most powerful model possible and throw as much data at it as possible that we can still get you know real benefit from um you know our computer science fundamentals and thinking a little bit more deeply about the problem um i think that's always a good lesson um in this context so despite um the ctc model being um you know reasonable to use like i said it can be better than the hybrid models and it is faster we still want to see if we can get closer in terms of accuracy to the performance of the encoder decoder models while still keeping the efficiency reasonable um for um for inference that we could actually put it into production so there are a bunch of different ways we can do this the first one is to add an externally trained language model so ctc the way the beam search works it's actually very easy to take scores from a language model and add them in technically this is no longer an end to end model because now we have a language model that's trained separately but in practice it does work well so we can use any kind of language model whether it's a neural language model or an n-gram model sort of an older statistical model and that model can be trained either to predict um words or sub words if it's some word level it should be the same vocabulary as as the asr model itself and these combine well there is some cost obviously to getting the probabilities out of the language model um so there's some trade-off here between accuracy and um and speed or compute cost um you know if you throw you know a humongous transformer language model in there it definitely is going to slow things down so it's still um there's still some some work there to understand you know what the trade-offs are and what what makes the most sense if we want to stick with a pure end-to-end model where all of the parameters are trained together um it's actually possible to basically add a language model to ctc as part of a deep neural network model and this is called transducer i think i most commonly see it called rnnt even if there are no rnn's in it anymore um it was rnnt when it was originally introduced but definitely as i said transformers have kind of taken over um but anyway this model here the prediction network the sort of green box in the diagram on the bottom left is essentially a language model so it takes um it takes previous outputs from the model and updates um updates this hidden embedding based on those previous outputs but this can be trained with the ctc loss and can use the same ctc decoding algorithm and again assuming that the prediction network and joint network are relatively small and not too costly this can definitely still fit into sort of the compute budget for a production system last thing i want to talk about is what we have actually decided to do at rev and that is a joint ctc and attention model so this is the encoder decoder model with the tension that we saw previously um so we can take the encoder um just add a softmax layer and turn the encoder into a ctc model and we can train all of this jointly by just adding the ctc loss and the attention based loss together and what this gives us at the end is essentially two complete models that just share encoder parameters this was originally developed as a way to actually improve the accuracy of the encoder decoder model with attention so there's a way to do the attention decoding while also incorporating scores from the ctc module and that actually gives i think that's the best performing asl architecture overall that i've seen um but it's um you know similarly slow to the encoder decoder model with attention um so what we do instead is um we use a two pass decoding framework so first we pass the audio through the encoder and we get our ctc we use our ctcd coding to get an end best list of hypotheses and then we can use the attention decoder to do what's called rescoring and so here we're able to feed each hypothesis that came out of ctc decoding into the into the attention decoder and get the probability of that hypothesis according to the attention decoder and what's nice about this is that we can actually do some of the parallelization that we saw during training in transformer models and we can do this at inference time as well um so here we're able to get actually word aerates very close to what we achieve with attention decoding but it's much faster than attention decoding is um so i have a couple of slides about like research projects that we're working on but i think i will pause and we can take questions um take questions here yeah awesome thank you so much jenny and nico for such an illuminating talk i think really touched on a lot of concepts and also connected them back to some of the topics we introduced in the lectures which we really appreciate um maybe as people are thinking of questions i can start with one the results you showed with respect to bias in the data sets and potential effects on model performance were very or very intriguing to me i'm curious if you can speak a little bit more to strategies that you're exploring for trying to handle and mitigate these biases in your asr pipeline yeah absolutely um so one thing that we already do to a certain extent but we're looking into doing more of is actually making more use of our rever human transcribers to have them help us label data um with accents we have some labeling but you know the more data we can collect the better the ultimate goal of that is really to do a better job of balancing the training data we think that's probably the sort of easiest approach in terms of you know getting the models to do a better or less biased job across different types of data um we are also um we have looked at least in the past with our hybrid model and i think it's something we would consider as well for this end-to-end model is um sort of post-processing steps that can potentially um account for some of these issues and try to do some um some error correction after the fact um yeah those are i think those are our two main strategies at the moment yeah um if i i can add that if you remember that iceberg you know slide that i think i think the answers are hidden in all the data you know and a big part of it is like us figuring out a way to uh to mine that data and and you know rebalance things uh as jenny said and there's a few techniques i think to learn like curriculum learning and things like that that that maybe we could explore one one of our research papers is around curriculum learning very interesting yeah i'd love to follow up on on that topic later on thank you sure yeah um i see one question in the chat about the first pass that we use in the two pass model um yeah so for for right now actually um we are um looking into the transducer models and it's simply definitely something we're interested in those seem to perform well um but we're currently using um ctc with an engram language model added after the fact as our first pass and that is a conformer the encoder that we use that we use for both ctc and the embeddings that get fed to the attention decoder as a conformer model thank you i just want to say quickly i did put our email addresses here um definitely feel free to reach out if anyone has other questions or wants to talk about rev or just asr in general i'm always happy to chat with people thanks 

thank you alex and uh yeah great to be part of this and i hope you had an exciting course uh until now and got a good foundation right on different aspects of deep learning as well as the applications um so as you can see i kept a general title because you know the aspect of question is you know is deep learning mostly about data is it about compute or is it about also the algorithms right so and how and ultimately how do these three come together and so what i'll show you is what is the role of some principal design of ai algorithms and when i say challenging domains i'll be focusing on ai for science which as alex just said at caltech kind of we have the ai for science initiative to enable collaborations across the campus and have domain experts work closely with the ai experts and to do that right how do we build that common language and foundation right and and why isn't it just an application a straightforward application of the current ai algorithms what is the need to develop new ones right and how much of the domain specificity should we have versus having uh domain independent frameworks and all this of course right the answer is it depends but the main aspect that makes it challenging that i'll keep emphasizing i think throughout this talk is the need for extrapolation or zero shot generalization so you need to be able to make predictions on samples that look very different from your training data right on many times you may not even have the supervision for instance if you are asking about the activity of the earth deep underground you haven't observed this so having the ability to do unsupervised learning is important having the ability to extrapolate and go beyond the training domain is important and so that means it cannot be purely data driven right you have to take into account the domain priors the domain constraints and laws the physical laws and the question is how do we then bring that together in an algorithm design and so you'll see some of that in this talk here yeah and uh the question is this is all great as an intellectual pursuit but is there a need right and the to me the need is huge because if you look at uh scientific computing and so many applications in the sciences right the requirement for computing is growing exponentially you know now with the pandemic you know the need to understand uh right the ability to develop new drugs vaccines and the evolution of new viruses is so important and this is a highly multi-scale problem right we can go all the way to the quantum level and ask you know how precisely can i do the quantum calculations but this would not be possible at the scale of millions or billions of atoms right so you cannot like do these fine scale calculations right especially if you're doing them through numerical methods and you cannot then scale up to uh millions or billions of atoms which is necessary to model the real world and similarly like if you want to tackle climate change and you know precisely predict climate change for the next uh century uh we need to also be able to do that at fine scale right so saying that the planet is going to warm up by one and a half or two degrees centigrade is of course it's disturbing but what is even more disturbing is asking what would happen to specific regions in the world right we talk about the global south or you know the middle east like india like places that uh you know may be severely affected by climate change and so you could go even further to a very fine spatial resolution and you want to ask what is the climate risk here and then how do we mitigate that and so this starts to then require lots of computing uh capabilities uh so you know there's a if you like look at the current numerical methods and you ask i want to do this let's say at one kilometer scale right so i want the resolution to be at the one kilometer level uh and then i wanna look at the predictions for the night just the next decade that alone would take ten to the love and more computing than what we have today and similarly you know we talked about understanding molecular properties if we try to compute this schrodinger's equation which is right the fundamental equation uh so we know right that characterizes everything about the molecule but even to do that for a 100 atom molecule it would take more than the age of the universe right the time it's needed in the current supercomputers so so that's the aspect that no matter how much computing we have we will be needing more and so like the hypothesis right that nvidia we're thinking about is yes gpu will give you right some amount of scaling and then you can build super computers and we can have the scale up and out but you need machine learning to have 1000 to further million x speed up right on top of that and then you could go all the way up to 10 to the 9 or further to you know close that gap so machine learning and ai becomes really critical to be able to speed up scientific simulations and also to be data driven right so we have you know lots of measurements of the planet uh in terms of the weather over the last few decades but then we have to extrapolate further but we do have the data and we are also you know collecting data through satellites right as we go along so how do we take the data along with the physical laws let's say fluid dynamics right of how clouds move how clouds form so you need to like take all that into account together and same with you know discovering new drugs we have data on the current drugs and we have a lot of the information available on those properties right so how do we use that data along with the um physical properties right whether it's at the level of like classical mechanics or quantum mechanics right so and how how how do we make the decision of at which level at which precision do we need to make ultimately discoveries right either discovering new drugs or coming up with the precise characterization of climate change and to do that with the right uncertainty quantification because we need to understand right like kind of it's not we're not going to be able to precisely predict what the climate is going to be over the next decade let alone the next century but can we predict also the error bars right so we need that precise error bars and all of this is a deep challenge for the current deep learning methods right because we know deep learning tends to result in models that are over confident when they're wrong and we've seen that you know the famous cases are like the gender shade studies where it was shown on darker colored skin and especially on women right those models are wrong but they're also very over confident when they're wrong right so so that you cannot just directly apply to the climate's case because you know trillions of dollars are on the line in terms of to design the policies based on those uncertainties that we get in the model and so we cannot abandon just the current uh numerical methods and say let's do purely deep learning um and in the case of uh of course right drug discovery right the aspect is the space is so vast we can't possibly search through the whole space so how do we then make right like the relevant uh you know design choices on where to explore and as well as there are so many other aspects right is this drug synthesizable you know is that going to be cheap to synthesize so there's so many other aspects beyond just the search space uh so yeah so i think i've convinced you enough that these are really hard problems to solve the question is where do we get started and how do we make headway in solving them and so what we'll see is uh in you know what i want to cover in this lecture is right if you think about predicting climate change and i emphasize the fine scale phenomena right so this is something that's well known in fluid dynamics that you can't just take measurements of the core scale and try to predict with that because the fine scale features are the ones that are driving uh the phenomena right so you will be wrong in your predictions if you do it only at the core scale and so then the question is how do we design machine learning methods that can capture this fine scale phenomena and that don't over fit to one resolution right because we have this underlying continuous space on which the fluid moves if we discretize and take only a few measurements and we fit that to a deep learning model it uh may be doing the you know it may be overfitting to the wrong just those discrete points are not the underlying continuous phenomena so we'll develop methods that can capture that um underlying phenomenon in a resolution invariant manner and the other aspect we'll see for molecular uh modeling uh we'll look at the symmetry aspect because you rotate the molecule in 3d right the result it should be equivariant so also how to capture that into our deep learning models uh we'll see that uh in the later part of the talk so that's hopefully an overview or in terms of the challenges and uh the um also some of the ways we can overcome that and so this is just saying that you know there is right lots of also data available so that's a good opportunity and we can now have large scale models and we've seen that in the language realm right like uh including now what's not shown here the nvidia 530 billion model uh with 530 billion parameters and with that we've seen language understanding have a huge quantum leap and so that also shows that if you try to capture complex phenomena like the uh earth's um weather or you know ultimately the climate or molecular modeling we would also need big models and we have now better data and more data available so all this will help us contribute to you know getting good impact in the end and the other aspect right is like also what we're seeing is bigger and bigger super computers and with ai and machine learning the benefit is we don't have to worry about high precision computing right so with traditional high performance computing you needed to do that in very high precision right 64 floating point uh computations whereas now with ai computing we could do it in 32 or even 16 or even eight right so we are kind of getting to lower and lower bits and also mixed precision so we have more flexibility on how we choose that precision and so that's another aspect that is deeply beneficial so okay so let me now get into some aspect of algorithmic design uh i mentioned uh briefly just a few minutes ago that if you look at standard neural networks right they are fixed to a given resolution so they expect image in a certain resolution right a certain size image and also the output whatever task you're doing is also a fixed size now if you're doing segmentation it would be the same size as the image so so why is this not enough because you know if you are looking to solve fluid flow for instance this is like kind of air foils right so you can uh you know with standard numerical methods what you do is you decide what the mesh should be right and depending on what task you want you may want a different mesh right and they want a different resolution and so we want methods that can remain invariant across these different resolutions and that's because what we have is an underlying continuous phenomenon right and we are discretizing and only sampling in some points but we don't want to over fit to only predicting on those points right we want to be predicting on other points other than what we've seen during training and also different um initial conditions boundary conditions right so that's what uh when we're solving partial differential equations we need all this flexibility so if you're saying we want to replace current partial differential equation solvers we cannot just straightforward use our standard neural networks and so that's what we want to then formulate what does it mean to be learning such a pde solver because if you're only solving one instance of a pte right as a standard solver what it does is it looks at what is the solution at different query points and numerical methods will do that by discretizing in space and time right at an appropriate resolution like it has to be fine enough resolution and then you numerically compute the solution on the other hand we want to learn uh solver for uh family of partial differential equations let's say like fluid flow right so i want to be learning to predict like uh say the velocity or the vorticity like all these properties as the fluid is flowing and to do that i need to be able to learn what happens under different initial conditions so different initial let's say velocities or different initial right and boundary conditions right like what is the boundary of this space so so i need to be given this right so if i tell you what the initial and boundary conditions are i need to be able to find what the solution is and so if we have multiple training points we can then train to solve for a new point right so if i now give you a new set of initial and boundary conditions i want to ask what the solution is and i could potentially be asking it at different query points right at different resolutions so that's the problem set up any questions here right so hope that's clear so the main difference from standard um supervised learning right that you're familiar with say images is here right it's not just fixed to one resolution right so you could have like different query points at different resolutions in different samples and different during training versus test samples and so now the question is how do we design a framework that does not fit to one resolution that can work across different resolutions and we can think of that by thinking of it as learning in infinite dimensions because if you learn from this function space of initial and boundary conditions to the solution function space then you can resolve at any resolution yeah so how do we go about building that in a principled way so to do that just look at a standard neural network right let's say um an mlp and so what that has is right a linear function which is matrix multiplication and then um right so on top of that some non-linearity so you're taking linear processing and on top of that adding a non-linear function and so with this you have good expressivity right because if you only did linear processing that would be limiting right that's not a very expressive model that can't fit to say complex data like images but if you now add non-linearity right you're getting this expressive network and same with convolutional neural networks right you have a linear function you combine it with non-linearity so this is the basic setup and we can ask can we mimic the same but now the difference is instead of assuming the input to be in fixed finite dimensions in general we can have like an input that's infinite dimensional right so that can be now uh a continuous set on which we define the initial or boundary conditions so how do we extend this to that scenario so in that case right we can still have the same notion that we will have a linear you know operator so here it's an operator because it's now in infinite dimensions potentially right so that's the only detail but we can ask it's still linear right and compose it with non-linearity so we can still keep the same principle but the question is what would be now some practical design of what these linear operators are and so for this well take inspiration from solving linear partial differential equations i don't know how many of you have taken a pde class right if not not to worry i'll give you some quick insights here so if you want to solve a linear partial differential equation uh you know the most popular example is heat diffusion so you have like a heat source and then you want to see how it diffuses in space right so that can be described as a linear partial differential equation system and the way you solve that is you know there is this is known as the greens function so this says how it's going to propagate in space right like so at each point uh what is this kernel function and then you integrate over it and that's how you get the temperature at any point so intuitively what this is doing is uh convolving with this green's function and doing that at every point to get the solution which is the temperature at every point so it's saying how the heat will diffuse right you can write it as the propagation of that heat as this integration or convolution operation and so this is linear and this is also now not dependent on the resolution because this is continuous so you can do that now at any point and query a new point and get the answer right so it's not fixed to finite dimensions and so this is conceptually right like a way to incorporate now a linear operator uh but then if you only did this right if you only did this operation that would only solve linear partial differential equations but on the other hand we are adding non-linearity and we are going to compose that over several layers so that now allows us to even solve non-linear pdes or any general system right so the idea is we'll be now learning how to do this integration right so these we will now uh learn over several layers and uh get uh a now what we call a neural operator that can learn in infinite dimensions so of course then the question is how do we come up with the practical architecture that would learn this right so that would learn to do this kind of global convolution and continuous convolution so here we'll do some signal processing 101. again uh you know if you haven't done the course or don't remember here's again a quick primer right so uh you know the idea is if you try to do convolution in the spatial domain it's much more efficient to do it by transforming it to the fourier domain right or the frequency space and so convolution in the right spatial domain you can change it to multiplication in the frequency domain so now you can multiply in the frequency domain and take the inverse fourier transform and then you solve this convolution operation and the other benefit of using fourier transform is it's global right so if you did a standard convolutional neural network the filters are small so the receptive field is small right even if you did over a few layers and so you're only capturing local phenomenon which is fine for natural images because you're only looking at edges that's all local right but uh for um especially fluid flow and all these partial differential equations there's lots of global correlations and doing that through the fourier transform can capture that so in the frequency domain you can capture all these global correlations effectively and so with this insight right sorry here that i wanna emphasize is we're gonna ultimately come up with an architecture that's you know very simple to implement because in each layer what we'll do is we'll take fourier transform so we'll transform our signal to the fourier space right or the frequency domain and learn weights on how to pick like a across different frequencies which one should i update which one should i down weight and so i'm going to learn these weights and also only limit to the low frequencies when i'm doing the inverse fourier transform so this is more of a regularization um of course if i did only one layer of this this highly limits expressivity right because i'm taking away all the high frequency content which is not good but i'm like now adding non-linearity and i'm doing several layers of it and so that's why it's okay and so this is now a hyper parameter of how much should i filter out right which is like a regularization and it makes it stable uh to trade so so that's an additional detail but at a high level what we are doing is we are processing now from we're doing the training by learning weights in the frequency domain right and then we're adding non-linear transforms in between that to give it expressivity so this is a very simple formulation but uh the previous slides with that what i try to also give you is an insight to why this is principled right and in fact we can theoretically show that this can universally approximate any operator right including solutions of non-linear pes and it can also like for specific families like fluid flows we can also argue that it can do that very efficiently with not a lot of parameters so which is like an approximation bound so so yeah so that's the idea that uh this is uh right all you know in many cases we'll also see that incorporates the inductive bias you know of the domain that expressing signals in the fourier domain or the frequency domain is much more efficient and even traditional numerical methods for fluid flows use spectral decomposition right so they do fourier transform and solve it in the fourier domain so we are mimicking some of that properties even when we are designing neural networks now um so that's been a nice benefit and the other thing is ultimately what i want to emphasize is now you can process this at any resolution right so if you now have input at a different resolution you can still take fourier transform and you can still go through this and the idea is um this would be a lot more generalizing across different resolutions compared to say convolutional filters which learn at only one resolution and don't easily generalize to another resolution any questions here i hope this concept was uh you know clear at least right you got some insights into why first of all fourier transform is a good idea it's a good inductive bias you can also process signals at different resolutions using that and you can also have this principle approach that you're solving convolution right a global convolution a continuous convolution in a layer and with nonlinear transforms together you have an expressive model we have a few questions coming into the chat are you able to see them or would you like me to read them uh would be helpful if you can yeah i can also see them okay now i can see them here okay great um yeah so how generalizable is the implementation of so you know so that really depends right you can do fourier transform on different domains you could also do non-linear fourier transform right so i and then the question is of course right if you want to keep it to just fft are there other transforms before that to like do that end to end and so these are all aspects we are now further right uh looking into uh for domains where it may not be uniform but the idea is if it's uniform right you can do fft and that's very fast and so the kernel r is um so so the kernel r is going to be yes it's going to like but it's not the resolution right so remember this r uh the weight matrix is in the frequency domain right so you can always transfer to the frequency space no matter what the spatial resolution is and so that's why this is okay i hope that answers your question great and the next question is essentially integrating over different resolutions and take the single integrated one to train your uh neural network model uh so you could right so depending on you know if your data is under different resolutions you can still feed all of them to this network and train one model um and and also the idea is a test time you now have different query points a different resolution you can still use the model and so that's the benefit because we don't want to be training different models for different resolutions because that's first of all clunky right it's expensive and the other is it may not you know if the model doesn't easily generalize from one resolution to another it may not be correctly learning the underlying phenomenon right because your goal is not just to fit to this training data or one resolution your goal is to be accurately let's say predicting fluid flow and so how do we ensure we're correctly doing that so better generalizability if you do this in a resolution in variant manner i hope those answer your questions great so i'll show you some quick results uh you know here right this is navier stokes to the two dimensions and so here we're training on only low resolution data and directly testing on high resolution right so this is zero shot so and you know you can visually see that right this is the predicted one and this is the ground truth that's able to do that but we can also see that in being able to capture the energy spectrum right so if you did the standard like unit right you know it starts you know i mean it's well known that the uh convolutional neural networks don't capture high frequencies that well right so that's first of all already a limitation even with the resolution with which it's trained but the thing is if you further try to extrapolate to higher resolution than the training data it you know deviates a lot more and our model is much closer and and right now we are also doing further uh versions of this to see how to capture this high frequency data well um so that's the idea that we can now you know think about handling different resolutions beyond the training data so let's see what the other um yeah so the phase information so remember we're keeping both the phase and amplitude right so the frequency domain we're doing it as complex numbers so we're not throwing away the phase information so we're not just keeping the amplitude it's amplitude and phase together that's a good point good good now yeah yeah so i know we intuitively think we're only processing in real numbers because that's what standard neural networks do uh and by the way if you're using these models uh just be careful by torch how to bug in complex numbers uh for gradient updates i think for adam algorithm and we had to redo that so they forgot a conjugate uh software uh so yeah so but yeah this is complex numbers great um so i know there are you know a lot of different uh examples of applications but uh i have towards the end but that you know i'm happy to share the slides and you can look so the remaining few minutes we have i want to add just another detail right in terms of how to develop the methods better which is to add also the physics laws right so here i'm given the partial differential equation it makes complete sense that i should also check how close is it to satisfying the equations so i can add like now this additional loss function that says am i satisfying the equation or not right and and one little detail here is if you want to do this at scale and if we want to like you know auto differentiation is expensive but we do it in the frequency domain and we do it also very fast so that's something we developed but the other i think useful detail here is right so you can train first on lots of different problem instances right so different initial boundary conditions you can train and you can learn a good model this is what i described before this is supervised learning but now you can ask i want to solve one specific instance of the problem now i tell you a test time this is the initial and boundary condition give me the solution now i can further fine tune right and get more accurate solution because i'm not just relying on the generalization of my pre-trained model i can further look at the equation loss and fine-tune on that and so by doing this we show that we can further you know we can get good errors and we can also ask you know what is the trade-off between having training data because training data requires right having a a numerical solver and getting enough right training points or just looking at equations right if i need to just impose the loss of the equations i don't need any solver any data from existing solver and so what we see is uh the balance is maybe right to get like uh really good error rates right to be able to quickly get to good solutions over a range of different conditions is something like maybe small amount of training data right so if you can query your existing solver get small amounts of training data but then be able to augment with um just you know this part is unsupervised right so you just add the equation laws over a lot more instances then you can get to good generalization capabilities and so this is like the balance between right being data informed or physics informed right so the hybrid where your you can do physics informed over a lot more samples because it's free right it's just data augmentation but you had a small amount of supervised learning right and that can pay off very well by you know having a good trade-off is the model assumed to be at a given time um so yes in this cases we looked at the overall error i think like l2 error both in space and time so it depends some of the examples of pdes we used were time independent others are time dependent right so it depends on the setup i think this one is time dependent so this is like the average error great so um i don't know how much longer uh i have um you know so alex are we stopping at 10 45 is that the yeah we can go until 10 45 and maybe wrap up some of the questions after that yeah so like quickly another i think conceptual aspect that's very useful in practice is solving inverse problems right so you know the way partial differential equation solvers are used in practice is you typically already have the solution you want to look at what the initial condition is for instance right like if you want to like you know ask what about the activity deep underground right so that's like the initial condition because that propagated and then you see on the surface what the activity is and same with the famous example of black hole imaging right so you don't know directly you don't observe what the black hole is and so all these indirect measurements means we are solving an inverse problem and so what we can do with this method is you know we could first like kind of do the way we did right now right we can saw learn a partial differential equation solver in the forward way right from the initial condition to the solution and then try to invert it and find the best uh fit or we can directly try to learn the inverse problem right so we can see like given solution learn to find the initial condition and do it over right lots of training data and so doing that is also fast and effective so you can avoid the loop of having to do mcmc which is expensive in practice and so you get both the speed up of you know replacing the partial differential equation solver as well as mcmc and you get good speed ups um so chaos is another aspect i won't get into it because here you know this is right more challenging because we are asking if it's a chaotic system can you like predict its ultimate right statistics and how do we do that effectively and we also have frameworks there i won't get into it and there's also a nice connection with transformers so it turns out that you can think of transformers as finite dimensional right systems and you can now look at continuous generalization where this attention mechanism becomes integration and so you can replace it with these right fourier neural operator kind of models in the spatial mixing and even potentially channel mixing frameworks and so that can lead to good efficiency like you know you have the same um performance as a full a self-potential model but you can be much faster because of the fourier transform operations yeah so we have many applications of these different frameworks right so just a few that i've listed here and but i'll stop here and take questions instead so lots of like i think application areas and that's what has been exciting collaborating across many different disciplines thank you so much anima are there any remaining questions from the the group and the students yeah we got a one question in the chat thanks again for the very nice presentation well the neural operators be used for various application domains in any form similar to print trade networks oh yeah yeah so pre-trained networks right so whether neural operators will be useful for many different so i guess like the question is similar to language models you have one big language model and then you apply it in different uh contexts right so there the aspect is there's one common language right so you wouldn't be able to do english and directly to spanish right so but you couldn't use that model to train again uh so it depends on the question is what is that nature of partial differential equations right for instance if i'm like you know having this model for fluid dynamics that's a starting point to do weather prediction or climate right so i can then use that and build other aspects because i need to also right like you know there's fluid dynamics in the cloud uh but there's also right like kind of precipitation there's other micro physics so so you could like either like kind of plug in models as modules in a bigger one right because uh you know there's parts of it that it knows how to solve well or you could like uh you know ask that uh or in a multi-scale way in fact like this example of stress prediction in materials right you can't just do all in one scale there's a core scale solver and a fine scale and so you can have solvers at different scales that you train maybe separately right as neural operators and then you can also jointly fine tune them together so in this case it's not straightforward as just language because you know yes we have the universal physical laws right but can we train a model to understand physics chemistry biology that seems uh too difficult maybe one day but the question is also what all kind of data do we feed in and what kind of constraints do we you know add right so i think i think one day it's probably gonna happen but it's gonna be very hard that's a good question yeah i have uh actually one follow-up question so i think the ability for extrapolation specifically is very fascinating and sorry alex i can't i think you're completely breaking up i can't oh sorry is this better no no let's see maybe you can type so the next question is from the physics perspective interpreters as learning the best renormalization scheme so indeed you know like uh even convolutional neural networks there's been like connections to renormalization right so um i mean here like you know the uh yeah so we haven't looked into it there's potentially an interpretation but the current interpretation have we have is uh much more straightforward in the sense we are saying each layer is like an integral operator right so which would be solving a linear partial differential equation and we can compose them together and then that way we can have a universal approximator but yeah that's to be seen it's a good point another question is whether we can augment uh to learn symbolic equations um yeah so this is right it's certainly possible but it's harder right to discover new physics or to discover some new equations new laws uh so this is always a question of like right we've seen all that here but what is the unseen uh but yeah it's definitely possible um so and alex i guess you're saying the ability for extrapolation is fascinating um so potentials for integration of uncertainty yes quantification and robustness i think these are really important uh you know on other threads we've been looking into uncertainty quantification on how to get conservative uncertainty for deep learning models right and so that's like the foundation is adversarial risk minimization or distributional robustness and we can scale them up uh so i think that's uh an important aspect uh in terms of robustness as well i think there are several you know uh other threats we are looking into like whether it is right designing say transformer models what is the role of self-attention to get good robustness or in terms of uh right the role of generative models to get robustness right so can you combine them to purify like kind of right the noise in certain way or denoise uh so we've seen all really good promising results there and i think there is ways to combine that here and we will need that in so many applications excellent yeah thank you maybe time for just one more question now um sorry i still couldn't hear you buddy i think you were saying thank you um yeah so the other question is the speed up versus of the um yeah so it is on the wall clock time with the traditional solvers and the speed increase with parallelism or um yeah so i mean right so we can always certainly further make this efficient right and now we are scaling this up one in fact uh more than thousand gpus uh in some of the applications and so there's also the aspect of the engineering side of it uh which is very important at nvidia like you know that's what we're looking into this combination of data and model parallelism how to do that at scale um so yeah so those aspects become very important as well when we're looking at scaling this up awesome thank you so much anina for an excellent talk and for fielding sorry i can't hear anyone for some reason it's [Music] can others in the zoom hear me yeah somehow yes i i don't know what maybe it's on my end but uh um it's fully muffled for me so uh but you know anyway i i think it has been a lot of fun so yeah and uh yeah i hope uh you got now a good foundation of deep learning and you can go ahead and do a lot of cool projects so yeah reach out to me if you have further questions or anything you want to discuss further thanks a lot thanks everyone 

uh thank you so much for having me i'm super excited to be here um thank you alexander so for the kind words and uh and ava um both of you for organizing so yeah i'm going to talk about practical uncertainty estimation and out of distribution robustness and deep learning this is actually a bit of an abridged and maybe slightly updated version of the nurbs tutorial like age in 2020 so if you want to see the the extended one uh check that out and a bunch of these slides are also by dustin and blaji uh wonderful colleagues of mine all right what do we mean by uncertainty um the basic idea is we want to return a distribution over predictions rather than just a single prediction in terms of classification that means we'd like to output a label along with along with its confidence so how sure are we about this label in terms of regression we want to output a mean but also its variance you know confidence in intervals or error bars around our predictions good uncertainty estimates are crucial because they quantify when we can trust the model's predictions so what do we mean by out of distribution robustness well in machine learning we usually assume actually all almost all the theory is is under the assumption that our training data and our test data are drawn iid independent and identically distributed from the same data set in practice in reality you know the data sets change and um and things change either temporally spatially or in other ways and in practice we often see data that's not when we're deploying models we see data that's not from the same distribution as the training set so the kinds of of data set shift you might imagine seeing are things like covariate shift so the inputs x may change in distribution but the label distribution changes open set recognition is a fun one it's actually really really terrible so you can have new classes appear at test time imagine for example you've got a cat and dog classifier that sees an airplane that's something that's that's really hard to deal with actually and then label shift so the distribution of labels may change while the input and output distributions are the same this is also prevalent in things like medicine so the distribution of the number of people who test positive for covet changes pretty drastically over time and your models will have to adapt to that kind of thing okay so here's an example of some data set shift um so the iid test that this is actually from imagenet which is a popular image data set we have clean images you could imagine the shift being just adding noise to the images with with additional severity um so here we've got our frog and we're adding just increasing amounts of noise there's actually a paper by hendrix and dietrich where they took various kinds of shifts like noise motion blur zoom blur snow actually just drove through the snow in an in an autonomous car and you know it didn't deal very well with that data set shift so um here's a bunch of data shifts at various different severities and um and we showed this to a a common architecture a resnet and looked at how the accuracy behaved with respect to these data set shifts and so you can see accuracy goes down as these various shifts are happening and that's that corresponds to the intensity of the shift that's maybe not surprising but what you would really like is for your model to say okay my accuracy is going down but my uncertainty goes up corresponding with that right i don't know what the right answer is but i'm telling you i don't know by saying like you know i in a binary classifier i have 0.5 confidence on a label for example okay so in our experiments was that true in kind of these classic deep learning architectures no definitely not so as accuracy got worse the ece is a measure of calibration error i'll tell you about that in a second but our measure of the quality of the uncertainty got worse as well and the model started to make over-confident mistakes when exposed to this changing distribution of data so that's pretty alarming it's pretty scary for any any application where you might deploy a machine learning model another kind of terrifying thing is here are a bunch of examples of of uh of data examples where the model actually said that it had a class with over 99.5 percent confidence so here's a here's like random noise shown to the model and said i am almost completely sure that's a robin or a cheetah or a panda which is also pretty pretty freaky um you think it would say i don't know what this what this noisy thing is but it doesn't one reason why you might imagine that happening is is just the way our models are constructed right so in an ideal sense imagine it being a two-dimensional input space um and you have three classes so you have this blue orange and red ideally right when you when you get further away from your class your model starts to become uncertain about what the right class is and so the red class here we call out of distribution and we'd like to say okay well the red is far away from orange and blue so we're uncertain about what the what the actual label is in reality you know these classifiers are are decision boundaries and the further you get from the boundary so this is the boundary between the two classes the further you get from the boundary the more confident you become that as one class or the other so an interesting pathology of a lot of these models is you know if you show a a bird class or sorry a cat a dog classifier or a bird it won't say oh i don't i don't know what this is it'll say oh this is more bird-like than or sorry more dog-like than cat-like so i'm 100 sure that this is a cat which is not not what you want all right so applications a very very important one that's becoming more and more important or relevant these days is healthcare so medical imaging radiology this is diabetic retinopathy and so of course you would think that you would like it to be able to pass along with a classification you know diseased or not diseased tumor or not tumor if you pass that down to a doctor you want to pass down a confidence measure as well so 80 sure it's a tumor 70 sure it's a tumor right rather than yes or no and have the doctor be able to reason about that probability and include it in in a downstream or maybe expectation calculation so like what is the expectation that the patient will survive or something so here right we'd really like to be able to pass good uncertainty downstream to a doctor or say we're not actually sure we'd like an expert to label it instead and pass it to an expert labeler or you know an actual doctor self-driving cars which you just heard a lot about like i said i i actually was in a in a self-driving car an hour ago driving through vermont to get here in snow and so that's definitely a a quite an out-of-distribution situation and it certainly was fooled a couple of times but here right you would imagine that the car can um encounter any number of out-of-distribution examples and you would like it to express uncertainty such that the system making decisions is able to incorporate that uncertainty one that we care about a lot at google is conversational dialogue systems right so if you have your google assistant or your amazon alexa or your siri or whatever then it should be able to express uncertainty about what you said or what you asked it to do and you could possibly then uh defer to a human or it could ask you to clarify right it could say oh please repeat that i didn't quite understand instead of like you know add something to your shopping cart when you really just wanted to know what the weather was all right so there's a lot of applications of uncertainty and added distribution robustness basically any place we deploy a machine learning model in the real world we want good uncertainty this is just a subset of tasks that i care about a lot and my team at google is working on um one of our our taglines is so a popular expression about machine learning is all models are wrong but some models are useful and our tagline we've changed that a little bit to say all models are wrong but models that know when they're wrong are useful all right so give you a little primer on on uncertainty and robustness so there's multiple sources of sources of uncertainty one is model uncertainty and the way to think about that is there's many models that fit the training data well so if you look at these this two-class situation you know there's actually infinitely many lines that you could draw that perfectly separate these two classes and so you would like to be able to express your uncertainty about which line is the right line right rather than make an arbitrary decision that theta one or theta two or theta three is you know the true model um this kind of model so a model about what is the right model is known as epistemic uncertainty and you can actually reduce it the way you reduce it is just by gathering more data right so if we filled in more new triangles and more red uh circles then maybe we could eliminate theta 3 and zeta 1 because you know they no longer separate the data well one thing to note is you know it doesn't necessarily have to be models in the same hypothesis class so the first one was just straight lines linear models but you could imagine non-linear models of various flavors also be incorporated all kinds of yeah all kinds of models and that significantly increases the scope as well the number of of plausible models that could describe the data okay then the second big source of uncertainty is known as data uncertainty and that's basically uncertainty that's inherent to the data it could be something like label noise just uncertainty in the labeling process you know humans i think these are cfar images maybe and they're really low resolution and humans may not even know you know what that thing is and two human writers may give too a few different labels it could be sensor noise you know and in a thermometer there's some inherent uncertainty about you know the decimal place that you're uh that you can estimate the rope the weather too and so this is irreducible uncertainty often is called alliatoric uncertainty um the distinction between epistemic and aliatoric actually you know experts constantly mistake the two and between us we've kind of stated that we need to change the language because those words are too too hard to memorize we can think of it as model and data uncertainty all right so how do we measure the quality of our uncertainty um this is something that we've been thinking about quite a bit as well one thing that's popular to think about is a notion called calibration error and that's basically the difference between the confidence of the models they say your model said i'm 90 sure this is a tumor or i'm 90 sure that was a stop sign uh minus the aggregate accuracy so if it said this is a stop sign a million times how many times was it actually right um or sorry if it said this is a stop sign with 90 certainty a million times how many times was it actually right and the calibration error is basically the difference between that confidence and the aggregate accuracy um so you know in in the limit how often do does or how does my confidence actually correspond to the actual accuracy okay so oh i kind of explained this already but another great example is with weather so um if you predict the rain with 80 accuracy your calibration error would be you know over many many days how uh how often was what was the difference between the confidence and the actual predicted accuracy and for regression you might imagine calibration corners corresponding to this notion of notion of coverage which basically how often do predictions fall within the confidence intervals of your of your predictions okay so a popular measure for this is something known as expected calibration error this equation is just showing you what i would have just said so you actually been your confidences and bins of maybe it's zero to ten percent ten to twenty twenty to thirty and then for each bin you uh you estimate for the prediction that landed in that bin you know what was the difference between the the accuracy of that prediction and the actual confidence or that the confidence in that bin okay and that gives us that can give us an idea of the you know the level of overconfidence or under confidence in um in our models here's an example from a from a paper by guodal where they showed that very often deep learning models could be very under confident in that you know the blue what they actually outputted was pretty far from the actual um you know confidence or the actual accuracy that we'd like for each bin okay one downside of calibration is that it actually has no notion of accuracy built in it's just saying like how often was my confidence aligned with the actual accuracy and so you could have a perfectly calibrated model that just predicted randomly all the time because it's just saying i don't know and i didn't know and so we've been looking at uh or actually the statistical meteorology community many years ago was looking at ways to actually score the the level of the quality of the uncertainty of weather forecasts and came up with a notion of something called proper explorables that that incorporates this notion of calibration but also a notion of accuracy and this paper by knighting and raftery is a wonderful paper that outlines these rules and it gives you a whole class of loss functions if you will or scoring functions that um that are that uh don't violate a set of rules and um and accurately kind of give you an idea of how good your uncertainty was negative log likelihood is popular in machine learning that's a proper scoring rule briar score is just squared error and that's also a proper score rule also used quite a bit in machine learning okay i'm gonna skip over that for the sake of time okay so how do we get uncertainty out of our models um so we know it's important uh how do we actually extract a good notion of uncertainty so um i assume you are all familiar with the setting of i have enrolled that and i want to train it with std and i have a loss function well every single i would say almost every loss function corresponds to a maximum so minimizing a loss function actually corresponds to maximizing a probability or maximizing a log probability of the data given the model parameters so if you think this think of this p theta that's uh this is saying or this r max is saying i want to maximize the probability of my parameters theta given the data set that i have to find one setting the parameters that maximizes this probability that corresponds to minimizing um log likelihood minus a prior and actually if you take uh if you think of that as a loss right then that's a log loss p my plus a regularization term this in this case it's squared error um which actually corresponds to a gaussian prior but i won't get into that okay so actually this this log prob uh corresponds to data uncertainty which is interesting so you can actually build into your model a notion of data uncertainty in the likelihood function um let's see oh yeah okay and a special case of this right is just soft max cross entropy with lt regularization which you optimize with sgd which is the standard way to treat train deep neural nets sorry there's a lag on my slide so i've skipped over a couple all right see if this gets okay all right we'll just go here okay so the the problem with this right is that we've we found just one set of parameters um this gives us just one prediction for example and it doesn't give us model uncertainty right we just have one model and we plug in an x and it gives us a y so how do we get uncertainty in the probabilistic approach which is my definitely my favorite way of thinking about things you would instead of getting the single arg max parameters you want a full distribution for the p theta given x y you want a whole distribution over parameters um rather than the single one a really popular thing to do actually is instead of getting this full distribution is just get multiple good ones and that corresponds to there's a number of strategies for doing this the most popular probably is something called ensembling which is just get a bunch of good ones and aggregate your predictions over this set of good models okay let's hope it goes forward all right the the recipe at least in the in the bayesian sense right is we have a model it's a joint distribution of outputs and parameters given given some set of inputs during training we want to compute the posterior which is the conditional distribution of the parameters given observations right so instead of finding the single setting of theta bayes rule gives us the equation that we need to get the entire distribution over thetas and that's actually the numerator is what we were doing before and to get the entire distribution you need to compute the denominator below which is actually a pretty pretty messy and scary integral um high dimensional because it's over all of our parameters which for deep nets can be millions or even billions then at prediction time we'd like to compute the likelihood given the parameters um where each parameter configuration is weighted by this posterior right so we compute an integral predicting condition on a set of parameters you know times the probability of those parameters under this posterior um and aggregate all those to get our our predictions in practice what is often done is you get a bunch of samples s and you say okay over a set of discrete samples i'm going to aggregate the predictions over this set which might look a lot like ensembling which i just talked about okay so what does this give us um instead of having just a single model now we have a whole distribution of models right and you're what you're looking at is actually such a distribution where you can see all the lines each line is a different model they all fit the data quite well but they do different things as they get away from the data so as you move away from the data they have different hypotheses about how the the behavior of the of the data will be as you move away and the that uh that difference gives you an interesting uncertainty as you move away from the data right so you might imagine just computing the variance for example and out near the tails yeah for a prediction all right i'm going to speed through this but there's a vast literature on different ways of approximating that integral over all parameters it's in general way too expensive to do you know and certainly in closed form or um or even exactly for deep nets so there's tons of approximations they correspond to things like if you imagine these lines being the lost surface of the of the network they correspond to things like putting a quadratic on top of the loss it's known as a laplace approximation or things like sampling to markov chain monte carlo is used quite a bit and that's just as you optimize usually you draw samples you grab a good model then you optimize a bit further grab a good model and so on and so forth one thing that i'm really interested in is this notion of um so a a parametrization of a deep neural net defines a function from x to y if you're if you have a classifier and or a regressor and so then a bayesian neural net gives you a distribution over these functions from x to y and reasoning about this distribution is something i find super interesting and there's a really neat property that under under a couple of assumptions you can show that if you take the limit of infinite hidden units it corresponds to a model that we know as a gaussian process i won't get into that but it gives you that integral in closed form and then we can use that closed form integral to make predictions or look at pretty pictures of what the posterior actually looks like [Music] this is actually a line of research of mine and a couple of my colleagues of thinking about the behavior of deep neural networks under this this infinite limit or you know thinking about how things behave as you move away from the data using this gaussian process representation which is a really neat and clean way to think about things at least in theory because we have a closed form model for what the integral over parameters actually is um and it turns out that they're really well calibrated which is awesome they have good uncertainty i won't won't describe what they are exactly but i highly recommend that you check out gaussian processes if you find that interesting okay so then um if you think okay this bayesian methodology is is pretty ugly because i have this crazy high dimensional interval it's really hard and mathy to figure out then you might think about doing ensemble learning which is basically just take a bunch of independently trained models and form a mixture distribution which is basically just average the predictions of the ensemble to get you a i guess in the in the case of classification you really just aggregate or average the predictions and that gives you uncertainty over the class prediction in regression you can compute uncertainty as a variance over the predictions of the different models and there's many different ways of doing this ensembling it's almost as old as machine learning itself just like take the kitchen sink of all the things that you tried and aggregate the predictions of these things and that turns out to give you usually better predictions and better uncertainty than just a single model all right there's so if you find it interesting you can you can wade into this debate if you go on twitter there's a there's a debate between experts in machine learning about you know are ensembles beijing are they not basin i won't get into that i fall into the not beijing camp but um there's an interesting it's interesting to think about you know what is the difference between these these strategies um i won't i won't spend too much time on that i've also spent a lot of time thinking about issues with the with bayesian models so um there's we spent in my group a ton of time trying to get asian models to work well on modern sized deep neural nets and it's really hard to get it to work um because it requires very course approximations you know you need this very high dimensional integral um people play around with bayes rule to get it to work and so it at the end it's not clear if it's totally kosher from the i guess a a purist bayesian view um and it requires you to specify a model well i won't get into that too much but it basically means that you know you specify a class of models and the ideal model needs to be in that in that well specified class of models for bays to work well and it turns out that for deep nets we don't really understand them well enough to specify uh what this class of models might look like and the problem there often hinges on the um on the prior which is you know how do you specify a prior over deep neural nets we don't really know anyway there's there's a paper i'm i'm particularly proud of called how good is the base posterior deep neural nets where we try to figure out you know what is wrong with the deep bronze why can't we get them to work well on um on modern problems okay i see there there are some chat messages should i should i be reading these um this is all the we we are um handling the chat so and some of the questions are directed specifically towards you so we can feel them moderate them to you at the end of your talk if that's okay perfect perfect yeah go ahead um all right so some some really simple ways to improve the uncertainty of your model a popular one is just known as recalibration this is done all the time in in real machine learning systems which is you know train your model and then look at the calibration on some withheld data set and recalibrate on that data set which you can actually do is take just the last layer and do something called temperature scaling which is actually just optimized via cross entropy on this withheld dataset and that can increase your calibration on the distribution that corresponds to that withheld dataset of course that doesn't give you much in terms of model uncertainty and that doesn't help you when you see you know yet another different data distribution than the one you you just recalibrated on but it can be really effective um i don't know if you talked about dropout in the course you probably did something that's surprisingly effective is called monte carlo dropout yarn gal and zubin garamani where they just did drop out at test time so when you're making predictions drop out a bunch of hidden units and average over the dropouts when you're predicting and you can imagine that gives you an ensemble like behavior and you get a distribution of predictions at the end and that actually seems seems to work pretty well as a as a baseline and then deep ensembles so bellagi i won't say his last name um found that deep ensembles work incredibly well for uh deep learning and this is basically just retraining your deep learning model uh n times n is usually something like five to ten just with different random initializations they end up in different maxima or optima of the lost landscape and give interestingly diverse predictions and this actually gives you a really good uncertainty at least empirically it seems really good yeah so here's a figure from if you recall one of the first slides i showed you the was different shifts of the data that same study we found deep ensembles actually worked better than basically everything else we tried um i lost the back to bellagi actually because i i said bayesian or approximate evasion methods were going to work better and it turned out that they that they didn't something that works even better than than this deep ensemble strategy is what we call hyper grammar ensembles which is basically also change the hyper parameters of your model um and that gives you even more diversity in the predictions i guess you might imagine that corresponding to broadening your model hypothesis in terms of the types of models that might fit well and uh so you ensemble over those that does even better than just on something over the same hyper parameters and architecture then another thing that works really well swag which was biomatic set out is you just optimize via sgd basically and then you fit a gaussian around around the average weight iterates so as you're bouncing around an optimum in std you're basically drawing out a gaussian and you're going to say that gaussian now is the distribution over parameters that i'll use okay [Music] i'm waiting for the slide to change i may have clicked twice we'll see huh do i dare click again okay yeah it's definitely skipped over a couple of slides let's see if we can go back all right okay uh i think this is the right slide anyway um one thing that we've been thinking about a lot uh within my team at google is okay what about scale um all of a lot of our systems operate in the regime where we have giant models and they barely fit in the hardware that we use to serve them and you know and also we care a lot about latency so you know things like ensembling yeah they're more efficient than than carrying around the entire posterior or entire distributional parameters but you're still copying carrying around five to ten copies of your model and for most practical purposes when i've talked to teams they say oh we can't afford to carry around you know five to ten copies of our model and we can't afford to predict five to ten times uh for every data example because that takes too long in terms of latency so scale is definitely a a problem you know i imagine for self-driving that's also a thing right if you need if you need to predict in real time and you have a model in hardware on your on your car you probably can't afford to carry around a whole bunch of them and predict um over all of them so within our team we've we've been drawing out an uncertainty and robustness frontier which is basically thinking about okay you know how can we get the the best bang for our buck basically in terms of uncertainty uh while we increase the number of parameters and it turns out it's actually really interesting um you know you can do much more sophisticated things if you're willing to carry around many copies of your model for example and then you can't do quite as much with bigger models but you can do quite a bit but this has certainly driven a lot of our our more recent research so one way that we think about this is if you think about ensembles as a giant model for example so it's basically think about an ensemble as being a bunch of models where there are no connections basically between the different ensemble members and then a connection bringing them all together at the end then there's basically a bunch of paths of independence of networks so you might imagine that you could make things cheaper if you know you just shared parts of the network and had independent networks at other parts or um where you found some way to maybe factorize the difference between the the different models and so that's basically what we've been doing so um this method known as batch ensemble by ethan nguyen at all was exactly take factors and use those to modulate the the model so you have a single model and then you have a factor for every every layer and that modulates the layer and you have n factors where n corresponds to your ensemble size you could imagine that you know this could produce drop out if the factors were zeros and ones or it could produce um you know different weightings that modulate different hidden units as you move through and so we call this batch ensemble and they're actually rank one factor so they're really cheap to carry around compared to you know having multiple copies of the model um i won't maybe we won't talk about this much but you know you can batch things so that you can actually compute across all factors in just one single forward pass which is really nice and this turned out to work almost really close almost as well as the full ensemble which is great because it requires something like five percent the number of parameters of a single model and so 90 or something less than a whole ensemble then um a neat way to kind of turn that batch ensemble thing into an approximate bayesian method is this is another big slide so it's taking a little while to switch but here we go something we call rank 1 bayesian neural nets which was basically be bayesian about those factors and so we'd have a distribution over factors and we could sample factors as we're making predictions and then sampling them you can imagine that definitely corresponds could correspond to something like dropout if you have some kind of binary distribution over the factors um but it could also correspond to interesting other distributions that modulate the weights of the model and give you an interesting um aggregate prediction and uncertainty at the end this is one flavor of a number of of exciting recent papers so the cyclical mcmc one as well um so exciting paper is where you think about being bayesian over a subspace basically so you can imagine integrating over a subspace that defines the greater space of models and using that to get your uncertainty rather than being expressing uncertainty over all the parameters of your model then actually something that uh that an intern did um that works really really well so martin havassi who's at harvard now uh he actually said okay let's take a standard neural net and instead of plugging in one input we'll plug in three different inputs and three different outputs or k different inputs and k different outputs and force the model to predict for all of them and they can be different classes which means that the model can't really share structure predicting for two at the same time and that gives so that basically forces the model to learn independent subnetworks through the through the whole configuration of the model and find some interesting diversity at the the outputs and so that actually you know tended to work pretty well so at test time then you just replicate the same input k times and it gives you k different predictions and those are interestingly diverse because they go through different sub networks of this bigger network here's just a a figure showing the diversity of the predictions so this is a dimensionality reduction on the distribution of predictions basically and we found that the predictions of the of the different outputs are actually interestingly diverse and then here's a couple pictures so as we increase the number of inputs and keep the the structure of the actual model the same so the number of parameters the same what does that do for the uncertainty and the the accuracy of a model i find really surprising is sometimes accuracy goes up um but certainly the um so if you look at this the solid lines so interestingly you know accuracy goes up sometimes and log likelihood so notion of the quality of uncertainty certainly goes up it's it's surprising that you don't need more parameters in the model to do this but it tends to work okay so i think i have basically at the end um maybe i can i can share just kind of an anecdote about what we're thinking about you know more imminently now since i've got a couple minutes left in our in our team we've been thinking a lot about um you may have noticed a number of papers coming out out calling uh large-scale pre-trained models a paradigm shift for machine learning and so the large scale so pre-trained models are basically saying you know instead of taking just my training data distribution what if i can access some giant other distribution and that might be you know if it's a text model i just rather than taking my labeled machine translation model where i have only n examples i just mine the whole web and i like find some way to model this this data and then i take that model and i chop off the last layer and then i point the last layer at my machine translation or whatever prediction task and then retrain you know starting from where that other model left off and that pre-training strategy works incredibly well it turns out in terms of accuracy it also seems to work well in terms of uncertainty so you know one thing that i think is is really interesting to think about is okay if we care about outer distribution uh robustness you know either we can do a lot of math and a lot of fancy tricks ensembling etc or we can go and try to get the entire distribution and that's what in my view pre-training is kind of doing um but in any case so that's something that we're really involved in and interested in right now which is you know how what does that free training actually do and what does this mean for uncertainty and robustness um okay and then [Music] the or the takeaways of of this uh of the previous slide slides is basically you know uncertainty and robustness is incredibly important um it's something that you know is at the tip of uh top of mind for a lot of researchers in deep learning and um you know as we increase compute as i said you know there's interesting new uh new ways to look at the frontier um and and i think a lot of promise to get better uncertainty out of our models okay and with that i'll i'll close and say thanks so this is actually a subset of many collaborators on uh on a bunch of the papers that i talked about and uh and from where the these slides are from so thank you and uh happy to take any questions thank you so much jasper really really fantastic overview with beautiful visualizations and explanations super super clear yeah so there are several questions from the chat which i'm sort of gathering together now so one question from stan lee asks is it possible to mitigate issues of high confidence on out of distribution data problem by adding in new images that are he describes as like nonsense images into the training set with label of belonging to an unknown class that's really interesting yeah so there is a bunch of work on effectively doing that yeah so um there there is a line of literature which is basically saying like let's create a a bucket or to discard things that are outside our distribution we'll call that an unknown class and then we just need to feed in to our model you know things that may fall into that class sorry my dog just barged into my office um so uh yeah that's certainly something that that's done quite a bit um danager i think uh had a paper on this um it was i forget what it's called something about priors and noise contrasting contrastive priors that's right yeah yeah um you could imagine yeah plugging in noise as being this this bucket or even coming up with um you know with the examples that hopefully would be closer to the boundary um and that corresponds to there's a couple of papers on on doing kinds of data augmentation just like augmenting your data maybe from from one class interpolating with another class and trying to use that as like helping to define the boundary of what is one or what is another and then trying to push just outside your class to say that's not part and putting that in the bucket of unknown but yeah great question it's definitely something people are doing and thinking about awesome one more question from mark asks can can you speak about the use of uncertainty to close the reality gap in sim to real applications okay yeah i mean that's a great question i personally don't know that much about sim to reel you know i'm thinking um in the robotics context you have a simulation you can train your robot in simulation space and then then you would like to deploy it as a real robot um i imagine that uncertainty and robustness are incredibly important um but i don't know how they think about it in in those particular applications clearly you know i think if you deploy your robot in real you would like it to express um reasonable uncertainty about things that are out of distribution or that it hasn't seen before i'm curious i don't know alexander ava if you know an answer to that question i think alexander can speak to it yeah yeah actually i was i was going to even ask a kind of related question but yeah so i think all of the approaches kind of and all of this interest in the field where we're talking about estimating uncertainty either through sampling or other approaches is super interesting and yeah i definitely agree like everyone is going to accept that these deep learning models need some measure of weight or some way to express their confidence right i think like one interesting application i haven't seen a lot of maybe there's like a good opportunity um for the class and this is definitely an interest of mine is kind of how we can build kind of the downstream ai models to kind of uh be advanced by these measures of uncertainty so for example how can we build better predictors that can leverage this uncertainty to improve their their own learning for example so if we have a robot that learns some measure of uncertainty and simulation and as it is deployed into reality can it leverage that instead of just conveying it to a human i'm not sure like um yeah if anyone in your group is kind of focusing more on like the second half of the problem almost not just conveying the uncertainty but um using it in some ways as well yeah yeah absolutely that's definitely something that we're we're super interested in yeah which is definitely you know once you have better uncertainty yeah how do you use it exactly there's yeah i think there are there's really interesting questions of how do you communicate the uncertainty to for example a layman or a you know like a doctor who's an expert in something but not in you know deep learning um and yeah how do you actually make decisions based on the uncertainty that's definitely a direction that we're moving more towards we've been spending a lot of time within our group just looking at the quality of uncertainty of models and looking at notions of calibration and and um and these proper scoring rules but they're kind of like intermediate measures right there what you really care about is the downstream decision loss which might be like for a medical task is how many people did you end up saving or you know how many decisions in your in your self-driving car were for correct decisions sure so that's definitely something that we're looking at a lot more there there's a great literature on on decision making it's called statistical decision making i think berger is the author of a book called statistical statistical decision making which is great a great book about you know how to think about what are optimal decisions given an amount of uncertainty awesome thank you great i think there was one final question and i um it's more about a practical hands-on type of thing so uh let me just get yes suggestions and pointers on the actual implementation and deployment of bayesian neural networks in sort of more industrial or practical oriented machine learning workflows in terms of production libraries or frameworks to use and what people are thinking about in terms of in terms of that yeah yeah that's a really great question so you know something that i think the the certainly the bayesian community or the uncertainty in robustness community something that they haven't been as good at is is producing really easy to use accessible implementations of models and that's something that we've definitely been been thinking about we've we've open sourced uh a um a library that includes benchmarks and model implementations for tensorflow it's called uncertainty baselines and i was exactly trying to to address this question we're like okay you know everyone will be bayesian or get better uncertainty if you could just hand them a model that had better uncertainty and so this uncertainty baselines was an attempt at that and we're still building it out it's built on built on top of edward which is a probabilistic programming framework for tensorflow but there are a bunch of libraries that do probabilistic programming which is making bayesian inference efficient and easy but they're not made for deep learning and so you know i think something that we need to do as a community is kind of bring those two together which is like libraries for easy [Music] prediction under uncertainty or easily approximating integrals effectively and incorporating that within deep learning libraries um but yeah that's definitely something we're we're working on check out uncertainty baselines i think it has implementations of everything that i talked about in this talk awesome thank you so much um are there any other questions from anyone in the audience either through chat or that i may have missed or if you would like to ask them live okay well let's please thank jasper once again for a super uh fascinating and very clear talk and for taking the time to join us and and speak with us today thank you so much it was my pleasure thank you um thank you alexander and thank you ava for for organizing and thank you everyone for uh for your attention yeah 

Good afternoon everyone and Welcome to MIT 
6.S191 -- Introduction to Deep Learning. My   name is Alexander Amini and I'm so excited to 
be your instructor this year along with Ava   Soleimany in this new virtual format. 6.S191 is 
a two-week bootcamp on everything deep learning   and we'll cover a ton of material in 
only two weeks so I think it's really   important for us to dive right in with 
these lectures but before we do that I   do want to motivate exactly why I think 
this is such an awesome field to study   and when we taught this class last year I decided 
to try introducing the class very differently and   instead of me telling the class how great 
6.S191 is I wanted to let someone else do   that instead so actually I want to start this year 
by showing you how we introduced 6s191 last year. [Obama] Hi everybody and welcome to MIT 
6.S191 the official introductory course on   deep learning taught here at MIT. Deep learning 
is revolutionizing so many things from robotics   to medicine and everything in between you'll 
learn the fundamentals of this field and how you   can build some of these incredible algorithms. 
In fact, this entire speech and video   are not real and were created using deep learning 
and artificial intelligence and in this class   you'll learn how. It has been an honor to speak 
with you today and I hope you enjoy the course. So in case you couldn't tell that was 
actually not a real video or audio   and the audio you actually heard was purposely 
degraded a bit more to even make it more obvious   that this was not real and avoid some potential 
misuse even with the purposely degraded audio   that intro went somewhat viral last year 
after the course and we got some really   great and interesting feedback and to be 
honest after last year and when we did   when we did this i thought it was going to 
be really hard for us to top it this year   but actually i was wrong because the one thing 
i love about this field is that it's moving so   incredibly fast that even within the past year 
the state of the art has significantly advanced   and the video you saw that we used last year used 
deep learning but it was not a particularly easy   video to create it required a full video of obama 
speaking and it used this to intelligently stitch   together parts of the scene to make it look and 
appear like he was mouthing the words that i said   and to see the behind the scenes here now 
you can see the same video with my voice. [Alexander] Hi everybody and welcome to mit 6.S191   the official introductory course on 
deep learning taught here at MIT. Now it's actually possible to use just a single 
static image not the full video to achieve   the exact same thing and now you can actually 
see eight more examples of obama now just   created using just a single static image no more 
full dynamic videos but we can achieve the same   incredible realism and result using deep learning 
now of course there's nothing restricting us to   one person this method generalizes to different 
faces and there's nothing restricting us even   to humans anymore or individuals that 
the algorithm has ever seen before [Alexander] Hi everybody and welcome to mit   6.S191 the official introductory course 
on deep learning taught here at MIT. The ability to generate these types of 
dynamic moving videos from only a single   image is remarkable to me and it's a testament 
to the true power of deep learning in this class   you're going to actually not only learn about 
the technical basis of this technology but also   some of the very important and very important 
ethical and societal implications of this work   as well now I hope this was a really great way 
to get you excited about this course and 6.S191   and with that let's get started we can actually 
start by taking a step back and asking ourselves   what is deep learning defining deep learning 
in the context of intelligence intelligence is   actually the ability to process information such 
that it can be used to inform a future decision   now the field of artificial intelligence or AI 
is a science that actually focuses on building   algorithms to do exactly this to build algorithms 
to process information such that they can inform   future predictions now machine learning you 
can think of this as just a subset of AI   that actually focuses on teaching an algorithm to 
learn from experiences without being explicitly   programmed now deep learning takes this idea even 
further and it's a subset of machine learning that   focuses on using neural networks to automatically 
extract useful patterns in raw data and then using   these patterns or features to learn to perform 
that task and that's exactly what this class is   about this class is about teaching algorithms 
how to learn a task directly from raw data   and we want to provide you with a solid 
foundation both technically and practically   for you to understand under the hood how these 
algorithms are built and how they can learn so this course is split between technical 
lectures as well as project software labs we'll   cover the foundation starting today with neural 
networks which are really the building blocks   of everything that we'll see in this 
course and this year we also have two   brand new really exciting hot topic lectures 
focusing on uncertainty and probabilistic deep   learning as well as algorithmic bias and fairness 
finally we'll conclude with some really exciting   guest lectures and student project presentations 
as part of a final project competition that all of   you will be eligible to win some really exciting 
prizes now a bit of logistics before we dive into   the technical side of the lecture for those of you 
taking this course for credit you will have two   options to fulfill your credit requirement the 
first option will be to actually work in teams   of or teams of up to four or individually 
to develop a cool new deep learning idea   now doing so will make you eligible to win some of 
the prizes that you can see on the right hand side   and we realize that in the context of this 
class which is only two weeks that's an   extremely short amount of time to come up 
with an impressive project or research idea   so we're not going to be judging you on the 
novelty of that idea but rather we're not   going to be judging you on the results of that 
idea but rather the novelty of the idea your   thinking process and how you how impactful this 
idea can be but not on the results themselves   on the last day of class you will actually 
give a three-minute presentation to a group   of judges who will then award the winners and the 
prizes now again three minutes is extremely short   to actually present your ideas and present your 
project but i do believe that there's an art to   presenting and conveying your ideas concisely 
and clearly in such a short amount of time   so we will be holding you strictly to that 
to that strict deadline the second option   to fulfill your grade requirement is to write a 
one-page review on a deep learning paper here the   grade is based more on the clarity of the writing 
and the technical communication of the main ideas   this will be due on thursday the last thursday 
of the class and you can pick whatever deep   learning paper you would like if you would 
like some pointers we have provided some   guide papers that can help you get started if 
you would just like to use one of those for   your review in addition to the final project 
prizes we'll also be awarding this year three   lab prizes one associated to each of the software 
labs that students will complete again completion   of the software labs is not required for grade of 
this course but it will make you eligible for some   of these cool prices so please we encourage 
everyone to compete for these prizes and   get the opportunity to win them all please 
post the piazza if you have any questions   visit the course website for announcements 
and digital recordings of the lectures etc   and please email us if you have any 
questions also there are software labs   and office hours right after each of these 
technical lectures held in gather town so   please drop by in gather town to ask any questions 
about the software labs specifically on those or   more generally about past software labs or 
about the lecture that occurred that day now this team all this course has a incredible 
group of TA's and teaching assistants that   you can reach out to at any time 
in case you have any issues or   questions about the material that you're learning and finally we want to give a 
huge thanks to all of our sponsors   who without their help this class would 
not be possible this is the fourth year   that we're teaching this class and each year it 
just keeps getting bigger and bigger and bigger   and we really give a huge shout out to our 
sponsors for helping us make this happen each year   and especially this year in light of the virtual 
format so now let's start with the fun stuff   let's start by asking ourselves a question about 
why why do we all care about deep learning and   specifically why do we care right now understand 
that it's important to actually understand first   why is deep learning or how is deep learning 
different from traditional machine learning   now traditionally machine learning algorithms 
define a set of features in their data usually   these are features that are handcrafted or hand 
engineered and as a result they tend to be pretty   brittle in practice when they're deployed the 
key idea of deep learning is to learn these   features directly from data in a hierarchical 
manner that is can we learn if we want to learn   how to detect a face for example can we learn 
to first start by detecting edges in the image   composing these edges together to detect 
mid-level features such as a eye or a nose   or a mouth and then going deeper and composing 
these features into structural facial features   so that we can recognize this face this is this 
hierarchical way of thinking is really core   to deep learning as core to everything 
that we're going to learn in this class   actually the fundamental building blocks though 
of deep learning and neural networks have actually   existed for decades so one interesting thing to 
consider is why are we studying this now now is an   incredibly amazing time to study these algorithms 
and for one reason is because data has become   much more pervasive these models are extremely 
hungry for data and at the moment we're living   in an era where we have more data than ever 
before secondly these algorithms are massively   parallelizable so they can benefit tremendously 
from modern gpu hardware that simply did not exist   when these algorithms were developed and finally 
due to open source toolboxes like tensorflow   building and deploying these models 
has become extremely streamlined so let's start actually with 
the fundamental building block   of deep learning and of every neural network that 
is just a single neuron also known as a perceptron   so we're going to walk through exactly what is 
a perceptron how it's defined and we're going to   build our way up to deeper neural networks all the 
way from there so let's start we're really at the   basic building block the idea of a perceptron 
or a single neuron is actually very simple so   i think it's really important for all of you 
to understand this at its core let's start by   actually talking about the forward propagation 
of information through this single neuron we   can define a set of inputs x i through x m 
which you can see on the left hand side and   each of these inputs or each of these numbers 
are multiplied by their corresponding weight   and then added together we take this single number 
the result of that addition and pass it through   what's called a nonlinear activation function 
to produce our final output y we can actually   actually this is not entirely correct because one 
thing i forgot to mention is that we also have   what's called a bias term in here which allows 
you to shift your activation function left or   right now on the right hand side of this diagram 
you can actually see this concept illustrated or   written out mathematically as a single equation 
you can actually rewrite this in terms of linear   algebra matrix multiplications and dot products 
to to represent this a bit more concisely   so let's do that let's now do that with x 
capital x which is a vector of our inputs   x1 through xm and capital w which is a vector 
of our weights w1 through wm so each of these   are vectors of length m and the output is very 
simply obtained by taking their dot product   adding a bias which in this case is 
w0 and then applying a non-linearity g one thing is that i haven't i've been mentioning 
it a couple of times this non-linearity g what   exactly is it because i've mentioned it now a 
couple of times well it is a non-linear function   one common example of this nonlinear activation 
function is what is known as the sigmoid function   defined here on the right in fact there 
are many types of nonlinear functions   you can see three more examples here including the 
sigmoid function and throughout this presentation   you'll actually see these tensorflow code 
blocks which will actually illustrate   uh how we can take some of the topics that we're 
learning in this class and actually practically   use them using the tensorflow software library now 
the sigmoid activation function which i presented   on the previous slide is very popular since it's 
a function that gives outputs it takes as input   any real number any activation value and it 
outputs a number always between 0 and 1. so   this makes it really really suitable for problems 
and probability because probabilities also have to   be between 0 and 1 so this makes them very well 
suited for those types of problems in modern deep   neural networks the relu activation function which 
you can see on the right is also extremely popular   because of its simplicity in this case it's a 
piecewise linear function it is zero before when   it's uh in the negative regime and it is strictly 
the identity function in the positive regime but one really important question that i 
hope that you're asking yourselves right now   is why do we even need activation functions 
and i think actually throughout this course   i do want to say that no matter what 
i say in the course i hope that always   you're questioning why this is a necessary step 
and why do we need each of these steps because   often these are the questions that can lead to 
really amazing research breakthroughs so why do   we need activation functions now the point of 
an activation function is to actually introduce   non-linearities into our network because these are 
non-linear functions and it allows us to actually   deal with non-linear data this is extremely 
important in real life especially because in   the real world data is almost always non-linear 
imagine i told you to separate here the green   points from the red points but all you could 
use is a single straight line you might think   this is easy with multiple lines or curved lines 
but you can only use a single straight line and   that's what using a neural network with a linear 
activation function would be like that makes the   problem really hard because no matter how deep the 
neural network is you'll only be able to produce   a single line decision boundary and you're 
only able to separate your space with one line   now using non-linear activation functions 
allows your neural network to approximate   arbitrarily complex functions and that's what 
makes neural networks extraordinarily powerful let's understand this with a simple example so 
that we can build up our intuition even further   imagine i give you this trained network now with 
weights on the left hand side 3 and negative 2.   this network only has two inputs x1 and x2 
if we want to get the output of it we simply   do the same story as i said before first take 
a dot product of our inputs with our weights   add the bias and apply a non-linearity 
but let's take a look at what's inside   of that non-linearity it's simply a 
weighted combination of our inputs   and the in the form of a two-dimensional line 
because in this case we only have two inputs   so if we want to compute this output it's the 
same story as before we take a dot product of x   and w we add our bias and apply our non-linearity 
what about what's inside of this nonlinearity g   well this is just a 2d line in fact since it's 
just a two dimensional line we can even plot it   in two-dimensional space this is called 
the feature space the input space   in this case the feature space and the input 
space are equal because we only have one neuron   so in this plot let me describe what you're seeing 
so on the two axes you're seeing our two inputs so   on one axis is x1 one of the inputs on the other 
axis is x2 our other input and we can plot the   line here our decision boundary of this trained 
neural network that i gave you as a line in this   space now this line corresponds to actually all 
of the decisions that this neural network can make   because if i give you a new data point for example 
here i'm giving you negative 1 2. this point lies   somewhere in this space specifically at x one 
equal to negative one and x two equal to two   that's just a point in the space i want you to 
compute its weighted combination and i i can   actually follow the perceptron equation to get 
the answer so here we can see that if we plug it   into the perceptron equation we get 1 plus minus 
3 minus 4 and the result would be minus 6. we   plug that into our nonlinear activation function 
g and we get a final output of 0.002 now in fact   remember that the sigmoid function actually 
divides the space into two parts of either   because it outputs everything between zero and 
one it's dividing it between a point at 0.5 and   greater than 0.5 and less than 0.5 when the input 
is less than 0 and greater than 0.5 that's when   the input is positive we can illustrate the space 
actually but this feature space when we're dealing   with a small dimensional data like in this case 
we only have two dimensions but soon we'll start   to talk about problems where we have thousands or 
millions or in some cases even billions of inpu   of uh weights in our neural network and then 
drawing these types of plots becomes extremely   challenging and not really possible anymore but 
at least when we're in this regime of small number   of inputs and small number of weights we can make 
these plots to really understand the entire space   and for any new input that we obtain for example 
an input right here we can see exactly that this   point is going to be having an activation function 
less than zero and its output will be less than   0.5 the magnitude of that actually is computed 
by plugging it into the perceptron equation so   we can't avoid that but we can immediately get an 
answer on the decision boundary depending on which   side of this hyperplane that we lie on when we 
plug it in so now that we have an idea of how to   build a perceptron let's start by building neural 
networks and seeing how they all come together so let's revisit that diagram of the perceptron 
that i showed you before if there's only a few   things that you get from this class i really 
want everyone to take away how a perceptron works   and there's three steps remember them always 
the dot product you take a dot product of your   inputs and your weights you add a bias and you 
apply your non-linearity there's three steps   let's simplify this diagram a little bit let's 
clean up some of the arrows and remove the bias   and we can actually see now that every line here 
has its own associated weight to it and i'll   remove the bias term like i said for simplicity 
note that z here is the result of that dot   product plus bias before we apply the activation 
function though g the final output though is is   simply y which is equal to the activation 
function of z which is our activation value now if we want to define a multi-output neural 
network we can simply add another perceptron to   this picture so instead of having one perceptron 
now we have two perceptrons and two outputs each   one is a normal perceptron exactly like we saw 
before taking its inputs from each of the x i x   ones through x m's taking the dot product adding a 
bias and that's it now we have two outputs each of   those perceptrons though will have a different set 
of weights remember that we'll get back to that if we want it so actually one thing to keep 
in mind here is because all the inputs are   densely connected every input has a connection to 
the weights of every perceptron these are often   called dense layers or sometimes fully connected 
layers now we're through this class you're going   to get a lot of experience actually coding up 
and practically creating some of these algorithms   using a software toolbox called tensorflow 
so now that we have the understanding of how   a single perceptron works and how a dense 
layer works this is a stack of perceptrons   let's try and see how we can actually build up 
a dense layer like this all the way from scratch   to do that we can actually start by initializing 
the two components of our dense layer   which are the weights and the biases now that we 
have these two parameters of our neural network   of our dense layer we can actually define the 
forward propagation of information just like   we saw it and learn about already that forward 
propagation of information is simply the dot   product or the matrix multiplication of our 
inputs with our weights at a bias that gives   us our activation function here and then we 
apply this non-linearity to compute the output now tensorflow has actually implemented 
this dense layer for us so we don't need   to do that from scratch instead we 
can just call it like shown here so   to create a dense layer with two outputs 
we can specify this units equal to two now let's take a look at what's called a single 
layered neural network this is one we have a   single hidden layer between our inputs and our 
outputs this layer is called the hidden layer   because unlike an input layer and an output layer 
the states of this hidden layer are typically   unobserved they're hidden to some extent they're 
not strictly enforced either and since we have   this transformation now from the input layer to 
the hidden layer and from the hidden layer to the   output layer each of these layers are going to 
have their own specified weight matrices we'll   call w1 the weight matrices for the first layer 
and w2 the weight matrix for the second layer if we take a zoomed in look at one of the neurons 
in this hidden layer let's take for example z2 for   example this is the exact same perceptron that we 
saw before we can compute its output again using   the exact same story taking all of its inputs x1 
through xm applying a dot product with the weights   adding a bias and that gives us z2 if we 
look at a different neuron let's suppose z3   we'll get a different value here because the 
weights leading to z3 are probably different than   those leading to z2 now this picture looks a bit 
messy so let's try and clean things up a bit more from now on i'll just use this symbol here to 
denote what we call this dense layer or fully   connected layers and here you can actually 
see an example of how we can create this   exact neural network again using tensorflow 
with the predefined dense layer notation   here we're creating a sequential model where 
we can stack layers on top of each other   first layer with n neurons and the second 
layer with two neurons the output layer and if we want to create a deep neural network 
all we have to do is keep stacking these layers   to create more and more hierarchical models 
ones where the final output is computed   by going deeper and deeper into the network and 
to implement this in tensorflow again it's very   similar as we saw before again using the tf keras 
sequential call we can stack each of these dense   layers on top of each other each one specified 
by the number of neurons in that dense layer   n1 and 2 but with the last output layer fixed to 
two outputs if that's how many outputs we have okay so that's awesome now we have an idea of not 
only how to build up a neural network directly   from a perceptron but how to compose them together 
to form complex deep neural networks let's take a   look at how we can actually apply them to a very 
real problem that i believe all of you should   care very deeply about here's a problem that we 
want to build an ai system to learn to answer   will i pass this class and we can start with 
a simple two feature model one feature let's   say is the number of lectures that you attend as 
part of this class and the second feature is the   number of hours that you spend working on your 
final project you do have some training data   from all of the past participants of success 191 
and we can plot this data on this feature space   like this the green points here actually indicate 
students so each point is one student that has   passed the class and the red points 
are students that have failed the pass   failed the class you can see their where they are 
in this feature space depends on the actual number   of hours that they attended the lecture the number 
of lectures they attended and the number of hours   they spent on the final project and then there's 
you you spent you have attended four lectures   and you have spent five hours on your 
final project and you want to understand   uh will you or how can you build a neural network 
given everyone else in this class will you pass   or fail uh this class based on the training data 
that you see so let's do it we have now all of the   uh to do this now so let's build a neural 
network with two inputs x1 and x2 with x1 being   the number of lectures that we attend x2 is the 
number of hours you spend on your final project   we'll have one hidden layer with three units and 
we'll feed those into a final probability output   by passing this class and we can see that 
the probability that we pass is 0.1 or 10   that's not great but the reason is because 
that this model uh was never actually trained   it's basically just a a baby it's never seen any 
data even though you have seen the data it hasn't   seen any data and more importantly you haven't 
told the model how to interpret this data it needs   to learn about this problem first it knows nothing 
about this class or final projects or any of that   so one of the most important things to do 
this is actually you have to tell the model   when it's able when it is making bad predictions 
in order for it to be able to correct itself   now the loss of a neural network actually defines 
exactly this it defines how wrong a prediction was   so it takes as input the predicted outputs 
and the ground truth outputs now if those   two things are very far apart from each other 
then the loss will be very large on the other   hand the closer these two things are from each 
other the smaller the loss and the more accurate   the loss the model will be so we always 
want to minimize the loss we want to incur   that we want to predict something that's 
as close as possible to the ground truth now let's assume we have not just the data 
from one student but as we have in this case   the data from many students we now care about 
not just how the model did on predicting just   one prediction but how it did on average 
across all of these students this is what   we call the empirical loss and it's 
simply just the mean or the average   of every loss from each individual 
example or each individual student when training a neural network we 
want to find a network that minimizes   the empirical loss between our 
predictions and the true outputs now if we look at the problem of binary 
classification where the neural network   like we want to do in this case is supposed to 
answer either yes or no one or zero we can use   what is called a soft max cross entropy loss now 
the soft max cross entropy loss is actually built   is actually written out here and it's 
defined by actually what's called the   cross entropy between two probability 
distributions it measures how far apart   the ground truth probability distribution is 
from the predicted probability distribution   let's suppose instead of predicting binary 
outputs will i pass this class or will i not   pass this class instead you want to predict the 
final grade as a real number not a probability   or as a percentage we want the the grade that you 
will get in this class now in this case because   the type of the output is different we also need 
to use a different loss here because our outputs   are no longer 0 1 but they can be any real number 
they're just the grade that you're going to get on   on the final class so for example here since this 
is a continuous variable the grade we want to use   what's called the mean squared error this measures 
just the the squared error the squared difference   between our ground truth and our predictions 
again averaged over the entire data set   okay great so now we've seen two loss functions 
one for classification binary outputs as well as   regression continuous outputs and the problem now 
i think that we need to start asking ourselves is   how can we take that loss function we've seen our 
loss function we've seen our network now we have   to actually understand how can we put those two 
things together how can we use our loss function   to train the weights of our neural network 
such that it can actually learn that problem   well what we want to do is actually 
find the weights of the neural network   that will minimize the loss of our 
data set that essentially means   that we want to find the ws in our neural network 
that minimize j of w jfw is our empirical cost   function that we saw in the previous slides that 
average loss over each data point in the data set now remember that w capital w is simply 
a collection of all of the weights in our   neural network not just from one layer 
but from every single layer so that's   w0 from the zeroth layer to the first layer 
to the second layer all concatenate into one   in this optimization problem we want to optimize 
all of the w's to minimize this empirical loss now remember our loss function is just a 
simple function of our weights if we have   only two weights we can actually plot this entire 
lost landscape over this grid of weight so on the   one axis on the bottom you can see weight number 
one and the other one you can see weight zero   there's only two weights in this neural network 
very simple neural network so we can actually plot   for every w0 and w1 what is the loss what is the 
error that we'd expect to see and obtain from this   neural network now the whole process of training a 
neural network optimizing it is to find the lowest   point in this lost landscape that will tell us our 
optimal w0 and w1 now how can we do that the first   thing we have to do is pick a point so let's pick 
any w0 w1 starting from this point we can compute   the gradient of the landscape at that point 
now the gradient tells us the direction of   highest or steepest ascent okay 
so that tells us which way is up   okay if we compute the gradient of our 
loss with respect to our weights that's   the derivative our gradient for the loss 
with respect to the weights that tells   us the direction of which way is up on that 
lost landscape from where we stand right now   instead of going up though we want to find the 
lowest loss so let's take the the negative of our   gradient and take a small step in that direction 
okay and this will move us a little bit closer   to the lowest point and we just keep repeating 
this now we compute the gradient at this point   and repeat the process until we converge 
and we will converge to a local minimum   we don't know if it will converge to a global 
minimum but at least we know that it should   in theory converge to a local minimum now we can 
summarize this algorithm as follows this algorithm   is also known as gradient descent so we start by 
initializing all of our weights randomly and we   start and we loop until convergence we start 
from one of those weights our initial point   we compute the gradient that tells us which way is 
up so we take a step in the opposite direction we   take a small step here small is computed by 
multiplying our gradient by this factor eta   and we'll learn more about this factor later 
this factor is called the learning rate   we'll learn more about that later now again in 
tensorflow we can actually see this pseudocode   of grading descent algorithm written out in 
code we can randomize all of our weights that   in that basically initializes our search our 
optimization process at some point in space   and then we keep looping over and over and over 
again and we compute the loss we compute the   gradient and we take a small step of our weights 
in the direction of that gradient but now let's   take a look at this term here this is the how 
we actually compute the gradient this explains   how the loss is changing with respect to 
the weight but i never actually told you   how we compute this so let's 
talk about this process   which is actually extremely important in training 
neural networks it's known as backpropagation so how does backpropagation work 
how do we compute this gradient   let's start with a very simple neural network 
this is probably the simplest neural network   in existence it only has one input one hidden 
neuron and one output computing the gradient of   our loss j of w with respect to one of the weights 
in this case just w2 for example tells us how much   a small change in w2 is going to affect our loss 
j so if we move around j infinitesimally small how   will that affect our loss that's what the gradient 
is going to tell us of derivative of j of w 2.   so if we write out this derivative we can actually 
apply the chain rule to actually compute it   so what does that look like specifically 
we can decompose that derivative into the   derivative of j d d w over d y multiplied by 
derivative of our output with respect to w2   now the question here is with the second part 
if we want to compute now not the derivative   of our loss with respect to w2 but now the 
loss with respect to w1 we can do the same   story as before we can apply the chain rule now 
recursively so now we have to apply the chain   rule again to this second part now the second 
part is expanded even further so the derivative   of our output with respect to z1 which is the 
activation function of this first hidden unit and   we can back propagate this information now you can 
see starting from our loss all the way through w2   and then recursively applying this chain rule 
again to get to w1 and this allows us to see   both the gradient at both w2 and w1 so 
in this case just to reiterate once again   this is telling us this dj dw1 is telling us how 
a small change in our weight is going to affect   our loss so we can see if we increase our weight a 
small amount it will increase our loss that means   we will want to decrease the weight to decrease 
our loss that's what the gradient tells us which   direction we need to step in order to decrease 
or increase our loss function now we showed this   here for just two weights in our neural network 
because we only have two weights but imagine   we have a very deep neural network one with more 
than just two layers of or one layer rather of of   hidden units we can just repeat this this process 
of applying recursively applying the chain rule   to determine how every single way in the model 
needs to change to impact that loss but really   all this boils down to just recursively applying 
this chain rule formulation that you can see here and that's the back propagation algorithm in 
theory it sounds very simple it's just a very   very basic extension on derivatives and the chain 
rule but now let's actually touch on some insights   from training these networks in practice that make 
this process much more complicated in practice   and why using back propagation as we saw there 
is not always so easy now in practice training   neural networks and optimization of networks can 
be extremely difficult and it's actually extremely   computationally intensive here's the visualization 
of what a lost landscape of a real neural network   can look like visualized on just two dimensions 
now you can see here that the loss is extremely   non-convex meaning that it has many many local 
minimum that can make using an algorithm like   gradient descent very very challenging because 
gradient descent is always going to step   closest to the first local minimum but it can 
always get stuck there so finding how to get   to the global minima or a really good solution for 
your neural network can often be very sensitive to   your hyperparameters such as where the optimizer 
starts in this lost landscape if it starts in a   potentially bad part of the landscape it can very 
easily get stuck in one of these local minimum now recall the equation that we talked about for 
gradient descent this was the equation i showed   you your next weight update is going to be your 
current weights minus a small amount called the   learning rate multiplied by the gradient so we 
have this minus sign because we want to step in   the opposite direction and we multiply it by the 
gradient or we multiply by the small number called   here called eta which is what we call the learning 
rate how fast do we want to do the learning   now it determines actually not just how fast 
to do the learning that's maybe not the best   way to say it but it tells us how large should 
each step we take in practice be with regards   to that gradient so the gradient tells us the 
direction but it doesn't necessarily tell us   the magnitude of the direction so eta can tell 
us actually a scale of how much we want to trust   that gradient and step in the direction of that 
gradient in practice setting even eta this one   parameters one number can be extremely difficult 
and i want to give you a quick example of why   so if you have a very non-convex loc or lost 
landscape where you have local minima if you   set the learning rate too low then the model 
can get stuck in these local minima it can   never escape them because it gets it actually does 
optimize itself but it optimizes it to a very sm   to a non-optimal minima and it can converge very 
slowly as well on the other hand if we increase   our learning rate too much then we can actually 
overshoot our our minima and actually diverge   and and lose control and basically uh explode the 
training process completely one of the challenges   is actually how to pre how to use stable learning 
rates that are large enough to avoid the local   minima but small enough so that they don't 
diverge and convert or that they don't diverge   completely so they're small enough to actually 
converge to that global spot once they reach it so how can we actually set this learning 
rate well one option which is actually   somewhat popular in practice is to actually 
just try a lot of different learning rates   and that actually works it is a feasible approach 
but let's see if we can do something a little bit   smarter than that more intelligent what if we 
could say instead how can we build an adaptive   learning rate that actually looks at its lost 
landscape and adapts itself to account for what   it sees in the landscape there are actually 
many types of optimizers that do exactly this   this means that the learning rates are no longer 
fixed they can increase or decrease depending on   how large the gradient is in that location and how 
fast we want and how fast we're actually learning and many other options that could be also with 
regards to the size of the weights at that point   the magnitudes etc in fact these have been widely 
explored and published as part of tensorflow as   well and during your labs we encourage each of 
you to really try out each of these different   types of optimizers and experiment with 
their performance in different types of   problems so that you can gain very important 
intuition about when to use different types of   optimizers are what their advantages are and 
disadvantages in certain applications as well   so let's try and put all of this together so 
here we can see a full loop of using tensorflow   to define your model on the first line define 
your optimizer here you can replace this with   any optimizer that you want here i'm just using 
stochastic gradient descent like we saw before   and feeding it through the model we loop 
forever we're doing this forward prediction   we predict using our model we compute the 
loss with our prediction this is exactly   the loss is telling us again how incorrect our 
prediction is with respect to the ground truth y   we compute the gradient of our loss with respect 
to each of the weights in our neural network and   finally we apply those gradients using our 
optimizer to step and update our weights   this is really taking everything that we've 
learned in the class in the lecture so far   and applying it into one one whole 
piece of code written in tensorflow   so i want to continue this talk and really talk 
about tips for training these networks in practice   now that we can focus on this very powerful 
idea of batching your data into mini batches   so before we saw it with gradient descent that 
we have the following algorithm this gradient   that we saw to compute using back propagation can 
be actually very intensive to compute especially   if it's computed over your entire training set so 
this is a summation over every single data point   in the entire data set in most real-life 
applications it is simply not feasible to   compute this on every single iteration in 
your optimization loop alternatively let's   consider a different variant of this algorithm 
called stochastic gradient descent so instead   of computing the gradient over our entire data 
set let's just pick a single point compute the   gradient of that single point with respect to the 
weights and then update all of our weights based   on that gradient so this has some advantages this 
is very easy to compute because it's only using   one data point now it's very fast but it's also 
very noisy because it's only from one data point   instead there's a middle ground instead of 
computing this noisy gradient of a single point   let's get a better estimate of our gradient by 
using a batch of b data points so now let's pick   a batch of b data points and we'll compute the 
gradient estimation estimate simply as the average   over this batch so since b here is usually not 
that large on the order of tens or hundreds of   samples this is much much faster to compute than 
regular gradient descent and it's also much much   more accurate than just purely stochastic gradient 
descent that only uses a single example now this   increases the gradient accuracy estimation which 
also allows us to converge much more smoothly   it also means that we can trust our gradient more 
than in stochastic gradient descent so that we can   actually increase our learning rate a bit more 
as well mini-batching also leads to massively   parallelizable computation we can split up the 
batches on separate workers and separate machines   and thus achieve even more parallelization and 
speed increases on our gpus now the last topic   i want to talk about is that of overfitting this 
is also known as the problem of generalization and   is one of the most fundamental problems in all 
of machine learning and not just deep learning now overfitting like i said is critical to 
understand so i really want to make sure that   this is a clear concept in everyone's mind ideally 
in machine learning we want to learn a model   that accurately describes our test data not the 
training data even though we're optimizing this   model based on the training data what we really 
want is for it to perform well on the test data   so said differently we want 
to build representations   that can learn from our training data but 
still generalize well to unseen test data   now assume you want to build a line to describe 
these points underfitting means that the model   does simply not have enough capacity to 
represent these points so no matter how good   we try to fit this model it simply does not 
have the capacity to represent this type of data   on the far right hand side we can see the 
extreme other extreme where here the model   is too complex it has too many parameters 
and it does not generalize well to new data   in the middle though we can see what's called 
the ideal fit it's not overfitting it's not   underfitting but it has a medium number of 
parameters and it's able to fit in a generalizable   way to the output and is able to generalize well 
to brand new data when it sees it at test time   now to address this problem let's talk about   regularization how can we make sure that our 
models do not end up over fit because neural   networks do have a ton of parameters how 
can we enforce some form of regularization   to them now what is regularization regularization 
is a technique that constrains our optimization   problems such that we can discourage these complex 
models from actually being learned and overfit   right so again why do we need it we need it so 
that our model can generalize to this unseen   data set and in neural networks we have many 
techniques for actually imposing regularization   onto the model one very common technique and very 
simple to understand is called dropout this is one   of the most popular forms of regularization 
in deep learning and it's very simple let's   revisit this picture of a neural network this is 
a two layered neural network two hidden layers   and in dropout during training all we simply 
do is randomly set some of the activations here   to zero with some probability so what we can do is 
let's say we pick our probability to be 50 or 0.5   we can drop randomly for each of the activations 
50 of those neurons this is extremely powerful as   it lowers the capacity of our neural network so 
that they have to learn to perform better on test   sets because sometimes on training sets it just 
simply cannot rely on some of those parameters   so it has to be able to be resilient to 
that kind of dropout it also means that   they're easier to train because at least on every 
forward passive iterations we're training only 50   of the weights and only 50 of the gradients so 
that also cuts our uh gradient computation time   down in by a factor of two so because now 
we only have to compute half the number of   neuron gradients now on every iteration we dropped 
out on the previous iteration fifty percent of   neurons but on the next iteration we're going 
to drop out a different set of fifty 50 of the   neurons a different set of neurons and this gives 
the network it basically forces the network to   learn how to take different pathways to get to its 
answer and it can't rely on any one pathway too   strongly and overfit to that pathway this is a way 
to really force it to generalize to this new data   the second regularization technique that 
we'll talk about is this notion of early   stopping and again here the idea is very basic 
it's it's basically let's stop training once   we realize that our our loss is increasing on a 
held out validation or let's call it a test set   so when we start training we all know the 
definition of overfitting is when our model   starts to perform worse on the test set so if we 
set aside some of this training data to be quote   unquote test data we can monitor how our network 
is learning on this data and simply just stop   before it has a chance to overfit so on the x-axis 
you can see the number of training iterations and   on the y-axis you can see the loss that we 
get after training that number of iterations   so as we continue to train in the beginning both 
lines continue to decrease this is as we'd expect   and this is excellent since it 
means our model is getting stronger   eventually though the network's testing 
loss plateaus and starts to increase   note that the training accuracy will always 
continue to go to go down as long as the   network has the capacity to memorize the data and 
this pattern continues for the rest of training   so it's important here to actually focus on this 
point here this is the point where we need to stop   training and after this point assuming 
that our test set is a valid representation   of the true test set the accuracy of the model 
will only get worse so we can stop training here   take this model and this should be the model that 
we actually use when we deploy into the real world   anything any model taken from the left hand 
side is going to be underfit it's not going   to be utilizing the full capacity of the 
network and anything taken from the right   hand side is over fit and actually performing 
worse than it needs to on that held out test set so i'll conclude this lecture by summarizing 
three key points that we've covered so far   we started about the fundamental building 
blocks of neural networks the perceptron   we learned about stacking and composing 
these perceptrons together to form complex   hierarchical neural networks and how to 
mathematically optimize these models with   back propagation and finally we address the 
practical side of these models that you'll find   useful for the labs today including adaptive 
learning rates batching and regularization   so thank you for attending the first 
lecture in 6s191 thank you very much 

Hi everyone, my name is Ava. I'm a lecturer and 
organizer for 6.S191 and welcome to lecture 2   which is going to focus on deep sequence modeling. 
So in the first lecture with Alexander we learned   about the essentials of neural networks and built 
up that understanding moving from perceptrons to   feed-forward models. Next we're going to turn our 
attention to applying neural networks to problems   which involve sequential processing of 
data and we'll see why these sorts of tasks   require a different type of network 
architecture from what we've seen so far.   To build up an understanding of these types of 
models we're going to walk through this step   by step starting from intuition about how these 
networks work and building that up using where   we left off with perceptrons and feed-forward 
models as introduced in the first lecture. So   let's dive right in. First, I'd like to motivate 
what we mean in terms of sequential modeling and   sequential processing by beginning with a very 
simple intuitive example. Let's suppose we have   this picture here of a ball and our task is 
to predict where it's going to travel to next.   Without any prior information about the 
ball's history or understanding of the   dynamics of its motion any guess on its 
next position is going to be exactly that   just to guess. But instead if in addition to the 
current location of the ball I also gave you its   previous locations now our problem becomes much 
easier and I think we can all agree that we have   a sense of where the ball is going to travel to 
next. So hopefully this this intuitive example   gives you a sense of what we mean in terms of 
sequential modeling and sequential prediction   and the truth is that sequential data and these 
types of problems are really all around us for   example audio like the waveform from my speech 
can be split up into a sequence of sound waves and   text can be split up into a sequence of characters 
or also words when here each of these individual   characters or each of the individual words can 
be thought of as a time step in our sequence   now beyond these two examples there are many 
more cases in which sequential processing   can be useful from medical signals to EKGs to 
prediction of stock prices to genomic or genetic   data and beyond so now that we've gotten a 
sense of what sequential data looks like let's   think about some concrete applications in which 
sequence modeling plays out in the real world   in the first lecture Alexander introduced 
feed-forward models that sort of operate   in this one-to-one manner going from a fixed 
and static input to a fixed and static output   for example he gave this this 
use case of binary classification   where we were trying to build a model that 
given a single input of a student in this class   could be trained to predict whether or not that 
student was going to pass or not and in this type   of example there's no time component there's no 
inherent notion of sequence or of sequential data   when we consider sequence modeling we now expand 
the range of possibilities to situations that can   involve temporal inputs and also potentially 
sequential outputs as well so for example   let's consider the case where we have a language 
processing problem where there's a sentence as   input to our model and that defines a sequence 
where the words in the sentence are the individual   time steps in that sequence and at the end our 
task is to predict one output which is going to   be the sentiment or feeling associated with that 
sequence input and you can think of this problem   as a having a sequence input single output 
or as sort of a many-to-one sequence problem   we can also consider the converse case where now 
our input does not have that time dimension so   for example when we're considering a static image 
and our task is now to produce a sequence of input   of outputs for example a sentence caption 
that describes the content in this image   and you can think of this as a one 
to many sequence modeling problem   finally we can also consider this situation 
of many to many where we're now translating   from a sequence to another sequence and perhaps 
one of the most well-known examples of this   type of application is in machine translation 
where the goal is to train a model to translate   sentences from one language to another all right 
so this hopefully gives you a concrete sense of   use cases and applications where sequence modeling 
becomes important now i'd like to move forward   to understand how we can actually build neural 
networks to tackle these sorts of problems and   sometimes it can be a bit challenging to sort of 
wrap your head around how we can add this temporal   dimension to our models so to address this and 
to build up really a strong intuition i want to   start from the very fundamentals and we'll 
do that by first revisiting the perceptron   and we're going to go step by step to develop a 
really solid understanding of what changes need   to be made to our neural network architecture 
in order to be able to handle sequential data   all right so let's recall and revisit the 
perceptron which we studied in lecture one   we defined the set of inputs right 
which we can call x1 through xn   and each of these numbers are going 
to be multiplied by a weight matrix   and then they're going to all be added together 
to form this internal state of the perceptron   which we'll say is z and then this value z is 
passed through a non-linear activation function   to produce a predictive output y hat and remember 
that with the perceptron you can have multiple   inputs coming in and since you know in this 
lecture overall we're considering sequence   modeling i'd like you to think of these inputs 
as being from a single time step in your sequence   we also saw how we could extend from the single 
perceptron to now a layer of perceptrons to yield   multi-dimensional outputs so for example here 
we have a single layer of perceptrons in green   taking three inputs in blue and predicting four 
outputs shown in purple but once again does this   have a notion of time or of sequence no it doesn't 
because again our inputs and our outputs you can   think of as being from a fixed time step in our 
sequence so let's simplify this diagram right   and to do that we'll collapse that hidden layer 
down to this green box and our input and output   vectors will be as depicted here and again our 
inputs x are going to be some vectors of length   m and our outputs are going to be of length n but 
still we're still considering the input at just   a specific time denoted here by t which is nothing 
different from what we saw in the first lecture   and even with this this simplified representation 
of a feed forward network we could naively already   try to feed a sequence into this model by just 
applying that same model over and over again once   for each time step in our sequence to get a sense 
of this and how we could handle these individual   inputs across different time step let's first just 
rotate the same diagram from the previous slide   so now again we have an input vector x of t 
from some time step t we feed it into our neural   network and then get an output vector at that time 
step but since we're interested in sequential data   let's assume we don't just have a single time step 
right we have multiple individual time steps which   start from let's say time zero the first time step 
in our sequence and we could take that input at   that time step treat it as this isolated point 
in time pass it into the model and generate a   predictive output and we could do that for the 
next time step again treating it as something   isolated and same for the next and to emphasize 
here all of these models depicted here are just   replicas of each other right with different 
inputs at each of these different time steps   but we we know sort of that our output and we 
know from the first lecture that our output vector   y hat at a particular time sub t is just going 
to be a function of the input at that time step   but let's take a step back here for a 
minute if we're considering sequential data   it's probably very likely that the output or the 
label at a later time step is going to somehow   depend on the inputs at prior time steps so what 
we're missing here by treating these individual   time steps as individual isolated time steps is 
this relationship that's inherent to sequence   data between inputs earlier on in the sequence 
to what we predict later on in the sequence so   how could we address this what we really 
need is a way to relate the computations   and the operations that the network is doing at 
a particular time step to both the prior history   of its computation from prior time steps as well 
as the input at that time step and finally to have   a sense of forward looking right to be able to 
pass that information the current information onto   future time steps so let's try to do exactly that 
what we'll consider is linking the information and   the computation of the network at different time 
steps to each other specifically we're going to   introduce this internal memory or cell state which 
we denote here as h of t and this is going to be   this memory that's going to be maintained by the 
neurons and the network itself and this state   can be passed on time step to time step across 
time and the key idea here is that by having   this recurrence relation we're capturing some 
notion of memory of what the sequence looks like   what this means is now the network's output 
predictions and its computations are not only   a function of the input at a particular time 
step but also the past memory of cell state   denoted by h that is to say that our output 
depends on both our current inputs as well as   the past computations and the past learning that 
occurred and we can define this relationship via   these functions that map inputs to output and 
these functions are standard neural network   operations that alexander introduced in the first 
lecture so once again our output our prediction is   going to depend not only on the current input at 
a particular time step but also on the past memory and because as you see in this relation here our 
output is now a function of both the current input   and the past memory at a previous 
time step this means we can describe   these neurons via a recurrence 
relation which means that the   we have the cell state that depends on the current 
input and again on the prior in prior cell states   and the depiction on the right of this line 
shows these individual time steps being sort   of unrolled across time but we could also 
depict the same relationship by this cycle   and this is shown on the loop on the left of the 
slide which shows this concept of a recurrence   relation and it's exactly this idea of recurrence 
that provides the intuition and the key operations   behind recurrent neural networks or rnns and 
we're going to continue for the remainder of   this lecture to build up from this foundation and 
build up our understanding of the mathematics of   these recurrence relations and the 
operations that define rnn behavior   all right so let's formalize this a little 
bit more the key idea here as i mentioned and   hopefully that you take away from this lecture 
is that these rnns maintain this internal state   h of t which is updated at each time step as 
the sequence is processed and this is done   by this recurrence relation which specifically 
defines how the state is updated at the time step   specifically we define this internal cell state 
h of t and that internal cell state is going to   be a function of that is going to be defined by 
a function that can be parametrized by a set of   weights w which are what we're actually trying to 
learn over the course of training such a network   and that function f of w is going to take as input 
both the input at the current time step x of t as   well as the prior state h of t minus 1. and how 
do we actually find and define this function again   it's going to be parametrized by a set of weights 
that are going to be specifically what's learned   over the course of training the model and a key 
key feature of rnns is that they use this very   same function and this very same set of parameters 
at every time step of processing the sequence   and of course the weights are going to 
change over time over the course of training   and later on we'll see exactly how but at 
each iteration of training that same set of   weights is going to be applied to each of 
the individual time steps in the sequence   all right so now let's let's step through the 
algorithm for updating rnns to get a better sense   of how these networks work we're going to 
begin by initializing our network which i'm   just abstracting away here in this code 
block as in the pseudo code block as rnn   and we're also going to initialize a hidden state 
as well as a sentence and let's say ours our task   here is to predict the network the next word 
in the sentence the rnn algorithm is as follows   we're going to loop through the words in the in 
this sentence and at each step we're going to   feed both the current word and the previous 
hidden state into our rnn and this is going   to generate a prediction for the next word as 
well as an update to the hidden state itself   and finally when we've done when we've when we're 
done processing these four words in this sentence   we can generate our prediction 
for what the next word actually is   by considering the rnn's output after all the 
individual words have been fed through the model all right so as you may have realized the 
rnn computation includes both this internal   cell state update to hft as well as the output 
prediction itself so now we're going to concretely   walk through how each of these computations is 
defined all right going from bottom to top right   we're going to consider our input vector x of t 
and we're next going to apply a function to update   the hidden state and this function is a standard 
neural network operation just like we saw in the   first lecture and again because this internal 
cell state h of t is going to depend on both the   input x of t as well as the prior cell state h of 
t minus one we're going to multiply each of these   individual terms by their respective weight 
matrices and we're going to add the result   and then apply a non-linear activation function 
which in this case is going to be a hyperbolic   tangent to the sum of these two terms to 
actually update the value of the hidden state   and then to generate our output at a given time 
step we take that internal hidden state multiply   it by a separate weight matrix which inherently 
produces a modified version of this internal state   and this actually forms our output prediction 
so this gives you the mathematics behind   how the rnn can actually update its hidden 
state and also to produce a predictive output all right so so far we've seen rnn's being 
depicted as having these internal loops that   feedback on themselves and we've also seen how we 
can represent this loop as being unrolled across   time where we can start from a first time step 
and continue to unroll the network across time   up until time set t and within this diagram 
we can also make explicit the weight matrices   starting from the weight matrix that defines how 
the inputs at each time step are being transformed   in the hidden state computation as 
well as the weight matrices that define   the relationship between the prior hidden 
state and the current hidden state and finally   the weight matrix that transforms the hidden 
state to the output at a particular time step   and again to re-emphasize in all of these 
cases for all of these weight matrices   we're going to be reusing the same weight matrix 
matrices at every time step in our sequence   now when we make a forward pass through the 
network we're going to generate outputs at   each of those individual time steps and from those 
individual outputs we can derive a value for the   loss and then we can sum all of these losses 
from the individual time steps together   to determine the total loss 
which will be ultimately what is   used to train our rnn and we'll get to 
exactly how we achieve this in a few slides all right so now this this gives you a an 
intuition and mathematical foundation for   how we actually can make a forward 
pass a forward step through our rnn   let's now walk through an example of how we can 
implement an rnn from scratch using tensorflow   we're going to define the rnn using a layer so 
we can build it up from as inheriting from the   layer class that alexander introduced in the first 
lecture we can also initialize the weight matrices   and also finally initialize the hidden state of 
the rnn to all zeros our next step is going to be   to define what we call the call function 
and this function is really important   because it describes exactly how we can make a 
forward pass through the network given a input   our first step in this forward pass is 
going to be to update the hidden state   according to that same exact equation 
we saw earlier where the hidden state   and the from the previous time step and an 
input x are multiplied by their relative   relevant wave matrices are summed and then 
passed through a non-linear activation function   we next compute the output by transforming 
this hidden state via multiplication by a   separate weight matrix and at each time step 
we're going to return both the current output as   well as the hidden state so this gives a sense 
of breaks down how we define the forward pass   through an rnn in code using tensorflow but 
conveniently tensorflow has already implemented   these types of rnn cells for us which you can 
use via the simple rnn layer and you're going   to get some practice doing exactly this and using 
the rnns later on in today's lab all right so to   recap now that we're at this point in this lecture 
where we've built up our understanding of rnn's   and their mathematical basis i'd like to turn back 
to those applications of sequence modeling that we   discussed earlier on and hopefully now you've 
gotten a sense of why rnns can be particularly   suited for handling sequential data again with 
feedforward or traditional neural networks   we're operating in this one-to-one manner 
going from a static input to a static output   in contrast with sequences we can go from a 
sequential input where we have many time steps   defined sequentially over time feed them into 
a recurrent neural network and generate a   a single output like a classification of 
sentiment or emotion associated with a sentence   we can also move from a static input for example 
an image to a sequential output going from one to   many and finally we can go from sequential input 
to sequential output many to many and two examples   of this are in machine translation and also in 
music generation and with the latter with music   generation you'll actually get the chance 
to implement an rnn that does exactly this   in later on in today's lab beyond this we can 
extend recurrent neural networks to many other   applications in which sequential processing 
and sequential modeling may be useful to to really appreciate why recurrent 
neural networks are so powerful   i'd like to sort of consider a concrete 
set of what i like to call design criteria   that we need to be keeping in mind when thinking 
about sequence modeling problems specifically we   need to be able to ensure that our recurrent 
neural network or any machine learning model   that we may be interested in will be equipped to 
handle variable length sequences because not all   sentences not all sequences are going to have 
the same length so we need to have the ability   to handle this variability we also need to have 
this critical property of being able to track   long-term dependencies in the data and to 
have a notion of memory and associated with   that is also the ability to have this sense 
of order and have a sense of how things that   occur previously or earlier on in the sequence 
affect what's going to happen or occur later on   and to do this we can achieve both points two and 
three by using weight sharing and actually sharing   the values of the way matrices across the entire 
sequence and we'll see i'm telling you now and   we'll see that recurrent neural networks do indeed 
meet all these sequence modeling design criteria   all right so to understand these criteria 
concretely i'd like to consider a very concrete   sequence modeling problem which is going to be the 
following given some series of words in a sentence   our task is going to be to predict the most likely 
next word to occur in that sentence all right   so let's suppose we have this sentence as an 
example this morning i took my cat for a walk   and our task is let's say we're given these 
words this morning i took my cat for a and   we want to predict the next word in the c in the 
sentence walk and our goal is going to be to try   to build a recurring neural network to do exactly 
this what's our first step to tackle this problem   well the first consideration before we even get 
started with training our model is how we can   actually represent language to a neural network so 
let's suppose we have a model where we input the   word deep and we want to use the neural network to 
predict the next word learning what could be the   issue here in terms of how we are passing in these 
in this input to our network remember that neural   networks are functional operators they execute 
functional mathematical operations on their inputs   and generate numerical outputs as a result so 
they can't really interpret and operate on words   if they're just passed in as words so what we 
have here is is just simply not going to work   instead neural networks require numerical inputs 
that can be a vector or an array of numbers such   that the model can operate on them to generate 
a vector or array of numbers as the output   so this is going to work for us but 
operating just on words simply is not all right so now we know that we need to have 
a way to transform language into this vector   or array based representation how exactly are we 
going to go about this the solution we're going   to consider is this concept of embedding which is 
this idea of transforming a set of identifiers for   objects effectively indices into a vector of fixed 
size that captures the the content of the input so   to think through how we could actually go about 
doing this for language data let's again turn   back to that example sentence that we've been 
considering this morning i took my cat for a walk   we want to be able to map any word that 
appears or could appear in our body of language   to a fixed size vector so our first step 
is going to be generate to generate a   vocabulary which is going to consist of 
all unique words in our set of language   we can then index these individual 
words by mapping individual unique words   to unique indices and these indices can 
then be mapped to a vector embedding   one way we could do this is by generating sparse 
and binary vectors that are going to have a length   that's equal to the number of unique words in 
our vocabulary such that we can then indicate   the nature of a particular word by encoding this 
in the corresponding index so for example for the   word cat we could encode this at the second index 
in this sparse binary vector and this is a very   common way of embedding and encoding language 
data and it's called a one hot encoding and   very likely you're going to encounter this in your 
journey through machine learning and deep learning   another way we could build up these embeddings 
is by actually learning them so the idea here   is to take our index mapping and feed that index 
mapping into a model like a neural network model   such that we can then transform that index mapping   across all the words of our vocabulary to 
a to a vector of a lower dimensional space   where the values of that vector are learned 
such that words that are similar to each other   are have similar embeddings and an example 
that demonstrates this concept is shown here   all right so these are two distinct ways in which 
we can encode language data and transform language   data into a vector representation that's going 
to be suitable for input to a neural network   now that we've now that we've built up this 
way to encode language data and to actually get   started with feeding it into our recurrent neural 
network model let's go back to that set of design   criteria where the first capability we desired is 
this ability to handle variable sequence lengths   and again let's consider this task of trying to 
predict the next word in a sentence we could have   very short sentences right where driving words 
driving the meaning of our prediction are going   to be very close to each other but we could 
also have a longer sequence or an even longer   sequence where the information that's needed 
to predict the next word occurs much earlier on   and the key requirement for our recurrent 
neural network model is the ability to   handle these inputs of varying length feed forward 
networks are not able to do this because they have   inputs of fixed dimensionality and then those 
fixed dimensionality inputs are passed into the   next layer in contrast rnns are able to handle 
variable sequence lengths and that's because   those differences in sequence lengths are just 
differences in the number of time steps that   are going to be input and processed by the rnn 
so rnns meet this first first design criterion   our second criterion is the ability to effectively 
capture and model long-term dependencies in data   and this is really exemplified in examples like 
this one where we clearly need information from   much earlier in the sequence or the sentence to 
accurately make our prediction rnns are able to   achieve this because they have this way of 
updating their internal cell state via the   recurrence relation we previously discussed 
which fundamentally incorporates information   from the past state into the cell state update 
so this criteria is also met next we need to   be able to capture differences in sequence order 
which could result in differences in the overall   meaning or property of a sequence for example in 
this case where we have two sentences that have   opposite semantic meaning but have the same words 
with the same counts just in a different order   and once again the cell state maintained by an 
rnn depends on its past history which helps us   capture these sorts of differences because we 
are maintaining information about past history   and also reusing the same weight matrices across 
each of the individual time steps in our sequence   so hopefully going through this example of 
predicting the next word in a sentence with a very   particularly common type of 
sequential data being language data   this shows how it shows you how sequential data 
more broadly can be represented and encoded for   input to rnns and how rnns can achieve these 
set the set of sequence modeling design criteria   all right so now we at this stage in the lecture 
we've built up our intuition and our understanding   of how recurrent neural networks work how they 
operate and what it means to model sequences   now we can discuss the algorithm for how we can 
actually train recurrent neural networks and it's   a twist on the back propagation algorithm that 
was introduced in lecture one it's called back   propagation through time so to get there let's 
first again take a step back to our first lecture   and recall how we can actually train feed-forward 
models using the back propagation algorithm   we first take a set of inputs and make a forward 
pass through the network going from input to   output and then to train the model we back 
propagate net gradients back through the network   and we take the derivative of the loss with 
respect to each weight parameter in our network   and then adjust the parameters the weights 
in our model in order to minimize that loss for rnns as we walked through earlier are 
forward pass through the network consists   of going forward across time and updating the cell 
state based on the input as well as the previous   state generating an output and fundamentally 
computing the loss values at the individual   time steps in our sequence and finally summing 
those individual losses to get the total loss   instead of back propagating errors through a 
single feed-forward network at a single time   step in rnns those errors are going to 
be back propagated from the overall loss   through each individual time step and then 
across the time steps all the way from where   we are currently in the sequence back to the 
beginning and this is the reason why it's called   back propagation through time because as 
you can see all of the errors are going   to be flowing back in time from the most recent 
time step to the very beginning of the sequence   now if we expand this out and take a closer 
look at how gradients can actually flow across   this chain of repeating recurrent neural network 
module we can see that between each time step we   have to perform this matrix multiplication 
that involves the weight matrix w h of h   and so computing the gradient with respect to the 
initial cell state h of 0 is going to involve many   factors of this weight matrix and also repeated 
computation of the gradients with respect to the   this weight matrix this can be problematic for a 
couple of reasons the first being that if we have   many values in this series this chain of matrix 
multiplications where the gradient values are   less or greater than 1 or the weight values are 
greater than 1 we can run into a problem that's   called the exploding gradient problem where our 
gradients are going to become extremely large and   we can't really optimize and the solution here is 
to do what is called gradient clipping effectively   scaling back the values of particularly 
large gradients to try to mitigate this   we can also have the opposite problem where now 
our weight values or our gradients are very very   small and this can lead to what is 
called the vanishing gradient problem   when gradients become increasingly smaller and 
smaller and smaller such that we can no longer   effectively train the network and today we're 
going to discuss three ways in which we can   address this vanishing gradient problem first by 
cleverly choosing our activation function also by   smartly initially initializing our weight matrices 
and finally we can we'll discuss how we can make   some changes to the network architecture itself 
to alleviate this vanishing gradient problem   all right in order to get into that you'll need 
some intuition about why vanishing gradients could   be a problem let's imagine you keeps multiplying 
a small number something in between 0 and 1 by   another small number over time that number 
is going to keep shrinking and shrinking and   eventually it's going to vanish and what this 
means when this occurs for gradients is that   it's going to be harder and harder and harder 
to propagate errors from our loss function   back into the distant past because we have this 
problem of the gradients becoming smaller and   smaller and smaller and ultimately what this 
will lead to is we're going to end up biasing   the weights and the parameters of our network 
to capture shorter term dependencies in the data   rather than longer term dependencies to see why 
this could be a problem let's again consider   this example of training a language model to 
predict the next word in a sentence of words   and let's say we're given this phrase the clouds 
are in the blank in this case it's it's pretty   obvious what the next word is likely going to be 
right sky because there's not that much of a gap   in the sequence between the relevant information 
the word cloud and the place where our prediction   is actually going to be needed and so 
an rnn could be equipped to handle that   but let's say now we have this other sentence 
i grew up in france and i speak fluent blank   where now there's more context that's needed from 
earlier in the sentence to make that prediction   and in many cases that's going to be exactly 
the problem where we have this large gap   between what's relevant and the point where we may 
need to make a prediction and as this gap grows   standard rnns become increasingly unable to 
connect the relevant information and that's   because of this vanishing grading 
problem so it relates back to this   need to be able to effectively model and 
capture long-term dependencies in data   how can we get around this the first trick 
we're going to consider is pretty simple we   can smartly select the activation function our 
networks use specifically what is commonly done   is to use a relu activation function where the 
derivative of the of this activation function   is greater than one for all instances in which x 
is greater than zero and this helps the value of   the gradient of our of our with respect to 
our loss function to actually shrink when um   the values of its input are greater than zero 
another thing we can do is to be smart in how   we actually initialize the parameters in our 
network and we can specifically initialize the   weights to the identity matrix to be able to try 
to prevent them from shrinking to zero completely   and very rapidly during back propagation our final 
solution and the one that we're going to spend the   most time discussing and it's also the most robust 
is to introduce and to use a sort of more complex   recurrent unit that can more effectively 
track long-term dependencies in the data   by intuitively you can think of it as controlling 
what information is passed through and and what   information is used to update the actual cell 
state specifically we're going to use what is   called gated cells and today we're going to focus 
on one particular type of gated cell which is   definitely the most common and most broadly used 
in recurrent neural networks and that's called   the long short term memory unit or lstm and what's 
cool about lstms is that networks that are built   using lstms are particularly well suited at better 
maintaining long-term dependencies in the data and   tracking information across multiple time steps 
to try to overcome this vanishing gradient problem   and more importantly to more effectively model 
sequential data all right so lstms are really the   the workhorse of the deep learning community 
for most sequential modeling tasks so let's   discuss a bit about how lcms work and my goal for 
this part of the lecture is to provide you with   the intuition about the fundamental operations of 
lstms abstracting away a little bit of the math   because it can get again a little confusing to 
wrap your mind around but hopefully i hope to   provide you with the intuitive understanding 
about how these networks work all right   so to understand the key operations that make 
lstm special let's first go back to the general   structure of an rnn and here i'm depicting 
it slightly differently but the concept is   exactly that from what i introduced previously 
where we build up our recurrent neural network   via this repeating module linked across time and 
what you're looking at here is a representation   of an rnn that shows a illustration of those 
operations that define the state and output   update functions so here we've simplified 
it down where these black lines effectively   capture weight matrix multiplications 
and the yellow rectangles such as the   tan h depicted here show the non-linear the 
application of a non-linear activation function   so here in this diagram this repeating module 
of the rnn contains a single computation neural   network computation node which is consisting of 
a tan h activation function layer so here again   we perform this update to the internal cell 
state h of t which is going to depend on the   previous cell state h of t minus 1 as well as 
the current input x of t and at each time step   we're also going to generate an output 
prediction y of t a transformation of the state   lstms also have this chain like structure but 
the internal repeating module that recurrent unit   is slightly more complex in the lcm this 
repeating recurrent unit contains these   different interacting layers again which are 
defined by standard neural network operations   like sigmoid and 10h non-linear activation 
functions weight matrix multiplications but   what's cool and what's neat about these different 
interacting layers is that they can effectively   control the flow of information through the 
lstm cell and we're going to walk through how   these updates actually enable the lstms to track 
and store information throughout many time steps   and here you can see how we can 
define a lstm layer using tensorflow   all right so the key idea behind lstms is 
that they can effectively selectively add   or remove information to the internal cell state 
using these structures that are abstracted away   calling by being called gates and these gates 
consist of a standard neural network layer like a   sigmoid shown here and a pointwise multiplication 
so let's let's take a moment to think about what a   gate like this could be doing in this case because 
we have the sigmoid activation function this is   going to force anything that passes through that 
gate to be between 0 and 1. so you can effectively   think of this as modulating and capturing how 
much of the input should be passed through   between between nothing 0 or everything one 
which effectively gates the flow of information   lstms use this type of operation to process 
information by first forgetting irrelevant   by first forgetting irrelevant history secondly 
by storing the most relevant new information   fi thirdly by updating their internal cell state 
and then generating a output the first step is to   forget irrelevant parts of the previous state 
and this is achieved by taking the previous state   and passing it through one of these sigmoid gates 
which again you can think of it as as modulating   how much should be passed in or kept out the 
next step is to determine what part of the new   information and what part of the old information 
is relevant and to store this into the cell state and what is really critical about lstms is that 
they maintain the separate value of the cell state   c of t in addition to what we introduced 
previously h of t and c of t is what is   going to be selectively updated 
by via these gatewise operations   finally we can return an output from our 
lstm and so there is a a interacting layer   an output gate that can control what information 
that's encoded in the cell state is ultimately   outputted and sent to the network as input 
in the following time step so this operation   controls both the value of the output y of t as 
well as the um the cell state that's passed on   time step to time step in the form of h of t the 
key takeaway that i want you to have about lstms   in this from this lecture is that they 
can regulate information flow in storage   and by doing this they can effectively better 
capture longer term dependencies and also help   us train the networks overall and overcome the 
vanishing gradient problem and the key way that   they help during training is that all of these 
different gating mechanisms actually work to allow   for what i like to call the uninterrupted 
flow of gradient computation over time   and this is done by maintaining the 
separate cell state c of t across which   the actual gradient computation so taking the 
derivative with respect to the weights updating derivative of the loss with respect to the 
weights and shifting the weights in response   occurs with respect to this uh separate separately 
maintained cell state c of t and what this   ultimately allows for is that we can mitigate 
the vanishing gradient problem that's seen with   traditional rnns so to recap on the key concepts 
behind lstms lstms maintain a separate cell state   from what is outputted that's c of t they use 
these gates to control the flow of information   by forgetting uh irrelevant information from 
past history storing relevant information   from the current input updating their cell state 
and outputting the prediction at each time step   and it's really this maintenance of the separate 
cell state cft which allows for back propagation   through time with uninterrupted gradient 
flow and more efficient and more effective   training and so for these reasons lscms are very 
very commonly used as sort of the backbone rnn in   modern deep learning all right so now that we've 
gone through the fundamental workings of rnn's   been introduced to the back propagation through 
time algorithm and also considered the lstm   architecture i'd like to consider a few very 
concrete practical examples of how recurrent   neural networks can be deployed for sequential 
modeling including an example that you'll get   experience with in today's lab and that's the 
the task and the question of music generation   or music prediction so let's suppose you're trying 
to build up a recurrent neural network that can   take sequences of musical notes and actually from 
that sequence predict what is the most likely next   note to occur and not only do you want to predict 
what's the most likely next note to occur we want   to actually take this trained model and use 
it to generate brand new musical sequences   that have never been heard before and we can 
do this by basically seeding a trained rnn   model with a first note and then um iteratively 
building up the sequence over time to generate   a new song and indeed this is one of the one of 
the most exciting and powerful applications of   recurrent neural networks and to motivate this 
which is going to be the topic of your lab today   i i'm going to introduce a really fun and 
interesting historical example and it turns out   that one of the most famous classical composers 
franz schubert had a famous symphony which was   called the unfinished symphony and the symphony 
is is described exactly like that it's unfinished   it was actually left at two movements rather than 
four and schubert did not was not able to finish   composing the symphony symphony before he died 
and recently they there was a neural network based   algorithm that was trained and put to the test 
to actually finish this symphony and compose   two new movements and this was done by training 
the the model of recurrent neural network model on   schubert's body of work and then testing 
it by tasking with tasking the model with   trying to generate the new composition 
given given the score of the previous two   movements of this unfinished symphony so 
let's listen in to see what the result was i'd like to continue because i'm actually 
enjoying listening to that music but we also   have to go on with the lecture so pretty awesome 
right and i hope you agree that i think you know   it's really exciting to see neural networks being 
put to the test here but also at least for me this   sparks some questioning and understanding 
about sort of what is the line between   artificial intelligence and human creativity and 
you'll get a chance to explore this in today's lab   another cool example is beyond music generation 
is one in language processing where we can go from   an input sequence like a sentence to a single 
output where we can train an rnn to take this   to train an rnn to let's say produce a a 
prediction of emotion or sentiment associated with   a particular sentence either positive or negative 
and this is effectively a classification task   much like what we saw in the first lecture 
except again we're operating over a sequence   where we have this time component so because 
this is a classification problem we can train   these networks using a cross-entropy loss and one 
application where that we may be interested in is   classifying the sentiments associated with 
tweets so for example this tweet we could   train an rnn to predict that this first tweet 
about our class 6s191 has a positive sentiment   but that this other tweet about the 
weather actually has a negative sentiment   all right so the next example i'll talk about 
is one of the most powerful applications of   recurrent neural networks and it's the backbone 
of things like google translate and that's   this idea of machine translation where our goal is 
to input a sentence in one language and train an   rnn to output a sentence in another language and 
this is can be done by having an encoder component   which effectively encodes the original sentence 
into some state vector and a decoder component   which decodes that state vector into 
the target language the new language   but it's it's quite remarkable that and it's quite 
remarkable that using the foundations and the   concepts that we learned today about sequential 
modeling and about recurrent neural networks   we can tackle this very complex problem of machine 
translation but what could be some potential   issues with using this approach using rnns or 
lstams the first issue is that we have this   encoding bottleneck we need to which means that 
we need to encode a lot of content for example a   long body of text of many different words into the 
single memory state vector which is a compressed   version of all the information necessary to 
complete the translation and it's this state   vector that's ultimately going to be passed on 
and decoded to actually achieve the translation   and by by forcing this this compression we could 
actually lose some important information by   imposing um this extreme encoding bottleneck which 
is definitely a problem another limitation is that   the recurrent neural networks that we learned 
about today are not really that efficient as they   require sequential processing of information 
which is the point that i've sort of been   driving home all along and it's the sequential 
nature that makes these recurrent neural networks   relatively inefficient on modern gpu hardware 
because it's difficult to parallelize them   and furthermore besides this problem of speed   we need to be able to to to train the 
rnn we need to go from the decoded output   all the way back to the original input 
which will involve going through order t or   number t iterations of the of the network where t 
is the number of time steps that we feed into our   sequence and so what this means in practice is 
that back propagation through time is actually   very very expensive especially when considering 
large bodies of text that need to be translated that's depicted here finally 
and perhaps most importantly   is this fact that traditional rnns have limited 
memory capacity and we saw that recurrent neural   networks suffered from this vanishing gradient 
problem lestm's helped us a bit but still they   both of these architectures are not super 
effective at handling very long temporal   dependencies that could be found in large bodies 
of text that need to be translated so how can we   build an architecture that could be aware of these 
dependencies that may occur in larger sequences or   bodies of text to overcome these these limitations 
a method that's called attention was developed   and instead the way it works is that instead of 
the decoder component having access to only the   final encoded state that state vector passed 
from encoder to decoder instead the decoder   now has access to the states after each of the 
time steps in the original sentence and it's   the weighting of these vectors that is actually 
learned by the network over the course of training   and this is a really interesting idea because 
what this attention module actually does is to   learn from the input which points and which states 
to attend to and it makes it very efficient and   capable of capturing long-term dependencies 
as easily as it does short-term dependencies   and that's because to train such a network 
it only requires a single pass through this   attention module no back propagation through 
time and what you can think of these attention   mechanisms as providing is this sort of learnable 
memory access and indeed this system is called   attention because when the network is actually 
learning the weighting it's learning to place   its attention on different parts of the input 
sequence to effectively capture a sort of   accessible memory across the entirety of the 
original sequence it's a really really really   powerful idea and indeed it's the basis of 
a new class and then rapidly emerging class   of models that are extremely powerful for large 
scale sequential modeling problems and that that   class of models is called transformers 
which you may have heard about as well this application and this consideration of 
attention is very important not only in language   modeling but in other applications as well so for 
example if we're considering autonomous vehicles   at any moment in time an autonomous vehicle like 
this one needs to have a understanding of not   only where every object is in its environment 
but also where a particular object may move in   the future so here's an example of a self-driving 
car and the the red boxes on the right hand side   depict a cyclist and as you can see the 
cyclist is approaching a stopped vehicle shown here in purple and the car the self-driving 
car can be able to recognize that the the cyclist   is now going to merge in front of the car and 
before it does so the self-driving car pulls back   and stops and so this is an example of trajectory 
prediction and forecasting in which it's clear   that we need to be able to attend to and make 
effective predictions about where dynamic objects   in a scene may move to in the future another 
powerful example of sequential modeling is in   environmental modeling and climate pattern 
analysis and prediction so here we can visualize   the predicted patterns for different environmental 
markers such as winds and humidity and   it's an extremely important and powerful 
application for sequence modeling and for   recurrent neural networks because effectively 
predicting the future behavior of such markers   could aid in projecting and planning for 
long-term climate impact all right so hopefully   over the course of this lecture you've gotten 
a sense of how recurrent neural networks work   and why they're so powerful for processing 
sequential data we saw how we can model sequences   by this defined recurrence relation and how 
we could train them using the back propagation   through time algorithm we then explored 
a bit about how gated cells like lstms   could help us model long-term dependencies in 
data and also discussed applications of rnns   to music generation machine translation and beyond 
so with that we're going to transition now to the   lab sessions where you're going to have a chance 
to begin to implement recurrent neural networks   on your own using tensorflow and we encourage 
you to come to the class and the lab office hour   gather town session to discuss the labs answer ask 
your questions about both lab content as well as   content from the lectures and we look 
forward to seeing you there thank you 

Hi Everyone and welcome back to MIT 6.S191! Today 
we're going to be talking about one of my favorite   topics in this course and that's how we can give 
machines a sense of vision now vision is one of   the most important human senses I believe sighted 
people rely on vision quite a lot from everything   from navigating in the world to recognizing 
and manipulating objects to interpreting facial   expressions and understanding very complex human 
emotions i think it's safe to say that vision is   a huge part of everyday human life and today we're 
going to learn about how we can use deep learning   to build very powerful computer vision 
systems and actually predict what is where   by only looking and specifically looking at only 
raw visual inputs i like to think that this is a   very super simple definition of what vision at its 
core really means but actually vision is so much   more than simply understanding what an image 
is of it means not just what the image is of   but also understanding where the objects 
in the scene are and really predicting   and anticipating forward in the future what's 
going to happen next take this scene for example   we can build computer vision algorithms that 
can identify objects in the scene such as this   yellow taxi or maybe even this white truck on the 
side of the road but what we need to understand   on a different level is what is actually going 
to be required to achieve true vision where are   all of these objects going uh for that we should 
actually focus probably more on the yellow taxi   than on the white truck because there are 
some subtle cues in this image that you can   probably pick up on that lead us to believe that 
probably this white truck is parked on the side   of the road it's stationary and probably won't 
be moving in the future at least for the time   that we're observing the scene the yellow taxi on 
the other hand is even though it's also not moving   it is much more likely to be stationary as a 
result of the pedestrians that are crossing in   front of it and that's something that is very 
subtle but can actually be reasoned about very   effectively by our brains and humans take this for 
granted but this is an extraordinarily challenging   problem in the real world and since in the 
real world building true vision algorithms   can require reasoning about all of these different 
components not just in the foreground but also   there are some very important cues that we can 
pick up in the background like this light this   uh road light as well as some obstacles in the 
far distance as well and building these vision   algorithms really does require an understanding 
of all of these very subtle details now   deep learning is bringing forward an incredible 
revolution or evolution as well of computer vision   algorithms and applications ranging from allowing 
robots to use visual cues to perform things like   navigation and these algorithms that you're going 
to learn about today in this class have become so   mainstreamed and so compressed that they are all 
fitting and running in each of our pockets in our   telephones for processing photos and videos 
and detecting faces for greater convenience   we're also seeing some extraordinarily exciting 
applications of vision in biology and medicine   for picking up on extremely subtle cues and 
detecting things like cancer as well as in the   field of autonomous driving and finally in a 
few slides i'll share a very inspiring story   of how the algorithms that you're going to learn 
about today are also being used for accessibility   to aid the visually impaired now deep learning 
has taken computer vision especially computer   vision by storm because of its ability to learn 
directly from the raw image inputs and learn to do   feature extraction only through observation of a 
ton of data now one example of that that is really   prevalent in the computer vision field is of 
facial detection and facial recognition on the   top left or on the left hand side you can actually 
see an icon of a human eye which pictorially i'm   using to represent images that we perceive and 
we can also pass through a neural network for   predicting these facial features now deep learning 
has transformed this field because it allows   the creator of the machine learning or the deep 
learning algorithm to easily swap out the end task   given enough data to learn this neural network 
in the middle between the vision and the task   and try and solve it so here we're performing an 
end task of facial detection but just equivalently   that end task could be in the context of 
autonomous driving here where we take an image as   an input which you can see actually in the bottom 
right hand corner and we try to directly learn   the steering control for the output and actually 
learn directly from this one observation of the   scene where the car should control so what is 
the steering wheel that should execute and this   is done completely end to end the entire control 
system here of this vehicle is a single neural   network learned entirely from data now this is 
very very different than the majority of other   self-driving car companies like you'll see with 
waymo and tesla et cetera and we'll talk more   about this later but i actually wanted to share 
this one clip with you because this is one of the   autonomous vehicles that we've been building 
in our lab and here in csail that i'm part of   and we'll see more about that 
later in the lecture as well   we're seeing like i mentioned a lot of 
applications in medicine and healthcare   where we can take these raw images and scans 
of patients and learn to detect things like   breast cancer skin cancer and now most recently 
taking scans of patients lungs to detect covid19 finally i want to share this inspiring story 
of how computer vision is being used to help   the visually impaired so in this project actually 
researchers built a deep learning enabled device   that can detect a trail for running and provide 
audible feedback to the visually impaired user   such that they can run and now to demonstrate 
this let me just share this very brief video   the machine learning algorithm that we have 
detects the line and can tell whether the   line is to the runners left right or center we 
can then send signals to the runner that guides   them left and right based on their positioning 
the first time we went out we didn't even know   if sound would be enough to guide me so it's a 
sort of that beta testing process that you go   through from human eyes it's very obvious it's 
very obvious to recognize the line teaching a   machine learning model to do that is not that 
easy you step left and right as you're running   so there's like a shake to the line left and 
right as soon as you start going outdoors   now the light is a lot more variable tree shadows 
falling leaves and also the line on the ground   can be very narrow and there may be only a few 
pixels for the computer vision model to recognize there was no tether there was no stick there 
was no furry dog it was just being with yourself ah that's the first time i run loading in decades so these are often tasks that 
we as humans take for granted   but for a computer it's really remarkable 
to see how deep learning is being applied   uh to some of these problems focused on really 
doing good and just helping people here in this   case the visually impaired a man who has never run 
without his uh guide dog before is now able to run   independently through the through the trails with 
the aid of this computer vision system and like   i said we often take these tasks for granted but 
because it's so easy for each sighted individual   for us to do them routinely but we can actually 
train computers to do them as well and in order   to do that though we need to ask ourselves 
some very foundational questions specifically   stemming from how we can build a computer that 
can quote unquote c and specifically how does a   computer process an image let's use an image as 
our base example of site to a computer so far   so to a computer images are just 
numbers there are two dimensional   lists of numbers suppose we have a picture here 
this is of abraham lincoln it's just made up of   what are called pixels each of those numbers 
can be represented by what's called a pixel   now a pixel is simply a number like i said here 
represented by a range of either zero to one or   in 0 to 255 and since this is a grayscale image 
each of these pixels is just one number if you   have a color image you would represent it by 
three numbers a red a green and a blue channel rgb   now what does the computer see so we can represent 
this image as a two-dimensional matrix of these   numbers one number for each pixel in the image and 
this is it this is how a computer sees an image   like i said if we have a rgb image not a 
a grayscale image we can represent this   by a three-dimensional array now we have 
three two-dimensional arrays stacked on   top of each other one of those two dimensional 
arrays corresponds to the red channel one for   the green one for the blue representing this 
rgb image and now we have a way to represent   images to computers and we can start to think 
about what types of computer vision algorithms   we can perform with this so there are very there 
are two very common types of learning tasks and   those are like we saw in the first and the second 
classes those are one regression and those are   also classification tasks in regression tasks our 
output takes the form of a continuous value and   in classification it takes a single class label so 
let's consider first the problem of classification   we want to predict a label for each image so 
for example let's say we have a database of all   u.s precedents and we want to build 
a classification pipeline to tell us   which precedent this image is of so we feed 
this image that we can see on the left hand   side to our model and we wanted to output the 
probability that this image is of any of these   particular precedents that this database consists 
of in order to classify these images correctly   though our pipeline needs to be able to tell what 
is actually unique about a picture of abraham   lincoln versus a picture of any other president 
like george washington or jefferson or obama another way i think about this uh 
these differences between these   images and the image classification pipeline 
is at a high level in terms of the features   that are really characteristics of that particular 
class so for example what are the features   that define abraham lincoln now classification 
is simply done by detecting the features in that   given image so if the features for a particular 
class are present in the image then we can predict   with pretty high confidence that that class is 
occurring with a high probability so if we're   building an image classic classification pipeline 
our model needs to know what are the features are   what they are and two it needs to be able to 
detect those features in a brand new image so   for example if we want to detect human faces some 
features that we might want to be able to identify   would be noses eyes and mouths whereas like if 
we want to detect cars we might be looking at   certain things in the image like wheels license 
plates and headlights and the same for houses and   doors and windows and steps these are all examples 
of features for the larger object categories now one way to do this and solve this problem is 
actually to leverage knowledge about a particular   field say let's say human faces so if we want 
to detect human faces we could manually define   in images what we believe those features 
are and actually use the results of our   detection algorithm for classification but there's 
actually a huge problem to this type of approach   and that is that images are just 3d arrays 
of numbers of brightness values and that each   image can have a ton of variation and this 
includes things like occlusions in the scene   there could also be variations in illumination 
the lighting conditions as well as you could   even think of intra class variation variation 
within the same class of images our classification   pipeline whatever we're building really 
needs to be invariant to all of these types   of variations but it still needs to be 
sensitive to picking out the different   inter-class variations so being able to 
distinguish a feature that is unique to this class   in comparison to features or variations of 
that feature that are present within the class now even though our pipeline could be used 
could use features that we as humans define   that is if a human was to come into 
this problem knowing something about   the problem a priori they could define 
or manually extract and break down   what features they want to detect for this 
specific task even if we could do that   due to the incredible variability of the scene 
of image data in general the detection of these   features is still an extremely challenging problem 
in practice because your detection algorithm needs   to be invariant to all of these different 
variations so instead of actually manually   defining these how can we do better and what we 
actually want to do is be able to extract features   and detect their presence in images automatically 
in a hierarchical fashion and this should remind   you back to the first lecture when we talked about 
hierarchy being a core component of deep learning   and we can use neural network-based approaches 
to learn these visual features directly from   data and to learn a hierarchy of features 
to construct a representation of the image   internal to our network so again like we saw in 
the first lecture we can detect these low-level   features and composing them together to 
build these mid-level features and then   in later layers these higher level features 
to really perform the task of interest   so neural networks will allow us to learn these 
hierarchies of visual features from data if we   construct them cleverly so this will require us to 
use some different architectures than what we have   seen so far in the class namely architectures 
from the first lecture with feedforward   dense layers and in the second lecture recurrent 
layers for handling sequential data this lecture   will focus on yet another type of way that we 
can extract features specifically focusing on   the visual domain so let's recap what we learned 
in lecture one so in lecture one we learned about   these fully connected neural networks also called 
dense neural networks where you can have multiple   hidden layers stacked on top of each other and 
each neuron in each hidden layer is connected to   every neuron in the previous layer now let's 
say we want to use a fully connected network   to perform image classification and we're going 
to try and motivate the the use of something   better than this by first starting with what we 
already know and we'll see the limitations of this   so in this case remember our input is 
this two-dimensional image it's a vector   a two-dimensional vector but it can be collapsed 
into a one-dimensional vector if you just stack   all of those dimensions on top of each other 
of pixel values and what we're going to do is   feed in that vector of pixel values to our hidden 
layer connected to all neurons in the next layer   now here you should already appreciate something 
and that is that all spatial information that we   had in this image is automatically gone it's 
lost because now since we have flattened this   two-dimensional image into one dimension we have 
now basically removed any spatial information   that we previously had by the next layer and our 
network now has to relearn all of that uh very   important spatial information for example that 
one pixel is closer to the its neighboring pixel   that's something very important in our input but 
it's lost immediately in a fully connected layer   so the question is how can we build some structure 
into our model so that in order so that we can   actually inform the learning process and provide 
some prior information to the model and help   it learn this very complicated and large input 
image so to do this let's keep our representation   of our image our 2d image as an array a 
two-dimensional array of pixel values let's not   collapse it down into one dimension now one 
way that we can use the spatial structure   would be to actually connect patches of our input 
not the whole input but just patches of the input   two neurons in the hidden layer so before 
everything was connected from the input layer to   the hidden layer but now we're just gonna connect 
only things that are within a single patch to the   next neuron in the next layer now that is really 
to say that each neuron only sees so if we look   at this output neuron this neuron is only going to 
see the values coming from the patch that precedes   it this will not only reduce the number of weights 
in our model but it's also going to allow us   to leverage the fact that in an image spatially 
close pixels are likely to be somewhat related and   correlated to each other and that's a fact that 
we should really take into account so notice how   the only that only a small region of the input 
layer influences this output neuron and that's   because of this spatially connected idea that 
we want to preserve as part of this architecture   so to define connections across the whole input 
now we can apply the same principle of connecting   patches in our input layer to single neurons in 
the subsequent layer and we can basically do this   by sliding that patch across the input image and 
for each time we slide it we're going to have a   new output neuron in the subsequent layer now this 
way we can actually take into account some of the   spatial structure that i'm talking about inherent 
to our input but remember that our ultimate task   is not only to preserve spatial structure but 
to actually learn the visual features and we   do this by weighting the connections between 
the patches and the neurons so we can detect   particular features so that each patch is going 
to try to perform that detection of the feature   so now we ask ourselves how can we rate this 
patch such that we can detect those features well   in practice there's an operation called a 
convolution and we'll first think about this   at a high level suppose we have a 4x4 patch 
or a filter which will consist of 16 weights   we're going to apply this same filter to by four 
patches in the input and use the result of that   operation to define the state of the neuron in 
the next layer so the neuron in the next layer   the output that single neuron is going to be 
defined by applying this patch with a filter   with of equal size and learned weights 
we're then going to shift that patch   over let's say in this case by two pixels we 
have here to grab the next patch and thereby   compute the next output neuron now this is how we 
can think about convolutions at a very high level   but you're probably wondering here well how does 
the convolution operator actually allow us to   extract features and i want to make this really 
concrete by walking through a very simple example so suppose we want to classify the letter x 
in a set of black and white images of letters   where black is equal to negative one 
and white is equal to positive one   now to classify it's clearly not possible to 
simply compare the two images the two matrices   on top of each other and say are they equal 
because we also want to be classifying this x   uh no matter if it has some slight deformations 
if it's shifted or if it's uh enlarged rotated   or deformed we need we want to build a classifier 
that's a little bit robust to all of these changes   so how can we do that we want to detect the 
features that define an x so instead we want   our model to basically compare images of a 
piece of an x piece by piece and the really   important pieces that it should look for are 
exactly what we've been calling the features   if our model can find those important features 
those rough features that define the x in the same   positions roughly the same positions then it can 
get a lot better at understanding the similarity   between different examples of x even in 
the presence of these types of deformities so let's suppose each feature is like a mini 
image it's a patch right it's also a small   array a small two-dimensional array of values and 
we'll use these filters to pick up on the features   common to the x's in the case of this x 
for example the filters we might want to   pay attention to might represent things like the 
diagonal lines on the edge as well as the crossing   points you can see in the second patch here so 
we'll probably want to capture these features   in the arms and the center of the x in order 
to detect all of these different variations   so note that these smaller matrices 
of filters like we can see on the   the top row here these represent the filters of 
weights that we're going to use as part of our   convolution operation in order to detect the 
corresponding features in the input image   so all that's left for us to define is actually 
how this convolution operation actually   looks like and how it's able to pick up on these 
features given each of these in this case three   filters so how can it detect given a filter where 
this filter is occurring or where this feature is   occurring rather in this image and that is 
exactly what the operation of convolution is   all about convolution the idea of convolution 
is to preserve the spatial relationship between   pixels by learning image features in small little 
patches of image data now to do this we need to   perform an element-wise multiplication between 
the filter matrix and the patch of the input image   of same dimension so if we have a patch 
of 3x3 we're going to compare that to an   input filter or our filter which is also of 
size 3x3 with learned weights so in this case   our filter which you can see on the top left all 
of its entries are of either positive one or one   or negative one and when we multiply this filter 
by the corresponding green input image patch   and we element wise multiply 
we can actually see the result   in this matrix so multiplying all of the positive 
ones by positive ones we'll get a positive one   multiplying a negative one by a negative one will 
also get a positive one so the result of all of   our element-wise multiplications is going to be 
a three by three matrix of all ones now the next   step in as part of the convolution operation is 
to add all of those element-wise multiplications   together so the result here after we add those 
outputs is going to be 9. so what this means now   actually so actually before we 
get to that let me start with   another very brief example suppose we want 
to compute the convolution now not of a   very large image but this is just of a five by 
five image our filter here is three by three so   we can slide this three by three filter over the 
entirety of our input image and performing this   element-wise multiplication and then adding 
the outputs let's see what this looks like so   let's start by sliding this filter over the top 
left hand side of our input we can element wise   multiply the entries of this patch of this filter 
with this patch and then add them together and   for this part this three by three filter is 
placed on the top left corner of this image   element-wise multiply add and we get this 
resulting output of this neuron to be four and we can slide this filter over one one spot by 
one spot to the next patch and repeat the results   in the second entry now would be corresponding 
to the activation of this filter applied to   this part of the image in this case three and 
we can continue this over the entirety of our   image until the end when we have completely filled 
up this activation or feature map and this feature   map really tells us where in the input image was 
activated by this filter so for example wherever   we see this pattern conveyed in the original input 
image that's where this feature map is going to   have the highest value and that's where we need 
to actually activate maximally now that we've   gone through the mechanism of the convolution 
operation let's see how different filters can be   used to produce feature maps so picture a woman 
of a woman a picture this picture of a woman's   face this woman's name is lena and the output of 
applying these three convolutional filters so you   can see the three filters that we're considering 
on the bottom right hand corner of each image   by simply changing the weights of these filters 
each filter here has a different weight we can   learn to detect very different features in 
that image so we can learn to sharpen the   image by applying this very specific type of 
sharpening filter we can learn to detect edges   or we can learn to detect very strong edges in 
this image simply by modifying these filters   so these filters are not learned filters these 
are constructed filters and there's been a ton   of research historically about developing hand 
engineering these filters but what convolutional   neural networks learn to want to do is actually 
to learn the weights defining these filters so   the network will learn what kind of features it 
needs to detect in the image doesn't need to do   edge detection or strong edge detection or does 
it need to detect certain types of edges curves   certain types of geometric objects etc 
what are the features that it needs to   extract from this image and by learning the 
convolutional filters it's able to do that   so i hope now you can actually appreciate 
how convolution allows us to capitalize on   very important spatial structure and to use sets 
of weights to extract very local features in the   image and to very easily detect different features 
by simply using different sets of weights and   different filters now these concepts of preserving 
spatial structure and local feature extraction   using the convolutional operation are actually 
core to the convolutional neural networks that   are used for computer vision tasks and that's 
exactly what i want to dive into next now that   we've gotten the operation the mathematical 
foundation of convolutions under our belts   we can start to think about how we can utilize 
this operation this operation of convolutions   to actually build neural networks for computer 
vision tasks and tie this whole thing in to this   paradigm of learning that we've been exposed to 
in the first couple lectures now these networks   aptly are named convolutional neural networks 
very appropriately and first we'll take a look   at a cnn or convolutional neural network designed 
specifically for the task of image classification   so how can you use cnns for classification let's 
consider a simple cnn designed for the goal here   to learn features directly from the image 
data and we can use these learned features   to map these onto a classification task for 
these images now there are three main components   and operations that are core to a cnn the first 
part is what we've already gotten some exposure to   in the first part of this lecture and that 
is the convolution operation and that allows   us like we saw earlier to generate these 
feature maps and detect features in our image   the second part is applying a non-linearity 
and we saw the importance of nonlinearities   in the first and the second lecture in order to 
help us deal with these features that we extract   being highly non-linear thirdly we need to 
apply some sort of pooling operation this is   another word for a down sampling operation 
and this allows us to scale down the size of   each feature map now the computation of a class 
of scores which is what we're doing when we define   an image classification task is actually 
performed using these features that we obtain   through convolution non-linearity and pooling 
and then passing those learned features into a   fully connected network or a dense layer like we 
learned about in the first part of the class in   the first lecture and we can train this model end 
to end from image input to class prediction output   using fully connected layers and convolutional 
layers end to end where we learn as part of the   convolutional layers the sets of weights of the 
filters for each convolutional layer and as well   as the weights that define these fully connected 
layers that actually perform our classification   task in the end and we'll go through each one of 
these operations in a bit more detail to really   break down the basics and the architecture 
of these convolutional neural networks so first we'll consider the convolution 
operation of a cnn and as before each neuron   in the hidden layer will compute a weighted 
sum of each of its inputs like we saw in the   dense layers we'll also need to add on a bias 
to allow us to shift the activation function   and apply and activate it with some non-linearity 
so that we can handle non-linear data   relationships now what's really special here is 
that the local connectivity is preserved each   neuron in the hidden layer you can see in the 
middle only sees a very specific patch of its   inputs it does not see the entire input neurons 
like it would have if it was a fully connected   layer but no in this case each neuron output 
observes only a very local connected patch as   input we take a weighted sum of those patches we 
compute that weighted sum we apply a bias and we   apply and activate it with a non-linear 
activation function and that's the   feature map that we're left with at the end of a 
convolutional layer we can now define this actual   operation more concretely using a mathematical 
equation here we're left with a 4x4 filter matrix   and for each neuron in the hidden layer its 
inputs are those neurons in the patch from the   previous layer we apply this set of weights wi 
j in this case like i said it's a four by four   filter and we do this element-wise multiplication 
of every element in w multiplied by the   corresponding elements in the input x we add the 
bias and we activate it with this non-linearity   remember our element-wise multiplication 
and addition is exactly that convolutional   operation that we talked about earlier so if you 
look up the definition of what convolution means   it is actually that exactly it's element-wise 
multiplication and then a summation of all of   the results and this actually defines also how 
convolutional layers are connected to these ideas   but with this single convolutional layer we 
can how can we have multiple filters so all   we saw in the previous slide is how we can take 
this input image and learn a single feature map   but in reality there are many types of features 
in our image how can we use convolutional layers   to learn a stack or many different types of 
features that could be useful for performing   our type of task how can we use this to do 
multiple feature extraction now the output layer   is still convolution but now it has a volume 
dimension where the height and the width are   spatial dimensions dependent upon 
the dimensions of the input layer   the dimensions of the filter 
the stride how how much we're   skipping on each each time that we apply the 
filter but we also need to think about the   the connections of the neurons in these layers 
in terms of their what's called receptive field   the locations of their input in the in the 
in the model in in the path of the model that   they're connected to now these parameters actually 
define the spatial arrangement of how the neurons   are connected in the convolutional layers and how 
those connections are really defined so the output   of a convolutional layer in this case will have 
this volume dimension so instead of having one   filter map that we slide along our image now we're 
going to have a volume of filters each filter   is going to be slid across the image and compute 
this convolution operation piece by piece for each   filter the result of each convolution operation 
defines the feature map that that convolution that   that filter will activate maximally so now we're 
well on our way to actually defining what a cnn is   and the next step would actually be to apply that 
non-linearity after each convolution operation we   need to actually apply this non-linear activation 
function to the output volume of that layer and   this is very very similar like i said in the 
first and we saw also in the second lecture   and we do this because image data is highly 
nonlinear a common example in the image domain   is to use an activation function of relu which 
is the rectified linear unit this is a pixel-wise   operation that replaces all negative values with 
zero and keeps all positive values with whatever   their value was we can think of this really as a 
thresholding operation so anything less than zero   gets thresholded to zero negative values indicate 
negative detection of a convolution but this   nonlinearity actually kind of uh clamps that to 
some sense and that is a nonlinear operation so   it does satisfy our ability to learn non-linear 
dynamics as part of our neural network model   so the next operation in convolutional 
neural networks is that of pooling   pooling is an operation that is commonly used to 
reduce the dimensionality of our inputs and of   our feature maps while still preserving spatial 
invariants now a common technique and a common   type of pooling that is commonly used in practice 
is called max pooling as shown in this example   max pooling is actually super simple and intuitive 
uh it's simply taking the maximum over these two   by two filters in our patches and sliding that 
patch over our input very similar to convolutions   but now instead of applying a element-wise 
multiplication and summation we're just simply   going to take the maximum of that patch so in 
this case as we feed over this two by two patch of   filters and striding that patch by a factor of two 
across the image we can actually take the maximum   of those two by two pixels in our input and that 
gets propagated and activated to the next neuron   now i encourage all of you to really think 
about some other ways that we can perform   this type of pooling while still making sure that 
we downsample and preserve spatial invariants   taking the maximum over that patch is 
one idea a very common alternative is   also taking the average that's called mean pooling 
taking the average you can think of actually   represents a very smooth way to perform the 
pooling operation because you're not just taking   a maximum which can be subject to maybe outliers 
but you're averaging it or also so you will get a   smoother result in your output layer but they 
both have their advantages and disadvantages so these are three operations three 
key operations of a convolutional   neural network and i think now we're actually 
ready to really put all of these together and   start to construct our first convolutional 
neural network end to end and with cnns   just to remind you once again we can layer 
these operations the whole point of this   is that we want to learn this hierarchy 
of features present in the image data   starting from the low-level features composing 
those together to mid-level features and then   again to high-level features that can be used 
to accomplish our task now a cnn built for image   classification can be broken down into two parts 
first the feature learning part where we actually   try to learn the features in our input image 
that can be used to perform our specific task   that feature learning part is actually done 
through those pieces that we've been seeing so far   in this lecture the convolution the non-linearity 
and the pooling to preserve the spatial invariance now the second part the convolutional layers and 
pooling provide output those the output excuse me   of the first part is those high-level features of 
the input now the second part is actually using   those features to perform our classification 
or whatever our task is in this case   the task is to output the class probabilities that 
are present in the input image so we feed those   outputted features into a fully connected or dense 
neural network to perform the classification we   can do this now and we don't mind about losing 
spatial invariance because we've already down   sampled our image so much that it's not really 
even an image anymore it's actually closer to a   vector of numbers and we can directly apply our 
dense neural network to that vector of numbers   it's also much lower dimensional now and we 
can output a class of probabilities using a   function called the softmax whose output actually 
represents a categorical probability distribution   it's summed uh equal to one so it does 
make it a proper categorical distribution   and it is each element in this is strictly between 
zero and one so it's all positive and it does sum   to one so it makes it very well suited for the 
second part if your task is image classification   so now let's put this all together what does a 
end-to-end convolutional neural network look like   we start by defining our feature extraction head 
which starts with a convolutional layer with 32   feature maps a filter size of 3x3 pixels and we 
downsample this using a max pooling operation   with a pooling size of 2 and a stride of 2. this 
is very exactly the same as what we saw when we   were first introducing the convolution operation 
next we feed these 32 feature maps into the next   set of the convolutional convolutional and pooling 
layers now we're increasing this from 32 feature   maps to 64 feature maps and still down scaling our 
image as a result so we're down scaling the image   but we're increasing the amount of features 
that we're detecting and that allows us to   actually expand ourselves in this dimensional 
space while down sampling the spatial information   the irrelevant spatial information now finally now 
that we've done this feature extraction through   only two convolutional layers in this case we can 
flatten all of this information down into a single   vector and feed it into our dense layers and 
predict these final 10 outputs and note here that   we're using the activation function of softmax 
to make sure that these outputs are a categorical   distribution okay awesome so so far we've talked 
about how we can use cnns for image classification   tasks this architecture is actually so powerful 
because it extends to a number of different tasks   not just image classification and the reason for 
that is that you can really take this feature   extraction head this feature learning part and 
you can put onto the second part so many different   end networks whatever and network you'd like to 
use you can really think of this first part as   a feature learning part and the second part as 
your task learning part now what that task is   is entirely up to you and what you desire so and 
that's that's really what makes these networks   incredibly powerful so for example we may want 
to look at different image classification domains   we can introduce new architectures for 
specifically things like image and object   detection semantic segmentation and even 
things like image captioning you can use   this as an input to some of the sequential 
networks that we saw in lecture two even so let's look at and dive a bit deeper into 
each of these different types of tasks that   we could use are convolutional neural networks 
for in the case of classification for example   there is a significant impact in medicine and 
healthcare when deep learning models are actually   being applied to the analysis of entire inputs of 
medical image scans now this is an example of a   paper that was published in nature for actually 
demonstrating that a cnn can outperform expert   radiologists at detecting breast cancer directly 
from mammogram images instead of giving a binary   prediction of what an output is though cancer or 
not cancer or what type of objects for example in   this image we may say that this image is an image 
of a taxi we may want to ask our neural network to   do something a bit more fine resolution and tell 
us for this image can you predict what the objects   are and actually draw a bounding box localize this 
image or localize this object within our image   this is a much harder problem since there may 
be many objects in our scene and they may be   overlapping with each other partially occluded 
etc so not only do we want to localize the object   we want to also perform classification on that 
object so it's actually harder than simply the   classification task because we still have to 
do classification but we also have to detect   where all of these objects are in addition 
to classifying each of those objects   now our network needs to also be flexible and 
actually and be able to infer not just potentially   one object but a dynamic number of objects in the 
scene now if we if we have a scene that only has   one taxi it should output a bounding box over just 
that single taxi and the bounding box should tell   us the xy position of one of the corners and maybe 
the height and the width of that bounding box as   well that defines our bounding box on the other 
hand if our scene contains many different types   of objects potentially even of different types of 
classes we want our network to be able to output   many different outputs as well and be flexible 
to that type of differences in our input even   with one single network so our network should not 
be constrained to only outputting a single output   or a certain number of outputs it needs to have a 
flexible range of how we can dynamically infer the   objects in the scene so what is one maybe naive 
solution to tackle this very complicated problem   and how can cnns be used to do that so what we 
can do is start with this image and let's consider   the simplest way possible to do this 
problem we can start by placing a random box   over this image somewhere in the image it has 
some random location it also has a random size   and we can take that box and feed it through 
our normal image classification network like   we saw earlier in the lecture this is 
just taking a single image or it's now   a sub image but it's still a single image and it 
feeds that through our network now that network is   tasked to predict what is the what is the class 
of this image it's not doing object detection   and it predicts that it has some class if there is 
no class of this box then it simply can ignore it   and we repeat this process then we pick another 
box in the scene and we pass that through the   network to predict its class and we can keep doing 
this with different boxes in the scene and keep   doing it and over time we can basically have many 
different class predictions of all of these boxes   as they're passed through our classification 
network in some sense if each of these boxes give   us a prediction class we can pick the boxes that 
do have a class in them and use those as a box   where an object is found if no object is found we 
can simply discard it and move on to the next box   so what's the problem with this well one is that 
there are way too many inputs the this basically   results in boxes and considering a number of 
boxes that have way too many scales way too   many positions too many sizes we can't possibly 
iterate over our image in all of these dimensions   and and and have this as a naive solute and have 
this as a solution to our object detection problem   so we need to do better than that so instead of 
picking random boxes or iterating over all of the   boxes in our image let's use a simple heuristic 
method to identify some places in the image   that might contain meaningful objects 
and use these to feed through our model   but still even with this uh extraction of region 
proposals the the rest of the store is the exact   same we extract the region of proposal and we feed 
it through the rest of our network we warp it to   be the correct size and then we feed it to our 
classification network if there's nothing in that   box we discard it if there is then we keep it and 
say that that box actually contained this image   but still this has two very important problems 
that we have to consider one is that it's still   super super slow we have to feed in each region 
independently to the model so if we extract   in this case 2000 regions we have here we have to 
feed this we have to run this network 2 000 times   to get the answer just for the single image it 
also tends to be very brittle because in practice   how are we doing this region proposal well 
it's entirely heuristic based it's not being   learned with a neural network and it's also 
even more importantly perhaps perhaps it's   detached from the feature extraction part so 
our feature extraction is learning one piece   but our region proposal piece of the network or 
of this architecture is completely detached so   the model cannot learn to predict regions 
that may be specific to a given task that   makes it very brittle for some applications 
now many variants have been proposed to   actually tackle and tackle some of these issues 
and advance this forward to accomplish object   detection but i'd like to touch on one extremely 
quickly just to point you on in this direction   for those of you who are interested and that's the 
faster rcnn method to actually learn these region   proposals the idea here is instead of feeding 
in this image to a heuristic based feedback or   region proposal network or method we can have a 
part of our network that is trained to identify   the proposal regions of our model of our image 
and that allows us to directly understand or   identify these regions in our original image where 
there are candidate patches that we should explore   for our classification and our for our object 
detection now each of these regions then are   processed with their own feature extractor as 
part of our neural network and individuals or   in their cnn heads then after these features for 
each of these proposals are extracted we can do   a normal classification over each of these 
individual regions very similar as before   but now the huge advantage of this is that it only 
requires a single forward pass through the model   we only feed in this image once we have a region 
proposal network that extracts the regions   and all of these regions are fed on to perform 
classification on the rest of the image   so it's super super fast compared to the previous 
method so in classification we predict one class   for an entire image of the model in object 
detection we predict bounding boxes over all   of the objects in order to localize them and 
identify them we can go even further than this   and in this idea we're still using cnns to predict 
this predict this output as well but instead of   predicting bounding boxes which are rather coarse 
we can task our network to also here predict   an entire image as well now one example 
of this would be for semantic segmentation   where the input is an rgb an image just a normal 
rgb image and the output would be pixel-wise   probabilities for every single pixel what is 
the probability that it belongs to a given class   so here you can see an example of this image 
of some two cows on the on some grass being   fed into the neural network and the neural 
network actually predicts a brand new image   but now this image is not an rgb image it's a 
semantic segmentation image it has a probability   for every single pixel it's doing a classification 
problem and it's learning to classify every single   pixel depending on what class it thinks it is 
and here we can actually see how the cow pixels   are being classified separately from the grass 
pixels and sky pixels and this output is actually   created using an up sampling operation not a down 
sampling operation but up sampling to allow the   convolutional decoder to actually increase its 
spatial dimension now these layers are the analog   you could say of the normal convolutional layers 
that we learned about earlier in the lecture   they're also already implemented in tensorflow so 
it's very easy to just drop these into your model   and allow your model to learn how to actually 
predict full images in addition or instead of   single class probabilities this semantic 
segmentation idea is extremely powerful   because it can be also applied to many 
different applications in healthcare as well   especially for segmenting for example 
cancerous regions on medical scans or   even identifying parts of the blood that are 
infected with diseases like in this case malaria let's see one final example here of how we can 
use convolutional feature extraction to perform   yet another task this task is different from 
the first three that we saw with classification   object detection and semantic segmentation now 
we're going to consider the task of continuous   robotic control here for self-driving cars 
and navigating directly from raw vision data   specifically this model is going to take as 
input as you can see on the top left hand side   the raw perception from the vehicle this is 
coming for example from a camera on the car and   it's also going to see a noisy representation of 
street view maps something that you might see for   example from google maps on your smartphone and it 
will be tasked not to predict the classification   problem or object detection but rather learn 
a full probability distribution over the space   of all possible control commands that this 
vehicle could take in this given situation   now how does it do that actually this entire model 
is actually using everything that we learned about   in this lecture today it can be trained end to 
end by passing each of these cameras through their   dedicated convolutional feature extractors and 
then basically extracting all of those features   and then concatenating them flattening them down 
and then concatenating them into a single feature   extraction vector so once we have this entire 
representation of all of the features extracted   from all of our cameras and our maps we can 
actually use this representation to predict the   full control parameters on top of a deterministic 
control given to the desired destination of the   vehicle this probabilistic control is very 
powerful because here we're actually learning   to just optimize a probability distribution over 
where the vehicle should steer at any given time   you can actually see this probability distribution 
visualized on this map and it's optimized simply   by the negative log likelihood which is the 
negative log likelihood of this distribution which   is a normal a mixture of normal distributions and 
this is nearly identical to how you operate in   classification as well in that domain you 
try to minimize the cross-entropy loss   which is also a negative log likelihood 
optim or probability function   so keep in mind here that this is composed of 
the convolutional layers to actually perform this   feature extraction these are exactly the same 
as what we learned about in this lecture today   as well as these flattening pooling layers and 
concatenation layers to really produce this single   representation and feature vector of our inputs 
and finally it predicts these outputs in this case   a continuous representation of control that this 
vehicle should take so this is really powerful   because a human can actually enter the car input 
a desired destination and the end to end cnn will   output the control commands to actuate the vehicle 
towards that destination note here that the   vehicle is able to successfully recognize when it 
approaches the intersections and take the correct   control commands to actually navigate that 
vehicle through these brand new environments   that it has never seen before and never 
driven before in its training data set   and the impact of cnns has been very wide 
reaching beyond these examples as well that   i've explained here today it has touched so many 
different fields in computer vision especially   and i'd like to really conclude this lecture 
today by taking a look at what we've covered we've   really covered a ton of material today we covered 
the foundations of computer vision how images are   represented as an array of brightness values and 
how we can use convolutions and how they work   we saw that we can build up these convolutions 
into the basic architecture defining convolutional   neural networks and discussed how 
cnns can be used for classification   finally we talked about a lot of the extensions 
and applications of how you can use these basic   convolutional neural network architectures as 
a feature extraction module and then use this   to perform your task at hand and a bit about 
how we can actually visualize the behavior   of our neural network and actually understand a 
bit about what it's doing under the hood through   ways of some of these semantic segmentation maps 
and really getting a more fine-grained perspective   of the the very high resolution classification 
of these input images that it's seeing   and with that i would like to conclude this 
lecture and point everyone to the next lab   that will be upcoming today this will be a 
lab specifically focused on computer vision   you'll get very familiar with a lot of the 
algorithms that we've been talking about today   starting with building your first convolutional 
neural networks and then building this up to build   some facial detection systems and learn how we 
can use unsupervised generative models like we're   going to see in the next lecture to actually 
make sure that these computer vision facial   classification algorithms are fair and unbiased 
so stay tuned for the next lecture as well on   unsupervised generative modeling to get more 
details on how to do a second part thank you 

Hi everyone and welcome to lecture 4 of MIT 
6.S191! In today's lecture we're going to be   talking about how we can use deep learning and 
neural networks to build systems that not only   look for patterns in data but actually can go a 
step beyond this to generate brand new synthetic   examples based on those learned patterns and 
this i think is an incredibly powerful idea   and it's a particular subfield of deep 
learning that has enjoyed a lot of success and   and gotten a lot of interest in the past couple 
of years but i think there's still tremendous   tremendous potential of this field of degenerative 
modeling in the future and in the years to come   particularly as we see these types of models and 
the types of problems that they tackle becoming   more and more relevant in a variety of application 
areas all right so to get started i'd like to   consider a quick question for each of you here we 
have three photos of faces and i want you all to   take a moment look at these faces study them and 
think about which of these faces you think is real   is it the face on the left is it the face in the 
center is it the face on the right which of these   is real well in truth each of these faces are 
not real they're all fake these are all images   that were synthetically generated by a deep 
neural network none of these people actually   exist in the real world and hopefully i think you 
all have appreciated the realism of each of these   synthetic images and this to me highlights the 
incredible power of deep generative modeling and   not only does it highlight the power of these 
types of algorithms and these types of models   but it raises a lot of questions about how we can 
consider the fair use and the ethical use of such   algorithms as they are being deployed in the 
real world so by setting this up and motivating   in this way i first i now like to take a step 
back and consider fundamentally what is the type   of learning that can occur when we are training 
neural networks to perform tasks such as these so   so far in this course we've been considering 
what we call supervised learning problems   instances in which we are given a set of data and 
a set of labels associated with that data and we   our goal is to learn a functional mapping that 
moves from data to labels and those labels can   be class labels or continuous values and in 
this course we've been concerned primarily with   developing these functional mappings that 
can be described by deep neural networks   but at their core these mappings could be 
anything you know any sort of statistical function   the topic of today's lecture is going to focus on 
what we call unsupervised learning which is a new   class of learning problems and in contrast to 
supervised settings where we're given data and   labels in unsupervised learning we're given only 
data no labels and our goal is to train a machine   learning or deep learning model to understand 
or build up a representation of the hidden and   underlying structure in that data and what this 
can do is it can allow sort of an insight into the   foundational structure of the data and then in 
turn we can use this understanding to actually   generate synthetic examples and unsupervised 
learning beyond this domain of deep generative   modeling also extends to other types of problems 
and example applications which you may be   familiar with such as clustering algorithms or 
dimensionality reduction algorithms generative   modeling is one example of unsupervised learning 
and our goal in this case is to take as input   examples from a training set and learn a model 
that represents the distribution of the data that   is input to that model and this can be achieved 
in two principle ways the first is through what   is called density estimation where let's say we 
are given a set of data samples and they fall   according to some density the task for building 
a deep generative model applied to these samples   is to learn the underlying probability density 
function that describes how and where these data   fall along this distribution and we can not only 
just estimate the density of such a probability   density function but actually use this information 
to generate new synthetic samples where again   we are considering some input examples that fall 
and are drawn from some training data distribution   and after building up a model using that data 
our goal is now to generate synthetic examples   that can be described as falling within 
the data distribution modeled by our model   so the key the key idea in both these 
instances is this question of how can we learn   a probability distribution using our 
model which we call p model of x that is   so similar to the true data distribution 
which we call p data of x this will not only   enable us to effectively estimate these 
probability density functions but also generate   new synthetic samples that are realistic and match 
the distribution of the data we're considering   so this this i think summarizes concretely what 
are the key principles behind generative modeling   but to understand how generative modeling may 
be informative and also impactful let's take   this this idea a step further and consider what 
could be potential impactful applications and   real world use cases of generative modeling what 
generative models enable us as the users to do is   to automatically uncover the underlying structure 
and features in a data set the reason this can be   really important and really powerful is often we 
do not know how those features are distributed   within a particular data set of interest so 
let's say we're trying to build up a facial   detection classifier and we're given a data set 
of faces which for which we may not know the exact   distribution of these faces with respect to key 
features like skin tone or pose or clothing items   and without going through our data set and 
manually ex inspecting each of these instances   our training data may actually be very biased with 
respect to some of these features without us even   knowing it and as you'll see in this lecture and 
in today's lab what we can actually do is train   generative models that can automatically learn 
the landscape of the features in a data set like   these like that of faces and by doing so actually 
uncover the regions of the training distribution   that are underrepresented and over represented 
with respect to particular features such as skin   tone and the reason why this is so powerful is we 
can actually now use this information to actually   adjust how the data is sampled during training 
to ultimately build up a more fair and more   representative data set that then will 
lead to a more fair and unbiased model   and you'll get practice doing exactly this and 
implementing this idea in today's lab exercise   another great example in use case where generative 
models are exceptionally powerful is this broad   class of problems that can be considered outlier 
or anomaly detection one example is in the case of   self-driving cars where it's going to be really 
critical to ensure that an autonomous vehicle   governed and operated by a deep neural network 
is able to handle all all of the cases that it   may encounter on the road not just you know the 
straight freeway driving that is going to be the   majority of the training data and the majority 
of the time the car experiences on the road so   generative models can actually be used to detect 
outliers within training distributions and use   this to again improve the training process so 
that the resulting model can be better equipped to   handle these edge cases and rare events all right 
so hopefully that motivates why and how generative   models can be exceptionally powerful and useful 
for a variety of real world applications to dive   into the bulk of the technical content for today's 
lecture we're going to discuss two classes of what   we call latent variable models specifically we'll 
look at autoencoders and generative adversarial   networks organs but before we get into that i'd 
like to first begin by discussing why these are   called latent variable models and what we 
actually mean when we use this word latent   and to do so i think really the best example that 
i've personally come across for understanding what   a latent variable is is this story that is from 
plato's work the republic and this story is called   the myth of the cave or the parable of the cave 
and the story is as follows in this myth there   are a group of prisoners and these prisoners are 
constrained as part of their prison punishment to   face a wall and the only things that they can 
see on this wall are the shadows of particular   objects that are being passed in front of a fire 
that's behind them so behind their heads and out   of their line of sight and the prisoners the only 
thing they're really observing are these shadows   on the wall and so to them that's what they can 
see that's what they can measure and that's what   they can give names to that's really their reality 
these are their observed variables but they can't   actually directly observe or measure the physical 
objects themselves that are actually casting these   shadows so those objects are effectively 
what we can analyze like latent variables   they're the variables that are not directly 
observable but they're the true explanatory   factors that are creating the observable 
variables which in this case the prisoners   are seeing like the shadows cast on the wall 
and so our question in in generative modeling   broadly is to find ways of actually learning these 
underlying and hidden latent variables in the data   even when we're only given the observations that 
are made and it's this is an extremely extremely   complex problem that is very well suited to 
learning by neural networks because of their power   to handle multi-dimensional data sets and to learn 
combinations of non-linear functions that can   approximate really complex data distributions all 
right so we'll first begin by discussing a simple   and foundational generative model which tries 
to build up this latent variable representation   by actually self-encoding the input and 
these models are known as auto encoders   what an auto encoder is is it's an approach 
for learning a lower dimensional latent space   from raw data to understand how it works 
what we do is we feed in as input raw data   for example this image of a two that's going to be 
passed through many successive deep neural network   layers and at the output of that succession 
of neural network layers what we're going to   generate is a low dimensional latent space a 
feature representation and that's really the   goal that we're we're trying to predict and so we 
can call this portion of the network an encoder   since it's mapping the data x into a encoded 
vector of latent variables z so let's consider   this this latent space z if you've noticed 
i've represented z as having a smaller size   a smaller dimensionality as the input x why would 
it be important to ensure the low dimensionality   of this latent space z having a low dimensional 
latent space means that we are able to compress   the data which in the case of image data can be 
you know on the order of many many many dimensions   we can compress the data into a small latent 
vector where we can learn a very compact and rich   feature representation so how can we actually 
train this model are we going to have are we going   to be able to supervise for the particular 
latent variables that we're interested in   well remember that this is an unsupervised problem 
where we have training data but no labels for   the latent space z so in order to actually train 
such a model what we can do is learn a decoder   network and build up a decoder network that is 
used to actually reconstruct the original image   starting from this lower dimensional latent 
space and again this decoder portion of our auto   encoder network is going to be a series of layers 
neural network layers like convolutional layers   that's going to then take this hidden latent 
vector and map it back up to the input space   and we call our reconstructed output x hat 
because it's our prediction and it's an imperfect   reconstruction of our input x and the way that we 
can actually train this network is by looking at   the original input x and our reconstructed output 
x hat and simply comparing the two and minimizing   the distance between these two images so for 
example we could consider the mean squared error   which in the case of images means effectively 
subtracting one image from another and squaring   the difference right which is effectively 
the pixel wise difference between the input   and reconstruction measuring how faithful our 
reconstruction is to the original input and again   notice that by using this reconstruction loss 
this difference between the reconstructed output   and our original input we do not require any 
labels for our data beyond the data itself right   so we can simplify this diagram just a little bit 
by abstracting away these individual layers in the   encoder and decoder components and again note once 
again that this loss function does not require any   labels it is just using the raw data to supervise 
itself on the output and this is a truly powerful   idea and a transformative idea because it enables 
the model to learn a quantity the latent variables   z that we're fundamentally interested in but we 
cannot simply observe or cannot readily model   and when we constrain this this latent space to 
a lower dimensionality that affects the degree   to which and the faithfulness to which 
we can actually reconstruct the input   and what this you the way you 
can think of this is as imposing   a sort of information bottleneck during 
the model's training and learning process   and effectively what this bottleneck does is it's 
a form of compression right we're taking the input   data compressing it down to a much smaller latent 
space and then building back up a reconstruction   and in practice what this results in is that the 
lower the dimensionality of your latent space   the poorer and worse quality reconstruction 
you're going to get out all right so in summary   these autoencoder structures use this sort of 
bottlenecking hidden layer to learn a compressed   latent representation of the data and we can 
self-supervise the training of this network   by using what we call a reconstruction loss 
that forces the forces the autoencoder network   to encode as much information about the data as 
possible into a lower dimensional latent space   while still being able to build up faithful 
reconstructions so the way i like to think of   this is automatically encoding information from 
the data into a lower dimensional latent space let's now expand upon this idea a bit more 
and introduce this concept and architecture of   variational auto encoders or vaes so as 
we just saw traditional auto encoders   go from input to reconstructed output and if 
we pay closer attention to this latent layer   denoted here in orange what you can hopefully 
realize is that this is just a normal layer   in a neural network just like any other layer 
it's deterministic if you're going to feed in   a particular input to this network you're going 
to get the same output so long as the weights are   the same so effectively a traditional auto 
encoder learns this deterministic encoding   which allows for reconstruction and 
reproduction of the input in contrast   variational auto encoders impose a stochastic 
or variational twist on this architecture   and the idea behind doing so is to generate 
smoother representations of the input data   and improve the quality of the of not only of 
reconstructions but also to actually generate   new images that are similar to the input data set 
but not direct reconstructions of the input data   and the way this is achieved is 
that variational autoencoders   replace that deterministic layer z 
with a stochastic sampling operation   what this means is that instead of learning the 
latent variables z directly for each variable   the variational autoencoder learns a mean and 
a variance associated with that latent variable   and what those means and variances do is that 
they parameterize a probability distribution for   that latent variable so what we've done in going 
from an autoencoder to a variational autoencoder   is going from a vector of latent variable z 
to learning a vector of means mu and a vector   of variances sigma sigma squared 
that parametrize these variables   and define probability distributions for each of 
our latent variables and the way we can actually   generate new data instances is by sampling 
from the distribution defined by these muse   and sigmas to to generate a latent sample and get 
probabilistic representations of the latent space   and what i'd like you to appreciate about this 
network architecture is that it's very similar to   the autoencoder i previously introduced just that 
we have this probabilistic twist where we're now   performing the sampling operation to compute 
samples from each of the latent variables   all right so now because we've introduced this 
sampling operation this stochasticity into our   model what this means for the actual computation 
and learning process of the network the encoder   and decoder is that they're now probabilistic in 
their nature and the way you can think of this is   that our encoder is going to be trying to learn 
a probability distribution of the latent space   z given the input data x while the decoder is 
going to take that learned latent representation   and compute a new probability distribution of the 
input x given that latent distribution z and these   networks the encoder the decoder are going to be 
defined by separate sets of weights phi and theta   and the way that we can train 
this variational autoencoder   is by defining a loss function that's going to be 
a function of the data x as well as these sets of   weights phi and theta and what's key to how vaes 
can be optimized is that this loss function is now   comprised of two terms instead of just one we 
have the reconstruction loss just as before which   again is going to capture the this difference 
between the input and the reconstructed output   and also a new term to our loss which we call 
the regularization loss also called the vae loss   and to take a look in more 
detail at what each of these   loss terms represents let's first emphasize again 
that our overall loss function is going to be   defined and uh taken with respect to the sets of 
weights of the encoder and decoder and the input x   the reconstruction loss is very similar to before 
right and you can think of it as being driven by   log likelihood a log likelihood 
function for example for image   data the mean squared error between the input 
and the output and we can self-supervise the   reconstruction loss just as before to force 
the latent space to learn and represent   faithful representations of the input data 
ultimately resulting in faithful reconstructions   the new term here the regularization term 
is a bit more interesting and completely   new at this stage so we're going to dive in 
and discuss it further in a bit more detail   so our probability distribution that's going 
to be computed by our encoder q phi of z of x   is a distribution on the latent space z given 
the data x and what regularization enforces   is that as a part of this learning process we're 
going to place a prior on the latent space z   which is effectively some initial hypothesis about 
what we expect the distributions of z to actually   look like and by imposing this regularization 
term what we can achieve is that the model will   try to enforce the zs that it learns 
to follow this prior distribution and   we're going to denote this prior as p of z 
this term here d is the regularization term   and what it's going to do is it's going to try 
to enforce a minimization of the divergence or   the difference between what the encoder is trying 
to infer the probability distribution of z given x   and that prior that we're going to place on 
the latent variables p of z and the idea here   is that by imposing this regularization factor we 
can try to keep the network from overfitting on   certain parts of the latent space by enforcing 
the fact that we want to encourage the latent   variables to adopt a distribution that's similar 
to our prior so we're going to go through now   you know both the mathematical basis for this 
regularization term as well as a really intuitive   walk through of what regularization 
achieves to help give you a concrete   understanding and an intuitive understanding 
about why regularization is important and why   placing a prior is important so let's first 
consider um yeah so to re-emphasize once   again this this regularization term 
is going to consider the divergence   between our inferred latent distribution 
and the fixed prior we're going to place   so before we to to get into this let's consider 
what could be a good choice of prior for each   of these latent uh variables how do we select 
p of z i'll first tell you what's commonly done   the common choice that's used very extensively in 
the community is to enforce the latent variables   to roughly follow normal gaussian distributions 
which means that they're going to be a normal   distribution centered around mean 0 and have a 
standard deviation and variance of 1. by placing   these normal gaussian priors on each of the latent 
variables and therefore on our latent distribution   overall what this encourages is that the learned 
encodings learned by the encoder portion of our   vae are going to be sort of distributed evenly 
around the center of each of the latent variables   and if you can imagine in picture when you have 
sort of a roughly even distribution around the   center of a particular region of the latent space 
what this means is that outside of this region   far away there's going to be a greater penalty 
and this can result in instances from instances   where the network is trying to cheat and 
try to cluster particular points outside   the center these centers in the latent space 
like if it was trying to memorize particular   outliers or edge cases in the data after we place 
a normal gaussian prior on our latent variables   we can now begin to concretely define the 
regularization term component of our loss function   this loss this term to the loss is very similar 
in principle to a cross-entropy loss that we saw   before where the key is that we're going to be 
defining the distance function that describes the   difference or the or the divergence between the 
inferred latent distribution q phi of z given x   and the prior that we're going to be placing p of 
z and this term is called the kublac libor or kl   divergence and when we choose a normal gaussian 
prior we res this results in the kl divergence   taking this particular form of this equation 
here where we're using the means and sigmas   as input and computing this distance metric 
that captures the divergence of that learned   latent variable distribution from the normal 
gaussian all right so now i really want to   spend a bit of time to get some build up 
some intuition about how this regularization   and works and why we actually want to regularize 
our vae and then also why we select a normal prior   all right so to do this let's let's 
consider the following question   what properties do we want this 
to achieve from regularization   why are we actually regularizing our our network 
in the first place the first key property   that we want for a generative model like a vae is 
what i can what i like to think of as continuity   which means that if there are points that 
are represented closely in the latent space   they should also result in similar reconstructions 
similar outputs similar content after they are   decoded you would expect intuitively that regions 
in the latent space have some notion of distance   or similarity to each other and this indeed is 
a really key property that we want to achieve   with our generative model the second property is 
completeness and it's very related to continuity   and what this means is that when we sample from 
the latent space to decode the latent space   into an output that should result in a meaningful 
reconstruction and meaningful sampled content   that is you know resembling 
the original data distribution   you can imagine that if we're sampling from the 
latent space and just getting garbage out that   has no relationship to our input this could be 
a huge huge problem for our model all right so   with these two properties in mind continuity and 
completeness let's consider the consequences of   what can occur if we do not regularize our model 
well without regularization what could end up   happening with respect to these two properties 
is that there could be instances of points that   are close in latent space but not similarly 
decoded so i'm using this really intuitive in   illustration where these dots represent abstracted 
away sort of regions in the latent space   and the shapes that they relate to you can think 
of as what is going to be decoded after those   uh instances in the latent space are passed 
through the decoder so in this example we have   these two dots the greenish dot and the reddish 
dot that are physically close in latent space but   result in completely different shapes when they're 
decoded we also have an instance of this purple   point which when it's decoded it doesn't result 
in a meaningful content it's just a scribble so   by not regularizing and i'm abstracting a 
lot away here and that's on purpose we could   have these instances where we don't have 
continuity and we don't have completeness   therefore our goal with regularization is to be 
able to realize a model where points that are   close in the latent space are not only similarly 
decoded but also meaningfully decoded so for   example here we have the red dot and the orange 
dot which result in both triangle like shapes but   with slight variations on the on the triangle 
itself so this is the intuition about what   regularization can enable us to achieve and what 
are desired properties for these generative models   okay how can we actually achieve this 
regularization and how does the normal prior   fit in as i mentioned right vaes they don't just 
learn the latent variable z directly they're   trying to encode the inputs as distributions 
that are defined by mean and variance so my first   question to you is is it going to be sufficient 
to just learn mean and variance learn these   distributions can that guarantee continuity 
and completeness no and let's understand why   all right without any sort of regularization 
what could the model try to resort to   remember that the vae or or that the vae the loss 
function is defined by both a reconstruction term   and a regularization term if there is no 
regularization you can bet that the model is going   to just try to optimize that reconstruction term 
so it's effectively going to learn to minimize the   reconstruction loss even though we're encoding 
the latent variables via mean and variance   and two instances two consequences of that 
is that you can have instances where these   learned variances for the latent variable 
end up being very very very small   effectively resulting in pointed distributions and 
you can also have means that are totally divergent   from each other which result in discontinuities 
in the latent space and this can occur while   still trying to optimize that reconstruction 
loss direct consequence of not regularizing by in order to overcome these pro these problems 
we need to regularize the variance and the mean   of these distributions that are being returned 
by the encoder and the normal prior placing that   normal gaussian distribution as our prior helps 
us achieve this and to understand why exactly this   occurs is that effectively the normal prior is 
going to encourage these learned latent variable   distributions to overlap in latent space recall 
right mean zero variance of one that means all the   all the latent variables are going to be enforced 
to try to have the same mean a centered mean and   all their variances are going to be regularized 
for each and every of the latent variable   distributions and so this will ensure a smoothness 
and a regularity and an overlap in the lane space   which will be very effective in helping us achieve 
these properties of continuity and completeness   centering the means regularizing the variances   so the regularization via this normal prior 
by centering each of these latent variables   regularizing their their variances is that 
it helps enforce this continuous and complete   gradient of information represented 
in the latent space where again points   and distances in the latent space have 
some relationship to the reconstructions   and the content of the reconstructions that result 
note though that there's going to be a trade-off   between regularizing and reconstructing the more 
we regularize there's also a risk of suffering the   quality of the reconstruction and the generation 
process itself so in optimizing gaze there's going   to be this trade-off that's going to try 
to be tuned to fit the problem of interest   all right so hopefully by walking through this 
this example and considering these points you've   built up more intuition about why regularization 
is important and how specifically the normal prior   can help us regularize great so now we've defined 
our loss function we know that we can reconstruct   the inputs we've understood how we can regularize 
learning and achieve continuity and completeness   by this normal prior these are all the components 
that define a forward pass through the network   going from input to encoding 
to decoded reconstruction   but we're still missing a critical step 
in putting the whole picture together   and that's of back propagation and the key 
here is that because of this fact that we've   introduced this stochastic sampling layer we 
now have a problem where we can't back propagate   gradients through a sampling layer 
that has this element of stochasticity   backpropagation requires deterministic nodes 
deterministic layers for which we can iteratively   apply the chain rule to optimize gradients 
optimize the loss via gradient descent all right   vaes introduced sort of a breakthrough idea 
that solved this issue of not being able to   back propagate through a sampling layer 
and the key idea was to actually subtly   re-parametrize the sampling operation such that 
the network could be trained completely end-to-end   so as we as we already learned right 
we're trying to build up this latent   distribution defined by these variables 
z uh defining placing a normal prior   defined by a mean and a variance and we can't 
simply back propagate gradients through the   sampling layer because we can't compute 
gradients through this stochastic sample   the key idea instead is to try to consider the 
sampled latent vector z as a sum defined by a   fixed mu a fixed sigma vector and scale that sigma 
vector by random constants that are going to be   drawn from a prior distribution such as a normal 
gaussian and by reparameterizing the sampling   operation as as so we still have this element of 
stochasticity but that stochasticity is introduced   by this random constant epsilon which 
is not occurring within the bottleneck   latent layer itself we've reparametrized and 
distributed it elsewhere to visualize how this   looks let's consider the following where 
originally in the original form of the vae we had this deterministic nodes which are 
the weights of the network as well as an input   vector and we are trying to back propagate 
through the stochastic sampling node z   but we can't do that so now by 
re-parametrization what we've   we've achieved is the following form where our 
latent variable z are defined with respect to   uh mu sigma squared as well as these noise 
factor epsilon such that when we want to do   back propagation through the network to update we 
can directly back propagate through z defined by   mu and sigma squared because this epsilon value 
is just taken as a constant it's re-parametrized   elsewhere and this is a very very powerful 
trick the re-parametrization trick because   it enables us to train variational auto encoders 
and to end by back propagating with respect to z   and with respect to the actual gradient the 
actual weights of the encoder network all right   one side effect and one consequence of imposing 
these distributional priors on the latent variable   is that we can actually sample from these latent 
variables and individually tune them while keeping   all of the other variables fixed and what 
you can do is you can tune the value of   a particular latent variable and run the 
decoder each time that variable is changed   each time that variable is perturbed to 
generate a new reconstructed output so an   example of that result is is in the following 
where this perturbation of the latent variables   results in a representation that has some semantic 
meaning about what the network is maybe learning   so in this example these images 
show variation in head pose   and the different dimensions of z the latent 
space the different latent variables are in   this way encoding different latent features 
that can be interpreted by keeping all other   variables fixed and perturbing the 
value of one individual lane variable   ideally in order to optimize vas and try to 
maximize the information that they encode we want   these latent variables to be uncorrelated with 
each other effectively disentangled and what that   could enable us to achieve is to learn the richest 
and most compact latent representation possible   so in this case we have head pose on the x-axis 
and smile on the y-axis and we want these to be   as uncorrelated with each other as possible one 
way we can achieve this that's been shown to   achieve this disentanglement is rather a quite 
straightforward approach called beta vaes so if   we consider the loss of a standard vae again we 
have this reconstruction term defined by a log   likelihood and a regularization term defined 
by the kl divergence beta vaes introduce a new   hyperparameter beta which controls the strength 
of this regularization term and it's been shown   mathematically that by increasing beta the effect 
is to place constraints on the latent encoding   such as to encourage disentanglement and there 
have been extensive proofs and discussions   as to how exactly this is achieved but to 
consider the results let's again consider   the problem of face reconstruction where using a 
standard vae if we consider the latent variable of   head pose or rotation in this case where beta 
equals one what you can hopefully appreciate   is that as the face pose is changing the 
smile of some of these faces is also changing   in contrast by en enforcing a beta much larger 
than one what is able to be achieved is that   the smile remains relatively constant while 
we can perturb the single latent variable   of the head of rotation and achieve 
perturbations with respect to head rotation alone   all right so as i motivated and introduced in the 
beginning in the introduction of this lecture one   powerful application of generative models and 
latent variable models is in model d biasing   and in today's lab you're actually going to get 
real hands-on experience in building a variational   auto encoder that can be used to achieve automatic 
de-biasing of facial classification systems facial   detection systems and the power and the idea of 
this approach is to build up a representation   a learned latent distribution of face data and 
use this to identify regions of that latent space   that are going to be over-represented or 
under-represented and that's going to all be   taken with respect to particular learned features 
such as skin tone pose objects clothing and then   from these learned distributions we can actually 
adjust the training process such that we can   place greater weight and greater sampling on those 
images and on those faces that fall in the regions   of the latent space that are under represented 
automatically and what's really really cool   about deploying a vae or a latent variable model 
for an application like model d biasing is that   there's no need for us to annotate and prescribe 
the features that are important to actually devise   against the model learns them automatically and 
this is going to be the topic of today's lab   and it's also raises the opens the door to a much 
broader space that's going to be explored further   in a later spotlight lecture that's going to focus 
on algorithmic bias and machine learning fairness   all right so to summarize the key points on vaes 
they compress representation of data into an   encoded representation reconstruction of the data 
input allows for unsupervised learning without   labels we can use the reparameterization trick 
to train vas end to end we can take hidden latent   variables perturb them to interpret their content 
and their meaning and finally we can sample   from the latent space to generate new examples 
but what if we wanted to focus on generating   samples and synthetic samples that were as 
faithful to a data distribution generally as   possible to understand how we can achieve 
this we're going to transition to discuss   a new type of generative model called a 
generative adversarial network or gam for short   the idea here is that we don't want 
to explicitly model the density or the   or the distribution underlying some data but 
instead just learn a representation that can   be successful in generating new instances that 
are similar to the data which means that we   want to optimize to sample from a very very 
complex distribution which cannot be learned and   modeled directly instead we're going to have to 
build up some approximation of this distribution   and the really cool and and breakthrough idea 
of gans is to start from something extremely   extremely simple just random noise and try 
to build a neural network a generative neural   network that can learn a functional transformation 
that goes from noise to the data distribution   and by learning this functional generative mapping 
we can then sample in order to generate fake   instances synthetic instances that are going 
to be as close to the real data distribution   as possible the breakthrough to achieving this was 
this structure called gans where the key component   is to have two neural networks a generator network 
and a discriminator network that are effectively   competing against each other they're adverse areas 
specifically we have a generator network which i'm   going to denote here on out by g that's going to 
be trained to go from random noise to produce an   imitation of the data and then the discriminator 
is going to take that synthetic fake data as   well as real data and be trained to actually 
distinguish between fake and real and in training   these two networks are going to be competing each 
other competing against each other and so in doing   so overall the effect is that the discriminator is 
going to get better and better at learning how to   classify real and fake and the better it becomes 
at doing that it's going to force the generator to   try to produce better and better synthetic data to 
try to fool the discriminator back and forth back   and forth so let's now break this down and go from 
a very simple toy example to get more intuition   about how these gans work the generator is 
going to start again from some completely   random noise and produce fake data and i'm 
going to show that here by representing these   data as points on a one-dimensional line the 
discriminator is then going to see these points   as well as real data and then it's going 
to be trained to output a probability that   the data it sees are real or if they are fake and 
in the beginning it's not going to be trained very   well right so its predictions are not going to 
be very good but then you're going to train it   and you're going to train it and it's going 
to start increasing the profit probabilities   of real versus not real appropriately such 
that you get this perfect separation where   the discriminator is able to perfectly distinguish 
what is real and what is fake now it's back to the   generator and the generator is going to come back 
it's going to take instances of where the real   data lie as inputs to train and then it's going to 
try to improve its imitation of the data trying to   move the fake data the synthetic data that is 
generated closer and closer to the real data   and once again the discriminator is 
now going to receive these new points   and it's going to estimate a probability 
that each of these points is real   and again learn to decrease the 
probability of the fake points being real   further and further and now we're going to repeat 
again and one last time the generator is going to   start moving these fake points closer and 
closer to the real data such that the fake   data are almost following the distribution of the 
real data at this point it's going to be really   really hard for the discriminator to effectively 
distinguish between what is real and what is fake   while the generator is going to continue 
to try to create fake data instances to   fool the discriminator and this is really the key 
intuition behind how these two components of gans   are essentially competing with each other 
all right so to summarize how we train gowns   the generator is going to try to synthesize fake 
instances to full discriminator which is going   to be trained to identify the synthesized 
instances and discriminate these as fake   to actually train we're going to see that we 
are going to define a loss function that defines   competing and adversarial objectives for each of 
the discriminator and the generator and a global   optimum the best we could possibly do would mean 
that the generator could perfectly reproduce the   true data distribution such that the discriminator 
absolutely cannot tell what's synthetic   versus what's real so let's go through how 
the loss function for again breaks down   the the loss term for again is based 
on that familiar cross-entropy loss and   it's going to now be defined between 
the true and generated distributions   so we're first going to consider the loss 
from the perspective of the discriminator   we want to try to maximize the probability 
that the fake data is identified as fake   and so to break this down here g of z 
defines the generator's output and so d   of g of z is the discriminator's estimate of the 
probability that a fake instance is actually fake   d of x is the discriminator's estimate of 
the probability that a real instance is fake   so one minus d of x is its probability 
estimate that a real instance is real   so together from the point of view of the 
discriminator we want to maximize this probability   maximize probability fake is fake maximize 
the estimate of probability really is real   now let's turn our attention to the 
generator remember that the generator   is taking random noise and generating an instance 
it cannot directly affect the term d of x   which shows up in the loss right because d of x is 
solely based on the discriminator's operation on   the real data so for the generator the generator 
is going to have the adversarial objective   to the discriminator which means is going to 
try to minimize this term effectively minimizing   the probability that the discriminator can 
distinguish its generated data as as uh fake   d of g of z and the goal for the generator 
is to minimize this term of the objective so the objective of the generator is to try 
to synthesize fake instances that fool the   discriminator and eventually over the course 
of training the discriminator the discriminator   is going to be as best as it possibly can be 
discriminating real versus fake therefore the   ultimate goal of the generator is to synthesize 
fake instances that fool the best discriminator   and this is all put together in this min max 
objective function which has these two components   optimized adversarially and then after training 
we can actually use the generator network which   is now fully trained to produce new data instances 
that have never been seen before so we're going   to focus on that now and what is really cool is 
that when the train generator of a gam synthesizes   new instances it's effectively learning a 
transformation from a distribution of noise to a   target data distribution and that transformation 
that mapping is going to be what's learned over   the course of training so if we consider one point 
from a latent noise distribution it's going to   result in a particular output in the target 
data space and if we consider another point   of random noise feed it through the generator it's 
going to result in a new instance that and that   new instance is going to fall somewhere else on 
the data manifold and indeed what we can actually   do is interpolate and trans and traverse in the 
space of gaussian noise to result in interpolation   in the target space and you can see an example of 
this result here where a transformation in series   reflects a traversal across the alert the 
target data manifold and that's produced in   the synthetic examples that are outputted by the 
generator all right so in the final few minutes of   this lecture i'm going to highlight some of the 
recent advances in gans and hopefully motivate   even further why this approach is so powerful so 
one idea that's been extremely extremely powerful   is this idea of progressive gans progressive 
growing which means that we can iteratively build   more detail into the generated instances that are 
produced and this is done by progressively adding   layers of increasing spatial resolution in 
the case of image data and by incrementally   building up both the generator and discriminator 
networks in this way as training progresses it   results in very well resolved synthetic images 
that are output ultimately by the generator   so some results of this idea of progressive a 
progressive gan are displayed here another idea   that has also led to tremendous improvement in the 
quality of synthetic examples generated by gans   is a architecture improvement called stylegan 
which combines this idea of progressive growing   that i introduced earlier with a principles of 
style transfer which means trying to compose an   image in the style of another image so for 
example what we can now achieve is to map   input images source a using application of coarse 
grained styles from secondary sources onto those   targets to generate new instances that mimic the 
style of of source b and that's that result is   shown here and hopefully you can appreciate that 
these coarse-grained features these coarse-grained   styles like age facial structure things like that 
can be reflected in these synthetic examples this   same style gan system has led to tremendously 
realistic synthetic images in the areas of both   face synthesis as well as for animals other 
objects as well another extension to the gan   architecture that's has enabled particularly 
powerful applications for select problems and   tasks is this idea of conditioning which imposes 
a bit of additional further structure on the types   of outputs that can be synthesized by again so the 
idea here is to condition on a particular label   by supplying what is called a conditioning 
factor denoted here as c and what this allows   us to achieve is instances like that of 
paired translation in the case of image   synthesis where now instead of a single input 
as training data for our generator we have pairs   of inputs so for example here we consider both a 
driving scene and a corresponding segmentation map   to that driving scene and the discriminator can 
in turn be trained to classify fake and real pairs   of data and again the generator is going to be 
learned to going to be trained to try to fool the   discriminator example applications of this idea 
are seen as follows where we can now go from a   input of a semantic segmentation map to generate 
a synthetic street scene mapping that mapping um   according to that segmentation or we can go 
from an aerial view from a satellite image   to a street map view or from particular labels 
of an architectural building to a synthetic   architectural facade or day to night black 
and white to color edges to photos different   instances of paired translation that are 
achieved by conditioning on particular labels   so another example which i think is really 
cool and interesting is translating from   google street view to a satellite view and vice 
versa and we can also achieve this dynamically   so for example in coloring given an edge input the 
network can be trained to actually synthetically   color in the artwork that is resulting from 
this particular edge sketch another idea   instead of pair translation is that of unpaired 
image to image translation and this is can be   achieved by a network architecture called cyclegan 
where the model is taking as input images from   one domain and is able to learn a mapping that 
translates to another domain without having a   paired corresponding image in that other domain 
so the idea here is to transfer the style and   the the distribution from one domain to another 
and this is achieved by introducing the cyclic   relationship in a cyclic loss function where 
we can go back and forth between a domain x   and a domain y and in this system there are 
actually two generators and two discriminators   that are going to be trained on their 
respective generation and discrimination tasks   in this example the cyclogan has been trained to 
try to translate from the domain of horses to the   domain of zebras and hopefully you can appreciate 
that in this example there's a transformation of   the skin of the horse from brown to a zebra-like 
skin in stripes and beyond this there's also a   transformation of the surrounding area from 
green grass to something that's more brown   in the case of the zebra i think to get 
an intuition about how this cyclogan   transformation is going is working let's 
go back to the idea that conventional gans   are moving from a distribution of gaussian noise 
to some target data manifold with cycle gans the   goal is to go from a particular data manifold x 
to another data manifold why and in both cases and   i think the underlying concept that makes gans 
so powerful is that they function as very very   effective distribution transform transformers and 
it can achieve these distribution transformations finally i'd like to consider one additional 
application that you may be familiar with   of using cycle gans and that's to transform 
speech and to actually use this psychogam   technique to synthesize speech in someone 
else's voice and the way this is done is   by taking a bunch of audio recordings in one 
voice and audio recordings in another voice and   converting those audio waveforms into an image 
spec representation which is called a spectrogram   we can then uh train the cycle gan to operate 
on these spectrogram images to transform   representations from voice a to make like to 
make them appear like they appear that they   are from another voice voice be and 
this is exactly how we did the speech   transformation for the synthesis of obama's voice 
in the demonstration that alexander gave in the   first lecture so to inspect this further let's 
compare side by side the original audio from   alexander as well as the synthesized version in 
obama's voice that was generated using a cyclegan   hi everybody and welcome to mit 6s191 the official 
introductory course on deep learning taught here   at mit so notice that the spectrogram that results 
for obama's voice is actually generated by an   operation on alexander's voice and effectively 
learning a domain transformation from obama domain   onto the domain of alexander domain and the end 
result is that we create and synthesize something   that's more obama-like all right so to summarize 
hopefully over the course of this lecture you   built up understanding of generative modeling and 
classes of generative models that are particularly   powerful in enabling probabilistic density 
estimation as well as sample generation   and with that i'd like to close the lecture and 
introduce you to the remainder of of today's   course which is going to focus on our second 
lab on computer vision specifically exploring   this question of de-biasing in facial 
detection systems and using variational   auto encoders to actually achieve an approach for 
automatic de-biasing of classification systems   so i encourage you to come to the class 
gather town to have your questions   on the labs answered and to discuss 
further with any of us thank you 

Hi everyone and welcome back to 6.S191! Today is 
a really exciting day because we'll learn about   how we can marry the very long-standing field 
of reinforcement learning with a lot of the very   recent advancements that we've been seeing so 
far in this class in deep learning and how we   can combine these two fields to build some really 
extraordinary applications and really agents that   can outperform or achieve super human performance 
now i think this field is particularly amazing   because it moves away from this paradigm that 
we've been really constrained to so far in this   class so so far deep learning the way we've seen 
it has been really confined to fixed data sets the   way we kind of either collect or have can obtain 
online for example in reinforcement learning   though deep learning is placed in some environment 
and is actually able to explore and interact with   that environment and it's able to learn how to 
best accomplish its goal usually does this without   any human supervision or guidance which makes 
it extremely powerful and very flexible as well this has huge obvious impact in fields 
like robotics self-driving cars and robot   manipulation but it also has really 
revolutionized the world of gameplay   and strategic planning and it's this really 
connection between the real world and deep   learning the virtual world that makes this 
particularly exciting to me and and i hope   this video that i'm going to show 
you next really conveys that as well   starcraft has imperfect information and is played 
in real time it also requires long-term planning   and the ability to choose what action to take from 
millions and millions of possibilities i'm hoping   for a 5-0 not to lose any games but i think the 
realistic goal would be four and one in my favor i think he looks more confident 
than __ was quite nervous before   the room was much more tense this time   really didn't know what to expect he's been 
playing starcraft pretty much since his fight i wasn't expecting the ai to be that good 
everything that he did was proper it was   calculated and it was done well 
i thought i'm learning something it's much better than i expected it i 
would consider myself a good player right   but i lost every single one of five games all right so in fact this is an example of 
how deep learning was used to compete against   humans professionally trained game players 
and was actually trained to not only compete   against them but it was able to achieve remarkably 
superhuman performance beating this professional   uh starcraft player five games to zero so let's 
start by taking a step back and really seeing how   reinforcement learning fits within respect to all 
the other types of learning problems that we have   seen so far in this class so the first piece and 
the most comprehensive piece of learning problems   that we have been exploring so far in this class 
has been that of supervised learning problems   so this was kind of what we talked about 
in the first second and third lectures   and in this domain we're basically given a 
bunch of data x and we try to learn a neural   network to predict its label y so this goal is 
to learn this functional mapping from x to y   and i like to describe this very intuitively 
if if i give you a picture of this apple for   example i want to train a neural network to 
determine and tell me that this thing is an apple   okay the next class of algorithms that we 
actually learned about in the last lecture was   unsupervised learning so in this case we 
were only given data with no labels so a   bunch of images for example of apples and we were 
forced to learn a neural network or learn a model   that represented this underlying structure in 
the data set so again in the apple scenario   we tried to learn a model that says back to us 
if we show it these two pictures of apples that   these things are basically like each other 
we don't know that they're apples because we   were never given any labels that explicitly 
tell the model that this thing is an apple   but we can tell that oh this thing is pretty 
close to this other thing that it's also   seen and it can pick out those underlying 
structure between the two to identify that   now in the last part in rl and reinforcement 
learning which is what today's lecture is   going to be focused on we're given only data in 
the form of what we call state action pairs now   states are what are the observations of the system 
and the actions are the behaviors that that system   takes or that agent takes when it sees those 
states now the goal of rl is very different than   the goal of supervised learning and the goal 
of unsupervised learning the goal of rl is to   maximize the reward or the future reward of that 
agent in that environment over many time steps   so again going back to the apple example what the 
analog would be would be that the agent should   learn that it should eat this thing because it 
knows that it will keep you alive it will make you   healthier and needs and you need food to survive 
again like the unsupervised case it doesn't know   that this thing is an apple it doesn't even 
recognize exactly what it is all it knows is   that in the past they must have eaten it and it 
was able to survive longer and because it was a   piece of food it was able to to become healthier 
for example and through these state action pairs   and somewhat trial and error it was able to 
learn these representations and learn these plans so our focus today will be explicitly on 
this third class of learning problems and   reinforcement learning so to do that i think it's 
really important before we start diving into the   details and the nitty-gritty technical details 
i think it's really important for us to build up   some key vocabulary that is very important 
in reinforcement learning and it's going   to be really essential for us to build up on 
each of these points later on in the lecture   this is very important part of the lecture so 
i really want to go slowly through this section   so that the rest of the lecture is going to make 
as much sense as possible so let's start with the   central part the core of your reinforcement 
learning algorithm and that is your agent   now the agent is something that can take actions 
in the environment that could be things like a   drone making a delivery in the world it could be 
super mario navigating a video game the algorithm   in reinforcement learning is your agent and you 
could say in real life the agent is each of you   okay the next piece is the environment the 
environment is simply the world in which the   agent lives it's the place where the agent exists 
and operates and and conducts all of these actions   and that's exactly the connection between 
the two of them the agent can send commands   to the environment in forms of actions 
now capital a or lowercase a of t is   the action at time t that the agent takes 
in this environment we can denote capital a   as the action space this is the set of all 
possible actions that an agent can make now   i do want to say this even though i think it 
is somewhat self-explanatory an action is uh or   the list by which an action can be chosen the set 
of all possible actions that an agent can make in   the environment can be either discrete or it can 
be from a set of actions in this case we can see   the actions are forwards right backwards or left 
or it could also be continuous actions for example   the exact location in the environment a real as 
a real number coordinate for example like the   gps coordinates where this agent wants to move 
right so it could be discrete as a categorical   or a discrete probability distribution or it 
could be continuous in either of these cases observations are how the environment interacts 
back with the agent it's how the agent the agent   can observe where in the environment it is and 
how its actions affected its own state in the   environment and that leads me very nicely into 
this next point the state is actually a concrete   and immediate situation in which the agent finds 
itself so for example a state could be something   like a image feed that you see through your eyes 
this is the state of the world as you observe it a reward now is also a way of feedback from the 
environment to the agent and it's a way that the   environment can provide a way of feedback to 
measure the success or failure of an agent's   actions so for example in a video game when 
mario touches a coin he wins points and from   a given state an agent will send out outputs 
in the form of actions to the environment and   the environment will respond with the agent's new 
state the next state that it can can achieve which   resulted on acting on that previous 
state as well as any rewards that may   be collected or penalized by reaching 
that state now it's important to note here   that rewards can be either immediate or delayed 
uh they basically you should think of rewards as   effectively evaluating that agent's actions but 
you may not actually get a reward until very late   in life so for example you might take many 
different actions and then be rewarded a   long time into the future that's called a very 
delayed reward but it is a reward nonetheless   we can also look at what's called the total reward 
which is just the sum of all rewards that an agent   gets or collects at after a certain time t so r 
of i is the reward at time i and r of capital r of   t is just the return the total reward from time t 
all the way into the future so until time infinity   and that can actually be written now expanded we 
can we can expand out that summation uh from our   from r of t plus r of t plus one all the way on 
into the future so it's adding up all of those   uh rewards that the agent collects from this 
point on into the future however often it's   it's very common to consider not just the summed 
return the total return as a straight-up summation   but actually instead what we call the discounted 
sum of rewards now this discounting factor which   is represented here of by gamma is multiplied 
by the future awards that are discovered by the   agent in order to dampen those rewards effect 
on the agent's choice of action now why would   we want to do this so this is actually this this 
formulation was created by design to make future   rewards less important than immediate rewards 
in other words it enforces a kind of short-term   learning in the agent a concrete example of 
this would be if i offered to give you five   dollars today or five dollars in five years from 
today which would you take even though it's both   five dollars your reward would be the same but 
you would prefer to have that five dollars today   just because you prefer short-term rewards over 
long-term rewards and again like before we can   expand the summation out now with this discount 
factor which has to be typically between zero   and one and the discount factor is multiplied by 
these future rewards as discovered by the agent   and like i said it is reinforcing this 
this concept that we want to prioritize   these short-term rewards more than uh 
very long-term rewards in the future   now finally there's a very very important function 
in rl that's going to kind of start to put a lot   of these pieces together and that's called the q 
function and now let's look at actually how this   q function is defined remembering the definition 
of this total discounted reward capital r of t   so remember the total reward r of t measures the 
discounted sum of rewards obtained since time t   so now the q function is very related to that 
the q function is a function that takes as input   the current state that the agent is in and the 
action that the agent takes in that state and   then it returns the expected total future reward 
that the agent can receive after that point so   think of this as if an agent finds itself in some 
state in the world and it takes some action what   is the expected return that it can receive after 
that point that is what the q function tells us   let's suppose i give you this magical q function 
this is actually it really is a magical function   because it tells us a lot about the problem 
if i give you this function an oracle that   you can plug in any state and action pair and it 
will tell you this expected return from point time   point t from your current time point i give you 
that function the question is can you determine   given the state that you're currently in can 
you determine what is the best action to take   you can perform any queries on this function 
and the way you can simply do this is   that you ultimately in your mind you want to 
select the best action to take in your current   state but what is the best action to take well 
it's simply the action that results in the highest   expected total return so what you can do is 
simply choose a policy that maximizes this future   reward or this future return well 
that can be simply written by finding   the arg max of your q function over all 
possible actions that you can take at the state   so simply if i give you this q function and a 
state that you're in you can feed in your state   with every single action and evaluate what the q 
function would tell you the expected total reward   would be given that state action pair and you 
pick the action that gives you the highest q value   that's the best action to take in this current 
state so you can build up this policy which here   we're calling pi of s to infer this best 
action to take so think of your policy now   as another function that takes this 
input your state and it tells you   the action that you should execute in this in 
the state so the strategy given a q function to   compute your policy is simply from this arcmax 
formulation find the action that maximizes   your q function now in this lecture you're going 
to we're going to focus on basically these two   classes of reinforcement learning algorithms 
into two categories one of which will actually   try to learn this q function q of s your state 
and your action and the other one will be called   what are called policy learning algorithms because 
they try to directly learn the policy instead of   using a q function to infer your policy now we're 
going to in policy learning directly infer your   policy pi of s that governs what actions you 
should take this is a much more direct way of   thinking about the problem but first thing we're 
going to do is focus on the value learning problem   and how we can do what is called q learning 
and then we'll build up to policy learning   after that let's start by digging a bit deeper 
into this q function so first i'll introduce this   game of atari breakout on the left for those who 
haven't seen it i'll give a brief introduction   into how the game works now the q value tells 
us exactly what the expected total expected   return that we can expect to see on any state in 
this game and this is an example of one state so   in this game you are the agent is this paddle 
on the bottom of the board it's this red paddle   it can move left or right and those are its two 
actions it can also stay constant in the same   place so it has three actions in the environment 
there's also this ball which is traveling in this   case down towards the bottom of the the board 
and is about to hit and ricochet off of this   paddle now the objective the goal of this game is 
actually to move the pedal back and forth and hit   the ball at the best time such that you can bounce 
it off and hit and break out all of these colored   blocks at the top of the board each time the ball 
touches one of these colored blocks it's able to   break them out hence the name of the the name of 
the game breakout and the goal is to knock off   as many of these as possible each time the ball 
touches one it's gone and you got to keep moving   around hitting the ball until you knock off all 
of the of the blocks now the q function tells us   actually what is the expected total return that 
we can expect in a given state action pair and   the point i'd like to make here is that it's 
actually it can sometimes be very challenging to   understand or intuitively guess what is the uh 
q value for a given state action pair so even if   let's say i give you these two state action pairs 
option a and option b and i ask you which one out   of these two pairs do you think has a higher q 
value on option a we can see the ball is already   traveling towards the paddle and the paddle is 
choosing to stay in the same place it's probably   going to hit the ball and it'll bounce back up 
and and break some blocks on the second option   we can see the ball coming in at an angle and the 
paddle moving towards the right to hit that ball   and i asked you which of these two options 
state action pairs do you believe will   return the higher expected total reward before 
i give you the answer i want to tell you   a bit about what these two policies actually 
look like when they play the game instead of   just seeing this single state action pair so let's 
take a look first at option a so option a is this   this relatively conservative option that doesn't 
move when the ball is traveling right towards it   and what you can see is that as it plays the game 
it starts to actually does pretty well it starts   to hit off a lot of the breakout pieces towards 
the center of the game and it actually does pretty   well it breaks out a lot of the ball a lot of the 
colored blocks in this game but let's take a look   also at option b option b actually does something 
really interesting it really likes to hit   the ball at the corner of the paddle uh it does 
this just so the ball can ricochet off at an   extreme angle and break off colors in the corner 
of the screen now this is actually it does this   to the extreme actually because it even even in 
the case where the ball is coming right towards   it it will move out of the way just so it can come 
in and hit it at these extreme ricocheting angles   so let's take a look at how option 
b performs when it plays the game and you can see it's really targeting the side of 
the paddle and hitting off a lot of those colored   blocks and why because now you can see once 
it breaks out the corner the two edges of the   screen it was able to knock off a ton of blocks 
in the game because it was able to basically   get stuck in that top region let's take another 
look at this so once it gets stuck in that top   region it doesn't have to worry about making any 
actions anymore because it's just accumulating a   ton of rewards right now and this is a very great 
policy to learn because it's able to beat the game   much much faster than option a and with much less 
effort as well so the answer to the question which   stay action pair has a higher q value in this case 
is option b but that's a relatively unintuitive   option at least for me when i first saw this 
problem because i would have expected that   playing things i mean not moving out of the way of 
the ball when it's coming right towards you would   be a better action but this agent actually has 
learned to move away from the ball just so it can   come back and hit it and really attack at extreme 
angles that's a very interesting observation   that this agent has made through learning now the 
question is how can we use because q values are so   difficult to actually define it's hard for humans 
to define them as we saw in the previous example   instead of having humans define that q value 
function how can we use deep neural networks to   model this function and learn it instead so in 
this case uh well the q value is this function   that takes this input to state in action so one 
thing we could do is have a deep neural network   that gets inputs of both its state and the desired 
action that it's considering to make in that state   then the network would be trained to predict the 
q value for that given state action pair that's   just a single number the problem with this is 
that it can be rather inefficient to actually   run forward in time because if remember how we 
compute the policy for this model if we want to   predict what is the optimal action that it should 
take in this given state we need to evaluate this   deep q network n times where n is the number 
of possible actions that it can make in this   time step this means that we basically have to run 
this network many times for each time step just to   compute what is the optimal action and this can be 
rather inefficient instead what we can do which is   very equivalent to this idea but just formulate 
it slightly differently is that it's often much   more convenient to output all of the q values at 
once so you input the state here and you output   a basically a vector of q values instead of one q 
value now it's a vector of q values that you would   expect to see for each of these states so the q 
value for state sorry for each of these actions   so the q value for action one the q value for 
action two all the way up to your final action   so for each of these actions and given 
the state that you're currently in   the this output of the network tells you what 
is the optimal set of actions or what is the   the breakdown of the q values given these 
different actions that could be taken   now how can we actually train this version of 
the deep queue network we know that we wanted   to output these what are called q values but it's 
not actually clear how we can train it and to do   this and well this is actually challenging 
you can think of it conceptually as because   we don't have a data set of q values right all 
we have are observations state action and reward   triplets so to do this and to train this type 
of deep queue network we have to think about   what is the best case scenario how would the 
agent perform optimally or perform ideally   what would happen if it takes all the best 
actions like we can see here well this would mean   that that the target return would be maximized 
and what what we can do in this case is we can   actually use this exact target return to serve 
as our ground truth our data set in some sense   in order to actually train this agent to train 
this deep q network now what that looks like   is first we'll formulate our expected return 
if we were to take all of the best actions the   initial reward r plus the action that we select 
that maximizes the expected return for the next   future state and then we apply that discounting 
factor gamma so this is our target this is our our   q value that we're going to try and uh optimize 
towards it's like the what we're trying to   match right that's what we want our prediction 
to mac match but now we should ask ourselves   what does our network predict well our 
network is predicting like we can see   in this in this network the network is predicting 
the q value for a given state action pair   well we can use these two pieces of information 
both our predicted q value and our target q value   to train and create this what we call q loss this 
is a essentially a mean squared error formulation   between our target and our predicted q values and 
we can use that to train this deep queue network so in summary let's uh walk through this 
whole process end to end of deep q learning   our deep neural network sees as input a state 
and the state gets fed through the network and   we try to output the q value for each pause 
of the three possible actions here there are   three different ways that the network 
can play you can either move to the left   move to the right or it can 
stay constant in the same place now in order to infer the optimal policy 
it has to look at each of these q values   so in this case moving to the left because it sees 
that the ball is moving to the left it can it sees   that okay if i step a little bit to the left i 
have a higher chance of probably hitting that   ball and and continuing the game so my q value 
for my expected total reward return my q value   for moving left is 20. on the other hand if i stay 
in the same place let's say i have a q value of 3   and if i move to the right out of the way of the 
ball in this case because the ball is already   moving towards me i have a q value of 0. 
so these are all my q values for all of the   different possible actions how do i compute 
the optimal policy well as we saw before   the optimal policy is obtained by looking at 
the maximum q value and picking the action   that maximizes our q value so in this case we can 
see that the maximum q value is attained when we   move to the left with action one so we we 
select action one we feed this back into   the game engine we send this back to the 
environment and we receive our next state   and this process repeats all over again the next 
state is fed through the deep neural network   and we obtain a list of q values for each of the 
possible actions and it repeats again now deepmind   showed that these networks deep queue networks 
could be applied to solve a variety of different   types of atari games not just breakout but many 
other games as well and basically all they needed   to do was provide the state pictorially as an 
input passing them through these convolutional   layers followed by non-linearities and pooling 
operations like we learned in lecture three   and at the right hand side it's predicting these q 
values for each possible action that it could take   and it's exactly as we saw in the previous 
couple slides so it picks an optimal action   to execute on the next time step depending on 
the maximum q value that could be attained and   it sends this back to the environment to execute 
and and receive its next state this is actually   remarkably simple because despite its remarkable 
simplicity in my opinion of essentially trial and   error they tested this on many many games in atari 
and showed that for over 50 percent of the games   they were able to surpass human level performance 
with this technique and other games which you can   see on the right hand side of this plot were more 
challenging but still again given how simple this   technique is and how clean it is it really 
is amazing that this works at all to me   so despite all of the advantages 
of this approach the simplicity   the the cleanness and how elegant the solution 
is i think it's and uh i mean above all that   the ability for this solution to learn superhuman 
uh policies policies that can beat humans even on   some relatively simple tasks there are some 
very important downsides to queue learning   so the first of which is the simplistic model 
that we learned about today this model can only   handle action spaces which are discrete and it 
can only really handle them when the action space   is small so when we're only given a few possible 
actions as each step it cannot handle continuous   action spaces so for example if an autonomous 
vehicle wants to predict where to go in the world   instead of predicting to go left right or 
go straight these are discrete categories   how can we use reinforcement learning to learn a 
continuous steering wheel angle one that's not not   discretized into bins but can take 
any real number within some bound   of where the steering wheel angle can execute this 
is a continuous variable it has an infinite space   and it would not be possible in the version of q 
learning that we presented here in this lecture   it's also its flexibility of q learning is also 
somewhat limited because it's not able to learn   policies that can be stochastic that can change 
according to some unseen probability distribution   so they're deterministically computed from the 
q function through this maximum formulation it   always is going to pick the maximum the action 
that maximally elevates your expected return   so it can't really learn from these stochastic 
policies on the other hand to address these   we're really going to dive into this next phase of 
today's lecture focused on policy gradient methods   which will hopefully we'll see 
tackle these remaining issues   so let's dive in the key difference now 
between what we've seen in the first part   of the lecture and the second part that we're 
going to see is that in value learning we try   to have a neural network to learn our q value 
q of our state given or action and then we use   this q value to infer the best action to take 
given a state that we're in that's our policy   now policy learning is a bit different it tries 
to now directly learn the policy using our neural   network so it inputs the state and it tries to 
directly learn the policy that will tell us which   action we should take this is a lot simpler since 
this means we now get directly the action for free   by simply sampling straight away from this policy 
function that we can learn so now let's dive into   the the details of how policy learning works 
and and first i want to really really narrow   or sorry drive in this difference from q learning 
because it is a subtle difference but it's a very   very important difference so deep q networks 
aim to approximate this q function again   by first predicting given a state the q value for 
each possible action and then it simply picks the   best action where best here is described by which 
action gives you the maximum q value the maximum   expected return and execute that that action now 
policy learning the key idea of policy learning is   to instead of predicting the q values we're going 
to directly optimize the policy pi of s so this   is the policy distribution directly governing 
how we should act given a current state that   we find ourselves in so the output here 
is for us to give us the desired action   in a much more direct way the outputs represent 
the probability that the action that we're going   to sample or select should be the correct action 
that we should take at this step right in other   words it will be the one that gives us the maximum 
reward so take for example this if we see that we   predict these probabilities of these given actions 
being the optimal action so we get the state and   our policy network now is predicting a probability 
distribution of uh we can basically aggregate them   into our policy so we can say our policy is 
now defined by this probability distribution   and now to compute the action that we should 
take we simply sample from this distribution   to predict the action that we should execute in 
this case it's the car going left which is a1   but since this is a probability distribution 
the next time we sample we might not we might   get to stay in the same place we might sample 
action 2 for example because this does have a   nonzero probability a probability of 0.1 now note 
that because this is a probability distribution   this this p of actions given our state must 
sum to one now what are some of the advantages   of this type of formulation over first of all 
over q learning like we saw before besides the   fact that it's just a much more direct way to get 
what we want instead of optimizing a q function   and then using the q function to create our policy 
now we're going to directly optimize the policy   beyond that though there is one very important 
advantage of this formulation and that is that it   can handle continuous action spaces so this was an 
example of a discrete action space what we've been   working with so far in this atari breakout game 
moving left moving right or staying in the center   there are three actions and they're discrete 
there's a finite number of actions here that can   be taken for example this is showing the prob our 
action space here is representing the direction   that i should move but instead a continuous action 
space would tell us not just the direction but   how fast for example as a real number that 
i should move questions like that that are   infinite in the number of possible answers 
this could be one meter per second to the left   half a meter per second to left or any 
numeric velocity it also tells us direction   by nature through plus or minus sign so if i say 
minus one meter per second it tells me that i want   to move to left at one meter per second if i say 
positive one it tells me i want to move to the   right at one meter per second but now when 
we plot this as a probability distribution   we can also visualize this as a continuous action 
space and simply we can visualize this using   something like a gaussian distribution in this 
case but it could take many different you can   choose the type of distribution that fits best 
with your problem set gaussian is a popular choice   here because of its simplicity so here again we 
can see that the probability of moving to the left   faster to the left is much greater than moving 
faster to the right and we can actually see that   the mean of this distribution the average the 
point where this normal distribution is highest   tells us an exact numerical value of how fast it 
should be moving not just that it should be moving   to the left but how fast it should be moving 
to the left now let's take a look at how we can   actually model these continuous action spaces with 
a policy gradient method instead of predicting   the probability of taking an action given 
a possible state which in this case since   we're in the continuous domain there would be an 
infinite number of actions let's assume that our   output distribution is actually a normal gaussian 
an output a mean and a variance for that gaussian   then we only have two outputs but it allows us 
to describe this probability distribution over   the entire continuous space which otherwise would 
have been infinite an infinite number of outputs   so in this case that's if we predict that the mean 
that uh the mean action that we should take mu   is negative one and the variance is of 0.5 we can 
see that this probability distribution looks like   this on the bottom left-hand side it should move 
to the left with an average speed of negative   one meters per second and with some variance so 
it's not totally confident that that's the best   speed at which it should move to the left but 
it's pretty set on that being the the place   to move to the left so for this picture we can 
see that the paddle needs to move to the left   if we actually plot this distribution like this 
and we can actually see that the mass of the   distribution does lie on the left-hand side of 
the number line and if we sample for example from   this distribution we can actually see that in this 
case we're getting that the action that we should   take the concrete velocity that should be executed 
would indicate that we need to move left negative   at a speed of 0.8 meters per second so again 
that means that we're moving left with a   speed of 0.8 meters per second note here that 
even though the the mean of this distribution   is negative one we're not constrained to that 
exact number this is a continuous probability   distribution so here we sampled an action that 
was not exactly the mean but that's totally fine   and that really highlights that the difference 
here between the discrete action space and the   continuous action space this opens up a ton 
of possibilities for applications where we   do model infinite numbers of actions and again 
like before like the discrete action case this   probability distribution still has all of the 
nice properties of probability distributions   namely that the integral of this computed 
probability distribution does still sum to one   so we can indeed sample from it which 
is a very nice confirmation property   okay great so let's take a look now of how 
the policy gradients algorithm works in a   concrete example let's start by revisiting this 
whole learning loop of reinforcement learning   again that we saw in the very beginning of 
this lecture and let's think of how we can   use the policy gradient algorithm that we have 
introduced to actually train an autonomous vehicle   using using this trial and error policy gradient 
method so with this case study study of autonomous   vehicles or self-driving cars what are all of 
these components so the agent would be our vehicle   it's traveling in the environment which is the 
the world the lane that it's it's traveling   in it has some state that is obtained through 
camera data lidar data radar data et cetera   it obtains sorry it makes actions what are the 
actions that it can take the actions in this   case are the steering wheel angle again this is a 
concrete example of a continuous action space you   don't discretize your steering wheel angle into 
unique bins your steering wheel angle is infinite   in the number of possibilities it can take and 
it can take any real number between some bounds   so that is a continuous problem that is a 
continuous variable that we're trying to   model through this action and finally uh it 
receives rewards in the in the form of the   distance it can travel before it needs to 
be uh needs some form of human intervention   so let's now dive in now that we've 
identified all of those so how can we train   this car using policy gradient network in this 
context here we're taking self-driving cars as   an example but you can hopefully see that we're 
only using this because it's nice and intuitive   but this will also apply to really any domain 
where you can identify and set up the problem   like we've set up the problem so far so let's 
start by initializing our agent again our agent   is the vehicle we can place it onto the road in 
the center of the road the next step would be to   let the agent run in the beginning it doesn't 
run very well because it crashes and well it's   never been trained before so we don't expect it 
to run very well but that's okay because this is   reinforcement learning so we run that policy until 
it terminates in this case we mark terminations by   the time that it it crashes and needs to be 
taken over along that uh what we call rollout   we start to record all of the state action pairs 
or sorry state action reward pairs so at each step   we're going to record where was the robot what 
was it state what was the action that it executed   and what was the reward that it obtained 
by executing that action in that state now next step would be to take all of 
those state action reward pairs   and actually decrease the probability of taking 
any action that it took close to the time   where the terminated determination happened so 
close to the time where the crash occurred we   want to decrease the probability of making 
any of those actions again in the future   likewise we want to increase the probability 
of making any actions in the beginning of this   episode note here that we don't necessarily 
know that there was something good in this   first part of the episode we're just assuming 
that because the crash occurred in the second   part of the episode that was likely due to an 
action that occurred in that second part this   is a very unintelligent if you could say algorithm 
because it that's all it assumes it just tries to   decrease the probability of anything that resulted 
in a low reward and increase the probability of   anything that resulted in a high reward it doesn't 
know that any of these actions were better than   the other especially in the beginning because 
it doesn't have that kind of feedback this is   just saying that we want to decrease anything that 
may have been bad and increase anything that would   have been good and if we do this again we can see 
that the next time the car runs it runs for a bit   longer and if we do it again we do the same thing 
now on this rollout we decrease the probability of   actions that resulted in low reward and increase 
the probability that resulted in positive or   high reward we reinitialize this and we run it 
until completion and update the policy again   and it seems to run a bit longer and we 
can do this again and we keep doing this   until eventually it learns to start to follow the 
lanes without crashing and this is really awesome   i think because we never taught this vehicle how 
anything well we never taught it anything about   lanes we never taught it what a lane marker is 
it learns to avoid lanes though and not crash   and not crash just by observing very sparse 
rewards of crashing so it observed a lot of   crashes and it learned to say like okay i'm not 
going to do any of these actions that occurred   very close to my crashes and just by observing 
those things it was able to successfully avoid   lanes and survive in this environment longer and 
longer times now the remaining question is how   we can actually update our policy on every 
training iteration to decrease the probability of   bad events and increase the probability of these 
good events or these good actions let's call them   so that really focuses and narrows us into points 
four and five in this this training algorithm   how can we do this learning process of decreasing 
these probabilities when it's bad and increasing   the probabilities when they're good let's 
take a look at that in a bit more detail   so let's look at specifically the loss function 
for training policy gradients and then we'll   dissect it to understand exactly why this works 
so this loss consists of really two parts that   i'd like to dive into the first term is this log 
likelihood term the log likelihood of our pro of   our our policy our probability of an action 
given our state the second term is where we   multiply this negative log likelihood by the total 
discounted reward or the total discounted return   excuse me r of t so let's assume that we get a 
lot of reward for an action that had very high   log likelihood this loss will be great and it 
will reinforce these actions because they resulted   in very good returns on the other hand if the 
reward is very low for an action that it had high   probability for it will adjust those probabilities 
such that that action should not be sampled again   in the future because it did not result in 
a desirable return so when we plug in these   uh this loss to the gradient descent algorithm to 
train our neural network we can actually see that the policy gradient term here 
which is highlighted in blue   which is where this this algorithm gets 
its name it's the policy because it has to   compute this gradient over the policy part of this 
function and again uh just to reiterate once more   this policy gradient term consists of these 
two parts one is the likelihood of an action   the second is the reward if the action is very 
positive very good resulting in good reward it's   going to amplify that through this gradient 
term if the action is very is very probable   or sorry not very probable but it did result in 
a good reward it will actually amplify it even   further so something that was not probable before 
will become probable because it resulted in a good   return and vice versa on the other side as well 
now i want to talk a little bit about how we can   extend some of these reinforcement 
learning algorithms into real life and   this is a particularly challenging question 
because this is something that has a particular   interest to the reinforcement learning field 
right now and especially right now because   applying these algorithms in the real world 
is something that's very difficult for one   reason or one main reason and that is this step 
right here running a policy until termination   that's one thing i touched on but i didn't spend 
too much time really dissecting it why is this   difficult well in the real world terminating means 
well crashing dying usually pretty bad things   and we can get around these types of things 
usually by training and simulation but then   the problem is that modern simulators do not 
accurately depict the real world and furthermore   they don't transfer to the real world when 
you deploy them so if you train something   in simulation it will work in simulation it will 
work very well in simulation but when you want   to then take that policy deployed into the real 
world it does not work very well now one really   cool result that we created in my lab was actually 
developing a brand new photo realistic simulation   engine specifically for self-driving cars that i 
want to share with you that's entirely data driven   and enables these types of reinforcement learning 
advances in the real world so one really cool   result that we created was developing this type of 
simulation engine here called vista and allows us   to use real data of the world to simulate brand 
new virtual agents inside of the simulation now   the results here are incredibly photorealistic as 
you can see and it allows us to train agents using   reinforcement learning in simulation using exactly 
the methods that we saw today so that they can be   directly deployed without any transfer learning 
or domain adaptation directly into the real world   now in fact we did exactly this we placed agents 
inside of our simulator train them using exactly   the same policy grading algorithm that we learned 
about in this lecture and all of the training was   done in our simulator then we took these policies 
and put them directly in our full-scale autonomous   vehicle as you can see in this video and on the 
left hand side you can actually see me sitting   in this vehicle in the bottom of the interior 
shot you can see me sitting inside this vehicle   as it travels through the real world completely 
autonomous this represented the first time at   the time of when we published these results the 
first time an autonomous vehicle was trained   using rl entirely in simulation and was able to be 
deployed in the real life a really awesome result so now we have covered the fundamentals behind 
value learning as well as policy gradient   reinforcement learning approaches i think now 
it's really important to touch on some of the   really remarkable deep reinforcement learning 
applications that we've seen in recent years   and for that look we're going to turn first 
to the game of go where reinforcement learning   agents were put to the test against human 
champions and achieved what at the time was   and still is extremely exciting results so first 
i want to provide a bit of an introduction to the   game of go this is a game that consists of 19 
by 19 uh grids it's played between two players   who hold either white pieces or black pieces 
and the objective of this game is to occupy   more board territory with your pieces than your 
opponent now even though the grid and the rules   of the game are very simple the problem of go 
solving the game of go and doing it to beat   the grand masters is an extremely complex problem 
it's it's actually because the number of possible   board positions the number of states that you 
can encounter in the game of go is massive   the full size with the full-size board there 
are greater number of legal board positions   than there are atoms in the universe now the 
objective here is to train an ai to train a   machine learning or deep learning algorithm 
that can master the game of go not only to   beat the existing gold standard software but 
also to beat the current world human champions   now in 2016 google deepmind rose to this challenge 
and a couple and several years ago they actually   developed a reinforcement learning based pipeline 
that defeated champion go players and the idea at   its core is very simple and follows along with 
everything that we've learned in this lecture   today so first a neural network was trained and 
it got to watch a lot of human expert go players   and basically learn to imitate their behaviors 
this part was not using reinforcement learning   this was using supervised learning you basically 
got to study a lot of human experts then   they use these pre-trained networks to play 
against reinforcement learning policy networks   which allows the policy to go beyond what 
the human experts did and actually play   against themselves and achieve actually 
superhuman performance in addition to this   one of the really tricks that brought this to be 
possible was the usage of this auxiliary network   which took the input of the state of the board 
as as input and predicted how good of a state   this was now given that this network the ai 
could then hallucinate essentially different   board position actions that it could take 
and evaluate how good these actions would   be given these predicted values this essentially 
allowed it to traverse and plan its way through   different possible actions that it could take 
based on where it could end up in the future finally a recently published extension of these 
approaches just a few years ago in 2018 called   alpha zero only used self-play and generalized to 
three famous board green board games not just go   but also chess shogi and go and in these examples 
the authors demonstrate that it was actually not   necessary to pre-train these networks from human 
experts but instead they optimized them entirely   from scratch so now this is a purely reinforcement 
learning based solution but it was still able to   not only beat the humans but it also beat the 
previous networks that were pre-trained with   human data now as recently as only last month 
very recently the next breakthrough in this   line of works was released with what is called 
mu0 where the algorithm now learned to master   these environments without even knowing the rules 
i think the best way to describe mu0 is to compare   and contrast its abilities with those previous 
advancements that we've already discussed earlier   already today so we started this discussion 
with alphago now this demonstrates superhuman   performance with go on go using self pro self 
play and pre-training these models using human   grand master data then came alphago 0 which showed 
us that even better performance could be achieved   entirely on its own without pre-training 
from the human grandmasters but instead   directly learning from scratch then came alpha 
zero which extended this idea even further   beyond the game of go and also into chess 
and shogi but still required the model to   know the rule and be given the rules of the 
games in order in order to learn from them   now last month the authors demonstrated 
superhuman performance on over 50 games   all without the algorithm knowing the rules 
beforehand it had to learn them as well as   actually learning how to play the game 
optimally during its training process   now this is critical because in many scenarios 
we do not know the rules beforehand to tell the   model when we are placed in the environment 
sometimes the rules are unknown the rules or   the dynamics are unknown objects may interact 
stochastically or unpredictably we may also   be in an environment where the rules are simply 
just too complicated to be described by humans so   this idea of learning the rules of the game 
or of the task is a very very powerful concept   and let's actually walk through very very briefly 
how this works because it's such an awesome   algorithm but again at its core it really builds 
on everything that we've learned today so you   should be able to understand each part of this of 
this algorithm we start by observing the board's   state and from this point we predict or we perform 
a tree search through the different possible   scenarios that can arise so we take some actions 
and we look at the next possible scenarios or the   next possible states that can arise but now since 
we don't know the rules the network is forced to   learn the dynamics the dynamics model of how to 
do this search so to learn what could be the next   states given the state that it currently sees 
itself in and the action that it takes now at   the base time this gives us this probability of 
executing each of these possible actions based on   the value that it can attain through this branch 
of the tree and it uses this to plan the next   action that it should take this is essentially 
the policy network that we've been learning   about but amplified to also encounter this tree 
search algorithm for planning into the future   now given this policy network it takes this 
action and receives a new observation from   the game and repeats this process over and over 
again until of course that it the game finishes or   the game is over a very similar this is very 
very similar to how we saw alpha zero work   but now the key difference is that the dynamics 
model as part of the tree search that we can see   at each of these steps is entirely learned 
and greatly opens up the possibilities for   these techniques to be applied outside of 
rigid game scenarios so in these scenarios   we do know the rules of the games very well so 
we could use them to train our algorithms better   but in many scenarios this type of advancement 
allows us to apply these algorithms to areas   where we simply don't know the rules and where we 
need to learn the rules in order to play the game   or simply where the rules are much harder 
to define which in reality in the real world   many of the interesting scenarios 
this would be exactly the case   so let's briefly recap with what we've learned in 
the lecture today we started with the foundations   of deep reinforcement learning we defined what 
agents are what actions are what environments   are and how they all interact with each other in 
this reinforcement learning loop then we started   by looking at a broad class of q learning 
problems and specifically the deep q network   where we try to learn a q function given a state 
and action pair and then determine a policy by   selecting the action that maximizes that q 
function and finally we learned how we could   optimize instead of optimizing the q value or the 
q function learn to directly optimize the policy   straight from straight from the state and we saw 
that this has really impactful applications in   continuous action spaces where q functions or 
this q learning technique is somewhat limited   so thank you for attending this lecture on deep 
reinforcement learning at this point we'll now be   taking the next part of the class which we focused 
on reinforcement learning and you'll get some   experience on how you can apply these algorithms 
onto all by yourself specifically focusing on   the policy gradient algorithms in the context 
of a very simple example of pong as well as   more complex examples as well so you'll actually 
build up this body and the brain of the agent and   the environment from scratch and you'll really 
get to put together a lot of the ideas that   we've seen today in this lecture so please 
come to the gather town for if you have any   questions and we'd be happy to discuss questions 
on the software lab specifically as well as any   questions on today's lecture so we look 
forward to seeing you there thank you 

Hi everyone and welcome to lecture six of MIT 
6.S191! This is one of my absolute favorite   lectures in the course and we're going to 
focus and discuss some of the limitations of   deep learning algorithms as well as some of the 
emerging new research frontiers in this field   before we dive into the technical content 
there are some course related and logistical   announcements that i'd like to make the first 
is that our course has a tradition of designing   and delivering t-shirts to students participating 
this year we are going to continue to honor that   so to that end we have a sign up sheet on canvas 
for all students where you can indicate your   interest in receiving a t-shirt and once you 
fill out that sign up sheet with the necessary   information will ensure that a t-shirt is 
delivered to you by the appropriate means   as soon as possible and if after the class if the 
canvas is closed and you can't access that signup   form please just feel free to send us an email 
and we'll find a way to get the t-shirt to you   so to provide a take a step back and give 
an overview of our schedule of this course   so far where we've been and where we're going 
following this lecture on limitations and new   frontiers we'll have the due date for our final 
software lab on reinforcement learning tomorrow   we're going to have two really exciting hot 
topic spotlight lectures with brand new content   and that will be followed by a series of four 
guest lectures you'll have time over the rest   of this week to continue to work on your final 
projects and the class will conclude on friday   with the student final project presentations 
and proposal competition as well as our award   ceremony so speaking of those final projects 
let's get into some details about those for   those of you taking the course for credit you 
have two options to fulfill your grade the first   is a project proposal where you will work 
in up to a group of four to develop a new   and novel deep learning idea or application and we 
realize that two weeks is a very short amount of   time to come up with and implement a project 
so we are certainly going to be taking this   into consideration in the judging then on friday 
january 29th you will give a brief three-minute   presentation on your project proposal to a group 
of judges who will then award the final prizes   as far as logistics and timelines you 
will need to indicate your interest   in presenting by this wednesday at midnight 
eastern time and will need to submit the slide   for your presentation by midnight eastern time 
on thursday instructions for the project proposal   and submission of these requirements are on 
the course syllabus and on the canvas site   our top winners are going to be awarded 
prizes including nvidia gpus and google homes   the key point that i'd like to make 
about the final proposal presentations   is that in order to participate and be eligible 
for the prize synchronous attendance is required   on friday's course so friday january 29th from 1 
to 3 p.m eastern time you will need to be present   your or your group will need to be present 
in order to participate in the final proposal   competition the second option for 
fulfilling the credit requirement   is to write a one-page review of a deep learning 
paper with the evaluation being based on   the completeness and clarity of your review this 
is going to be due by thursday midnight eastern   time and further information and instruction 
on this is also available on canvas so   after this lecture next we're going to have a 
series of two really exciting hot topic spotlight   talks and these are going to focus on two rapidly 
emerging in developing areas within deep learning   deep learning research the first is going 
to highlight a series of approaches called   evidential deep learning that seeks to develop 
algorithms that can actually learn and estimate   the uncertainties of neural networks and the 
second spotlight talk is going to focus on machine   learning bias and fairness and here we're going to 
discuss some of the dangers of implementing biased   algorithms in society and also emerging strategies 
to actually mitigate these unwanted biases   that will then be followed by a series of 
really exciting and awesome guest lectures   from leading researchers in industry and 
academia and specifically we're going to have   talks that are going to cover a diversity 
of topics everything from ai and healthcare   to document analysis for business applications 
and computer vision and we highly highly highly   encourage you to join synchronously for these 
lectures if you can on january 27th and january   28th from 1 to 3 p.m eastern these are going to 
be highlighting very exciting topics and they   may extend a bit into the designated software lab 
time so that we can ensure we can have a live q a   with our fantastic guest speakers all right 
so that concludes the logistical and course   related announcements let's dive into the fun 
stuff and the technical content for this lecture   so so far in taking success 191 i hope that 
you've gotten a sense of how deep learning   has revolutionized and is revolutionizing so 
many different research areas and fields from   advances in autonomous vehicles to medicine 
and healthcare to reinforcement learning   generative modeling robotics and a variety 
of other applications from natural language   processing to finance and security and alongside 
with understanding the tremendous application   utility and power of deep learning i hope that 
you have also established concrete understanding   of how these algorithms actually work and how 
specifically they have enabled these advances to take a step back at the types of algorithms and 
models that we've been considering we've primarily   dealt with systems that take as input data as the 
in the form of signals images other sensory data   and move forward to produce a decision as the 
output this can be a prediction this can be a   outputted detection it can also be an action as 
in the case of reinforcement learning we've also   considered the inverse problem as in the case 
of generative modeling where we can actually   train neural networks to produce new data 
instances and in both these paradigms we can   really think of neural networks as very powerful 
function approximators and this relates back to a   long-standing theorem in the theory of neural 
networks and that's called the universal   approximation theorem and it was presented in 
1989 and generated quite the stir in the community   and what this theorem the universal approximation 
theorem states is that a neural network with a   single hidden layer is sufficient to approximate 
any arbitrary function to any arbitrary position all it requires is a single layer and 
in this class we've primarily dealt with   deep neural models where we are stacking multiple 
hidden layers on top of each other but this   theorem completely ignores that fact and says 
okay we only need one layer so long as we can   reduce our problem to a set of outputs inputs and 
a set of outputs this means there has to exist a   neural network that can solve this problem it's 
a really really powerful and really big statement   but if you consider this closely there are a 
couple of caveats that we have to be aware of   the first is that this theorem makes no 
guarantees on the number of hidden units   or size of the layer that's 
going to be required to solve   such a problem right and it also leaves open 
the question of how we could actually go about   training such a model finding the weights 
to support that architecture it doesn't make   any claims about that it just says it proves 
that one such network exists but as we know   with gradient descent finding these weights 
is highly non-trivial and due to the very   non-convex nature of the optimization problem 
the other critical caveat is that this theorem   places no guarantees on how well the resulting 
model would actually generalize to other tasks   and indeed i think that this this theorem 
the universal approximation theorem points   to a broader issue that relates to the possible 
effects of overhype in artificial intelligence and   us as a community as students invested 
in advancing the state of this field   i think we need to be really careful in how 
we consider and market and advertise these   algorithms while the universal approximation 
theorem was able to generate a lot of excitement   it also provided a false sense of 
hope to the community at the time   which was that neural networks could be used to 
solve any problem and as you can imagine this   overhype is very very very dangerous and this 
over hype has also been tied in to what were two   historic a.i winters where research in artificial 
intelligence and neural networks more specifically   slowed down very significantly and i think we're 
still in this phase of explosive growth which is   why today for the rest of the lecture i want 
to focus in on some of the limitations of the   algorithms that we've learned about and extend 
beyond to discuss how we can go beyond this to   consider new research frontiers 
all right so first the limitations   one of my favorite and i think one of 
the most powerful examples of a potential   danger and limitation of deep neural networks 
come from this paper called understanding deep   neural networks requires rethinking generalization 
and what they did in this paper was a very simple   experiment they took images from the dataset 
imagenet and each of these images are associated   with a particular class label as seen here and 
what they did was they did this experiment where   for every image in the data set not class but 
individual images they flipped a die a k sided   die where k was the number of possible classes 
they were considering and they used this this   flip of the die to randomly assign a brand new 
label to a particular image which meant that these   new labels associated were completely random with 
respect to what was actually present in the image   so for example a remapping could be visualized 
here and note that these two instances of dogs   have been mapped to different classes altogether 
so we're completely randomizing our labels   what they next did was took this data this 
scrambled data and tried to fit a deep neural   network to the to the imagenet data by applying 
varying degrees of randomization from the original   data with the untouched class labels to the 
completely randomized data and as you ex may   expect the model's accuracy on the test set an 
independent test set progressively tended to zero   as the randomness in the data increased but what 
was really interesting was what they observed when   they looked at the performance on the training 
set and this is what they found they found that   no matter how much they randomized the labels the 
model was able to get close to 100 accuracy on the   training set and what this highlights is that in a 
very similar way to the statement of the universal   approximation theorem it gets at this idea that 
deep neural networks can perfectly fit to any   function even if that function is associated with 
entirely random data driven by random labeling so to draw really drive this point home i 
think the best way to consider and understand   neural networks is as very very good function 
approximators and all the universal approximation   theorem states is that neural networks are very 
good at this right so let's suppose here we have   some data points and we can learn using a neural 
network a function that approximates this this   data and that's going to be based on sort of a 
maximum likelihood estimation of the distribution   of that data what this means is that if we give 
the model a new data point shown here in purple   we can expect that our neural network is going 
to predict a maximum likelihood estimate for that   data point and that estimate is probably going 
to lie along this function but what happens now   if i extend beyond this in distribution region to 
now out of domain regions well there are really   no guarantees on what the data looks like in 
this region in these regions and therefore we   can't make any statements about how our model 
is going to behave or perform in these regions   and this is one of the greatest limitations 
that exist with modern deep neural networks   so there's a revision here to this 
statement about neural networks being really   excellent function approximators they're 
really excellent function approximators   when they have training data and this 
also raises the question of what happens   in these out-of-distribution regions where the 
network has not seen training examples before   how do we know when our network doesn't know 
is not confident in the predictions it's making building off this idea i think there can be this 
conception that can be amplified and inflated   by the media that deep learning is basically 
alchemy right it's this magic cure it's this be   all and all solution that can be applied to any 
problem i mean its power really seems awesome and   i'm almost certain that was probably a draw 
for you to attend and take this course but   you know if we can say that deep learning 
algorithms are sort of this be all   all convincing uh solution that can be applied 
to any arbitrary problem or application there's   this also resulting idea and belief that you can 
take some set of training data apply some network   architecture sort of turn the crank on your 
learning algorithm and spit out excellent results   but that's simply not how deep learning works your 
model is only going to be as good as your data   and as the adage in the community goes if you 
put garbage in you're going to get garbage out   i think an example that really highlights 
this limitation is the one that i'm going   to show you now which emphasizes just how much 
these neural network systems depend on the data   they're trained with so let's say we have this 
image of a dog and we're going to pass it into   a cnn based architecture where our goal is to try 
to train a network to take a black and white image   and colorize it what happened to this image of 
a dog when it was passed into this model was   as follows take a cl close look at this result if 
you'll notice under the nose of the dog there's   this pinkish region in its fur which probably 
doesn't make much sense right if if this was just   a natural dog but why could this be the case why 
could our model be spitting out this result well   if we consider the data that may 
have been used to train the network   it's probably very very likely that amongst 
the thousands upon thousands of images of   dogs that were used to train such a model the 
majority or many of those images would have   dogs sticking their tongues out 
right because that's what dogs do   so the cnn may have mapped that region under the 
mouth of the dog to be most likely to be pink   so when it saw a dog that had its mouth closed 
it didn't have its tongue out it assumes in a way   right or it's it's built up representation is such 
that it's going to map that region to a pink color   and what this highlights is that deep learning 
models build up representations based on the   data they've seen and i think this is a really 
critical point as you go out you know you've taken   this course and you're interested in applying 
deep learning perhaps to some applications   and problems of interest to you your model is 
always going to be only as good as your data and this also raises a question of how do 
neural networks handle data instances where   that they have not encountered before and 
this i think is highlighted uh very potently   by this infamous and tragic example from a 
couple years ago where a car from tesla that   was operating autonomously crashed while operating 
autonomously killing the driver and it turned out   that the driver who was the individual killed 
in that crash had actually reported multiple   instances in the weeks leading up to the crash 
where the car was actually swiveling towards   that exact same barrier into which it crashed why 
could it have been doing that well it turned out   that the images which were representative 
of the data on which the car's autonomous   system was trained the images from that region 
of the freeway actually lacked new construction   that altered the appearance of that barrier 
recent construction such that the car   before it crashed had encountered a data instance 
that was effectively out of distribution and it   did not know how to handle this situation 
because i had only seen particular bear a   particular style and architecture of the barrier 
in that instance causing it tragically to crash   and in this instance it was a a occurrence where 
a neural network failure mode resulted in the loss   of human life and this points these sorts of 
failure modes points to and motivate the need   for really having systematic ways to understand 
when the predictions from deep learning models   cannot be trusted in other words when it's 
uncertain in its predictions and this is a very   exciting and important topic of research in deep 
learning and it's going to be the focus of our   first spotlight talk this notion of uncertainty 
is definitely very important for the deployment   of deep learning systems and what i like 
to think of as safety critical applications   things like autonomous driving things like 
medicine facial recognition right as these   algorithms are interfacing more and more with 
human life we really need to have principled ways   to ensure their robustness uncertainty metrics 
are also very useful in cases where we have to   rely on data sets that may be imbalanced 
or have a lot of noise in present in them   and we'll consider these different use 
cases further in the spotlight lecture   all right so before as a preparation for 
tomorrow's spotlight lecture i'd like to   give a bit of an overview about what uncertainties 
we need and what uncertainties we can talk about   when considering deep learning algorithms so 
let's consider this classification problem where   we're going to try to build a neural network that 
models probabilities over a fixed set of classes   so in this case we're trying to train a neural 
network on images of cats images of dogs and   then output whether a new image has a cat or has 
a dog right keep in mind that um the probabilities   of cat and dog have to sum to one so what happens 
when we now train our model we're ready to test it   and we have an image that contains both a cat and 
a dog still the network is going to have to output   class probabilities that are going to sum to one 
but in truth this image has both a cat and a dog   this is an instance of what we can think about 
as noise or stochasticity that's present in the   data if we train this model on images of cats 
alone or dogs alone a new instance that has   both a dog and a cat is noisy with respect to 
what the the model has seen before uncertainty   metrics can help us assess the noise that's the 
statistical noise that's inherent in the data   and present in the data and this is called 
data uncertainty or alliatoric uncertainty   now let's consider another case let's take our 
same cat dog classifier and input now an image   of a horse to this classifier again the output 
probabilities are going to have to sum to one   but even if the network is predicting that this 
image is most likely containing a dog we would   expect that it should really not be very confident 
in this prediction and this is an instance where   our model is now being tested on an image that's 
totally out of distribution an image of a horse   and therefore we're going to expect that 
it's not very confident in its prediction   this type of uncertainty is a different type 
of uncertainty than that data uncertainty   it's called model or epistemic uncertainty and it 
reflects how confident a given prediction is very   very important for understanding how well neural 
networks gener generalized to out of distribution   regions and how they can report on their 
performance in out-of-distribution regions and   in the spotlight lecture you'll really take a deep 
dive into these ideas of uncertainty estimation   and explore some emerging approaches to actually 
learn neural network uncertainties directly the third failure mode i'd like to 
consider is one that i think is super fun   and also in a way kind of scary and 
that's this idea of adversarial examples   the idea here is we take some input example for 
example this image of a temple and a standard cnn   trained on you know a set of images is 
going to classify this particular image   as a temple with 97 probability we then take that 
image and we apply some particular perturbations   to that image to generate what we call an 
adversarial example such that if we now feed   this perturbed example to that same cnn it 
no longer recognizes that image as a temple   instead it incorrectly classifies this image 
as an ostrich which is kind of mind-boggling   right so what was it about this perturbation that 
actually achieved this complete adversarial attack   what is this perturbation doing remember that when 
we train neural networks using gradient descent   our our task is to take some objective j and try 
to optimize that objective given a set of weights   w an input x and a prediction y and our goal 
and what we're asking in doing this gradient   descent update is how does a small change in 
the weights decrease the loss specifically how   can we perturb these weights in order to minimize 
the loss the objective we're seeking to minimize in order to do so we train the network with a 
fixed image x and a true label y and perturb   only the weights to minimize the loss with 
adversarial attacks we're now asking how can   we modify the input image in order to increase the 
error in the network's prediction therefore we're   trying to predict to perturb the input x in some 
way such that when we fix the set of weights w   and the true label y we can then increase the 
loss function to basically trip the network up   make it make a mistake this idea of adversarial 
perturbation was recently extended by a group   here at mit that devised an algorithm that could 
actually synthesize adversarial examples that were   adversarial over any set of transformations 
like rotations or color changes and they   were able to synthesize a set of 2d adversarial 
attacks that were quite robust to these types   of transformations what was really cool was 
they took this a step further to go beyond 2d   images to actually synthesize physical 
objects 3d objects that could then be   used to fool neural networks and this was the 
first demonstration of adversarial examples that   actually existed in the real physical world so the 
example here these turtles that were 3d printed   adversarial to be adversarial were incorrectly 
classified as rifles when images of those turtles   were taken again these are real physical 
objects and those images were then fed into   a classifier so a lot of interesting questions 
raised in terms of what how can we guarantee   the robustness and safety of deep learning 
algorithms to such adversarial attacks which can   be used perhaps maliciously to try to perturb the 
systems that depend on deep learning algorithms the final limitation but certain that i'd like 
to introduce in this lecture but certainly not   the final limitation of deep learning overall is 
that of algorithmic bias and this is a topic and   an issue that deservingly so has gotten a lot of 
attention recently and it's going to also be the   focus of our second hot topic lecture and this 
idea of algorithmic bias is centered around the   fact that neural network models and ai systems 
more broadly are very susceptible to significant   biases resulting from the way they're built the 
way they're trained the data they're trained on   and critically that these biases can lead to very 
real detrimental societal consequences so we'll   discuss this issue in tomorrow's spotlight talk 
which should be very exciting so these are just   some of many of the limitations of neural networks 
and this is certainly not an exhaustive list and   i'm very excited to again re-emphasize that we're 
going to focus in on two of these limitations   uncertainty and algorithmic bias in 
our next two upcoming spotlight talks   all right for the remainder 
of this talk this lecture   i want to focus on some of the really exciting 
new frontiers of deep learning that are being   targeted towards tackling some of these 
limitations specifically this problem of   neural networks being treated as like black box 
systems that uh lack sort of domain knowledge and   structure and prior knowledge and finally 
the broader question of how do we actually   design neural networks from scratch 
does it require expert knowledge   and what can be done to create more generalizable 
pipelines for machine learning more broadly   all right the first new frontier that we'll delve 
into is how we can encode structure and domain   knowledge into deep learning architectures to take 
a step back we've actually already seen sort of an   example of this in our study of convolutional 
neural networks convolutional neural networks   cnns were inspired by the way that visual 
processing is thought to work in in the brain   and cnns were introduced to try to capture spatial 
dependencies in data and the idea that was key to   enabling this was the convolution operation 
and we saw and we discussed how we could use   convolution to extract local features present 
in the data and how we can apply different sets   of filters to determine different features and 
maintain spatial invariance across spatial data this is a key example of how the structure of 
the problem image data being defined spatially   inspired and led to a advance in encoding 
structure into neural network architecture   to really tune that architecture specifically for 
that problem and or class of problems of interest moving beyond image data or sequence data the 
truth is that all around us there are there   are data sets and data problems that have 
irregular structures in fact there can be a   the paradigm of graphs and of networks is one 
where there's a very very high degree of rich   structural information that can be encoded in a 
graph or a network that's likely very important   to the problem that's being considered but it's 
not necessarily clear how we can build a neural   network architecture that could be well suited to 
operate on data that is represented as a graph so   what types of data or what types of examples could 
lead naturally to a representation as a graph well   one that we're all too immersed in and 
familiar with is that of social networks   beyond this you can think of state machines which 
define transitions between different states in   a system as being able to be represented 
by a graph or patterns of human mobility   transportation chemical molecules where you can 
think of the individual atoms in the molecule   as nodes in the graph connected by the bonds 
that connect those atoms biological networks   and the commonality to all these instances and 
graphs as a structure more broadly is driven by   this appreciation for the fact that there are so 
many real world data examples and applications   where there is a structure that can't be readily 
captured by a simple a simpler data encoding like   an image or a temporal sequence and so we're going 
to talk a little bit about graphs as a structure   that can provide a new and non-standard 
encoding for a series of of of problems all right to see how we can do this and to build 
up that understanding let's go back to a network   architecture that we've seen before we're 
familiar with the cnn and as you probably know   and i hope you know by now in cnn's we 
have this convolutional kernel and the   way the convolutional operation in cnn layers 
works is that we slide this rectangular kernel   over our input image such that the kernel can 
pick up on what is inside and this operation   is driven by that element-wise multiplication 
and addition that we reviewed previously   so stepping through this if you have an 
image right the the convolutional kernel is   effectively sliding across the image applying 
its filter its set of weights to the image   going on doing this repeatedly and repeatedly 
and repeatedly across the entirety of the image   and the idea behind cnns is by designing these 
filters according to particular sets of weights   we can pick up on different types of 
features that are present in the data   graph convolutional networks operate on 
a very using a very similar idea but now   instead of operating on a 2d image the network is 
operating on data that's represented as a graph   where the graph is defined by nodes shown here in 
circles and edges shown here in lines and those   edges define relationships between the nodes 
in the graph the idea of of how we can extract   information from this graph is very similar in 
principle to what we saw with cnns we're going   to take a kernel again it's just a weight matrix 
and rather than sliding that kernel across the 2d   the 2d matrix representation of our image that 
kernel is going to pop around and travel around   to different nodes in the graph and as it does so 
it's going to look at the local neighborhood of   that node and pick up on features relevant to the 
local connectivity of that node within the graph   and so this is the graph convolution operation 
where we now learn to the the network learns to   define the weights associated with 
that filter that capture the edge   dependencies present in the graph so let's 
step through this that weight kernel is going   to go around to different nodes and it's 
going to look at its emergent neighbors   the graph convolutional operator is going to 
associate then weights with each of the edges   present and is going to apply those weights across 
the graph so the kernel is then going to be moved   to the next node in the graph extracting 
information about its local connectivity   so on applying to all the different nodes in the 
graph and the key as we continue this operation   is that that local information is going to be 
aggregated and the neural network is going to   then learn a function that encodes that local 
information into a higher level representation   so that's a very brief and intuitive introduction 
hopefully about graph confirmation on neural   networks how they operate in principle and it's 
a really really exciting network architecture   which has now been enabling 
enormously powerful advances   in a variety of scientific domains for example 
in chemical sciences and in molecular discovery   there are a class of graph neural networks 
called message passing networks which have been   very successfully deployed on 2d two-dimensional 
graph-based representations of chemical structures   and these message passing networks build up a 
learned representation of the atomic and chemical   bonds and relationships that are present in a 
chemical structure these same networks based on   graph neural networks were very recently applied 
to discover a novel antibiotic a novel drug   that was effective at killing resistant bacteria 
in animal models of bacterial infection i think   this is an extremely exciting avenue for research 
as we start to see these deep learning systems   and neural network architectures being applied 
within the biomedical domain another recent and   very exciting application area is in mobility and 
in traffic prediction so here we can take streets   represent them as break them up to represent 
them as nodes and model the intersections and the   regions of the street network by a graph where the 
nodes and edges define the network of connectivity   and what teams have done is to build up 
this graph neural network representation   to learn how to predict traffic patterns across 
road systems and in fact this modeling can   result in improvements in how well estimated 
time of arrivals can be predicted in things   and interfaces like google maps another very 
recent and highly relevant example of graph   neural networks is in forecasting the spread of 
covin-19 disease and there have been groups that   have looked into incorporating both geographic 
data so information about where a person lives and   is located who they may be connected to as well as 
temporal data information about that individual's   movement and trajectory over time and using 
this as the input to graph neural networks and   because of the spatial and temporal component to 
this data what has been done is that the graph   neural networks have been integrated with temporal 
embedding components such that they can learn   to forecast the spread of the covid19 disease 
based not only on spatial geographic connections   and proximities but also on temporal patterns 
another class of data that we may encounter is   that of three-dimensional data three-dimensional 
sets of points which are often referred to   as point clouds and this is another domain in 
which the same idea of graph neural networks is   enabling a lot of powerful advances 
so to appreciate this you will first   have to understand what exactly these 
three-dimensional data sets look like   these point clouds are effectively unordered sets 
of data points in space a cloud of points where   there's some underlying spatial dependence 
between the points so you can imagine having   these sort of point-based representations of 
a three-dimensional structure of an object   and then training a neural network on these 
data to do many of the same types of of tasks   and problems that we saw in our computer vision 
lecture so classification taking a point cloud   identifying that as an object as a particular 
object segmentation taking a point cloud   segmenting out instances of that point cloud 
that belong to particular objects or particular   content types we what we can do is we can extend 
graph convolutional networks to be able to operate   to point clouds the way that's done which i 
think is super awesome is by taking a point cloud   expanding it out and dynamically computing a graph 
using the meshes inherent in the point cloud and   this is example is shown with this this structure 
of a rabbit where we're starting here from the   point cloud expanding out and then defining the 
local connectivity uh across this 3d mesh and   therefore we can then apply graph convolutional 
networks to sort of maintain invariances about   the order of points in 3d space and also still 
capture the local geometries of such a data system all right so hopefully that gives you a sense 
of different types of ways we can start to   think about encoding structure internal 
neural network architectures moving beyond   the architectures that we saw in the first five 
lectures for the second new frontier that i'd like   to focus on and discuss in the remainder of this 
talk it's this idea of how we can learn to learn   and i think this is a very 
powerful and thought-provoking   domain within deep learning research and 
it spawns some interesting questions about   how far and how deep we can push the 
capabilities of machine learning and ai systems the motivation behind this field of what is now 
called automated machine learning or auto ml   is the fact that standard deep neural network 
architectures are optimized for performance on   a single task and in order to build a new model we 
require sort of domain expertise expert knowledge   to try to define a new architecture that's going 
to be very well suited for a particular task the   idea behind automated machine learning is that 
can we go beyond this this tuning of of you know   optimizing a particular architecture robustly 
for a single task can we go beyond this to build   broader algorithms that can actually learn what 
are the best models to use to solve a given   problem and what we mean in terms of best model 
or which model to use is that its architecture   is optimal for that problem the hyper parameters 
associated with that architecture like the number   of layers it has the number of neurons per layer 
those are also optimized and this whole system   is built up and learned via an a a 
algorithm this is the idea of automl   and in the original automl work which stands 
for automated machine learning the original work   used a framework based on reinforcement learning 
where there was a neural network that is   referred to as a controller and in this case this 
controller network is a recurrent neural network   the controller what it does is it proposes a 
sample model architecture what's called the child   architecture and that architecture is going 
to be defined by a set of hyper parameters   that resulting architecture is can then be trained 
and evaluated for its performance on a particular   task of interest the feedback of the performance 
of that child network is then used as sort of the   reward in this reinforcement learning framework 
to try to promote and inform the controller   as to how to actually improve its network 
proposals for the next round of optimization   so this cyclic process is repeated thousands upon 
thousands of times generating new architectures   testing them giving that feedback to 
the controller to build and learn from   and eventually the controller is going to tend 
towards assigning high probabilities to hyper   parameters and regions of the architecture 
search space that achieve higher accuracies   on the problem of interest and will assign low 
probability to those areas of the search space   that perform poorly so how does this agent 
how does this controller agent actually work   well at the broad view at the macro scale it's 
going to be a rnn based architecture where   at each step each iteration of this pipeline the 
model is this controller model is going to sample   a brand new network architecture and that this 
controller network is specifically going to be   optimized to predict the hyper parameters 
associated with that spawned child network   so for example we can consider the 
optimization of a particular layer   that optimization is going to involve prediction 
of hyper parameters associated with that layer   like as for a convolutional layer the size 
of the filter the length of the stride   and so on and so forth then that resulting 
network that child network that's spawned and   defined by these predicted hyper parameters 
is going to be tested trained and tested   such that after evaluation we can take the 
resulting accuracy and update the recurrent   neural network controller system based on how 
well the child network performed on our task   that rnn controller can then learn to create an 
even better model and this fits very nicely into   the reinforcement learning framework where the 
agent of our controller network is going to be   rewarded and updated based on the performance 
of the child network that it spawns this idea   has now been extended to a number of different 
domains for example recently in the context of   image recognition with the same principle of a 
controller network that spawns a child network   that's then tested evaluated to improve the 
controller was used to design a optimized   neural network for the task of image 
recognition in this paradigm of designing this   designing an architecture can be thought of as 
neural architecture search and in this work the   controller system was used to construct and design 
convolutional layers that were used in an overall   architecture tested on image recognition tasks 
this diagram here on the left depicts what that   learned architecture of a convolutional cell in 
a convolutional layer actually looked like and   what was really really remarkable about this work 
was when they evaluated was the results that they   found when they evaluated the performance of 
these neural network designed neural networks   i know that's kind of a mouthful but let's 
consider those results so first here in black   i'm showing the accuracy of the state-of-the-art 
human-designed convolutional models on an image   recognition task and as you can appreciate 
the accuracy shown on the y-axis scales with   the number of parameters in the millions shown 
on the x-axis what was striking was when they   compared the performance of these human-designed 
models to the models spawned and returned by the   automl algorithm shown here in red these neural 
designed neural architectures achieved superior   accuracy compared to the human-designed 
systems with relatively fewer parameters   this idea of using machine learning using deep 
learning to then learn more general systems or   more general paradigms for predictive modeling and 
decision making is a very very powerful one and   most recently there's now been a lot of emerging 
interest in moving beyond automl and neural   architecture search to what we can think of more 
broadly as auto ai an automated complete pipeline   for designing and deploying machine learning 
and ai models which starts from data curation   data pre-processing to model selection and design 
and finally to deployment the idea here is that   perhaps we can build a generalizable pipeline 
that can facilitate and automatically   accelerate and design all steps of this process   i think this idea spawns a very very 
thought-provoking point which is can we   build ai systems that are capable of generating 
new neural networks designed for specific tasks   but the higher order ai system that's built is 
then sort of learning beyond a specific task   not only does this reduce the need for us as 
experienced engineers to try to hand design   and optimize these networks it also makes these 
deep learning algorithms more accessible and more   broadly we start to get at this consideration of 
what it means to be creative what it means to be   intelligent and when alexander introduced this 
course he spoke a little bit about his thoughts   on what intelligence means the ability to take 
information using it to inform a future decision   and as humans our learning pipeline is definitely 
not restricted to optimization for a very specific   task our ability to learn and achieve and 
solve problems impacts our ability to learn   completely separate problems are and 
improves our analytical abilities the   models and the neural network 
algorithms that exist today   are certainly not able to extend to this point 
and to capture this phenomena of generalizability   i think in order to reach the point of true 
artificial intelligence we need to be considerate   of what that true generalizability 
and problem-solving capability means   and i encourage you to think about this this 
point to think about how automl how auto ai   how deep learning more broadly falls into 
this broader picture of the intersection   and the interface between artificial and human 
intelligence so i'm going to leave you with that   as a point of reflection for you at this 
point in the course and beyond with that   i'm going to close this lecture and remind 
you that we're going to have a software   lab and office hour session we're going to be 
focusing on providing support for you to finish   the final lab on reinforcement learning 
but you're always welcome to come   discuss with us ask your questions discuss with 
your classmates and teammates and for that we   encourage you to come to the class gather town 
and i hope to see you there thank you so much 

hi everyone and welcome back to today's lecture in the first six lectures of 6s191 we got a taste of some of the foundational deep learning algorithms and models in the next two lectures we're going to actually be diving into a lot more detail focusing specifically on two critically important hot topic areas in modern deep learning research that have really impacted everything that we have been learning so far in this class now these two hot topic lectures are going to focus first on probabilistic modeling with deep neural networks for uncertainty estimation in this lecture as well as algorithmic bias and fairness in the next lecture now today we'll learn about a very powerful new technique called evidential deep learning for learning when we can trust the output of the neural networks that we're training and interpret when they're not confident or when they are uncertain in their predictions now this whole field of uncertainty estimation in deep learning is super important today more than ever as we're seeing these deep learning models like we've been learning about so far in this class start to move outside of the lab and into reality interacting with or at the very least impacting the lives of humans around them traditional deep learning models tend to propagate biases from training and are often susceptible to failures on brand new out-of-distribution data instead we need models that can reliably and quickly estimate uncertainty in the data that they are seeing as well as the outputs that they're predicting in this lecture we'll actually start to learn about evidential deep learning for uncertainty estimation of neural networks so training our model not only just to make a prediction and predict an answer but also to understand how much evidence it has in that prediction how much we should trust its answer a big reason and actually cause for uncertainty estimation for uncertainty in deep learning is due to the very large gap in how neural networks are trained in practice and how they're evaluated in deployment in general when we're training machine learning models we make the following assumption that our training set is drawn from the same distribution as our test set but in reality this is rarely true for example when we look at many of the state-of-the-art state-of-the-art algorithms that we've learned about so far they are almost all trained on extremely clean pre-processed data sets often with minimal occlusions minimal noise or ambiguity in the real world though we are faced with so many edge cases of our model that our model is going to be completely unequipped to handle for example if we're training this classifier to identify dogs and we train it on these clean images of dogs on the left hand side it will yield very poor performance when we take it into the real world and we start showing it dogs in brand new positions dogs in upside down configurations or even this parachuting dog coming through the sky or if we take this driving model trained on clean urban streets and then we take it out into the real world and starts to see all kinds of strange unexpected edge cases in reality now the point of today's lecture is to build models that are not only super accurate and have high performance but also to build quantitative estimation techniques into our learning pipeline such that our model will be able to tell us when it doesn't know the right answer now there's this famous quote here by george box that i've adapted a bit to convey this now in the end all models are wrong but some that know when they can be trusted are actually going to be useful in reality now very rarely do we actually need a model to achieve a perfect 100 accuracy but even if our accuracy is slightly lower if we can understand when we can trust the output of that model then we have something extremely powerful now the problem of knowing when we don't know something turns out to be extremely difficult though and this is true even for humans there are so many tasks that we believe we can achieve even though we don't really know the right answer uh and are probably more likely to sorry are more likely to fail than to succeed now this picture probably doesn't even need an explanation for anyone who has driven in a new city before we had things like google maps in our phones there's often this tendency to refuse to accept that you might be lost and ask for help even though we may truly be lost in a location that we've never been in before so today we're going to learn about how we can teach neural networks to predict probability distributions instead of purely deterministic point outputs and how formulating our neural networks like this can allow us to model one type of uncertainty but not all types and specifically we'll then discuss how it does not capture one very important form of uncertainty which is the uncertainty in the prediction itself finally we'll see how we can learn neural representations of uncertainty using evidential deep learning which will allow us to actually capture this uh this other type of uncertainty and estimate it quickly while also scaling to some very high dimensional learning problems and high dimensional output problems let's start and discuss what i mean when i say that we want to learn uncertainties using neural networks and what this term of probabilistic learning and how it what is this term of probabilistic learning and how does it tie into everything that we've been seeing so far in this class now in the clay in the case of supervised learning problems there have been two main sources of data input to our model the first is the data itself we've been calling this x in the past and it's what we actually feed into the model given our data we want to predict some target here denoted as y this could be a discrete class that x belongs to it could also be any real number that we want to forecast given our data either way we're going to be given a data set of both x and y pairs and have been focused so far in this class on really in the case of at least supervised learning learning a mapping to go from x to y and predict this expected value of y given our inputs x this is exactly how we've been training deterministic supervised neural networks in the past classes now the problem here is that if we only model the expectation of our target the expectation of why then we only have a point estimate of our prediction but we lack any understanding of how spread or uncertain this prediction is and this is exactly what we mean when we talk about estimating the uncertainty of our model instead of predicting an answer on average the expectation of y we want to also estimate the variance of our predicted target y this gives us a deeper and more probabilistic sense of the output and you might be thinking when i'm saying this that this sounds very very similar to what we have already seen in this lecture in in this class and well you would be completely right because in the very first lecture of this course we already had gotten a big sense of training neural networks to output full distributions for the case of classification specifically so we saw an example where we could feed in an image to a neural network and that image needed to be classified into either being a cat or being a dog for example now each output here is a probability that it belongs to that category and both of these probabilities sum to one or must sum to one since it's a probability distribution our output is a probability distribution now this is definitely one example of how neural networks can be trained to output a probability distribution in this case a distribution over discrete class categories but let's actually dive into this a bit further and really dissect it and see how we're able to accomplish this well first we had to use this special activation function if you recall this was called the softmax activation function we had to use this activation function to satisfy two constraints on our output first was that each of the probability outputs had to be greater than zero and second that we needed to make sure that the sum of all of our class probabilities was normalized to one now given an output of class probabilities emerging from this softmax activation function we could then define this special loss that allowed us to optimize our distribution learning we could do this by minimizing what we called the negative log likelihood of our predicted distribution to match the ground truth category distribution now this is also called the cross entropy loss which is something that all of you should be very familiar with now as you've implemented it and used it in all three of your software labs let's make some of this even more formal on why we made some of these choices with our activation function and our loss function well it really all boils down to this assumption that we made before we even started learning and that was that we assumed that our target class labels y are drawn from some likelihood function in this case a categorical distribution defined by distributional parameters p the distributional parameters here actually define our distribution and our likelihood over that predicted label specifically the probability of our answer being in the i class is exactly equal to the ith probability parameter now similarly we also saw how we could do this for the case of continuous class targets as well in this case where we aren't learning a class probability but instead a probability distribution over the entire real number line like the classification domain this can be applied to any supervised learning problem we saw when we saw it in the previous lectures we were focusing in the case of reinforcement learning where we want to predict the steering wheel angle that a vehicle should take given a raw image pixel image of the scene now since the support of this output is continuous and infinite we cannot just output raw probabilities like we did in the case of classification because this would require an infinite number of outputs coming from our network instead though we can output the parameters of our distribution namely the mean and the standard deviation or the variance of that distribution and this defines our probability density function our mean is unbounded so we don't need to constrain it at all on the other hand though our standard deviation sigma must be strictly positive so for that we can use an exponential activation function to enforce that constraint and again in the classification domain we are similar to the classification domain we can optimize these networks by using a negative log likelihood loss and again how did we get to this point well we assumed we made this assumption about our labels we assumed that our labels were drawn from a normal distribution or a gaussian distribution with known parameters mu and sigma squared our mean and our variance which we wanted to train our model to predict to output now i think this is actually really amazing because we don't have any ground truth uh variables in our data set for ground truth means and ground truth variances all we have are ground truth wives our labels but we use this formulation and this loss function to learn not a point estimate of our label but a distribution a full distribution a gaussian around describing that data likelihood now we can summarize the details of these likelihood estimation problems using neural networks for both the discrete classification domain as well as the regression the continuous regression domain now fundamentally the two of these domains differ in terms of the type of target that they're fitting to in the classification domain the targets can be one of a fixed set of classes one to k in the regression domain our target can be any real number now before getting started we assume that our labels came or were drawn from some underlying likelihood function in the case of classification again they were being drawn from a categorical distribution whereas in the case of regression we assume that they were being drawn from a normal distribution now each of these likelihood functions are defined by a set of distributional parameters in the case of categorical we have these probabilities that define our categorical distribution and in the case of a normal or a gaussian distribution regression we had a mean and a variance to ensure that these were valid probability distributions we had to apply some of the relevant constraints to our parameters through way of activation functions cleverly constructed activation functions and then finally for both of these like we previously saw we can optimize these entire systems using the negative log likelihood loss this allowed us to learn the parameters of a distribution over our labels while we do have access to the probability and the variance here it is critically important for us to remember something and that is that probability or likelihood let's call it that we get by modeling the problem like we have seen so far in this lecture should never be mistaken for the confidence of our model these probabilities that we obtain are absolutely not the same as confidence what we think of as confidence at least and here's why let's go back to this example of classification where we feed in an image it can be either cats or dogs to this neural network and we have our neural network predict what is the probability that this image is of a cat or what probability that it is of a dog now because if we feed in an image of let's say a cat our model should be able to identify assuming it's been trained to identify some of the key features specific to cats and say okay this is very likely a cat because i see some cat-like features in this image and likewise if i fed in a dog input then we should be more confident in predicting a high probability that this image is of a dog but what happens if we say that let's feed in this image of both a cat and a dog together in one image our model will identify some of the features corresponding to the successful detection of a cat as well as a successful detection of a dog simultaneously within the same image and it will be relatively split on its decision now this is not to say that we are not confident about this answer we could be very confident in this answer because we did detect both cat features and dog features in this image it simply means that there's some ambiguity in our input data leading to this uncertainty that we see in our output we can be confident in our prediction even though our answer we are answering with a probability or a percentage of 50 percent cat and 50 dog because we're actually because we we're actually training on images that share these types of features but what about in cases where we are looking at images where we did not train on these types of features so for example if we take that same neural network but now we feed in this image of a boat something completely new unlike anything that we have seen during training the model still has to output two things the probability that this image is of a cat and the probability that this image is of a dog and we know that because this is a probability distribution trained with the soft max activation function we know that this is a categorical distribution both of these probabilities probability of cat plus probability of dog must sum to one no matter what thus the output likelihoods in this scenario will be severely unreliable if our input is unlike anything that we have ever seen during training and we call this being out of distribution or out of the training distribution and we can see that in this case our outputs or of these two probabilities are unreasonable here they should not be trusted now this is really to highlight to you that when we say uncertainty estimation there are different types of uncertainty that we should be concerned about while training neural networks like this captures probabilities they do not capture the uncertainty in the predictions themselves but rather the uncertainty in the data this brings into question what are the different types of uncertainty that exist and how can we learn them using different algorithmic techniques in deep learning and what this all boils down to is the different ways that we can try to estimate when the model doesn't know or when it is uncertain of its answer or prediction we saw that there are different types of these uncertainties and this is true even in our daily lives i think a good way to think about this is through this two by two matrix of knowns and unknowns and i'll give a quick example just illustrating this very briefly so imagine you are in an airport for a flight you have some known knowns for example that there will be some flights taking off from that airport that's a known known you're very confident and you're very certain that that will happen there are also things like known unknowns things that we know uh we know there are things that we simply cannot predict for example we may not know when our flight the exact time that our flight is going to take off that's something that we cannot predict maybe because it could get delayed or it's just a it's just something that we don't have total control over and can possibly change then there are unknown knowns things that others know but you don't know a good example of that would be someone else's scheduled departure time their scheduled flight time you know that someone else knows their scheduled departure time but to you that's an unknown known and finally there are unknown unknowns these are completely unexpected or unforeseeable events a good example of this would be a meteor crashing into the runway now this is an emerging and exciting field of research in fundamental machine learning understanding how we can build algorithms to robustly and efficiently model and quantify uncertainties of these deep learning models and this is really tough actually because these models have millions and millions billions and now even trillions of parameters uh and understanding and introspecting them inspecting their insides uh to estimate understanding when they are going to not know the correct answer is definitely not a straightforward problem now people do not typically train neural networks to account for these types of uncertainties so when you train on some data for example here you can see the observations in black we can train a neural network to make some predictions in blue and the predictions align with our observations inside this region where we had training data but outside of this region we can see that our predictions start to fail a lot and estimating the uncertainty in situations like this comes in two forms that we're going to be talking about today the first form is epistemic uncertainty which models the uncertainty in the underlying predictive process this is when the model just does not know the correct answer and it's not confident in its answer the other form of uncertainty is aliatoric uncertainty this is uncertainty in the data itself think of this as statistical or sensory noise this is known as this is known as irreducible uncertainty so since no matter how much data that you collect you will have some underlying noise in the collection process it is inherent in the data itself the only way to reduce alliatoric uncertainty is to change your sensor and get more accurate data now we care a lot about both of these forms of uncertainty both alliatoric uncertainty and epistemic uncertainty and again just to recap the differences between these two forms of uncertainty we have aliatoric on the left-hand side this focuses on the statistical uncertainty of our data it describes how confident we are in our data itself it's highest when our data is noisy and it cannot be reduced by adding more data on the other hand we have epistemic uncertainty this is much more challenging to estimate than aliatoric uncertainty and there are some emerging approaches that seek to determine epistemic uncertainty now the key is that epistemic uncertainty reflects the model's confidence in the prediction we can use these estimates to begin to understand when the model cannot provide a reliable answer and when it's missing some training data to provide that answer unlike aliatoric uncertainty epistemic uncertainty can be reduced by adding more data and improving that confidence of the model so while alliatoric uncertainty can be learned directly using neural networks using likelihood estimation techniques like we learned about earlier in this lecture today epistemic uncertainty is very challenging to estimate and this is because epistemic uncertainty reflects the uncertainty inherent to the model's predictive process itself with a standard neural network a deterministic neural network we can't obtain a sense of this uncertainty because the network is deterministic with a given set of weights passing in one input to the model multiple times will yield the same output over and over again it's going to always have the same fixed output no matter how many times we feed in that same input but what we can do instead of having a deterministic neural network where each weight is a deterministic number a single number for each weight we can represent every single weight by a probability distribution so in this case we're going to actually model each of these weights by a distribution such that when we pass in an input to our neural network we sample from the point in this distribution for every single weight in the neural network this means that every time we feed in an input to the model we're going to get out a slightly different output depending on the samples of our weights that we we realized that time that we fed it through the model these models are called bayesian neural networks these model distributions likelihood functions over the network weights themselves so instead of modeling a single number for every weight bayesian neural networks try to learn neural networks that capture a full distribution over every single weight and then use this distribution to actually learn the the uncertainty of our model the epistemic uncertainty of our model now we can formulate this epistemic uncertainty and formulate this learning of bayesian neural networks as follows while deterministic neural networks learn this fixed set of weights w bayesian neural networks learn a posterior distribution over the weights this is a probability of our weights given our input data and our labels x and y now these are called bayesian neural networks because they formulate this posterior probability of our weights given our data using bayes rule they actually write it out like this using bayes rule however in practice this posterior is intractable to compute analytically it's not possible for any real or non-toy examples which means that we have to resort to what are called sampling techniques to approximate and try to estimate this posterior so we can approximate this posterior through sampling where the idea is to make multiple stochastic evaluations through our model each using a different sample of our weights now this can be done many different ways one way to do this approximation is using a technique that we've learned about in class already called dropout typically dropout is used during training but now we're talking about using dropout during testing time to obtain these multiple samples through our network so the way this works is nodes or neurons are dropped or dropped out or not dropped out based on the value of a bernoulli random variable with some probability p that we define as part of our dropout procedure each time we feed our input our same input through the model depending on which nodes are dropped in and out we're going to get a slightly different output and that's going to each one of those outputs is going to represent a different sample through our network now alternatively we can sample a different way using an ensemble of independently trained models each of these will learn a unique set of weights after seeing a unique set of training data or potentially the same training data just shown in a different order or sequence in both of these cases though what we're doing here is very similar we're drawing a set of t samples of our weights from and using these to compute t forward passes using either dropout or t models in an ensemble this allows us to formulate two terms one is the expectation of our prediction y as well as the variance of our prediction over the course of these forward passes so if our variance over these predictions is very large so if we take t stochastic forward passes through the model and we have a very large variance if none of our outputs really agree with each other this is a great indicator that our model has a high epistemic uncertainty and in fact it's not only an indicator that it has high fsm uncertainty this is the epistemic uncertainty the variance of our prediction over these t stochastic forward passes while these sampling based approaches are very commonly used techniques for epistemic uncertainty estimation they do have a few very notable downsides and limitations first as you may have realized from the fact that they draw multiple samples to approximate this weight distribution this means that they require running the model multiple times except t times just to obtain their predictions for ensembles it's even worse than this because you have to initialize and train multiple independent models which is extremely computationally costly then you have to store them in memory and repeatedly run them in memory as well repeatedly relatedly this imposes a memory constraint due to having to actually keep all of these models in parallel on on your network on your computer and together this means that these sampling approaches are not very efficient which is a significant limitation for applications where uncertainty estimation is necessary to be made in real time on edge devices for example on robotics or other mobile devices and the last point i'd like to make is that bayesian approaches abrasion approximation methods such as dropout tend to produce typically overconfident uncertainty estimates which may be problematic also in safety critical domains where we really need calibrated uncertainty estimates and we really prefer consider ourselves uncertain rather than confident so the backup case we don't want to assume in the backup case that we are confident that we know what we're doing when we don't really know what we're doing but what are sampling approaches really trying to do when they try to approximate this uncertainty let's look at this in a bit more detail and to answer this question let's suppose we're working within the context again of self-driving cars since this is a very nice and intuitive example we have a model that's looking to predict the steering wheel angle of the car given this raw image that it sees on the left the mean here it's predicting two things a mean and a variance the mean here is the angle that the wheel should be the variance is the alliatoric uncertainty the data uncertainty if you remember now consider the epistemic uncertainty this is the model uncertainty that we said was really difficult to capture as we've seen with sampling based approaches we can we can compute the epistemic uncertainty using ensemble of many independently trained instances of this model so we could take one model and obtain its estimates of mu and sigma squared and we can plot for this given image where that model believes mu and sigma squared should be on this two-dimensional graph on the right on the x-axis is mu on the axis is sigma squared and we can place exactly where that network believes the output should be in this two-dimensional space and we can repeat this for several different models each model we take its output and we can plot it in this space and over time if we do this for a bunch of models we can start to estimate the uncertainty by taking the variance across these predictions to get a metric of uncertainty intuitively if we get a huge variance here a huge variance over our muse if the answers are said differently if the answers are very spread out this means that our model is not confident on the other hand if our answers are very close to each other this is a great indicator that we are very confident because even across all these different models that we independently trained each one is getting a very similar answer that's a good indicator that we are confident in that answer indeed this is the epistemic or the model uncertainty in fact these estimates these these estimates that are coming from individually trained models are actually being drawn from some underlying distribution now we're starting to see this distribution shape up and the more samples that we take from that distribution the more it will start to appear like this background distribution that we're seeing capturing this distribution though instead of sampling if we could just capture this distribution directly this could allow us to better and more fully understand the model uncertainty so what if instead of drawing samples from this distribution approximating it we just tried to learn the parameters of this distribution directly now this approach this is the approach that has been taken by an emerging series of uncertainty quantification methods called evidential deep learning which consider learning as an evidence acquisition process evidential deep learning tries to enable direct estimation of both alliatoric uncertainty as well as epistemic uncertainties by trying to learn what what we call these higher order distributions over the individual likelihood parameters in this case over the parameters of mu and sigma we try to learn a distribution on top of them so to understand this consider how different types of degrees of uncertainties may manifest in these evidential distributions when we have very low uncertainty like we can see on the left hand side the spread along our mean or mu and our sigma squares is going to be very small we're going to have very concentrated density right in a specific point this indicates that we have low uncertainty or high confidence when we have something like high aleatoric uncertainty on the other hand we might see high values of sigma squared which we can actually represent by an increase along the y axis the sigma squared axis of this plot here represented in the middle finally if we have high epistemic uncertainty we'll have very high variability in the actual values of mu that are being returned so you can see this along spread along the mu axis now evidential distributions allow us to capture each of these modes and the goal is to train a neural network now to learn these types of evidential distributions so let's take a look at how we can do evidential learning specifically in the case of regression first now we call these distributions over the likelihood parameter evidential distributions like i've been referring to them now the key thing to keep in mind when trying to wrap your head around these evidential distributions is that they represent a distribution over distributions if you sample from an evidential distribution you will get back a brand new distribution over your data how we can formulate this is is uh using or how can we actually formulate these evidential distributions well first let's consider like i said the case of continuous learning problems like regression we assume like we saw earlier in the class that our target labels y are going to be drawn from some normal distribution with parameters distribution parameters mu and sigma squared this is exactly like we saw earlier in the class no different the key here is that instead of before when we assumed that mu and sigma were known things that our network could predict now let's say that we don't know mu and sigma and we want to probabilistically estimate those as well we can formally we can formalize this by actually placing priors over each of these parameters each of these distribution parameters so we assume that our distribution parameters here mu and sigma squared are not known and let's place parameters over each of these and try to probabilistically estimate them so drawing from mu will we can draw mu from a normal parametrized as follows and we can draw sigma squared our variance from an inverse gamma parameterized as follows here using these new hyperparameters of this evidential distribution now what this means is that mu and sigma squared are now being drawn from this normal inverse gamma which is the joint of these two priors the normal inverse gamma distribution is going to be parametrized by a different set of parameters gamma epsilon alpha and beta now this is our evidential distribution or what we call our evidential prior it's a distribution this normal inverse gamma defined by our model parameters that gives rise when we sample it when we sample from our evidential distribution we're actually getting back individual realizations of mu and sigma squared these are their own individual gaussians defining this original distribution on the top line over our data itself y so we call these evidential distributions since they have greater density in areas where there is more evidence in support of a given likelihood distribution realization so on the left hand side or in the middle here you can see an example of this evidential distribution one type of this evidential distribution with this normal inverse gamma prior over mu and sigma squared both parameters of the gaussian distribution which is placed over our data our likelihood function but you can see on the top left hand corner now different realizations of mu and sigma over this space of this evidential distribution then correspond to distinct realizations of our likelihood function which describes the distribution of our target values y so if we sample from any point in this evidential distribution we're going to receive back a mu and a sigma squared that defines its own gaussian that we can see on the right hand side we can also consider the analog of evidential learning in the case of classification once again keep in mind that if you sample from an evidential distribution you get a brand new distribution over the data so for classification our target labels y are over a discrete set of classes of k classes to be specific we assumed that our class labels were drawn from a likelihood function of the categorical form parameterized by some probabilities now in this case we can probabilistically estimate those distribution parameters p using what is called a dirichlet prior the dirichlet prior is itself parametrized by a set of concentration parameters here called alpha again per class so there are k alpha parameters in this dirichlet distribution and when we sample from this dirichlet we're going to receive realizations of our distributional parameters distributional probabilities defining our categorical loss function again it's this hierarchy of distributions let's take a look at the simple example of what this evidential distribution in the case of classification looks like here we have three possible classes in this case the probability mass of our dirschlag distribution will live entirely on this triangular simplex sampling from any point within this triangle corresponds to sampling a brand new categorical probability distribution for example let's say we sample from the center of this simplex this will correspond to equal probabilities over the three classes so for those who have not seen the simplex plot before imagine the corners of these triangles represent perfect prediction in one of the classes so sampling from the middle of the simplex corresponds to equal probabilities of each of the three classes being at one of the corners like i said corresponds to all of the mass being on one of those classes and zero mass being on the other two and anywhere else in this simplex corresponds to a sampled categorical distribution that is defined by these class probabilities that have to sum to one the color inside of this triangle this gradient of blues that you can see provides one example of how this mass can be distributed throughout the simplex where the categorical distributions can be sampled more frequently so this is one example representing that the majority of the mass is placed in the center of the simplex but the whole power of evidential learning is that we're going to try to learn this distribution so our network is going to try to predict what this distribution is for any given input so this distribution can change and the way we're going to sample our categorical likelihood functions will as a result also change so to summarize here is a breakdown of evidential distributions for both regression and classification in the regression case the targets take continuous values from real numbers we assumed here that the targets are drawn from a normal distribution parameterized by mu and sigma and then we had in turn a higher order evidential distribution over these likelihood parameters according to this normal inverse gamma distribution in the case of classification the targets here represented a set of k or were shown actually here over a set of k independent classes here we assumed that our likelihood of observing a particular class label y was drawn from a categorical distribution of class probabilities p and this p was drawn from a higher order evidential dirichlet distribution parameterized by alphas now here's a also a quick interesting side note you may be asking yourself why we chose this specific evidential distribution in each of these cases why did we pick the dirichlet distribution why did we pick the normal inverse gamma distribution there are many distributions out there that we could have picked for our likelihoods picked over our likelihoods rather but we chose these very special forms because these are known as what are called conjugate priors picking them to be of this form makes analytically computing our loss tractable specifically if our prior or evidential distribution p of theta is of the same family of our likelihood p of y given theta then we can analytically compute this integral highlighted in yellow as part of our loss during training which makes this whole process feasible so with this formulation of these evidential distributions now let's consider concretely how we can build and train models to learn these evidential distributions and learn and use them to estimate uncertainty what is key here is that the network is trained to actually output the parameters of these higher order evidential distributions so in the case of regression we're predicting gamma epsilon alpha and beta in the case of classification we're predicting a vector of k alphas where k is the number of classes that we have and once we have these parameters we can directly formulate estimates for each of the alliatoric and epistemic uncertainties and that is determined by the resulting distributions over these likelihood parameters we optimize these distributions by incorporating both maximization of our model fit and minimization of wrong evidence into the objective for regression this is manifested as follows where our maximization captures the likelihood of seeing data given the likelihood parameters which are controlled by this evidential prior parameters here denoted as m the minimization of incorrect evidence is captured in this regularization term on the right hand side by minimizing we seek to lower the incorrect evidence in instances where model where the model is making errors so think of the right hand side that's really fitting all of our evidential distributions to our data and the left-hand side is inflating that uncertainty when we see that we get some incorrect evidence and start to make some errors in training we can evaluate this method actually on some simple toy example learning problems where we're given some data points in the distribution and some some regions in our in our scene here where we seek to predict the target value or in some cases the class label in regression we have this case where we try to fit to our data set where we have data in the middle white region but we don't have data on the two edge regions and we can see that our evidential distributions are able to inflate the uncertainty in the regions where we are out of distribution which is exactly what we want to see that we're able to recognize that those predictions where we don't have data should not be trusted similarly in the case of classification operating on the mnist data set we can generate out of distribution examples by synthetically rotating the handwritten digits so here along the bottom side you can actually see an example one digit being rotated from left to right being rotated and you can see the uncertainty of our evidential distribution increasing in this out of distribution regime where the one no longer is even closely resembling a one but the uncertainty really drops down on the two end points where the one comes back into shape of a true one evidential learning can also be applied in much more complex high dimensional learning applications as well recently it's been demonstrated to learn neural networks to output thousands of evidential distributions simultaneously while learning to quantify pixel-wise uncertainty of monocular depth estimators given only raw rgb inputs this is a regression problem since the predicted depth of each pixel is a real number it's not a class but in the case of classification evidential deep learning has also been applied to perform uncertainty aware semantic segmentation of raw lidar point clouds also extremely high dimensional where every point in the point cloud must be predicted must predict which object or what type of class it belongs to evidential deep learning allows us to not only classify each of these points as an object but also recognize which of these objects in the scene express a form of an object that we don't know the answer to so evidential deep learning really gives us the ability to express a form of i don't know when it sees something in its input that it doesn't know how to predict confidently and it's able to let the user know when its prediction should not be trusted now to start wrapping up i wanted to provide a brief comparison of all of the different types of approaches of uncertainty estimation that we've learned about today and how evidential neural networks fit into these there were three techniques that we touched on today starting first with manila likelihood estimation of our over our data then moving to bayesian neural networks and finally exploring evidential neural networks each of these methods really has its own differences strengths and advantages at the highest level of difference we saw that fundamentally each of these methods placed probabilistic priors over different aspects of the pipeline over the data in the case of the likelihood estimators that we saw very early on in the lecture over the weights in the case of bayesian neural networks and over the likelihood for function itself in the case of evidential neural networks unlike bayesian neural networks though evidential neural networks are very fast and very memory efficient since they don't require any sampling to estimate their uncertainty and even though both methods capture a form of epistemic uncertainty this is one huge advantage that means that you don't need to train an ensemble of models you can train just one model and you only need to run it once for every single input there's no sampling required so in summary in this lecture we got to dive deep into uncertainty estimation using neural networks this is a super important problem in modern machine learning as well as as we as really start to deploy our models into the real world we need to quickly be able to understand when we should trust them more and more importantly when we should not we learned about some of the different forms of uncertainty and how these different methods can help us capture both uncertainty in the data as well as uncertainty in the model and finally we got to have some insight in how we can use evidential deep learning to learn fast and scalable calibrated representations of uncertainty using neural networks thank you for attending this lecture in the next lecture we're going to be going through another very impactful topic in today's world focusing on ai bias and fairness and seeing some strategies for mitigating adverse effects of these models so we look forward to seeing you for that lecture as well thank you 

hi everyone welcome to our second hot topic lecture in 6s191 where we're going to learn about algorithmic bias and fairness recently this topic is emerging as a truly pervasive issue in modern deep learning and ai more generally and it's something that can occur at all stages of the ai pipeline from data collection all the way to model interpretation in this lecture we'll not only learn about what algorithmic bias is and how it may arise but we will also explore some new and exciting methodological advances where we can start to think about how we can build machines capable of identifying and to some degree actually mitigating these biases the concept of algorithmic bias points to this observation that neural networks and ai systems more broadly are susceptible to significant biases such that these biases can lead to very real and detrimental societal consequences indeed today more than ever we are already seeing this manifesting in society from everything from facial recognition to medical decision making to voice recognition and on top of this algorithmic bias can actually perpetuate existing social and cultural biases such as racial and gender biases now we're coming to appreciate and recognize that algorithmic bias in deep learning is a truly pervasive and severe issue and from this we really need strategies on all levels to actually combat this problem to start first we have to understand what exactly does algorithmic bias actually mean so let's consider this image what do you see in this image how would you describe it well the first thing you may say to describe this image is watermelon what if i tell you to look closer and describe it in more detail okay maybe you'll say watermelon slices or watermelon with seeds or other descriptors like juicy watermelon layers of watermelon watermelon slices next to each other but as you were thinking about this to yourself i wonder how many of you thought to describe this image as red watermelon if you're anything like me most likely you did not now let's consider this new image what is in this image here now you're probably much more likely to place a yellow descriptor when describing this watermelon your top answer is probably going to be yellow watermelon and then with slices with seeds juicy etc etc but why is this the case and why did we not say red watermelon when we saw this original image well when we see an image like this our tendency is to just think of it as watermelon rather than red watermelon and that's because of our own biases for example due to geography that make us used to seeing watermelon that look like this and have this red color as this represents the prototypical watermelon flesh that i expect to see but perhaps if you're from another part of the world where the yellow watermelon originated from you could have a different prototypical sense of what color watermelons should be this points to this broader fact about how we as humans go about perceiving and making sense of the world in all aspects of life we tend to label and categorize things as a way of imposing order to simplify and make sense of the world and as a result what this means is that for everything there's generally going to be some typical representation what we can think of as a prototype and based on the frequencies of what each of us observe our tendency is going to be to point out things that don't fit what we as individuals consider to be the norm those things that are atypical to us for example the yellow watermelon for me and critically biases and stereotypes can arise when particular labels which may not necessarily be the minority label can confound our decision making whether that's human-driven or suggested by an algorithm and in this lecture we're going to focus on sources of algorithmic bias and discuss some emerging approaches to try to combat it to do that let's first consider how exactly bias can and does manifest in deep learning and ai one of the most prevalent examples of bias in deep learning that we see today is in facial detection and recently there have been a couple of review analyses that have actually evaluated the performance of commercial facial detection and classification systems across different social demographics for example in an analysis of gender classifiers this review showed that commercial pipelines performed significantly worse on faces of darker females relative to other demographic groups and another analysis which considered facial recognition algorithms again found that error rates were highest on female faces of color this notion of algorithmic bias can manifest in a myriad of different ways so as another example let's consider the problem of image classification generally and let's say we have a trained cnn and this image on the left here which shows a prototypical example of a bride in some north american and european countries now in recent analysis when this particular image of a bride was passed into a cnn that was trained on a open source large scale image data set the predicted data class labels that were outputted by the cnn were perhaps unsurprisingly things like bride dress wedding ceremony women as expected however when this image which is a prototypical example of a bride in other parts of the world such as in south asia was passed into that very same cnn now the predicted classroom labels did not in fact reflect the ground truth label of bride clothing event costume art as you can see nothing here about a bride or a wedding or even a human being so clearly this is a very very significant problem and this is not at all the desired or expected behavior for something that deep learning we may think of has solved quote-unquote image classification and indeed the similar behavior as what i showed here was also observed in another setting for object recognition when again this image of spices which was taken from a home in north america was passed in to a cnn trained to do object detection object detection and recognition the labels for the detected objects in this image were as expected seasoning spice spice rack ingredient as we'd expect now again for this image of spices shown now on the left which was in fact taken from a home in the philippines when that image was fed into that very same cnn once again the predicted labels did not reflect the ground truth label that this image was an image of spices again um pointing to something really really concerning going on now what was really interesting about this analysis was they asked okay not only do we observe this bias but what could be the actual drivers and the and the reasons for this bias and it turned out from this analysis that the accuracy of the object recognition model actually correlated with the income of the homes where the test images were taken and generated this points to a clear bias in these algorithms favoring data from from homes of higher incomes versus those from lower incomes why could this be what could be the source for this bias well it turns out that the data that was used to train such a model the vast majority of it was taken from the united states canada and western europe but in reality this distribution does not at all match the distribution of the world's population given that the bulk of the world's population is in east and south asia so here i think this is a really telling and powerful example because it can show it shows how bias can be perpetuated and exist on multiple levels in a deep learning or ai pipeline and this particular analyses sort of started to uncover and unearth some of those biases and indeed as i mentioned bias can truly poison all stages of the ai development and life cycle beginning with the data where imbalances with respect to class labels or even features can result in unwanted biases to the model itself to the actual training and deployment pipeline which can reinforce and perpetuate biases to evaluation and the types of analyses that are and should be done to evaluate fairness and performance across various demographics and subgroups and finally in our human interpretation of the results and the outcomes and the decisions from these ai systems where we ourselves can inject human error and impose our own biases that distort the meaning and interpretation of such results so in today's lecture we're going to explore this problem of algorithmic bias both in terms of first different manifestations and sources of this bias and we'll then move to discuss different strategies to mitigate each of these biases and to ultimately work towards improving uh fairness of ai algorithms and by no means is this is is this a solved problem in fact the setup and the motivation behind this lecture is to introduce these topics so we can begin to think about how we can continue to advance this field forward all right so let's start by thinking about some common types of biopsies that can manifest in deep learning systems i think we can broadly categorize these as being data driven or interpretation driven on the data-driven side we can often face problems where data are selected such that proper randomization is not achieved or particular types of data or features in the data are represented more or less frequently relative to others and also instances in which the data that's available to us as users does not reflect the real world likelihood of particular instances occurring all of these as you'll see and appreciate are very very intertwined and very related interpretation driven biases refer more to issues in how human interpretation of results can perpetuate some of these types of problems for example in with respect to falsely equating correlation and causation trying to draw more general conclusions about the performance or the generalization of a ai system even in the face of very limited test data and finally in actually favoring or trusting decisions from an algorithm over that of a human and we're going to um by by no means is this this survey of common biases that can exist an exhaustive list it's simply meant to get you thinking about different ways and different types of biases that can manifest so today we're going to touch on several of these types of biases and i'd first like to begin by considering interpretation driven issues of correlation fallacy and over generalization all right so let's suppose we have this plot that as you can see shows trends in two variables over time and as you as you notice these the data from these two variables are tracking very well together and let's say and it turns out that in fact these black points show the number of computer science phds awarded in the united states and we could easily imagine building a machine learning pipeline that can use these data to predict the number of computer science doctorates that would be awarded in a given year and specifically we could use the red variable because it's it seems to correlate very well with the number of cs doctorates to try to as our input to our machine learning model to try to predict the black variable and ultimately what we would want to do is you know train on a particular data set from a particular time frame and test in the current time frame 2021 or further beyond to try to predict the number of computer science phds that are going to be awarded well it turns out that this red variable is actually the total revenue generated by arcades in a given year and it was a variable that correlated with the number of computer science doctorates over this particular time frame but in truth it was obscuring some underlying causal factor that was what was ultimately driving the observed trend in the number of computer science doctorates and this is an instance of the correlation fallacy and the correlation fallacy can actually result in bias because a model trained on data like this generated revenue generated by arcades as input computer science doctorates as the output could very very easily break down because it doesn't actually capture the fundamental driving force that's leading to this observed trend in the variable that we're ultimately trying to predict so correlation fallacy is not just about correlation not equating to causation it can also generate and perpetuate bias when wrongfully or incorrectly used let's also consider the assumption of over generalization so let's suppose we want to train a cnn on some images of mugs from some curated internal data set and take our resulting model and deploy in the real world to try to predict and identify mugs well mug instances in the real world are likely to not be very similar to instances on which the model was trained and the over-generalization assumption and bias means that or reflects the fact that our model could perform very well on select manifestations of mugs those that are similar to the training examples it's seen but actually fail and show performance poor performance on mugs that were represented less significantly in data although we expect it to generalize and this phenomena is can be often thought of and described as distribution shift and it can truly bias networks to have worse performance on examples that it has not encountered before one recent strategy that was recently proposed to try to mitigate this source of bias is to start with the data set and try to construct an improved data set that already account for potential distribution shifts and this is done for example by specifying example sets of say images for training and then shifting with respect to a particular variable to construct the test data set so for example in this instance the distribution shift that occurs between the train and the test is with respect to the time and the geographic region of the images here or in the instance of medical images this could mean sourcing data from different hospitals for each of train validation and test and as greater awareness of this issue of distribution shift um is brought to light data sets like this could actually help try to tame and tune back the generalization bias that can occur because they inherently impose this necessity of already testing your model on a distribution shifted series of examples all right so that gives you hopefully a sense of interpretation driven biases and why they can be problematic next we're going to turn most of our attention to to what are in my opinion some of the most pervasive sources and forms of bias in deep learning which are driven by class or feature imbalances that are present in the data first let's consider how class imbalances can lead to bias let's consider some example data set as shown here or some some example data shown here and let's say that this plot on the left shows the real world distribution of that data that we're trying to model with respect to some series of classes and let's suppose that the data that is available to us in that data the frequency of these classes is actually completely different from what occurs in the real world what is going to be the resulting effect on the model's accuracy across these classes will the model's accuracy reflect the real world distribution of data no what instead is going to happen is that the model's accuracy can end up biased based on the data that it has seen specifically such that it is biased towards improved or or greater accuracies rather on the more frequently occurring classes and this is definitely not desired what is ultimately desired is we want the resulting model to be unbiased with respect to its performance its accuracy across these various classes the accuracies across the classes should be about the same and if our goal is then to train a a model that exhibits fair performance across all these classes in order to understand how we can achieve that we first need to understand why fundamentally class imbalance can be problematic for actually training the model to understand the root of this problem let's consider a simple binary classification task and let's suppose we have this data space and our task is to build a classifier that sees points somewhere in this data space and classifies them as orange or blue and we begin in our learning pipeline by randomly initializing the classifier such that it divides this up the space now let's suppose we start to see data points they're starting to be fed into the model but our data set is class imbalanced such that for every one orange point the model sees it's going to see 20 blue points now the process of learning as you know from from gradient descent is that incremental updates are going to be made to the classifier on the basis of the data that it has observed so for example in this instance after seeing these blue points the shift will be to try to move the decision boundary according to these particular observations and that's going to occur now we've made one update more data is going to come in and again they're all going to be blue points again due to this 1 to 20 class imbalance and as a result the decision boundary is going to move accordingly and so far the random samples that we have seen have reflected this underlying class imbalance but let's suppose now we see an orange data point for the first time what's going to happen to that decision boundary well it's going to shift to try to move the decision boundary closer to the orange point to account for this new observation but ultimately remember this is only one and one orange point and for every one orange point we're going to see 20 blue points so in the end our classifier's decision boundary is going to end up occupying more of the blue space since it will have seen more blue samples and it will be biased towards the majority class so this is a very very simplified example of how learning can end up skewed due to stark class imbalances and i assure you that class imbalance is a very very common problem which you almost certainly will encounter when dealing with real world data that you will have to process and curate and in fact one setting in which class imbalance is particularly relevant is in medicine in healthcare and this is because that the incidence of many diseases such as cancer is actually relatively rare when you look at the general population so to understand why this could be problematic and why this is not an ideal setting for training and learning let's imagine that we want to try to build a deep learning model to detect the presence of cancer from medical images like mri scans and let's suppose we're working with a brain tumor called glioblastoma which is the most aggressive and deadliest brain tumor that exists but it's also very rare occurring at a incidence of approximately 3 out of every 100 000 individuals our task is going to be to try to train a cnn to detect glioblastoma from mri scans of the brain and let's suppose that the class incidence in our data set reflected the real-world incidence of disease of this disease meaning that for a data set of 100 000 brain scans only three of them actually had brain tumors what would be the consequences on the model if it was trained in this way remember that a classification model is ultimately being trained to optimize its classification accuracy so what this model could basically fall back towards is just predicting healthy all the time because if it did so it would actually reach 99.997 percent accuracy even if it predicted healthy for instances when it saw a brain tumor because that was the rate at which healthy occurred in its data set obviously this is extremely problematic because the whole point of building up this pipeline was to detect tumors when they arise all right so how can we mitigate this to understand this we're going to discuss two very common approaches that are often used to try to achieve class balance during learning let's again consider our simple classification problem where we randomly initialize our classifier dividing our data space the first approach to mitigate this class in balance is to select and feed in batches that are class balanced what that means is that we're going to use data batches that exhibit a one-to-one class ratio now during learning our classifier is going to see equal representation with respect to these classes and move the decision boundary accordingly and once again the next batch that comes in is again going to be class balance and the decision boundary will once again be updated and our end result is going to be a quite a reasonable decision boundary that divides the space roughly equally due to the fact that the data the model has seen is much more informative than what would have been seen with starkly imbalanced data and in practice this balanced batch selection is an extremely important technique to try to alleviate this issue another approach is to actually weight the likelihood of individual data points being selected for training according to the inverse of the frequency at which they occur in the data set so classes that are more frequent will have lower weights classes that are less frequent will have higher weights and the end result is that we're going to produce a class balance data set where different classes will ultimately contribute equally to the model's learning process another way we can visualize this rewaiting idea is by using the size of these data points to reflect their probability of selection during training and what example waiting means is that we can increase the probability that rare classes will be selected during training and decrease the probability that common classes will be selected so so far we have focused on the issue of class balance class imbalance and discuss these two approaches to mitigate class imbalance what if our classes are balanced could there still be biases and imbalances within each class absolutely to get at this let's consider the problem where we're trying to train a facial detection system and let's say we have an equal number of images of faces and non-faces that we can use to train the model still there could be hidden biases that are lurking within each class which in fact may be even harder to identify and even more dangerous and this could reflect a lack of diversity in the within class feature space that is to say the underlying latent space to this data so continuing on with the facial detection example one example of such a feature may be the hair color of the individuals whose images are in our face class data and it turns out that in the real world the ground truth distribution of hair color is about 75 to 80 percent of the world's population having black hair eighteen to twenty percent having brown hair two to five percent having blonde hair and approximately two percent having red hair however some gold standard data sets that are commonly used for image classification and face detection do not reflect this distribution at all in that they're over representing brown and blonde hair and under representing black hair and of course in contrast to this a perfectly balanced data set would have equal representation for these four hair colors i'll say here that this is a deliberate and over deliberate oversimplification of the problem and in truth all features including hair color will exist on a spectrum a smooth manifold in data space and so ideally what we'd ultimately like is a way that we can capture more subtlety about how these features are distributed across the data manifold and use that knowledge to actively de-bias our deep learning model but for the purpose of this example let's continue with the simplified view and let's suppose we take this gold standard data set and use it to train a cnn for facial detection what could end up occurring at test time is that our model ends up being biased with respect to its performance across these different hair color demographics and indeed as i introduced in the beginning of this lecture these exact same types of biases manifest quite strongly in large-scale commercial-grade facial detection and classification systems and together i think these result in considerations raise the critical question of how can we actually identify potential biases which may be hidden and not overly obvious like skin tone or hair color and how can we actually integrate this information into the learning pipeline from there going be a step beyond this how can learning pipelines and techniques actually use this information to mitigate these biases once they are identified and these two questions introduce an emerging area of research within deep learning and that's in this idea of using machine learning techniques to actually improve fairness of these systems and i think this can be done in two principle ways the first is this idea of bias mitigation and in this case we're given some bias model data set learning pipeline and here we want to try to apply a machine learning technique that is designed to remove aspects of the signal that are contributing to unwanted bias and the outcome is that this bias is effectively mitigated reduced along the particular axis from which we remove the signal resulting in a model with improved fairness we can also consider techniques that rather than trying to remove signal try to add back signal for greater inclusion of underrepresented regions of the data space or of particular demographics to ultimately increase the degree to which the model sees particular slices of the data and in general this idea of using learning to improve fairness and improve equitability is an area of research which i hope will continue to grow and advance in the future years as these as these problems gain more traction all right so to discuss and understand how learning techniques can actually mitigate bias and improve fairness we first need to set up a few definitions and metrics about how we can actually formally evaluate the bias or fairness of a machine or deep learning model so for the sake of these examples we'll consider the setting of supervised learning specifically classification a classifier should ultimately produce the same output decision across some series of sensitive characteristics or features given what it should be predicting therefore moving from this we can define that a classifier is biased if its decision changes after it is exposed to particular sensitive characteristics or feature inputs which means it is fair with respect to particular variable z if the classifier's output is the same whether we condition on that variable or not so for example if we have a single binary variable z the likelihood of the prediction being correct should be the same whether or not z equals 0 or z equals 1. so this gives a a framework for which we can think about how to evaluate the bias of a supervised classifier we can do this we can take this a step further to actually define performance metrics and evaluation analyses to determine these degrees of bias and fairness one thing that's commonly done is to measure performance across different subgroups or demographics that we are interested in this is called disaggregated evaluation so let's say if we're working with colored shapes this could be with respect to the color feature keeping shape constant or the shape feature keeping color constant we can also look at the performance at the insert intersections of different subgroups or demographics which in our shape and color example would mean simultaneously considering both color and shape and comparing performance on blue circles against performance on orange squares and so on and so forth so together now that we've defined what a fair um super what a fair classifier would look like and also some ways we can actually evaluate bias of a classification system we now have the framework in place to discuss some recent works that actually used deep learning approaches to mitigate bias in the context of supervised classification so the first approach uses a multi-task learning setup and adversarial training in this framework the way it works is that we the human users need to start by specifying an attribute z that we seek to devise against and the learning problem is such that we train we're going to train a model to jointly predict and output y as well as the value of this attribute z so given a particular input x the the network is going to this is going to be passed in to the network via embedding and hidden layers and at the output the network will have two heads each corresponding to one of the prediction tasks the first being the prediction of the target label y and the second being the prediction of the value of the sensitive attribute that we're trying to devise against and our goal is to be to try to remove any confounding effect of the sensitive attribute on the outcome of the task prediction decision this effect removal is done by imposing an adversarial objective into training specifically by negating the gradient from the attribute prediction head during back propagation and the effect of this is to remove any confounding effect that that attribute prediction has on the task prediction when this model was proposed it was first applied to a language modeling problem where the sensitive attribute that was specified was gender and the task of interest was this problem of analogy completion where the goal is to predict the word that is likely to fill an analogy for example he is the she as doctor is to blank and when a biased model was tested on this particular analogy the top predictions it returned were things like nurse nanny fiance which clearly suggested a potential gender bias however a de-biased model employing this multi-task approach with specification of gender as the attribute was more likely to return words like pediatrician or physician examples or synonyms for doctor which suggested some degree of mitigation of the gender bias however one of the primary limitations of this approach is this requirement for us the human user to specify the attribute to devise against and this can be limiting in two ways first because there could be hidden and unknown biases that are not necessarily apparent from the outset and ultimately we want to actually actually also devise against these furthermore by specifying what the sensitive attribute is we humans could be inadvertently propagating our own biases by way of telling the model what we think it is biased against and so ultimately what we want and what we desire is an automated system that could try to identify and uncover potential biases in the data without any annotation or specification and indeed this is a perfect use case for generative models specifically those that can learn and uncover the underlying latent variables in a data set and in the example of facial detection if we're given a data set with many many different faces we may not know what the exact distribution of particular latent variables in this data set is going to be and there could be imbalances with respect to these different variables for example face pose skin tone that could end up resulting in unwanted biases in our downstream model and as you may have seen in working through lab 2 using generative models we can actually learn these latent variables and use this information to automatically uncover underrepresented and over-represented features and regions of the latent landscape and use this information to mitigate some of these biases we can achieve this by using a variational auto encoder structure and in recent work we showed that based on this va network architecture we can use this to learn the underlying latent structure of a data set in a completely unbiased unsupervised manner for example picking up in the case of of face images particular latent variables such as orientation which were once again never specified to the model it picked up and learned this as a particular latent variable by looking at a lot of different examples of faces and recognizing that this was an important factor from this learned latent structure we can then estimate the distributions of each of these learned latent variables which means the distribution of values that these latent variables can take and certain instances are going to be over represented so for example if our data set has many images of faces of a certain skin tone those are going to be overrepresented and thus the likelihood of selecting a particular image that has this particular skin tone during training will be unfairly high which could result in unwanted biases in favor of these types of faces conversely faces with rare features like shadows darker skin glasses hats may be under represented in the data and thus the likelihood of selecting instances with these features to actually train the model will be low resulting in unwanted bias from this uncovering of the distribution of latent structure we showed that this model could actually adaptively adjust sampling probabilities of individual data instances to re-weight them during the training process itself such that these latent distributions and this resampling approach could be used to adaptively generate a more fair and more representative data dataset for training to dig more into the math behind how this resampling operation works the key point to this approach is that the latent space distribution is approximated by this joint histogram over the individual latent variables specifically we estimate individual histograms for each of the individual latent variables and for the purpose of this approximation assume that these latent variables are independent such that we can take their product to arrive at a joint estimate a an estimate of the joint distribution across the whole latent space and based on this estimated joint distribution we can then define the adjusted probability for sampling a particular data point x during training based on the latent space for that input instance x specifically we define the probability of selecting that data point according to the inverse of the approximated joint distribution across latent space which is once again defined by each of these individual histograms and furthermore weighted by a devicing parameter alpha which tunes the degree of debian that is desired using this approach and applying it to facial detection we showed that we could actually we could actually increase the probability of resampling for faces that had underrepresented features and this qualitatively manifested when we inspected the top faces with the lowest and highest resampling probabilities respectively we then could deploy this approach to actually select batches during training itself such that batches sampled with this learned debian algorithm would be more diverse with respect to features such as skin tone pose and illumination and the power of this approach is that it conducts this resampling operation based on learn features that are automatically learned there's no need for human annotation of what the attributes or biases should be and thus it's more generalizable and also allows for de-biasing against multiple factors simultaneously to evaluate how well this algorithm actually mitigated bias we tested on a recent benchmark data set for evaluation of facial detection systems that is balanced with respect to the male and female sexes as well as skin tone and to determine the degree of bias present we evaluated performance across subgroups in this data set grouped on the basis of the male female annotation and the skin tone annotation and when we considered the performance first of the model without any devising so the supposedly bias model we observed that it exhibited the lowest accuracy on dark males and the highest accuracy on light males with around a 12 difference between the two we then compared this accuracy to that of the d bias models and found that with increasing de-biasing the accuracy actually increased overall and in particular on the subsets and subgroups such as dark males and dark females and critically the difference in accuracy between dark males and light male faces decreased substantially with the debias model suggesting that this approach could actually significantly decrease categorical bias to summarize in today's lecture we've explored how different biases can arise in deep learning systems how they manifest and we also went beyond this to discuss some emerging strategies that actually use deep learning algorithms to mitigate some of these biases finally i'd like to close by offering some perspectives on what i think are key considerations for moving towards improved fairness of ai the first is what i like to call best practices things that should become standard in the science and practice of ai things like providing documentation and reporting on the publication of data sets as well as that of models that summarize things like training information evaluation metrics and model design and the goal of this being to improve the reproducibility and transparency of these data sets and models as they're used and deployed the second class of steps i think that need to be taken are these new algorithmic solutions to actually detect and mitigate biases during all aspects of the learning pipeline and today we consider two such approaches but there's still so much work to be done in this field to really build up to robust and scalable methods that can be seemly seamlessly integrated into existing and new ai pipelines to achieve improved fairness the third criterion i think will be improvements in terms of data set generation in terms of sourcing and representation as well as with respect to distribution shift and also formalized evaluations that can become standard practice to evaluate the fairness and potential bias of new models that are output above all i think what is going to be really critical is a sustained dialogue and collaboration between ai researchers and practitioners as well as end users politicians corporations ethicists so that there is increased awareness and understanding of potential societal consequences of algorithmic bias and furthermore discussion about new solutions that can mitigate these biases and promote inclusivity and fairness with that i'll conclude and i'd like to remind you that for those of you uh taking the course that the entries for the lab competitions are going to be due today at midnight eastern time please submit them on canvas and as always if you have any questions on this lecture the labs any other aspects of the course please come to the course gather town thank you for your attention 

thank you so i i lead uh ai at eui globally and and today we're going to talk to you about some work we've done in information extraction specifically deep cpcfg so hopefully we'll introduce some concepts that you may not have come across before and and uh before we get started maybe a couple of disclaimers the views expressed in this presentation are mine and friday's they're not necessarily those of our employer and i'll let you read those other disclaimers ai is very important for ernst young many of you will be familiar with the firm we are a global organization of more than 300 000 people in more than 150 countries and we provide a range of services to our clients that include assurance consulting strategy and transformations and tax both in the context of the services that we deliver and in the context of our clients own transformations ai is incredibly important we see a huge disruption happening for many industries including our own driven by ai and because of that we're making significant investments in this area we've established a network a global network of ai research and development labs around the world as you see in various parts of the world and we also have a significant number of global ai delivery centers and we there's huge energy and passion for ai within ey and so that we have a community of more than four and a half thousand members internally and we maintain uh meaningful relationships with academic institutions including for example mit and we of course engage heavily with policy makers regulators legislators and ngos around issues related to ai some areas of particular focus for us uh the first is document intelligence which we'll talk about today and this is really the idea of re using ai to read and interpret business documents the key phrase there is business documents we're not necessarily talking about emails or web posts or social media posts or product descriptions we're talking more about things like contracts lease contracts revenue contracts employment agreements we're talking about legislation and regulation we're talking about documents like invoices and purchase orders and proofs of delivery so there's a very wide range of these kinds of documents and hundreds thousands maybe tens of thousands of different types of documents here and we see these in more than 100 languages in more than 100 countries in tens of different industries and industry segments at ui we have built some i think really compelling technology in the space we've built some products that are deployed now in more than 85 countries and used by thousands of engagements and and this space is sufficiently important to us that we helped to co-organize the first ever workshop on document intelligence at nurips in 2019 and of course we publish and patent in this area two other areas that are important to us that will not be the subject of this talk but i thought i would just allude to are transaction intelligence so the idea here is that we see many transactions uh that our clients execute and we review those transactions for tax purposes and also for audit purposes and we'll process hundreds of billions of transactions a year so it's this enormous amount of transaction data and and there's huge opportunities for machine learning and ai to help analyze those transactions to help determine for example tax or accounting treatments but also to do things like identify anomalies or unusual behavior or potentially identify fraud another area that's very important for us is trusted ai so given the role that we play in the financial ecosystem we are a provider of trust to the financial markets it's important that we help our clients and ourselves and the ecosystem at large build trust around ai and we engage heavily with academics ngos and regulators and legislators in order to achieve this but the purpose of this talk is really to talk about the purpose of this talk is really to talk about document intelligence and specifically we're going to talk about information extraction from what we call semi-structured documents things like tax forms you see the document on the screen in front of you here this is a tax form there's going to be information in this form that that is contained in these boxes and so we'll need to pull that information out these forms tend to be relatively straightforward because it's consistently located in these positions but you see to read this information or to extract this information it's not just a matter of reading text we also have to take layout information into account and there are some complexities even on this document right and there's a description of property here this is a list and so we don't know how many entries might be in this list might be one there might be two there might be more of course this becomes more complicated when these documents for example are handwritten or when they're scanned or when they're more variable like for example uh a check so many of you may have personalized checks and those checks are widely varied in terms of their background in terms of their layout typically they're handwritten and typically when we see them they're scanned often scanned at pretty poor quality and so pulling out the information there can be a challenge and again this is driven largely by the variability we have documents like invoices the invoice here is very very simple and but you'll note the key thing to know here is that there are line items and there are multiple line items each of these corresponds to an individual transaction under that invoice and there may be zero line items or there may be ten or hundreds or thousands or even in some cases tens of thousands of line items in an invoice invoices are challenging because a large enterprise might have hundreds or thousands or even tens of thousands of vendors and each one of those vendors will have a different format for their invoice and so if you try to hand code rules to extract information from these documents it tends to fail and so machine learning approaches are designed really to deal with that variation in these document types and this complex information extraction challenge let me just talk about a couple of other examples here and you'll see a couple of other problems this receipt on the left hand side is pretty typical right it's a scan document clearly it's been crumpled or creased a little bit the the information that's been entered is offset by maybe half an inch and so this customer id is not lined up with the customer id tag here and so that creates additional challenges and this document on the right hand side is a pretty typical invoice and you'll see the quality of the scan is relatively poor it contains a number of line items here and the layout of this is quite different than the invoice we saw on the previous slide or on the receipt on the left hand side here so there's lots of variability for the duration of the talk we're going to refer to this document which is a receipt and we're going to use this to illustrate our approach so our goal here is to extract key information from this receipt and so let's talk a little bit about the kinds of information we want to extract so the first kind is what we call header fields and this includes for example the date of that receipt right when when were these transactions executed it includes a receipt id um or an invoice id and it might include this total amount and these pieces of information are very important for accounting purposes make sure that we have paid the receipts and paid the invoices that we should they're important for tax purposes is this expense and taxable or not taxable have we paid the appropriate sales tax and so we do care about pulling this information out we refer to these as header fields because often they do appear at the top of the document and usually it's at the top or the bottom and but they're information that appear typically once right there's one total amount for an invoice or receipt there aren't multiple values for that and so there's a you know a fairly obvious way that we could apply deep learning to this problem we can take this document and run it through optical character recognition and optical character recognition and services or vendors have gotten pretty good and so they can produce essentially bounding boxes around tokens so they produce a number of bounding boxes and each boundary box contains a token and some of these tokens relate to the information we want to extract so this five dollars and ten cents is the total and so what we could do is we could apply deep learning to classify these bounding boxes right we can use the input to that deep learning could be this whole image it could be some context of that bounding box but we can use it to classify these bounding boxes and that can work reasonably well for this header information but there are some challenges so here for example there is this dollar amount five dollars and 10 cents there's also 4.50 60 cents 60 cents over here 1.50 if we independently classify all of those we may get multiple of them being tagged as the receiptal and how do we disambiguate those so this problem of disambiguation is fundamental and and what often happens in these systems is that there is post-processing that encodes heuristics or rules that are human engineered to resolve these ambiguities and that becomes a huge source of brittleness and this huge maintenance headache over time and we'll say more about that later the other kind of challenge we see here are fields like this vendor address so this vendor address contains multiple tokens and so we need to classify multiple tokens as belonging to this vendor address and then we have the challenges to which of those tokens actually belong to the vendor address how many of them are there and what order do we read them in to recover this address so while a straightforward machine learning approach can achieve some value it still re leaves many many problems to be resolved that are typically resolved with this hand-engineered post-processing this becomes even more challenging for line items and throughout the talk we'll emphasize line items because this is where many of the significant significant challenges arise so here we have two line items they both correspond to transactions for buying postage stamps maybe they're different kinds of postage stamp and each one will have a description posted stamps this one has a transaction number associated with the two it will have a total amount for that transaction might have a quantity you might have a unit price right so there's multiple pieces of information that we want to extract so now we need to identify where that information is we need to identify how many line items there are we need to identify which line items this information is associated with so is this 60 cents associated with this first line item or this second one and we as humans can read this and computers obviously have a much harder time especially given the variability there are thousands of different ways in which this information might be organized so this is the fundamental challenge so these are the documents we want to to read and on the other side of this are is the system of record data right typically this information will be pulled out typically by human beings and entered into some database this illustrates some kind of database schema if this was a relational database we might have two tables the first table contains the header information and the second table contains all of the line items so this is the kind of data that we might have in a system of record this is both the information we might want to extract but also uh the information that's available to us for training this system for the purposes of this talk it's going to be more appropriate to think of this uh as a document type schema think of it as json for example where we have the header information now the first three fields and this is not exactly json schema but it's meant to look like that so it's these first three fields have some kind of type information and then we have a number of line items and the number of line items isn't specified it may be zero and maybe more than one and then each one of those has has its own information so our challenge then is to extract this kind of information from those documents and the training data we have available is raw documents and this kind of information and so i want to take a little aside for a second and talk about our philosophy of deep learning and you know many people think about deep learning simply as large deep networks we have a slightly different philosophy and if you think how classical machine learning systems were built the first thing that we would do is decompose the problem into sub pieces and those sub pieces in this case might include for example something to classify bounding boxes it might include something to identify tables or extract rows and columns of tables each one of them then becomes its own machine learning problem and in order to solve that machine learning problem we have to define some learning objectives and we have to find some data and then we have to train that model and so this creates some challenges right this data does not necessarily naturally exist right we don't for these documents necessarily have annotated bounding boxes that tell us where the information is in the document it doesn't tell us which bounty boxes correspond to the information we want to extract and so in order to train in this classical approach we would have to create that data we also have to define these objectives and there may be a mismatch between the objectives we define for one piece of this problem and another and that creates friction and error propagation as we start to integrate these pieces together and then finally typically these systems have lots of post-processing at the end that is bespoke to the specific document type and is highly engineered so what happens is these systems are very very brittle if we change anything about the system we have to change many things about the system if you want to take the system and apply it to a new problem we typically have to re-engineer that post-processing and for us where we have thousands of different types of document documents in hundreds or a hundred plus languages and you know we simply cannot apply engineering effort to every single one of these problems we have to be able to apply exactly the same approach exactly the same software to every single one of them and really to me this is the core value of deep learning as a philosophy is it's about end-to-end training we have this idea that we can train the whole system end-to-end based on the problem we're trying to solve and the data we fundamentally have the natural data that we have available so again we begin by decomposing the problem into sub-pieces but we build a deep network a network component that corresponds to each of those subproblems and then we compose those networks into one large network that we train end to end and this is great because the integration problem appears once when we design this network it's easy to maintain the data acquisition problem goes away because we're designing this as an end-to-end approach to model the natural data that exists for the problem and of course there are some challenges in terms of how we design these networks and really uh that's the key challenge that arises in this case is how do we build these building blocks and how do we compose them and so we're going to talk about how we do this in this case it's about composing deep network components to solve the problem end to end so here what we do is we treat this problem as a parsing problem we take in the documents and we're going to parse them in two dimensions and this is where some of the key innovations are on parsing in two dimensions where we disambiguate the different parses using deep networks and so the deep network is going to tell us of all the potential parse trees for one of these documents which is the most probable or the most that matches the data the best and then we're going to simply read off from that parse tree the system of record data right no post processing we just literally read the parse tree and we read off that json data as output so again just uh uh we so we have these documents on the left-hand side right these input documents we run them through ocr to get the boundary boxes that contain tokens sets the input to the system and then the output of the system is this json record that describes the information we have extracted from the document right it describes the information we extracted it doesn't describe the layout of the document it just describes the information we have extracted okay so that's the fundamental approach and the machinery that we're going to use here uh is context-free grammars and context-free grammars you know anytime you want to parse you have to have a grammar to parse against context for grammars for those of you with a computer science background are really the workhorse of computer science they're the basis for many programming languages and and they're they're nice because they're relatively easy to parse we won't get into the technicalities of a context for grammar i think that the key thing to know here is that they consist of rules rules have a left-hand side and a right-hand side and the way we think about this is we can take the left-hand side and think about think of it as being made up of or composed of the right-hand side so a line item is composed of a description and a total amount the descript description can be simply a single token or it can be a sequence of descriptions right description can be multiple tokens and the way we encode that in this kind of grammar is in this recursive fashion okay so now we're going to apply this grammar to parse this kind of json and we do have to augment this grammar a little bit to capture everything we care about but still this grammar is very very simple right that's a small simple grammar really to capture the schema of the information we want to extract so now let's talk about how we parse uh a simple line item we have a simple line item it's postage stamps we have three stamps each at a dollar 50 for a total of 450. and so the first thing we do is uh for each of these tokens we replace we identify a rule where that token appears on the right-hand side and we replace it with the left hand side so if we look at this postage token we replace it by d for description we could have replaced it by t for total amount or c for count or p for price in this case i happen to know that d is the right token so i'm going to use that for illustration purposes but the key observation is that there is some ambiguity here right it's not clear and which of these substitutions is the right one to do and again this is where the deep learning is going to come in to resolve that ambiguity so the first stage of parsing is that we uh we substitute everywhere we see a right-hand side of a rule we substitute the left-hand side of the rule resolving ambiguity as we go and the next step is by construction and for technical reasons these grammars always have they either have a single token on the left-hand side or they have um a sorry this looks like maybe there's some question here and so they either have a single token uh on the right-hand side or they have two uh symbols on the right-hand side and so since we've dealt with all the tokens we're now dealing with these pairs of symbols and so we have to identify pairs of symbols that we substitute again with the left-hand side so this is description description that we substitute with with a description and and likewise for count and price we substitute with u okay so we just repeat this process and and get a full parse tree where the final symbol here is a line item so that tells us this whole thing is linear and made up of count price and total amount okay so as i said there's some ambiguity and one place for this ambiguity here is uh three and a dollar fifty how do we know that this is in fact a count and a price right this could just as easily have been a description and a description so resolving this ambiguity is hard but this is the opportunity for learning this is where the learning comes in that can learn that typically a dollar fifty is probably not part of the description it probably relates to some other information we want to extract and so that that's what we want the learning to learn and so the way we do this is we associate every rule with a score so each rule has a score and now we try to use rules that have high scores so that we produce in the end a parse tree that has a high total score so what we're actually going to do is we're going to model these scores with a deep network so for every rule we're going to have a deep network corresponding to that rule which will give the score for that rule okay now let me illustrate that on one simple example here we have a dollar fifty and this could be a description a total amount account or a price and we might intuitively think well this should be biased towards total amount or price because it's it's a monetary value but the way we're going to resolve this is we're going to apply the deep networks corresponding to each of these rules right there's four deep networks we're going to apply these deep networks and each of them will return a score and we expect that over time they will learn that the deep network for total amount will have a higher score and the network for price will have a higher score so that's fundamentally the idea at this for these bottom set of rules these um token-based rules for the more uh involved rules where we have two terms on the right-hand side we have the similar question about resolving ambiguity and so there's two i think important insights here the first is that um you know we do have this ambiguity as to how we tag these first tokens we could do cp or we could do dp but we quickly see that there is no rule that has dp on the right hand side and so the grammar itself helps to correct this error right because there is no rule that would allow this parse the grammar will correct that error and so the grammar allows us to impose some constraints about the kind of information these documents contain it allows us to encode some prior knowledge of the problem and that's really important and valuable and the second uh kind of ambiguity is where you know it is allowed but maybe it's not the right answer so in this case we cp could be replaced by a u and we're going to evaluate the model for this rule based on the left hand side of this tree and the right hand side of the tree so this is going to have two inputs the left hand side and the right hand side and and likewise for this rule which uh tries to model this as a description description and so this each of these models will have a score which will help us to disambiguate between these two choices so the question then arises how do we score a full tree and i'm going to introduce a little notation for this this is hopefully not too much notation but the idea is we're going to call the full parse tree t and we're going to denote the score for that tree by c t and i'm going to abuse notation here a little bit and re-parameterize c as having three parameters the first is the symbol at the root of the tree and the other two are the span of the sentence the span of the input covered by that tree right in this case it's the whole sentence so we use the indices 0 to n now the same notation works for a subtree right where again we the first term is the symbol at the root of that subtree and we have the indices of the span here it's not the whole sentence it just goes from i to j okay so this is the score for um for a subtree let's see if there's questions here real quick um so this is the score for um for a sub tree and we're going to define that again in terms of the deep network corresponding to the rule at the root of that subtree but also it's made up of these other sub trees so we're going to have terms that correspond to the left-hand side and the right-hand side of this tree so these are the uh we've defined this score now recursively in terms of the trees that make it up and i do see there's a question here and it says by taking into account the grammar constraints does this help the neural network learn and be more sample efficient yes exactly the point is that we know things about the problem that allow the network to be more efficient because we've applied that prior knowledge and that's really helpful in complex problems like this so okay so as i said we've defined now to score for the entire tree in terms of these three terms and uh it's key to note here that actually what we want is this to be the best possible score that could result in d at the root and so uh in fact we're going to model this score as the max over all possible rules that end with a d and over all possible ways to parse the left tree and the right tree and this might seem challenging but actually we can apply dynamic programming fairly straightforward to find this in a fairly efficient manner okay so we've defined now a scoring mechanism for parse trees for these line items and what happens is that we're then going to be able to choose among all possible parse trees using that score and the one with the highest score is the one that we consider the most likely the one that's most likely is the one that contains the information we care about and then we just read that information off the parse tree and so we're now going to train the deep network in order to select those most likely our most probable parse trees so just a reminder this first term as i've mentioned is a deep network there's a deep network for every single one of these rules and then we have these recursive terms that again are defined in terms of these deep networks so if we unroll this recursion if we unroll this recursion we will build a large network composed out of the networks for each of these individual rules and we'll build that large network for each and every document that we're trying to parse and so uh the the deep network is this dynamic object that has been composed out of solutions to these sub-problems in order to identify which is the correct parse tree so how do we train that it's fairly straightforward we use an idea from structure prediction and the idea in structure prediction is we have some structure in this case laparse tree and we want to maximize the score of good parse trees and minimize the score of all other parse trees right so what this loss function here is trying to do or this objective is trying to do is maximize the score of the correct parse trees the ones that we see in our training data and minimize the scores of all other parse trees the ones that we don't see in our training data and this can be optimized using back propagation and gradient descent or any of the machinery that we have from deep learning so this now becomes a classic machine learning our sorry deep learning optimization problem and and the result of this is an end-to-end system for parsing these documents now what i haven't talked about is the fact that and these documents are actually in two dimensions so far i've just focused on one-dimensional data so now i'll hand over to freddie and he will talk about how we deal with this two-dimensional nature of this data freddie well thank you nigel uh okay so this is the portion of the receipt that we were showing earlier on and we are going to focus on the lines within this blue boundary region over here moving along what we do is we apply ocr to the receipts and we get the bounding boxes that represents each tokens on the left side that is the line items as shown in the receipt on the right side that is the annotation that we are trying to pass we're trying to get the paths to match these annotations um what you will see is that um there are many boxes over here that we considered as irrelevant because it wouldn't be shown in the annotations and that wouldn't be part of the information that we want to extract so let's call these extra boxes and to simplify the parsing i'm going to remove them and then after going to the 2d parsing we're going to come back and and see how we handle these extra boxes so before talking about 2d parsing let me just motivate why we need to do 2d passing so what you see over here is this 2d layer of the first line item and if we were to take this line item and we reduce it into a 1d sequence what happens is that the the description which was originally in a contiguous layout in the 2d layout is no longer continuous in the 1d representation um you can see that this the blue the yellow region is continuous in the 2d layout and it's no longer contiguous in the sequence and that's because there's this blue region over here the 60 sense which has truncated the description so while we can add additional rules to handle this situation uh it typically wouldn't generalize to all the situations of all cases um a better solution would be actually to pass it in 2d and pausing it in 2d would be more consistent with how we humans interpret documents in general so we begin with the tokens and as what nigel had mentioned we put it through the deep network the deep network is going to give us uh what it thinks each token represents so in this case we get the classes for the tokens and now we can begin to merge them beginning with the the token in the top left corner uh is the word postage so we know that post stage is the description the most logical choice to pass is to combine with the token that is nearest to it and that's the stance so we can pass it in horizontal direction and we can do that because there's a rule in the grammar that says two description boxes can merge to form one description now the next thing to do is we can either pass it to the right as we can see over here or you know we can pass it in the vertical direction so which one do we choose if we do it we deposit horizontally like what we showed over here this works because there is a rule in the grammar that says that a line can be simply the total amount but what happens is that all the other boxes are left dangling there and then it wouldn't belong to any line item and in fact this wouldn't be a ideal path because then it wouldn't match the annotations that we have so an alternative path is to pass it in the vertical direction as shown over here what's important to note is that we do not hard code the direction of the pass instead the deep network is going to tell us which is the more probable path in this case you can combine them together because we know that postage stamps are description and the d network has told us that this string of numbers over here there's a description you can join them to be a description again the next thing to do is to look at um the box the token one over here and 60 cents we know that one is a count six cents is a price and we can join them because uh we have a rule in the grammar disease you can join a counter price to get a simple u now then the next thing is we can join a description and a simple u and we get a simple cube finally let's not forget that we still have a token over there we know that the token is a total amount finally we can join horizontally and we and as a result of the whole line come over here so moving along that early on we have simplified the parsing problem by removing all these extra boxes they're not there but what if we put them back if we put them back it complicates the parsing it's not in the annotations and we i didn't show it earlier so what do we do okay right so early on we already know that postage stamps they are descriptions and we can join them to become a description again so there's this extra words over here the transaction number words uh what do we do about them we introduce a new rule in the grammar and the new rule is saying we allow a token to be a noise so noise becomes a class that the deep network will possibly return to us if we know that transaction number they can be classified as noise then the next thing to do is we have oops sorry about that we can join two noise uh tokens together and we get one noise token because we have introduced a rule in the grammar that allows us to do that next thing we're gonna add another rule that says the description can be surrounded by some noise around them in this case i've added a rule over here you can see the exclamation mark here represents a permutation on this rule what this means is that we're not going to put a constraint on how these right hand side symbols can appear in this case noise can come before description or description can come after noise in this case the example shown over here the noise comes b for the description and we can combine a noise and a description together to get another description the simple d over here and moving along i can combine two description and i get a description so you can see that this is how i how we handle irrelevant tokens in in the documents so continuing with the logic eventually we would end up with the right freeze for the line items in the documents and this is what we get matching the current information okay so um finally i would like to talk about some experimental results that we have well our firm is mostly focused on invoices and most of these invoices tends to be confidential documents so while we believe that there could be other labs other companies also working on the same problem it's really very hard to find a public data set to compare our results with fortunately there is this interesting piece of work from clover ai is a lab we're doing this company in south korea called naval corp they also look at the problem online items and to their credit they have released a data set release a set of data set of receipts a set of reasons that they have written about and their paper is on exit as a preprint the main differences between our approach and their approach is that what they require is for every bounding box within the receipt to be annotated which means every bounding box you're gonna go in you're gonna say this bounding box belongs to this class and every boundary box you need to have the associated coordinates with it in our case all we do is to rely on the system on records in front of a json format which doesn't have the bounding box coordinates so effectively what we are doing is that we are training we are relying on less information than we should have and based on the results we achieve pretty comparable results as far as possible we try to implement the metrics close as possible to what they described so uh i guess with that uh i can anybody nigel great thanks freddie let me see if i have control back okay let me okay i think i do okay so um you know this was uh a number of people helped us helped us with this work and so i want to acknowledge that help and and and please do get in touch we do lots of interesting hand work at ui and all around the globe and we are hiring please reach out to us at aiadyy.com and we referenced some other work during the talk lots of really interesting great papers here definitely worth having a look at and and with that there were a couple of questions uh in the chat that i i thought were really great and so maybe let me try and answer them here because i think they also help to clarify the question the the the content so there was one question which is and can we let the ai model learn the best grammar to parse rather than defining the grammar constraints and it's it's a really good question but actually the grammar comes from the problem right the grammar is intrinsic to the problem itself the grammar can be automatically produced from the schema of the data we want to extract so it's it's natural for the problem it's not something we have to invent it comes from the problem itself there was another question about is it possible to share networks across the rules and and again really good question uh i think there's a few ways to think about this so number one is that each of these rules has its own network and we share the weights across every application of those rules whether that will be applied multiple times in a single parse tree or across multiple parse trees from multiple documents the other is that oftentimes we will leverage things like a language encoder and bert for example to embed to provide an embedding for each of the tokens and so there's lots of shared parameters there so there are many ways in which these parameters are shared and they end up it ends up being possible to produce relatively small networks to solve even really complicated problems like this and there was a question as to whether the 2d parsing is done greedily and so again really good question the the algorithm for parsing cfgs leverages dynamic programming so it doesn't uh it's not a greedy algorithm it actually produces the highest core parse tree and it does that in an efficient manner so naively that algorithm looks like it would be exponential but with the application of dynamic programming i believe it's enqueued um and then there's a question uh do the rules make any attempt to evaluate the tokenized data for example total actually equaling price times count when evaluating the likelihood of a tree again really good question we have not done that yet but it's something that we do have in mind to do because that's a really useful constraint right it's something we know about the problem is that a line item total tends to equal the unit count times the unit price and so that constraint should be really valuable in helping with a problem like this um and then a final question the grammar rules generalizable to different document types and so again these grammar rules are fundamental or natural to the problem that they correspond to the schema of the information we want to extract so that notion of generalizability of the grammar between document types is less important so thank you uh i'm happy to answer other questions hand it back to you alex 

i'm really happy to be here today to talk to you guys about something that i'm very excited and interested in because it's my research area so it's always fun to give a talk about your own research so the topic of my talk is taming data set bias via domain adaptation um and i believe you've already had some material in this course talking about sort of bias issues and perhaps fairness so this will dovetail with that pretty well i think okay so um i don't probably don't need to tell you guys or spend a lot of time on how successful deep learning has been for various applications here i'll be focusing mostly on computer vision applications because that's my primary research area so we know that in computer vision deep learning has gotten to the point where we can detect different objects pretty accurately in a variety of scenes and we can even detect objects that are not real people or could be even cartoon characters as long as we have training data we can train models to do this and we can do things like face recognition and emotion recognition so there are a lot of applications where deep learning has been super successful but there's also been some problems with it um and the one that i want to talk about is data set bias so in data set bias what happens is you you have some data set let's say you're training a computer vision model to detect pedestrians and you want to put it on a self-driving car uh and so you went and collected some data um you labeled the the pedestrians with bounding boxes and you trained your deep neural network and it worked really well on your held out test set that you held out from that same data set but now if you put that same model on your car and have it try to recognize pedestrians in a different environment like in new england which is where i am right now and in fact if i look on my window that's exactly what it looks like there's snow uh there could be sort of people wearing heavy coats and looks different from my training data which i would say i collected it in california where we don't see a lot of snow so visually this new data that i'm supposed to label with my model looks quite different from my training data so this is what we refer to as data set shift and it leads to problems in terms of missing detections and generally just lower accuracy of our trained model right so it's called data set bias it's also referred to as domain shift right in the primary issue here again is that the training data uh looks well i'm gonna just say looks different but i'll define a little more more um specific in a specific way later uh it's it looks different from the target test data when does data set bias happen well it happens in a few different scenarios i'll just show a few here and they're actually i will argue that it happens with every data set you collect but one example is you collect as already mentioned collect a data set in one city and you want to test on a different city or maybe you collect a data set from the web and then you want to put your model on a robot that gets images from its environment where the angle and the background and the lighting is all different another very common issue is with simulated training that we then want to transfer to the real world so that's a sim to real data set shift very common in robotics and another way that this could happen is if your training data set um is primarily of a particular demographic say if we're dealing with people it could be mostly light-skinned faces and then at test time you're given darker skin faces and you didn't train on that kind of data so again you have a data set bias or perhaps you're classifying weddings but your training data comes from images of weddings in western culture and then at test time you have other cultures so you again have a data set by so it could happen my point is that it can happen in many different ways and in fact i i believe that no matter what data set you collect it will have data set bias no matter what just because especially in the visual domain our visual world is so complex it's just very hard to collect enough variety to cover all possible situations so let's talk about more specifically why this is a problem and i'll give you an example that i'll use throughout my talk just to put some real numbers on this right so we probably all know by now the mnist data set so it's just a handwritten digits very common for benchmarking neural networks so if we train a neural network on amnest and then we test it on the same domain on mnist we know that we'll have very good performance upwards of 99 accuracy this is more or less a solved task but if we train our network on this street view house numbers data set which is uh also 10 digits the same time 10 digits but visually it's a different domain it's from the street view data set now when we test that model on the mnist data set performance drops considerably this is really really bad performance for this task right the 10 digits and in fact even if we train with this much smaller shift so this is from usps to mnist visually actually they look very similar to the human eye but there it there are some some small differences between these two data sets still performance drops uh pretty much the same uh as before and if we swap we still have bad performance when training on mnist and testing on usps so that's just to put some numbers like even for such a very simple task that we should have solved a long time ago in deep learning um it doesn't work right so we if we have this data set shift and we test the model on a new domain it pretty much breaks um so okay but this is a very academic data set um what about the real world what are the implications of data set bias have we seen any actual implications of data set bias in the real world and i would argue that yes we have this is one example where there have been several studies of face recognition models and gender recognition models commercial software that's being deployed for these problems in the real world and these studies show that facial recognition algorithms are far less accurate at identifying african-american and asian faces compared to caucasian faces and a big part of the reason why is data set shift because the training data sets for use for these models are biased towards caucasian faces so another real world example that i want to show is a very sad example actually where a while back there was a self-driving car accident it's actually the first time that a robot has killed a person so there was an accident that was fatal with an uber self-driving car and according to some reports the reason uh they think that the car failed to stop is that its algorithm was not designed to detect pedestrians outside of a crosswalk right so you could actually think of this as a data set bias problem again if your training data contains only pedestrians on a crosswalk which is reasonable to assume right because majority of the time pedestrians follow the rules and cross on the crosswalk and only a few times they you know you might see people jay walking you will probably not see a lot of examples like that in your data set so you might be wondering at this point well wait a minute can't we just fix this problem by collecting more data just getting more data and labeling it well yes we could theoretically however it gets very very expensive very quickly and to illustrate why let's take this example this is again in the self-driving domain this is a images from the berkeley bdd dataset which has actually quite a variety of domains already so it has the nighttime images also has daytime images and the labels here are semantic segmentation labels so each pixel is labeled like with road or pedestrian so on so if we wanted to label um 1000 pedestrians with these polygons that would cost around one thousand dollars this is just you know kind of standard market price however if we now want to uh multiply that times how many different variations we want in the pose times variations in gender times variations in age race clothing style so on so on we very quickly see how much data we have to collect right and somewhere in there we also want people who ride bicycles so this becomes this blows up very quickly becomes very expensive so instead maybe what we want to do is design models that can use unlabeled data rather than labeled data that's what i'm talking about today um so now let's think about okay what causes this poor performance that we've already seen and there are basically two main reasons i think the first reason is that the training and test data distributions are different and you can see that in this picture so here the blue points are feature vectors extracted from the digit domain the mnist digit domain using a network that was trained on the digit domain so we train a network and we take the second to last layer activations and we plot them using t-sne embeddings so that we can plot them in 2d so you see these blue points are the training eminence points and then we take that same network and extract features from our target domain which you can see here it's basically m this but with different colors as opposed to black and white and so those are the red points and you can see very clearly that the distribution over the inputs is very different in the training and test domains so that's one issue is that our classifier which is trained on the blue points will not generalize to the red points because of this distribution shift another problem is actually it's a little bit more subtle but if you look at how the blue points are much better clustered together with spaces between them and so these are clusters according to category of the digit but the red points are a lot more kind of spread out and not as well clustered into categories and this is because the model learned discriminative features for the source domain and these features are not very discriminative for the target domain so the test target points are not being grouped into classes using those features because you know they just the kinds of features the model needs weren't learned from the source domain all right so what can we do um well we can actually do quite a lot and here actually is a list of methods that you could use to try to deal with data set shift that are fairly simple standard things that you could do for example if you just use a better backbone for your cnn like resnet 18 as opposed to alexnet you will have a smaller performance gap due to domain shift batch normalization done per domain is a very good trick um you can combine it with instance normalization of course you could do data augmentation use semi-supervised methods like pseudo-labeling and then what i'll talk about today is domain adaptation techniques okay so let's define domain adaptation all right so we have a source domain which has a lot of unlabeled data sorry which has a lot of labeled data so we have inputs x i and labels y i in our source domain and then we have a target domain which has unlabeled data so no labels just the inputs and our goal is to learn a classifier f that achieves a low expected loss under the target distribution dt right so we're learning on the source labels but we want to have good performance on the target and a key assumption in domain adaptation that is really important to keep in mind is that in domain adaptation we assume that we get to see the unlabeled data we we get access to it which is which is important we don't get the labels um because again we assume it's very expensive or we just can't label it for some reason um but we do get the unlabeled data okay so what can we do um i'll so here's the outline of the rest of my talk um and i'm i'm sure i'm gonna go pretty quickly um and i'll try to have time at the end for questions so please if you have questions note them down um so i'll talk about kind of the standard very very uh at this point conventional technique called adversarial domain alignment and then i'll talk about a few more recent techniques that have been applied to this problem um and then we'll wrap up okay so let's start with adversarial domain alignment okay so say we have our source domain with labels and we're trying to train a neural network here i've split it into the encoder cnn it's a convolutional neural network because we're dealing with images and then the classifier which is just the last layer of the network and we can train it in a normal way using standard classification laws and then we can extract features from the encoder to plot them here to visualize the two categories just for illustration purposes i'm showing just two and then we can also visualize some notion of discriminator between classes that the classifier is learning this decision boundary now we also have unlabeled target data which is coming from our target domain we don't have any labels but we can take the encoder and generate features and as we've already seen we'll see a distribution shift between the source blue features and the target orange features so the goal in adversarial domain alignment is to take these two distributions and align them so update the encoder cnn such that the target features are distributed in the same way as the source okay so how can we do this well it involves adding a domain discriminator think of it as just another neural network which is going to take our features from the source and the target domain and it's going to try and predict the domain label so its output is a binary label source or target domain okay and so this domain discriminator is trying to distinguish the blue points from the orange points okay so we train it just with classification loss on the domain labels and then that's our first step and then our second step is going to be to fix the domain discriminator and instead update the encoder such that the encoder results in a poor domain discriminator accuracy so it's trying to fool the domain discriminator by generating features that are essentially indistinguishable between the source and target domain okay so it's an adversarial approach because of this adversarial back and forth first we train the domain discriminator to do a good job at telling the domains apart then we fix them we train the encoder to fool the discriminator so that it can no longer tell the domains apart if everything goes well we have a line distributions so does this actually work let's take a look at our digits example from before so here we have again two digit domains and you see before adaptation the distributions of the red and the blue points were very different now after applying this adversarial domain alignment on the features we can see that in fact the feature distributions are very well aligned now you you more or less cannot tell the difference in terms of the distribution between the red and the blue points okay so it works and not only does it work to align the features only also works to improve the accuracy of the classifier we train because we're still here training this classifier uh using the source domain labels right and this actually is what prevents our alignment from diverging into something uh you know really silly like mapping everything to one point because we still have this classification loss that has to be satisfied so the classifier also improves so let's see how much so here i'm going to show results from our cdr17 paper called ada or adversarial discriminative domain adaptation so with this technique we can improve accuracy when training on these domains and then testing on these target domains quite a lot so it's a significant improvement it's a little harder on it's not as good on the svh end to end this shift because that is the the hardest of these shifts great so the takeaway uh so far is that domain adaptation can improve the accuracy of the classifier on target data without requiring any labels at all so we didn't label our target domains here at all we just trading with unlabeled data and so you can think of this as a form of unsupervised fine tuning right so fine tuning is something we often do to improve a model on some target task but it requires labels so this is something we can do if we have no labels and we just have a label data we can do this kind of unsupervised fine-tuning great so okay so so far i talked about domain alignment in the feature space because we were updating features next i want to talk about pixel space alignment so the idea in pixel space alignment is what if we could take our source data the images themselves the pixels and actually just make them look like they came from the target domain and we can actually do that thanks to adversarial generative models organs which work in a very similar way to what i already described but they the discriminator is looking at the whole image not the features but the actual image that's being generated so we can take this idea and apply it here and train again that will take our source data and translate it in the image domain to make it look like it actually comes from the target domain right so we can do this because we have unlabeled target data and there are a few approaches for this uh just for doing this image to image translation a famous one is called cyclogan but essentially these are conditional gans that use some kind of loss to align the two domains in the pixel space so what's the point of this well if we now have this translated source data we still have labels for this data but it now looks like it comes from the target domain so we can just train on this new fake target like data with the labels and hopefully it improves our classifier error on the target by the way we can still apply our previous feature alignment so we can still add a domain discriminator on the features just like we did before and do these two things in tandem and they actually do improve performance when you do both on many problems okay so let me show you an example here's a training domain which is uh so we're trying to do semantic pixel labeling so our goal is for a neural network is to label each pixel as one of several categories like road or car or pedestrian or sky and we want to train on this gta domain which is from the grand theft auto game which is a nice source of data because it basically is free the labels are free we just get them from the game and then we want to test on this cityscapes dataset which is a real world dataset collected in germany in multiple cities so you can see what it looks like so i'm going to show you the result of doing pixel to pixel domain alignment between these two domains so you see that here we're actually taking the real data set and translating it to the game so the original video here is from cityscapes and we're translating it to the gta game all right so what happens if we apply this idea to domain adaptation um so here our source domain is gta here's an example of the adapted source image that we take from gta and translate it now into the real domain and then when we then train the classifier on these translated images our accuracy goes up from 54 to 83.6 per pixel accuracy on this task so it's a really good improvement in accuracy again without using any additional labels on the target domain and also going back to our digit problem remember that really difficult shift we had from the street view image digits to the mnist digits well now with this pixel space adaptation we can see that we can take those source images from from the street view domain and make them look like mnist images so this middle plot middle image shows the images on the left that are original from svhn and we translated them to look like amnest and so if we train on these now we can improve our accuracy to 90.4 on mnist and if we compare this with our previous result using only feature space alignment we were getting around 76 so so we're improving on that quite a bit so the takeaway here is that unsupervised image to image translation can discover and align the corresponding structures in the two domains so there so there is corresponding structure right we have digits in both domains and they have similar structure and what this method is doing is trying to um align them by discovering these structures and making them correspond to each other great um so next i want to move on to talk about fu shot pixel alignment okay so so far i didn't tell you this explicitly but we actually assume that we have quite a lot of unlabeled target data right so in the case of that game adapting from the game to the real data set we took a lot of images from the real world uh they weren't labeled but we had a lot of them so we had like i don't know how many thousands of images what happens if we only have a few images from our target domain well it turns out these methods that i talked about can't really handle that case they need a lot more images so what we did was with my graduate student and my collaborator at nvidia we came up with a method that can do translation with just one or a few maybe two or three or up to five images in the target domain so suppose we have our source domain where we have a lot of images that are labeled here we're going to look at an example of different animal species so the domain will be the species of the animal so here we have a particular breed of dogs and now we want to translate this image into a different domain which is this other breed of dog but we only have one example of this breed of dog so our target domain is only given to us in one image so and then our goal is to output a translated version of our source image that preserves the content of that source image but adds the style of the target domain image so here the content is the pose of the animal and the style is the species of the animal so in this case it's the breed of the dog and you can see that we're actually able to do that fairly successfully because as you can see the we've we've preserved the pose of the dog but we've changed the the breed of the dog to the one from the target image okay so so this is a pretty uh cool result um and the way we've achieved this is by modifying an existing model called funet which you see on the left here basically by updating the style encoder part of this model so we call it coco or content conditioned style encoder and so the way our model works is it takes the content image and the style image it encodes the content using an encoder this is just a convolutional network and then it also takes both the style and the content encodes it as a style vector and then this image decoder takes the content vector and the style vector and combines them together to generate the final output image and there's there's a gan loss on that image make sure that we're generating um images that look like our target so the main difference between the previous work unit and ours that we call cocof unit is that this style encoder is structured differently it's conditioned on both the content image and the style image okay so if we um kind of look under the hood of this model a little more some more detail the main difference again and this is in the style encoder right so it takes the style image um encodes it with features and it also learns a separate style bias vector which is concatenated with the image encoding and these are parameters that are learned for the entire data set so they're they're constant they don't believe they don't depend on the image essentially what that does is it helps the model kind of learn how to account for pose variation because in different images we'll have sometimes very drastic change in pose in one image we see the whole body of the animal and then we could have very occluded animal with just the head visible like in this example and then the content encoding is combined with these style encodings to produce the final style code which is used in the adaptive instance normalization framework if you're familiar with that if not don't worry about it just some way to combine these two vectors to generate an image so here is some example outputs from our models on top we have a style so it's an image from our target species that we want our animal to look like and then below that is the content which is the the source essentially the pose that we want to preserve and then at the bottom in the bottom row you see the generated result which our model produced and so you can see that we're actually able to preserve the pose of the content image pretty well but combine it with the style or the the species of the source sorry of the target style image and sometimes we even you know make something that is a cat look more like a dog because the the target domain is a dog breed but the pose is the same as the original cat image or here in the last one it's actually a bear that's generated to look like a dog so if we compare this to the previous method called unit that i mentioned before we see that our model is getting significantly better generations than funit which in this case a lot of the time fails to produce realistic images it's just not generating images that are convincing or photorealistic and so here i'm going to play a video just to show you a few more results here we're actually taking the whole um a whole video and translating it into some target domain so you see various domains on top here so the same input video is going to be translated into each of these target domains where we have two images for each target domain right so the first one is actually a fox and now there's another example here with different bird species so you can see that the pose of the bird is preserved from the original video but its species is changed to the target so there's varying levels of you know success there but overall it's doing better than the previous approach and here's another final example here we're again taking the content image combining it with the style and generating the output not really sure what species this would be some kind of strange new species okay so the takeaway is that by conditioning on the content and style image together we're able to improve the encoding of style and improve the the domain translation in this view shot case all right so uh i have a little bit of time left i think how much time do i have actually about 10 minutes yes okay so in just the last few minutes i want to talk about more recent work that we've done that goes beyond these alignment techniques that i talked about and actually improves on them and the first one is self-supervised learning so one assumption that all of these methods i talked about make is that the categories are the same in the source and target and they actually break if that assumption is violated so why would we violate this assumption so suppose we have a source domain of of objects and we want to transfer to a target domain from this real source to say a drawings domain but in the drawings domain we have some images of which some of those images are the same categories that we have in the source but some of the source categories are missing in our target domain suppose like we don't have cup or cello and also we might even have new categories in the target domain that are not present in the source okay so here what we have is a case of category shift not just feature shift not just visual domain shift but actually the categories are shifting and so this is a difficult case for domain alignment because in domain alignment we always assume that the whole domain should be aligned together and and if we try to do that in this case we'll have catastrophic uh adaptation results so we actually could do worse than just doing nothing doing no adaptation so uh in our recent paper from europe's in 2020 we propose a solution to this that uses doesn't use domain alignment but uses self-supervised learning okay so the idea is let's say we have some source data which is labeled that's the blue points here we have some target which is unlabeled and some of those points could be unknown classes right so the first thing we do is we find points pairs of points that are close together and we train a feature extractor in such a way that these points are embedded close together so we're basically trying to cluster neighboring points even closer together while pushing far away points even further apart and we can do that because we're starting with a pre-trained model already so let's say it's pre-trained on imagenet so already gives us a pretty good initialization so after this neighborhood clustering which is an unsupervised loss or we can call it self-supervised we get a better clustering of our features that already is clustering the unlabeled target points from the known classes closer to the source points from the known classes and then it's clustering the yellow unknown classes in the target away from those known classes now what we want to do is add an entropy separation loss which further encourages points that have excuse me points that um have a certain entropy away from the known classes so this is essentially an outlier rejection mechanism right so if we look at a point and we see that it has very high entropy it's probably an outlier so we want to reject it and push it even further away and so finally what we obtain is an encoder that gives us this feature distribution where points of the same class are clustered close to the source but points of novel classes are clustered away from the source okay so if we apply it on this data set called the vista challenge which is training on synthetic images and adapting to a target domain which is real images but some of those categories are missing in the target and again we don't know which ones in real life because the target is unlabeled right so if we approve if we apply this dance approach that i just described we can improve performance compared to a lot of the recent domain adaptation methods and also compared to just training on the source so that's we get pretty low performance if we trade only on the source data um and then if we do this uh domain alignment on the entire domain this that's this d a and n method we actually see that we have worse accuracy than doing nothing than just training on the source again because this same category uh assumption is violated in this problem okay but with our method we're actually able to do much better than just training on source and improve accuracy okay and then finally i want to mention another cool uh idea that um has become uh more prevalent recently in semi-supervised literature and we can actually apply it here as well so here we start again with self-supervised pre-training on the source and target domains but in this case we're doing a different uh cell supervised task instead of clustering points here we are predicting rotation of images so we can rotate an image and we know exactly what orientation it's in but then we train our feature extractor to predict that orientation for example is it is it rotated 90 degrees or zero degrees okay so but again that's just another self-supervised task it helps us pre-train a better feature encoder which is um more discriminative for our source and target domain and then we apply this consistency loss so what is the consistency loss so here we're going to do some data augmentation on our unlabeled images okay so we're going to take our pre-trained model and then use that model to generate probability distributions of the target uh sorry of the class um on the original image and also on the augmented unlabeled image where the augmentation is you know cropping color transformation adding noise adding small rotations and things like that so it's it's designed to preserve the category of the object but it changes the image and then we take these two probability outputs and we add a loss which ensures that they're consistent right so we're telling our model look if you see an augmented version of this image you should still predict the same category for that image we don't know what it is because the image is unlabeled but it should be the same as the original image so with this idea so we call the combination of this rotation prediction pre-training and consistency training we call this pack and this is just one small taste of the results we got just because i don't have much time but essentially here we are again adapting from the synthetic data set in the visited challenge to real images but now we are assuming a few examples are labeled in our target domain and we are actually able to just with this pack method improve a lot on the domain alignment method which is called mme that's our previous work in this case so basically the point that i want you to take away from this is that domain alignment is not the only approach and we can use other approaches like self-supervised training and consistency training to improve performance on target data all right so i'll stop here just to summarize what i talked about i hope i've convinced you that data set bias is a major problem and i've talked about how we can solve it using domain adaptation techniques which try to transfer knowledge using unlabeled data and we can think of this as a form of unsupervised fine-tuning and the technique i talked about include adversarial alignment and also some other techniques that are relying on self supervision and consistency training and so i hope you enjoyed this talk and if you have any questions they'll be very happy to answer them 

great yeah thanks for a nice introduction um i'm gonna talk to you about 3d content creation and particularly deep learning techniques to facilitate 3d content creation most of the work i'm going to talk about is the work um i've been doing with my group at nvidia and the collaborators but it's going to be a little bit of my work at ufc as well all right so you know you guys i think this is a deep learning class right so you heard all about how ai has made you know so much progress in the last maybe a decade almost but computer graphics actually was revolutionized as well with you know many new rendering techniques or faster rendering techniques but also by working together with ai so this is a latest video that johnson introduced a couple of months ago [Music] quietly so this is all done all this rendering that you're seeing is done uh real time it's basically rendered in front of your eyes and you know compared to the traditional game you're used to maybe real time in gaming but here there's no baked lights there's no brake light everything is computed online physics real time retracing lighting everything is done online what you're seeing here is rendered in something called omniverse is this visualization a collaboration software that nvidia has just recently released so you guys should check it out it's really awesome all right [Music] oops these lights always get stuck yeah so when i joined nvidia this was two years and a half ago it was actually the orc that i'm in uh was creating the software called omniverse the one that i just showed and i got so excited about it and you know i wanted to somehow contribute in this space so somehow introduce ai in in into this content creation and graphics pipeline and and 3d content is really everywhere and graphics is really in a lot of domains right so in architecture you know designers would create office spaces apartments whatever everything would be done uh you know you know in some some modeling software with computer graphics right such that you can judge whether you like some space before you go out and build it all modern games are all like heavy 3d um in film there's a lot of computer graphics in fact because directors just want too much out of characters or humans so you just need to have them all done with computer graphics and animate in realistic ways now that we are all home you know vr is super popular right everyone wants a tiger in the room or have a 3d character version 3d avatar of yourself and so on there's also robotics so healthcare and robotics there's actually also a lot of computer graphics and and in these areas and these are the areas that i'm particularly excited about and and why is that um it's actually for simulation so before you can deploy any kind of robotic system in the real world you need to test it in a simulated environment all right you need to test it against all sorts of challenging scenarios on healthcare for you know robotic surgery robotics are driving cars you know warehouse robots and and stuff like that i'm going to show you this uh simulator called drive sim that nvidia has been developing um and this is uh this video is a couple of years old now it's not a lot better than this um but basically simulation is kind of like a game it's really a game engine for robots where now you expose a lot more out of the game engine you want to have the creator the roboticist some control over the environment right you want to decide how many cars you're going to put in there what's going to be the weather night or day and so on so this gives you some control over the scenarios you're going to test against but the nice thing about you know having this computer graphics pipeline is um everything is kind of labeled in 3d you already have created a 3d model of car you know it's a car and you know the parts of the car is you know something is a lane and so on and instead of just rendering the picture you can also render you know grand truth for ai to both train on and be tested against right so you can get ground truth lanes crunch with weather ground truth segmentation all that stuff that's super hard to collect in the real world okay um my kind of goal would be you know if we if we want to think about all these applications and particular robotics you know can we simulate the world in some way can we just load up a model like this which looks maybe good from far but we want to create really good content at street level and you know both assets as well as behaviors and just make these virtual cities alive so that we can you know test our robot inside this all right so it turns out that actually is super slow let me play this require significant human effort here we see a person creating a scene aligned with a given real world image the artist places scene elements edits their poses textures as well as scene or global properties such as weather lighting camera position this process ended up taking four hours for this particular scene so here the artist already had the assets you know bottom online or whatever and the only goal was to kind of recreate the scene above and it already took four hours right so this is really really slow and i don't know whether you guys are familiar with you know games like grand theft auto that was an effort by a thousand engineers a thousand people working for three years um basically recreating la los angeles um going around around the city and taking tons of photographs you know 250 000 photographs many hours of footage anything that would give them you know an idea of what they need to replicate in the real world all right so this is where ai can help you know we know computer vision we know deep learning can we actually just take some footage and recreate these cities both in terms of the construction the the assets as well as behavior so that we can simulate all this content or this live content all right so this is kind of my idea what we need to create and i really hope that some guys you know some of you guys are are going to be equally excited about these topics and i'm going to work on this so i believe that we we need ai in this particular area so we need to be able to synthesize worlds which means both you know scene layouts uh you know where am i placing these different objects maybe map of the world um assets so we need some way of creating assets like you know cars people and so on in some scalable way so we don't need artists to create this content very slowly as well as you know dynamic dynamic parts of the world so scenarios you know which means i need to be able to have really good behavior for everyone right how am i going to drive as well as you know animation which means that the human or any articulated objective animate needs to look realistic okay a lot of this stuff you know it's already done there for any game the artists and engineers need to do that what i'm saying is can we have ai to do this much much better much faster all right so you know what i'm going to talk about today is kind of like our humble beginning so this was this is the main topic of my um you know toronto nvidia lab and and i'm gonna tell you a little bit about all these different topics that we have been slowly addressing but there's just so much more to do okay so the first thing we want to tackle is can we synthesize worlds by just maybe looking at real footage that we can collect let's say from a self-driving platform so can we take those videos and and you know train some sort of a generative model is going to generate scenes that look like the real city that you know we want to drive in so if i'm in toronto i might need brick walls if i'm in la i just need many more streets like i need to somehow personalize this content based on the part of the world that i'm gonna be in okay if you guys have any questions just write them up i i like if the uh lecture is interactive all right so how can we compose scenes and our thinking was really kind of looking into how games are built right in games you know people need to create very diverse levels so they need to create in a very scalable way very large walls and one way to do that is using some procedural models right or probabilistic grammar which basically tells you you know rules about how the scene uh is created such that it looks like a valid scene so in this particular case and i would i would sample a road right with some number of lanes and then on each lane you know sample some number of cars and maybe there's a sidewalk next to a lane with maybe people on walking there and there's trees or something like that right so this this this probabilistic models can be fairly complicated you can quickly imagine how this can become complicated but at the same time it's not so hard to actually write this anyone could would be able to write a bunch of rules about how to create this content okay so it's not it's not too tough but the tough is to really the the tough part is you know setting all these distributions here and you know such that the render scenes are really going to look like your target content right meaning that if i'm in toronto maybe i want to have more cars if i'm in a small village somewhere i'm going to have less cars so for all that i need to go and you know kind of personalize these models set the distributions correctly so this is just some one example of you know sampling from a probabilistic model here the uh the probabilities for the orientations of the cars become randomly set but there's so much the scene already looks kind of kind of okay right because it already incorporates all the rules that we know about the world and the model will be needing to to training all right so you can think of this as some sort of a graph right where each node defines the type of asset we want to place and then we also have attributes meaning we need to have location height pose anything that is necessary to actually place this car in the scene and render it okay and and this this these things are typically said by an artist right they they look at the real data and then they decide you know how many pickup trucks i'm going to have in the city or so on all right so basically they said this distribution by hand what we're saying is can we actually learn this distribution by just looking at data okay and we had this paper column metasim a couple of years ago where the the idea was let's assume that the structure of the scenes that i'm sampling so in this particular case apps you know how many lanes i have how many cars i have that comes from some distribution that artist has already designed so the the graphs are going to be correct but the attributes um should be modified so if i sample this original scene graph from that i can render like you saw that example before the cars were kind of randomly rotated and so on the idea is can a neural network now modify the attributes of these nodes modify the rotations the colors maybe even type of object such that when i render those those scene graphs i get images that look like real images that i have recorded in distribution so we don't want to go after exact replica of each scene we want to be able to train a generative model it's going to synthesize images that are going to look like images we have recorded that's the target okay so basically we have some sort of a graph neural network that's operating on scene graphs and it's trying to repredict attributes for each node i don't know whether you guys talked about graph neural nets and then the loss that's coming out is through this renderer here and we're using something called maximum indiscreptency so i'm not going to go into details but basically the idea is you could you need to compare two different distributions you could compare them by you know comparing the means of the two distributions or maybe higher order moments and mmd was designed to to compare higher order moments okay now this last can be back prop through this non-differentiable renderer back to graph neural net okay and we just use numerical gradients to do this step and the cool part about this is we haven't really needed any sort of annotation on the image we're comparing images directly because we're assuming that the image the synthesized images already look pretty good all right so we actually don't need data we just need to drive around and record these things okay you can do something even cooler you can actually try to personalize this data to the task you're trying to solve later which means that you can train this network to generate data that if you train some other neural net on top of this data it's an object detector it's going to really do well on you know whatever task you have in the end collected in the real world okay which might not mean that the object need to look really good in the scene you just might it just means that you need to generate scenes that are going to be useful for some network that you want to train on that data okay and that you you again back prop this and you can do this with reinforcement learning okay so this was now training the distribution for the attributes we were kind of the easy part and we were sidestepping the issue of well what about the structure of these graphs meaning if i had always generated you know five or eight or ten cars in a scene but now i'm in a village i will just not train anything very useful right so the idea would be can we learn the structure the number of lanes the number of cars and so on as well okay and and it turns out that actually you can do this as well where here we had a probabilistic context free grammar which basically means you have a you have a root note you have some symbols and which can be non-terminal terminal symbols and rules that they that basically expand non-terminal symbols into new symbols so an example would be here right so you have a road which you know generates lanes lanes can go into lane or more lanes right and so on so these are the the rules okay and basically what we want to do is we want to train a network that's going to learn to sample from this probably the context-free grammar okay so we're going to have some sort of a latent vector here we know where we are in the tree that or the graph we've already generated before so imagine we are in in we have sampled some lane or whatever so we now we know the the corresponding symbols that we can actually sample from here we can use that to mask the probabilities for everything else out all right and our network is basically gonna learn how to produce the correct probabilities for the next symbol we should be sampling okay so basically at each step i'm going to sample a new rule until i hit all the terminal symbols okay that basically gives me something like that these are the sample the rules in this case which can be converted to a graph and then using the previous method we can you know augment this graph with attributes and then we can render the scene okay so basically now we are also learning how to generate um the the the actual scenario the actual structure of the same graph and the attributes and and this is super hard to train so there's a lot of bells and whistles to make this to work but essentially because this is all non-differentiable steps you need something like reinforcement learning and and there's a lot of tricks to actually release to work but i was super surprised how well you this can actually turn out so on the right side you see samples from the real data set uh kitty is like a real driving data set on the left side is samples from probabilistic grammar here we've set this first probabilities manually and we purposely made it really bad which means that this probably is the grammar when you sample you got really few cars almost no buildings and you can see this is like almost not not populated scenes after training you the generative model learned how to sample this kind of scenes because they were much closer to the real target data so these were the final trained things okay and now how can you actually evaluate that we have done something reasonable here you can look at for example the distribution of cars in the real real data set this is kitty over here so here you will have a histogram of how many cars you have in each scene um you have this orange guy here which is the prior meaning this badly initialized prolistic grammar where we only were sampling most of the time very few cars and then the learned model which is the green the line here so you can see that the generated scenes really really closely follow this distribution of the real data without any single annotation at hand right now you guys could argue well it's super easy to write you know these distributions by hand and and we're done with it i think there's just this just shows that this can work and the next step would just be make this really large scale make this you know really huge probabilistic models where it's hard to tune all these parameters by hand and the cool part is that because everything can be trained now automatically from real data no any end user can just take this and it's going to train on their end they know they don't need to go and set all this stuff by hand okay now the next question is you know how can i evaluate that my model is actually doing something reasonable and one one way to do that is by actually sampling from this model synthesizing these images along with gran truth and then train some some you know end model like a detector on top of this data and testing it on the real data and and just seeing whether the performance has somehow improved um compared to you know let's say on that badly initialized um probabilistic grammar and it turns out that that's that's the case okay now this was the example shown on driving but oh sorry so so this model is is just here i'm just showing basically what's happening during training let me just go quickly so the first snapshot is the first sample from the model and then what you're seeing is how this model is actually training so how is modifying the scene during training let me show you one one more time so you can see the first frame was really kind of badly placed cars and then it's slowly trying to figure out where to place them and to be correct and of course this generative model right so you can sample tons of scenes and everything comes labeled cool right um this model here was shown on on on driving but you can also apply it everywhere else like in other domains and here you know medical or healthcare now is very um you know important in particular these days when everyone is stuck at home um so you know can you use something like this to also synthesize medical data and what do i mean by that right so doctors need to take you know city or mr mri volumes and go and label every single slice of that with you know let's say a segmentation mask such as they can then train like a you know cancer segmentation or card segmentation or lung segmentation kobe detection whatever right so first of all data is very hard to come by right because in some diseases you just don't have a lot of this data the second part is that it's actually super time consuming and you need experts to label that data so in the medical domain it's really important if we can actually somehow learn how to synthesize this data label data so that we can kind of augment the real data sets with that okay and the model here is going to be very simple again you know we have some generative model let's go from a latent codes to some parameters of a of a mesh in this case this is our asset within a material map and then uh we synthesize this with a physically based um ct simulator uh which you know looks a little bit blurry and then we train a enhancement model with something like again and then you get simulated data out obviously again there is a lot of belts and whistles but you know you can get really nice looking synthesized volumes so here the users can actually play with the shape of the heart and then they can click synthesize data and you get some some labeled volumes out where the label is basically the stuff on the left and this is the simulated sensor in this case okay all right so now we talked about using procedural models to generate to generate worlds and of course the question is well do we need to write all those rules can we just learn how to recover all those rules and here was our first take on this um and here we wanted to generate or learn how to generate city road layouts okay which means you know we want to be able to generate something like that where you know the lines over here representing roads okay this is the base of any city and we want to again have some control over these worlds we're going to have something like interactive generation i want this part to look like cambridge it's parked to look like new york inspired to look like uh toronto whatever and we want to be able to generate or synthesize everything else you know according to these styles okay you can interpret road layout as a graph okay so what does that mean i have some control points and two control points being connected means i have a road line segment between them so really the problem that we're trying to solve here is can we have a neural net generate graphs graphs with attributes where each attribute might be an x y location of a control point okay and again giant graph because this is an entire city we want to generate um so we had actually a very simple model where you're kind of iteratively generating this graph and imagine that we have already you know generated some part of the graph what we're going to do is take an a node from from like an unfinished set what we call we encode every path that we have already synthesized and leads to this node which basically means we wanna we wanna kind of encode how this node already looks like what are the roads that it's connecting to and we want to generate the remaining nodes basically how these roads continue in this case okay and this was super simple you just have like r and n's encoding each of these paths and one rnand that's decoding these neighbors okay and you stop where basically you hit some predefined size of the city okay let me show you some some results so here you can condition on the style of the city so you can generate barcelona or berkeley you can have this control or you can condition on part of the city being certain style and you can use the same model the generative model to also parse real maps or real aerial images and create and create variations of those maps for something like simulation because for simulation we need to be robust uh to the actual layouts so now you can turn that graph into an actual small city where you can maybe procedurally generate the rest of the content like we were discussing before where the houses are where the traffic signs are and so on cool right so now we can generate you know the map of the city um we can place some objects somewhere in the city so we're kind of close to our goal of synthesizing worlds but we're still missing objects objects are still a pain that the artists need to create right so all this content needs to be manually designed and that just takes a lot of time to do right and maybe it's already available you guys are going to argue that you know for cars you can just go online and pay for this stuff i first of all it's expensive and second of all it's not really so widely available for certain classes like if i want a raccoon because i'm in toronto there's just tons of them there's just a couple of them and they don't really look like real raccoons right so the question is can we actually do this all these tasks by taking just pictures and synthesizing this content from pictures right so ideally we would have um something like an image and we want to produce out you know a 3d model 3d texture model right there can i then insert in my real scenes and ideally we want to do this on just images that are widely available on the web right i think the new iphones all have lidar so maybe this world is going to change because everyone is going to be taking 3d pictures right with some 3d sensor but right now the majority of pictures that are available of objects on flickr let's say it's all single images people just snapshotting a scene or snapshotting on a particular object so the question is you know how can we learn from all the data and go from an image on the left to a 3d model and in our case we're going to want to produce as an output from the image and mesh which basically has you know location of vertices xyz and you know some color material properties on each vertex right and 3d vertices along with faces which means which vertices are connected that's basically defining this 3d object okay and now we're going to turn to graphics to help us with our goal to do this from you know the kind of without supervision learning from the web okay and in graphics we know that images are formed by geometry interacting with light right that's just principle of rendering okay so we know that you can you you if you have a mesh if you have some light source or sources and you have a texture and also materials and so on which i'm not writing out here and some graphics renderer you know there's many issues choose from you get out a rendered image okay now if we make this part differentiable if you make the graphics renderer differentiable then maybe there is hope of going the other way right you can think of computer vision being inverse graphics graphics is going for 3d to images computer vision wants to go from images into 3d and if this model is differentiable maybe there's hope of doing that okay so there's been quite a lot of work lately on basically this kind of a pipeline with different modifications um but basically this summarizes the the ongoing work where you have an image you have some sort of a neural net that you want to train and you're making this kind of button like predictions here which smash light texture maybe material okay now instead of having the loss over here because you don't have it you don't have the ground truth mesh for this car because you otherwise you need to annotate it what we're going to do instead is we're going to send these predictions over to this renderer which is going to render an image and we're going to have the loss defined on the rendered image and the input image we're basically going to try to make these images to match okay and of course there's a lot of other losses that people use here like multi-view alloys you're assuming that in training you have multiple pictures multiple views of the same objects you have masks and so on so there's a lot of bells and whistles how to really make this pipeline work but in principle it's a very clean idea right where we want to predict these properties i have this graphics renderer and now i'm just comparing input and output and because this is this render is differentiable i can propagate these slots back to all my desired you know neural light weights so i can predict this these properties okay now we in particularly had a very simple like opengl type render which we made differentiable there's also versions where you can make rich racing differentiable and so on but basically the idea that we employed was super simple right a mesh is basically projected onto an image and you get out triangles and each pixel is basically just a butter centric interpolation of the vertices of this projected triangle and now if you have any properties defined of those vertices like color or you know texture and so on then you can compute this value here through your you know renderer that assumes some lighting or so on um in a differentiable manner using this percentage coordinates this is a differential function and you can just go back through whatever lighting or whatever um shader model you're using okay um so very simple and there's you know much much richer models that are available richer differentiable renders available these days but here we try to be a little bit clever as well with respect to data because most of the related work was taking synthetic data to train their model why because most of the work needed multi-view data during training which means i have to have multiple pictures from multiple different views of the same object and that is hard to get from just web data right it's hard to get so people will just basically taking synthetic cars from synthetic data sets and rendering in different views and then training the model which really just maybe makes a problem not so interesting because now we are actually relying on synthetic data to solve this and the question is how can we get data and and and we try to be a little bit clever here and we turn to generative models of images i don't know whether you guys cover in class uh you know image gans but if you take something like stylogen which is uh you know generative adversarial network designed to really produce high quality images by by sampling from some some prior you get really amazing pictures out like all these images have been synthesized none of this is real this is all synthetic okay you know these guys basically what they do is you have some latent code and then there's a you know some nice progressive architecture that slowly transforms that latent code into an actual image okay what happens is that if you start analyzing this this latent code or i guess i'm going to talk about this one if you take certain dimensions of that code and you try and you freeze them okay and you just manipulate the rest of the code it turns out that you can find really interesting controllers inside this latent code basically the gun has has learned about the 3d world and it's just hidden in that latent code okay what do i mean by that so you can find some latent dimensions that basically control the viewpoint and the rest of the code is kind of controlling the content meaning the type of car and the viewpoint means the viewpoint of that car okay so if i look at it here we basically varied the viewpoint code and kept the this content called the rest of the code frozen and and this is just basically synthesized and the cool part is that it actually looks like you know multiple views of the same object it's not perfect like this guy the third the third object in the top row doesn't look exactly matched but most of them look like the same car in different views and the other side also also holds so if i keep a content like a viewpoint code fixed in each of these columns but they vary the the content code meaning different rows here i can actually get different cars in each viewpoint okay so this is basically again synthesized and that's precisely the data we need so we didn't do anything super special to our technique the only thing we were smart about was how we got the data and and no now now you can use these data to train our you know differentiable rendering pipeline and you got you know predictions like this you have an input image and a bunch of 3d predictions but also now we can do cars so the input image on the left and then the 3d prediction rendered in that same viewpoint here in this column and that's that prediction rendered in multiple different viewpoints just to showcase the 3d nature of the predictions and now we basically have this tool that can take any image and produce a 3d asset so we can have tons and tons of cars by just basically taking pictures okay here is a little demo in that omniverse tool where the user can now take a picture of the car and get out the 3d model notice that we also estimate materials because you can see the windshields are a little bit transparent and the car body looks like it's shiny so it's metal because we're also predicting 3d parts and you know it's not perfect but they're pretty good and now just uh you know a month ago we have a new version that can also animate this prediction so you can take an image predict this guard this guy and we can just put you know tires instead of the predicted tires you can estimate physics and you can drive these cars around so they actually become useful assets this is only on cars now but of course the system is general so we're gonna we're in the process of applying it to sorts of different content cool i think i don't know how much more time i have so maybe i'm just gonna skip today and i have always too much slides um so i have all these behaviors and whatever and i wanted to show you just the last project that we did because i think you guys gave me only 40 minutes um so you know i we also have done some work on animation using reinforcement learning and behavior that you know maybe i skipped here but we basically are building modular deep learning blocks for all the different aspects and the question is can we can we even sidestep all that can we just learn how to simulate data everything with one neural net and we and we're going to call it neural simulation so can we have one ai model that can just look at our interaction with the world and then be able to simulate that okay so you know in computer games we know that you know they accept some user action left right keyboard control or whatever and then the computer engine is basically synthesizing the next frame which is going to tell us you know how how the world has changed according to your action okay so what we're trying to attempt here is to replace the game engine with a neural net which means that we still want to have the interactive part of the of the game where the user is going to be inputting actions gonna be playing but the screens are going to be synthesized by a neural net which basically means that you know this neural net needs to learn how the world works right if i hit into a car it needs to you know produce a frame that's gonna look like that okay now in the beginning our first project was can we just learn how to emulate a game engine right can we take a pac-man and try to mimic it try to see if the neural net can can learn how to mimic pac-man but of course the interesting part is going to start where we don't have access to the game engine like the world right you can think of the world as being the matrix where we don't have access to the matrix but we still want to learn how to simulate and emulate the matrix um and that's really exciting future work but basically we have you know now we're just kind of trying to mimic how what what a game engine does where you're inputting some you know action and maybe the previous frame and then you'll have something called dynamics engine which is basically just an ls stand that's trying to learn how the dynamics in the world looks like how how frames change uh we have a rendering engine that takes that latent code is going to actually produce a nice looking image and we also have some memory which allows us to push any information that we want to be able to consistently produce you know the consistent gameplay uh in some some additional block here okay and and here's here was like our first result on pac-man and release this on the 40th birthday of batman [Music] what you see over here is all synthesized and the to me is even if it's such a simple simple game it's actually not so easy because you know the neural net needs to learn that uh pac-man if it eats the food the food needs to disappear if the ghost can become blue and then if you eat a blue ghost you survive otherwise you die so there's already a lot of different rules that you need to recover along with just like synthesizing images right and of course our next step is can we can we scale this up can we go to 3d games and can we eventually go to the real world okay so again here the control is going to be the steering control so like speed and the steering wheel this is done by the user by a human and what you see on the right side is you know the frames painted by by game gun by this model so here we're driving this car around and you can see what what the model is painting is is a pretty consistent world in fact and there's no 3d there's no nothing we're basically just synthesizing frames and here is a little bit more complicated version where um we try to synthesize other cars as well and this is on a carla simulator that was the game engine we're trying to emulate it's not perfect like you can see that the cars actually change color and resume it's quite amazing that it's able to do that entirely um and right now we have a version actually training on the real driving videos like a thousand hours of real driving and it's actually doing an amazing job already and you know so i think this could be a really good alternative on to the rest of the pipeline all right you know one thing to realize when you're doing something that's so broad and such a big problem is that you're never gonna solve it alone you're never gonna solve it alone so one one mission that i have is also to provide tools to community such that you know you guys can take it and build your own ideas and build your own 3d content generation methods okay so we just recently released 3d deep learning is an exciting new frontier but it hasn't been easy adapting neural networks to this domain cowlin is a suite of tools for 3d deep learning including a pi torch library and an omniverse application cowlin's gpu optimized operations and interactive capabilities bring much needed tools to help accelerate research in this field for example you can visualize your model's predictions as its training in addition to textured meshes you can view predicted point clouds and voxel grids with only two lines of code you can also sample and inspect your favorite data set easily convert between meshes point clouds and voxel grids render 3d data sets with ground truth labels to train your models and build powerful new applications that bridge the gap between images and 3d using a flexible and modular differentiable renderer and there's more to come including the ability to visualize remote training checkpoints in a web browser don't miss these exciting advancements in 3d deep learning research and how cowlin will soon expand to even more applications yeah so a lot of the stuff i talked about all the basic tooling is available so you know please take it and do something amazing with it i'm really excited about that just to conclude you know my goal is to really become democratized 3d content creation you know i want my mom to be able to create really good 3d models and she has no idea even how to use microsoft word or whatever so it needs to be super simple um have ai tools that are going to be able to also assist maybe more advanced users like artists game developers but just you know reduce the load of the boring stuff just enable their creativity to just come to play much faster than it can right now um and all that is also connecting to learning to simulate for robotics simulation is just a fancy game engine that needs to be real as opposed to being from fan fantasy but it can be really really useful for robotics applications right and what we have here is really just like two years and a half of our lab but you know there's so much more to do and i'm really hoping that you guys are gonna do this i just wanted to finish with one slide because you guys are students um my advice for for research um you know just learn learn learn this deep learning course is one don't stop here continue um one in very important aspect is just be passionate about your work and never lose that passion because that's where you're really going to be productive and you're really going to do good stuff if you're not excited about what the research you're doing though you know choose something else through something else don't rush for papers focus on getting really good papers as opposed to the number of papers that's not a good metric right hunting citations maybe also not the best metrics right some some not so good papers have a lot of citations some good papers don't have a lot of citations you're going to be known for the good work that you do um find collaborations find collaborators and that's particularly kind of in my style of research i want to solve real problems i want to solve problems which means that how to solve it is not clear and sometimes we need to go to physics sometimes we need to go to graphics sometimes we need to go to nlp whatever and i have no idea about some of those domains and you just want to learn from experts so it's really good to find collaborators and the last point which you know i have always used as guidance it's very easy to get frustrated because 99 of the time things won't work but just remember to have fun um because research is really fun and that's all from me whether you guys have some questions 

i've been at google for 16 years um the last six years i've been in life sciences and healthcare i i generally like running more interactive classes given the size of the group we thought polls might work so i'll launch a couple of polls throughout the talk and i'll try and keep an eye on chat as well if you guys have questions but um i might save them for the end as well so let me talk through the agenda a little bit i'm hoping to give you uh some information about ai in particular deep learning and healthcare and i will be using ai and deep learning interchangeably because that's just the name of our team is google ai but the examples you'll be seeing are all deep learning examples and as you know ai does include other things like robotics and non-neural network approaches so i just wanted to be clear that when i use them i don't need to be conflating them entirely once i cover what some of the key applications are for what we've done in ai and healthcare i'd like to discuss with you what the kind of unique opportunity i think we have because of deep learning to be able to uh create a much more equitable society while we're deploying ai models and we can talk about how that's possible and finally i'll touch on one last set of applications for ai and healthcare at the end here so on the in terms of uh the history behind ai and healthcare we are benefiting from the fact that we have uh the maturation of deep learning um and especially the end-to-end capabilities where we can learn directly from the raw data this is extremely useful for advances in computer vision and speech recognition which is highly valuable in the field of medical the other area as you all know is the increase in localized compute power via gpus um so that's allowed for neural networks to outperform non-neural networks um in the past and then the third is the value of all these open source large label data sets and internet being one for non-health related areas but there is uh public data sets like uk bio bank and even mimic which has been truly helpful and it's uh was developed actually um and produced at the mit labs so you'll be hearing about some of the applications of ai in healthcare next uh one of things that we do is to make sure we look at the needs in the industry and match that up to the tech capabilities healthcare specifically has enormous amounts of complex data sets annually it's estimated to be generating on the order of several thousand exabytes of healthcare data a year um just to put that in perspective a bit it's estimated that if you were to take the internet data um that's around something with more like hundreds of exabytes so it's it's several thousand times more um and what we're looking at in terms of uh those applications you'll see in bit is the pattern detection um and the ability to recognize for things like lesions and uh tumors and um really nuanced subtle imagery another area that it's useful for is just the addressing the limited medical expertise globally if you look to the right what you'd like to see is uh one medical specialist like a radiologist to about 12 000 people in the population but and what you can see on the graph to the right is that in developing countries it looks more like one to a hundred thousand or one to a million even and so the benefit of ai and healthcare is that it can help scale up running some of these complex tax tasks that are valuable that middle experts are capable of the third is uh really addressing human inconsistencies and we'll talk a little bit about this especially when we're talking about generating labels um ai models don't obviously suffer from recency or cognitive biases and they are also able to work tirelessly which is an issue when when you have to work overtime uh as in medical field which often happens let me just talk a little bit through the next application which is lung cancer uh the application what we developed was a computer diagnostic uh and in this case it was to help screen uh for lung cancer using low-dose ct scans um what you normally see um is the survival rates increasing dramatically if you catch it at earlier stages but about 80 percent of lung cancers are not caught early and what they use usually to do these screenings are these low-dose ct scans that if you look in this diagram to the right is these three-dimensional uh imaging that happens to your entire body it creates hundreds of images for the radiologists to look through and uh typically the actual um uh lung cancer signs are very subtle so what our models were able to do um when we looked at this was to actually not just outperform the state of the art but actually more importantly we compared it to the radiologists to see if there was an absolute reduction in both false positives and false negatives so false positives will lead to overutilization of the system and false negatives will lead to uh not being able to catch the cancer early enough and usually want to see both both reduced pathology is another area that's a hard deep learning problem and even more complex data this is on the left you can see when you take a biopsy you have slices of the body tissue and these are magnified up to 40 times and creates about 10 to 15 megapixels of information per slide the part that is inherently complex is when you're doing pathology you want to know both the magnified uh level highly magnified level of this tissue so that you can characterize the lesion and you also need to understand um the overall tissue architecture to provide context for it um and so that's at a lower power so you have a multi-scale problem and it is also inherently complex to be able to differentiate between benign and malignant tumors i there's hundreds of the different pathologies that can affect the tissue and so being able to visually differentiate is very challenging we built the model to detect breast cancer from pathology images and the pathologists actually had no false positives the model was able to capture more of the cancer lesions so it was greater than 95 compared to 73 that pathologists were getting but it also increased the number of false positives um this meant that what we tried uh then was to actually combine uh and have the model and pathologists work together um to see if the accuracy could improve and it absolutely did um and this com combined effort led to also development of an augmented microscope where you can see the model um detecting the patches inside the microsoft microscope view itself and we'll come back to the fact that the models had certain weaknesses and how we dealt with that later uh genomics is another area that's uh benefited significantly from uh deep learning um it's worth noting that uh when you do um whole genome sequences what you're you're doing is tearing up your um dna into a billion reads of about 100 bases i mean there's about a 30x over sampling with errors when you do that um when you try and uh figure out the sequence what you're trying to do is something like i take 30 years of a sunday newspaper 30 copies each with errors introduced and then shred them into 20 word snippets and then you try and put them back together that's essentially what's happening when you're doing your sequencing um and so we recast this problem as a deep learning problem uh we looked at how image recognition and and specifically convolutional neural networks would be able to perform in the space and developed a tool called deep variant which is open sourced and available for for anyone to use and we've been improving it over time this is proven to be uh highly accurate um the usfda runs a precision fta competition every few years and it's uh outperformed uh mostly won the awards for three out of four accuracy areas and you can see on the right that when you it's quite visually obvious when you actually get an error a false variant in the sequencing um so this is a clever way to actually be able to rapidly detect errors and variant calls so we talked about the different needs that um are in the medical field and one of them was the limited medical expertise there's one way to help them which is scaling up the tasks that they run so that they can be automated this is another way of addressing it which is returning time to the doctors what's happened is what you're seeing on in this picture is a girl who drew um her experience when visiting a doctor you can see the doctor's actually facing the computer to the left this sparked a lot of discussion within the health care industry about the cost of technology and how it's interfering with patient care um the doctors now at this point spend something on the order of six hours a day interacting with their electronic health records to get data entered one of the areas that's ripe for being able to support medical doctors is scribes human scribes have been deployed medical dictation has gone much better the automatic speech recognitions now have end-to-end models they're highly accurate and it's improved significantly also on natural language processing so these are all ways that um is more like an assistive kind of ai to help doctors relieve the burden of documentation from them i'm going to launch the poll right now just to see what people think is the most valuable application let me see here if i can do that and as i just to quickly recap there was um computer diagnostics which are useful for screening and diagnoses um there is and that was demonstrated with the radiology um there was approved prognosis um that's pathology is useful for um determining therapeutics um being able to determine treatment efficacy and and the progression of the disease um and that's what both pathology and genomics is highly utilized for and then returning time to experts is really the ai assistance through medical dictation describing okay great so let me just keep going while the is going um i want to talk about how we can actually achieve a greater moonshot so let me take a step back here where um we look at how the healthcare the world of healthcare looks right now it's tremendously filled with fragmentation it's fairly impersonal and it's inequitably distributed and one of the things i noted was that in tech we do amplify a system if you apply it to it so tech is is a way to both augment and scale up what exists um and so if you have if you're applying it to a broken system with perverse incentives it won't fix the system inherently it will accelerate it um but at the core of machine learning um and these deep learning technologies what we're doing is uh we're looking at the data very carefully and uh utilizing that to build predictions and um and determine outcomes in this case given that the world is not full of equity you run the risk of training the wrong models we published also a paper to help address this so societal inequities and biases are often codified in the data that we're using we actually have the opportunity to examine those historical biases and proactively promote a more equal future when we're developing the models you can do that by correcting for bias in the training data you can also uh correct bias in the model design and the problem formulation which and what you're trying to solve for and we'll talk about that in a bit uh and and finally if if none of that is applicable then you all you can also test and ensure for equal outcomes and resource allocations at the end of when you're deploying the ai models so this is um i used to work in google x which is google's effort to do moon shots the way we design define moon shots is the intersection of a huge problem breakthrough technology and radical solutions and a huge problem here is that the world is uncertain impersonal and it's also entire accuracy we have the benefit of a breakthrough tech right now which is ai and deep learning um and i'm just gonna say digital mobile tools is actually breakthrough tech for healthcare because they tend to lag about a decade behind other industries do the regulations safety privacy and quality needs and so a radical solution here is is we we actually think about not just improving the quality of care that we're delivering but making sure that when we do that we also make it more equitable um and i there's at every point in time when i see a technological wave happen um i do realize that at this point that it's an opportunity for us to reshape our future so in the case of um deep learning i i'd like to talk about the opportunities for actually moving um so i didn't realize the slides weren't advancing i want to talk about the opportunity to actually make the ai models much more um equitable and how we would do that so the two key areas i'll talk about is community participation um and how that's gonna affect the models and in the data evaluation um and then also planning for model limitations and how you can do that effectively with ai one of the things that we did was work with the uh regions that we were directly going to deploy the models with and so on the left here you see us working with um the team in india and on the right it's our team working with those in in thailand i what we found was that the social economic situation absolutely mattered in terms of where you would deploy the model um an example is while we developed the model with the ophthalmology centers and that's where the eye disease is happening and diabeteopathy is the leading cause of growing cause of blindness um in the world and this is where the models were developed but actually the the use case was most acute in uh the diabetic centers so the endocrinology offices um and people were not making the 100 meter distance to be able to go from the endocrinology offices to the ophthalmology offices because of access issues and uh challenges with um with lines and and and so on so forth so so this is an area that we explo uh explored using user research extensively to make sure that we thought through where the ai models would land and how that would impact the users another area that we looked at is when we're generating labels for the models you can see on the left that as classically you you would expect when you get more data on the models continues to improve so it kind of flattens out here at 60 000 images and and at some point that's sufficient and you're not going to get much more improvement from that um what you actually benefit from if you look to the right graph is improvement of the what we the quality of the labels or the what we refer to as the grades on the images um each doctor gives an image and grade which is their diagnostic opinion of what they think they're seeing um as we got multiple opinions on single images and were able to reconcile that we were able to continuously improve the model output um and improve the accuracy so this is something that's often said in the healthcare spaces if you ask three doctors you get four opinions because um even the doctors themselves may not be consistent with themselves over time uh the way that this is addressed um in some countries is to use this delphi method which is which was developed during the cold war it helps determine consensus where individual opinions vary and we developed a tool to do asynchronous adjudication of different opinions this has led to much higher ground truth uh data creation and and it's because of the fact that doctors actually sometimes just miss what what another doctor notices and so they generally do reconcile and are able to come to agreement on what the actual um severity or diagnosis should be um and so this was something that we saw uh really and that was really impactful because um when we were doing the analysis with ophthalmologists we would see things like 60 consistency across the doctors um and this was a way to actually address that level of variance and here's the last area um that i want to talk about for community engagement um this is around if you go even further upstream to the problem formulation this is a case where they didn't think through uh the inputs to their models and algorithms this algorithm was trying to determine the utilization needs of the community and they were using the health costs as a proxy for the actual health needs um this has led to uh inadvertently a racial bias because less money was spent on black patients and um and this was caught after the fact and so if you just click one more time um this is this is one of the key areas where um having input from the community uh would have actually got that much earlier on when they were doing the algorithm development this is something that we practice now frequently um and i know you guys are working on projects so i it'd be uh one of the polls that i want to put out there which just let me see if we can get it to launch is uh which one of these um uh approaches are actually potentially relevant at all for um the projects that you guys are working on okay great i'll keep going with the toxin while this is being saved and be nice to look back on it i on the left here um i mentioned earlier how our pathology models had certain weaknesses um in terms of false positives um but it also was capturing more of the cancer allegiance than the pathologists were so we developed a way to try to explain the models um through similar image lookup and what this has allowed to have happen was um it showed it uses a cluster algorithm and is able to find features that were not known um as before to pathologists that might be meaningful indicators of the actual uh diagnosis or prognosis um and in this case uh pathologists have started to use the tool to learn from it and um and there's also the benefit of the pathologist being able to recognize any issues with the models and inform the models to improve so you get a virtuous cycle of uh the models and the pathologists learning from each other on the right is another way that we use to explain the model output you see saliency maps um which is a way to just be able to identify which features are the model is actually paying attention to and in this case which pixels um the model is paying attention to and and light those up we do this so that we know that the um the way that the model is actually determining the uh the diagnostic whether it's a particular skin condition um that they're looking at the actual skin abnormalities and not some side potential unintentional correlation to skin tone or our demographic information and so this has been valuable to you as a way of checking the models that way and the last that i mentioned is is doing monolith model evaluation for equal outcomes um there's something for in the dermatology space known as the fitzpatrick skin type it allows you to see the different skin tones and what we do is to have test sets that are in the different skin tones to do the model evaluation to see if we do get equal outcomes and this is something where as the model developer and you have to make some hard choices if you see that your models aren't performing well off a particular um category or demographic i ideally what happens is you supplement your data set so that you can improve your models further to appropriately address those regions or you may have to make a decision to limit your model output so that there can be equal outcomes and sometimes you don't actually choose not to deploy the models and so these are some of the kind of real world implications of developing ai models in the healthcare space the last application i wanted to talk through with this group is the concept of health care typically in the past healthcare is thought of for patients and while every patient is a person not every person is a patient and patients typically are considered on the left here people who are sick or at risk they're entering the health care system the models are quite different when you're thinking about people of this nature whether they have acute or chronic diseases and they're the ones that we talked about a bit earlier which are screening diagnostics prognosis treatment those are what the models tend to focus on um if you're looking at people uh they are considered quote unquote well um but their health is impacted every single day by what we call like social determinants of health which are your environmental and social circumstances your behavioral and lifestyle choices um and uh how your genes are interacting with the environment and um the models here look dramatically different in terms of how you would approach the problem uh they tend to focus on preventative care so eating while sleeping well exercising and they also focus on public health which i think has been a big um known issue now with coronavirus and and of course screening is actually consistent across both sides so um when we talk about uh public health there's can be things like epidemiological models which are extremely valuable but there's also things that are happening right now especially probably one of the biggest global threats to public health is climate change and so one of the things that's happening in places like india is flood forecasting uh for public health alerts in in india there's a lot of alert fatigue actually and so it's it's actually unclear when they should care about the alerts or not what this team did was they focused on building a scalable high resolution hydraulic model using convolutional neural nets to estimate inputs like snow levels soil moisture estimation and permeability these hydraulic bundles uh simulates the water behavior across floodplains um and were high far more accurate than what was being used before and this has been deployed now to help with alerting and across the india regions for during the monsoon seasons um let's see and so i just want to leave this group with with the idea that uh on the climate change side there is a lot going on right now um nature is uh essential to health and plant but also the people that live on it so uh we currently rely on these ecosystem services and what that means is people rely on things like clean air water supply pollination of agriculture for food land stability and climate regulations and this is an area that's ripe for ai to be able to help understand far better and value those services um that we currently don't pay a lot for um but we'll probably have to in the future and so this last slide let me see if we can get it to appear is for the pull and just i wanted to compare and understand if the perception around health is any different in terms of what might be the most exciting for uh ai to be applied to thanks for launching the final poll and the last thing i want to leave the group with is none of the work that we do in the in healthcare is possible there's a huge team and a lot of collaboration that happens across medical research organizations and research institutes and healthcare systems um and so that is uh you know this is our team as it's grown over the years and um in different forms it's not all of our team even anymore but this is certainly where a lot of the work has been generated from let me take a look at the questions in chat now and so that i'll just recap what the pull results were so it looks like um the diagnostic models um uh 54 hmm oh yeah so i guess you guys can do multiple choice uh you can pick multiple ones so 50 but 60 people half the people um felt that the diagnostics and the therapeutics were valuable um and less interested in but still valuable the assistance thanks for filling those out um definitely let me look at the questions given the fast advancement in terms of new models what is the main bottleneck to scaling ml diagnostic solutions to more people around the world i it's the regulatory automation of meeting the regulatory needs um the long pull for diagnostics is ensuring patient safety proper regulation usually you go through fda rce marks and that can take uh time there's quality management systems that have to be built to make sure that the system is robust from a development perspective so it's software as a medical device and this is always going to be true in the case of when you're dealing with patients in terms of the other part maybe is the open source data sets having more labeled data sets out there so that uh everyone can have access and move that space forward uh is is valuable second question here good day sets are essential to developing useful equitable models what effort and technologies do we need to invest in to continue collecting data sets and for more models uh so one of the things that's happening is developing a scalable labeling infrastructure um that's that's one way to be able to generate better data sets but raw data is also ones that are directly reflecting the outcomes is valuable so an example is if you're thinking about data that's coming straight from the user in terms of their vital signs or their physiological signals these are things that are as close to ground truth as you can get about the individual's well-being and um and obviously uh what we saw with cobia 19 was it was even harder to get information like um how many deaths were actually happening um and what were the causes of those deaths and so these are the kinds of data sets that need to um these pipelines need to be thought of in the context of how can they be supporting public health goods and and how does that did it accurately get out the door so we do have an effort right now that um lots of people pulled into you especially for coronavirus which was um uh on github now which i can provide a link for later um and it's volunteers um who have built a transparent data pipeline for the source of the data provenance is very important to keep track of when you create these data sets to make sure you know where what what the purpose of the data is and who's how reliable it is and where the source is coming from so these are the kinds of things that need to be built out to inform the models that you build this last question how do you facilitate conversation awareness around potential algorithmic bias related to the products you were developing um several things one is that the team you build as much as of them that can be reflective of uh the be representative of a popular broader population is actually more meaningful than than i think people realize um there's so what i mean by that is if you have a diverse team working on it or you bring in people who uh can be contributors or um part of a consortium that can reflect on the problem space that you're trying to tackle that is actually um a really good way to hear and discover things that you might not have ever thought of before but again it can start with the team that you build and then the network around you that you are actually getting feedback loops from and you know if you can't afford it uh you you would want to do that in a way that is quite measurable and quantitative but if you can it's it's actually still quite meaningful to you um to proactively just have these conversations about what space you're going into and how you're going to think about the inputs to your models um all right so thank you uh it looks like those were majority of questions 

    hi everyone, let's get started. Good
afternoon and welcome to MIT 6.S191! TThis is really incredible to see the
turnout this year. This is the fourth year now we're teaching this course and
every single year it just seems to be getting bigger and bigger. 6.S191 is a
one-week intensive boot camp on everything deep learning. In the past, at
this point I usually try to give you a synopsis about the course and tell you
all of the amazing things that you're going to be learning. You'll be gaining
fundamentals into deep learning and learning some practical knowledge about
how you can implement some of the algorithms of deep learning in your own
research and on some cool lab related software projects. But this year I
figured we could do something a little bit different and instead of me telling
you how great this class is I figured we could invite someone else from outside
the class to do that instead.  So let's check this out first. Hi everybody and
welcome MIT 6.S191 the official introductory course on deep
learning to taught here at MIT. Deep learning is revolutionising so many
fields from robotics to medicine and everything in between. You'll the learn the fundamentals of this field and how you can build some of these
incredible algorithms. In fact, this entire speech and video are not real and
were created using deep learning and artificial intelligence. And in this
class you'll learn how. It has been an honor to speak with you today and I hope you enjoy the course! Alright. so as you can tell deep learning
is an incredibly powerful tool. This was just an example of how we use deep
learning to perform voice synthesis and actually emulate someone else's voice, in
this case Barack Obama, and also using video dialogue replacement to
actually create that video with the help of Canny AI. And of course you might as
you're watching this video you might raise some ethical concerns which we're
also very concerned about and we'll actually talk about some of those later
on in the class as well. But let's start by taking a step back and actually
introducing some of these terms that we've been we've talked about so far now. Let's start with the word intelligence. I like to define intelligence as the
ability to process information to inform future decisions. Now the field of
artificial intelligence is simply the the field which focuses on building
algorithms, in this case artificial algorithms that can do this as well: process information to inform future
decisions. Now machine learning is just a subset of artificial intelligence
specifically that focuses on actually teaching an algorithm how to do this
without being explicitly programmed to do the task at hand.
Now deep learning is just a subset of machine learning which takes this idea
even a step further and says how can we automatically extract the useful pieces
of information needed to inform those future predictions or make a decision
And that's what this class is all about teaching algorithms how to learn a task
directly from raw data. We want to provide you with a solid foundation of
how you can understand or how to understand these algorithms under the
hood but also provide you with the practical knowledge and practical skills
to implement state-of-the-art deep learning algorithms in Tensorflow which
is a very popular deep learning toolbox. Now we have an amazing set of lectures
lined up for you this year including Today which will cover neural networks
and deep sequential modeling. Tomorrow we'll talk about computer vision and
also a little bit about generative modeling which is how we can generate
new data and finally I will talk about deep reinforcement learning and touch on
some of the limitations and new frontiers of where this field might be
going and how research might be heading in the next couple of years. We'll spend
the final two days hearing about some of the guest lectures from top industry
researchers on some really cool and exciting projects. Every year these
happen to be really really exciting talks so we really encourage you to come
especially for those talks. The class will conclude with some final project
presentations which we'll talk about in a little a little bit and also some
awards and a quick award ceremony to celebrate all of your hard work. Also I
should mention that after each day of lectures so after today we have two
lectures and after each day of lectures we'll have a software lab which tries to
focus and build upon all of the things that you've learned in that day so
you'll get the foundation's during the lectures and you'll get the practical
knowledge during the software lab so the two are kind of jointly coupled in that
sense. For those of you taking this class for credit you have a couple different
options to fulfill your credit requirement first is a project proposal
I'm sorry first yeah first you can propose a project in optionally groups
of two three or four people and in these groups you'll work to develop a cool new
deep learning idea and we realized that one week which is the span of this
course is an extremely short amount of time to really not only think of an idea
but move that idea past the planning stage and try to implement something so
we're not going to be judging you on your results towards this idea but
rather just the novelty of the idea itself on Friday
each of these three teams will give a three-minute presentation on that idea
and the awards will be announced for the top winners judged by a panel of judges the second option in my opinion is a bit
more boring but we like to give this option for people that don't like to
give presentations so in this option if you don't want to work in a group or you
don't want to give a presentation you can write a one-page paper review of the
deep learning of a recent deepening of paper or any paper of your choice and
this will be due on the last day of class as well also I should mention that
and for the project presentations we give out all of these cool prizes
especially these three nvidia gpus which are really crucial for doing any sort of
deep learning on your own so we definitely encourage everyone to enter
this competition and have a chance to win these GPUs and these other cool
prizes like Google home and SSD cards as well also for each of the labs the three
labs will have corresponding prizes so it instructions to actually enter those
respective competitions will be within the labs themselves and you can enter to
enter to win these different prices depending on the different lab please
post a Piazza if you have questions check out the course website for slides
today's slides are already up there is a bug in the website we fixed that now so
today's slides are up now digital recordings of each of these lectures
will be up a few days after each class this course has an incredible team of
TAS that you can reach out to if you have any questions especially during the
software labs they can help you answer any questions that you might have and
finally we really want to give a huge thank to all of our sponsors who without
their help and support this class would have not been possible ok so now with
all of that administrative stuff out of the way let's start with the the fun
stuff that we're all here for let's start actually by asking ourselves a
question why do we care about deep learning well why do you all care about
deep learning and all of you came to this classroom today and why
specifically do care about deep learning now well to answer that question we
actually have to go back and understand traditional machine learning at its core
first now traditional machine learning algorithms typically try to define as
set of rules or features in the data and these are usually hand engineered and
because their hand engineered they often tend to be brittle in practice so let's
take a concrete example if you want to perform facial detection how might you
go about doing that well first you might say to classify a face the first thing
I'm gonna do is I'm gonna try and classify or recognize if I see a mouth
in the image the eyes ears and nose if I see all of those things then maybe I can
say that there's a face in that image but then the question is okay but how do
I recognize each of those sub things like how do I recognize an eye how do I
recognize a mouth and then you have to decompose that into okay to recognize a
mouth I maybe have to recognize these pairs of lines oriented lines in a
certain direction certain orientation and then it keeps getting more
complicated and each of these steps you kind of have to define a set of features
that you're looking for in the image now the key idea of deep learning is that
you will need to learn these features just from raw data so what you're going
to do is you're going to just take a bunch of images of faces and then the
deep learning algorithm is going to develop some hierarchical representation
of first detecting lines and edges in the image using these lines and edges to
detect corners and eyes and mid-level features like eyes noses mouths ears
then composing these together to detect higher-level features like maybe jaw
lines side of the face etc which then can be used to detect the final face
structure and actually the fundamental building blocks of deep learning have
existed for decades and they're under underlying algorithms for training these
models have also existed for many years so why are we studying this now well for
one data has become much more pervasive we're living in a the age of big data
and these these algorithms are hungry for a huge amounts of data to succeed
secondly these algorithms are massively parallel izybelle which means that they
can benefit tremendously from modern GPU architectures and hardware acceleration
that simply did not exist when these algorithms were developed and finally
due to open-source tool boxes like tensor flow which are which you'll get
experience with in this class building and deploying these models has
become extremely streamlined so much so that we can condense all this material
down into one week so let's start with the fundamental building block of a
neural network which is a single neuron or what's also called a perceptron the
idea of a perceptron or a single neuron is very basic and I'll try and keep it
as simple as possible and then we'll try and work our way up from there let's
start by talking about the forward propagation of information through a
neuron we define a set of inputs to that neuron as x1 through XM and each of
these inputs have a corresponding weight w1
through WN now what we can do is with each of these inputs and each of these
ways we can multiply them correspondingly together and take a sum
of all of them then we take this single number that's summation and we pass it
through what's called a nonlinear activation function and that produces
our final output Y now this is actually not entirely correct we also have what's
called a bias term in this neuron which you can see here in green so the bias
term the purpose of the bias term is really to allow you to shift your
activation function to the left and to the right regardless of your inputs
right so you can notice that the bias term doesn't is not affected by the X's
it's just a bias associate to that input now on the right side you can see this
diagram illustrated mathematically as a single equation and we can actually
rewrite this as a linear using linear algebra in terms of vectors and dot
products so instead of having a summation over all of the X's I'm going
to collapse my X into a vector capital X which is now just a list or a vector of
numbers a vector of inputs I should say and you also have a vector of weights
capital W to compute the output of a single perceptron all you have to do is
take the dot product of X and W which represents that element wise
multiplication and summation and then apply that non-linearity which here is
denoted as G so now you might be wondering what is
this nonlinear activation function I've mentioned it a couple times but I
haven't really told you precisely what it is now one common example of this
activation function is what's called a sigmoid function and you can see an
example of a sigmoid function here on the bottom right one thing to note is
that this function takes any real number as input on the x-axis and it transforms
that real number into a scalar output between 0 & 1
it's a bounded output between 0 & 1 so one very common use case of the sigmoid
function is to when you're dealing with probabilities because probabilities have
to also be bounded between 0 & 1 so sigmoids are really useful when you want
to output a single number and represent that number as a probability
distribution in fact there are many common types of nonlinear activation
functions not just the sigmoid but many others that you can use in neural
networks and here are some common ones and throughout this presentation you'll
find these tensorflow icons like you can see on the bottom right or sorry all
across the bottom here and these are just to illustrate how one could use
each of these topics in a practical setting you'll see these kind of
scattered in throughout the slides no need to really take furious notes at
these codeblocks like I said all of the slides are published online so
especially during your labs if you want to refer back to any of the slides you
can you can always do that from the online lecture notes now why do we care
about activation functions the point of an activation function is to introduce
nonlinearities into the data and this is actually really important in real life
because in real life almost all of our data is nonlinear and here's a concrete
example if I told you to separate the green points from the red points using a
linear function could you do that I don't think so right so you'd get
something like this oh you could do it you wouldn't do very good job at it and
no matter how deep or how large your network is if you're using a linear
activation function you're just composing lines on top of lines and
you're going to get another line right so this is the best you'll be able to do
with the linear activation function on the other hand nonlinearities allow you
to approximate arbitrarily complex
functions by kind of introducing these nonlinearities into your decision
boundary and this is what makes neural networks extremely powerful let's
understand this with a simple example and let's go back to this picture that
we had before imagine I give you a train network with weights W on the top right
so W here is 3 and minus 2 and the network only has 2 inputs x1 and x2 if
we want to get the output it's simply the same story as we had before we
multiply our inputs by those weights we take the sum and pass it through a
non-linearity but let's take a look at what's inside of that non-linearity
before we apply it so we get is when we take this dot product of x1 times 3 X 2
times minus 2 we mul - 1 that's simply a 2d line so we can plot that if we set
that equal to 0 for example that's a 2d line and it looks like this so on the x
axis is X 1 on the y axis is X 2 and we're setting that we're just
illustrating when this line equals 0 so anywhere on this line is where X 1 and X
2 correspond to a value of 0 now if I feed in a new input either a test
example a training example or whatever and that input is with this coordinates
it's has these coordinates minus 1 and 2 so it has the value of x1 of minus 1
value of x2 of 2 I can see visually where this lies with respect to that
line and in fact this this idea can be generalized a little bit more if we
compute that line we get minus 6 right so inside that before we apply the
non-linearity we get minus 6 when we apply a sigmoid non-linearity because
sigmoid collapses everything between 0 and 1 anything greater than 0 is going
to be above 0.5 anything below zero is going to be less than 0.5 so in is
because minus 6 is less than zero we're going to have a very low output this
point Oh 200 to we can actually generalize this idea for
the entire feature space let's call it for any point on this plot I can tell
you if it lies on the left side of the line that means that before we apply the
non-linearity the Z or the state of that neuron will be negative less than zero
after applying that non-linearity the sigmoid will give it a probability of
less than 0.5 and on the right side if it falls on the right side of the line
it's the opposite story if it falls right on the line it means that Z equals
zero exactly and the probability equals 0.5 now actually before I move on this
is a great example of actually visualizing and understanding what's
going on inside of a neural network the reason why it's hard to do this with
deep neural networks is because you usually don't have only two inputs and
usually don't have only two weights as well so as you scale up your problem
this is a simple two dimensional problem but as you scale up the size of your
network you could be dealing with hundreds or thousands or millions of
parameters and million dimensional spaces and then visualizing these type
of plots becomes extremely difficult and it's not practical and pause in practice
so this is one of the challenges that we face when we're training with neural
networks and really understanding their internals but we'll talk about how we
can actually tackle some of those challenges in later lectures as well
okay so now that we have that idea of a perceptron a single neuron let's start
by building up neural networks now how we can use that perceptron to create
full neural networks and seeing how all of this story comes together let's
revisit this previous diagram of the perceptron if there are only a few
things you remember from this class try to take away this so how a perceptron
works just keep remembering this I'm going to keep drilling it in you take
your inputs you apply a dot product with your weights and you apply a
non-linearity it's that simple oh sorry I missed the step you have dot
product with your weights add a bias and apply your non-linearity so three steps
now let's simplify this type of diagram a little bit I'm gonna remove the bias
just for simplicity I'm gonna remove all of the weight labels so now you can
assume that every line the weight associated to it and let's
say so I'm going to note Z that Z is the output of that dot product so that's the
element wise multiplication of our inputs with our weights and that's what
gets fed into our activation function so our final output Y is just there our
activation function applied on Z if we want to define a multi output neural
network we simply can just add another one of these perceptrons to this picture
now we have two outputs one is a normal perceptron which is y1 and y2 is just
another normal perceptron the same ideas before they all connect to the previous
layer with a different set of weights and because all inputs are densely
connected to all of the outputs these type of layers are often called dense
layers and let's take an example of how one might actually go from this nice
illustration which is very conceptual and and nice and simple to how you could
actually implement one of these dense layers from scratch by yourselves using
tensor flow so what we can do is start off by first defining our two weights so
we have our actual weight vector which is W and we also have our bias vector
right both of both of these parameters are governed by the output space so
depending on how many neurons you have in that output layer that will govern
the size of each of those weight and bias vectors what we can do then is
simply define that forward propagation of information so here I'm showing you
this to the call function in tensor flow don't get too caught up on the details
of the code again you'll get really a walk through of this code inside of the
labs today but I want to just show you some some high level understanding of
how you could actually take what you're learning and apply the tensor flow
implementations to it inside the call function it's the same idea again you
can compute Z which is the state it's that multiplication of your inputs with
the weights you add the bias right so that's right there
and once you have Z you just pass it through your sigmoid and that's your
output for that now tension flow is great because it's
already implemented a lot of these layers for us so we don't have to do
what I just showed you from scratch in fact to implement a layer like this with
two two outputs or a percept a multi layer a multi output perceptron layer
with two outputs we can simply call this TF Harris layers dense with units equal
to two to indicate that we have two outputs on this layer and there is a
whole bunch of other parameters that you could input here such as the activation
function as well as many other things to customize how this layer behaves in
practice so now let's take a look at a single layered neural network so this is
taking it one step beyond what we've just seen this is where we have now a
single hidden layer that feeds into a single output layer and I'm calling this
a hidden layer because unlike our inputs and our outputs these states of the
hidden layer are not directly enforced or they're not directly observable we
can probe inside the network and see them but we don't actually enforce what
they are these are learned as opposed to the inputs which are provided by us now
since we have a transformation between the inputs and the hidden layer and the
hidden layer and the output layer each of those two transformations will have
their own weight matrices which here I call W 1 and W 2 so its corresponds to
the first layer and the second layer if we look at a single unit inside of that
hidden layer take for example Z 2 I'm showing here
that's just a single perceptron like we talked about before it's taking a
weighted sum of all of those inputs that feed into it and it applies the
non-linearity and feeds it on to the next layer same story as before this
picture actually looks a little bit messy so what I want to do is actually
clean things up a little bit for you and I'm gonna replace all of those lines
with just this symbolic representation and we'll just use this from now on in
the future to denote dense layers or fully connected layers between two
between an input and an output or between an input and hidden layer and again if we wanted to implement this
intensive flow the idea is pretty simple we can just define two of these dense
layers the first one our hidden layer with n outputs and the second one our
output layer with two outputs we can cut week and like join them together
aggregate them together into this wrapper which is called a TF sequential
model and sequential models are just this idea of composing neural networks
using a sequence of layers so whenever you have a sequential message passing
system or sequentially processing information throughout the network you
can use sequential models and just define your layers as a sequence and
it's very nice to allow information to propagate through that model now if we
want to create a deep neural network the idea is basically the same thing except
you just keep stacking on more of these layers and to create more of an more of
a hierarchical model ones where the final output is computed by going deeper
and deeper into this representation and the code looks pretty similar again so
again we have this TF sequential model and inside that model we just have a
list of all of the layers that we want to use and they're just stacked on top
of each other okay so this is awesome so hopefully now you have an understanding
of not only what a single neuron is but how you can compose neurons together and
actually build complex hierarchical models with deep with neural networks
now let's take a look at how you can apply these neural networks into a very
real and applied setting to solve some problem and actually train them to
accomplish some task here's a problem that I believe any AI system should be
able to solve for all of you and probably one that you care a lot about
will I pass this class to do this let's start with a very simple two input model
one feature or one input we're gonna define is how many let's see how many
lectures you attend during this class and the second one is the number of
hours that you spend on your final projects I should say that the minimum
number of hours you can spend your final project is 50 hours now I'm just joking
okay so let's take all of the data from previous years and plot it on this
feature space like we looked at before green points are students that have
passed the class in the past and red points are people that have failed we
can plot all of this data onto this two-dimensional grid like this and we
can also plot you so here you are you have attended four lectures and you've
only spent five hours on your final exam you're on you're on your final project
and the question is are you going to pass the class given everyone around you
and how they've done in the past how are you going to do so let's do it we have
two inputs we have a single layered set single hidden layer neural network we
have three hidden units in that hidden layer and we'll see that the final
output probability when we feed in those two inputs of four and five is predicted
to be 0.1 or 10% the probability of you passing this class is 10% that's not
great news the actual prediction was one so you did pass the class now does
anyone have an idea of why the network was so wrong in this case exactly so we
never told this network anything the weights are wrong we've just initialized
the weights in fact it has no idea what it means to pass a class it has no idea
of what each of these inputs mean how many lectures you've attended and the
hours you've spent on your final project it's just seeing some random numbers it
has no concept of how other people in the class have done so far so what we
have to do to this network first is train it and we have to teach it how to
perform this task until we teach it it's just like a baby that doesn't know
anything so it just entered the world it has no concepts or no idea of how to
solve this task and we have to teach at that now how do we do that the idea here
is that first we have to tell the network when it's wrong so we have to
quantify what's called its loss or its error and to do that we actually just
take our prediction or what the network predicts and we compare it to what the
true answer was if there's a big discrepancy between the
prediction and the true answer we can tell the network hey you made a big
mistake right so this is a big error it's a big loss and you should try and
fix your answer to move closer towards the true answer which it should be okay
now you can imagine if you don't have just one student but now you have many
students the total loss let's call it here the empirical risk or the objective
function it has many different names it's just the the average of all of
those individual losses so the individual loss is a loss that takes as
input your prediction and your actual that's telling you how wrong that single
example is and then the final the total loss is just the average of all of those
individual student losses so if we look at the problem of binary classification
which is the case that we're actually caring about in this example so we're
asking a question will I pass the class yes or no binary classification we can
use what is called as the softmax cross-entropy loss and for those of you
who aren't familiar with cross-entropy this was actually a a formulation
introduced by Claude Shannon here at MIT during his master's thesis as well and
this was about 50 years ago it's still being used very prevalently today and
the idea is it just again compares how different these two distributions are so
you have a distribution of how how likely you think the student is going to
pass and you have the true distribution of if the student passed or not you can
compare the difference between those two distributions and that tells you the
loss that the network incurs on that example now let's assume that instead of
a classification problem we have a regression problem where instead of
predicting if you're going to pass or fail to class you want to predict the
final grade that you're going to get so now it's not a yes/no answer problem
anymore but instead it's a what's the grade I'm
going to get what's the number what so it's it's a full range of numbers that
are possible now and now we might want to use a different
type of loss for this different type of problem and in this case we can do
what's called a mean squared error loss so we take the actual prediction we take
the the sorry excuse me we take the prediction of the network we take the
actual true final grade that the student got we subtract them we take their
squared error and we say that that's the mean squared error that's the loss that
the network should should try to optimize and try to minimize so ok so
now that we have all this information with the loss function and how to
actually quantify the error of the neural network let's take this and
understand how to train train our model to actually find those weights that it
needs to to use for its prediction so W is what we want to find out W is the set
of weights and we want to find the optimal set of weights that tries to
minimize this total loss over our entire test set so our test set is this example
data set that we want to evaluate our model on so in the class example the
test set is you so you want to understand how likely you are to pass
this class you're the test set now what this means is that we want to find the
W's that minimize that total loss function which we call as the objective
function J of W now remember that W is just a aggregation or a collection of
all of the individual w's from all of your weights so here this is just a way
for me to express this in a clean notation but W is a whole set of numbers
it's not just a single number and you want to find this all of the W's you
want to find the value of each of those weights such that you can minimize this
entire loss function it's a very complicated problem and remember that
our loss function is just a simple function in terms of those weights so if
we plot in the case again of a two-dimensional weight problem so one of
the weights is on the x-axis one of the weights is on this axis and on the z
axis we have the loss so for any value of w we can see what the loss
would be at that point now what do we want to do we want to find the place on
this landscape what are the values of W that we get the minimum loss okay so
what we can do is we can just pick a random W pick a random place on this
this landscape to start with and from this random place let's try to
understand how the landscape is changing what's the slope of the landscape we can
take the gradient of the loss with respect to each of these weights to
understand the direction of maximum ascent okay that's what the gradient
tells us now that we know which way is up we can take a step in the direction
that's down so we know which way is up we reverse the sign so now we start
heading downhill and we can move towards that lowest point now we just keep
repeating this process over and over again until we've converged to a local
minimum now we can summarize this algorithm which is known as gradient
descent because you're taking a gradient and you're descending down down that
landscape by starting to initialize our rates wait randomly we compute the
gradient DJ with respect to all of our weights then we update our weights in
the opposite direction of that gradient and take a small step which we call here
ADA of that gradient and this is referred to as the learning rate and
we'll talk a little bit more about that later but ADA is just a scalar number
that determines how much of a step you want to take at each iteration how
strongly or aggressively do you want to step towards that gradient in code the
picture looks very similar so to implement gradient descent is just a few
lines of code just like the pseudocode you can initialize your weights randomly
in the first line you can compute your loss with respect to those gradients and
with respect to those predictions and your data given that gradient you just
update your weights in the opposite direction of that event of that vector
right now the magic line here is actually how
do you compute that gradient and that's something I haven't told you and that's
something it's not easy at all so the question is given a loss and given all
of our weights in our network how do we know which way is good which way is a
good place to move given all of this information and I never told you about
that but that's a process called back propagation and let's talk about a very
simple example of how we can actually derive back propagation using elementary
calculus so we'll start with a very simple network with only one hidden
neuron and one output this is probably the simplest neural network that you can
create you can't really get smaller than this computing the gradient of our loss
with respect to W to here which is that second way between the hidden state and
our output can tell us how much a small change in W 2 will impact our loss so
that's what the gradient tells us right if we change W 2 in the differential
different like a very minor manner how does our loss change does it go up or
down how does it change and by how much really so that's the gradient that we
care about the gradient of our loss with respect to W 2 now to evaluate this we
can just apply the chain rule in calculus so we can split this up into
the gradient of our loss with respect to our output Y multiplied by the gradient
of our walk or output Y with respect to W 2 now if we want to repeat this
process for a different way in the neural network let's say now W 1 not W 2
now we replace W 1 on both sides we also apply the chain rule but now you're
going to notice that the gradient of Y with respect to W 1 is also not directly
computable we have to apply the chain rule again to evaluate this so let's
apply the chain rule again we can break that second term up into with respect to
now the the state Z ok and using that we can kind of back propagate all of these
gradients from the output all the way back to the input that allows our error
signal to really propagate from output to input and
allows these gradients to be computed in practice now a lot of this is not really
important or excuse me it's not as crucial that you understand the
nitty-gritty math here because in a lot of popular deep learning frameworks we
have what's called automatic differentiation which does all of this
back propagation for you under the hood and you never even see it which is
incredible it made training neural networks so much easier you don't have
to implement back propagation anymore but it's still important to understand
how these work at the foundation which is why we're going through it now ok
obviously then you repeat this for every single way in the network here we showed
it for just W 1 and W 2 which is every single way in this network but if you
have more you can just repeat it again keep applying the chain rule from output
to input to compute this ok and that's the back prop algorithm in theory very
simple it's just an application of the chain rule in essence but now let's
touch on some of the insights from training and how you can use the back
prop algorithm to train these networks in practice optimization of neural
networks is incredibly tough in practice so it's not as simple as the picture I
showed you on the colorful one on the previous slide here's an illustration
from a paper that came out about two or three years ago now where the authors
tried to visualize the landscape of a of a neural network with millions of
parameters but they collapsed that down onto just two-dimensional space so that
we can visualize it and you can see that the landscape is incredibly complex
it's not easy there are many local minima where the gradient descent
algorithm could get stuck into and applying gradient descent in practice in
these type of environments which is very standard in neural networks can be a
huge challenge now we're called the update equation
that we defined previously with gradient descent this is that same equation we're
going to update our weights in the direction in the opposite direction of
our gradient I didn't talk too much about this parameter ADA I pointed it
out this is the learning rate it determines
how much of a step we should take in the direction of that gradient and in
practice setting this learning rate can have a huge impact in performance so if
you set that learning rate to small that means that you're not really trusting
your gradient on each step so if ADA is super tiny
that means on each time each step you're only going to move a little bit towards
in the opposite direction of your gradient just in little small increments
and what can happen then is you can get stuck in these local minima because
you're not being as aggressive as you should be to escape them now if you set
the learning rate to large you can actually overshoot completely and
diverge which is even more undesirable so setting the learning rate can be very
challenging in practice you want to pick a learning rate that's large enough such
that you avoid the local minima but small offs such that you still converge
in practice now the question that you're all probably asking is how do we set the
learning rate then well one option is that you can just try a bunch of
learning rates and see what works best another option is to do something a
little bit more clever and see if we can try to have an adaptive learning rate
that changes with respect to our lost landscape maybe it changes with respect
to how fast the learning is happening or a range of other ideas within the
network optimization scheme itself this means that the learning rate is no
longer fixed but it can now increase or decrease throughout training so as
training progressive your learning rate may speed up you may take more
aggressive steps you may take smaller steps as you get closer to the local
minima so that you really converge on that point and there are many options
here of how you might want to design this adaptive algorithm and this has
been a huge or a widely studied field in optimization theory for machine learning
and deep learning and there have been many published papers and
implementations within tensor flow on these different types of adaptive
learning rate algorithms so SGD is just that vanilla gradient descent that I
showed you before that's the first one all of the others are all
adaptive learning rates which means that they change their learning rate during
training itself so they can increase or decrease depending on how the
optimization is going and during your labs we really encourage you again to
try out some of these different optimization schemes see what works what
doesn't work a lot of it is problem dependent there are some heuristics that
you can you can get but we want you to really gain those heuristics yourselves
through the course of the labs it's part of building character okay so let's put
this all together from the beginning we can define our model which is defined as
this sequential wrapper inside of this sequential wrapper we have all of our
layers all of these layers are composed of perceptrons or single neurons which
we saw earlier the second line defines our optimizer which we saw in the
previous slide this can be SGD it can also be any of
those adaptive learning rates that we saw before now what we want to do is
during our training loop it's very it's the same stories again as before
nothing's changing here we forward pass all of our inputs through that model we
get our predictions using those predictions we can evaluate them and
compute our loss our loss tells us how wrong our network was on that iteration
it also tells us how we can compute the gradients and how we can change all of
the weights in the network to improve in the future and then the final line there
takes those gradients and actually allows our optimizer to update the
weights and the trainable variables such that on the next iteration they do a
little bit better and over time if you keep looping this will converge and
hopefully you should fit your data no now I want to continue to talk about
some tips for training these networks in practice and focus on a very powerful
idea of batching your data into mini batches so to do this let's revisit the
gradient descent algorithm this gradient is actually very computationally
expensive to compute in practice so using the backprop algorithm is
a very expensive idea and practice so what we want to do is actually not
compute this over all of the data points but actually computed over just a single
data point in the data set and most real-life applications it's not actually
feasible to compute on your entire data set at every iteration it's just too
much data so instead we pick a single point randomly we compute our gradient
with respect to that point and then on the next iteration we pick a different
point and we can get a rough estimate of our gradient at each step right so
instead of using all of our data now we just pick a single point I we compute
our gradient with respect to that single point I and what's a middle ground here
so the downside of using a single point is that it's going to be very noisy the
downside of using all of the points is that it's too computationally expensive
if there's some middle ground that we can have in between so that middle
ground is actually just very simple you instead of taking one point and instead
taking all of the points let take a mini batch of points so maybe something on
the order of 10 20 30 100 maybe depending on how rough or accurate you
want that approximation of your gradient to be and how much you want to trade off
speed and computational efficiency now the true gradient is just obtained by
averaging the gradient from each of those B points so B is the size of your
batch in this case now since B is normally not that large like I said
maybe on the order of tens to a hundreds this is much faster to compute than full
gradient descent and much more accurate than stochastic gradient descent because
it's using more than one point more than one estimate now this increase in
gradient accuracy estimation actually allows us to converge to our target much
quicker because it means that our gradients are more accurate in practice
it also means that we can increase our learning rate and trust each update more
so if we're very noisy in our gradient estimation we probably want to lower our
learning rate a little more so we don't fully step in the wrong direction if
we're not totally confident with that gradient if we have a larger batch of
gradient of data to they are gradients with we can trust
that learning great a little more increase it so that it steps it more
aggressively in that direction what this means also is that we can now massively
paralyze this computation because we can split up batches on multiple GPUs or
multiple computers even to achieve even more significant speed ups with this
training process now the last topic I want to address is that of overfitting
and this is also known as the problem of generalization in machine learning and
it's actually not unique to just deep learning but it's a fundamental problem
of all of machine learning now ideally in machine learning we want a model that
will approximate or estimate our data or accurately describes our data let's say
like that said differently we want to build models that can learn
representations from our training data that's still generalize to unseen test
data now assume that you want to build a line that best describes these points
you can see on the on the screen under fitting describes if we if our model
does not describe the state of complexity of this problem or if we
can't really capture the true complexity of this problem while overfitting on the
right starts to memorize certain aspects of our training data and this is also
not desirable we want the middle ground which ideally we end up with a model in
the middle that is not too complex to memorize all of our training data but
also one that will continue to generalize when it sees new data so to
address this problem of regularization in neural network specifically let's
talk about a technique of regularization which is another way that we can deal
with this and what this is doing is it's trying to discourage complex information
from being learned so we want to eliminate the model from actually
learning to memorize the training data we don't want to learn like very
specific pinpoints of the training data that don't generalize well to test data
now as we've seen before this is actually crucial for our models to be
able to generalize to our test data so this is very important the most popular
regularization technique deep learning is this very basic idea of
drop out now the idea of drop out is well actually let's start with by
revisiting this picture of a neural network that we had introduced
previously and drop out during training we randomly set some of these
activations of the hidden neurons to zero with some probability so I'd say
our probability is 0.5 we're randomly going to set the
activations to 0.5 with probability of 0.5 to some of our
hidden neurons to 0 the idea is extremely powerful because it allows the
network to lower its capacity it also makes it such that the network can't
build these memorization channels through the network where it tries to
just remember the data because on every iteration 50% of that data is going to
be or 50% of that memorization or memory is going to be wiped out so it's going
to be forced to to not only generalize better but it's going to be forced to
have multiple channels through the network and build a more robust
representation of its prediction now we just repeat this on every iteration so
on the first iteration we dropped out one 50% of the nodes on the next
iteration we can drop out a different randomly sampled 50% which may include
some of the previously sampled nodes as well and this will allow the network to
generalize better to new test data the second regularization technique that
we'll talk about is the notion of early stopping so what I want to do here is
just talk about two lines so during training which is the x-axis here we
have two lines the y-axis is our loss curve the first line is our training
loss so that's the green line the green line tells us how our training data how
well our model is fitting to our training data we expect this to be lower
than the second line which is our testing data
so usually we expect to be doing better on our training data than our testing
data as we train and as this line moves forward into the future both of these
lines should kind of decrease go down because we're optimizing the network
we're improving its performance eventually though there becomes a point
where the training data starts to diverge from the testing data now what
happens is that the training day should always continue to fit or the
model should always continue to fit the training data because it's still seeing
all of the training data it's not being penalized from that except for maybe if
you drop out or other means but the testing data it's not seeing so at some
point the network is going to start to do better on its training data than its
testing data and what this means is basically that the network is starting
to memorize some of the training data and that's what you don't want so what
we can do is well we can perform early stopping or we can identify this point
this inflection point where the test data starts to increase and diverge from
the training data so we can stop the network early and make sure that our
test accuracy is as minimum as possible and of course if we actually look at on
the side of this line if we look at on the left side that's where a model is
under fit so we haven't reached the true capacity of our model yet so we'd want
to keep training if we didn't stop yet if we did stop already and on the right
side is where we've over fit where we've passed that early stopping point and we
need to like basically we've started to memorize some of our training did and
that's when we've gone too far I'll conclude this lecture by just
summarizing three main points that we've covered so far first we've learned about
the fundamentals of neural networks which is a single neuron or a perceptron
we've learned about stacking and composing these perceptrons together to
form complex hierarchical representations and how we can
mathematically optimize these networks using a technique called back
propagation using their loss and finally we address the practical side of
training these models such as mini batching regularization and adaptive
learning rates as well with that I'll finish up I can take a couple questions
and then we'll move on to office lecture on deep sequential modeling I'll take
any like maybe a couple questions if there are any now thank you 

you so let's get started my name is avi I'm also a co organizer and lecturer for success 1 9 1 and in the second lecture we're going to really dive in and extend off the foundations that Alexander describes to discuss deep sequence modeling and in the first lecture we really learned about the essentials of neural networks and how we can sack perceptrons to build what are called feed-forward models will now turn our attention to applying neural networks to problems which involve sequential processing of data and we'll learn why these sorts of tasks require a fundamentally different type of network architecture from what we've seen so far so to begin and really motivate the need for for processing sequential data let's consider a super super simple example suppose we have this picture of a ball right and we want to predict where this ball will travel to next now if I gave you no prior information about the trajectory of the ball or its history any guess on its next position would be exactly that just a random guess but if in addition to this current location I also gave you a bit of information about its previous locations the problem becomes much easier and I think we can all agree that we have a very clear sense of where this ball is likely to travel to next and so this is really you know to kind of set up this this idea of processing and handling sequential data and in truth if if you consider it sequential data is all around us for example audio can be split up into a sequence of sound waves and text can be split up into sequences of either characters or words and between beyond these two-bit kritis examples that we encounter every day there are many more cases in which sequential processing may be useful from an medical signals - like aegs - projecting stock prices - inferring and understanding genomic sequences so now that we've gone in a sense of some examples of what sequential data looks like let's take a look at another simple problem and that's the question of being able to predict the next word and here let's suppose we have a language model right where we have a sentence or a phrase and we're trying to predict the next word that comes next in that sentence or afraid or phrase so for example let's consider this sentence this morning I took my cat for a walk and yes you heard that right this morning I took my cat for a walk and let's say we're given these words this morning I took my cat for a and we want to try to predict the next word walk and since we're here to learn about deep learning and this is a class on deep learning let's say we want to build a deep neural network like a feed-forward neural network from lecture 1 to do exactly this and one problem that we will very immediately run into is that this feed-forward Network can only take a fixed length input vector as its input we have to specify the size of that input right from the start and the reason why you can maybe imagine that this is going to be a problem for our task is because sometimes the model will have seen a phrase that has five words sometimes it will have the scene of phrase that has seven sometimes it will have a scene ten words so our model needs a way to be able to handle variable length inputs and one way we can we can do this is by using a fixed window and that means that we force our input vector to be just a certain length so in this case - so given these two words we want to we want to try to predict the next one and this means that no matter where we're trying to make that next prediction our model will only be taking in those previous two words as its infant and you know because we have to also think about how we can numerically represent this data and one way we can do that is by taking a fixed length vector and allocating some space in that vector for the first word and some space in that vector for the second word and in those spaces encoding the identity of each word but this is really problematic and that's because since we're using this fixed window of only two words we're giving ourselves a very limited history in trying to solve this this problem of being able to predict the next word in the sentence and that means that we cannot effectively model long term dependencies which is really important too in sentences like this one where we clearly need information from much earlier in the sentence to be able to accurately predict the next word and if we were only looking at the past two words or the past three words or even the past five words we wouldn't be able to make this next prediction which we all know is French and so this means that we really need a way to integrate the information this in the sentence from start to finish but also that we need to be able to represent this information as a fixed length input vector and so one way we can do this is by actually using the entire sequence but representing it as a set of counts over a particular vocabulary and this representation is commonly called a bag of words where now each flaw in our input vector represents a word and the value in that slot represents the number of times that that word appears in our sentence and so here we have a fixed length vector regardless of the the identity of the sentence but what differs sentence the sentence is how the counts over this vocabulary change and we can feed this into our model as an input to be able to generate a prediction but there's another big problem with this and that's the fact that using just accounts means that we lose all sequential information and all information about the prior history and so these two sentences which have completely opposite semantic meanings would have the exact same representations in this bag of words format and that's because they have the exact same words just in a different order so obviously this is going isn't going to work right another idea could be simply to extend our fixed window and think that by okay let's let's look at more words maybe we can get more of the context we need to be able to generate this prediction and so we can represent our sentence in this way feed it into our model generate our prediction and the problem here is that if we were to feed this vector into say a feed-forward neural network each of these inputs right this morning took the cat would have a separate weight content connecting it to the network and so if we repeatedly were to see the words this morning at the beginning of the sentence the network may be able to learn that this morning represents a time or a setting but if this morning were to suddenly appear later in that fixed fixed length vector at the end of a sentence the network may have difficulty recognizing that this morning means this morning because the parameters that are here at the end of this they have seen the end of this vector may never have seen the phrase this morning before and the parameters from the beginning of the sentence weren't shared across the entire sequence and so this really this really motivates this need for for being able to attract long term dependencies and have parameters that can be shared across the entirety of our of our sequence and so I hope that this this simple example of having a sentence where we're trying to predict the next word motivates sort of a concrete set of design criteria that we need to be keeping in mind when we are thinking about sequence modeling problems specifically we need to develop models that can handle variable length input sequences that are able to track long term dependencies in the data that can maintain information about the sequences order and to share parameters across across the entirety of the sequence and today we're going to explore how we can use a particular type of network architecture called a recurrent neural network or RNN as sort of this general paradigm for sequential processing and sequence modeling problems and so first I'll go through the the key principles behind are n ends and how they're a fundamentally different type of architecture from what alexander introduced and how the the RNN computation actually works and before we do that let's take one step back again and consider the standard feed-forward neural network that we discussed in the first lecture and in this architecture as you can see data is propagating in one direction right from input to output and we already motivated Y and network like this can't really handle sequential data and recurrent neural networks are particularly well-suited for handling cases where we have a sequence of inputs rather than a single input and they're great for problems in which a sequence of data is being propagated to give a single output so for example we can imagine where we're training a model that takes as input a sequence of words and outputs a sentiment or an emotion associated with that sequence we can also consider cases where instead of returning a single output we could have a sequence of inputs propagate them through our network and and I each time step in the sequence generate an output so an example of that would be in text or music generation and you'll actually get the chance to explore a model like this later on in the lab and beyond these two examples there are really many many other cases in which recurrent neural networks have been applied for a sequential processing so I still haven't told you the answer of what fundamentally is a recurrent neural network and as I mentioned before and hopefully that we've hammered home standard neural networks go from input to output in one direction and we see that they're not able to maintain information about a previous events in a sequence of events in contrast recurrent neural networks or RNs are these networks that have these loops in them which actually allows for information to persist over time and so in this diagram at some time step denoted by T or RN n takes in as input X of T and at that time step it computes this value Y hat of T which is then outputted as the output of the network and in addition to that output it's computing this internal state update H of T and then it passes this information about its internal state from this time step of the network to the next and we call these networks with loops in them recurrent because information is being passed from one time step to the next internally within the network and so what's going on sort of under the hood of this network and how is information actually being passed time step to time step the way that's done is by using a simple recurrence relation to process the sequential data specifically RN ends maintain this internal state H of T and I each time step they apply a function f that's a parameter I by a set of weights W to update this state H and the key concept here is that this stay update is based both on the previous state from the previous time step as well as the current input the network is receiving and the really important point here is that in this computation it's the same function f of W and the same set of parameters that are used at every time step and it's those set of parameters that we actually learn during the course of training and so to get more intuition about rnns in in sort of a more codified manner let's step through the aren't an algorithm to get a better sense of how these networks work and so this pseudocode kind of breaks it down into a few simple steps we begin by initializing our RM and we also initialize the hidden state of that network and we can know a sentence for which we are interested in predicting the next word the RNN computation simply consists of them looping through the words in this sentence and I each time step we feed both the current word that we're considering as well as the previous hidden state of our Arnon into the network which can then generate a prediction for the next word in the sequence and uses this information to update its hidden state and finally after we've looped through all the words in the sentence our prediction for that missing word is simply the ardennes output at that final time step after all the words have been fed through the model and so as I mentioned as you've seen right this RNN computation includes both this internal state update as well as this this formal output vector and so we'll next walk through how these computations actually occur and the concept is really similar to what alexander introduced in lecture 1 given our input vector X of T the RNN applies a function to update its hidden state and this function is simply a standard neural net operation just like we saw in the first lecture it consists of multiplication by a weight matrix and application of a non-linearity but the key difference is that in this case since we're feeding in both the input vector X of T and the previous state as inputs to this function we have to separate weight matrices and we then can apply our non-linearity to the sum of the two terms that result after multiplication by these two weight matrices and finally our output Y of T at a given time step is then a modified transformed version of this internal state and that simply results from fall from another multiplication by a separate weight matrix and that's how the RNN can actually update both its hidden state and actually produce an output and so so far we've we've seen this one depiction of our n ends that shows them as having these loops that feed back in on themselves but another way we can represent this is by actually unrolling this loop over time and so if we do that what we'll see is that we can think of our n ends as having sort of multiple copies of the same Network where each passes a message on to its descendant and what is that message that's passed it's based on H of T the internal state and so when we chain the RNN module together in this chain like structure this can really highlight how and why RNs are so well suited for processing sequential data and in this representation we can actually make our weight matrices explicit beginning with the weights that transform the input to the hidden state the weights that are used to transform the previous hidden state to the current hidden se and finally the hidden state to the output and to remind you once again it's important to know here that we are using the same weight matrices at every time step and from these outputs we can compute a loss at each time step and this computation of the loss will then complete our forward pass our forward propagation through the network and finally to define the total loss we simply sum the losses from all the individual time steps and since our loss consists of these individual contributions over time this means that in training the network we will have to also somehow involve this time component so now that you've gone a bit of a sense of how these rnns are constructed and how they function we can actually walk through a simple an example of how we can implement an RNN from scratch in tensor flow and so the RNN is defined as a layer and we can build it from inheriting the inheriting from the layer class that alexander introduced in the first lecture and so we can also initialize our weight matrices and initialize the hidden state of our RNN cell to zero and they're really the key step here is defining the call function which describes how we make a forward pass through the network given an input X and to break down this call function what occurs is first we update the hidden state according to that equation we saw earlier we take the previous hidden state and the input X multiply them by the relevant weight matrices sum them and then pass them through a non-linearity like at an H shown here and then the output is simply a transformed version of the hidden state and at each time step we return both the current output and the the updated hidden state and as a kind of Alexander showed you in how you know we can define a dense layer from scratch but tensorflow has made it really easy for us by having this built-in dense layer the same applies for our Nance and conveniently tensorflow has implemented these types of Arlen's cells for us and it's called the simple RNN layer and you'll get some practice using these in the lab later on all right so our next step is how to how do we actually develop an algorithm for training RN ends and that's an algorithm that's called back propagation through time and to remind you as we saw we trained feed-forward models by first making a forward pass through the network that goes from input to output and this is the standard feed-forward model where the layers are densely connected and in order to train this model we can then back propagate the gradients back through the network taking the derivative of the loss with respect to each weight parameter in the network and then adjusting the parameters in order to minimize the loss but in RN ends as we walk through earlier right our forward pass through the network also consists of going forward across time updating the cell state based on the input and the previous state generating an output Y at that time step computing a loss at that time step and then finally summing these losses from the individual time steps to get our total loss and so that means that instead of back propagating errors through a single feed-forward network at a single time set in RN ends errors are back propagated at each individual time step and then finally across all time steps all the way from where we are currently to the beginning of the sequence and this is the reason why it's called back propagation through time right because as you can see right all the errors are flowing back in time to the beginning of our data sequence and so if we take a quick closer look at how gradients actually flow this chain of repeating modules you can see that between each time step we need to perform a matrix and multiplication involving this way matrix W and remember right that this the cell update also results from a nonlinear activation function and that means that this computation of the gradient that is the derivative of the loss with respect to the parameters all tracing all the way back to our initial state requires many repeated multiplications of this weight matrix as well as repeated use of the derivative of our activation function and this can be problematic and the reason for that is we can have one of two scenarios that could be particularly problematic if many of these values that are involved in these repeated multiplications such as the way matrix or the gradients themselves are large greater than one we can run into a problem that's called the exploding gradients problem and that describes what happens when in gradients become extremely large and we can to optimize them and one way we may be able to mitigate this is by performing what's called gradient clipping which essentially amounts to scaling back large gradients so that their values are smaller we can also have the opposite problem where now our wave values or gradients are too small and this is what is known as the vanishing gradient problem when gradients become increasingly and increasingly smaller as we make these repeated multiplications and we can no longer train the network and this is a big and very real problem when it comes to training Arn ins and today we'll we'll go through three ways in which we can address this problem choosing our activation function initializing our weights cleverly and also designing our network architecture to actually be able to handle this efficiently and so to provide some further motivation and intuition as to why this vanishing gradient issue is a problem let's consider an example where you keep multiplying a number by some some number by something that's in between zero and one and as you keep doing this repeatedly that number is going to keep shrinking and shrinking and in eventually it's going to vanish and when this happens two gradients what this means is that it's harder to propagate errors further back into the past because the fact that the gradients are becoming smaller and smaller which means that will actually end up biasing our network to capture more short term dependencies which may not always be a problem right sometimes we only need to consider very recent information to perform our tasks of interest and so to make this concrete let's consider our example from the beginning of lecture a language model where we are trying to predict the next word and so in this phrase if we're trying to predict the last word in this phrase it's relatively obvious what the next word is going to be and there's not that much of a gap between the key relevant information like the word cloud and the place where the prediction is needed and so here an RNN can you know use relatively short term dependent dependencies to generate this prediction however there are other cases where more context is necessary like this example and so here the more recent information suggests that the next word is most likely the name of a language but we need we need the context of France which is way earlier in this sentence to be able to fill in the relevant the gaps that are missing and to identify which language is is correct and so as this gap between what is semantically important grows standard rnns become increasingly unable to sort of connect the dots and link these relevant pieces of information together and that's because of this issue of the vanishing gradient problem so how can we alleviate this right the first trick is pretty simple we can choose our activation function that the network is actually using specifically both the tan H and sigmoid activation functions have derivatives that are less than 1 except for the 10 H which can have derivatives that except for when when tan H is equal to 0 when X is equal to 0 in contrast if we use this particular activation function called arulu the derivative is 1 whenever X is greater than 0 and this helps prevent the value of our derivative F prime from shrinking the gradients but keep keep in mind that this relu function is only only has a gradient of 1/4 when x is greater than 0 which is another significant consideration another simple trick we can do is to be smart in how we initialize the weights the parameters of our network and it turns out that initializing the weights to the identity matrix helps them helps prevent them from shrinking to zero too rapidly during back propagation but the final and really most robust solution is to use a slightly more complex recurrent unit that can more effectively track long term dependencies in the data by controlling what information is passed through and what information is used to update its internal state specifically this is the the concept of a gated cell and many types of gated cells and architectures exist and today we're going to focus our attention on one type of gated cell called a long short-term memory network or LST M which is really well-suited for learning long-term dependencies to overcome this vanishing gradient problem and Ellis hams work really well on a bunch of different types of tasks and they're extremely extremely widely used by the deep learning community so hopefully this gives you a good overview and a concrete sense of why they're so powerful okay so to understand why Ellis hams are special let's think back to the general structure of an RNN and in this depiction I'm it's a slightly different depiction than what I've showed you before but it reinforces this idea that all recurrent neural networks have this form of a series of repeating modules and what we're looking at here is a representation where the the inner functions of the RNN are defined by by these black lines that depict how information is flowing within the RNN cell and so if we break this down in a standard RNN we have this single neural net layer that performs this computation in this case it's a tan H layer our cells state H of T is a function of the previous cell state H of T minus one as well as the current input X of T and as you can see that the red lines depict sort of how this information is flowing within the RN and so and at each time step we also generate an output Y of T which is a transformation of the internal RNN state l STM's also have this chain like structure but now the repeating module that's contained within the RN n cell is slightly more complex and so in an LS TM this repeating unit contains these different interacting layers that you know I don't want you to get too bogged down into the details of what these computations are but the key the key point is that these layers interact to selectively control the flow of information within the cell and we'll walk through how how these how these layers work and how this enable LSTs to track and store information throughout many time steps and the key building block behind the LS TM is this structure called a gate which functions to enable the L SCM to be able to selectively add or remove information to its cell state and gates consist of a neural net layer like a sigmoid shown in yellow and a point wise multiplication shown in red and so if we take a moment to think about what a gate like this may be doing in this case the sigmoid function is forcing its input to be between 0 and 1 right and intuitively you can think of this as capturing how much of the information that's passed through the gate should be retained it's between nothing right which is 0 or it's between everything 1 and this is effectively gating the flow of information right and Ellis hams process information through four simple steps and if there's anything that you take away from this lecture I hope this is that point they first forget they're irrelevant history they then perform computation to store relevant parts of new information use these two steps together to selectively update their internal state and finally they generate an output forget store update output so to walk through this a little bit more the first step in the LCM is to decide what information is going to be thrown away from the cell state to forget irrelevant history and that's a function of both the prior internal state H of T minus one as well as the input X of T because some of that information may not be important next the LCM can decide what part of new of the new information is relevant and use this to store store this information into its cell state then it takes both the relevant parts of both the prior information as well as the current input and uses this to selectively update its so state and finally it can return an output and this is known as the output gate which controls what information encoded in the cell state is sent to the network in the next as input in the next time step and so to re-emphasize right I don't want you two to get too stuck into the details of how these computations work and in the mathematical sense but the intuition and the key takeaway that we want you to have about LS TMS is the sequence of how they regulate information flow and storage forgetting irrelevant history storing what's new and what's important using that to update the internal state and generating an output so this hopefully gives you a bit of sense of how Ella stamps can selectively control and regulate the flow of information but how does this actually help us to train the network an important property of STM's is that all of these different gating and update mechanisms actually work to create this internal cell state C which allows for the uninterrupted flow of gradients through time and you can think of this as sort of a highway of cell states where gradients can flow uninterrupted shown here in red and this enables us to alleviate and mitigate that vanishing gradient problem that's seen with vanilla or standard RN ends yeah question yeah so forgetting irrelevant information goes back to the question that was asked a little bit earlier and if you think back to the example of the the language model where you're trying to predict the now next word there may be some words very early on in the sequence that carry a lot of content and semantic meaning for example France but there could be other words that are superfluous or don't carry that much semantic meaning so over the course of training you want your LS TM to be able to specifically learn what are those bits of prior history that carry more meaning that are important in in trying to actually learn the problem of predicting the next word and you want to discard what is not relevant to to really enable more robust training okay so to review the key concepts behind LS EMS and I know this was you know this is when Alexander said this is a bootcamp course he he was not joking around right things go quickly and things yeah and so I hope you know by sort of providing these periodic summaries that we can really distill down all this material into the key concepts and takeaways that we want you to have at the end of each lecture and ultimately at the end of the course and so for LST ends right let's break it down Ellis hams are able to maintain this separate cell state independent of what is outputted and they they use gates to control the flow of information by forgetting irrelevant history storing relevant new information selectively updating their cell state and then outputting a filtered version as the output and the key point in terms of training and Alice TM is that maintaining this separate independent cell state allows for the efficient training of an Ellis TM to back propagation through time alright so now that we've gone through the fundamental workings of RN ends the back propagation through time algorithm and a bit about the LS TM architecture in particular I like to consider a few concrete examples of how RNs can be used and some of the amazing applications they enable imagine that we're trying to learn an RNN that can predict the next musical note in a sequence of music and to actually use this to generate brand new musical sequences and the way you could think about this potentially working is by inputting a sequence of notes and the output of the RNN at each time step in that sequence is a prediction of what it thinks the next note in the sequence will be and then we can actually train this network and after we've trained it we can sample from the train network to predict to generate a brand-new musical sequence that has never been heard before and so for example here's a case where an RNN was trained on the music of my favorite composer Chopin and the sample I'll play for you is his music that was completely generated by this ai [Music] so it gives you a little sample that it sounds pretty realistic right and you'll actually get some practice doing this in today's lab where you'll be training an RNN to generate brand new Irish folk songs that have never been heard before and there's a competition for you know who has the sweetest tunes at the end of the at the end of the lab so we encourage you to to you know try your best okay as another cool example we can go from an input sequence to just a single output and an example of this is in training and RNN that takes as input the words in the sentence outputs the sentiment or the feeling or the emotion of that particular sentence which can be you know either positive or negative and so for example if we train a model like this with a bunch of tweets source from Twitter we can train our RNN to predict that this first tweet about our class six s-191 has a positive sentiment but that this other tweet wishing for cold weather and snow maybe has a negative sentiment the another example that I'll mention quickly is one of the most powerful and widely used applications of our Nance in industry and it's the backbone of Google Translate and that's machine translation where you input a sequence in one language and the task is to train the RNN to output that sequence in a new language and this is done by having sort of this dual structure with an encoder which encodes their original sentence in its original language into a state vector and a decoder which then takes that encoded representation as input and decodes it into a new language there's a key problem though in this approach and that's the fact that the entire content that's fed into the encoder structure has to be encoded into a single vector and this can be actually a huge information bottleneck in practice because you may be you may imagine having a large body of text that you may want to translate but to get around this problem the researchers at Google were very clever and developed a extension of rnns called attention and the idea here is that instead of the decoder only having access to the final encoded state it can now actually access the states of all the time steps in the original sentence and the weighting of these vectors that connect the encoder States to the decoder are actually learned by the network during training and it's called attention because when the network learns this waiting its placing its attention on different parts of the input sentence and in this sense it's actually very effectively capturing a sort of memory access to the important information in that original sentence and so with these building blocks like attention and gated cells like LS TMS RN ins have really taken off in recent years and are being used in the real world world for many complex and impactful applications and so for example let's consider autonomous vehicles and at any moment in time an autonomous vehicle like a self-driving car needs to have an understanding of not only where every object in its environment is but also have a sense of where those objects may move in the future and so this is an example of a self-driving car from the company way mo from Google that encounters a cyclist on its right side which is denoted in red and the cyclist is approaching a stopped vehicle and the way Moe car can actually recognize that it's very likely that the cyclist is going to cut into its lane and so before the cyclist begins to cut the car off right as you can see right there the way Moe vehicle actually pulls back and slows down allowing for the cyclists to enter another example of how we can use a deep sequence modelling is in environmental modeling and climate pattern analysis and prediction and so this is a visualization of the predicted patterns for different environmental markers ranging from wind speeds to humidity to levels of particulate matter in the air and effectively predicting the future behavior of these markers could really be key in projecting and planning for the into the climate impact of particular human interventions or actions and so in this lecture hopefully you've gotten a sense of how our needs work and why they are so powerful for processing sequential data we've discussed why and how we can use rnns to perform sequence modeling tasks by defining this recurrence relation how we can train our intends and we also looked at how gated cells like lc-ms can help us model long-term dependencies and finally we discussed some applications of rnns including music generation and so that leads in very nicely to the next portion of today which is our software lab and today's lab is going to be broken down into two parts we're first going to have a crash course in tensor flow that covers all the fundamentals and then you'll move into actually training RN ends to perform music generation and so for those of you who who will stay for the labs the instructions are up here and alexander myself and all the TAS will be available to assist and field questions as you work through them so thank you very much [Applause] 

hi everyone and welcome back to MIT
6.S191 today we're going to be talking about one of my favorite topics
in this course and that's how we can give machines a sense of vision
now vision I think is one of the most important senses that humans possess
sighted people rely on vision every single day from things like navigation
manipulation how you can pick up objects how you can recognize objects recognize
complex human emotion and behaviors and I think it's very safe to say that
vision is a huge part of human life today we're going to be learning about
how deep learning can build powerful computer vision systems capable of
solving extraordinary complex tasks that may be just 15 years ago would have not
even been possible to be solved now one example of how vision is transforming
computer or how deep learning is transforming computer vision is is
facial recognition so on the top left you can see an icon of the human eye
which visually represents vision coming into a deep neural network in the form
of images or pixels or video and on the output on the bottom you can see a
depiction of a human face or detecting human face but this could also be
recognizing different human faces or even emotions on the face recognizing
key facial features etc now deep learning has transformed this field
specifically because it means that the creator of this AI does not need to
tailor that algorithm specifically for towards facial detection but instead
they can provide lots and lots of data to this algorithm and swap out this end
this end piece instead of facial detection they can swap it out for many
other detection types or recognition types and the neural network can try and
learn to solve that task so for example we can replace that facial detection
task with the detection of disease region in the retina of the eye and
similar techniques could also be applied throughout healthcare matters
and towards the detection and classification of many different types
of diseases in biology and so on now another common example is in the context
of self-driving cars where we take an image as input and try to learn an
autonomous control system for that car this is all entirely end-to-end so we
have vision and pixels coming in as input and the actuation of the car
coming in as output now this is radically different than the vast
majority of autonomous car companies and how they operate so if you look at
companies like way Moe and Tesla this end-to-end approach is radically
different we'll talk more about this later on but this is actually just one
of the autonomous vehicles that we build here as part of my lab at csail so
that's why I'm bringing it up so now that we've gotten a sense of at a very
high level some of the computer vision tasks that we as humans solve every
single day and that we can also train machines to solve the next natural
question I think to ask is how can computers see and specifically how does
a computer process an image or a video how do they process pixels coming from
those images well to a computer images are just numbers and suppose we have
this picture here of Abraham Lincoln it's made up of pixels now each of these
pixels since it's a grayscale image can be represented by a single number and
now we can represent our image as a two dimensional matrix of numbers one for
each pixel in that image and that's how a computer sees this image it sees that
that's just a matrix of two-dimensional numbers or two-dimensional matrix of
numbers rather now if we have an RGB image a color image instead of a
grayscale image we can simply represent that as three of these two-dimensional
images concatenated or stacked on top of each other one for the red channel one
for the green channel one for the blue channel and that's RGB now we have a way
to represent images to computers and we can think about what types of computer
vision tasks this will allow us to solve and what we can perform given this this
foundation well two common types of machine learning that we actually saw in
lecture 1 and 2 yesterday are those of classification and those of progression
in regression we take we have our output take a continuous
value in classification we have our output take a continuous label so let's
first start with classification and specifically the the problem of image
classification we want to predict a single label for each image for example
we have a bunch of US presidents here and we want to build the classification
pipeline to determine which President is in this image that we're looking at
outputting the probability that this image is each of those US presidents in
order to collect correctly classify this image our pipeline needs to be able to
tell what is unique about a picture of Lincoln versus what is unique about a
picture of Washington versus a picture of Obama it needs to understand those
unique differences in each of those images or each of those classifications
each of those features now another way to think about this and this image
classification pipeline at a high level is in terms of features that are
characteristics of a particular class classification is done done by detecting
these types of features in that class if you detect enough of these features
specific to that class then you can probably say with pretty high confidence
that you're looking at that class now one way to solve this problem is to
leverage knowledge about your field your domain knowledge and say let's suppose
we're dealing with human faces we can use our knowledge about human faces to
say that if we want to detect human faces we can first detect noses eyes
ears mouths and then once we have a detection pipeline for those we can
start to detect those features and then determine if we're looking at a human
face or not now there's a big problem with that approach and that's that
preliminary detection pipeline how do we detect those noses ears mouths and like
this hierarchy is kind of our bottleneck in that sense and remember that these
images are just three dimensional arrays of numbers well actually they're just
three dimensional arrays of brightness values and that images can hold tons of
variation so there's variations such as occlusions that we have to deal with
variations in illumination and even intro class very
and when we're building our classification pipeline we need to be
invariant to all of these variations it'll and be sensitive to inter class
variation so sensitive to the variations between classes but invariant to the
variations within a single class now even though our pipeline could use the
features that we as humans defined the manual extraction of those features is
where this really breaks down now due to the incredible variability in image data
specifically the detection of these features is super difficult in practice
and defining the manually extracting these features can be extremely brittle
so how can we do better than this that's really the question that we want
to tackle today one way is that we want to extract both these visual features
and detect their presence in the image simultaneously and in a hierarchical
fashion and for that we can use neural networks like we saw in lab in class
number one and two and our approach here is going to be to learn the visual
features directly from data and to learn a hierarchy of these features so that we
can reconstruct a representation of what makes up our final class label so I
think now that we have that foundation of how images work we can actually move
on to asking ourselves how we can learn those visual features specifically with
a certain type of operation in neural networks and neural networks will allow
us to directly learn those features from visual data if we construct them
cleverly and correctly in lecture one we learned about fully connected or dense
neural networks where you can have multiple hidden layers and each hidden
layer is densely connected to its previous layer and densely connected
here let me just remind you is that every input is connected to every output
in that layer so let's say that we want to use these densely connected networks
in image classification what that would mean is that we're going to take our
two-dimensional image right it's a two-dimensional spatial structure we're
going to collapse it down into a one dimensional vector and then we can feed
that through our dense Network so every pixel in that that one dimensional
vector will feed into the next layer and you cannot already imagine that or you
can you should already appreciate that all of our two-dimensional structure in
that image is completely gone already because we've collapsed a
two-dimensional image into one dimension we've lost all of that very useful
spatial structure in our image and all of that domain knowledge that we could
have used a priori and additionally we're going to have a ton of parameters
in this network because it's densely connected we're connecting every single
pixel in our input to every single neuron in our hidden layer so this is
not really feasible in practice and instead we need to ask how we can build
some spatial structure into neural networks so we can be a little more
clever in our learning process and allow us to tackle this specific type of
inputs in a more reasonable and and well-behaved way also we're dealing with
some prior knowledge that we have specifically that spatial structure is
super important in image data and to do this let's first represent our
two-dimensional image as a array of pixel values just like they normally
were to start with one way that we can keep and maintain that spatial structure
is by connecting patches of the input to a single neuron in the hidden layer so
instead of connecting every input pixel from our input layer and our input image
to a single neuron in the hidden layer like with dense neural networks we're
going to connect just a single patch a very small patch and notice here that
only a region of that input layer or that input image is influencing this
single neuron at the hidden layer to define connections across the entire
input we can apply the same principle of connecting patches in the input layer in
single neurons in the subsequent layer we do this by simply sliding that patch
window across the input image and in this case we're sliding it by two units
each time in this way we take into account we maintain all of that spatial
structure that spatial information inherent to our image input but we also
remember that the final task that we really want to do here that I told you
we want to do was to learn visual features and we can
do this very simply by waving those connections in the patches so each of
the patches instead of just connecting them uniformly to our hidden layer we're
going to weight each of those pixels and apply a similar technique like we saw in
lab 1 instead of we can basically just have a weighted summation of all of
those pixels in that patch and that feeds into the next hidden unit in our
hidden layer to detect a particular feature now in practice this operation
is simply called convolution which gives way to the name convolutional neural
network which we'll get to later on we'll think about this at a high level
first and suppose that we have a four by four filter which means that we have 16
different weights 4 by 4 we are going to apply the same filter of four by four
patches across the entire input image and we'll use the result of that
operation to define the state of the neurons in the next hidden layer we basically shift this patch across the
image we shifted for example in units of two pixels each time to grab the next
patch we repeat the convolution operation and that's how we can start to
think about extracting features in our input but you're probably wondering how
does this convolution operation actually relate to feature extraction so so far
we've just defined the sliding operation where we can slide a patch over the
input but we haven't really talked about how that allows us to extract features
from that image itself so let's make this concrete by walking through an
example first suppose we want to classify X's from a set of black and
white images so here black is represented by -1 white is represented
by the pixel 1 now to classify X's clearly we're not going to be able to
just compare these two matrices because there's too much variation between these
classes we want to be able to get invariant to certain types of
deformation to the images scale shift rotation we want to be able to handle
all of that so we can't just compare these two like as they are right now so
instead what we're gonna do is we want to model our model to compare
these images of exes piece by piece or patch by patch and the important patches
are the important pieces that it's looking for are the features now if our
model can find rough feature matches across these two images then we can say
with pretty high confidence that they're probably coming from the same image if
they share a lot of the same visual features then they're probably
representing the same object now each feature is like a mini image
each of these patches is like a mini image it's also a two-dimensional array
of numbers and we'll use these filters let me call them now to pick up on the
features comment 2x in the case of X's filter is representing diagonal lines
and crosses are probably the most important things to look for and you can
see those on the top the top row here so we can probably capture these features
in terms of the arms and the main body of that X so the arms the legs and the
body will capture all of those features that we show here and note that the
smaller matrices are the filters of weights so these are the actual values
of the weights that correspond to that patch as we slide it across the image
now all that's left to do here is really just define that convolution operation
and tell you when you slide that patch over the image what is the actual
operation that takes that patch on top of that image and then produces that
next output at the hidden neuron layer so convolution preserve is that spatial
structure between pixels by learning the image features in these small squares or
the small patches of the input data to do this to cut the entire equation or
the entire computation is as follows we first place that patch on top of our
input image of the same size so here we're placing this patch on the top left
on this part of the image in green on the X there and we perform an
element-wise multiplication so for every pixel on our image where the patch
overlaps with we element-wise multiply every pixel in the filter the result you
can see on the right is just a matrix of all ones because there's perfect overlap
between our filter in this case and our image at the patch location the only
thing left to do here is sum up all of those numbers and when you sum them up
you get nine and that's the output at the next layer now let's go through one
more example a little bit more slowly of how we did this and you might be able to
appreciate what this convolution operation is intuitively telling us now
that's mathematically how it's done but now let's see intuitively what this is
showing us suppose we want to compute the convolution now of this 5x5 image in
green with this 3x3 filter or this 3x3 patch to do this we need to cover that
entire image by sliding the filter over that image and performing that
element-wise multiplication and adding the output for each patch and this is
what that looks like so first we'll start off by placing that yellow filter
on the top left corner we're going to element-wise multiply and add all of the
outputs and we're gonna do it four and we're gonna place that four in our first
entry of our output matrix this is called the feature map now we can
continue this and slide that 3x3 filter over the image element wise multiply add
up all the numbers and place the next result in the next row in the next
column which is three and we can just keep repeating this operation over and
over and that's it the feature map on the right reflects where in the image
there is activation by this particular filter so let's take a look at this
filter really quickly you can see in this filter this filter is an X or a
cross it has ones on both diagonals and then the image you can see that it's
being activated also along this main diagonal on the four where the four is
being maximally activated so this is showing that there is maximum overlap
with this filter on this image along this central diagonal now let's take a
quick example of how different types of filters are changing the weights in your
filter can impact different feature Maps or different outputs so simply by
changing the weights in your filter you can change what your filter is
looking for what it's going to be activating so take for example this
image of this woman Lenna on the left that's the original image on
the left if you slide different filters over this image you can get different
output feature Maps so for example you can sharpen this image by having a
filter shown on the second column you can detect edges in this image by having
the third column by using the third columns features filter sorry and you
can even detect stronger edges by having the fourth column and these are ways
that changing those weights in your filter can really impact the features
that you detect so now I hope you can appreciate how convolution allows us to
capitalize on spatial structure and use sets of weights to extract these local
features within images and very easily we can detect different features by
simply changing our weights and using different filters okay now these
concepts of preserving spatial information and spatial structure while
local feature extraction while also doing local feature extraction using the
convolution operation are at the core of neural networks and we use those for
computer vision tasks so now that we've gotten convolutions under our belt we
can start to think about how we can utilize this to build full convolutional
neural networks for solving computer vision tasks and these networks are very
appropriately named convolutional neural networks because the backbone of them is
the convolution operation and we'll take a look first at a CNN or convolutional
neural network architecture define designed for image classification tasks
and we'll see how the convolution operation can actually feed into those
spatial sampling operations so that we can build this full thing end to end so first let's consider the simple very
simple CN n for image classification now here the goal is to learn features
directly from data and to use these learn feature Maps for classification of
these images there are three main parts to a CNN that I want to talk about now
first part is the convolutions which we about before these are for extracting
the features in your image or in your previous layer in a more generic saying
the second step is applying your non-linearity and again like we saw in
lecture 1 and 2 nonlinearities allow us to deal with nonlinear data and
introduce complexity into our learning pipeline so that we can solve these more
complex tasks and finally the third step which is what I was talking about before
is this pooling operation which allows you to down sample your spatial
resolution of your image and deal with multiple scales of that image or
multiple scales of your features within that image and finally the last point I
want to make here is that the computation of class scores let's
suppose if we're dealing with image classification can be outputted using
maybe a dense layer at the end after your convolutional layers so you can
output a dense layer which which represents those probabilities of
representing each class and that can be your final output in this case and now
we'll go through each of these operations and break these ideas down a
little bit further just so we can see the basic architecture of a CNN and how
you can implement one as well okay so going through this step by step those
three steps the first step is that convolution operation and as before this
is the same story that we've going we've been going through each neuron here in
the hidden layer we'll compute a weighted sum of its inputs from that
patch and we'll apply a bias like in lecture one and two and activate with a
local non-linearity know what's special here is that local connectivity that I
just want to keep stressing again each neuron in that hidden layer is only
seeing a patch from that original input image and that's what's really important
here we can define the actual computation for a neuron in that hidden
layer its inputs are those neurons in the patch and the previous layer we
apply a matrix of weights again that's that filter a 4x4 filter in this case we
do an element-wise multiplication add the result apply a bias activate with
that non-linearity and that's it that's our single neuron at the hidden layer
and we just keep repeating this by sliding that patch
over the input remember that our element-wise multiplication and addition
here is simply that convolution operation that we talked about earlier
I'm not saying anything new except the addition of that bias term before our
non-linearity so this defines how neurons and convolutional layers are
connected but with a single convolutional layer we can have multiple
different filters or multiple different features that we might want to extract
or detect the output layer of a convolution therefore is not a single
image as well but rather a volume of images representing all of the different
filters that it detects so here at D the depth is the number of filters or the
number of features that you want to detect in that image and that's set by
the human so when you define your network you define at every layer how
many features do you want to detect at that layer now we can also think about
the connections in a neuron in a convolutional neural network in terms of
their receptive field and the locations of their input of that specific node
that they're connected to right so these parameters define the spatial
arrangement of that output of the convolutional layer and to summarize we
can see basically how the connections of let's see so how the connections of
these convolutional layers are defined first of all and also how the output of
a convolutional layer is a volume defined by that depth or the number of
filters that we want to learn and with this information this defines our single
convolutional layer and now we're well on our way to defining the full
convolutional neural network the remaining steps are are kind of just
icing on the cake at this point and it starts with applying that non-linearity
so on that volume we apply an element-wise non-linearity in this case
I'm showing a rectified linear unit activation function this is very similar
in idea to lectures 1 & 2 where we also applied nonlinearities to deal with
highly nonlinear data now here the relative activation function rectified
linear unit we haven't talked about it yet but this is just an activation
function that takes as input any real number and essentially
ships everything less than zero to zero and anything greater than zero it keeps
the same another way you can think about this is it make sure that the minimum of
whatever you feed in is zero so if it's greater than zero it doesn't touch it if
it's less than zero to make sure that it caps it at zero now the key idea
the next key idea let's say of convolutional neural networks is pulling
and that's how we can deal with different spatial resolutions and become
spatially or like invariant to spatial size in our image now the pooling
operation is used to reduce the dimensionality of our input layers and
this can be done on any layer after the convolutional layer so you can apply on
your input image a convolutional layer apply a comp a non-linearity and then
down sample using a pooling layer to get a different spatial resolution before
applying your next convolutional layer and repeat this process for many layers
and a deep neural network now a common technique here for pooling is called max
pooling and when and the idea is as follows so you can slide now another
window or another patch over your network and for each of the patches you
simply take the maximum value in that patch so let's say we're dealing with
two by two patches in this case the red patch you can see on the top right we
just simply take the maximum value in that red patch which is six and the
output is on the right-hand side here so that six is the maximum from this patch
this 2x2 patch and we repeat this over the entire image this makes us or this
allows us to shrink the spatial dimensions of our image while still
maintaining all of that spatial structure so actually this is a great
point because I encourage all of you to think about what are some other ways
that you could perform a pooling operation how else could you down sample
these images max pooling is one way so you could always take the maximum of
these these 2x2 patches but there are a lot of other really clever ways as well
so it's interesting to think about some ways that we can also in another ways
potentially perform this down sampling operation now the key idea here of these
convolutional neural networks and how we're now with all of this
knowledge we're kind of ready to put this together and perform these
end-to-end networks so we have the three main steps that I talked to you about
before convolution local nonlinearities and pooling operations and with CNN's we
can layer these operations to learn a hierarchy of features and a hierarchy of
features that we want to detect if they're present in the image data or not
so a CNN built for image classification I'm showing the first part of that CNN
here on the Left we can break it down roughly into two parts so the first part
I'm showing here is the part of feature learning so that's where we want to
extract those features and learn the features from our image data this is
simply applying that same idea that I showed you before we're gonna stack
convolution and nonlinearities with pooling operations and repeat this
throughout the depth of our network the next step for our convolutional neural
network is to take those extracted or learned features and to classify our
image right so the ultimate goal here is not to extract features we want to
extract features but then use them to make some classification or make some
decision based on our image so we can feed these outputted features into a
fully connected or dense layer and that dense layer can output a probability
distribution over the image membership in different categories or classes and
we do this using a function called softmax which you actually already got
some experience with in lab 1 whose output represents this categorical
distribution so now let's put this all together into coding our first
end-to-end convolutional neural network from scratch we'll start by defining our
feature extraction head which starts with a convolutional layer here I'm
showing with 32 filters so 32 is coming from this number right here that's the
number of filters that we want to extract inside of this first
convolutional layer we down sample the spatial information using a max pooling
operation like I discussed earlier and next we feed this into the next set
of convolutional layers in our network so now instead of 32 features we're
gonna be extracting even more features so now we're extracting 64 features then
finally we can flatten this all of the spatial information and the spatial
features that we've learned into a vector and learn our final probability
distribution over class membership and that allows us to actually classify this
image into one of these different classes that we've defined so far we've
talked only about using CN NS for image classification tasks in reality this
architecture extends to many many different types of tasks and many many
different types of applications as well when we're considering CN NS for
classification we saw that it has two main parts first being the feature
learning part shown here and then a classification part and the second part
of the pipeline what makes a convolutional neural network so powerful
is that you can take this feature extraction part of the pipeline and at
the output you can attach whatever kind of output that you want to it so you can
just treat this convolutional feature extractor simply as that a feature
extractor and then plug in whatever other type of neural network you want at
its output so you can do detection by changing the output head you can do
semantic segmentation which is where you want to detect semantic classes for
every pixel in your image you can also do ant and robotic control like we saw
with the tongue that's driving before so what's an example of this we've seen a
significant impact in computer vision in medicine and healthcare over the last
couple of years just a couple weeks ago actually there was this paper that came
out where deep learning models have been applied to the analysis of a whole host
of breast and the sry mammogram cancer detection or yeah breast cancer
detection in mammogram images so what we showed what was showed here was that CNN
were able to significantly outperform expert radiologists and detecting breast
cancer directly from these mammogram images that's done by feeding these
images through a convolutional feature extract
they're out putting that those features those learn features to dense layers and
then performing classification based on those dense layers instead of predicting
a single number breast cancer or no breast cancer you could also imagine for
every pixel in that image you want to predict what is the class of that pixel
so here we're showing a picture of two cows on the left
those are fed into a convolutional feature extractor and then they're up
scaled through the inverse convolutional decoder to predict for every pixel in
that image what is the class of that pixel so you can see that the network is
able to correctly classify that it sees two cows and brown whereas the grass is
in green and the sky is in blue and this is basically detection but not for a
single number over the image yes or no there's a cow or in this image but for
every pixel what is the class of this pixel this is a much harder problem and
this output is actually created using these up sampling operations so this is
no longer a dense neural network here but we have kind of inverse or what are
called transpose convolutions which scale back up our image data and allow
us to predict these images as outputs and not just single numbers or single
probability distributions and of course this idea can be you can imagine very
easily applied to many other applications in healthcare as well
especially for segmenting various types of cancers such as here we're showing
brain tumors on the top as well as parts of the blood that are infected with
malaria on the bottom so let's see one final example before before ending this
lecture and here we're showing again and going back to the example of
self-driving cars and the idea again is pretty similar so let's say we want to
learn a neural network to control a self-driving car and learn autonomous
navigation specifically we want to go from a model we're using our model to go
from images of the road maybe from a camera attached to that car on top of
the car you can think of the actual pixels coming from this camera that are
fed to the neural network and in addition to the pixels coming
from the camera we also have these images from a bird's-eye Street view of
where the car roughly is in the world and we can feed both of those images
these are just two two-dimensional arrays so this is one two-dimensional
array of images or pixels excuse me and this is another two-dimensional array of
pixels both represent different things so this represents your perception of
the world around you and this represents roughly where you are in the world
globally and what we want to do with this is then to directly predict or
infer a full distribution of possible control actions that the car could take
at that's instant so if it doesn't have any goal destination in mind they could
say that I could take any of these three directions and steer in those directions
and that's what we want to predict with this Network one way I do this is that
you can actually train your neural network to take as input these camera
images coming from the car pass them each through these convolutional
encoders or feature extractors and then now that you've learned features for
each of those images you can concatenate all of them together so now you have a
global set of features across all of your sensor data and then learn your
control outputs from those on the right hand side now again this is done
entirely end to end right so we never told the car what a lane marker was what
a road was or how to even turn right or left or what's an intersection so we
never told any of that information but it's able to learn all of this and
extract those features from scratch just by watching a lot of human driving data
and learn how to drive on its own so here's an example of how a human can
actually enter the car input a desired destination which you can see on the top
right the red line indicates where we want the car to go in the map so think
of this as like a Google map so you plug into Google Maps where you want to go
and the antenna see and then the convolutional neural network will output
the control commands given what it sees on the road to actually activate that
vehicle towards that destination note here that the vehicle is able to
like sickness successfully navigate through those intersections even though
it's never been driving in this area before it's never seen these roads
before and we never even told it what in an intersection was it learned all of
this from data using convolutional neural networks now the impact of cnn's
has been very very wide reaching beyond these examples that I've given to today
and it has touched so many different fields of computer vision ranging across
robotics and as medicine and many many other fields I'd like to conclude by
taking a look at what we've covered in today's lecture we first considered the
origins of computer vision and how images are represented as brightness
values to a computer and how these convolution operations work in practice
right so then we discussed the basic architecture and how we get build-up
from convolution operations to build convolutional layers and then pass that
to convolutional neural networks and finally we talked about the extensions
and applications of convolutional neural networks and how we can visualize a
little bit of the behavior and actually actuate some of the real world with
convolutional neural networks either by predicting some parts of medicine or
some parts of medical scans or even activating robots to interact with
humans in the real world and that's it for the CNN lecture on computer vision
next up we'll hear from alpha a deep generate generative modeling and thank
you you 

you so so far in this class we've talked about how we can use neural networks to learn patterns within data and so in this next lecture we're going to take this a step further and talk about how we can build systems that not only look for patterns and data but actually can learn to generate brand-new data based on this learned information and this is a really new and emerging field within deep learning and it's enjoying a lot of success and attention right now and in the past couple years especially and so this broadly can be considered at this field of deep generative modeling so I'd like to start by taking a quick poll so study these three phases for a moment these are three phases raise your hand if you think phase a is real-- is real-- is real-- okay so that roughly it follows with with the first vote face B is real raise your hand if you think face B is real okay and finally face C it doesn't really matter because all these faces are fake and so hopefully you know this is this is really recent work this was just posted on archive in December of 2019 the results from this latest model and today by the end of this lecture you're going to have a sense of how a deep neural network can be used to generate data that is so realistic that it fooled many of you or rather all of you okay so so far in this course we've been considering this this problem of supervised learning which means we're given a set of data and a set of labels that go along with that data and our goal is to learn a functional mapping that goes from data to labels and in this course right this is a course on deep learning and we've been largely talking about functional mappings that are described by deep neural networks but the core these mappings could be anything now we're going to turn our attention to a new class of problems and that's what is called unsupervised learning and is the topic of today's lecture and in unsupervised learning we're given only data and no labels and our goal is to try to understand the hidden or underlying structure that exists in this data and this can help us get insights into sort of what is the the foundational level of explanatory factors behind this data and as we'll see later to even generate brand-new examples that resemble the true data distribution and so this is this topic of generative modeling which is an example of unsupervised learning and as I kind of alluded to our goal here is to take input examples from some training distribution and to learn and infer a model that represents that distribution and we can do this for two main goals the first being this concept of density estimation where we we see a bunch of samples right and they lie along some probability distribution and we want to learn model that approximates the underlying distribution that's describing or generating where this data was drawn from the other task is this idea of sample generation and so in this in this context we're given input samples and we want our model to be able to generate brand-new samples that represent those inputs and so that's the idea with the fake faces that we we showed in the first slide and really the core question behind generative modeling is how can we learn some probability distribution how can we model some probability distribution that's similar to the true distribution that describes how the data was generated so why care about generative modeling besides the fact that could it can you know be used to generate these realistic human-like faces right well first of all generative approaches can under uncover the underlying factors and features present in a data set so for example if we consider the problem of facial detection we may be given a data set with many many different faces and we may not know the exact distribution of faces in terms of different features like skin color or gender or clothing or occlusion or the orientation of the face and so our training data may actually end up being very biased to particular instances of those features that are over-represented in our data set without us even knowing it and today and in the in the lab as well we'll see how we can use generative models to actually learn these underlying features and uncover the over and underrepresented parts of the data and use this information to actually create fair more representative datasets to train an unbiased classification model another great example is this question of outlier detection so if we go back to the example problem of self-driving cars most of the day that we may want to train a control network that Alexander was describing may be very common driving scene so on a freeway or on a straight road where you're driving but it's really critical that our autonomous vehicle would be able to handle all cases that it could potentially encounter in the environment including edge cases and rare events like crashes or pedestrians not just you know the straight freeway driving that is the majority of the time on the road and so generative models can be used to actually detect the outliers that exist within training distributions and use this to train a more robust model and so we'll really dive deeply into two classes of models that we call latent variable models but first before we get into those data details we have a big question what is a latent variable and I think a great example to describe the difference between latent and observed variables is this little parable and story from Plato's Republic from thousands of years ago which is known as the myth of the cave and in this myth a group of prisoners are constrained to face a wall as punishment and the only things that they can see and observe are the shadows of objects that pass in front of a fire that's actually behind them so they're just observing the shadows in front in front of their faces and so from the prisoners perspective these shadows are the observed variables they can measure them they can give them names because to them you know that's their reality they don't know that behind them they're these true objects that are actually casting the shadows because of this fire and so those objects that are behind the prisoners are like the latent variables they're not directly observable by the prisoners but they're the true explanatory factors that are casting the shadows that the prisoners see and so our goal in generative modeling is to find ways of actually learning these hidden and underlying latent variables even when we are only given the observed data okay so let's start by discussing a very simple generative model which tries to do this by encoding its input and these models are known as auto-encoders so to take a look at how an auto encoder works what is done is we first begin by feeding raw input data into the model it's passed through a series of neural network layers and what is outputted at the end of that encoding is what we call a low dimensional latent space which is a feature representation that we're trying to predict and we call this network an encoder because it's mapping this data X into a vector of latent variables Z now let's ask ourselves a question right why do we care about having this low dimensional latent space see any ideas yeah yeah it's easier to process and the I think the key that you're getting at is that it's a compressed representation of the data and in the case of a pair of images right these are pixel based data they can be very very very highly dimensional and what we want to do is to take that high dimensional information and encode it into a compressed smaller latent vector so how do we actually train a network to learn this latent variable vector Z do we have training data Frizzi do we observe these the true values of Z and can we do supervised learning the answer is no right we don't have training data for those latent variables but we can get around this by building a decoder structure that is used to reconstruct a resemblance of the original image from this learned latent space and again this decoder is a series of neural network layers which can be convolutional layers or fully connected layers that are now taking the hidden latent vector and mapping it back to the dimensionality of the input space and we called this reconstructed output X hat right since it's going to be some imperfect reconstruction or estimation of what the input X looks like and the way we can train a network like this is by simply considering the input X and our reconstructed output X hat and looking at how they are different right and we want to try to minimize the distance between the reconstruction and the input to try to get as realistic of a reconstruction as possible and so in the case of this this image example we can simply take the mean squared error right X minus X hat and square it from the input to the the reek outputted reconstructions and so the really important thing here is that this loss function doesn't have any labels right the only components of the loss are input X and the reconstructions X hat it's not supervised in any sense and so we can simplify this this diagram a little bit further by take abstracting away those individual layers in the encoder and the decoder and this is this idea of unsupervised learning is really a powerful idea because it allows us to in a way quantify these latent variables that we're interested in but we can't directly observe and so a key consideration when building a model like an auto encoder is how we select the dimensionality of our latent space and the the latent the latent space that we're trying to learn it presents this sort of bottleneck layer because it's a form of compression and so the lower the dimensionality of the latent space that we choose the poorer the quality of the reconstruction that's generated in the end and so in this example this is the data set of very famous data set of written digits called em missed and on the right you can see the ground truth for example digits from this data set and as you can hopefully appreciate in these images by going just from A to D late in space to a 5d late in space we can drastically improve the quality of the reconstructions that are produced as output by an auto encoder structure okay so to summarize autoencoders are using this bottlenecking hidden layer that forces the network to learn a compressed latent representation of the data and by using this reconstruction loss we can train the network in a completely unsupervised manner and the name auto encoder is comes from you know comes from the fact that we're automatically encoding information within the data into this smaller latent space so we can now we will now see how we can build on this foundational idea a bit more with this concept of variational autoencoders or V AES and so well auto-encoders with with a traditional auto encoder what is done as you can see here is going from input to a reconstructed output and so if I feed in an input to the network I will get the same output so long as the weights are the same this is a deterministic encoding that allows us to reproduce the input as best as we can but if we want to learn a more smooth representation of the latent space and use this to actually generate new images and sample new images that are similar to the input data set we can use a structure called a variational auto encoder to more robustly do this and this is a slight twist on the traditional auto encoder and what it's done is that instead of a deterministic bottleneck layer z we've replaced this deterministic layer with a stochastic sampling operation and that means that instead of learning the latent variables directly for each variable we learn a mean and a standard deviation Sigma that actually parameterize a probability distribution for each of those latent variables so now we've gone from a vector of latent variable Z to learning a vector of means mu and a vector of standard deviations Sigma which describe the probability distributions associated with each of these latent variables and what we can do is sample from from these described distributions to obtain a probabilistic representation of our latent space and so as you can tell and as I've emphasized the VA II structure is just an autoencoder with this probabilistic twist so now what this means is instead of deterministic functions that describe the encoder and decoder both of these components of the network are probabilistic in nature and what the encoder actually does is it computes this probability distribution P of Phi of the latent space Z given an input X while the decoder is doing sort of the reverse inference it's computing a new probability distribution Q of theta of X given the latent variables Z and because we've introduced this this probabilistic probabilistic aspects to this network our loss function has also slightly changed the reconstruction loss in this case is very is basically exactly like what we saw with the traditional auto encoder the reconstruction loss is capturing the pixel wise difference between our input and the reconstructed output and so this is a metrics of how well our network is doing at generating outputs that are similar to the input then we have this other term which is the regulars regularization term which gets gets back to that earlier question and so because the VA II is learning these probability distributions we want to place some constraints on on how this probability distribution is computed and what that probability distribution resembles as a part of regularizing and training our network and so the way that's done is by placing a prior on the latent distribution and that's P of Z and so that's some initial hypothesis or guess about what the distribution of Z looks like and so this essentially helps enforce the learn Z's to follow the shape of that of that prior distribution and the reason that we do this is to help the network not over fit right because without this regularization it may it may over fit on certain parts of the latent space but if we enforce that each latent variable adopts something similar to this prior it helps smooth out the landscape of the lane space and the learned distributions and so this regularization term is a function that captures the divergence between the inferred latent distribution and this fixed prior that we've placed so as I mentioned a common choice for this prior distribution is a normal Gaussian which means that we center it around with a mean of 0 and a standard deviation 1 and as the great question pointed out what what this enables us to do is derive some very nice properties about the optimal bounds of how well our network can do and by choosing this normal Gaussian prior what is done is the encoder is encouraged to sort of put the distribute the latent variables evenly around the center of this latent space distributing the encoding smoothly and actually the network will learn to penalise itself when it tries to cheat and cluster points outside sort of this smooth Gaussian distribution as it would be the case if it was overfitting or trying to memorize particular instances of the data and what also can be derived in the instance of when we choose a normal Gaussian as our prior is this specific distance function which is formulated here and this is called the KL divergence the KU black lie blur divergence and this is specifically in the case when the prior distribution is a zero one Gaussian the divergence that measures the the separation between our inferred latent distribution and this prior takes this particular form okay yeah so to re-emphasize this term is the regularization term that's used in the formulation of the total loss okay so now that we've seen a bit about the reconstruction loss and a little more detail into how the regularization term works we can discuss how we can actually the train the network and to end using back propagation and what is what immediately jumps out as an issue is that Z here is the prod is the result of a stochastic sampling operation and we cannot back propagate gradients through a sampling layer because of their stochastic nature because back propagation requires deterministic nodes to be able to iteratively iteratively pass gradients through and apply the chain rule through but what was a really breakthrough idea that enabled the AES to really take off was to use this little trick called a reap reap parameterization trick to repair amateur eyes the sampling such that the network can now be trained end to end and I'll give you the key idea about how this operation works and so instead of drawing Z directly from a normal distribution that's parametrized by mu and Sigma which doesn't allow us to compute gradients instead what we can do is consider the sampled latent vector Z as a sum of a fixed vector mu a fixed variance vector Sigma and then scaled this variance vector by a random constant that is drawn from a prior distribution so for example from a normal Gaussian and what is the key idea here is that we still have a stochastic node but since we've done this repair motorisation with this factor epsilon which is drawn from a normal distribution this stochastic sampling does not occur directly in the bottleneck layer z we've repairmen tries where that sampling is occurring and a visualization that I think really helps drive this point home is as follows so originally if we were to not perform this repair motorisation we have our our flow looks a little bit like this where we have deterministic nodes shown in blue the weights of the network as well as our input vector and we have this stochastic node z that we're trying to sample from and as we saw we can't do back propagation because we're going to get stuck at this stochastic sampling node when the network is parametrized like this instead when we re parameterize we get this diagram where we've now diverted the sampling operation off to the side to this stochastic node epsilon which is drawn from a normal distribution and now the latent variable z are deterministic with respect to epsilon the sampling operation and so this means that we can back propagate through Z without running - into errors that arise from having stochasticity and so this is a really powerful trick because this simple repair motorisation is what actually allows for VA es to be trained and to end okay and so what what do these latent variables actually look like and what do they mean because we impose these distributional priors on the latent variables we can sample from them and actually fix fix all but one latent variable and slowly tune the value of that latent variable to get an interpretation of what the network is learning and what is done is after that value of that latent variable is tuned you can run the decoder to generate a reconstructed output and what you'll see now in the example of these faces is that that output the results from tuning a single latent variable has a very clear and distinct semantic meaning so for example this is differences in the pose or the orientation of a face and so to really to re-emphasize here the network is actually learning these different latent variables in a totally unsupervised way and by perturbing the value of a single latent variable we can interpret interpret what they actually mean and what they actually represent and so ideally right because we're learning this compressed latent space what we would want is for each of our latent variables to be independent and uncorrelated with each other to really learn the richest and most compact representation possible so here's the same example as before now with faces again right where we're looking at faces and now we're walking along two axes which we can interpret as pose or orientation on the X and maybe you can tell the smile on the y axis and to re-emphasize right these are reconstructed images but the Ricans structured by keeping all other variables fixed except these two and then tuning the the value of those latent features and this is this idea of disentanglement by trying to encourage the network to learn variables that are as independent and uncorrelated with each other as possible and so to motivate the use of V AE in generative models a bit further let's go to back to this example that I showed from the beginning of lecture the question of facial detection and as I kind of mentioned right if we're given a data set with many different faces we may not know the exact distribution of these faces with respect to different features like skin color and why this could be problematic is because imbalances in the training data can result in unwanted algorithmic bias so for example the faces on the Left are quite homogeneous right and standard classifier that's trained on a data set that contains a lot of these types of examples will be better suited recognizing and classifying those faces that have features similar to those shown on the left and so this is can generate unwanted bias in our classifier and we can actually use a generative model to learn the latent variables present in the data set and automatically discover which parts of the feature space are underrepresented or over-represented and since this is a topic of today's lab I want to spend a bit of time now going through how this approach actually works and so what is done is a bae network is used to learn the underlying features of training data set in this case images of faces in a unbiased and unsupervised manner without any annotation and so here we're showing an example of one such learned latent variable the orientation of the face and again right we never told the network that orientation was important learned this by looking at a lot of examples of faces and deciding right that this was an important factor and so from from these latent distributions that are learned we can estimate the probability distribution of each of the learned latent variables and certain instances of those variables may be over-represented in our data set like homogeneous skin color or pose and certain instances may have lower probability and fall sort of on the tails of these distributions and if our data set has many images of a certain skin color that are over represented the likelihood of selecting an image with those features during training will be unfairly high right that can result in unwanted bias and similarly these faces with rarer features like shadows or darker skin or glasses may be underrepresented in the data and so the likelihood of selecting them during sampling will be low and the way this algorithm works is to use these inferred distribution x' to adaptively resample data during training and then and this is used to generate a more balanced and more fair training data set that can be then fed into the network to ultimately result in a unbiased classifier and this is exactly what you'll explore in today's lab ok so to reiterate and summarize some of the key points of V AES these VA is in encode a compressed representation of the world by learning these underlying latent features and from this representation we can sample to generate reconstructions of the input data in an unsupervised fashion we can use the repair motorisation trick to train our networks and to end and use the this perturbation approach to in her print and understand the meaning behind each of these hidden latent variables okay so these are looking at this question of death probability density estimation as their core problem what if we just are concerned with generating new samples that are as realistic as possible as the output and for that we'll transition to a new type of generative model called a generative adversarial Network or again and so the idea here is we don't want to explicitly model the density or the district distribution underlying some data but instead just generate new instances that are similar to the data that we've seen which means we want to try to accomplish ample from a really really complex distribution which we may not be able to learn directly in an efficient manner and so the approach that Ganz take is really simple they start they have a generator network which just starts from random noise and this generator in network is trained to learn a transformation going from that noise to the training distribution and our goal is we want this generated fake sample to be as close to the real data as possible and so the breakthrough to really--it reach eaving this was this gam structure where we have two neural networks one we call a generator and one we call a discriminator that are competing against each other their adversaries specifically the goal of the generator is to go from noise to produce imitations of data that are close to real as possible then the discriminator network takes both the fake data as well as true data and learns how to classify the fake from the real to distinguish between the fake and the real and by having these two networks competing against each other we can force the discriminator to become as good as possible at distinguishing fake and real and the better it becomes at doing that the better and better the generator will become at generating new samples that are as close to real as possible to try to fool the discriminator so to to get more intuition behind this let's break this down into a simple example so the generator is starting off from noise and it's drawing from that noise to produce some fake data which we're just representing here as points on a 1d line the discriminator then sees these points and it also sees some real data and over the course of the training of the discriminator it's learning to output some probabilities that a particular data point is fake or real and in the beginning if it's not trained its predictions may not be all that great but then you can train the discriminator and it starts increasing the probabilities of what is real decreasing the probabilities of what is fake until you get this perfect separation where the discriminator is able to distinguish real from fake and now the generator comes back and it sees where the real data lie and once it sees this it starts moving the fake data closer and closer to the real data and the and it goes back to the discriminator that receives these new points estimates the probability that each point is real learns to decrease the probability of the fake points maybe a little bit and it continues to adjust right and now the cycle repeats again right one last time the generator sees the real examples and it starts moving these fake points closer and closer to the real data such that the fake data is almost following the distribution of the real data and eventually eventually it's going to be very hard for the discriminator to be able to distinguish between what's real what's fake while the generator is going to continue to try to create fake data points to fool the discriminator so and and with this with this example what I'm hoping to convey is really sort of the core intuition right behind the not necessarily the detailed specifics of of how these networks are actually trained okay and so this this is you can think of this as an adversarial competition right between these two networks the generator and the discriminator and what we can do is after training use the trained generator network to create to sample and create new data that's not been seen before and so to look at examples of what we can achieve with this approach the the examples that I showed at the beginning of the lecture were generated by using this idea of progressively growing Ganz to iteratively build more detailed image generations and the way this works is that the generator in the discriminator start by having very low spatial resolution and as training progresses more and more layers are incrementally added to each of the two networks to increase the spatial resolution of of the outputted generation images and this is good because it allows for stable and robust training and generates outputs that are quite realistic and so here are some more examples of fake celebrity faces that were generated using this approach another idea is to involves unpaired image to image translation or style transfer which uses a network called 'cycle Gann and here we're taking a bunch of images in one domain for example the horse domain and without having the corresponding image in another domain we want to take the input image generate an image in a new style that follows the distribution of that new style so this is essentially transferring the style of one domain from a second and this works back and forth right and so the way this cycle gann is trained is by using a cyclic loss term where if we go from domain X to domain Y we then take the result and go back from domain Y back to domain X and we have two generators and two discriminators that are working at this at the same time so maybe you'll notice in this example of going from horse to zebra that the network has not only learned how to transform the skin of the horse from brown to the stripes of a zebra but it's also changed a bit about the background right in the scene it's learned that zebras are more likely to be found in maybe the savanna grasslands so the grass is browner and maybe more savanna like in the zebra example compared to the horse and well we actually did is to use this approach of cycle Gann to synthesize speech in someone else's voice and so what we can do is we can use a bunch of audio recordings in one voice and audio recordings in another voice and build a cycle Gantt to learn to transform representations of that one voice to make them appear like they are representations from another voice so what can be done is to take an audio waveform convert it into an image representation which is called a spectrogram and you see that image on the bottom and then train a cycle Gantt to perform the transfer from one domain to the next and this is actually exactly how we did the speech transformation for yesterday's demonstration of Obama's introduction to the course and so we're showing you what happened under the hood here and what we did is we took original audio of Alexander saying that script that was played yesterday and took the audio waveforms converted them into the spectrogram images and then trained a cycle Gann using this information and audio recordings of Obama's voice to to transfer the style of Obama's voice onto our script so welcome s-191 April introductory course on deep learning here at MIT and so yeah on the on the left right that was original of the Alexander's original audio spectrogram and the spectrogram on the right was what was generated by the cycle Gann in the style of Obama's voice okay so to summarize today we've we've covered a lot of ground on auto-encoders and variational auto-encoders and generative adversarial networks and hopefully this these this discussion of these approaches gives you a sense of how we can use deep learning to not only learn patterns and data but to use this information in a rich way to achieve generative modeling and I really appreciate the great questions and discussions and where all of us are happy to continue those that dialogue during the lab session and so our lab today is going to focus on computer vision and as alexander mentioned there is another corresponding competition for lab two and we encourage you all to to stick around if you wish to ask us questions and thank you again [Applause] 

you now I think this field is really incredible because at its core it moves away from this paradigm that we've seen so far in this class in the first three or four lectures actually so so far in this class we've been using deep learning on fixed datasets and we've really been caring about our performance on that fixed dataset but now we're moving away from that and we're thinking about scenarios where our deep learning model is its age its own self and it can act in an environment and when it takes actions in that environment it's exploring the environment learning how to solve some tasks and we really get to explore these type of dynamics scenarios where you have a autonomous agent potentially working in the real world with humans or in a simulated environment and you get to see how we can build agents that learn to solve these tasks without any human supervision in some cases or any guidance at all so they learn to solve the tasks entirely from scratch without any data set just by interacting with their environment now this has huge obvious implications in fields like robotics where you have self-driving cars and also manipulation so having hands that can grasp different objects in the environment but it also impacts the world of gameplay and specifically strategy and planning and you can imagine that if you combine these two worlds robotics and gameplay you can also create some pretty cool applications where you have a robot playing against the human in real life [Music] okay so this is a little bit dramatized and the robot here is not actually using deep reinforcement learning I'd like to say that first so this is actually entirely choreographed for a TV ad but I do hope that it gives you a sense of what this marriage of having autonomous agents interact in the real world and the potential implications of having efficient learning of the autonomous controllers that define the actions of those autonomous agents so actually let's first take a step back and look at how deep reinforcement learning fits into this whole paradigm of what we've seen in this class so far so so far what we've explored in the first three lectures actually has been what's called supervised learning and that's where we have a data set of our data X and our labels Y and what we've tried to do in the first three lectures really is learn a neural network or learn a model that takes us input the data X and learns to predict the labels Y so not an example of this is if I show you this picture of an apple or we want to train our model to predict that this is an apple it's a classification problem next we discussed in the fourth lecture the topic of unsupervised learning and in this realm we only have access to data as there are no labels at all and the goal of this problem is that we just want to find structure in the data so in this case we might see an example of two types of apples and we don't know that these are apples per se because there's no labels here but we need to understand that there's some structure underlying structure within these apples and we can identify that yes these two things are the same even if we don't know that they're specifically apples now finally in reinforcement learning we're going to be given data in the form of what are called state action pairs so States are the observations or the inputs to the system and the actions are the actions well that the agent wants to take in that environment now the goal of the agent in this world is just to maximize its own rewards or to take actions that result in rewards and in as many rewards as possible so now in the Apple example we can see again we don't know that this thing is an apple but our agent might have learned that overtime if it eats through eat an apple it counts as food and might survive longer in this world so it learns to eat this thing if it sees it so again today in this class our focus is going to be just on this third realm of reinforcement learning and seeing how we can build deep neural networks that can solve these problems as well and before I go any further I want to start by building up some key vocabulary for all of you just because in reinforcement learning a lot of the vocabulary is a little bit different than in supervised or unsupervised learning so I think it's really important that we go back to the foundations and really define some important vocabulary that's going to be really crucial before we get to building up to the more complicated stuff later in this lecture so it's really important that if any of this doesn't make sense in these next couple slides you stop me and make sure you ask questions so first we're gonna start with the agent the agent is like the central part of the reinforcement learning algorithm it is the neural network in this case the agent is the thing that takes the actions in real life you are the agents each of you if you're trying to learn a controller for a drone to make a delivery the drone is the agent the next one is the environment the environment is simply the world in which the agent operates or acts so in real life again the world is your environment now the agent can send commands to the environment in the form of what are called actions now in many cases we simplify this a little bit and say that the agent can pick from a finite set of actions that it can execute in that world so for example we might say that the agent can move forward backwards left or right within that world and at every moment in time the agent can send one of those actions to the environment and in return the environment will send back observations to that agent so for example the agent might say that okay I want to move forward one step the the environment is going to send back in observation in the form of the state and a state is a concrete or immediate situation that the action that the agent finds itself in so again for example the state might be the actual vision or the scene that the agency is around it it could be in form of an image or a video maybe sound whatever you can imagine it's just the data that the agencies in return and again this loop continues it the agency is that observation or that state and it takes a new action in return and we continue this loop now the goal of reinforcement learning is that the agent wants to try to maximize its own reward in this environment so at every step the agent is also getting back a reward from that environment now the reward is simply just a feedback measure of success or failure every time the agent acts and you don't have to get a reward every time you act but your reward might be delayed you might only get one reward at the very end of your episode so you might live a long time and then at the end of your life get a reward or not so it doesn't have to be like every moment in time you're getting a reward these rewards effectively you can think about them as just evaluating all of the agents actions so from them you can get a sense of how well the agent is doing in that environment and that's what we want to try and maximize now we can look at the total reward as just the summation of all of the individual rewards and time so if you start at some time T we can call capital R of T as the sum of all of the rewards from that point on to the future and then so simply expanding the summation out you can see it's little R of T which is the reward at this time step right now after taking this action at this time plus all of the rewards into the future potentially on an infinite time horizon now often it's very useful to consider not just the sum of all rewards but what's called the discounted sum of rewards and that's obtained by simply multiplying this discounting factor lambda by each of the rewards that at any point in time and the reason you do this is simply so that you can discount the future rewards so they don't count quite as much as a current reward so let me give a concrete example if I could offer you five dollars today or if I dollars in ten years it's still a reward of five dollars but you'd take the one today and that's because mentally you're discounting overtime that five dollars it's not worth as much to you because it's coming so far into the future so you'd prefer rewards that come as quickly as possible and again just showing this discounting total reward expanded out from a summation you can see that at each time point it's multiplying the reward at that time multiplied by the discounting factor which is typically between 0 & 1 ok so now that we've defined all these terms there's one very important function called the cue function and reinforcement learning that we now need to define so let's go back a step and remember how this total reward the total discounted reward or what's also called the return is defined so that's again just taking the current reward at time T multiplying it by a discounting factor and then adding on all future rewards also multiplied by their discounting factor as well now the Q function takes as input the current state of the agent and also takes as input the action that the agent executes at that time and it returns the expected total discounted return that the agent could expect at that point in time so let's think about what this means so this is telling us if the agent is in state s with and it takes an action a the total amount of reward total amount of discounted reward that it could obtain if it takes that action in that state is that the result from that q function and that's all the q function is telling you so it's a higher Q value is gonna tell us that we're taking an action that's more desirable in that state a lower Q value is going to tell us that we've made an undesirable action in that state so always we want to try and take actions that maximize our Q value okay so now the question is if we take this magical Q function and we have an agent that has access to this Oracle of the Q function so assume that I give you the Q function for now and the agent has access to it and I place that agent in an environment the question is how how can that agent use that cue function to take actions in the environment so let me actually ask you this as a question so if I give you this Q value Q function and you're the agent and all you see as the state how would you use that q function to take your next action exactly yeah so what would you do you would feed in all of the possible actions that you could execute at that time you evaluate your Q function your Q function is gonna tell you for some actions you have a very high Q value for other actions you have a very low Q value you pick the action that gives you the highest Q value and that's the one that you execute at that time so let's actually go through this so ultimately what we want is to take actions in the environment the function that will take us input a state or an observation and predict or evaluate that to an action is called the policy denoted here as PI of s and the strategy that we always want to take is just to maximize our Q value so PI of s is simply going to be the Arg max over our actions of that Q function so we're going to evaluate our Q function over all possible actions and then just pick the action that maximizes this Q function that's our policy now in this lecture we are going to focus on two classes of reinforcement learning algorithms and the two categories that we're going to primarily focus on first are cases where we want our deep neural network to learn the Q function so now we're actually not given the Q function as ground truth or as an Oracle but we want to learn that q function directly and the second class of algorithms is we're sorry so when you take we learned that q function and we use that q function to define our policy the second class of functions second class of algorithms is going to directly try and learn that policy without the intermediate q function to start with okay so first we'll focus on value learning which again just to reiterate is where we want the deep neural network to learn that q function and then we'll use that learn Q function to determine our policy through the same way that I did before okay so let's start digging little bit deeper into that q-function so you can get a little more intuition on how it works and what it really means and to do that I'd like to introduce this breakout game which you can see on the left the idea of the breakout game is that you have you are the paddle you're on the bottom you can move left or right or stay in the middle or don't move it all rather and you also have this ball that's coming towards you and your objective in the game is to move left and right so that you hit that ball you bounces off your paddle and it tries to hit as many of the colored blocks on top as possible every time you hit a colored block on top you break off that block hence the name of the game is called breakout the objective of the game is to break out all of the blocks or break out as many of the blocks as possible before that ball passes your paddle and yeah so the ball bounces off your peddling you try and break off as many colored blocks as possible the cue function basically tells us the expected total return that we can expect at any state given a certain action that we take at that state and the point I'd like to make here is that estimating or guessing what the Q value is is not always that intuitive in practice so for example if I show you these two actions that are two states and action pairs that this agent could take and I ask you which one of these probably has a higher Q value or said differently which one of these will give you a higher total return in the future so the sum of all of those rewards in the future from this action and state forward how many of you would say state action pair a okay how many of you would say state action pair B okay so you guys think that this is a more desirable action to take in that state state B or a scenario B okay so first let's go through these and see the two policies working in practice so we'll start with a let me first describe what I think a is gonna be acting like so a is a pretty conservative policy it's not gonna move when it sees that ball coming straight toward it which means that it's probably going to be aiming that ball somewhere towards the middle of the or uniformly across the top of the board and it's gonna be breaking off color or colored blocks across the entire top of the board right so this is what that looks like it's making progress it's killing off the blocks it's doing a pretty good job it's not losing I'd say it's doing a pretty good job but it doesn't really dominate the game okay so now it's good to be so what B is doing is actually moving out of the way of the ball just so that it can come back towards the ball and hit the ball on its corner so like ball ricochets off at an extreme angle and tries to hit the color blocks at a super EXTREME angle now what that means well actually let me ask why might this be a desirable policy mm-hmm yeah so if you catch someone's on this really extreme edges what might happen is that you might actually be able to sneak your ball up into a corner and start killing off all of the balls on the top or all of the blocks on the top rather so let's see this policy in action so you can see it's really hitting at some extreme angles and eventually it it breaches a corner on the left and it starts to kill off all the blocks on the top it gets a huge amount of reward from this so this is just an example of how it's not always intuitive to me when I first saw this I thought a was going to be the safer action to take it would be the one that gives me de Morgan's like more of a turn but it turns out that there are some unintuitive actions that reinforcement learning agents can learn to really I don't know if I would call it cheating the environment but really doing things that we as humans would not find intuitive okay so the way we can do this practically with deep learning is we can have a deep neural network which takes as input a state which could in this case is just the pixels coming from that game at that instant and also some representation of the action that we want to take in this case maybe go right move the paddle to the right it takes both of those two things as input and it returns the Q value just a single number of what the neural network believes the Q value of that state action pair is now that's fine you can do it like this there's one minor problem with doing it like this and that's if you want to create your policy you want to try out all of the different possible actions that you could execute at that time which means that you're gonna have to run this network n times at every time instant where n is the number of actions that you could take so every time you'd have to execute this network many times just to see which way to go the alternative is that you could have one network that output or takes as input that state but now it has learned to output all of the different Q values for all of the different actions so now here we have to just execute this once before we propagate once and we can see that it gives us back the Q value for every single action we look at all of those Q values we pick the one that's maximum and take the action that corresponds now that we've set up this network how do we train it to actually output the true Q value at a particular instance or the Q function over many different states now what we want to do is to maximize the target return right and that will train the agent so this would mean that the target return is going to be maximized over some infinite time horizon and this can serve as the ground truth to train that agent so we can basically roll out the agent see how it did in the future and based on how we see it got rewards we can use that as the ground truth okay now I'm gonna define this in two parts first is the target Q value which is the the real value that we got by just rolling out the episode of the agent inside this simulator or environment let's say that's the target Q value so the target Q value is composed of the reward that we got at this time by taking this at this action plus the expected or plus the maximum like the the best action that we could take at every future time so we take the best action now and we take the best action at every future time as well assuming we do that we can just look at our data see what the rewards were add them all up and discount appropriately that's our true Q value okay now the predicted Q value is obviously just the output from the network we can train these we have a target we have a predicted we can train this whole network and to end by subtracting the two taking the squared difference and that's our loss function it's a mean squared error between the target root target Q value and the predicted Q value from the network okay okay great so let's just summarize this really quickly and see how this all fits together in our Atari game we have a state we get it as pixels coming in and you can see that on the left-hand side that gets fed into a neural network our neural network outputs in this case of Atari it's going to output three numbers the Q value for each of the possible actions that can go left it can go right or it can stay and don't do anything each of those Q values will have a numerical value that's in that the neural network will predict now again how do we pick what action to take given this Q function we can just take the arc max of those Q values and just see okay if I go left I'm gonna have an expected return of 20 that means I'm gonna probably break off 20 colored blocks in the future if I stay in the center maybe I can only break off a total of three blocks in the future if I go right I'm gonna miss that ball and the game is gonna be over so I'm gonna have a return of zero so I'm gonna take the action that's going to maximize my total return which in this case is left does it make sense okay great that action is then fed back into the Atari game in this case the game repeats next frame goes and this whole process loops again now deepmind actually showed how these networks which are called deep Q networks could actually be applied to solve a whole variety of Atari games providing the state as input through pixels so just raw input status pixels and showing how they could learn the Q function so that all of the possible actions are shown on the left hand on the right hand side and it's learning that q function just by interacting with its environment and in fact they showed that on many different Atari games they were able to achieve superhuman performance on over 50% of them just using this very simple technique that I presented to you today and it's actually amazing that this technique worked so well because to be honest it is so simple and it is extremely clean how how clean the idea is it's very it's very elegant in some sense how how simple it is and still it's able to achieve superhuman performance which means that it beat the human on over 50% of these Atari games so now that we saw the magic of q-learning I'd like to touch on some of the downsides that we haven't seen so far so so far the main downside of q-learning is that it doesn't do too well with complex action in bar action scenarios where you have a lot of actions a large action space or if you have a continuous action space which would correspond to infinite number of actions right so you can't effectively model or parameterize this problem to deal with continuous action spaces there are ways that you can kind of tweak it but at its core what I've presented today is not amenable to continuous action spaces it's really well suited for small action spaces where you have a small number of possible actions and can and discrete possibilities right so a finite number of possible actions at every given time it's also its policy is also deterministic because you're always picking the action that maximizes your your Q function and this can be challenging specifically when you're dealing with stochastic environments like we talked about before so q q sorry Q value learning is really well-suited for a deterministic action spaces sorry deterministic environments discrete action spaces and we'll see how we can ask you learning to something like a policy gradient method which allows us to deal with continuous action spaces and potentially stochastic environments so next up we'll learn about policy learning to get around some of these problems of how we can deal with also continuous action spaces and stochastic environments or probabilistic environments and again just to reiterate now we've gone through this many times I want to keep drilling it in you're taking as input in queue networks you're taking as input the state you're predicting Q values for each of your possible actions and then your final answer your policy is determined by just taking the arc max of that Q function and taking an action that maximizes a q function okay the differentiation with policy gradient methods is that now we're not going to take we're still going to take as input the state at that time but we're not going to output the Q function we're directly going to output the policy of the network or rather let me say that differently we're going to output a probability distribution over the space of all actions given that state so this is the probability that taking that action is going to result in the highest Q value this is not saying that what Q value am I going to get is just saying that this is going to be the highest Q value this is the probability that this action will give me the highest Q value so it's a much more direct formulation we're not going with this intermediate Q function we're just directly saying let's optimize that policy automatically does that make sense okay so once we have that probability distribution we can again we see how our policy executes very naturally now so that probability distribution may say that taking a left will result in the maximum Q value of 0.9 with probability 0.9 staying in the center will result in a probability or a maximum reward or return with 0.1 going to the right is a bad action you should not do that because you're definitely not going to get any return now with that probability distribution that defines your policy like that is your policy you can then take an action simply by sampling from that distribution so if you draw a sample from that probability distribution that exactly tells you the action you should take so if I sample from this probability distribution here I might see that the action I select is a 1 going left but if I sample again it since it's probably sztyc I could sample again and it could tell me a 2 because a 2 also has a probability point 1 on average though I might see that 90 percent of my samples will be a 1 10% of my samples will be a 2 but at any point in time if I want to take an action all I do is just sample from that probability distribution and act accordingly and note again that since this is a probability distribution it follows all of the typical probability distribution properties so all of its elements all like its its total mass must add up to 1 because it's a probability distribution now already off the bat does anyone see any advantages of this formulation why we might care about directly modeling the policy instead of modeling the q function and then using that to deduce a policy if you formulate the problem like this your output is a probability distribution like you said but what that means is now we're not really constrained to dealing only with categorical action spaces we can parameterize this probability distribution however we'd like in fact we could make it continuous pretty easily so let's take an example of what that might look like this is the discrete action space so we have three possible actions left right or stay in the center and a discrete action space is going to have all of its mass on these three points the summation is going to be one of those masses but still they're concentrated on three points a continuous action space in this realm instead of asking what direction should I move the continuous action space is going to say maybe how fast should I move in in whatever direction so on the right is gonna be faster and faster to the right it's a speed now and on the left of the axis is gonna be faster and faster to the left so you could say I want to move to the left with speed 0.5 meters per second or 1.5 meters per second or whatever real number you want it's a continuous action space here now when we plot the probability density function of this of this policy we might see that the probability of taking an action giving a state has a mass over the entire number line not just on these three points because now we can take any of the possible actions along this number line not just a few specific categories so how might we do that with policy gradient networks that's really the interesting question here and what we can do is we assume that our output follows a Gaussian distribution we can parameterize that Gaussian or the output of that Gaussian with a mean and a variance so at every point in time now our network is going to predict the mean and the variance of that distribution so it's outputting actually a mean number and a variance number now all we have to do then let's suppose that mean in variances minus 1 and 0.5 so it's saying that the center of that distribution is minus 1 meters per second or moving one meter per second to the left all of the mass then is centered at minus 1 with a variance of 0.5 okay now again if we want to take an action with this probability distribution or this policy we can simply sample from this distribution if we sample from this distribution in this case we might see that we sample a speed of minus 0.8 which corresponds to or sorry a velocity of minus 0.8 which corresponds to a speed of 0.8 to the left okay and again same idea is before now that's continuous if we take an integral over this probability distribution it has to add up to 1 ok makes sense great ok so that's a lot of material so let's cover how policy gradients works in a concrete example now so let's walk through it and let's first start by going back to the original reinforcement learning loop we have the agent the environment agent sends actions to the environment environment sends observations back to the agent let's think about how we could use this paradigm combined with policy gradients to Train like a very I guess intuitive example let's train a self-driving car ok so the agent in this case is the vehicle it's state is whatever sensory information that receives maybe it's a camera attached to the vehicle the action it could take let's stay simple it's just the steering wheel angle that it should execute at that time this is a continuous variable it can take any of the angles within some bounded set and finally the reward let's say is the distance traveled before we crash okay great so the training algorithm for policy gradients is a little bit different than the training algorithm for cue function or Q deep neural networks so let's go through it step by step in this example so to train our self-driving car what we're gonna do is first initialize the agent the agent is in self-driving car we're gonna start the agent in the center of the road and we're going to run a policy until termination okay so that's the policy that we've ran in the beginning it didn't do too good it crashed pretty early on but we can train it okay so what we're gonna do is record all of the states and all the actions and all of the rewards at every single point in time during that entire trajectory given all of these state action reward pairs we're gonna first look at right before the crash and say that all of these actions because they happened right before a crash or Viper's right before this undesirable event we're gonna penalize all of those actions so we're gonna decrease the probability of selecting those actions again in the future and we're gonna look at actions that were taken farther away from that undesirable event with higher rewards and we're going to increase the probability of those actions because those actions resulted in more desirable events the car stayed alive longer when it took those actions when it crashed it didn't stay alive so we're gonna decrease the probability of selecting those actions again in the future okay so now that we've tried this once through one training iteration we can try it again we've reinitialized the agent we run a policy until termination we do the same thing again decrease the probability of things closer to the crash increase the probability of actions farther from the crash and just keep repeating this over and over until you see that the agent starts to perform better and better drive farther and farther and accumulate more and more reward until eventually it starts to follow the lanes without crashing now this is really awesome because we never taught anything about what our lane markers it's just seeing images of the road we never taught anything about how to avoid crashes it just learned this from sparse rewards the remaining question here is how we can actually do these two steps I think how can we do the step of decreasing the probability of actions that were undesirable and how can we increase the probability of actions that were desirable I think everything else is conceptually at least is pretty clear I hope the question is how do we improve our policy over time so to do that let's first look at the loss function for training policy gradients and then we'll dissect it to understand a little bit why this works the loss consists of two terms the first term is the log likelihood of selecting the action given the state that you were in so this really tells us how likely was this action that you selected the second term is the total discounted return that you received by taking that action that's really what you want to maximize so let's say if the agent or if the car got a lot of return a lot of reward for an action that had very high log likelihood so it was very likely to be selected and they got a lot of reward from that action that's going to be a large number multiplied by a large number when we multiply them together and we multiply them together you get another large number you add in this negative in front of this loss function so now it's gonna be an extremely negative number remember that neural network tried to minimize their loss so that's great so we're in a very desirable place we're in the we're in a pretty good minimum here so we're not gonna touch that probability at all let's take another example now we have an example of where the reward is very low for an action so R of T is very small and let's assume that the probability of selecting this action that we took was very high so we took an action that we were very confident in taking but we got a very low reward for it or very low return for it what are we going to do so that's a small number now multiplied by this probability distribution our loss is going to be very small we multiply it by the negative in front the total loss is going to be large right so on the next training iteration we're going to try and minimize that loss and that's going to be either by trying out different actions that may result in higher return or higher reward moving some of the probability of taking that action that we took again in the future so we don't want to take that same action again because we just saw that it didn't have a good return for us and when we plug this loss into the gradient descent algorithm that we saw in lecture 1 to train our neural network we can actually see that the policy gradient itself is highlighted here in blue so that's the that's why it's called policy gradients right because you're taking the gradient of this policy function scaled by the return and that's where this this method really gets its name from so now I want to talk a little bit about how we can extend this to perform reinforcement learning in real life so far I've only really shared examples of you with you about doing reinforcement learning in either games or in the simple toy example with the car what do you think is the shortcoming of this training algorithm so there's a real reason here why we haven't seen a ton of success of reinforcement learning in the real life like we have seen with the other fields that we've covered so far in this class and that's because one of these steps has a severe limitation when you're trying to play it in the real world does anyone have an idea okay so reinforcement learning in real life the big limitation here obviously is you can't actually run a lot of these policies in real life safety critical domains especially let's think about self-driving cars I said run until termination I don't think you want to do that on every single training iteration not just like at the end goal this is like every single step of your gradient descent algorithm millions of steps I don't think that's a desirable outcome we can get around this though so we can think about about training in simulation before deploying in the real world the problem is that a lot of modern simulators are not really well suited for very photo realistic simulation that would support this kind of transfer transfer from the simulator to the real world when you deploy them one really cool result that we created in in my lab also that some of the TAS so you can ask them if you have any questions has been developing a brand new type of photo realistic simulation engine for self-driving cars that is entirely data-driven so the simulator we created what's called Travis sorry Vista and it allows us to use real data of the world to simulate virtual agents so these are virtual reinforcement learning agents that can travel within these synthesized environments and the results are incredibly photorealistic and they allow us to train agents in reinforcement learning environments entirely in simulation so that they can be deployed without any transfer in the real world in fact that's exactly what we did we placed agents inside of our simulator we trained them using policy gradient algorithms which is exactly what we talked about today and we took these train policies and put them on board our full-scale autonomous vehicle without changing anything they learn to drive in the real world as well just like they learn to drive in the simulator on the left hand side you can actually see us sitting in the vehicle but it's completely autonomous it's executing a policy that was trained using reinforcement learning entirely within the simulation engine and this actually represented the first time ever a full-scale autonomous vehicle was trained using only reinforcement learning and able to successfully to be deployed in the real world so this was really awesome result that we had and now we've covered some fundamentals of policy learning also value learning with cue functions what are some exciting applications that have sparked this field I want to talk about these now as well for that we turned to the game of Go so in the game of Go agents humans or autonomous agents can be playing against each other and an autonomous agent specifically was trained to compete against humans specifically a human or many human champions and achieved what at the time was a very very exciting result so first I want to give some background very quick background on to the game of go because probably a lot of you are not too familiar with go-go is played on a 19 by 19 grid it's played between two players who rolled white and black pieces the objective of the game is to basically occupy as much space on the board as as possible you want to claim territory on the board but the game would go and the strategy behind go is incredibly complex that's because there are more positions more possible positions and go than the number of atoms in the universe so the objective of our AI is to learn this incredibly complex state space and learn how to not only beat other autonomous agents but learn how to eat the existing gold standard human professional goal players now google deepmind rose to the challenge a couple years ago and developed a reinforcement learning pipeline which defeated champion players and the idea at its core was actually very simple and I'd like to go through it just in a couple steps in the last couple minutes here first they trained a neural network to watch a human play go so these are expert humans and they got a whole dataset of how humans play go and they trained their neural network to imitate the moves of those humans and imitate those behaviors they then used these pre trained networks trained from the expert go players to play against their own reinforcement learning agents and the reinforcement learning policy network which allowed the policy to go beyond what was imitating the humans and actually go beyond human level capabilities to go superhuman the other trick that they had here which really made all of this possible was the usage of an auxilary neural network which not only was the policy network she took as input the state to predict the action but also an auxilary network which took it and put the state and predicted the how good of a state this was so you can think of this as kind of again similar an idea to the Q network but it's telling you for any particular state of the board how good of a state is this and given this network what the AI could do is basically hallucinate different possible trajectories of actions that it could take in the future and see where it ends up use that network to say how good of a state have all of these board positions become and use that to determine where I where that action or where that agent needs to act in the future and finally a recently published extension of these approaches just a year ago called alpha zero used use basically self to play all the way through so the previous example I showed used a buildup of expert data to imitate and that was what started the foundation of the algorithm now in alpha zero they start from zero they start from scratch and use entirely self play from the beginning in in these examples they showed examples on let's see it was chess go many other games as well they showed that you could use self play all the way through without any need of training with or pre training with human experts and instead optimize these networks entirely from scratch ok so finally I just like to summarize this lecture today we saw how deep reinforcement learning could be used to train agents in environments in this learning loop where the agent interacts with the environments we've got a lot of foundations into reinforcement learning problems we learned about cue learning where agents try to optimize the total expected return of their for their actions in the future and finally we got to experience how policy gradient algorithms are trained and how we can directly optimize the policy without going through the cue function at all and in our lab today we will get some experience of how we can train some of these reinforcement learning agents in the game of pong and also some simpler examples as well through the debug with and play around with next we will hear from abba who's gonna be talking about limitations as a very wide stretching lecture it's very exciting lecture it's actually one of my favorite lectures its limitations of deep learning so of all of these approaches that we've been talking about and also some really exciting new advances of this field and where it's moving in the future so I hope you enjoyed that as well thank you you [Applause] you 

you okay so as Alexander mentioned in this final lecture I'll be discussing some of the limitations of deep learning and you know as with any technology not just deep learning or other areas of computer science it's important to be mindful of not only what that technology can enable but also what are some of the caveats and limitations when considering such approaches and then we'll move into discuss some of the new research directions that are specifically being taken to address some of those limitations before we dive into the technical content we have some important logistical and course related announcements which I think will be very relevant to most of you first and foremost we have class t-shirts and they've arrived and so we'll be distributing them today at the end of the lecture portion of the class and at that time we'll take a little bit of time to discuss the logistics of how you can come and receive a t-shirt for your participation in the course so to check in quickly on where we are in the course this is going to be the last lecture given by Alexander and myself and tomorrow and Friday we will have a series of guest lectures from leading researchers in industry today we'll also have our final lab on reinforcement learning and thank you everyone who has been submitting your submissions for the lab competitions the deadline for doing so is tomorrow at 5:00 p.m. and that's for lab 1 2 & 3 so if you're interested in that please email us with your entries and on Friday will be our final guest lectures and project presentations and so for those of you who are taking the course for credit as was mentioned on day one you have two options to fulfill your credit requirement and we've received some questions about the logistics so I'd like to go through them briefly here so you can present a proposal for the project proposal competition and the requirements for this you can present as an individual or in a group from one person to four people and in order to be eligible for a prize you must have at least one registered student a registered MIT student in your group and we recognize that one week is an extremely short period of time to implement you know a new deep learning approach but of course will not necessarily be judging you based on your results although results will be extremely helpful for you in the in the project competition but rather more on the novelty and the potential impact and the quality of your presentation so these are going to be really short they're going to be three minutes and we're going to hold you to that three-minute window as strictly as we can and so there's a link on this slide which you can find on the PDF version that's going to take you to a document where the instructions for the final project are laid out including the details for group submission and slide submission and yeah here are additional links for the for the final project proposal and the second option to fulfill the credit requirement is a short one page review of a recent deep learning paper and this is going to be on do on the last day of class by Friday at 1:00 p.m. via email to us okay so tomorrow we're going to have two guest speakers we're going to have David Cox from IBM who is actually the director of the MIT IBM Watson AI lab come and speak and we're also going to have an amass a professor at U Toronto and a research scientist at Nvidia and he's going to speak about robotics and robot learning and the lab portion of tomorrow's class will be dedicated to just open office hours where you can work with your project partners on the final project you can continue work on the labs you can come and ask us and the TAS any further questions and on Friday we're going to have two additional guest speakers so Tuan Lee who is the chief scientific officer at lamda Labs it's a company that builds new hardware for deep learning is going to speak about some of the research that they're doing and then we're going to have a exciting talk from Google the Google brain team on how we can use machine learning to understand the scent and smell properties of small molecules and importantly on Friday will be our project proposals and our awards ceremony so if you have submitted entries for the lab competitions that is when you would be awarded prizes and so we really encourage you to attend Thursday and Friday's lectures and classes in order to be eligible to receive the prizes okay so now to get into the technical content for this last lecture from Alexander at nine so hopefully over the course of the past lectures you've seen a bit about how deep learning has enabled such tremendous applications in a variety of fields from autonomous vehicles to medicine and healthcare to advances in reinforcement learning that we just heard about generative approaches robotics and a whole host of other applications and areas of impact like natural language processing finance and security and along with this hopefully you've also established a more concrete understanding of how these neural networks actually work and largely we've been dealing with algorithms that take as input data in in some form you know as signals as images or other sensory data to directly produce a decision at the ALPA or a prediction and we've also seen ways in which these algorithms can be used in the opposite direction to generatively sample from them to create brand-new instances and day examples but really what we've been talking about is algorithms that are very well optimized to perform at a single task but they failed to generalize and go beyond that to achieve sort of a higher-order level of power and I think one really good way to understand this limitation is to go back to a fundamental theorem about the capabilities of neural networks and this was a theorem that was presented in 1989 and it generated quite the stir and it's called the universal approximation theorem and what it states is that a feed-forward feed-forward neural net with a single hidden layer could be sufficient to approximate any function and we've seen you know with deep deep learning models that use multiple hidden layers and this this theorem is actually completely ignoring guy and say saying oh you just need one hidden layer if you believe that any problem can be reduced to a functional mapping between inputs and outputs you can build a neural net that would approximate this and while you may think that this is really an incredibly powerful statement if you look closely there are a few key limitations and considerations that we need to have first this theorem is making no guarantees about the number of hidden units or the size of the hidden layer that would be required to make this approximation and it's also leaving open the question of how you actually go about finding those weights and optimizing the network for that task it just proves that one theoretically does exist and as we know from gradient descent this optimization can actually be really tricky and difficult in practice and finally there's no guarantees that are placed about how well such a network would generalize to other related tasks and this theorem is is sort of a perfect example of the possible effects of overhype of deep learning and artificial intelligence broadly and as a community and now you know you you all are part of that community and as a community that's interested in advancing the state of deep learning I believe that we need to be really careful about how we market and advertise these algorithms and while the universal approximation theorem generated a lot of excitement when it first came out also in some senses provided some degree of false hope to the AI community that neural nets could be used to solve any problem and this hype can be very dangerous and when you look back at the history of AI and sort of the the peaks and the Falls of the literature there have been these two AI winters where research in AI and neural networks specifically came to a halt and experienced a decline and that's kind of motivating why for the rest of this lecture we want to discuss further some of the limitations of these approaches and how we could potentially move towards addressing them okay so what are some of those limitations one of my favorite examples of a potential danger of deep neural nets comes from this paper from a couple years ago named understanding deep neural networks requires rethinking generalization and this was a paper from Google and really what they did was quite simple they took images from this huge image data set called image net and each of these images is annotated with a label right and what they did was for every image in the dataset they flipped a die that was K sided they made a random assignment of what that of what a new label for that image was going to be so for example if you randomly randomly choose the the labels for these images you could generate something like this and what this means is that these new labels that are now associated with each image are completely random with respect to what is actually present in that image and so and so if you see the two examples of the dog have two completely different labels right and so we're literally trying to randomize our data and and the labels entirely and after they did that what they then tried to do was to fit a deep neural net to this sampled data ranging from the original untouched data to data on the right where the labels were now completely randomly assigned and as you may expect the accuracy of the resulting model on the test set progressively tended to zero as you move from the true labels to the random labels but what was really interesting was what happened when they looked at the accuracy on the training set and this is what they found they found that no matter how much they randomized the labels the model was able to get 100% accuracy or a close to 100% accuracy on the training set meaning that it's basically fitting to the data and their labels and this is really a powerful example because it shows once again in a similar way as the universal approximation theorem that deep neural Nets are very very good at being able to perfectly fit or very close to perfectly fit any function even if that function is this random mapping from data to labels and to drive this point home even further I think the best way to understand neural nets is as function approximator z' and all the universal approximation function approximation theorem states is that neural networks are very good at doing this so for example if we have this data visualized on a 2d grid we can learn we can use a neural network to learn a function a curve that fits this data and if we present it with a candidate point on the x axis it may be able to produce a strong very likely estimate of what the corresponding Y value would be but what happens to the left and to the right right what if we extend the spread of the data a bit in those directions how does the network perform well there are absolutely no guarantees on what the training data looks like in these regions right what the data looks like in these regions that the network hasn't seen before and this is absolutely one of the most significant limitations that exist in modern deep learning and this races are the questions of what happens when we look at these places where the model has insufficient or no training data and how can we as implementers and users of deep learning and deep neural nets have a sense of when we know that the model doesn't know when it's not confident when it's uncertain in making a prediction and I think this notion leads very nicely into this other idea of adversarial attacks on neural nets and the idea here is to take some data instance for example this image of a temple which you know a standard CNN trained on image data can classify with very high accuracy and then apply some perturbation to that image such that when we take the results after that perturbation and now feed it back into our neural network it generates a completely nonsensical prediction like ostridge about what is actually in that image and so this is maybe a little bit shocking why is it doing this and how how is this perturbation being created to fool the network in such a way so remember when we're training our networks we use gradient descent and what that means is we have this objective function J that we're trying to optimize for and what specifically we're trying to optimize is the set of weights W which means we fix our data and our labels and iteratively adjust our weights to optimize this objective function and the way an adversarial example is created is kind of taking the opposite approach where we now ask how can we model why the input image our data X in order to increase the error in the networks prediction to fool the network and so we're trying to perturb and adjust X in some way by fixing the weights fixing the labels and iteratively changing X to generate a robust adversarial attack and an extension of this was recently done by a group of students here at MIT where they devised an algorithm for synthesizing adversarial examples that were robust to different transformations like changing the shape scaling color changes etc and what was really cool is they moved from beyond the 2d setting to the 3d setting where they actually 3d printed physical objects that were designed to fool a neural network and this was the first demonstration of actual adversarial examples that existed in the physical world in the 3d world and so here they 3d printed a bunch of these adversarial Turtles and when they fit fed images of these turtles to a neural network trained to classify these images the network incorrectly classified these adversarial examples as rifles rather than Turtles and so this just gives you a taste of some of the limitations that exists for neural networks and deep learning and other examples are listed here including that the fact that they can be subject to algorithmic bias that they can be susceptible to these adversarial attacks that they're extremely data hungry and so on and so forth and moving forward to the next half of this lecture we're going to touch on three of these sets of limitations and how we can enable how we can bit how we can push research to sort of address some of these limitations specifically we'll focus on how we can encode structure and prior domain knowledge into designing our network architecture we'll talk about how we can represent uncertainty and understand when our model is uncertain or not confident in its predictions and finally how we can move past deep learning where models are built to solve a single problem and potentially move towards building models that are capable to address many different tasks ok so first we'll talk about how we can encode structure and domain knowledge into designing deep neural nets and we've already seen an example of this in the case of convolutional neural networks that are very well equipped to deal with spatial data and spatial information and if you consider you know a fully connected network as sort of the baseline there's no sense of structure there the the nodes are connected to all other nodes and you have these dense layers that are fully connected but as we saw you know CN NS can be very well-suited for processing visual information and visual data because they have this structure of the convolution operation and recently research has researchers have moved on to develop neural networks that are very well suited to handle another class of data and that's of graphs and graphs are an irregular data structure that encode this very very rich structural information and that structural information is very important to the problem that can be considered and so some examples of data that can be well represented by a graph is that of social networks or Internet traffic those problems that can be represented by state machines patterns of human mobility or transport small molecules and chemical structures as well as biological networks and you know there are a whole class of problems that can be represented this way an idea that arises is how can we extend neural networks to learn and process the data that's present in these graph structures and this folds very nicely from an extension of convolutional neural nets and with convolutional neural networks as we saw what we have is this rectangular filter right that slides across an image and applies this patchy convolution operation to that image and as we go across the entirety of the image the idea is we can apply the set of weights to extract particular local features that are present in the image and different sets of weights extract different features in graph convolutional networks the idea is very similar where now rather than processing a 2d matrix that represents an image we're processing a graph and what graph convolutional networks use is a kernel of weights a set of weights and rather than sliding this set of weights across a 2d matrix the weights are applied to each of the different nodes present in the graph and so the network is looking at a node and the neighbors of that node and it goes across the entirety of the graph in this manner and aggregates information about a node and its neighbors and encodes that into a high-level representation and so this is a really very brief and a high-level introduction to what a graph convolutional network is and on Friday we'll hear from a expert in this domain Alex will Chico from Google brain who will talk about how we can use graph convolutional networks to learn small molecule representations okay and another another class of data that we may encounter is not 2d data but rather 3d sets of points and this is what is often referred to as a point cloud and it's basically just this unordered cloud of points where there is some spatial depend and it represents you know sort of the depth and our perception of the 3d world and just like images you can perform classification or segmentation on this 3d point data and it turns out that graph convolutional networks can also be extended to handle and analyze this point cloud data and the way this is done is by dynamically computing a graph based on these point clouds that essentially creates a mesh that preserves the the local depth and the spatial structure present in the point cloud ok so that gives you a taste of how different types of data and different network structures can be used to encode prior knowledge into our network another area that has garnered a lot of interest in research recent years is this question of uncertainty and how do we know how confident a model is in its predictions so let's consider a really simple example a classification example and what we've learned so far is that we can use in network to output a problem a classification probability so here we're training a network to classify images of cats versus images of dogs and it's going to output a probability that a particular image is a cat or it's a dog but what happens if we feed the network an image of a horse it's still going to output a probability that that image is a cat or that it's a dog and because probabilities have to sum to one they're going to sum to one right and so this is a clear distinction between the probability the prediction of the network and how confident the model is in in that prediction a probability is not a metric of confidence and so in this case we would you could imagine it would be desirable to have our network also give us a sense of how confident it is in prediction so maybe when it sees an image of a horse it says okay this is a dog with probabilities 0.8 but I'm not confident at all in this prediction that I just made right and one possible way to accomplish this is through Bayesian deep learning and this is a really new and emerging field and so to understand this right to reiterate our learning problem is the following we're given some data X and we're trying to learn an output Y and we do that by learning this functional mapping F that's parametrized by a set of weights W in bayesian neural Nets what is done is rather than directly learning the weights the neural network actually approximates a posterior probability distribution over the weights given the data X and the labels Y and Bayesian neural networks are considered Bayesian because we can rewrite this posterior P of W given x and y using Bayes rule but computationally it turns out that actually computing this posterior distribution is infeasible and intractable so what has been done is there have been different approaches in different ways to that try to approximate this distribution using sampling operations and one example of how you can use sampling to approximate this posterior is by using dropout which was a concept that we introduced in the first lecture and in do in doing this you can actually obtain a metric and an estimate of the models uncertainty and to think about a little bit how this may work let's consider a convolutional Network where we have sets of weights and what is done is we perform different passes through the network and each time a pass is made through the network the set of weights that are used are stochastically sampled and so here right these are our convolutional kernels are sets of weights and we apply this dropout filter this dropout mask where any each filter some of those weights are going to be dropped out to zero and as a result of taking a element-wise multiplication between the kernel and that mask we generate these resulting filters where some of the weights have been stochastically dropped out and if we do this many times say tea times we're going to obtain different predictions from the model every time and by looking at the expected value of those predictions and the variance in those predictions we can get a sense of how uncertain the model is and one application of this is in the context of depth of estimation so the goal here is to take images and to train a network to predict the depth of each pixel in that image and then you also ask it okay provide us a uncertainty that's associated with each prediction and what you can see here in the image on the right is that there's this particular band a hotspot of uncertainty and that corresponds where to that portion of the image where the two cars are overlapping which kind of makes sense right you may we may not have as clear of a sense of the depth in that region in particular and so to to conceptualize this a bit further this is a general example of how you can ensemble different instances of models together to obtain estimates of uncertainty so let's say we're working in the context of self-driving cars right and our task is given an input image to predict a steering wheel angle that will be used to control the car and that's new the mean and in order to estimate the uncertainty we can take an ensemble of many different instances of a model like this and in the case of dropout sampling each model will have different sets of weights that are being dropped out and from each model we're going to get a different estimate of the predicted steering wheel angle right and we can aggregate many of these different estimates together and they're going to lie along some distribution and to actually estimate the uncertainty you can consider the variance right the spread of these estimates and intuitively if these different estimates are spread out really really far right to the left and to the right the model is going to be more uncertain in its prediction but if they're clustered very closely together the model is more certain more confident in its prediction and these estimates are actually being drawn from an underlying distribution and what ensemble is trying to do is to sample from this underlying distribution but it turns out that we can approximate and model this distribution directly using a neural network and this means that we're learning what is called an evidential distribution and effectively the evidential distribution captures how much evidence the model has in support of a prediction and the way that we can train these evidential networks is by first trying to maximize the fit of the inferred distribution to the data and also minimizing the evidence that the model has for cases when the model makes errors and if you train a network using this approach you can generate calibrated accurate estimates of uncertainty for every prediction that the network makes so for example if we were to train a regression model and suppose we have this case where in the white regions the model has training data and in the gray regions the model has no training data and as you can see a deterministic regression model fits this region this white region very well but in the gray regions it's not doing so well because it hasn't seen data for these regions before now we don't really hair too much about how well the model does on those regions but really what would be more important is if the model could tell us oh I'm uncertain about my prediction in this region because I haven't seen the data before and by using an evidential distribution our network actually generates these predictions of uncertainty that scale as as the model has less and less data or less and less evidence and so these uncertainties are also robust to adversarial perturbations and adversarial changes like similar to those that we saw previously and in fact if an input suppose an image is adversely perturbed and it's increasingly adversely perturbed the estimates of uncertainty are also going to increase as the degree of perturbation increases and so this example shows depth estimation where the more the input is perturbed the more the unknowns associated uncertainty of the network's prediction increases and so there and I won't spend too much time on this but uncertainty estimation can also be integrated into different types of tasks beyond depth estimation or regression also semantic and instant segmentation and this was work done a couple years ago where they actually used estimates of uncertainty to improve the quality of the segmentations and death estimations that they made and what they showed was compared to a baseline model without any estimates of uncertainty they could actually use these metrics to improve the performance of their model at segmentation and depth estimation okay so the final area that I'd like to cover is how we can go beyond you know I have us as users and implementers of neural networks to where we can potentially automate this approach that this pipeline and as you've hopefully seen through the course of these lectures and in the labs neural networks need to be finely tuned and optimized for the task of interest and as models get more and more complex they require some degree of expert knowledge right which hopefully some of what you've hopefully learned through this course too you know select the particular architecture of the network that's being used to selecting and tuning hyper parameters and adjusting the network to perform as best as it possibly can what Google did was they built a learning algorithm that can be used to automatically learn a machine learning model to solve a given problem and this is called Auto ml or automated machine learning and the way it works is it uses a reinforcement learning framework and in this framework there's a controller neural network which is sort of the agent and what the controller neural network does is it proposes a child model architecture in terms of the hyper parameters that that model architecture would theoretically have and then that resulting child network is trained and evaluated for a particular task say image classification and its performance is used as feedback or as reward for the controller agent and the controller agent takes this feedback into account and iteratively improves the resulting child network over thousands and thousands of iterations to iteratively produce new architectures test them the feedback is provided and this cycle continues and so how does this controller agent work it turns out it's an RNN controller that sort of at the macro scale considers what are the different values of the hyper for a particular layer in a generated Network so in the case of convolution and in the case of a CNN that may be the number of convolutional filters the size of these convolutional filters etc and after the controller proposes this child Network the child network is trained and its accuracy is evaluated right through the normal training and testing pipeline and this is then used as feedback that goes back to the controller and the controller can then use this to improve improve the child Network in in future iterations and what Google has done is that they've actually generated a pipeline for this and put this service on the cloud so that you as a user can provide it with a data set and a set of metrics that you want to optimum optimize over and this Auto ml framework will spit out you know candidate child networks that can be deployed for your tasks of interest and so I'd like to think use this example to think a little bit about what this means for deep learning and AI more generally this is an example of where we were Google was able to use a neural network and AI to generate new models that are specialized for particular tasks and this significantly reduces the burden on us as engineers in terms of you know having to perform a hyper parameter up optimization and choosing our architectures wisely and I think that this gets at the heart of what is the distinction between the capabilities that AI has now and our own human intelligence we as humans are able to learn tasks and use you know the analytical process that goes into that to generalize to other examples in our life and other problems that we may encounter whereas neural networks in AI right now are still very much constrained and optimized to perform well at particular individual problems and so I'll leave you with with that and sort of encourage you to think a little bit about what steps may be taken to bridge that gap and if those steps should be taken to bridge that gap so that concludes this talk [Applause] 

you thanks so much for inviting me this is a great this is a great class and a great program and I'm really excited to see deep learning front and center as part of IEP I understand you've covered kind of the basics of deep learning and I'm going to tell you today about something that's a little bit of a mash up on top of sort of standard deep learning kind of going beyond deep learning but before I start I just want to say something about this word artificial intelligence because you know I had artificial intelligence in the last slide if you look at my business card it actually says artificial intelligence in two separate places on that card and I actually also have a confession I'm a recovering academic so I was a professor at Harvard for ten years I just joined IBM about two years ago this is my first real job my mom is very proud of me that I finally got out of school and I will say as an academic researcher working on AI we hated this term 2017 and before we would do anything we could not to say these words you know we'd say machine learning or we'd say deep learning be more specific but 2018 and beyond for whatever reason we've all given up and we're all calling it a I you know we're calling it AI Google's calling it AI academics are calling it AI but when I when I got to IBM they had done something that I really appreciated and it helps kind of frame the discussion it will frame the discussions about what I'm gonna tell you about research wise in a minute that's just to do something very simple since are part of something that IBM does called the global technology outlook which is like a annual process where we vision for that company for the corporation what the future lies holds ahead and they did something very simple just to put adjectives in front of AI just to distinguish what we're talking about when we're talking about different things so this will be relevant to where I where we want to push relative to where we are today with deep learning to where we want to go and that's to distinguish what we have today as narrow AI so it's not to say it's not powerful or disruptive but just to say that it's limited in important ways and also to kind of distinguish it from general AI which is the stuff that the public and the press likes to talk about sometimes when IBM research which if you don't know where a 4,300 person global research organization they have six Nobel Prizes been around for 75 years when IBM research tried to decide when this was going to happen when generally I was gonna happen we said 2050 and Beyond and basically when you when you ask scientists something and they tell you 2015 beyond when it's coming that means we have no idea but it's no time soon but in the middle of this notion of broad iya and that's really what what we're we're here today about and what the lab Iran is about and you know just to unpack this one level deeper you know we have on one hand we have have general AI this is this idea of you know broadly autonomous systems that can decide what they do on their own this is the kind of thing that Elon Musk described as summoning the demon so congratulations everyone you're helping to summon the demon according to Johann Musk or you know slightly more level-headed people like Stephen Hawking warning that artificial intelligence could end and mankind yeah this is kind of you know maybe we need to worry about this in the future but actually what I'll argue in just a minute is that we're actually in quite a bit more limited space right now and really if this this broad AI that we really need to focus on so how do we build systems that are multi task and multi domain they can take knowledge from one place apply it in another that can incorporate lots of different kinds of data not just you know images or video but images video tax structured data unstructured data it's distributed it runs in the cloud but also runs an edge devices and it's explainable so we can understand what these systems do and this is basically then the the roadmap for everything that my lab does so we're we're asking what are the barriers we need to break down to bring in this era where we can apply AI to all the different kinds of problems that we need to apply to so things like explain ability we need to have systems that aren't just black boxes but we can look inside and understand why they make decisions when they make a right decision we know why it made that decision when they make a wrong decision we have the ability to get reach in and figure out how we would do bug that system one interesting thing about the AI revolution and you know back in the day people said that software was gonna eat the world and these days Jennsen Wang the CEO of Nvidia is on record saying that AI is going to eat software I think increasingly that's true we're gonna have data-driven software systems that are based on technology like deep learning but the terrifying thing about that is we don't really yet have debuggers it's very hard in many cases to figure out why systems aren't working so this is something that's really holding ái today security I'll tell you a little bit about the kind of weird world of security we live in now with AI where AI systems can be hacked in interesting ways we close those gaps to be able to really realize the full potential of AI systems need to be fair and I'm biased you know we that's both the thing that's good for the world but it's also the case that in many regulated industries like the kinds of companies that I've VM serves like banks they're regulated such that the government insists that their systems be provably fair we need to be able to look inside see and understand that the decisions the system will make will be fair and unbiased and then on a practical level you know I think the real battleground going forward for deep learning it for AI in general as much as people talk about big data actually the most interesting battlegrounds that we see across many different industries all have to do with small data so how do we work with very small amounts of data you know it turns out if you look across all the businesses that make the world run heavy industries health care financial services most of the problems that those companies faced and that we face in the world in general don't have enormous annotated carefully curated data sets to go with them so if we're gonna be able to use AI broadly and tackle all of these you know hard problems that we want to solve we need to be able to learn how to do more with less data so part of that has to do with things like transfer learning learning to transfer from one domain to another so learn in one domain and then use that knowledge somewhere else but increasingly and this is what I'm going to tell you about today there's this notion of reasoning so how do we not only extract the structure of the data we're looking at the data domain we're interested in but then also be able to logically and fluidly reason about that data and then finally just to close it out just give you a little bit of a pitch about what the lab is and what we do there's also a piece about infrastructure that we think is really important so if you track energy usage from computing year-over-year by some estimates by the year 2040 if we keep increasing our energy usage due to computing will exceed the power budget of the planet Earth there won't be enough solar radiation from the Sun not enough stuff we can dig up out of the earth and burn to fuel our computing habit and AI isn't helping the planet is not helping so many models that we train we'll take the equivalent energy of running a whole city for several days just for one model and that's obviously not gonna not going to last for a long time so we also do work both at the algorithmic level some of which I'll tell you about today but also at the at the physics level to ask can we build different kinds of computers so this is a member of a diagram of a member steff device this is an analog computer which we think we can get power consumption and for deep learning workloads down by maybe a factor of 100 or even a thousand and we're also working in quantum computing IBM as you may know is one of the leaders in quantum we have some of the biggest quantum computers that are available today and we're asking how that all interacts with AI so you know when IBM when we set up through this challenge of how do we make AI broadly applicable to all the kinds of problems that we'd like to apply AI to just as a small plug for the lab since we're here we decided we didn't want to do it alone and we chose a partner and a particularly chose MIT and actually you know the idea being that this is one of like the the last you know last standing industrial research labs of the bell lab era IBM Research together with MIT which obviously needs no introduction because we're here right now and we're partnering around AI and just give you a little bit of historical context it actually turns out that IBM and MIT have been together since the beginning of AI literally since the term artificial intelligence was coined way back in 1956 so right when the very first computers were being developed Nathaniel Rochester who's the the gentleman right there who developed the IBM 701 which is one of the first practical computers got together with MIT professors like Emma like John McCarthy and dreamed up this future of AI and it's actually really fascinating I encourage you all to go and find the the proposal for this workshop because a lot of the language including neural network language is all here like bigger they got a lot of the words right they were just a little bit off on the timescale you know you know baby like seven decades off but but really interesting and you know the partnership here the idea here is that we're combining the long horizon time horizon that MIT brings to the creation of knowledge you know you know maybe a hundred year time horizons you know departments of everything from chemistry biology economics and physics together with IBM where we have a lot of those same departments because we're such a big research organization but to bring those together with industry problems to bring data to the table so we can do the kind of research we want to do and to bring the compute resources along as well so this is what the lab is and what we do were we were founded with a quarter billion dollar investment over ten years from from IBM and we have 50 projects currently more than 50 projects currently running over a hundred and 50 research across researchers across MIT and IBM and there are opportunities for undergraduates for graduate students to be involved in these projects so if you're interested in the things I show you today we'd love to have you join our team either on the MIT side or on the IBM side and we're basically drawing from all of the different departments of MIT and even though we've only been running for about a year and a half we have over a hundred publications and top academic conferences and journals we had 17 papers in Europe's just a few months ago just to give you a sense of that everything is up and running so you know this is this is the evolution this is where we're going so why why do I say that today's AI is narrow why would I say that because clearly AI is powerful you know in particular you know in 2015 Forbes said you know that deep learning machine intelligence would eat the world and you know I think it's safe to say that the progress you know since 2012 or so has been incredibly rapid so this was a paper that really for me as a researcher was working in computer vision really convinced me that something dramatic was happening so this was a paper from Andre Carpathia now leads Tesla's AI program together with Faye Faye Lee who created the image net data set and they built a system that you probably had study a little bit in this course where they can take an image and produce a beautiful natural language caption so it takes an input like this and it produces a caption like a man in a black shirt is playing a guitar or you taking this image can you get a construction worker in an orange safety vest he's working on on the road when I started studying computer vision and AI and machine learning I wasn't sure we were gonna actually achieve this even in my career or perhaps even in my lifetime like it was it was it seems like such science-fiction it's so commonplace now that we have systems that can do that so it's hard to overstate how important deep learning has been in the progress of AI and machine learning you know meanwhile there are very few games left that humans are better than machines app everything from you know jeopardy which IBM did way back in 2011 to go with alphago from deep mind group at Carnegie Mellon created a system that could beat the world champion in poker and recently my own company created a system called Project debater that can actually carry on a pretty credible natural language debate with a debate champion so if you like your computers to argue with you we can we can do that for you now and even domains like art which we would have thought maybe would have been privileged domains for humanity like surely machines can't create art right but you know that's not the case so even way back in 2015 which now feels like a long time ago Mattias Becca's group in at the moxa plunk in tubing and created a system of with style transfer where you could go from a photograph of your own and then re-render it in the style of any artist you like this is a very simple style transfer model that leveraged the internal representation of a convolutional neural network up to what we have today which is again just astonishing how fast progress is moving these this is a these are the outputs from a system called big game which came from deep mind and all four of these images are all of things that don't exist in the real world so this dog not a real dog you put it in a random vector into the game and into the begin and it generates whole cloth this this beautiful high-resolution dog or this bubble or this Cup and this is actually getting to be a problem now because now we have this notion of deep fakes we're getting so good creating fake images that now we we were having to come up with actual countermeasures and that's actually one thing we're working on in in the laboratory I run it at IBM where we're trying to find Gantt add oats you know antidotes countermeasures against Gans as we move forward so clearly the progress is impressive so why am I saying that AI is still narrow today well does anyone know what this is anyone have any ideas yeah good job but you're wrong it turns out it's a teddy bear so if you ask a state-of-the-art imagenet trained CNN and you'll often see these CNN's described as being superhuman in their accuracy has anyone heard that before like they'll say object recognition is a solved problem these image net trained CNN's can do better than humans if you've ever actually done looked at image that carefully the reason that's true is because image net required it has huge numbers of categories of dogs so you basically need to be a dog show judge to be able to outperform a human at image net but this is starting to you know illustrate a problem this image is real so this is a piece of art and in the Museum of Modern Art in New York by Meret Oppenheim called the DNA on for a luncheon in fur a little bit unsettling image but we not like who thought it was a teddy bear right like like the most untidy bear like image ever right why did why did the CNN think this was a teddy bear soft and fluffy it's round it's got fur what kinds of things in the training set would be round and furry teddy bears you know it's a little bit of a garbage in garbage out kind of scenario this is in many ways you know people talk about corner cases or edge cases those rare things that happen but are different from the distribution you've trained on previously and and this is a great example of such a such a thing so this is starting to show that even though deep learning systems we have today are amazing and they are amazing there's you know there's room for improvement this on missing here and actually if we dig a little bit deeper which you know a variety of researchers have done so this is from Alan eul's group at Johns Hopkins even in cases where you know the objects are the standard objects that the system knows how to detect its supposedly superhuman you know sort of levels if we take this guitar and we put it on top of this monkey in the jungle a couple of funny things happen one is it thinks the guitar is a bird they might have an idea why that is yeah I hear pieces of the answer all around so it's it's colorful right it's a color that you would expect a tropical bird things that are in the jungle in distribution would tend to be colorful tropical birds interestingly because you put the guitar in front of the monkey now the monkeys a person and you know again you know monkeys don't play guitars in the training set and that's that's clearly messing with the results so even though we have no trouble at all telling that this is that these objects are a guitar and a monkey this the state-of-the-art systems are falling down and then even this is this captioning example which I highlighted as being you know an amazing success for deep learning and it is an amazing success for deep learning when you poke a little bit harder which you know Josh Tenenbaum and Sam Gershman and Brendan like and Tamar Holman did you find things like this so this image is captioned as a man riding a motorcycle on the beach this next one is an airplane is parked on the tarmac at an airport and this one next one is a group of people standing on top of a beach which is correct so score one for the AI but what you can see is there's a strong sense in which the system doesn't really understand what it's looking at and that leads to mistakes and that leads to sort of you know you know missing the point you know in many cases and again this is this has to do with the fact that these systems are trained on the data and largely they're constrained by what data they've seen before and things that are out-of-sample these edge cases these corner cases tend to perform poorly now the success of deep learning you know I think it's safe to say it's you know two things happened you know deep learning as you already know is a rebrand of a technology called artificial neural networks it dates all the way back to that Dartmouth conference and at least to the 80s you know a lot of the fundamental math back prop was worked out in the 80s when we went through decades of time where it was disreputable to study neural networks and I lived through that era where you would try and publish a paper about neural networks and people would tell you that everyone knows that neural networks don't work but what happened was the amount of data that was available grew enormously so we digitalized the world we got digital cameras now we're all carrying I'm carrying like four cameras on me right now and we took a lot of images and then the compute caught up as well and particularly graphics process units graphics processing unit CPUs came available and it turned out they were even better for doing deep learning than they were for doing graphics and really the seminal moment that really flipped the switch and made deep only take off was the collection of this data set called image net which Bailey collected and it's basically millions of carefully curated images with categories associated with them now you need to have data sets of this scale to make deep learning work so if you're working on projects now and you're training neural networks you'll know that you need to have thousands to millions of images to be able to train a network and have it perform well that's in stark contrast to how we work so does anyone know what this object is just quick raise of your hands okay a few people not so many but even though you've never seen this object before a single training example you're now all experts in this object just one training example so I can show you to you and ask is that object present in this image I think we all agree yes I can ask you questions like how many are in this image and I think we'd all agree there are two and I can even ask you is it present in this image and I think you'd all agree yeah but it's weird right so so not only can you recognize the object from a single training example not thousands not millions one you can reason about it now in contexts where it's just weird right and that's why you can tell that it's a fur-covered sauce or cup and spoon and not a teddy bear could you have this ability to reason out a sample and that's really a remarkable ability that we'd love to have because when you get past imagery you have past digital images there are very few data sets that have this kind of scale that imagenet has but even image net turns out you know if there's something else wrong with it so does anyone notice anything about these chairs in the image these were all taken from image net from the chairs category does anyone notice anything you know unusual about these all facing the camera their own canonical views they're all more or less centered in the case where there's multiple chairs that kind of like almost like a texture of chairs right so these are very unusual images actually I mean we look at them and we think these are normal images of chairs but these are actually very carefully posed and crafted images and one of the projects that we've been working on together with MIT across the MIT IBM lab was this is a project that was led by Boris Katz and Andre barboo together with our own Dan Gouffran they asked okay well what if we collected a data set where that wasn't true so where we didn't have carefully perfectly centered objects and what they did is they enlisted a whole bunch of Mechanical Turk on Amazon Mechanical Turk and they told them take a hammer take it into your bedroom put it on your bed and here's a smartphone app and please put it in this bounding box so so basically you would have to go and these people get instructions you know take your chair we want you to put it in the living room and we want you to put it on its side and put in that bounding box or we want you to take a knife out of your kitchen put it in the bathroom and make it fit in that bounding box or you know take that that bottle and put it on a chair on this orientation so they went through and they just collected a huge amount of this data so it corrected 50 thousand of these images from 300 object classes that overlap with imagenet and then they asked the Mechanical Turk occurs to go to four different rooms with those things so remember everyone talks about how imagenet is state-of-the-art in object categorization and that's a solve problem but it turns out when you take these images of these objects that are not in the right place humans can perform at well over ninety five percent accuracy on this task but but the AIC cnn's that were previously performing you know at state-of-the-art levels drop all the way down forty to forty five percent down in their performance so there's a very real sense in which as amazing as deep loading is and i i'm gonna keep saying this deep learning is amazing but some of the gains in the you know the sort of declarations of victory are a little bit overstated and they all circle around this idea of small data of corner cases edge cases and being able to reason about situations that are a little bit out of the ordinary alright and of course the last piece you know that's that's concerning for anyone who's trying to deploy neural networks in the real world is that they're weirdly vulnerable to hacking so i don't know if you guys covered adversarial examples in this class yet but here's an example targeting that same captioning system so we're you know the captioning system can see this picture of a stop sign and produced this beautiful natural language caption a red stop sign sitting on the side of a road our own pin you chen who's an expert in this area at IBM created this image which is a very subtle perturbation of the original and you can get that to now say a brown teddy bear lying on top of the bed with high confidence so this is a case again where there's something divergent between how we perceive images and understand what the content of an image is and how these end and trained neural networks do the same and you know this kind of this you know this image the the perturbations of the the pixels in this image we're done in such a way that they'd be small so that you couldn't see them so they're specifically hidden from us but it turns out that you don't have to actually have access to the digital image you can also do real-world in the wild adversarial attacks and this is one that was it was kind of fun some some folks in my lab in my group decided to be fun to have a t-shirt that was adversarial so you took a person detector and so this is like you know it will detect a person you could imagine like an AI powered surveillance system if you were to intrude in the building you might wanna have a person detector that could detect a person and warn somebody hey there's a person in your building it doesn't belong but what they did is they created this shirt so this shirts very carefully crafted it's a very loud ugly shirt you could we have it in the lab if you want to come over anytime and try it on you're welcome to but this shirt basically makes you invisible to AI so this is CJ who's who's who's our wizard adversarial examples he's not wearing the shirt so you can see the person detectors detecting him just fine Tron foo is wearing the shirt therefore he is invisible he is camouflaged and you can see even as you walk around even if the shirt is folded and bent and wrinkled it makes you invisible so weird right like that like if anything for us this ugly looking shirt makes you even more visible so there's something just something weird about how deep learning seems to work relative to how weird we work but there are also problems where even under the best of conditions no adversarial perturbation you could have as much training day as you like where deep learning still struggles and these are really interesting for us because these are cases where no matter how much data you have deep learning just doesn't cut it for some reason and and we want to know why so problems like this if you ask the question so I give you a picture and ask question how many blocks are on the right of the three-level tower or well the block tower fall if the top block is removed or are there more animals than trees or what is the shape of the object closest to the large cylinder these are all questions that even a child could answer I mean provided they understand language and you know and read and stuff you know it's very easy for us to work on these things but it turns out the deep loading systems irrespective of how much training data you give struggle so that's you know case where they know there's smoke and you kind of want to know where's the fire actually the answer we think or one of the things we're exploring is the idea that maybe the answer lies all the way back in 1956 so this is a picture of that Dartmouth workshop back in 1956 and back in this time period neural networks were already you know had already been sort of born we were thinking about neural networks but we were also thinking about another kind of AI back then and that kind of AI is is interesting a different hasn't really enjoyed a resurgence the way that neural networks have but just to step back for a moment this is what you've been studying neural network basically is a nonlinear function approximator you take an input in and you get some output that you want out and it learns the weights of the network through training with data so this is what an apple looks like to a neural network you know you put an apple picture in and you light up a you know a unit that says there's probably an apple in that scene there's another kind of AI that's been around since the beginning called symbolic AI and this is from a book by Marvin Minsky and in 1991 was created here and this is what an apple looks like to symbolic AI so we know things about an apple we know that an apple has an origin it comes from an apple tree we know that an apple is a kind of fruit you know the apple has a PEZ parts it has a body and has a stem the body has a shape it's round it's has a size it can fit in your hand it's got a color could be red or green we know lots of knowledge about what an apple is and that's a very different take on AI and you know basically this field of what we call good old-fashioned AI or symbolic AI has been around since the very beginning and it just hasn't yet enjoyed that resurgence that neural networks did and one of the central theses that we're exploring is that just the same way that neural networks have been waiting they were waiting for compute and data to come along to make them really work we think that symbolic eyes also been waiting but what it's been waiting for his neural networks so now that neural networks work can we go back to some of these ideas from symbolic AI and do something something different and the work I'm going to tell you about is a collaboration as part of the MIT IBM lab chuan GaN is one of the researches my group together with Josh Tenenbaum and particularly John Woo who's now an assistant professor at Stanford along with some others and what they're asking is can we can we mix together the ideas of neural networks together with the ideas from symbolic AI and do something that's more than the sum of its parts and this picture that I showed you here I should do this earlier this is actually a data set called clever that was it was basically created to illustrate this problem where irrespective of how much training data you have this very simple kind of question answering tasks where you have to answer questions like are there an equal number of large things in metal spheres seems to be hard so the data set was created to illustrate the problem and if you tackle this the way you're supposed to with neural networks and deep learning and and perhaps you've learned this you know over the course of this IEP course that the best way to train a system is end-to-end right there the best way to get what you want is to start from what you have and end with what you need and don't get in the way in the middle just just let the neural network do its thing the problem is that when you build end-to-end neural networks and try and train them to go from from these inputs to these outputs for data sets like this it just doesn't work well at all and the reason for that is that the concepts things like colors and shapes and objects and things like that and then the portions of reasoning like counting the number of objects or reasoning about the relationships between objects are fundamentally entangled inside the representation of the neural network and then not only does that cause problems where it's very difficult to cover the entire distribution and they not get caught by corner cases but it also means it's hard to transfer to other kinds of tasks like image captioning or instance retrieval or other kinds of things so fundamentally this end end approach just doesn't seem to work very well so I'm already telling you something that's sort of probably against the advice you've gotten that's far but when you step back and look at well how do we solve this problem of visual reasoning you'd have a question like this are there an equal number of large things in metal spheres you know when we tackle this problem well first we read the question and we see there's something about large things we use our visual system to sort of find the large things then we read the question we see that something about that phears we use our visual system to find the metal spheres and then critically we do an operation a symbolic operation and a quality operation where we decide are these an equal number and we say yes so that's what we do and if you unpack that yeah it's got visual perception and CNN's are a great candidate for doing that and yeah it's got a question understanding natural language processing and yeah or intends are a great tool for doing that but critically it also has this component of logical reasoning where you can very flexibly apply operations in a compositional way so what the team did then was to basically this is kind of like the the neuro symbolic hello world hello world is the first program you write in most programming languages this is kind of the simplest example that we could tackle and this is the sort of diagram of the flow system and don't worry I'm gonna unpack all this where you know because you know neural networks are good at vision we we use a CNN to do the vision part but instead of going straight to an answer it's used to basically D render the scene so rendering goes from a symbolic representation to an image D rendering goes from the image back to some kind of symbolic structured representation so we're gonna take apart the image using the neural network and then the question you know you'd be crazy not to use something like an LS TM to parse the language but instead of going from the language straight to an answer or going from the language to you know a label or something but language is gonna be parsed into a program into a symbolic program which is then going to be executed on the structured representation so just to walk you through that you know we have a vision part we have a language part you parse the scene using a CNN so you turn you know you find the objects in the scene and you basically create a table that says what are the objects what are their properties and where are they and then you do semantic parsing on the language part and again the goal here is to go from natural language with all of its you know sort of vagaries and messiness to a program a program that we're going to run in a minute and so you need to learn how to take this language and turn it into a series of symbolic operations and then you're going to run at symbolic program on the structured symbolic information and get an answer so you would start by filtering and saying I want to look the questions asking about something Reds I need to filter on red and then I need to query the shape of that object that I've just filtered so this is basic you know sort of program program execution and critically the system is trained jointly with reinforcement learning so the neural network that does vision and the neural network that translates from language to program fundamentally learned something different by virtue of being part of a hybrid symbolic system right so it gets reward or doesn't get reward and you you've probably a great and small that based on whether or not the symbolic system got the right answer and we use reinforcement learning of course because you can't differentiate through the symbolic part but but you know fundamentally this isn't just a matter of bolting neural networks onto a symbolic reasoner but rather training them jointly so that the symbolic so the neural networks learn how to extract the symbols learn to give the right symbolic representations through experience and learning on the data so this does a couple of really interesting things so one of the first things you'll notice this data set was created this clever days that was created because it Illustrated a problem with end and learning but it turns out that with just a dash of symbolic execution now you can be effectively perfect on the clever data set so clever is now solved and this was actually an oral spotlight paper at NURBS because this is a big deal previously unsolved with a problem now solved that's good but interestingly more than that remember I said the biggest problem with deploying neural networks in the real world is that we rarely have big data we usually have pretty small data so anything that reduces the the sample that it improves the sample efficiency of these methods is really valuable and so here is the number of training examples that the system is given this is the accuracy of the system the neuro symbolic system is up here in blue I'll just point out several things one is it's always better and but if you look at you know down here the end-to-end train systems require close to a million examples they are kind of you know okay results not perfect but okay the neuro symbolic system again with just a dash of symbolic mixed in with just one percent of the data can do better than most of the end-to-end train systems and with just ten percent of the data one tenth of the data can perform at effectively perfect performance so drastically lower you know requirement for data drastically higher sample efficiency and then the last piece remember I said explain ability was super important now people are actually don't really use AI systems they need to be able to look inside and understand why the decision is made otherwise they won't trust the AI system they won't use it because the system has a symbolic choke point in the middle where you parse the question into a series of symbolic operations we can debug the system the same way you would debug a traditional coded system so you can see okay well what did it do it filtered on cyan it filtered on metal was that the right thing you can just you can understand why it made the decision it made if it made the wrong decision now you have some guidance on what you'd want to do next so that was a paper from from this team in 2018 neuro symbolic vqa actually since then there's basically been a parade of papers that have made this more and more sophisticated so there was a paper and I clear in 2019 called the neuro symbolic concept learner that relaxed the requirement that that the concepts be pre coded another paper that just came out in NURBS just a few months ago or last month called the neuro symbolic Matt a concept learner that autonomously learns new concepts it can can sort of use meta concepts to do better and we're even now getting this to work in not just these toy images but also in real world images which is obviously important and we think this is actually really interesting and profitable direction to go forward so here's a neural symbolic concept learning basically what's happened is relaxing now these concepts into concept embeddings so you can when you look at an object with the CNN you can now embed into a space of color and then compare that color to store concept embeddings which means you can now learn new concepts dynamically you can learn them from concept from context so you don't need to know that Green is a color you can figure that out and learn that this is important because the world's full of new concepts that were constantly you know encountering so that was the sort of next innovation on the system also remember I said that one of the things that's magical about symbology is that we can leverage lots of different kinds of knowledge and in fact we can leverage these sort of meta relationships between different different concepts we know there are synonyms for instance which led to this paper which was just presented last month called the neuro symbolic meta concept learner where you can have a notion of is read the same kind of concept as green or as Cuba synonym of block which then of course lets you do things like you know if if I go through in the regular mode here I'm creating you know a representation of the object and I'm creating a symbolic program I can do the regular thing but then also critically I can now use relationships I know about synonyms and and concepts equivalencies to meta verify these things and then I can take advantage of the fact that if I know that there's an airplane that I also know there's a plane because a plane and an airplane are synonyms I can know that if there's any kind of kid the answer is yes that is there any kind of child you know that's also yes because child and kid are synonyms so we can start to see how we can get more and more complex and more and more sophisticated with our symbolic reasoning and and do more and more and more of course it works well we're also now extending since clever is now beaten we're now looking also at we're releasing a new data set called video clever is video it's called clever or which is a very tortured acronym looking at the relationships between objects and counterfactuals what would happen if this block weren't there so you can see that we can kind of expand to more and more sophisticated environments as we go I'll just also say you know that that this this notion of symbolic program execution isn't the only idea from Somali ki that we can bring together with neural networks we're also looking at the field of planning so there's a field of symbolic AI called planning where you try and start from an initial state and then use an action plan to arrive at some target state which is really good for solving problems like the Tower of Hanoi which you may have encountered or these kinds of slider puzzles where you need to produce a series of operations to achieve a certain end state like make the picture into the right shape and another area of projects that we're working on is mixing these together with neural networks so that we don't just have to rely on on sort of static symbolic representations but we can actually work in the latent space of an autoencoder so we have binary discrete autoencoders and we can actually plan in the latent space of an autoencoder so obviously these are topics that would you know be a whole talk unto themselves I just wanna give you a little bit of a flavor that this idea of mashing up neural networks and symbolic AI has a lot of a lot of range and there's a lot of a lot of a room to grow and explore and lots of ideas in symbolic AI now that we can bring together and every time we do we seem to find that good things happen so with that I'll stop just to give you one picture in your mind you know these sort of to venerable traditions of AI I think we're coming to a place where we can bring the symbolic stuff you know off out of the you know out of the closet dusted off and in many ways the power of neural networks solves many of the problems they complement each other's strengths and weaknesses and really important and useful ways so with that I'll stop and thank you all for your attention and if you have any questions I'm very happy [Applause] 

you I wanted to start with this cute little video 
what I really study is algorithmic methods to   make robot manipulation generalizable and and 
why I really like this video is this is how I   got inspired to work in robotics this is science 
fiction and as a researcher you are always chasing   science fiction and in trying to make some of 
it reality and and really to think about this   if you think of something like this if I were to 
have a system like this in my home I would want   it to do a variety of things maybe clean cook do 
laundry perhaps help me sweep or other stuff and   not only that I would probably want it to work 
outside of my lap I would want it to work in   a variety of settings maybe my home your home or 
maybe in settings which are much more complex then   you can show it perhaps in the door and and the 
idea really is how do we enable such complexity   and learning in such sort of general purpose 
diversity of skills which interaction in the   real world requires and this is where I argue that 
a lot of my research agenda lies we are trying to   build these systems physical agents particularly 
that can really extend our ability and when I use   extending augment it's both in cognitive and 
physical sense but there's nothing new about   that dr. Cox already talked about the the talk 
of emergence of AI that happened in 1956 and   soon after we were dreaming of robot assistants 
in the kitchen this is the first industrial robot   I don't know how many of you are aware of 
this robot anybody in the room know the name interesting the robot is called unimate 
it is as big as the kitchen but this is   this is actually 50 years to date in the 
time since I would argue people have done   variety of tremendous stuff this is not my 
work this is boss robotics but this is one   particularly good example robot can walk 
on ice lift heavy boxes and whatnot cut to   this year in CES this is a video from Sony 
doing pretty much the same thing this is   a concept video of a robot cooking circa 
20 2015 2 years after the original video what what is what is very striking is despite 
the last 50 years when you put real robots in   real world it doesn't really work work it turns 
out doing long term planning real time perception   in real robots is really hard so what gives 
what gives from from 1950 1960 to today I argue   that we need algorithms that can generalize to 
unstructured settings that a robot is going to   encounter both in terms of perception in terms 
of dynamics perhaps task definition and I will   only given you examples of kitchen but there's 
nothing particularly about special about kitchen   it this sort of lack of generalization happens in 
all sorts of robots application from manufacturing   to healthcare to person and service robotics 
so this is where I argue that to tackle this   problem what we really need is to inject some 
sort of structured inductive bias and priors   to achieve the generalization in simpler terms 
you can really think about this that we need   algorithms to learn from specifications of tasks 
where specifications can be easy language video or   let's say kinesthetic demonstrations and then 
we need mechanisms where the system can self   practice to generalize to new but similar scenes 
but often imitation gets a bad rep imitation   is just copying but it actually is not let's 
let's think about this very simple example of   of scooping some leaves in your yard if you have 
a two-year-old looking at you trying to imitate   they may just move them up around they're trying 
to just basically try to get the motion right but   nothing really happens so they get what you'd call 
movement skills as they grow up probably they can   do a bit better they they can get some sort of 
planning they can do some sort of generalization   some of the tasks actually works and they even 
grow to a point where now they understand the   concept of limitation is really not the motion 
but it is actually semantics you need to do the   task not an exact not exactly always the how of 
it the what matters so they may actually use a   completely different set of tools to do the exact 
same task and this is precisely what we want in   algorithmic let's say equivalence so today what 
I'm going to talk about is at all three levels   of these let's say imitation at level of control 
planning and perception how can we get this kind   of generalization through structured priors and 
inductive biases so let's start with control so   I started with these simple skills in the house 
let's take one of these skills and think about   what we can do with this what we are really 
after is algorithms which would be general so I   don't have to code up new algorithms for let's say 
sweeping versus cleaning or or create a completely   new set up for I don't know let's say cutting so 
let's think about that this is where precisely   learning based algorithms come into play but one 
of the things that is very important is let's   say we take the example of cleaning cleaning is 
something that is very very common in everyday   household you can argue wiping is a motion that 
is required across the cross board not very many   people clean radios though but still the concept 
is generalization perhaps cleaning a harder stain   would require you to push hard you have some 
sort of reward function where you wipe until it   is clean it's just that the classifier of what 
is clean is not really given to you explicitly   or maybe you know the concept or context where if 
you're wiping glass do not push too hard you might   describe it how do we build the algorithms that 
can get this sort of generalization one way can   be this recent wave of what you would call machine 
learning and reinforcement honey you get magical   input of images some sort of torque or output 
in actions and this is actually done very well   in robotics we have seen some very interesting 
results for long-standing problems being able to   open closed doors which is handling some sort of 
fluids or at least deformable media but and this   is actually surprising and very impressive but one 
of the things that strikes out is these methods   are actually very very sample inefficient it may 
take days if not weeks to do very simple things   with specification and even then these methods 
would be very let's say iffy very unstable you   change one thing and then the whole thing comes 
shattering down you have to start all over again   now the alternate to do this might be something 
that is more classical let's say look at controls   you are given a robot model the robot model maybe 
specification of the dynamics it may include the   environment if the task is too complicated given 
something like this what would you do you would   come up with some sort of task structure T I 
need to I need to do this particular step maybe   go to the table do wiping and when it is wipe 
then have a comma so this this has worked for   a long time actually when you have particular 
tasks but there is a problem generalization is   very hard because I need to create this tree for 
every task to perception is very hard because I   have to build a perception for particular task 
wiping this table may be very different from   wiping the whiteboard because I need to build a 
classifier to detect when it is white so one of   the one of the algorithms we started working on is 
like can we take advantages of or the best of both   worlds argument in this case so the idea is the 
enforcement learning or learning in general can   allow you to be general-purpose but it's very 
sample inefficient on the contrary model-based   methods allow you to rely on priors things that 
you know about the robot but require you to code   up a lot of the stuff about the task so what we 
thought is maybe the way to look at this problem   is break up this problem in modular settings where 
you take the action space of the learning do not   be in the robot space but in the task base itself 
what do we really want to do is think about a   modular method where the output of a policy learn 
the policy that is taking in images is actually   not at the level of what what joint angles do 
you want to change but really think about the   pause and the velocity of the end effector but 
also the gains or the impedances how hard or   stiff dude because the end effector needs to 
be at this why does why is it important it   is important because this enables you to manage 
different stiffnesses when you are in different   stages of the task this basically obviates the 
need for you to create a task structure tree   so the system can learn when it's free when it 
needs to be stiff and in what dimensions it needs   to be stiff and this is very important now the 
policy is essentially outputting this stiffness   parameters which can then be put into a robot 
model that we already know the robot model can   be nonlinear rather complicated why bother wasting 
time spending learning effort to do this and this   is the only sort of best of both worlds where you 
are using the model however much you you can but   you are still keeping the environment which is 
general to be to be unmodeled what benefit does   this sort of give you so what we do here what 
you see here is image input to the agent and   this is environment behavior we model this as 
a reinforcement learning problem with a fairly   simple objective the objectives up top basically 
clean up all of the tiles do not apply any forces   that would kill the robot that is basically and 
what you really need to see is we tested this   against a bunch of different actions basis so 
actions faces the prior here that you're using   and the only thing that you should sort of take 
away is at the bottom is the favorite image to   talk and at the top is basically impedance 
controlled or variable impedance directly   provided through the policy and this is basically 
the difference between failure and success in both   terms of sample efficiency and all the smoothness 
and control because you're using known mechanisms   of doing control at high frequency where you can 
actually safeguard the system without wetting   about what the important learning algorithm 
can do to your system interestingly what you   can do with this is because you have a decoupled 
system now you can train purely in simulation F   theme is can be a model in the simulation you can 
replace this model with a real robot model on the   fly you do not need fine tuning because again 
the policy is outputting end-effector spaces in   end effector spaces so you can replace this model 
and even though there might be finite differences   in parameters at least in these cases we found 
that the policy generalizes very well without   any sort of let's say computational loss we did 
not have to do simulation based randomization   we did not have to do techniques on either 
fine-tuning the policy when you do go to the   real world so this is kind of zero short transfer 
in this case which is pretty interesting basically   identifying the right prior enables you to do this 
generalization very efficiently and also gets you   sample efficiency learning so moving on let's 
let's talk about reinforcement learning again   we always want reinforcement learning to do any 
sort of interesting tasks which can do let's say   image to control kind of tasks this is yet again 
an example from Google but but often when you want   to do very complicated things it is it can be 
frustrating because the amount data required on   realistic systems can actually be very big so when 
tasks get slightly harder the enforcement learning   starts to what you would call stutter so you you 
want to do longer time longer term things which   it can be very hard and interestingly something 
that came out last year this is not my work but   friends from a company actually showed that even 
though reinforcement learning is doing fancy stuff   you can code that up in about 20 minutes and you 
still do better so what was interesting is that in   these cases at least from a practical perspective 
you could code up a simple solution much faster   than a learn solution which basically gave us 
the idea what is going on here what we really   want to do is exploration in reinforcement 
learning or in these sort of learn systems   is very slow often in these cases you already 
know part of the solution you know a lot about   the problem as as a designer of the system the 
system isn't actually working ab initio anyways   so the question we were asking is how can human 
intuition guide exploration how how can we do   simple stuff which can help learning faster so the 
intuition here was let's say you are given a task   the task requires you some sort of reasoning it is 
basically move the block to a particular point if   you can reach the block you will move the block 
directly if you cannot reach the block then you   use the tool so we thought that what we can do 
easily is instead of writing a policy it is very   easy to write sub parts of the policy basically 
specify what you know but don't specify the full   solution so provide whatever you can but you don't 
actually have to provide the full solution so this   basically results in you get a bunch of teachers 
which are essentially blackbox controllers most   of them are some suboptimal in fact they may be 
incomplete so in this case you can basically say   that I can provide a teacher which only goes to a 
particular point it doesn't know how to solve the   task there is no notion or concept of how to 
complete the task so you can start with these   teachers and then the idea would be you want to 
complete you want you still want a full policy   that is both faster than the teachers in terms 
of learning and a test time doesn't necessarily   use the teachers because teacher may have access 
to privileged information that a policy may not   have but the idea even though it's simple the 
specify is actually non-trivial think about this   if you have teachers multiple of them some of the 
teachers can actually be partial you might need to   sequence them together maybe they are partial 
and they are not even complete in the sense   that there is no single sequence that even will 
complete the task because there is no requirement   we put in on on these teachers sometimes the 
teachers may actually be contradictory you   did not say that they are all helpful they can 
be adversarial independently they are useful   because they provide information but when you 
try to put them together let's say go back and   and move forward and you can keep them you keep 
using them without making progress in the task so how do we do this so let's review some 
basics in reinforcement learning I believe   you went through a lecture in reinforcement 
learning this is an off policy reinforcement   learning algorithm called DD PG what DD PG does 
is you start with some sort of state that an   environment of rates you run a policy a policy is 
let's say your current iterate of your system when   you operate with the policy the policy gives you 
the next state the reward and you put that pupil   in a database that is called experience replay 
buffer this is a standard trick in modern deep   reinforcement learning algorithms now what you 
do is you sample mini batches in the same way   you would do let's say any sort of deep learning 
algorithm from this database that is constantly   updating to compute two gradients one gradient 
is the value of what you call a function critic   which is value function which is basically 
telling what is the value of this state the   value of this state can be thought of as 
how far would the goal be from my current   state and then you use the policy gradient in this 
particular case something called deterministic pol   policy gradient that is what the name is deep 
deterministic policy gradient so the you have   these two gradients one to update the critic and 
one to update the policy and you can do them in   a synchronous manner offline offline in the sense 
that the data is not being generated by the same   policy so you can have these two update process 
and the and the rollout process separated that   is why it's called off policy so now let's assume 
you have some RL agent whether it's DDP G or not   doesn't really matter you have an environment 
you get in state and you can run the policy but   then now the problem is not only the policy you 
actually have a bunch of other teachers which   are giving you advice so they can all basically 
tell you what to do now you have to not only   decide how the agent should behave you also need 
to figure out if I should trust the teacher or   not how do I do this one way to do this is think 
about how Bandit algorithms work I can basically   at any point of time think about the value of 
any of these teachers in a particular state I   can basically think of an outer loop running RL 
in the outer loop or well of the policy learning   and I basically state if the problem that I was 
solving was selecting which agent to use or which   one of these teachers to use then I just need 
to know which will result in the best outcome   in the current state this formalism can basically 
be stated you learn a critic or a value function   where it is basically choosing which which of the 
actions you should you should pick simultaneously   as you run this thing then the policy which is 
called the behavioral policy basically runs that   agent whether it's your own agent the learned 
agent or one of the teachers but then now the   trick is regardless of who operates whether it's 
the teacher or your agent the data goes back to   the replay buffer and then my agent can still 
learn from that data so basically I am online   running some teachers so using supervisors and 
using the data to to train my agent so whenever   a supervisor is useful the agent will learn 
from it if the supervisor is not used then   standard reinforcement learning will happen 
so what does this result in so we go back to   the same task we provide for teachers they look 
something like grab position push pull but we do   not provide any sort of mechanism to complete 
the task so the first question we were asking   is if we give one full teacher the method should 
basically be able to copy the teacher and that's   what we refined that in terms of baseline we are 
basically able to do something that is Porsche   at least copy one teacher when the teacher is 
near optimum a more interesting thing happens   when you actually get multiple teachers so 
when you get multiple teachers the problem   gets a bit more complicated because you have 
to decide which one to use in and if you use   a suboptimal one you waste time this is where 
we basically see that using our method results   in sample efficiency even more interestingly 
what happens is if you provide teachers which   are let's say incomplete I only provide teachers 
for part of the task and the other part needs to   be filled in this is where all of the other 
methods kind of fail but the fact that you   are essentially using reinforcement learning with 
imitation you still maintain the sample efficient so just taking taking a breather here what 
we really learned from this line of work   is understanding domain specific action 
representations even though even though   they're domain-specific they're fairly general 
manipulation is fairly general in that sense and   using weekly supervised systems so in this case as 
we are using weekly supervised teacher suboptimal   teachers provides enough structure to promote both 
sample efficiency in learning and generalization   to variations of distance so let's go back to 
those notes setup we started with low-level   skills let's graduate to a bit more complicated 
skills so we started with simple skills like   grasping pushing what happens when you need to do 
sequential skills things that you need to reason   for a bit longer so we started by studying this 
problem which is fairly sort of interesting let's   say you have a task to do maybe it's sweeping 
or hammering and you're given an object but the   identity of the object is not given to you you 
basically have given a random object whether it's   a pen or a bottle how would you go about this 
one way to go about this is look at the task   look at the object predict some sort of optimal 
grasp for this object and then try to predict   the optimal task policy but it I I can argue that 
optimally grasping the hammer near the center of   mass is suboptimal for both of these tasks what is 
more interesting is you actually have to grab the   hammer only in a manner that you'll still succeed 
but not optimally because what the goal standard   that you're really after is the task success 
not the grasping success nobody grabs stuff for   the purpose of grabbing them so how do we go about 
this problem you have some sort of input some sort   of task and the problem is we need to understand 
and evaluate there there can be many ways to grasp   an object and we still need to optimize for the 
policy so there it is a very large discrete safe   space where you are basically grafting in objects 
in different ways each of those ways will result   in a policy some of those policies will succeed 
some of those will not but the intuition or   at least the realization that that enables the 
problem to be computationally tractable is the   fact that whenever the task succeeds grasp must 
succeed but the other way round is not actually   true you can grab the object and still fail at the 
task but you can never succeed at the task without   grabbing the object this enables us to factorize 
this value function into two parts which is a   condition grass or grass model and a grass model 
itself just an independent gas model and this sort   of factorization enables us to create a model with 
three last terms one is are you able to grab the   object whenever you grab the object does the task 
succeed and a standard policy grading locks this   model then can be jointly trained in simulation 
where you have a lot of these simulated objects   in a simulator trying to do the tasks where the 
reward function is parse you're basically only   told do you succeed in the task or not there is no 
other reward function going on so a test time what   you get is you get some object a real object that 
is not in the test set you get the RGB D image of   that what you do is you generate a lot of these 
grasps samples the interesting part is what you're   doing here is you're ranking grasps based on the 
task so this ranking is generated by your belief   of task success and then given this ranking you 
can pick a grasp and evaluate the task so you can   actually go out and do the task and this is what 
the errors from here is back dropped in to picking   this ranking the way this problem is set up you 
can generalize to arbitrary new objects because   nothing about object category is given to you so 
in this particular case you are seeing simulation   for the hammering task and for pushing task and 
we evaluated this against a couple of base lines   basically a very simple graph in base line then 
you have this sort of two-stage pipeline where   you optimally graph the object and then optimally 
try to do the task and then our method where you   are jointly optimizing the system what we find 
is in this case end-to-end optimization basically   gets you more than double the performance there's 
nothing special about simulation because we are   using depth images we can directly go to real 
world without without any fine-tuning because   input is depth so in this case it is basically 
doing the same task but in real world and pretty   much the trend for the performance still holds 
so we are much more than double the performance   then let's say a two-stage biplane where you're 
optimally grasping the object so moving forward   in the same setup we wanted to now ask the 
question can we do more interesting sequential   tasks which require reasoning as dr. Clarke was 
mentioning earlier that can we do something that   is requiring you to do but discrete and continuous 
planning simultaneously so think of these kind of   spaces where the task is to roll the pin to 
you but if you keep rolling the pin will roll   off the table hence you need a support and you 
may have objects that are blocking the task and   there can be variants of this service set up 
what it requires you to do is basically both   discrete and continuous reasoning the discrete 
reason is which object to push and in the scene   and continuous reasoning is how much to push 
it by or what is the actual sort of mechanism   of control so basically the kind of question 
we are asking is can a robot efficiently learn   to perform these sort of multi-step tasks under 
various both physical and semantic constraints   these are usually kind of things that people use 
to let's say test animal intelligence behaviors   so we attempted to study this question in a 
manipulation setting in a simple manipulation   setting where the robot is asked to move a 
particular object to a particular position   the interesting thing is that there is constraints 
so in this particular setup the constraint can be   the object can only move along a given path in 
this case along let's say gray tiles and there   can be other objects along the way so in this 
particular case in presence an obstacle multiple   decisions need to be made you cannot just push 
the can to the yellow square you actually need   to push this object out of the way first and then 
you can do a greedy decision making so you have   to think about this at different levels of time 
scale so now doing something like this you would   argue can be done with a model-based approach 
you can learn a model of the dynamics in the   system you can roll the model out and use this use 
this model to come up with some sort of optimal   action sequence to do this and one would argue 
that in in recent times we have seen the number   of these papers well such model can be learned 
in pure image spaces so you are basically doing   some sort of pushing in pure image space then the 
question we were asking is since this is such a   general solution basically its visuals are going 
it seems natural that that these sort of models   will do it and and we're really surprised that 
that even though the solution is fairly general   and there's nothing new about these papers from 
the perspective of the solution it's basically   learning a model and then do optimally control 
these particular classes of models do not scale   to more complicated setups so you cannot ask these 
complicated questions of doing hybrid reasoning   with these simple geometric models the reason is 
to be able to learn a complicated model that can   do long term planning or long term prediction 
the amount of data that you would need skills   super linearly so to be able to do this something 
like this would require many many robots and many   many months of data even then we do not know if 
it'll work on the contrary what insight we had is   there is hierarchical nature to this action space 
basically there is some sort of long-term symbolic   effects rather than the actual space of the US and 
then there is a local motion if you can learn both   of these things simultaneously then perhaps you 
can generalize to do an action sequence that can   that can achieve this reasoning task so what we we 
propose is basically a latent variable model where   you are learning both long-term effects as what 
you would call the effect curve and local motions   so what this does is essentially you can think of 
the long term planner doesn't really tell you how   to get to the airport but it only gets what would 
be the milestones when you get to the airport when   you do that then the local local planner can tell 
you how to get to each of these milestones as you   go along so think of it like this that you can 
sample a meta dynamics model which generates these   multiple trajectories multiple ways to get to the 
airport you select one of those depending on your   cost function given the sequence of subtasks now 
you can actually generate actions in a manner that   would give you a distribution of those actions 
for for going forward let's say from milestone   to milestone and you would check it against a 
learnt low-level dynamics model the value any   of that action sequence so you are basically 
saying that the action sequence generated by   module is that going to be valid based on the 
data that I've seen so far and then you can wait   these action sequences based on cost functions so 
essentially what you are doing is you're trying to   train a model of dynamics for multiple levels but 
you're training all of this purely in simulation   without any task label so you are not actually 
trying to go to the airport only you are basically   just pushing around the other thing is you do not 
actually get labels for let's say milestones which   is equal into saying you don't get labels for 
latent variables so motion codes and effect codes   are essentially related so you set this up as a 
variational inference problem so you see these   modules at the bottom these are these are used 
to infer latent codes without explicitly boots so   the set up overall looks something like this you 
have a robot the robot input image is parsed into   objects represented object centric representations 
then this representation is passed into the   planner the planner outputs a sequence of states 
that can basically be now fed into the system and   you can basically look through it I gave you this 
example of a simple task of moving the object but   we did other tasks as well where you are basically 
trying to move the object a particular goal in a   field of obstacles or trying to clear a space with 
all of the multiple objects needs to be pushed   out so what we found is comparing this map model 
with a bunch of baselines which were using simpler   models that having a more complicated model works 
better especially when you have dense reward   function but when you have sparse about functions 
which is basically saying oh you only get reward   when the task completes no intermediate reward 
then the performance gap is bigger and again   the way this is set up you can go to a real world 
system without any fine-tuning pretty much get the   same performance okay the the trick is input is 
depth images okay so just to give you qualitative   examples of how the system works in this case the 
jello box needs to go to the yellow yellow bin and   there are multiple obstacles along the way and the 
system is basically doing discrete and continuous   planning simultaneously without doing any sort 
of modeling for us to do sort of discrete and   continuous models or-or-or specifically designed 
or task specific design models so the interesting   thing is there is only single model that we 
learned for all of these tasks it is not separate   yet another example is the system basically 
takes a longer path rather than pushing through   the greedy greedy path to get this bag of chips 
in this particular case the system figures out   that the object needs to create a path by pushing 
some other object along the way or out of the way   so in both of these projects what we learned is 
the power of self supervision in in robotics is   very strong you can actually do compositional 
fires with latent variable models using purely   self supervision both the both the task where we 
are doing hammering and in this case we basically   had models which were doing pure self supervised 
data in simulated setups and we were able to get   real-world performance out of these so moving on 
the next thing I wanted to study was what happens   when tasks grow a bit more complex so we live that 
simple let's say to stage tasks what happens when   you are graph structured that's when you actually 
have to reason about longer tasks let's say towers   of Hanoi problem so we talked about these problems 
clearly RL would be much harder to do imitation   even even imitation in these cases starts to fail 
because specification of imitation let's say to do   a very long multistage task whether it's building 
Legos or block worlds is actually very hard what   we nearly want is meta mutation journey so meta 
mutation learning can really be thought of as you   have our environment the environment is bounded 
but it can be in many final states which can be   thought of as a task so you get many examples 
of reconfigurations of that environment this   can be thought of as examples of tasks that you 
are seeing in train distribution in a test time   you are given a specification of one final task 
that can be new most likely and you still need   to be able to do this how do we do these kind 
of tasks in the current solutions actually let   me skip this so the way we do this right now is 
write programs these programs essentially enable   you to reason about long term tasks even at a 
very granular scale this is how you would code   up a robot to put two blocks on top of each other 
now if you were to do this slightly differently   you need to write a new program so this basically 
gave us an idea that perhaps instead of doing the   enforcement learning we can pose this problem as 
a program induction or noodle program induction   it's essentially reducing an enforcement learning 
problem or decision making problem to a supervised   learning problem yet in very large phase so you 
get an input video a meta learning model which is   basically taking current state and outputting what 
is the next program you should output not only the   next program but of course also the arguments that 
you need to pass using that API it's essentially   equivalent to saying that if you give the robot 
an API can the system use that API itself so   what you need is a data set of demos video 
demonstrations and let's say a planner that is   giving you what what sub programs were called in 
that execution and the loss basically looks very   much like a surprise learning loss where you have 
a prediction and you compare it with your planet   Earth what does this look like okay should not be 
like this okay so you can really think of this as   a high-level neuro symbolic manner well you at 
the start out put something like block stacking   no let's see this should be better you start with 
block stacking the block stacking unpacked to pick   in place pick in place can unpack to pick and 
once you are unpacked to pick you can basically   say the robot will actually execute the API level 
command as the API level command execute the the   executor goes back to pick moves forward with the 
pick and and then actually does the pick itself   once the pick is complete the pick in place moves 
forward to the place aspect of it and then goes   on to pick up the object by grabbing the object 
and picking it up in in sequence and once place   is complete the executor basically goes back 
to pick in place to block stacking and you can   continue doing this so this is just an example of 
one pick in place but this can actually continue   to multiple blocks we tested with over 40 of 
these examples sorry the videos didn't play as   well so what does this enable what this enables 
is you can now input the specification of the   task through let's say doing VR execution what you 
see in the inset and then the robot can actually   look at the video to try to do this task what is 
important is to understand what is happening in   this sequence of block executions the system is 
not just parsing the video because that would be   easy the system is actually creating a policy out 
of out of this sequence so one way to test this   is let's say if there is an adversarial human 
in the system that will break the model so if   you have done the task half way through the world 
is stochastic the world goes back it should not   continue doing the same thing that you saw it 
should actually be a reactive policy so it is   actually state dependent in terms of numbers 
when you look at this basically what we find   is that if you have a flat policy or a deep RL 
style policy it does not work on test tasks but   this sort of task programming or programming 
action works very well and it actually works   even with vision so you have pure visual input no 
specification of where the tasks where the objects   are so you get generalization with visual input 
without specifying particular domain specific   visual design but again none of this thing works 
perfectly it would not be robotics if it worked so   so so what feels often what happens is because we 
are using API if the API doesn't declare when the   failure happens let's say you're trying to grab 
something but the grab action did not succeed the   high-level planner does not know any continuance 
so we went back to the model and said what is what   is it that is actually causing it to fail so we 
found that even though we used the output as the   program we were able to inject structure the model 
itself was still a black box it was basically   an illustration we thought perhaps we can open 
the black box and actually put a compositional   problem what does this compositional prior look 
like you basically say think of graph neural   networks so graph can basically be this idea of 
executing the task in a planner you know you know   in a PDF style planner where nodes are active 
States edges are graphs and you plant through   these things so this can actually still result 
in a two-stage model where you are learning the   graph of the task itself rather than a black box 
LST and predicting this but there is a problem   the problem is in these kind of setups the number 
of states can actually be combinatorial millions   maybe and the number of actions are finite so the 
concept of learning this graph in a neural sense   was to understand that the graph will not be in 
this set up but actually a conjugate graph the   conjugate graph flips the model by saying nodes 
are now actions and edges are States so you can   really think of these are nodes are telling you 
what to do and edges are pre and post conditions   and how does this model work now you can actually 
have an observation model which tells you where   to go in any particular state where what action 
was executed and each action tells you what state   you end up in which tells you what it would 
be the next action to do because this graph   is learned your base you're basically getting the 
policy for free and the training is very similar   to the program induction except you do not need 
full execution with the lowest level of actors or   lowest level of actions you are sufficiently this 
that would be sufficient and what that basically   gives us is stronger generalization in both videos 
and states with much less supervision again so we   have fewer data points or weaker supervision but 
we get better generalization so the big picture   key insight again is compositional priors such 
as let's say new programs or new tasks graphs   enable us a modular structure that is needed to 
achieve one-shot generalization in these long   term sequential plans in the one or two minutes 
that I have left I want to leave you with this   question often we are looking at robotics as azzam 
as the sort of ultimate challenge for a high and   we compare the performance of Robotics with our 
colleagues in vision and language where we have   done a lot of progress but if you notice one 
of the things what happens is as the tasks grow   smaller the datasets grow very small very quickly 
in robotics but to do more interesting tasks   more complicated tasks we still need to keep the 
datasets large enough for us to be able to sort of   leverage powers of these algorithms if you look at 
robotics in recent times the data that have been   essentially miniscule about 30 minutes of data 
that can be collected by a single person this is   a chart of large data sets in robotics this is 
not very old actually this is coral 20 19 20 80   just to compare this with NLP and and envisioned 
a license we are about three orders of magnitudes   offs so we were basically asking the task why 
is it the problem the problem is both vision   and language have Mechanical Turk they can get 
a lot of labels but in robotics labeling doesn't   work you actually to show so we spent a lot of 
time to create the system which is very very   similar to Mechanical Turk we call it Robo turk 
well you can use essentially commodity devices   like a phone to get large-scale data sets which 
are actual demonstrations full 3d demonstrations   and this this can now enable us to get data both 
on real systems and simulated systems at scale   so you can be in places where ever you want 
collect data from crowdsource workers at very   large scale we did some pilots we were able to 
collect hundreds of hours of data just to give   you a sense of how this compare that's 13 hours 
of data collected in about seven months we were   able to collect about 140 hours of data in six 
days now the next question would be why is this   data useful so we did reinforcement learning what 
we find is if you do pure RL with no data even on   three days of of doing this with multiple machines 
you get no progress as you keep injecting data   in the system the performance keeps improving so 
there is actually value in collecting data so the   take-home lesson was basically that the that more 
data with structure and semantics supervision can   fuel robot learning in increasingly complex tasks 
and scalable crowdsourcing methods such as robotic   are really sort of enable us to access this 
treasure trove so going back what I really want   to leave you with is we talked about a variety of 
methods in different levels of abstraction from   controls to planning to perception and then we 
talked about how to collect data but if there's   one thing that I want to leave you today with is 
if you want to do learning in in complex tasks   and complex domains such as robotics it is very 
important to understand the value of injecting   structured priors and and inductive biases in your 
models generic models from deep learning that have   worked for may or may not work for you that is one 
- the use of modular components in modularization   of your problem well you use domain dependent 
expertise with data-driven problems can enable you   to actually do practice build practical systems 
for much more diverse and complex applications   that I would like to thank you all for being such 
a patient audience and happy to take questions 

you thanks a lot for having me here today I'm going to talk about newer rendering because rendering is such a massive topic start with some clarifications so as far as this lecture goes rendering can be both forward process and the inverse process the forward rendering computes a imaging from some 3d scene parameters such as the shape of a serve and the shape of object the color of object the surface with material and the light source etc for rendering has been one of the major focus of came the graphics for many years the orbit of this problem is thinking was rendering it studies the problem that given some images we are trying to work out what are the three DS things that was used to produce this imagery and it was rendering is closely related to come division with applications such as 3d reconstruction and motion capture etc and forward rendering and in what's running increasingly related because the high-level presentation of a vision system should look like not even tation abusing condom affix in this lecture I'm going to talk about how much in learning can be used to improve the solution to both of these two problems but the people we drive into neural networks let's first to take a very quick tour to the conventional method this is a sort of a toy example of a ray tracing which is a widely used forward rendering technique imagine you are inside of a cave the red bar is a light source and the greeter is in the image plane rate risk works by shooting rays from a imaginary eye to every pixel in the grid in the piece of onion imaging grid and it tries to compute the color of the object than the rate that you can see through an array in this case the rate directly hastened by source so we're using a color of the nice or nice sauce to occur in the pixel however more often than not Lorraine will hear some object surface before either bouncing to no light source in this case we need to calculate the color of the surface the color of surface can be computed as an integral of the instance radiant however this is very typical do in the analytic way so what a PMO normally do is to use bode Carlo sampling which generated random rays within the integral domain and then compute an average of its range as an approximation of the integral we can also change the sampling function to approximate surface material this is how we can make the surface look more glassy or rough most of the time are trained in multiple balance before either hitting the light source and this soon develop into a recursive problem which is very expensive to do so there are many other one street racing techniques have been invented to deal with this problem which I'm going I'm not I'm not going to talk here but a general concerns is rate racing with Monte Carlo sampling super expensive because of the very high estimate of variance and a low convention convergence rate for complex thing like this you need 100 minions or maybe Binion's array to render so the question we ask is whether in version learning can be used to speed up his process and the answer as we see later in the lecture is yes but before we dive into the answer that quickly switch to the universe problem for a moment so this is a classical stereo from a safe shape on stereo problem where you have cue images on the same object and in trying to work on the 3d shape of the object you do this by first finding the features across two images then you can compute the camera motion between the two photos the camera motion usually is parameterised by a rotation and the translation with this camera motion we can work on the 3d location of these features by translation more cameras can be brought into improving the result and the modern method can scale up to work with thousand and even hundred thousand photos this is truly amazing however the output of such compute vision system is always is often noisy and sparse and in contrast compared aphek application needs very clean that I mean in the nice razor-sharp details so often times people human height have two-stepping and a cleaner result and sometimes we even need to handcraft from scratch so every time you hear the word handcraft nowadays is a strong signal for machine learning to stabbing and automate the process so in the rest of the lecture I'm going to talk about how neural networks can be used as a sub module and endian pipeline for forward rendering and also I'm going to talk about how neural networks can be used I say differentiable renderer that opens the door for many interesting inverse applications but let's first start from the for rendering process as we mentioned before Monte Carlo sampling is very expensive and this example on the top left we have done noisy rendering with one sample rate per pixel and then the number of samples doubled from left to right and from top to bottom as you can seen the result also improves but the same time the confessional cost also improves also increase and I'm going to make a very fast analogy here most of you should be familiar with alphago by now which user a policy network and the value network that you speed up in a multicolored research for those who skip some of the previous lectures and the value network takes an input board precision and a predict a scalar value has no probability no winning probability he asses it reduced in the tabs of that research and similarly the policy network taken a board position as input and output a probability distribution for the next move in essence it introducing the breath of the search so the analogues I'm trying to make here is we can also use policy network and a value network to speed up the Monte Carlo sampling for rendering for example we can use a very network to denoise the rendering with low sample per pixel PC you try to predict in the correct piece of value from some noisy input as far as policy Network goes we can use a net way to generate a useful policy that is smartly example the race so in the whole rendering converged faster as first to take a look at the value based approach this is a reasonable work we did for the noisy multicolor rendering on the Left we have a noisy implemented sample that for picks for sample per pixel in the middle isn't the noisy result on the right it's not ground chose reference imagery render with the 32 km per pixel it takes about 90 minutes to do our traffic or CPU in contrast that the noisy result only takes about a second to run on a commodity GPU so there's a very good trade-off between speed and quality the whole network is trade to an end to end as auto encoder with two laws the first loss is our one loss of a BG feature of the output imagery the second loss is again lost pagano's here is obviously trying to retain the details in the output imaging so this is a side-by-side comparison between the result of training with and without again lost in owing natural images has been studied for a long time but in order Monte Carlo rendering has some very unique point the first thing is we can suffer in a diffuse and specular components and a random through different paths of network and then version result together and this tends to improve the result a lot secondarily there are some very inexpensive by-product of the rendering pipeline that we have used to further improvements out such as a better map normal map and depth this by-product can be used as a I can scenery feature that a generator contacts where the noise should be conditioned on however how to feed this a gazillion feature in the pipeline is still pretty much a open research question the way we did in this paper is something called a mean and whites biasing and scaling the animal was bouncing takes event scenery features and random through a bunch of levels and layers and the atom the result in tune input the feature X and no wisely what improve this is equivalent to feature contamination I know why scaling runs animal wise multiplication between auxiliary features and input feature X the argument you have both scaling and biasing here is they capture different aspect of the relationship between two inputs you can think I know wise biasing is sort of a or operator which check if I feature is in annual fees to input in contrast annual scaling is an end operator which checks whether like feature in the feature is in both of these two inputs so by combining them together the zero feature can be utilized in a better way and this is retinoids result and taking noisy input of sample data at the full range per pixel and then we compare our in other ways alternative method in general our method has a nice annoyed no it has a nice noise and the more details now let's move on to the policy based approach I'm now going to cover the entire literal here but just want to point point you to a very recent work from Disney research which is called the neuro important sampling so the idea is we want to find for each location in the scene a very good policy then it had helped us to sample race Muttley and reduce the convergence time and in practice the best possible policy is actually the instant irradiance map at that point because it literally tells you where the lies come from so the question is having generated this instant radius map from some local surface property through a new network and the answer is yes just like how we can how we can nowadays generated images from London impre noise because also trying a German network that a gerund is instant radius map from some local surface property such as location the the direction of the incoming rain and then the surface normal however the catch is such mapping from the surface local property to an instant radius map varies from scene to scene so the learning has to be carried online during the rendering process meaning the natural start front generally some random policies and the gradually learns in the scene structure so it's able to produce better policies as an easily result on the left is the conventional ray tracing on the right is no ray tracing which gyro important sampling which can work much faster as you can see here okay so far we have been talking about how neural networks can be used as a sub module for forwarding rendering the next I'm going talk about how we can use your networks I said end-to-end pipeline remember we talked about ray tracing which start from a catchment rays from the pixel to a 3d scene this is so called a image centric approach approach it's actually this approach it's kind of very difficult phone your neighbor to learn because first of all is recursive second secondarily we need new sample you need to do to screen sample which is very difficult do analytically and in contrast there's another way of doing rendering which is called a reservation which called which is object centric what it does is for every 3d point you can kind of shooting risk towards in the image and it only needs to shoot one primary range so there's no recursion and then you do not need to do any sampling so this turns out to be very different how to be easier for anyone that what you learn and the rustle rate rasterization contains two main step the first step is for every 3d primitives the kind of project nub primitives to know to dmg playing and impose them onto each other based on their distance tuner image so in this way the front of most surface can always be visible in the final rendering next step is to compute a shading basically it calculates in the pixel color by interpolating the color of the 3d primitives such as a vertex color in general reservation is faster than retracing and as we mentioned before it's easier for neural network to learn because it does not have a recursion or screen sampling process Oh sounds great a path around lesson another catch which is the import data format here are some major mainstream 3d format depth map voxels 3d point cards and match and some of them are not very friendly to neural networks and my policy language tells me I should avoid them in this lecture so anyway that start on the tabs map this is probably the easiest one because all you needed only three is to change the number of input channels for the first layer then you can run your favorite new networks with it and it's also very memory efficient because nowadays the celebrators are designed to run images another reason for the EPS map to be convenient to use is we do not need to cover calculating the visibility because every information in the - map is already in front of front of most surface all we need to do is compute the trading and there are many works in rendering depth map in two images on the other way around and I'm not going to talk about them in this lecture so let's move on to work so what I saw it's also kind of friendly to neural networks because owner data are arranged in a grid structure however once always very memory-intensive it is actually one order of magnitude higher than image data so conventional neural networks only run work sauce of very low resolution but what make voxel very ancient to us is in a niche to computer both visibility and shading so this is a very good opportunity for us to learn and and pipeline for newer rendering so we try this end-to-end neural voxel rendering called render night it starts from transforming the input of walk so into your camera Queen late I'd like to quickly access here that such a 3d Richard party transformation is something we actually do not want that which learn because it's very easy to do with coordinated transformation but very hard for convolution in such operation to to to perform and we will come back to this later having having transform implement so in your camera during the next step is to learn a neural voxel representation of the 3d shape what we can do here is to pass the info box off to a sequence of three deconvolution then output neuron voxel contains deep features that is going to be used for computing the shading and the visibility next time you come to know visibility one might be attempted to basically say okay we can use no standard and taps buffer algorithm here but it turns out this is not so easy because when you do this free deconvolution you kind of distribute diffusional value within the entire box of grid so it's not clear that that which grades are from the front most surface at the same time since every voxel contains two features now you have to integrate allow across all these channels to compute the visibility to deal with this problem we use something called a projection unit this projection unit first taken the 40 internal tensor a neuro Box old and then reshape it into a 3d tensor by squeezing the last two dimensions last two dimensions at a depth dimension and the future channel dimension then you learn say multi-layer perception which learns to computer visibility from allowing the squeeze last channel so on a higher level you can think it's multi-level perception is a inception network that I learned to integrate visibility along the both steps and the features last step is to use a sequence of 2d up convolution to render the project to the neural voxel into a in depict into a picture and the we train is network and to end with a mean square pixel loss here are some result the first row is an input of voxels the second row is no output of the new render net as you can see running at the table to learn to how we do the commutation visibility and the shading you can also learn you run an ad queue generated different and rendering effect such as contour map tone shading and ambient occlusion in terms of generalization performance we can use in the retina to trade on chair model to render some unseen object such and bounty and the scene with multiple of a scene with sabbatical objects you can also handle data with corruption and the low resolution the first row renders English ape that is readily corrupted the second row renders implicate with teddys has a resolution that is 50% lower than a training resolution the retina can also be used to render textured models in this case we learn a additional texture network that encodes in protector into a new rotational voxels and these are neural natural tightrope walkers will be concatenated with no-english a voxel in the channel right way and a concatenated voxel is going to be fed into a network to render these are some without of render detection models the first row is the input of voxel the second row is no ground truth reference image the third row is our result from render net as you can see that the ground truth image obviously has more Sheppard details but in general the retina is able to capture the major facial features and the computer it miscibility and shading correctly as a fine experiment we try to mix and match the shape and the texture in this example in the first row the image are rendered with the same shape in Provocateur and a different texture box on the second row render is rendered or is in the same texture by different shapes okay now let's move on to a 3d point cloud 3d point cloud is actually not so friendly to neural networks the first of all the data is not ranged on a range on a grid structure in the meantime depends on how your sample points on a surface the number of a point and order of the points can also vary I just want to quickly point out this recent work called neural point based graphics from I think it's a song AI lab but before we talk about that as first talk about how conventional reservation is done for 3d point cards basically for every point in the 3d scene we project the point in unit imaging as a square and size of the square is kind of inversely proportional to the distance between the distance on the point tuner image obviously you have to impose no projective square on top of each other based on depth to next what we do is you try to encourage the squares using a RGB color on the 3d points however if we do this there's a lot of holes in the result in the same time you can see a lot of color blocks so what this neural point based graphics did is they replace the RGB color by a learned neuro descriptor this new the script is sort of an 8 dimensional vector that is associated with each input point you can think about is a deep feature that a compensate not sparsely on the point cloud this is a visualization of the newer descriptor using the first three piecing component we start by randomly initialize these newer descriptors for each point in the scene and optimize for that particular scene obviously you cannot use native of one scene to describe another thing so this optimization has to be done posting the training and the testing stage for every eye before each scene and then then the authors use a autumn encoder to encode the projected neural descriptor into into a photo risky imaging this render network is jointly trained with no optimization of the neuro descriptor during the training but can be reused in the testing stage these are some result which i think is really amazing the first row is no rendering which conventional RGB descriptor the second row is the rendering with the neuro descriptor as you can see there's no hole and the result is in general much sharper and the very cool thing about this method is the newer descriptor is training that you'll be viewing variant meaning once they are optimized for seeing you can render the scene from different angles that's really cool okay last we have these Nash models which which is difficult for neural networks because of its graphical representation I just want to quickly point how these two papers the first one is quality fern we were rendering it actually use a very similar idea so we just talked about which is indecent neuro sorry these newer point-based graphics they use very similar ideas but this paper applies ideal to render mesh models another paper is your 3d mesh render it is cool in a way that you can even do a 3ds star transfer however the new Annette were part of his method is used may need to change in a vertex color and the position is not so much internal rendering powder but I just put this reference here for people who are interested okay so far we have been talking about the for the rendering let's move on to let me know universal rendering we call the universe rendering is the problem that given some input the image we want to work out a 3d scene that was used to generate this image the particular method I'm going to talk about today is called differentiable rendering it works as follows first we start from a target image then we generate a some kind of approximation of a 3d scene this approximation does not to be very good as far as we can render it we can compare and result with no target image and then we can define some meant metric to measure the different quantity major difference between the render image and the target image and even though rendering process is differentiable we come back propagate the loss to update an input model and if we integrated we do this eventually in the hope is the implement will converge to something meaningful and the key point here is the photo rendering process has to be differentiable the all that you calculate is backpack backpack for gate operation and this is where we immediately seen the value of neural networks because modern journals are designed to be differentiable designed to perform its publication so we got the gradient for free another reason for new and I would to be helpful here is as you can imagine this intuitive optimization process is going to be very expensive so what we can do within your network is to learn a field forward process that are approximate is intuitive organization for example we can learn an auto encoder which encode that input imaging into some sort of a latent presentation that enables some really interesting downstream tasks such as noble view synthesis however in order to that encoded to learn the use for invitation we needed you to know correct in tactical BIOS and why inactive pile and very interesting very exciting about it is like that a learning can be a lot easier if we can separate in the post from the appearance and I truly believe is something human do this is my four-year-old son playing shaped puzzle the task is to build a compact shape in some basic shape primitive such as strangle and square in order to do this task he has to apply through the rigid partition mission through these primitive shapes in order to match what is required on the board it's amazing that human can do this work rather effortlessly well this is something the newer networks was invented to suffer with for example the two deconvolution or the general in the convolution is a local operation which there's no way they can carry this global transformation fully kinda layers might be able to do this but at the course of a network capacity because it has to memorize owner different configurations on the same object so we ask the question how about we just use simple coordinated transformation to encode the pose of the object and suffer in the post from the appearance whether that will make the learning easier so we try this idea called hologram which which learns a 3d representation of from natural images without the 3d supervision by without a 3d vision I mean there's no 3d data that there's no grunge was label from the pose of object in the image during the learning unit training process everything is learned from purely from 2d unlabeled data so the cool thing about this idea is the learning is driven by inductive bias as opposed to supervision let's first take a look at how conventional German German German TV network works conventional for example conventional joining networks Germany images use 2d convolutions we survive a few assumptions about a 3d world for example this condition again and concatenate post vectors or apply feature by feature wise transformation to control the post of the genital nope no post of the object in the geometry images a less branch of the label was used in the training process the post is called to be learned as a latent variable which is hard to interpret at the same time using 2d convolution to generate the 3d motion will generate an artifact in a result in contrast hollow again generated much better without by separating the post from the motion listen to some random faces generated by collagen and I'd like to emphasize there's no 3d data used in the training process and the key point here is hologram used a neuro voxel as its latent reputation to learn such neuro voxels we use a 3d generator Network and the learned 3d voxel is going to be rendered by render net so we just talk about the 3d generator is basically a extension of star gang into 3d it has two input the first one is a 4 D tensor account learned learned 4 D tensor a constant tensor which is learned and as a sort of templates of a particular class of object and this this tensioner is going to run through a sequence of 3d convolution and to become no neuro voxel with intention the second input is this random vector used as a control controller that will be first transformed into a fine parameter of the adaptively instant normalization right here Osuna pipeline node and generate the learned 3d vox orientation is called as I said before it's going to be rendered rendered by render net and then you already trained this network in an unsupervised way we use a discriminative work to classify not render image against real water image the key here is it is crucially important that during a training process we happy to apply random rigid participation in the until this box of invitation and this actually how the internship bias is injected during a learning process because in this way the network is forced to learn some very strong information that is unbreakable under arbitrary post and in fact if we do not apply random transformation doing a learning process the net OVA was not able to learn these are some result as you can see hologram is pretty robust to view transition and also complex backrub one limitation of hollow gain is it can only learn post that exists in the dataset in the training dataset for example in Scardino said there's very little variation in the elevation direction so the network cannot Expo late however when they are in knob training data the network has truly learned for example we use shift Netta to generate more poses for chairs a network is able to learn to do 180 degree rotation in elevation we also try to network with some really Chinese dataset for example this background data set in students is very challenging due to the fact that there's a very strong appearance for vibration across the dataset you can hardly find a new bedroom that looks like each other from different views in this sense that's why we wake post signal in the dataset however the network is still able to generate something reasonable and I think that's really interesting another surprise is the network is able to further decompose the appearance into shape and the texture as a test we feed to different control vectors one tune of 3d part of network neither to a 2d part of network and it turns out on the 3d controller the controller fit into the 3d part of network controls the shape and the control 3d into 2d part of network controls in picture so these are some result every row in this image using a saying texture controller but a different shape controller I recall him in this image is the same shape controller but different controller I think it is truly amazing because it obviously reminds me about the vertex shader and the fragment shader in a conventional kind of graphics pipeline we're underwater shader changed in geometry and the conventional trader doing a coloring okay I think it's probably a good time to start with your conclusions at the beginning of this talk we asked a question can your network be helpful to for the rendering and the inverse rendering and I think the answer is yes we haven't seen you in her books being used as a sound module to speed up retracing and then we have a scene example of no value based approach and the policy based approach we also have seen neural network being used as a end-to-end the system that helps 3d as a reason and as far as no inverse problem goes we see new network can be usually is a very powerful differential differentiable renderer that opens not water many entry to downstream applications such as view synthesis and the key thing here is your network is able to use the correct internet files to learn a strong replantation and before I finish my talk I just want to say this is a pretty much still a opening question a very new research frontier there's a lot of opportunities for example the still huge gap between the quality of the end-to-end rendering and the conventional physical based rendering and as far as I know there's really no good solution for euro based next renderer and the in terms of the universe problem we have seen in careers out of learning strongly plantation however it's easy to see what more effective inductive bias and network architecture can be used to push in learning forward and I before and then talk I'd like to thank my colleague and collaborators who did an amazing job all these papers especially to could be the most of worked for and again and the whole again I'm sorry Rena net and hooligan and also being put in the mortal work from the neural multicolor denoising with that I thank you 

you hey everybody first of all thank you for inviting me thank you for organizing all this this seems like a really really cool what's it called No so j-term is Harvard what's this called IP okay cool so this I'm sure there's many different courses you could choose from it's really cool that you were able to choose this one so I'm going to tell you a bit about some of the work that I'm doing now in the past I've done kind of machine learning for biology have done machine learning systems I'm kind of pure methodology stuff and this is a bit of a weird project but it's my favorite project I've done in my life so far I've waited a long time to do it it's still really early so this isn't like a collection of you know great works and great conferences at this point this is like kind of fresh off the presses so any feedback actually is is definitely welcome so kind of venturing into unknown territories and I should mention this is the work of many people I'm very proud to represent them here but this is definitely not a solo effort at all even though I'm the only person here so my name is Alex I'm a research scientist at Google research there's a when you work at a mega Corp there's interesting levels of kind of organization of things so if you hear kind of these words I'll kind of explain what the different tiers mean so Google research is all the researchers at Google Google brain is the sub team that's existed for some time that focuses on deep learning and then I lead a team inside of Google brain that focuses on machine learning for olfaction and Google research is big when I joined it didn't appreciate just how big it was so there's 3,500 researchers and engineers across 18 offices I think is actually out of date in 11 countries and the large scale mandate is to make machines intelligent and improve people's lives and that could mean a lot of different things and our approach generally and the number one bullet here is kind of my favorite and this is where I spend most of my time is doing foundational research right so we're kind of in at least in my opinion another golden era of industrial research kind of like you know Bell Labs and Xerox PARC like those those eras now we have really wonderful thriving industrial research labs today and I feel really fortunate to be able to kind of work with people who are in this environment we also build tools to enable research and democratize artificial intelligence and machine learning tensorflow like we've got the shirts up there I don't have that shirt so that's kind of a collector's item I guess we you know open-source tools to help people I'll be able to use and deploy machine learning in their own endeavors and then also internally enabling Google products with artificial intelligence and that was kind of one of the original or one of the activities that Google brain has done a lot historically is to collaborate with teams across Google to to add artificial intelligence and here's some logos of some of the products that AI and ml has impacted you can see you know YouTube on there you can see search ads Drive Android and a lot of these have some things in common which is Google knows a lot about what the world looks like and a lot about what the world sounds like but this is kind of where it gets a little bit sci-fi this is where I step in it doesn't know a lot about what the world smells like and tastes like and that might seem silly but there's a lot of restaurants out there a lot of menu items there's a lot of natural gas leaks there's a lot of sewage leaks there's a lot of things that you might want to smell or avoid and further than that in terms of you know building like a Google Maps for what the world tastes and smells like there's control systems that were you might actually want to know what something smells like like a motor is burned out right or you might want to know what something tastes like if there's a contaminant in some giant you know shipment of orange juice or something like that so we haven't digitized the sense of smell it seems a little bit silly that we might want to do that but that was perhaps something that seemed silly for vision before the talkies right before movies and before audition when the Kronecker or the phonograph came about so those those were weird things to think about for those sensory modalities in the 1800s and in the nineteen hundreds but right now it seems like digitizing sense and flavor are the silly things to think about but nonetheless we persevere and work on this so we're starting from the very very beginning with the simplest problem and I'll describe that in a second but first simple faction facts so my training is actually in olfactory neuroscience and I kind of through a secured this route ended up in machine learning and so since I have you captive here I want to teach you a little bit about how the olfactory system works so this is if you took somebody's face you sliced it in half and the interesting do I have a pointer here great so there's a big hole in the center of your face and that's where when you breathe in through your nose air is going through there most of the air that goes into your head is not smelled most of it just goes right to your lungs there's a little bit at the top there which is called it's just a patch of tissue it's like five or ten millimeters square seems a little bit more than that but it's very small and that's the only part of your head that I can actually smell and the way it's structured is nerves or axons from the nerve fibers from the olfactory bulb actually poke through your skull and they innervate the olfactory epithelium and it's it's one of only two parts of your brain that leaves your skull and contacts the environment the other ones the pituitary gland that one dips into your bloodstream which is kind of cheating and there's three words that kind of sometimes get used in the same sentence a taste scent and flavor so taste lives on your tongue flavor is a collaboration between your nose and your tongue right so what happens when you eat something is you not you masticate it you chew it up and that releases a bunch of vapors and there's a chimney effect where the heat of those vapors and of your own body shoots it back up your nose it's called retro days or olfaction have you noticed if you've had a cold things taste kind of more bland that's because your sense of smell is not working as much and is not participating in flavor so little factoids there for you before you get to the machine learning part so there's three colors envision RGB and there's three cones or cell types photoreceptor types in your eye that roughly correspond to RGB there's 400 types in the nose and we know a lot less about each one of these that we do about the photoreceptors we don't know what the receptors actually look like they've never been crystallized so we can't like build deterministic models of how they work and in in mice there's actually a thousand of these and there's two thousand in elephants and maybe elephants smell better this could be an evolutionary byproduct but they don't you we actually don't know but it's another fun party fact they're also they comprise an enormous amount of your genome so it's two percent of your genome which is of the protein coding genome which is an immense expense so like for something that we actually pay comparatively little attention to in our daily lives it's actually an enormous part of our makeup right so worth paying attention to and we don't really know which receptors respond to which ligand so basically we don't know enough about the sense itself to moderate model it deterministically which is kind of a hint like maybe we should actually skip over modeling the sense and model the direct perception that people have this is my other favorite view of the nose instead of cutting you know sagittal II like this this is a coronal section this outline is the Airways of your turbinates I think this is a really beautiful structure I think it's under top the curly bits that are big down here this is just where kind of air is flowing it's it's humidified and then this little bit up top is where you actually smells so the upper and lower turbinates and this is what I used to study in mice this is a mouse upper and lower turbinates you notice it's a lot more curly the more that smell is important to an organism the curlier this gets meaning the higher surface area there is inside of this particular sensory organ you should there's actually some cool papers for this in otters it's like this times a hundred it's really incredible if you go look at other and there's this notion that that smell is highly personalized and there's no logic to it like vision and audition we've got like fast Fourier transforms and we've got you know good board but we've got a lot of theory around how vision and hearing are structure and the idea that that is kind of pervasive is sent to somehow wishy-washy and people do smell different things it is somewhat personal but people also see different things right so who similar the this is black and blue to me but I'm sure it's white and gold to some people who is it white and gold for who is it black and blue - right so maybe vision isn't as reliable as we thought doesn't maintain itself on top of the pedestal I actually cannot unsee this as white and gold let you resolve that between your neighbors so there are examples of specific dimorphism x' in the sense of smell that can be traced to individual nucleotide difference --is right for single molecules which is pretty incredible though there's genetic underpinnings to the dye morphisms that we perceive and smell and they are common there's a lot there's a lot more snips that look like likely dye morphisms I just haven't been tested but you know 5% of the world is colorblind and 15% of the world has selective hearing loss right so let's give the sense of smell a little bit more credit and let's be aware that we each see the world hear the world and smell the world a little bit differently it doesn't mean there's not a logic to it that doesn't mean that the brain evolutionarily speaking has adapted to tracking patterns out there in order to transduce them into useful useful behaviors that help us survive so that's a bit of background on olfaction what we're doing is starting with the absolute simplest problem right so when I mentioned foundational research I really meant it so this is gonna look quite basic so this is the problem you got a molecule on the left this is um vanillin right so this is the main flavor and olfactory component of the vanilla bean the vanilla plant the flower is white so we think vanilla soap should be white but the bean is actually this kind of nice dark black when it's dried and if you've ever seen vanilla extract that's the color real vanilla is actually incredibly expensive and is the subject of a lot of criminal activity because the beans can be easily stolen and then sold for incredible markups that's the case for a lot of natural products in the flavor and fragrance space there's a lot of interesting articles so you can you can google to find out about that so part of the goal here is like if we can actually design better olfactory molecules we can like actually reduce crime and strife so the problem is we've got a molecule let's predict what it smells like sounds simple enough but there's a lot of different ways that we can describe something is having can describe it with a sentence Oh smells sweet with a hint of vanilla sand notes of creamy and a back note of chocolate which just sounds funny for vanillin but that's indeed the case what we'd like to work with is a multi label problem or you've got some finite taxonomy of what something smells it wasn't it could smell like and then only some number of them are activated here creamy sweet vanilla chocolate right so that's what we like to work with so why is this hard like what why is this something that you haven't heard is already being solved so this is Laurel you've all probably smelled this molecule you've all probably had it on your skin this is the dryer sheets knob right this is the fresh laundry smell it's a very commercially successful molecule that is now being declared is illegal in Europe because it's an allergen in some cases the u.s. we don't really care about that generally as far as I can tell I always have different standards I suppose I should say the four main characteristics are muguet which is another word for lily of the valley that's that flower of the dryer sheett smell fresh floral sweet so here are some different molecules that smell about the same right they look about the same right so for this guy we just clipped a carbon off of this chain here for this guy we just attached it to the functional group on the end right so why is this so hard right the main kind of structure the scaffold here is the same this molecule looks very different and smells the same right you can have a wild structural differences for many many different odor classes and still maintain the same odor percepts right this is a difference of where the electrons are in this ring it structurally it turns out this kind of stiffens things but even in 3d representations that are static and certainly in the graph representation here these look pretty close but you've rendered something very commercially valuable into something useless so we built a benchmark dataset I'll kind of describe where this is from and what the characteristics are but we took two sources of basically perfume materials catalogs so like the Sears catalog for things that perfumers might want to buy all right so these are think we've got about 5,000 molecules in this data set and they include things that will make it into fine fragrances or into fried chicken flavoring or all kinds of different stuff and on average there's four to five labels per molecule so here's an example this is vanillin it's actually labeled twice in the data set there's some consistency here so that's another question you might have is like how good are people at labeling what these odors actually smell like and the answer is you and me probably everybody in the room most people two-room are bad right and that's because we grew up or at least I did with a Crayola crayon box that helped me do color word associations but I didn't have anything to help me do outer word associations you can learn this it's harder to learn this as an adult but you definitely can I took my team to get perfume training it's incredibly difficult but it is it's a craft it can be practiced but amongst experts that are trained on the taxonomy people end up being quite consistent we have some human data that I don't have in this talk that indicates that that's you know that's the case there is a lot of bias in this data set skew because this is for perfumery materials who were we have a lot that we have over representation of things that smell nice alright so lots of fruit green sweet floral woody you don't see a lot of solvent bread radish perfumes and so we have kind of less of those molecules which I guess is to be expected the reason why there's a spike at 30 as we impose a hard cutoff there just we don't want to have too little representation of a particular odor class and this this is there's no modeling here this is a picture I'll kind of break it down we have a hundred and thirty-eight odor descriptors and they're arrayed on the rows and columns and so each row each column has the same kind of indexing system it's an odor ID and each IJ entry here is the frequency with which those two odor descriptors show up in the same molecule right so if I have a lot of molecules that smell like both pineapple and vanilla it doesn't happen but if I did then the pineapple vanilla index would be yellow so which shows up just in the data is the structure that reflects common sense right it's like clean things show up together pine lemon mint right toasted stuff like cocoa and coffee those go together they often co-occur together in molecules at the mono molecular level these things are cope you know correlated savory stuff like onion and beef that has kind of loco occurrence with popcorn you don't want beef popcorn generally baby actually that'd be good I have no idea dairy cheese milk stuff like that so this there's a lot of structure in this data set and historically what people have done is treat the prediction problem of going from a molecule structure to its odor one odor at a time and what this indicates is there's a ton of correlation structure here you should exploit you should do it all at once in hint deep-learning etc so what people did in the past is basically use pen and paper and intuition so this is crafts vetiver rule I presented this slide and took a little bit of a swipe at how simplistic this is and Kraft was sitting right there which was a bit of an issue we had a good back and forth and he was like yeah we did this a long time ago and we can do better but the essence of this is you observe patterns with your brain your occipital cortex and you write down what they are and you basically train on your test set and you publish a paper and that seems to be kind of the trend in like classic structure at a relationship literature there are examples of these rules being used to produce new molecules so that are generalizing the ones that I've been able to find and searching the literature on the are in the upper right hand corner here but generally these are really hard to code up right because there's some kind of fudge factor it's not quite algorithmic well people do now what kind of the incumbant approach is to take a molecule and to treat it as kind of a bag of subgraphs let me explain what that means so go through the molecule think of as a graph pick a radius so like I'm only gonna look at atoms and then yes okay what atoms are in this molecule they say okay I'm gonna look at things of radius one okay what atom atom pairs are there other carbon carbons or there you know carbon sulfur carbon oxygens you go through this comprehensively up to a radius that you choose and you hash that into a bit vector and that's your representation of the molecule it's kind of like a bag of words or back up fragments representation the modern sensation of this is called morgan fingerprints so fever here morgan fingerprints or circular fingerprints this is effectively the strategy that's going on you typically put a random forest on top of that make a prediction you predict all kinds of things toxicity you solubility whether or not it's going to be a good material for photos photovoltaic cells whether that's gonna be even battery and the reason why you know this is the baseline is because it worked really well and its really simple to implement right so you can once you've got your molecules loaded in you can start making predictions in two lines of code with RT kit and scikit-learn and it's a strong baseline so you kind of hard to beat sometimes so you know you'd expect the next slide is we did deep learning at it but let's take a little bit of a step back first so in the course so far you've certainly touched on feed-forward neural networks and kind of their you know most famous instantiation the convolutional neural network and the trick there is you've got a bunch of images and you've got labels that are the human labeled ground truth for what's in that image you pass in the pixels use you know through successive layers of mathematical transformations you progressively generalize what's in that image until you've distilled down the essence of that image then you make a decision yes cat no cat and you know when you see some new image they was not in your training set you hope that if your model is well trained that you're able to take this unseen image and predict you know yes dog no dog even if it's a challenging situation and this has been applied in all kinds of different domains and neural rendering is one that I did not know a lot about and that is super cool stuff so pixels is input predict what's in it right audio is input the trick here is to turn it into an image so to calculate the spectrogram of audio and then and then do a comment on that or an LST M on the on the time slices and then you transcribe that to the speech that's inside of that spectrogram text-to-text for translation image captioning all of these tasks have something in common which is that as was alluded to in the previous talk they all have a regularly shaped input the pixels are a rectangular grid right that's very friendly to the whole history of all the kind of statistical techniques we have text is very regular it's like a string of characters from a finite size alphabet you can think of that as a rectangle like a1 wherever there's character in a 0 for the unused part of the alphabet and the next character etc hard to put this into a rectangle right you can imagine taking a picture of it and then trying to predict on that but if you rotate this thing it's still the same molecule but you've probably broken your prediction that's really data inefficient so these things are most naturally represented as graphs and kind of like meshes like that's actually not very natural to give to classical machine learning techniques but what's happened in the past three four or five years there's been an increasing maturity of some techniques that are broadly referred to as graft neural networks and the idea here is to not try to you know fudge the graph into being something that it's not but to actually take the graph as input and to make predictions on top of it and this is this has opened up a lot of different application areas so I'll talk about chemistry today where we're predicting a property of a whole graph but this has also been useful for like protein-protein interaction networks where you actually care about the nodes or you care about the edges will there be an interaction social network graphs where people are nodes and friendships or edges and you might want to predict does this person exist or does this friendship potentially exist what's the likelihood of it in citation that works as well so interactions between anything right are naturally phrased as graphs and so that's that's what we use but let me show you kind of how it works in practice for chemistry so first thing is you've got a molecule it's nice to save make it a graph but like what exactly does that mean so the way we do this is so pick an atom and you want to give each node in this graph of vector representation so you want to load information into a graph about kind of what's what's at the atom so you might say with the hydrogen count the charge the degree the atom type that might be a one hot doctor you concatenate in and then you place it into the node of the graph and then this is the GNN part right so this is a message passing process by which you pick an atom and you induce for every atom pick an atom you go grab its neighbors you grab the vector representation and its neighbors you can sum it you can concatenate it basically you get to choose what that function is you passed that to a neural network that gets transformed you're gonna learn what that neural network is based on some loss and then you put that new vector representation back in the same place you've got it from and that's one round of message passing right the number of times you do this we just call that the number of layers in the in the graph neural network sometimes you parameterize the neural network differently a different layers sometimes you share the weights but that's the essence of it and what happens is around for this molecule like five or six or so the atom at the far or the node now because it's no longer an atom because it's informations been mixed the node at the far end of the molecule actually has information from the other end of the molecule so it's you know over successive rounds of message passing you can aggregate a lot of information and if you want to make predictions it seems a little bit silly but you just take every vector in every node once you're done with this and you sum them you can take the average you can take the max whatever I mean the kind of hyper parameter tune that for your application but for whole graph predictions something works really well in practice and there's some reasons for that that have been talked about in the literature and then that's a new vector that's your graph representation pass that to neural network and you can make whatever prediction you like in our case the multi-label problem of what does it smell like and how well can we predict really good predict real good on the x-axis is the performance of the strongest baseline model that we could come up with which is a random forest on count based fingerprint so when I said it's a bag of fragments you either say 0 or 1 the fragments present what's better to do is to count the number of those sub fragments that are present this is the strongest count of chemo and formatic baseline that we know of and on the y-axis is the performance of our graph neural network and you can see we're better than the baseline for almost everything what's interesting is the ones that were not better at bitter and music medicinal are some selections in talking with experts in the field the consensus is that the best way to predict these things is to use molecular weight and it's actually surprisingly difficult to get these graphed neural networks to learn something global like molecular weight and so we've got some tricks that we're working - basically sideload graph level information and hopefully improve performance across the board so this is a this kind of hardest benchmark that we know about a structure odor relation prediction and we're pretty good at it but you know we'd actually like to understand what our neural network has learned about odor because we're not just making these predictions for the sake of beating a benchmark we actually want to understand how odor is structured and use that representation to build you know other technologies so in the penultimate layer of the neural network stack on top of the GNN there's an embedding you guys talked about embedding from this course okay cool so the embedding is a notion of like the general representation of some input that the neural network has learned that it's good and that's the last thing is going to use to actually make a decision so let me show you what our embeddings look like so the first two dimensions here are these are the two first principal components of a 63 dimensional vector this is not TC or anything like that so you can think of this as like a two-dimensional shadow of a 63 dimensional object each dot here is a molecule and each molecule has a smell and if you pick one odor like Musk we can draw a little boundary around where we find most of the musk molecules and color them but we've got other odors as well and you know it seems like they're sorting themselves up very nicely we know this they have to sort themselves out nicely because we classify well so it's kind of tautologically true but what's interesting is we actually have these macro labels like floral we have many flowers in our data set you might wonder ok floral where is that in relation to rose or lily or or muguet and it turns out that you know floral is this macro class and inside of it are all the flowers this is kind of like fractal structure to this embedding space we didn't tell it about that all right I just kind of learned naturally that there's an assortment of how odor is arrayed and you know there's the meaty cluster which conveniently looks like a t-bone steak if you squint your eyes my favorite is the alcoholic cluster because it looks like a bottle i'm never retraining this network because that's definitely not going to be true next time and you know this is kind of an indication that something is being learned about odor there's a structure that's happening here it's amazing this comes out in PCA like this almost never happens at least in in the stuff that I've worked with for a linear dimensionality reduction technique to reveal a real structure about the task and the way we kind of this it's this itself as an object of study and we're in the beginning stages of trying to understand what this is what it's useful for what it does we view it a little bit as like the first draft or V 0.01 of an RGB for odor or is like an odor space an odour codec right color spaces are really important in vision and without it we wouldn't really be able to have our cameras talk to our computers talk to our display devices right so we need something like that if we're gonna digitize the sense of smell we need a theory or a structure and because odor is something that doesn't exist in outer space it's not universally true it's uniquely planet Earth it's uniquely human taking a data-driven approach might not be that unreasonable it might not be something that Newton or guff could have come across themselves through first principles since evolution had a lot to do with it so that's the embedding space it's kind of a global picture of how odor is structured through the eyes of a fancy neural network but what about local right so I told you about global structure what about local structure I you know I could maybe wave my hands and tell you that nearby molecules smell similar because there's a little clumps of stuff but we can actually go and test that and this is also the task that our indie chemists and flavor and fragrance engage in they say here's my target molecule it's being taken off the market or it's too expensive find me stuff nearby and let's see what its properties are maybe it's less of an allergen maybe it's cheaper maybe it's easier to make so let's first use structure let's use nearest neighbors lookups nearest neighbor lookups using those bag of fragments representations that I showed you the chemo informatics name for this distance is called 10 Emoto distance it's jacquard on the bit base morgan fingerprints and this is kind of the standard way to do lookups in chemistry and let's start with something like dihydrogen so if you look at the structural nearest neighbors it gets little sub fragments right right little pieces of the molecule they all match right but almost none of them actually smell like the target molecule now if you use our GCN features so if you use cosine distance in our embedding space what you get is a lot of molecules that have the same kind of convex hull they look really really similar and they also smell similar we showed this to a fragrance R&D chemist and she said oh those are all bio Isis tears and I was like that's awesome well it's that what's a bio Isis there I have no idea what that is as you said bio isis tears are lateral moves in chemical space that maintain biological activity so the little things you can do to a molecule that make it look almost the same but it's now a different structure and don't mess with its biological activity and to her I and again I'm not an expert in in this specifically these were all kind of lateral moves and chemical space that she would have come up with except for this one she said that's really interesting I wouldn't have thought of that and my colleagues said the highest praise you can ever get from a chemist is high I wouldn't have thought of that so you know that's great we've hit the ceiling so I've showed you we can predict well relative to baselines the embedding space has some really interesting structure both globally and locally and the question now is like well is this all true inside of this you know bubble inside of this task that we designed using a data set that we curated and this is generally really good test to move out of your data set so will this model generalize to new adjacent tasks right and you know this is you talk about transfer learning in this course yet domain adaptation so one of the big tricks in industrial machine learning is transfer learning so train a big model on imagenet and then use that model freeze it and you know take off the top layer that just has the decisions for what the image classes are so like you've got dog and cat maybe you want to predict you know house and car you take off the dog and you know dog and cat layer you put on the house and car layer you just train that last layer it's called fine-tuning or transfer learning they're kind of related that works extremely well in images yes you might hear a fine-tuning or transfer learning there's really no examples of this working in a convincing way to my eye and chemistry so the question is like do we expect this to work at all there's an excuse I like this progression in time there was an xkcd cartoon which is you know alright we would say when a user takes a photo we should check if they're in a national park easy GPS lookup and then we want to check if the photo is of a bird and the response in I think this was like 2011 or something like that is I'll need a research team in five years for fun a team at Flickr made this by fine-tuning an image net model right so is this a bird or is this a park so there's that there was it was a large technological leap in between these two so this this really really works in images but it's unclear if it works in general on graphs or specifically in chemistry and what we did is we took our embeddings we froze the embeddings and we added a random forests on top of it or logistic regression I don't remember the models here there's two main data sets that are kind of the benchmarks and odered the dream will factory challenge and the drought mix data set they are both interesting they both have challenges they're not very large but that's kind of what's the standard in the field we don't have state of the art on both of these through transfer learning to these tasks so this actually really surprised us and it's really encouraged us that we actually learned something fundamental about how humans smell molecular structures so the you know the kind of remaining question to me is like this is all really great you've got a great neural network but I occasionally have to convince chemists to make some of these molecules and a question that often comes up is why should I make this molecule what about this makes it smell like vanilla or popcorn cinnamon and so we'd like to try to open up the innards of the neural network or at least expose what the model is attending to when it's making decisions so we set up a really simple positive control and built some methodology around attribution and I'll show you what we did so the first thing we did is to set up a simple task predict if there's a benzene in a molecule the benzene is a six atom ring where you've got three double bonds it's trivial this task is trivial but there's a lot of ways to cheat on the task because there's any statistical anomalies like benzene Kokura for chlorine you might just say okay look at the chlorine and predict that so we wanted to make sure that our attributions weren't weren't cheating and so we built an attribution method this is something that's being submitted right now and what should come out and what does come out indeed is a lot of weight on the benzene zin no weight elsewhere and we've kind of verified that this is the case across lots of different molecules and when it's not a benzene there's no weight when there is a benzene there's a weight on the benzene and sometimes some leakage and we've improved this this is kind of not our current best at this point so this means we can go look at the actual odors that are in our dataset like garlic garlic's actually really easy to predict you count the number of Sulphurs and if there's a lot it's gonna smell really bad like rotten eggs or sulfurous or garlicky and so this is a bit of a sanity check you'll notice that this guy down here has a sulfur these types of molecules show up and beer a lot they're responsible for a lot of the hop aroma in beers so there's this one is is a like a grapefruit characteristic or something like that and these sulfur's are the sulfur's that eventually contribute to the skunked smell or taste of beer because these molecule can oxidize those sulfurous can then leave the molecule and contribute to a new odor which is less pleasant the Walter a part of the molecule they don't contribute to that sulfuric smell they contribute to the grapefruit smell fatty this one we thought was going to be easy and we showed it to flavor and fragrance experts and they were a little bit astounded that we could predict as well so apparently having a big long chain of you know like a fatty chain is not sufficient for its knowing fatty it turns out that a class of molecules called terpenes this is like the main flavor component in marijuana has incredible diversity and we kind of have like an olfactory phobia on molecules like this so one carbon differences can take something from like coconut to pineapple and we have incredible acuity here because perhaps they're really prevalent in plants and things that are safe to eat right so we might have an O representation of sensitivity to molecules of this kind total speculation I'm on video I guess it probably shouldn't have said that and then vanilla so this is a commercially really interesting class and this is the attributions here have been validated by the intuitions of Rd flavor chemists I can't explain that intuition to you because they don't have it but I got a lot of this so that's the best best as we've we've got at this point we haven't done a formal evaluation of how useful these things are but this is to me a tool in the toolbox of trust building so it's not enough really to build a machine learning model if you want to do something with it you have to solve the sociological and cultural problem of getting it in the hands of people who will use it that is often more challenging than building the model itself so data cleaning is the hardest thing and then convincing people that you should use the thing that you built is the second hardest thing the fancy machine learning stuff is neat but you can you can learn it pretty quickly you're all extremely smart that will not end up being the hardest thing very very quickly and then whiny we don't know what's going on here but this is something that we're investigating in collaboration with with experts so that's kind of the state of the state of things this is really early research we're exploring what it means to digitize the sense of smell and we're starting with the simplest possible task which is why does it molecule smell the way that it does we're using graph neural networks to do this which is a really fun new emerging area of technology in machine learning and deep learning there's really interesting and interpretable embedding space that we're looking into that could be used as a codec for electronic noses or for scent delivery devices and we've got state of the art on the existing benchmarks that's a nice validation that we're doing a good job in the modeling but there's you know there's a lot more there's a lot more to do we really want to test this all right so we want to we want to see if this actually works in human beings that are not part of our historical data set but ones that have really never been part of our evaluation process and that's something we're thinking about right now you also never smell molecules alone that's a very rare it's actually hard to do even if you order seeing them all with those those contaminants can be a bit Challenge so thinking about this in the context of mixtures is a bit of a challenge so what should that representation be is that a weighted set of graphs is it like a meta graph of graphs like I don't actually know how to represent mixtures in an effective way and machine learning models and that's something that we're thinking about and then also the dataset that we have is what we're able to pull together right it's not the ideal data set it's definitely gotten us off the ground but there is no image Annette of sent but there you know there wasn't an image meta vision for a long long time but we want to get a head start on this and so this is something that we're thinking about and investing in as well and again I'm super fortunate to work with incredible people I just want to call them all out here Ben Bryan Kari Emily and Jennifer are absolutely amazing fantastic people they did the actual real work and I you know feel fortunate to be able to represent them here so thanks again for having me and yeah any questions happy to have it answer [Applause] 

This year I figured we could do
something a little bit different and instead of me telling you how great this
class is I figured we could invite someone else from outside the class to do that instead. So let's check this out first. Hi everybody and welcome MIT 6.S191, the official introductory course on deep learning taught here at MIT. Deep learning
is revolutionizing so many fields from robotics to medicine and everything in
between. You'll learn the fundamentals of this field and how you can build some of these
incredible algorithms. In fact, this entire speech and video are not real and were created using deep learning and artificial intelligence. And in this
class, you'll learn how. It has been an honor to speak with you today and I hope you enjoy the course! Hi everybody and welcome to MIT 6.S191. Hi everybody and welcome to MIT 6.S191. The official introductory course on deep learning taught here at MIT! 

good afternoon everyone thank you all for joining us my name is Alexandra Meany and one of the course organizers for six s-191 this is mi t--'s official course on introduction to deep learning and this is actually the third year that we're offering this course and we've got a really good one in store for you this year with a lot of awesome updates so I really hope that you enjoy it so what is this course all about this is a one-week intensive boot camp on everything deep learning you'll get up close and personal with some of the foundations of the algorithms driving this remarkable field and you'll actually learn how to build some intelligent algorithms capable of solving incredibly complex problems so over the past couple years deep learning has revolutionized many aspects of research and industry including things like autonomous vehicles medicine and healthcare reinforcement learning generative modeling robotics and a whole host of other applications like natural language processing finance and security but before we talk about that I think we should start by taking a step back and talking about something at the core of this class which is intelligence what is intelligence well I like to define intelligence as the ability to process information to inform future decisions the field of artificial intelligence is actually building algorithms artificial algorithms to do exactly that bit process information to inform future predictions now machine learning is simply a subset of artificial intelligence or AI that actually focuses on teaching an algorithm how to take information and do this without explicitly being told the sequence of rules but instead learn the sequence of patterns from the data itself deep learning is simply a subset of machine learning which takes this idea one step further and actually tries to extract these patterns automatically from raw data without being needed without the need to for the human to actually come in and annotate these rules that the system needs to learn and that's what this class is all about teaching algorithms how to learn a task from raw data we want to provide you with a solid foundation that so that you can learn how these algorithms work under the hood and with the practical skills so that you can actually implement these algorithms from scratch using deep learning frameworks like tensor flow which is the current most popular deep learning framework that you can code some of neural networks and deep learning model and other deep learning models we have an amazing set of lectures lined up for you this week including today which will kick off an introduction on neural networks and sequence based modeling which you'll hear about in the second part of the class tomorrow we'll cover some about some stuff about computer vision and deep generative modeling and the day after that we'll talk even about reinforcement learning and end on some of the challenges and limitations of the current deep learning approaches and and kind of touch on how we can move forward as a field past these challenges we'll also spend the final two days hearing from some guest lectures from top a AI researchers these are bound to be extremely interesting though we have speakers from Nvidia IBM Google coming to give talks so I highly recommend attending these as well and finally the class will conclude with some final project presentations from students like you and the audience will where you'll present some final projects for this class and then we'll end on an award ceremony to celebrate so as you might have seen or heard already this class is offered for credit you can take this class for grade and if you're taking this class for grade you have two options to fulfill your grade requirement first option is that you can actually do a project proposal where you will present your project on the final day of class that's what I was saying before on Friday you can present your project and this is just a three minute presentation we'll be very strict on the time here and we realized that one week is a super short time to actually come up with a deep learning project so we're not going to actually be judging you on the results you create during this week instead what we're looking for is the novelty of the ideas and how well you can present it given such a short amount of time in three minutes and we kind of think it's like an art to being able to present something in just three minutes so we kind of want to hold you to that tight time schedule and kind of enforce it very tightly just so that you're forced to really think about what is the core idea that you want to present to us on Friday your projects your presentations will be judged by a panel of judges and will be awarding GPUs and some home Google home AI assistants this year we're offering three NVIDIA GPUs each one worth over $1,000 some of you know these GPUs are the backbone of doing cutting-edge deep learning research and it's really foundational or essential if you want to be doing this kind of research so we're really happy that we can offer you these types this type of hardware the second option if you don't want to do the project presentation but you still want to receive credit for this class you can do the second option which is a little more boring in my opinion but you can write a one-page review of a deep learning paper and this will be doing the last day of class and this is for people I don't want to do the project presentation but you still want to get credit for this class please post to Piazza if you have questions about the labs that we'll be doing today or any of the future days if you have questions about the course in general there's course information on the website enter deep learning com along with announcements digital recordings as well as slides for these classes today's slides are already released so you can find everything online and of course if you have any questions you can email us at intro to deep learning - staff at MIT edu this course has an incredible team that you can reach out to in case you have any questions or issues about anything so please don't hesitate to reach out and finally we want to give a huge thanks to all of the sponsors that made this course possible so now let's start with the fun stuff and actually let's start by asking ourselves a question why do we even care about this class why did you all come here today what is why do we care about deep learning well traditional machine learning algorithms typically define sets of rules or features that you want to extract from the data usually these are hand engineered features and they tend to be extremely brittle in practice now the key idea is a key insight of deep learning is that let's not hand engineer these features instead let's learn them directly from raw data that is can we learn in order to detect the face we can first detect the edges in the picture compose these edges together to start detecting things like eyes mouth and nose and then composing these features together to detect higher-level structures in the face and and this is all performed in a hierarchical manner so the question of deep learning is how can we go from raw image pixels or raw data in general to a more complex and complex representation as the data flows through the model and actually the fundamental fundamental building blocks of deep learning have existed for decades and their underlying algorithms have been studied for many years even before that so why are we studying this now well for one data has become so prevalent in today's society we're living in the age of big data where we have more access to data than ever before and these models are hungry for data so we need to feed them with all the data and a lot of this datasets that we have available like computer vision datasets natural language processing datasets this raw amount of data was just not available when these algorithms were created second these algorithms require or these albums are massively parallel lies about their core at their most fundamental building blocks that you'll learn today they're massively paralyzed Abul and this means that they can benefit tremendously from very specialized hardware such as GPUs and again technology like these GPUs simply did not exist in the decades that deep learning or the foundations of deep learning were devil and finally due to open-source tool boxes like tensorflow which will you learn to use in this class building and deploying these models has become more streamlined than ever before it is becoming increasingly and increasingly easy to abstract away all of the details and build a neural network and train a neural network and then deploy that neural network in practice to solve a very complex problem in just tens of lines of code you can solve you can create a facial classifier that's capable of recognizing very complex faces from the environment so let's start with the most fundamental building block of deep learning and that's the fundamental building block that makes up a neural network and that is a neuron so what is the neuron in deep learning we call it a perceptron and how does it work so the idea of a perceptron or a single neuron is very simple let's start by talking about and describing the feed-forward information of information through that model we define a set of inputs x1 through XM which you can see on the left hand side and each of these inputs are actually multiplied by a corresponding weight w1 through WM so you can imagine if you have x1 you x w1 you have x2 you x w2 and so on you take all of those multiplications and you add them up so these come together in a summation and then you pass this weighted sum through a nonlinear activation function to produce a final output which we'll call Y so that's really simple let's I actually left out one detail in that previous slide so I'll add it here now we also have this other turn term this green term which is a bias term which allows you to shift your activation function left and right and now on the right side you can kind of see this diagram illustrated as a mathematical formula as a single equation we can actually rewrite this now using linear algebra using vectors dot products and matrices so let's do that so now is a vector of our inputs x1 through M so instead of now a single number X capital X is a vector of all of the inputs capital W is a vector of all of the weights 1 through m and we can simply take their weighted sum by taking the dot product between these two vectors then we add our bias like I said before or biased now is a single number W not and applying that non linear term so the nonlinear term transfers that transforms that scalar input to another scalar output Y so you might now be wondering what is this thing that I've been referring to as an activation function I've mentioned it a couple times I called it by a couple different names first was a nonlinear function then was an activation function what is it so one common example of a nonlinear activation function is called the sigmoid function and you can see one here defined on the bottom right this is a function that takes as input any real number and outputs a new number between 0 and 1 so you can see it's essentially collapsing your input between this range of 0 and 1 this is just one example of an activation function but there are many many many activation functions used in neural networks here are some common ones and throughout this presentation you'll see these tensorflow code blocks on the bottom like like this for example and I'll just be using these as a as a way to kind of bridge the gap between the theories that you'll learn in this class with some of the tensor flow that you'll be practicing in the labs later today and through the week so the sigmoid function like I mentioned before which you can see on the left-hand side is useful for modeling probabilities because like I said it collapses your your input to between 0 & 1 since probabilities are modeled between 0 & 1 this is actually the perfect activation function for the end of your neural network if you want to predict probability distributions at the end another popular option is the r lu function which you can see on the far right-hand side this function is an extremely simple one to compute it's piecewise linear and it's very popular because it's so easy to compute but has this non-linearity at Z equals zero so at Z less than 0 this function equals 0 and at Z greater than 0 it just equals the input and because of this non-linearity it's still able to capture all of the great properties of activation functions while still being extremely simple to compute and now I want to talk a little bit about why do we use activation functions at all I think a great part of this class is to actually ask questions and not take anything for granted so if I tell you we need an activation function the first thing that should come to your mind is well why do we need that activation function so activation functions the purpose of activation functions is to introduce nonlinearities into the network this is extremely important in deep learning or in machine learning in general because in real life data is almost always very nonlinear imagine I told you to separate here the green from the red points you might think that's easy but then what if I told you you had to only use a single line to do it well now it's impossible that actually makes the problem not only really hard like I said it makes it impossible in fact if you use linear activation functions in a neural network no matter how deep or wide your neural network is no matter how many neurons it has this is the best that I will be able to do produce a linear decision boundary between the red and the green points and that's because it's using linear activation functions when we introduce a nonlinear activation function that allows us to approximate arbitrarily complex functions and draw arbitrarily complex decision boundaries in this feature space and that's exactly what makes neural networks so powerful in practice so let's understand this with a simple example imagine I give you a trains Network with weights W on the top here so W 0 is 1 and let's say W 0 is 1 the W vector is 3 negative 2 so this is a trained neural network and I want to feed in a new input to this network well how do we compute the output remember from before it's the dot product we add our bias and we compute a non-linearity there's three steps so let's take a look at what's going on here what's inside of this nonlinear function the input to the nonlinear function well this is just a 2d line in fact we can actually plot this 2d line in what we call the feature space so on the x axis you can see X 1 which is the first input and on the y axis you can see X 2 which is the second input this neural network has two inputs we can plot the line when it is equal to zero and you can actually see it in the feature space here if I give you a new point a new input to this neural network you can also plot this new point in the same feature space so here's the point negative 1 2 you can plot it like this and actually you can compute the output by plugging it into this equation that we created before this line if we plug it in we get 1 minus 3 minus 4 right which equals minus 6 that's the input to our activation function and then when we feed it through our activation function here I'm using sigmoid again for example our final output is zero point zero zero two ok what does that number mean let's go back to this illustration of the feature space again what this feature space is doing is essentially dividing the space into two hyperplanes remember that the sigmoid function outputs values between 0 and 1 and at z equals 0 when the input to the sigmoid is 0 the output of the sigmoid is 0.5 so essentially you're splitting your space into two planes one where Z is greater than zero and one more Z is less than zero and one where Y is greater than 0.5 and one where Y is less than 0.5 the two are synonymous but when we're dealing with small dimensional input data like here we're dealing with only two dimensions we can make these beautiful plots and these are very valuable and actually visualizing the learning algorithm visualizing how our output is relating to our input we're gonna find very soon that we can't really do this for all problems because while here we're dealing with only two inputs in practical applications and deep neural networks we're gonna be dealing with hundreds thousands or even millions of inputs to the network at any given time and then drawing one of these plots in thousand dimensional space is going to become pretty tough so now that we have an idea of the perceptron a single neuron let's start by building neural networks from the ground up using one neuron and seeing how this all comes together let's revisit our diagram of the perceptron if there's a few things that you remember from this class I want to remember this so there's three steps to computing the output of a perceptron dot product add a bias taking non-linearity three steps let's simplify the diagram a little bit I just got rid of the bias I removed the weights just for simplicity to keep things simple and just note here that I'm writing Z as the input to the to the activation function so this is the weighted combination essentially of your inputs Y is then taking the activation function with input Z so the final output like I said Y is is on the right-hand side here and it's the activation function applied to this weighted sum if we want to define a multi output neural network now all we have to do is add another perceptron to this picture now we have two outputs each one is a normal perceptron like we defined before no nothing extra and each one is taking all the inputs from the left-hand side computing this weighted sum adding a bias and passing it through an activation function let's keep going now let's take a look at a single layered neural network this is one where we have a single hidden layer between our inputs and our outputs we call it a hidden layer because unlike the input and the output which are strictly observable or hidden layers learned so we don't explicitly enforce any behavior on the hidden layer and that's why we call it hidden in that sense since we now have a transformation from the inputs to the hidden layer and hidden layer to the outputs we're going to need two weight matrices so we're going to call it W one to go from input to hidden layer and W two to go from hidden layer to output but again the story here's the same dot product add a bias for each of the neurons and then compute an activation function let's zoom in now to a single hidden hidden unit in this hidden layer if we look at the single unit take z2 for example it is just the same perceptron that we saw before I'm going to keep repeating myself we took a dot product with the inputs we applied a bias and then actually so since it's Z we had not applied our activation function yet so it's just a dot product plus a bias so far if we took it and took a look at a different neuron let's say z3 or z4 the idea here is gonna be the same but we're probably going to end up with a different value for Z 3 and C 4 just because the weights leading from Z 3 to the inputs are going to be different for each of those neurons so this picture looks a little bit messy so let's clean things up a little bit more and just replace all of these hidden layers all these lines between the hidden layers with these symbols these symbols denote fully connected layers where each input to the layer is connected to each output of the layer another common name for these is called dense layers and you can actually write this in tensor flow using just four lines of code so this neural network which is a single layered multi output neural network can be called by instantiating your inputs feeding those inputs into a hidden layer like I'm doing here which is just defined as a single dense layer and then taking those hidden outputs feeding that into another dense layer to produce your outputs the final model is to find it end to end with that single line at the end model of inputs and outputs and that just essentially connects the graph and to end so now let's keep building on this idea now we want to build a deep neural network what is the deep neural network well it's just one where we keep stacking these hidden layers back to back to back to back to create increasingly deeper and deeper models one where the output is computed by going deeper into the network and computing these weighted sums over and over and over again with these activation functions repeatedly applied so this is awesome now we have an idea on how to actually build a neural network from scratch going all the way from a single perceptron and we know how to compose them to create very complex deep neural networks as well let's take a look at how we can apply this to a very real problem that I know a lot of you probably care about so I was thinking of a problem potential that some of you might care about it took me a while but I think this might be one so at MIT we care a lot about passing our classes so I think a very good example is let's train a neural network to determine if you're gonna pass your class so to do this let's start with a simple two input feature model one feature is the number of lectures that you attend the other feature is the number of hours that you spend on the final project again since we have two inputs we can plot this data on a feature map like we did before green points here represent previous students from previous years that pass the class red points represent students that failed the class now if you want to find out if you're gonna pass or fail to class you can also apply yourself on this map you spent you came to four lectures spend five hours on your final project and you want to know if you're going to pass or fail and you want to actually build a neural networks that's going to learn this look at the old the the previous people that took the scores and determine if you all pass or fail as well so let's do it we have two inputs one is four one is five these are fed into a single layered neural network with three hidden units and we see that the final output probability that you will pass this class is 0.1 or 10% not very good that's actually really bad news can anyone guess why this person who actually was in the part of the feature space it looked like they were actually in a good part of this feature space looked like they were gonna pass the class why did this neural network give me such a bad prediction here yeah exactly so the network was not trained essentially this network is like a baby that was just born it has no idea of what lectures are it doesn't know where final labs are it doesn't know anything about this world it's these are just numbers to it it's been randomly initialized it has no idea about the problem so we have to actually train it we have to teach it how to get the right answer so the first thing that we have to do is tell the network when it makes a mistake so that we can correct it in the future now how do we do this in neural networks the loss of a network is actually what defines when the network makes the wrong prediction it takes the input and the predicted output sorry it takes as input the predicted output and the ground truth actual output if your predicted output and your ground truth output are very close to each other then that essentially means that your loss is going to be very low you didn't make a mistake but if your ground truth output is very far away from your predicted output that means that you should have a very high loss you just have a lot of error and your network should correct that so let's assume that we have data not just from one student now but we have data from many many different students passing and failing the class we now care about how this model does not just on that one student but across the entire population of students and we call this the empirical loss and that's just the mean of all of the losses for the individual students we can do it by literally just computing the mean sorry just computing the loss for each of these students and taking their mean when training a network what we really want to do is not minimize the loss for any particular student but we want to minimize the loss across the entire training set so if we go back to our problem on path predicting if you'll pass or fail to class this is a problem of binary classification your output is 0 or 1 we already learned that when outputs are 0 or 1 you're probably going to want to use a soft max output for those of you who aren't familiar with cross entropy this was an idea introduced actually at MIT and a master's thesis here over 50 years ago it's widely used in different areas like thermodynamics and we use it here in machine learning as well it's used all over information theory and what this is doing here is essentially computing the loss between this zero one output and the true output that the student either passed or failed to class let's suppose instead of computing a zero one output now we want to compute the actual grade that you will get on the class so now it's not 0-1 but it's actually a grade it could be any number actually right now we want to use a different loss because the output of our net of our neural network is different and defining losses is actually kind of one of the arts in deep learning so you have to define the questions that you're asking so you can define the loss that you need to optimize over so here in this example since we're not optimizing over zero one loss we're optimizing over any real number we're gonna use a mean squared error loss and that's just computing the squared error so you take the difference between what you expect the output to be and what you're actually output was you take that difference you square it and you compute the mean over your entire population okay great so now let's put some of this information together we've learned how to build neural networks we've learned how to quantify their loss now we can learn how to actually use that loss to iteratively update and train the neural network over time given some data and essentially what this amounts to what this boils down to is that we want to find the weights of the neural network W that minimize this empirical loss so remember again the empirical loss is the loss over the entire training set it's the mean loss of all of the popular of all of the individuals in the training set and we want to minimize that loss and that essentially means we want to find the weights the parameterization of the network that results in the minimum loss remember again that W here is just a collection it's just a set of all of the weights in the network so before I define W as W 0 W 1 which is the weights for the first layer second layer third layer etc and you keep stacking all of these weights together you combine them and you want to compute this optimization problem over all of these weights so again remember our loss function what does our loss function look like it's just a simple function that takes as inputs our weights and if we have two weights we can actually visualize it again we can see on the x-axis one way so this is one scaler that we can change and another way on the y axis and on the z axis this is our actual loss if we want to find the lowest point in this landscape that corresponds to the minimum loss and we want to find that point so that we can find the corresponding weights that were set to achieve that minimum loss so how do we do it we use this technique called loss optimization through gradient descent we start by picking an initial point on this landscape an initial w0 w1 so here's this point this black cross we start at this point we compute the gradient at this local point and in this landscape we can see that the gradient tells us the direction of maximal ascent now that we know the direction of the maximal ascent we can reverse that gradient and actually take a small step in the opposite direction that moves us closer towards the lowest point because we're taking a greedy approach to move in the opposite direction of the gradient we can iteratively repeat this process over and over and over again we computing the gradient at each time and keep moving moving closer towards that lowest minimum we can summarize this algorithm known as gradient descent in pseudocode by this the pseudocode on the left-hand side we start by initializing our weights randomly computing this gradient DJ DW then updating our weights in the opposite direction of that gradient we used this small amount ADA which you can see here and this is essentially what we call the learning rate this is determining how much of a step we take and how much we trust that with that gradient update that we computed we'll talk more about this later but for now let's take a look at this term here this gradient DJ DW is actually explaining how the lost changes with respect to each of the weights but I never actually told you how to compute this term this is actually a crucial part of deep learning and neural networks in general computing this term is essentially all that matters when you try and optimize your network is the most computational part of training as well and it's known as back propagation we'll start with a very simple network with only one hidden input sorry with one input one hidden layer one handed and unit and one output computing the gradient of our loss with respect to W to corresponds to telling us how much a small change in our and W two affects our output or loss so if we write this as a derivative we can start by computing this by simply expanding this derivative into a chain by using the chain rule backwards from the loss through the output and that looks like this so DJ DW 2 becomes DJ dy dy DW 2 ok and that's just a simple application of the chain rule now let's suppose instead of computing DJ DW 2 we want to compute DJ DW 1 so I've changed the W 1 the W 2 to a W 1 on the left hand side and now we want to compute this well we can simply apply the chain rule again we can take that middle term now expand it out again using the same chain rule and back propagate those gradients even further back in in the network and essentially we keep repeating this for every weight in the network using the gradients for later layers to back propagate those errors back into the original input we do this for all of the weights and and that gives us our gradient for each weight yeah you're completely right so the question is how do you ensure that this gives you a global minimum instead of a local minimum right so you don't we have no guarantees on that this is not a global minimum the whole training of stochastic gradient sent is a greedy optimization algorithm so you're only taking this greedy approach and optimizing only a local minimum there are different ways extensions of stochastic gradient descent that don't take a greedy approach they take an adaptive approach they look around a little bit these are typically more expensive to compute stochastic gradient side is extremely cheap to compute in practice and that's one of the reasons it's used the second reason is that in practice local minimum tend to be sufficient so that's the back propagation algorithm in theory it sounds very simple it's just an application of the chain rule but now let's touch on some insights on training these neural networks in practice that makes it incredibly complex and this gets back to that that previous point that previous question that was raised in practice training neural networks is incredibly difficult this is a visualization of the lost landscape of a neural network in practice this is a paper from about a year ago and the authors visualize what a deep neural network lost landscape really looks like you can see many many many local minimum here lot minimizing this loss and finally the optimal true minimum is extremely difficult now recall the update equation that we fought defined for a gradient descent previously we take our weights and we subtract we move towards the negative gradient and we update our weights in that direction I didn't talk too much about this parameter heydo this is what we called the learning rate I briefly touched on it and this is essentially determining how large of a step we take at each iteration in practice setting the learning rate can be extremely difficult and actually very important for making sure that you avoid local minima again so if we set the learning rate to slow then the model may get stuck in local minimum like this it could also converge very slowly even in the case that it gets to a global minimum if we set the learning rate too large the gradients essentially explodes and we diverge from the loss itself and it's also been setting the learning rate to the correct amount can be extremely tedious in practice such that we overshoot some of the local minima get ourselves into a reasonable local global minima and then converge in within that global minima how can we do this in a clever way so one option is that we can try a lot of different possible learning rates see what works best in practice and in practice this is actually a very common technique so a lot of people just try a lot of learning rates and see what works best let's see if we can do something a bit smarter than that as well how about we design an adaptive algorithm that learnt that you that adapts its learning rate according to the lost landscape so this can take into account the gradient at other locations and loss it can take into account how fast we're learning how how large the gradient is at that location or many other options but now since our learning rate is not fixed for all of the iterations of gradient descent we have a bit more flexibility now in learning in fact this has been widely studied as well there are many many different options for optimization schemes that are present in tensorflow and here are examples of some of them during your labs I encourage you to try out different of these different ones of these optimizers and see how they're different which works best which doesn't work so well for your particular problem and they're all adaptive in nature so now I want to continue talking about tips for training these networks in practice and focus on the very powerful idea of batching gradient descent and batching your data in general so to do this let's revisit this idea of gradient descent very quickly so the gradient is actually very computational to compute this back propagation algorithm if you want to compute it for all of the data samples in your training data set which may be massive in modern data sets it's essentially amounting to a summation over all of these data points in most real life problems this is extremely computational and not feasible to compute on every iteration so instead people have come up with this idea of stochastic gradient descent and that involves picking a single point in your data set computing the gradient with respect to that point and then using that to update your grade to update your your weights so this is great because now computing a gradient of a single point is much easier than computing the gradient over many points but at the same time since we're only looking at one point this can be extremely noisy sure we take a different point each time but still when we move and we take a step in that direction of that point we may be going in in a step that's not necessarily representative of the entire data set so is there a middle ground such that we don't have to have a stochastic a stochastic gradient but we can still be kind of computationally efficient in the sense so instead of computing a noisy gradient of a single point let's get a better estimate by batching our data into mini batches of B data points capital B data points so now this gives us an estimate of the true gradient by just averaging the gradient from each of these points this is great because now it's much easier to compute than full gradient descent it's a lot less points typically B is on the order of less than 100 or approximately in that range and it's a lot more accurate than stochastic gradient descent because you're considering a larger population as well this increase in gradient accuracy estimation actually allows us to converge much quicker as well because it means that we can increase our learning rate and trust our gradient more with each step which ultimately means that we can train faster this allows for massively parallel lyza become potations because we can split up batches across the GPU send batches all over the GPU compute their gradients simultaneously and then aggregate them back to even speed up even further now the last topic I want to address before ending is this idea of overfitting this is one of the most fundamental problems in machine learning as a whole not just deep learning and at its core it involves understanding the complexity of your model so you want to build a model that performs well and generalized as well not just to your training set but to your test set as well assume that you want to build a model that describes these points you can go on the left-hand side which is just a line fitting a line through these points this is under fitting the complexity of your model is not large enough to really learn the full complexity of the data or you can go on the right-hand side which is overfitting where you're essentially building a very complex model to essentially memorize the data and this is not useful either because when you show a new data it's not going to sense it's not going to perfectly match on the training data and it means that you're going to have high generalization error ideally we want to end up with a model in the middle that is not too complex to memorize all of our training data but still able to generalize and perform well even we have when we have brand new training and testing inputs so to address this problem let's talk about regularization for deep neural networks deep neural regularization is a technique that you can introduce to your networks that will discourage complex models from being learned and as before we've seen that it's crucial for our models to be able to generalize to data beyond our training set but also to generate generalize to data in our testing set as well the most popular regularization technique in deep learning is a very simple idea called dropout let's revisit this and a picture of a deep neural network again and drop out during training we randomly set some of our activations of the hidden neurons to 0 with some probability that's why we call it dropping out because we're essentially killing off those neurons so let's do that so we kill off these random sample of neurons and now we've created a different pathway through the network let's say that you dropped 50 percent of the neurons this means that those activations are set to zero and the network is not going to rely too heavily on any particular path through the network but it's instead going to find a whole ensemble of different paths because it doesn't know which path is going to be dropped out at any given time we repeat this process on every training iteration now dropping out a new set of 50 50 % of the neurons and the result of this is essentially a model that like I said creates an ensemble of multiple models through the paths of the network and is able to generalize better to unseen test data so the second technique for a regularization is this notion that we'll talk about which is early stopping and the idea here is also extremely simple let's train our neural network like before no dropout but let's just stop training before we have a chance to overfit so we start training and the definition of overfitting is just when our model starts to perform worse on the test set then on the training set so we can start off and we can plot how our loss is going for both the training and test set we can see that both are decreasing so we keep training now we can see that the training the validation both losses are kind of starting to plateau here we can keep going the training loss is always going to decay it's always going to keep decreasing because especially if you have a network that is having such a large capacity to essentially memorize your data you can always perfectly get a training accuracy of 0 that's not always the case but in a lot of times with deep neural networks since they're so expressive and have so many weights they're able to actually memorize the data if you let them train for too long if we keep training like you see the training set continues to decrease now the validation set starts to increase and if we keep doing this the trend continues the idea of early stopping is essentially that we want to focus on this point here and stop training when we reach this point so we can key basically records of the model during training and once we start to detect overfitting we can just stop and take that last model that was still occurring before the overfitting happened right so on the left hand side you can see the under fitting you don't want to stop too early you want to let the model get the minimum validation set accuracy but also you don't want to keep training such that the validation accuracy starts the increase on the other end as well so I'll conclude this first lecture by summarizing three key points that we have covered so far first we learned about the fundamental building blocks of deep learning which is just a single neuron or called the perceptron we learned about back propagation how to stack these neurons into complex deep neural networks how to back propagate and errors through them and learn complex loss functions and finally we discussed some of the practical details and tricks to training neural networks that are really crucial today if you want to work in this field such as batching regularization and and others so now I'll take any questions or if there are no questions and I'm gonna hand the mic over to ovah who will talk about sequence modeling thank you [Applause] 

hi everyone my name is Alvin and welcome to our second lecture on deep sequence modeling so in the first I was lecture Alexander talked about the essentials of neural networks and feed-forward models and now we're going to turn our attention to applying neural networks to problems which involve sequential processing of data and why these sorts of tasks require a different type of network architecture from what we've seen so far so before we dive in I like to start off with a really simple example suppose we have this picture of a ball and we want to predict where it will travel to next without any prior information about the ball's history any guess on its next position will be exactly that just a guess however if in addition to the current location of the ball I also gave you a history of its previous locations now the problem becomes much easier and I think we can all agree that we have a pretty clear sense of where the ball is going to next so this is a really really simple sequence modeling problem given this this image this you know a thought experiment of a balls travel through space can we predict where it's going to go next but in reality the truth is that sequential data is all around us for example audio can be split up into a sequence of sound waves while text can be split up into a sequence of either characters or words and beyond these two Ubiquiti examples there are many more cases in which sequential processing may be useful from analysis of medical signals like EKGs to predicting stock trends to processing genomic data and now that we've gone in the sense of what sequential data looks like I want to turn our attention to another simple problem to to motivate motivate the types of networks that we're going to use for this task and in this case suppose we have a language model where we're trying to Train neural network to predict the next word in a phrase or a sentence and suppose we have this sentence this morning I took my cat for a walk yes you heard and read that right this morning I took my cat for a walk and let's say we're given these words this morning I took my cat for a and we want to predict the next word in the sequence and since this is a class on deep learning we're going to try to build a deep neural network like a feed-forward Network from our first lecture to do this and one problem that we're immediately going to run into is that our feed-forward network can only take a fixed length vector as its input and we have to specify this size of this input right at the start and you can imagine that this is going to be a problem for our task in general because sometimes we'll have a sentence we'll have seen five words sometimes seven words sometimes ten words and we want to be able to predict what comes next so fundamentally we need a way to handle variable length input and one way we can do this is to use this idea of a fixed window to force our input vector to be a certain length in this case - and this means that no matter where we're trying to make our prediction we just take the previous two words and try to predict the next word and we can represent these two words as a fixed length vector where we take a larger vector allocate some space for the first word some space for the second word and encode the identity of each word in that vector but this is problematic because because of the fact that we're using this fixed window we're giving ourselves a limited history which means that we can't effectively model long term dependencies in our input data which is important in sentences like this one where we clearly need information from much earlier in the sentence to accurately predict the next word if we were only looking at the past two words or the past three words or the past five words even we wouldn't be able to make this prediction being the the word French so we need a way to integrate information from across the sentence but also still represent the input as a fixed length vector and another way we could do this is by actually using the entire sequence but representing it as a set of counts and this representation is what's called a bag of words where we have some vector and each slot in this vector represents a word and the value that's in that slot represents the number of times that that word appears in this sentence and so we have a fixed length vector over some vocabulary of words regardless of the length of the input sentence but the counts are just going to be different and we can feed this into our feed-forward neural network to generate a prediction about the next word and you may have already realized that there's a big problem with this approach in using counts we've just completely abolished all sequence information and so for example these two sentences with completely opposite semantic meanings would have the exact same bag of words representation same words same counts so obviously this isn't going to work another idea could be to simply extend our first idea of a fixed window thinking by thinking that by looking at more words we can get most of the context we need and so we can represent our sentence in this way right just a longer fixed window feed it into our feed-forward model and make a prediction and if we were to feed this vector into a feed-forward neural network each of these inputs each 0 or 1 in the vector would have a separate weight connecting it to the network and so if we repeatedly were to see the words this morning at the beginning of the sentence the neural network would be able to learn that this morning represents a time or a setting but if in another sentence this morning were to now appear at the end of that sentence the network is going to have difficulty recognizing that this morning actually means this morning because the that see the end of the vector have never seen that phrase before and the parameters from the beginning haven't been shared across the sequence and so at a higher level what this means is that what we learn about the sequence at one point is not going to transfer anywhere to anywhere else in the sequence if we use this representation and so hopefully by by walking through this I've motivated that why a traditional feed-forward neural network is not really well suited to handle sequential data and this simple example further motivates a concrete set of design criteria that we need to keep in mind when thinking about building a neural network for sequence modeling problems specifically our network needs to be able to handle variable length sequences be able to track long term dependencies in the data maintain information about the sequence order and share the parameters it learns across the entirety of the sequence and today we're going to talk about using recurrent neural networks or RN ends as a general framework for sequential processing and sequence modeling problems so let's go through the general principle behind RN ends and explore how they're a fundamentally different architecture from what we saw in the first lecture so this is a abstraction of our standard feed-forward neural network and in this architecture data propagates in one direction from input to output and we already motivated why a network like this can't really handle sequential data RNs in contrasts are really well-suited for handling cases where we have a sequence of inputs rather than a single input and they're great for problems like this one in which a sequence of data is propagated through the model to give a single output for example you can imagine training a model that takes as input a sequence of words and outputs a sentiment that's associated with that phrase or that sentence alternatively instead of returning a single output could also train a model where we take in a sequence of inputs propagate them through our recurrent neural network model and then return an output at each time step in the sequence and an example of this would be in text or music generation and you'll get a chance to explore this type of model later on in the lab and beyond these two these two examples there are a whole host of other recurrent neural network arctor architectures for sequential processing and they've been applied to a range of problems so what fundamentally is a recurrent neural network as I mentioned before to reiterate our standard vanilla feed-forward neural network we're going from input to output in one direction and this fundamentally can't maintain information about sequential data our Nan's on the other hand are networks where they have these loops these loops in them which allow for information to persist so in this diagram our RNN takes as input this vector X of T outputs a value like a prediction Y hat of T but also makes this computation to update an internal state which we call H of T and then passes this information about its state from this step of the network to the next and we call these networks with loops in them recurrent because information is being passed internally from one time step to the next so what's going on under the hood how is information being passed our nuns use a simple recurrence relation in order to process sequential data specifically they maintain this internal state H of T and at each time step we apply a function parametrized by a set of weights W to update this state based on both the previous state H of T minus 1 and the current input X of T and the important thing to know here is that the same function and the same set of parameters are used at every time step and this addresses that important design criteria from earlier of why it's useful to share parameters in the context of sequence modeling to be more specific the RNN computation includes both a state update as well as the output so given our input vector we apply some function to update the hidden state and as we saw in the first lecture this function is a standard neural net operation that consists of multiplication by a weight matrix and applying a non linearity but in this case since we both have the input vector X of T as well as the previous state H of T minus 1 as inputs to our function we have two weight matrices and we can then apply our non-linearity to the sum of these two terms finally we generate an output at a given time step which is a transformed version of our internal state the foes from a multiplication by a separate weight matrix so so far we've seen our n ends as depicted as having these loops that feedback back in on themselves another way of thinking about the RNN can be in terms of unrolling this loop across time and if we do this we can think of the RNN as multiple copies of the same network where each copy is passing a message onto its descendant and continuing this this scheme throughout time you can easily see that our n ends have this chain like structure which rely really highlights how and why they're so well suited for processing sequential data so in this representation we can we can make our weight matrices explicit beginning with the weights that transform the inputs to the hidden state transform the previous hidden se to the next hidden safe and finally transform the hidden sate to the output and it's important once again to note that we are using the same weight matrices at every time step and from these outputs we can to loss at each time step and this completes our what is called our forward pass through the network and finally to define the total loss we simply sum the losses from all the individual time steps and since our total loss consists of these individual contributions over time this means that training the network will also have to involve some time component okay so in terms of in terms of actually training our Nets how can we do this and the algorithm that's used is an extension of the back propagation idea that alexander introduced in the first lecture and it's called back propagation through time so to remind you let's let's think back to how we trained feed-forward models given our given our inputs we first make a forward pass through the network going from input to output and then back propagate gradients back through the network taking the derivative of the loss with respect to each parameter in the network and then tweaking our parameters in order to minimize the lost for our n ends our forward pass through the network consists of going forward across time updating the cell state based on the input and the previous state generating an output at each time step computing a loss I each time set and then finally summing these individual losses to get the total loss and what this means is that instead of back propagating errors through a single feed-forward network at a single time step in our n ends errors are back propagated at each individual time step and then across time steps all the way from where we are currently to the very beginning of the sequence and this is the reason why it's called back propagation through time because as you can see all the errors are flowing back in time to the beginning of our data sequence and so if we take a closer look at how gradients flow across this chain of repeating modules you can see that in between these these time steps in doing this back propagation we have this factor whh which is a matrix and this means that in each step we have to perform a matrix multiplication that involves this way matrix W and furthermore each cell say update results from a nonlinear activation and what this means is that in computing the gradient in an RNN the derivative of the loss with respect to our initial state H naught we have to make many matrix multiplications that involve the weight matrix as well as repeated use of the derivative of the activation function why might this be problematic well if we consider these multiplication operations if many of these values are greater than one what we can encounter is what's called this exploding gradient problem where our gradients become extremely large and we can't do any optimization and to combat this one thing that's done is in practice is called gradient clipping which basically means you scale back your gradients when they become too large and this is a really good practical option especially when you have a network that's not too complicated with and doesn't have many parameters on the flip side we can also have the opposite problem where if our matrix values are too small we can encounter what's called the vanishing gradient problem and it's really the motivating factor behind the most widely used RNN architectures and today we're gonna address three ways in which we can alleviate the vanishing gradient problem by changing the activation function that's used being clever about how we initialize the weights in our network and finally how we can fundamentally change the RNN architecture to combat this and so before we go into that let's take a step back and try to establish some more intuition as to why vanishing gradients are such a big problem so imagine you have a number right and you keep multiplying that number by something in between zero and one that number is going to keep shrinking and shrinking and eventually it's going to vanish when this happens two gradients this means it's going to be harder and harder to propagate errors further back into the past because the gradients are going to become smaller and smaller and this means that during training will end up biasing our network to capture short term dependencies which may not always be a problem sometimes we only need to consider very recent information to perform our tasks of interest so to make this concrete right let's go back to our example from the beginning of the lecture a language model we're trying to predict the next word in a phrase so in this case if we're trying to predict the last word in the phrase the clouds are in the blank it's pretty obvious what the next word is going to be and there's not much of a gap between the relevant information like the word cloud and the place where the prediction is needed and so an a standard RNN can use the past information to make the prediction but there can be other cases where more context is necessary like in this example more recent information suggests that the next word is most likely the name of a language but to identify which language we need information from further back the context of France and in many cases the gap between what's relevant and the point where that information is needed can become really really large and as that grid gap grows standard rnns become increasingly unable to connect the information and that's all because of the vanishing gradient problem so how can we alleviate this the first trick is pretty simple we can change the activation function the network uses and specifically both the 10h and sigmoid activation functions have derivatives less than one pretty much everywhere right in contrast if we use a rel ooh activation function the derivative is one four four whenever X is greater than zero and so this helps prevent the value of the derivative from shrinking our gradients but it's only true for when for when X is greater than zero another trick is to be smart in terms of how we in the initialize parameters in our network by initialing our weights to the identity matrix we can help prevent them from shrinking to zero too rapidly during back back propagation the final and most robust solution is to use a more complex type of recurrent unit that can more effectively track long term dependencies by controlling what information is passed through and what's used to update the cell state specifically we'll use what we call gated cells and there are many types of these gated cells that exist and today we'll focus on one type of gated cell called a long short-term memory network or LS TMS for short which are really good at learning long-term dependencies and overcoming this vanishing gradient problem and Ellis hams are basically the gold standard when it comes to building RN ends in practice and they're very very widely used by the deep learning community so to understand what makes LS am special right let's think back to the general structure of an RN n all recurrent neural networks have this form of a series of repeating modules right the RN n being unrolled across time and in a standard RN n the repeating module contains one computation node in this case it's a 10 8 10 H layer LS PMS also have this chain like structure but the repeating module is slightly more complex and don't get too frightened hopefully by you know what these flow diagrams mean we'll walk through except bicep but the key idea here is that the repeating unit in an LS TM contains these different interacting layers that control the flow of information the first key idea behind Alice hands is they maintain an internal cell state which will denote C of T in addition to the standard are n n say H of T and this cell state runs through throughout the chain of repeating modules and as you can see there are only a couple of simple linear interactions this is a point wise multiplication and this is addition that update the value of C of T and this means that it's really easy for information to flow along relatively unchanged the second key idea that L stems use is that they use these structures called gates to add or remove information to the cell state and gates consists of a sigmoid neural net layer followed by a point wise multiplication so let's take a moment to think about what these gates are doing this sigmoid function is special because it's forcing the input to the gate to be between 0 and 1 and intuitively you can think of this as capturing how much of the input should be passed through the gate if it's 0 don't pass any of that information through the gate if it's 1 pass all the information through the gate and so this regulates the flow of information through the LS TM so now you're probably wondering ok these lines look really complicated what's how do these LS Siam's actually work thinking of the lsdm operations at a high level it boils down to three key steps the first step in the LS TM is to decide what information is going to be thrown away from the prior cell state forget irrelevant history right the next step is to take both the prior information as well as the current input process this information in some way and then selectively update the cell state and our final step is to return an output and for this Alice hams are going to use an output gate to return a transformed version of the cell state so now that we have a sense of these three key lsdm operations forget update output let's walk through each step by step to get a concrete understanding of how these computations work and even though we're gonna walk through this I really want you to keep in mind the high-level concepts of each operation because that's what's important in terms of establishing the intuition behind how Alice teams work and we'll go again go back to our language model example that we've been using throughout this lecture where we're trying to predict the next word in a sequence so our first task is to identify what past information is relevant and what's irrelevant and we achieve this using a sigmoid layer called the forget gate f of T right and F of T is parametrized by a sets a set of weights and biases just like any neural network layer and this layer looks at the previous previous information H of T minus one as well as the input X of T and then outputs a number between zero and one between completely forgetting that information and completely keeping that information and then passes it along this decision so in our language model example the cell state might have included some information about the gender pronoun of a subject in a sentence for example so you can imagine updating the LST M to forget the gender pronoun of a sentences past subject once it encounters a new subject in that set in that sentence our second step is to decide what new information is going to be stored in our updated cell state and to actually execute that update so there are two steps to this the first is a sigmoid layer which you can think of as gating the input which identifies what values we should update secondly we have a tan H layer that generates a new vector of candidate values that could be added to the state and in our language model we may to add the gender of a new subject in order to replace the gender of the old subject now we can actually update our old cell states EFT minus one into the new cell states EFT our previous two steps decided what we should do now it's about actually executing that so to perform this update we first multiply our old cell state C of t minus one by our forget state or forget gate F of T this forgets what we decided to forget right we can then add our our set of new candidate values scaled by the input gate to selectively update each state value and so in our language model example this means that we're dropping information about the old subjects gender and then adding the new information finally we just need to decide what we're going to output and actually output it and what we are going to output H of T is going to be a filtered version of our internal state that we've been maintaining and updating all along so again we're going to use a sigmoid layer to gate where we're going to output and we put our recently updated cell state C of T through through a tan H layer and then multiply this by the output of the sigmoid gate essentially this amounts to transforming the updated South State using that tan H and then dating it and in our language model for example you may want to output information that relates to a verb for example if we've just seen the subject a new subject in the sentence so this gives us a sense of the internal workings of the LS TM but if there's one thing I that you take away right it's sort of those three high-level operations of the LS TM forget old information update the cell state and output a filtered version right but to really appreciate how the LS TM helps overcome the vanishing gradient problem let's consider the really in ship between C of T and C of T minus one right when we back propagate from C of T our current cell state right to C of t minus one what you'll notice is that we only have to perform elementwise multiplication and an addition and doing this back propagation there's no matrix multiplication that's involved and that's entirely because we're maintaining this separate cell state C of T apart from H of T and that C of T is only involved in really simple computations and so when you link up these repeating LS p.m. units in a chain what you'll see is that you get this completely uninterrupted gradient flow unlike in a standard RNN where you have to do repeated matrix multiplications and this is really great for training purposes and for overcoming the vanishing gradient problem so to recap the key ideas behind LS CMS we maintain a separate cell state from what's outputted we use gates to control the flow of information first forgetting what's irrelevant selectively updating the cell state based on both the past history and the current input and then outputting some filtered version of what we just computed and this this maintain this maintenance of this separate cell state allows for simple back propagation with uninterrupted gradient flow so now that we've gone through the fundamental workings of our n ends back propagation through time the vanishing and exploding gradient problems and the STM architecture I'd like to close by considering three really concrete examples of how to use our nuns let's first imagine we're trying to learn a RN n to predict the next musical note and to use this model to generate brand new musical sequences so you can imagine inputting a sequence of notes right producing an output at each time step where our output at each time step is what we think is the next note in the sequence right and if you train this model like this you can actually use it to generate brand new music that's never been heard before and so for example [Music] right you get the idea this sounds like classical music right but in reality this was music that was generated by a recurrent neural network that trained on piano pieces from Chopin and after the training process was asked okay now generate some some new music based on what it is you've learned and you can see right this sounds like extremely realistic you may not have been able to tell that this was music generated by a machine unless maybe you're you're an expert piano aficionado and you'll actually get some practice with building a model to do exactly this in today's lab where you'll be training an RNN to generate brand new Irish folk music that has never been heard before as another cool example where we're going from an input sequence to just a single output we can train an RNN to take as input words in a sentence and actually output the sentiment or the feeling of that particular sentence either positive or negative so for example if we if we were to train a model like this on a set of tweets we could train our RNN to predict that this wonderful first tweet about our class success 191 has a really positive sentiment which hopefully you agree with but that this other tweet about the weather is actually negative the final example I'll briefly talk about is one of the most powerful and widely used applications of Arlen's in industry and it's the backbone of Google's Translate algorithm and that's machine translation where you input a sentence in one language and train an RNN to output a sentence in a new language and this is done by having an encoder that encodes encodes their original sentence into a state vector and a decoder which decodes that state vector into a new language but there's a big problem in in the approach as depicted here and that's the fact that this entire original sentence needs to be encoded in a single vector that's passed from the encoder to the decoder and this is a huge bottleneck when you're considering large bodies of text that you're trying to translate and actually you know researchers devised a clever way to get around this problem which is this idea of attention and the basic idea here is that instead of the decoder only having access to the final encoded state and now has access to each of these states after each of the steps in the original sentence and the actual weighting of these vectors from encoder to decoder is learned by the network during training and this this technique is called attention because when the network learns this waiting its placing its attention on different parts of the input sequence and in this sense you can think of it as actually capturing a sort of memory of the original sentence so hopefully you've gotten a sense of how our ends work and why they're so powerful for sequential processing we've discussed why they're so well-suited for sequential modeling tasks seen how to define their operation using this recurrence relation how to train them using back propagation through time and also looked at how gated cells can let us model long-term dependencies and finally we discussed three concrete applications of RN ends and so this concludes right the lecture portion of our first day of six s-191 and we're really excited now to transition to the lab portion which as I mentioned is going to mostly focus on training and RNN to generate brand new music and so the the lab is going to be broken down into two parts but sorry but before I go into the specifics of getting started with the labs I'd like to take a few minutes pause for those of you who you know plan to stick around for the labs we're happy to have you we're also happy to address any questions that you may have about either lecture one or lecture two here at the front and we'll just take a 5-minute break or so to orient ourselves and get set up for the lab thank you [Applause] 

all right so let's get started so thank you all for coming to day two of six s-191 we're really excited to to have you for these two lectures and lab today which are largely going to be focused on deep learning for computer vision so to motivate I think we can all agree that vision is one of the most important human senses and sighted people rely on vision quite a lot for everything from navigating in the physical world to recognizing and manipulating objects to interpreting facial expressions and understanding emotion and I think it's safe to say that for many of for all of us or many of us vision is a huge part of our lives and that's largely thanks to the power of evolution evolutionary biologists traced the origins of vision back 540 million years ago to the Cambrian explosion and the reason that vision seems so easy for us as humans is because we have 540 million years of data that evolution has effectively trained on and if you compare that to other capabilities like bipedal movement and language the difference is is quite significant and starting in the 1960s there was a surge of interest in both the neural basis of vision and how to systematically characterize visual processing and this led to computer scientists beginning to wonder about how findings in neuroscience could be applied to achieve artificial computer vision and it all started with these series of seminal experiments from two neuroscientists David Hubel and Torsten vessel who were working at Harvard at the time and they were looking at processing in the visual cortex of cats and what they were able to demonstrate was that their neural mechanisms for spatially invariant pattern recognition and that certain neurons in the visual cortex respond very specifically to specific patterns and regions of visual stimuli and furthermore that there's an exquisite hierarchy of neural layers that exist within the visual cortex and these concepts have transformed both neuroscience and artificial intelligence alike and today we're going to learn about how to use deep learning to build powerful computer vision systems that have been have been shown to be capable of extraordinary complex computer vision tasks so now that we've gone in a sense at a very high level of why this is important and sort of how our brains may process visual information we can turn our attention to what computers see how does a computer process an image so well to a computer images are just numbers so suppose we have this picture of Abraham Lincoln it's made up of pixels and since this is a grayscale image each of these pixels is just a single number and we can represent this image as a 2d matrix of numbers one for each pixel in the image and this is how a computer sees this image likewise if we were to have a RGB color image not grayscale we can represent that with a 3d array where now we have two D matrices for each of the channels are g and b so now that we have a way to represent images to computers we can think about what types of computer computer vision tasks we can perform and two very common types of tasks and machine learning broadly are those of regression and those of classification in a regression our output takes a continuous value while in classification our output takes a single class label so for example let's consider the task of image classification say we want to predict a single label for some image and let's say we have a bunch of images of US presidents and we want to build a classification pipeline to tell us which President is in an image outputting the probability that that image is of a particular President and so you can imagine right that in order to cry classified these images our pipeline needs to be able to tell what is unique about a picture of Lincoln versus a picture of Washington versus a picture of Obama and another way to think about this problem at a high level is in terms of the features that are characteristic of a particular class and classification it can then be thought of as involving detection of the features in a given image and sort of deciding okay well if the feature is for a particular class are present in an image we can then predict that that image is of that class with a higher probability and so if we're building a image classification pipeline our model needs to know what those features are and it needs to be able to detect those features in an image in order to generate this prediction one way to solve this problem is to leverage our knowledge about a particular field say those of human faces and use our prior knowledge to define those features ourselves and so a classification pipeline would then try to detect these manually defined features and images and use the results of some sort of detection algorithm to do the classification but there's a big problem with this approach and if you remember images are just 3d arrays of effectively brightness values and they can have lots and lots and lots of variation such as occlusion variations in illumination and intraclass variation and if we want to build a robust pipeline for doing this classification task our model has to be invariant to these variations while still being sensitive to the differences that define the individual classes even though our pipeline could use these features that we the human define where this manual extraction will break down is actually in the detection task and that's again due to the incredible variability in visual data because of this the detection of these features is actually really difficult in practice because your detection algorithm would need to withstand each of these different variations so how can we do better we want a way to both extract features and detect their presence in images automatically in a hierarchical fashion and again right you came to a class on deep learning we we we hypothesize right that we could use a neural network based approach to learn visual features directly from data without any you know manual definition and to learn a hierarchy of these features to construct a representation of the image that's internal to the network for example if we wanted to be able to classify images of faces maybe we could learn how to detect low-level features like edges and dark spots mid level features like eyes ears and noses and then high level features that actually resemble facial structure and we'll see how neural networks will allow us to directly learn these visual features from visual data if we construct them cleverly going back in lecture 1 right we learned about fully connected architectures where you can have multiple hidden layers and where each neuron in a given layer is connected to every single neuron in the subsequent layer and let's say that we wanted to use a fully connected neural network for image classification in this case our 2d input image is transformed into a vector of pixel values and this vector is then fed into the network where each neuron in the hidden in the first hidden layer is connected to all neurons in the input layer and here hopefully you can appreciate that by squashing our 2d our 2d matrix into this 1d vector and defining these fully connected connections all spatial information is completely lost furthermore in in defining the network in this way we end up having men many different parameters right you need a different weight parameter for every single neural connection in your network because it's fully connected and this means that training in network like this on a task like image classification becomes infeasible in practice importantly right visual data has this really rich spatial structure how can we leverage this to inform the architecture of the network that we design to do this let's represent our 2d input image as an array of pixel values like I mentioned before and one way we can immediately use the spatial structure that's inherent to this input is to connect patches of the input to neurons in the hidden layer another way of thinking about this is that each neuron in a hidden layer only sees a particular region of what the input to that layer is and this not only reduces the number of weights in our model but also allows us to leverage the fact that in an image pixels that are spatially close to each other are probably somehow related and so I'd like you to really notice how the only region how only a region of the input layer influences this particular neuron and we can define connections across the whole input by applying the same principle of connecting patches in the input layer to neurons in the subsequent subsequent layer and we do this by actually sliding the patch window across the input image in this case we're sliding it by two units and in doing this we take into account the spatial structure that's inherent to the input but remember right that our ultimate task is to learn visual features and the way we achieve this is by weighting these connections between the patch and the neuron and the neuron in the next layer so as to detect particular features so this this principle is is called we think of what you can think of it as is applying a filter essentially a set of weights to extract some sort of local features that are present in your input image and we can apply multiple different filters to extract different types of features and furthermore we can spatially share the parameters of each of these filters across the input so that features that matter in one part of the image will still matter elsewhere in the image in practice this amounts to this patchy operation that's called convolution and if we first think about this at a high level suppose we have a four by four filter which means we have 16 different weights right and we're going to apply this same filter to four by four patches in the input and use the result of that filter operation to define the state of the neuron that the patch is connected to right then we're going to shift our filter over by a certain width like two pixels grab the next patch and apply that filtering operation again and this is how we can start to think about convolution at a really high level but you're probably wondering how does this actually work what am I talking about when I keep saying oh features extract visual features how does this convolution operation allow us to do this so let's make this concrete by walking through a couple of examples suppose we want to classify X's from a set of black and white images of letters where black is equal to minus 1 and white is represented by a value of 1 to classify it's it's really not possible to simply compare the two matrices to see if they're equal because we want to be able to classify and act as an X even if it's transformed rotated reflected deformed etc instead we want our model to compare the images of an X piece by piece and those important pieces that it learns to look for are the features and our model can get rough feature matches in roughly the same positions relatively speaking in two different images it can get a lot better sense at seeing the similarity between different examples of exes so each feature is like a mini image right a small two-dimensional array of values and we can use these filters to pick up on the features that are common to exes so in the case of exes right filters that can pick up on diagonal lines and a crossing capture what's important about an X so we can probably capture these features in the arms and center of any image of an X and I'd like you to notice that these smaller matrices are the filters of weights that we'll actually use to detect the corresponding features in the input image now all that's left is to define an operation that picks up where these features pop up in our image and that operation is convolution and convolution is able to preserve the spatial relationship between pixels by learning image features in small squares of the input and to do this what we do is we simply perform an element-wise multiplication between the filter weight matrix and the patch of the input image of the same dimensions and this results in this case in a three by three matrix and here in this example all the entries in this matrix are 1 and that's because everything is black and white either minus 1 and 1 and this indicates that there is a perfect correspondence between our filter matrix and the patch of the input image where we multiplied it right so this is our filter this is our patch they directly correspond and the result is is as follows finally if we add all the elements of this of this matrix this is the result of convolving this 3x3 filter with that particular region of the input and we get back to number nine so let's consider another example right right to hopefully drive this home even further suppose we want to compute the convolution of this 5x5 representation of an image and this 3x3 filter to do this we need to cover the entirety of the input image by sliding this filter over over the over the image performing this element wise multiplication at each step and adding the outputs that result after each element wise multiplication so let's see what this looks like first we start off in this upper left corner we multiply this filter by the values of our of our input image and add the result and we end up with the value of 4 this results in the first entry in our output matrix which we can call the feature map we next slide the 3 by 3 filter over by 1 to grab the next patch and repeat this element wise multiplication in addition this gives us our second entry 3 we continue this process until we have covered the entirety of this input image progressively sliding our filter to cover it doing this element wise multiplication patch by patch adding the result and filling out our feature map and that's it that's convolution and this feature map right this is a toy example but in practice you can imagine that this feature map reflects where in the input image was activated by this filter where in the input image that filter picked up on right because higher values are going to represent like sort of a greater activation if you will and so this is really the the bare-bones mechanism of this convolution operation and to consider really how powerful this is different different different weight filters can be used to produce distinct feature Maps so this is a very fit famous picture of this woman who's called Lenna and as you can see here in these three examples we've taken three different filters applied this filter to the same input image and generated three very different outputs and as you can see by simply changing the weights of the filters we can detect we can detect and extract different features like edges that are present in the input image this is really really powerful so hopefully you can now appreciate how convolution allows us to capitalize on the spatial structure that's inherent in visual data and use these sets of weights to extract local features and we can very easily detect different features simply by applying different filters and these concepts of preserving spatial structure and local feature extraction using these convolutions are at the core of the neural networks used for computer vision tasks which are called convolutional neural networks or CNN's so sort of with these bare bones mechanism under our belt we can think about how we can utilize this to build neural networks for computer vision tasks so let's first consider a CNN designed for image classification remember the goal here is to learn features directly from the image data this means learning the weights of those filters and using these learned feature maps for classification of these images now there are three main operations to a CNN the first is convolution which we went through right and we saw how we can apply these filters to generate future maps the second is applying non-linearity the same concept from lecture one and the third key idea is pooling which is effectively like a down sampling operation to reduce the size of of a map and finally the computation of class scores and actually outputting a prediction for the class of an image is achieved by a fully connected layer at the end of our network and so in training we train our model on a set of images and we actually learn those weights of the filters that are going to be used in the network as well as the weights in the fully connected layer and we'll go through each of these to break down this basic architecture of CNN so first right as we've already seen let's consider the convolution operation as before each neuron in a hidden layer will compute a weighted sum of its inputs apply a bias and activate with a non-linearity that's the same exact concept from lecture one but what's special here is this local connectivity the fact that each neuron in a hidden layer is only seeing a patch of what comes before it and so this relation defines how neurons and convolutional layers are connected what it boils down to is the same idea of applying a window of weights computing the linear combination of those weights against the input and then activating with non with a nonlinear activation function after applying a bias another thing we can think about is the fact that within a single convolutional layer we can actually have many different filters that we are learning different sets of weights to be able to extract different features and so the output layer after a convolution operation will have a volume where the height and the width are the spatial dimensions dependent on the input layer so if we had say a 40 by 40 input image the width and height would be 40 by 40 assuming that you know the dimensionality scales after the operation and and these dimensions are dependent on the size of the filter as well as the degree to which we're sliding it over the over the input layer finally the depth is defined by the number of different filters that we we are using the last key thing that I would like to like for you to keep in mind is this notion of the receptive field which is essentially a term to describe the fact that locations and input layers and are connected to excuse me a neuron in a downstream layer is only connected to a particular location in its in its respective input layer and that is termed its receptive field okay so this kind of at a high level explains sort of how these convolutional operations work in within convolutional layers the next step that I mentioned is applying a non-linearity to the output of a convolutional layer and exactly the same concept as the first lecture we do this because image data is highly nonlinear right and in CNN's it's very common practice to apply nonlinearities after every convolution operation that is after each convolutional layer and the most common activation function that is used is called the relu function which is essentially a pixel by pixel operation that reply replaces all negative values that follow from a convolution with zero and you can think of this as sort of a threshold incurring indicates sort of negative direction of a negative detection of that associated feature the final key operation in cnn's is pooling and pooling is a operation that's used to reduce dimensionality and to preserve spatial invariants and a common technique is called max pooling it's shown in this example and it's exactly what it sounds you simply take the maximum value in a patch in this case a 2x2 patch that is is being applied with a stride of 2 over this over this array and the key idea here is that we're reducing the dimensionality going from one layer to the next and I encourage you to think about other ways in which we can perform this sort of down sampling operation so these are the three key operations to cnn's and we're now ready to put them together to actually construct our network and with cnn's the key the key is that we can layer these operations hierarchically and by layering them in this way our network can be trained to learn a hierarchy of features present in the image data so CNN for image classification can be broken down into two parts first this feature learning pipeline where we learn features in our input images through convolution and through these convolutional layers and finally the the second half is that these convolutional and pooling layers will output high level features that are present in the input images and we can then pass these features on to fully connected layers to actually do the classification task and these fully connected layers can effectively output a probability distribution for the images membership over a set of possible classes and a common way that this is achieved is using this function called the softmax whose output represents a categorical probability distribution over the set of classes that you're interested in okay so the final key piece right is how do we train this and it's the same idea as we introduced in lecture 1 back propagation the important thing to keep in mind is what it is we're learning when we're training SIA what we learn when we train a CNN model is the weights of those convolution filters at another degree of abstraction right you can think of this as what features the network is learning to detect in addition we also learn the weights for the fully connected layers if we're performing a classification task and since our output in this case is going to be a probability distribution we can use that cross entropy loss that was introduced in lecture 1 to optimize via back propagation so arguably the most famous example of cnn's and for cnn's for classification and maybe cnn's in general is those trained and tested on the famous imagenet data set an image net is a massive data set with over 14 million images across over 20,000 different categories and this was created and curated by a lab at Stanford and is really become a very widely used data set across a machine learning so just to take an example in image net there are 1,400 nine different pictures of bananas and even better than the size of this data set I really appreciated their description of what a banana is succinctly described as an elongated crescent-shaped yellow fruit with soft sweet / which both gives a pretty good description of what a banana is and speaks to its obvious deliciousness so the creators of image net also created a set of visual recognition challenges on their data set and most notably the image net classification task which is really simple it's produced a list of the object different object categories that are present in images across a ground truth set of 1,000 different categories and in this competition they measured the accuracy of the models that were submitted in terms of the rate at which the model did not output the correct label in its top five predictions for a particular image and the results of this image net classification challenge are pretty astonishing 2012 was the first time a CNN won the challenge and this was the famous CNN called Alex net and since then the neural networks that have have neural networks have completely dominated this competition and the error that the state of the art is able to achieve keeps decreasing and decreasing surpassing human error in 2015 with the famous ResNet Network which had 152 convolutional layers in its design but with improved accuracy the number of layers in these networks has steadily been increasing so take it as what you will you know there's something to be said about building deeper and deeper networks to achieve higher and higher accuracies so so far we've only talked about classification but in truth CNN's are extremely flexible architecture and have been shown to be really powerful for a number of different applications and when we considered a CNN for classification I showed this general pipeline schematic where we had two parts the feature learning part and the classification part and what makes a convolutional neural network a convolutional neural network is really this feature learning portion and after that we can really change the second part to suit the application that we desire so for example this portion is going to look different for different image classification domains and we can also introduce new architectures for different types of tasks such as object recognition segmentation and the image captioning so I'd like to consider three different applications of CNN's beyond image classification the first is semantic segmentation where the task is to assign each pixel in the image an object class to produce a segmentation of the image object detection where we want to detect instances of specific objects in the image and finally image captioning where the task is to generate a language description of the image that captures its semantic meaning so first let's talk about semantic segmentation with this architecture called fully convolutional networks or f c ends and here the way it works is the network takes in an input of arbitrary size and produces an output of of corresponding size where each pixel has been assigned an object class which we can then visualize as a segmentation and so as before we have a series of convolutional layers arranged in this hierarchical fashion to create this learned hierarchy of features but then we can supplement this these down sampling operations with up sampling operations that increase the resolution of the output from those feature learning layers and then you can combine the output from these up sampling layers with those from the down sampling layers to actually produce a segmentation one application of this sort of architecture is to the real-time segmentation of driving scenes so this was a results from a couple years ago where the authors were using this encoder decoder like structure where you have down sampling layers followed by up sampling layers to to produce these these segmentations and last year this was sort of the state of the art performance that you could achieve with an architecture like this in in doing semantic segment pation and deep learning is moving extremely fast and now the new state of the art in semantic segmentation is what you see here and it's actually the same the same authors as that previous result but with an improved architecture where now they're using one network trained to do three tasks simultaneously semantic segmentation shown here depth estimation and instant segmentation which means identifying different instances of the same object type and as you can see in this upper right corner these segmentation results are pretty astonishing and they've they significantly improved in terms of their crypts crispness compared to the previous result another way CNN's have been extended is for object detection where here the task is to learn features that characterize particular regions of the input image then classify those regions as belonging to particular object classes and the pipeline for doing this is is an architecture called our CNN and it's pretty straight forward so given an input image this algorithm extracts a set of region proposals bottom-up computes features for these proposals using convolutional layers and then classifies each region proposal and there have been many many different approaches to computing these different computing and estimating these region proposals step two in this in this pipeline and as this has resulted in a number of different extensions of this of this general principle the final application that I'd like to consider is image captioning and so suppose we're given this image of a cat riding the skateboard in classification our task could be to output the class label for this particular image cat and as we saw this is done by feeding the image through a set of convolutional layers to extract features and then a set of fully connected to generate a prediction in image captioning what we want to do is generate a sentence that describes the semantic content of the image so if we take that same CNN Network from before and instead of fully connected layers at the end we replace it with an RNN what we can do is we can use a set of convolutional layers to extract visual features encode them in put them into a recurrent neural network which we learned about yesterday and then generate a sentence that describes the semantic content that's present in that image and the reason we can do this is that the output of these convolutional layers gives us a fixed length encoding that initializes our that we can use to initialize an RNN and train it on this captioning task so these are three very concrete fundamental applications of of CNN's but to take it a step further in terms of sort of the depth and breadth of impact that these sort of architectures have had across a variety of fields I'd like to first appreciate the fact that these advances would not have been possible without the curation and availability of large well annotated image datasets and this is what has been fundamental to really rapidly accelerating the progress in the development of convolutional neural networks and so some really famous examples of image data sets are shown here amnesty in today's lab image net which I already mentioned and the places data set which is out of MIT of different scenes and landscapes and as I as I sort of alluded to the impact of these sorts of approaches has been extremely far-reaching and and deep no pun intended and one area that convolutional neural networks have been have made a really big impact is in face detection and recognition software and this is you know every time you pick up your phone this your your phone is running these sorts of algorithms to pick up you know your face and your friends face and this type of software is pretty much everywhere from social media to security and in today's lab you'll have the chance to build a CNN based architecture for facial detection and you'll actually take this a step further by exploring how these how these models can be potentially biased based on the nature of the training data that they use another application area that has led to a lot of excitement is in autonomous vehicles and self-driving cars so the man in this video was a guest lecturer that we had last year and he was really fun and dynamic and this is work from Nvidia where they have this pipeline where they take a single image from a camera on the car feed it into a CNN that directly outputs a single number which is a predicted steering wheel angle and Beyond self-driving cars NVIDIA has a really large-scale research effort that's focused on computer vision and on Friday we'll hear from the leader of Nvidia's entire computer vision team and he'll talk about some of the latest and greatest research that they're doing there finally there's been a pretty significant impact of of these types of architectures in medicine and healthcare where deep learning models are being applied to the analysis of a whole host of types of medical images so this is a paper from Nature Medicine from just a few weeks ago where it was a multi Institute team and they presented a CNN that uses a pretty standard architecture to identify rare genetic conditions from analysis of just a picture of a child's face and in their paper they report that their model can actually outperform physicians when Tess on a set of images that are would be relevant to a clinical scenario and one reason that work like this is really exciting is because it presents sort of another standard approach standardized approach to identifying and diagnosing in this case genetic disorders and you can imagine that this could be combined with already existing clinical tests to improve classification or subtyping alright so to summarize what we've covered in today's lecture we first considered sort of the origins of the computer vision problem and how we can represent images as arrays of brightness values and what convolutions are and how they work and we then discussed the basic architecture of convolutional neural networks and kind of went in depth on how cnn's can be used for classification and finally we talked a bit about extensions and the applications of the basic CNN architecture and why they have been so impactful over the past several years so I'm happy to take questions at the end of the at the end of the lecture portion you feel free feel free to come to the front to speak to Alexander or myself so with that I'm going to hand it off to him for the second lecture of today [Applause] 

hi everyone so nice to see so many familiar faces and glad we haven't scared you off yet so today I'm gonna be talking about an extension on a lot of the computer vision techniques that Avila was discussing in the previous lecture and specifically focusing on a class of learning problems for generating new data and we refer to these problems or these models as generative models so these are actually systems that don't actually look to only extract patterns and data but they go step beyond that and actually use those patterns to actually learn the underlying distribution of that data and use it to generate brand new data this is an incredibly complex idea and it's something that we really haven't seen in this course previously and it's a particular subset of deep learning and machine learning that's enjoying a lot of success especially in the past couple years so first to start off I want to show you an example of two images so these are two faces and I want to take a poll now about which face you guys think is fake which is a which face is not real so if you guys think that the left face this one is not fake how about you raise your hands sorry fake okay I think like 20 30 percent okay how about this one okay that's a lot higher interesting okay so the answer is actually both are fake okay so this is incredibly amazing to me because both of these faces when I first looked at them first of all incredibly high dimensional they're very high-resolution and to me I see a lot of details in the faces I choose a wrinkle structure I see shadows I mean like I even see coloring in the teeth and special like defects in the teeth it's incredibly detailed what's going on in these images and they're both completely artificial these people don't exist in real life so now let's take a step back and actually learn what is generative modeling so generative modeling is a subset of unsupervised learning so far in this class we've been dealing with models which are focused on supervised learning models so supervised learning takes as input the data X which we've been calling it and the labels Y it attempts to learn this functional mapping from X to Y so you give it a lot of data and you want to learn the classification or you want to learn the regression problem for example so in in one example that Abba gave you give it pictures of the road from self-driving car and you want to learn the steering wheel angle that's a single number that's a supervised learning problem now an unsupervised learning we're actually talking a complete tackling a completely different problem we're just giving it data now and we're trying to learn some underlying patterns or features from this data so there's no labels involved but we still want to learn something meaningful we want to actually learn some underlying structure that's capable of generating brand-new data from this from this set of inputs that we receive so like I said the goal here is to take training examples from one input distribution and actually learn a model that represents that distribution once we have that model we can use that model to then generate brand new data so let me give you an example we can do this in one of two ways the first way is to density estimation so let's suppose I give you a lot of points in a one dimensional grid drawn at random from this distribution that I'm not showing you but I'm giving you the points here can you create this density function just by observing those points so can you observe the underlying dis abuse that generated those points if you can create this probability distribution here well now you can actually generate brand new points according to that distribution another example more complex example is using sample generation so now you're not focused on only estimating the density of this function but now you actually want to generate brand new samples go go straight to the generation portion so here I give you a lot of training examples which are just images on the left hand side right here and these were drawn some sorry these were drawn from some distribution which we'll call P data okay this is a probability distribution of the data and we want to generate brand new training examples that are drawn from another distribution which we'll call from the probability of the model the probability distribution of the model and our goal in this whole task is to actually learn a probability distribution of the model that is as similar as possible to the probability distribution of the data so when we draw samples from the model they almost look like they were being drawn from the original probability distribution that generated the real data so why do we care about actually learning the underlying model here one really good example is something that you'll get exposure to in the lab that Abu is mentioning where we're doing facial classification for D biasing the key aspect of D biasing is K is being able to learn the underlying latent variables intrinsic within the data set a lot of times when we were presented with a machine learning problem we're given a lot of data sometimes it's not properly vetted or labeled and we want to make sure that there are no biases hidden inside this data set so sometimes you might see that maybe if you're given a data set of a facial classifier you're given a lot of faces and a lot of not faces you want to learn now a classifier to predict faces maybe all the faces that you're given are falling from one distribution that is not actually representative of the real distribution that you want to be testing on so let's suppose maybe you have a lot of faces from one ethnicity or one gender type can you detect in an unsupervised manner by learning the underlying distributions without being told to look for something like gender or ethnicity can you just learn this underlying distribution of the data and then use that distribution during training time 2d bias your model this is something that you'll get exposure with during the lab today and you'll actually do biased biased facial classifiers that you'll create another great example is for outlier detection and it's actually a very related example to the D biasing one that I just gave here the problem is a great example of the problem is in self-driving cars so when we're collecting self-driving car data we collect a lot of data that looks extremely similar so 95% of the data that we collect is probably going to be on very simple roads straight roads maybe highway driving not very complex scenarios and not really interesting scenarios not the edge cases that we really care about one strategy is to use generative modeling to detect the outliers here so we want to learn the underlying distribution of the data and then actually understand that when we see events like this can we detect that they are coming from the tail end of the distribution that we should pay more attention to these points instead of always focusing on the very simple or not complex examples which are coming from the main mode of the distribution so let's start discussing two classes of models that we'll talk about in this lecture the first being auto-encoders and variational autoencoders the second class of models I'm probably sure that you've all heard of is called generative adversarial networks and that's what was creating the faces that I showed you on the first slide both of these models are classes of latent variable models so let's actually start by talking about what is a latent variable I saw some giggles so does anyone know what this image is yeah okay nice so this is an image of it's called the myth of the cave it's from one of plato's books I believe and I'll quickly describe what's going on in this image it's a great example I think of describing what is a latent variable and maybe this helps motivate why we care about latent variables even further so in this example a group of prisoners is actually chained their back start to a wall and they're forced to look only forward and they can see these shadows projected onto the wall but they can only see the shadows they'd never see the actual objects behind them that are generating the shadows because of this fire to those prisoners the shadows are the observed variables they can measure them they can give them names even though they aren't real objects they aren't the real objects that are underlying what is actually projected onto this wall but because that's all I can see that is their reality that's their observations they cannot directly observe or measure the true objects behind them because they can't turn around but they and they don't actually know that these things are shadows they have no concept of that the latent variables here are like the objects that they cannot directly observe but are the ones that are informing or generating these shadows so in this case the observable variables just to reiterate once again are the shadows that the prisoners see the hidden variables are the latent variables are the underlying variables or the underlying objects that are generating these shadows now the question here is going back to like a deep learning frame of mind can we have learn the two variables the true latent variables from only observable data so if I give you a lot of data can you learn some underlying latent variables from that structure so to start with we're going to talk about the first class of models which is called auto-encoders it's a very simple generative model which actually tries to do this by encoding its own input so just to reiterate let's suppose we want to have an unsupervised approach remember again this is all unsupervised so we're not given any labels here so we don't know that this is a - we're simply given an image and we don't know what it is now we want to feed these raw input image pixels and pass it through a series of convolutional layers in this case let's suppose just because this is an image so the green the green layers here you can see our convolutional layers let's suppose and you're going from your input X through convolutional layers to this output latent variable which we'll call Z you might notice here that the latent variable is much smaller than that of the observed variable and then actually this is a very common case so we're calling this this model that I'm showing here an encoder so this is encoding your input X into a lower dimensional latent space Z and now I actually want to raise a question to see how many of you are following me why do we care about having low dimensional Z's in this case is it important that we have a low dimensional z does it not really matter why is it the case that usually we care only about low dimensional Z's yeah exactly yeah so I think so the suggestion was get rid of noise that's a great idea by getting along the same lines I think what you're getting at is that you want to find the most meaningful features in your input right that are really contributing to what's going on what's underlying this data distribution pixels are incredibly sparse representation of images and often we don't need all that information to accurately describe an image so in this case this - it's found that you can usually model n this digit so that these images are drawn from the data set called amnesty which is just handwritten digits you can usually model endless digits with about five latent variables and these are images with hundreds of pixels in them but you can compress them down and very accurately describe them with just five numbers so doing this actually allows us to find the most accurate or sorry the most rich features in the image that accurately describe and abstract away all of the details underlying that image so how can we learn this late in space I mentioned it's not a supervised problem here so we don't actually know z so we can't directly just apply back propagation by supervising on Z so instead let's apply a trip where we actually trying to reconstruct the original data so now on the right hand side you can see a reconstruction by applying these up sampling convolutions into a reconstructed version of the original data so you can might notice here that the right hand side is a blurry version of the left hand side and that's because we lose some information by going through such a small latent space and that's to be expected that's the whole point of finding what are the most important features and also to get rid of noise like was the suggestion in the audience so here we denote that reconstruction as X hat it's just denoting the estimated or reconstructed version of the input data we can supervise this by basically just comparing the output which is the reconstructed version of the input with the original input and you might think of something just like computing the difference of these two images subtracting them and computing their square and adding up all these differences that will give you a loss term that we can use in back propagation just like before so now we're kind of shifting the problem from a completely unsupervised problem where we have no notion of how to create these latent variable Z now we have some notion of supervising it end to end and by doing it end to end this reconstruction loss is forcing the decoder to learn the most accurate version or sorry the most rich latent variable possible so that it can describe for so that it can reconstruct the image as as much as possible right so if this latent space was not descriptive at all of the image then the decoder could never create such a such an accurate image depicting its input so by forcing it to reconstruct this too you're forcing the lane space to be very rich in its latent variables right and just to reiterate it again this loss function does not require any labels we're not telling it that it's a two here we're simply feeding it images of twos or or whatever numbers and we're learning the underlying latent variables associated with this data now let's just start abstracting away this image a little bit so we can get more complexity here I'm removing all of the convolutional layers and I'm just drawing the encoder with a single trapezoid so basically denoting that we're going from this X to a low dimensional Z and then going from Z to a higher dimensional X hat and reconstructing it and this idea of bottlenecking the network forcing all of the activations and all of the information to be bottle necked into this Lane space is essentially this idea of compression Auto encoding is a form of compression and basically smaller latent spaces will result in noisier and noisier or should I say blurrier and blurrier outputs the more bottleneck that you apply on the network means the smaller that your latent space will become and the result applied on n this you can see something like this so if you only have two two variables in your latent space you can abstract away a little bit of the problem you can still generate the images but you're losing a lot of the crisp detail in the images so you still get the high level structure of what's going on here but you're missing a lot of the details increasing that up to five dimensional now you can alway already see a lot of the details that you had previously lost and comparing that to ground truth you can at least for these low dimensional inputs it's very close at human high level so in summary auto-encoders utilize this bottleneck layer to actually learn the rich representation of the latent space and that forces the network to learn a hidden representation of the data it's a self supervising approach because the latent loss that actually forces the network to encode as much information as possible in the data in order to reconstruct it later on in the network and that's where the name come from so it's called Auto encoding because it's trying to automatically encode that data so that it can learn to reconstruct it as well so now let's build on this idea in a bit more advanced a concept called variational autoencoders which are simply just an extension on auto-encoders with the probabilistic spin so let's revisit the standard auto encoder picture that I just showed you here at the Leighton space is a deterministic function right so all of these layers are completely deterministic if I feed in the same input multiple times I will always get the same output on the other side that's the definition of determinism what's the difference in a variational auto encoder so now in a variational autoencoder we're replacing that intermediate latent space which was deterministic now with a probabilistic or stochastic operation so instead of predicting a single deterministic Zee we're now predicting a single mu and a single Sigma which will then sample from in order to create a stochastic Zee so now if we feed in the same image this two on the left-hand side multiple times through the network we'll actually get different twos appearing on the other side and that's because of the sampling operation from the muse and Sigma's that we predict so again just to reiterate the muse and Sigma is up until that point these are computed deterministically you take this as input to then compute the stochastic sample of Z which you then use for the decoding and like I said before variational auto-encoders essentially represent a probabilistic spin on normal or vanilla autoencoders where we can sample from the mean and standard deviation of the distributions that they predict in order to compute their latent sample so let's break this up a little bit more so it's broken up just like a normal autoencoder into two parts an encoder which takes the image and produces now the probabilistic a probabilistic a probability distribution over the latent space and as as opposed to before which was deterministic now we have an actual probability distribution here parameterize by these weights phi which are the parameters of the encoder and we have a decoder which does the opposite it takes as it finds a probability distribution of the data given your latent distribution and it has parameters theta and these are just the parameters of your decoder so you can think of these parameters as the weights of the convolutional neural network that define this decoding operation and this is V is the weights of the encoder the weights of convolutional layers that define the encoding operation our loss function is very similar to the loss function in vanilla auto-encoders we simply want to compute the reconstruction loss like before that forces this bottleneck layer to learn a rich representation of the latent space but now we also have this other term which is called a regularization term or a or sometimes it's called like the VA II lost because it's specific to VA es so what is this let's take a look at this in a bit more detail so like I said the loss function here is comprised of inputs from both the encoder and the decoder it also takes us input the input data X and it wants to actually by training this you want to learn some loss function that's a weighted sum between the reconstructions and your latent loss reconstruction is the same as before you can think of it as just a pixel wise difference between your inputs and your outputs and you self supervise them to force the lane spaces to learn the regularization term here is a bit more interesting so let's touch on it in a little more detail before moving forward so Phi or the probability distribution Phi of Z given X is the probability distribution it's the distribution of the encoder it's the distribution of your latent space given your data and it's computed like I said by the encoder as part of regularizing we want to place a fixed prior on this distribution to make sure that the Z's that we compute follows some prior that we define so essentially what this term here is describing is d stands for distance so the regularization term is minimizing the distance between our inferred distribution which is the Meuse and sigmaz that we learn and we want to minimize the distance between that distribution and a distribution prior distribution that we define I haven't told you what that prior is yet and we'll get to that soon so a common choice of a prior here a very common choice for VA E's and specific is to make this prior a normal Gaussian prior so we want it to be centered at meeting zero and standard deviation 1 what this does it basically encourages our latent space or it encourages our encoder to project all of the input images or all of our input data in general into a latent space which is centered at zero and has roughly a variance of 1 if the network tries to deviate from that and place images on different parts of latent space potentially by trying to memorize or cheat and put special inputs in different parts of the latent space it's going to be penalized for doing that it's going to be kind of constrained to work only in this probabilistic setting by following this Gaussian normal Gaussian prior and like we saw it's very similar in the cross entropy loss we can actually define this distance function that we can use to regularize our neural network or VI e and if we're using this case like I said before where we want to place a prior on a zero one normal Gaussian it's just going to take this form between the predicted muse and sigmaz and we want to constrain those Meuse and sigmaz to be as close as possible to a normal Gaussian of zero and one sometimes you'll see this term denoted the KL divergence of the data and and that's where this comes from great so now we've defined our loss function that lets us know how we can actually train this network using back propagation and reconstruct the inputs and how that actually different differs from a vanilla auto encoder the regularization term here is taken care of and we can use it with a normal Gaussian prior let's assume for the rest of this lecture but we actually still have a big problem here so I've defined the forward pass for you pretty clearly I have to find the sampling operation between the muse and the Sigma's to create these Z's but I think I forgot a very important step which is one of the reasons why VA E's were never being used until a few years ago because you could never actually back propagate gradients through a sampling layer so these sampling layers that take as input stuff deterministic and using Sigma's an output a stochastic sample at the output you can compute their forward pass it's simple but then if you want a back propagate through a sampling node there's no notion of back propagation through stochastic nodes or layers and one of the key ideas that occurred about a few years ago was this idea of reprioritizing reaper a motorizing the sampling layer so that you could perform back propagation through it and train this network and to end I'll quickly give you a brief intro or introduction to how the repairment translation trick occurs so we already learned that we can't just simply sample from this distribution in a backward pass because we can't compute that backwards pass and we can't compute a channel through it so let's instead consider a different alternative which is let's sample the latent vector as a sum between the Meuse and the Sigma's so we'll start with the Mews these are the means of the distribution we'll add those to a weighted version of the Sigma vectors which is the standard deviation and we'll just scale those standard deviations by a random number which we can normally which we can draw from a normal distribution 0 1 so we still have a stochastic node here because we're doing this this sampling of epsilon but the difference here is the sampling is actually not occurring within the layer itself it's occurring as a by-product we're reaper amat rising where the sampling is occurring so instead of a cry instead of sampling directly within this see you can imagine the sampling occurring off to the side and being fed into the sampling layer so let's take a look at what that might look like so in the original example we have deterministic nodes which are just the weights of the network and the input data that we have and we have a sampling layer which takes those two to compute a sample according to the distribution defined by the encoder we already learned that we can't do back propagate we can't do propagation through this layer because of because of the nature of the sampling node and it's not deterministic so when we Reaper it we get a chart that looks like this which is a lot nicer and now if you look at it we have the same encoder weights we have the same data but now our sampling node has moved off to the side okay so the sampling node is being drawn from normal distribution 0 1 and Z is now deterministic with respect to that sampling node okay so now when we want to do back propagation and actually back propagate gradients to update our encoder because this is ultimately what we want to back propagate gradients to these are the weights of the neural network that we want to update when we back propagate we can back propagate through this z now because these epsilon czar just taken as constants so we don't need to back propagate in this direction if we did then that would be a problem because this is a sampling node again but we don't care about that we want to back propagate this way now this is a really powerful idea it lets us now train these variational auto-encoders end to end and since we impose these distributional priors on them we can actually slowly increase or decrease latent variables that we learn to get some really cool features of the output and basically by taking a latent vector you fix all of the data except for one variable in that vector and you basically increase or decrease that variable and then you run the decoder each time you increase or decrease it and you can see that the output actually has some semantic meaning so what does it look like this variable is actually capturing does anyone see it yeah exactly it's the tilt of the face or the pose of the face on the left hand side the face is pointing to the right and on the right hand side the face is pointing on the left and as you move from left to right you can see the face kind of turning and it's a smooth transition and you can actually do this because the latent variables are continuous in nature they're following that normal distribution and you can just walk along them and create the output at each step this is just one example of a latent variable but the network is learning many different latent variables it's that whole vector Z that we showed before and each of these vector each of these elements in Z is encoding a different interpreted latent feature ideally we want these features to be independent and uncorrelated with each other so that we can actually have we can walk along each dimension and observe different semantic meanings so here's the same example as before where we're walking left and right an observing head pose we can also walk up and down and observe something like smile so these are all again fake images this person doesn't exist but you can actually create and remove smiles and change the position of their head just by perturbing these two numbers this is the idea of disentanglement so when you have two variables that are uncorrelated with each other and when affecting one of them affects some semantic meaning or changing one of them affects some semantic meaning and changing the other one it affects a different semantic meaning when those two semantic meanings are disentangled that's when these variables are uncorrelated with each other right so those were in exam that was an example for images here's another pretty cool example for music so let's see if this plays so here the four quadrants are representing different positions in the latent space so this quadrant is one particular song this quadrant is another particular song and you're using now your auto encoder to actually interpolate and walk anywhere in this space so currently we're at this location and we can just move where we're sampling from and generate brand new music that's essentially an interpolation between any of these songs so here's an example so now you're moving down and the song changes to the other style right so going back to images we can get the same idea again going back to endless we can walk along different dimensions of a two-dimensional amnesty problem and observe that we can sample an interpolation between the different endless figures and actually see these really cool visualizations where we can generate all the different figures from just two latent variables right ok so just to summarize now in VI use we learn to compress the world down into some low dimensional latent space that we can use to learn we learned that reconstruction allows for unsupervised learning without labels read parameterization trick to actually train these networks end to end and interpret the hidden layer hidden latent variables using perturbations and increasing and decreasing them iteratively to actually generate brand new examples so now the question I'd like to bring up is in VA ease we brought up density estimation as core now we'll transition to a new type of models called generative adversarial networks which are focused on a slightly different problem and that's focused mainly on sample generation so now you're not concerned as much with estimating the density of your distribution but you care more about just generating samples at the output and the key idea here is that you have the same Z this encoding but now in generative adversarial networks there's not as much semantic meaning as there was in via is instead you're just feeding in raw noise so it's just a random noise label just a random noise vector and you train you want to train this generator to predict these fake images at the output you want these fake images to be as close as possible - the real images from your training distribution and the way we can do this is actually using generative adversarial networks which we have this generator on the bottom generating fake images we have a discriminator which is taking in the fake images and also the real images and learning to distinguish between fake and real and by having these two neural networks - generator and discriminator compete against each other you force the discriminator to learn how to distinguish between real and fake and the better that the discriminator becomes at distinguishing real from fake it forces the generator to produce better and better or more realistic and more realistic fake examples to keep fooling the generator so let's walk through a really quick example a toy example on the intuition behind guns so the generator starts from noise and a crowd tries to create some imitation of data so here's one dimensional data is trying to just just generate some random data because it hasn't been trained so these are the points on a one dimensional line of fake data the discriminator sees these points but it also sees some real data now you train the discriminator to recognize what is the probability that this is real the discriminator knows what's real and what's fake so you train it to recognize what's real and what's fake and in the beginning again it's not trained but then you train it and it starts increasing the probabilities what of what's real decreasing the probabilities of what's fake until you get this perfect separation point where the discriminator is able to separate and distinguish what is real from what is fake now the generator comes back and sees how well the discriminator is doing and it tries to move its generated points closer to the real data to start fooling the generator to start fooling the discriminator so now it's moving those points closer and closer to the green points now let's go back to the discriminator discriminator gets these new points now its previous predictions are a little bit messed up right so it's it's used to seeing some of those red points farther away it you can retrain it now it can learn again decreasing the probability of those red points coming in increasing the probability of those green or real points even more and we repeat again now the generator one last time starts moving those points even closer to the real distribution what you can see here is now that the points are following almost in this toy example the same distribution as the real data and it's very hard for the discriminator to distinguish between what is real and what is fake and that's the idea behind ganz so get in ganz we have a discriminator just to summarize a discriminator that is trying to identify real from fake well the generator tries to imitate real data and fooled the discriminator this can be formalizing using a min/max objective function where the discriminator d so the the parameters of the discriminator d is trying to maximize the likelihood objective and increase the chances that this term on the left this is the probability that the real data is as close as possible to one and it wants to also get this so let's get this probability as close as possible to one it wants to get this probability as close as possible to zero because that's the probability of seeing fake data and on the other hand the generator now which is theta g tries to change its weights in order to maximize that same objective so i minimize that same objective so now it's going to change its weights theta g inside of here to generate new fake data that is going to fool that generate that discriminator so briefly what are the benefits of gans as opposed to variational autoencoder these are two versions of very latent variable models but there are some benefits to using gans as opposed to VA e's imagine we're trying to fit a model to this latent manifold which you can see cartoon ax fide here so if we're using a traditional maximum likelihood estimation we can see on the left-hand side that we're having a very fuzzy estimation here it's noisy and it's kind of taking a smooth round not really capturing a lot of the details of the manifold the difference here is that again it's not using a maximum likelihood estimate it's using this minimax formulation between two neural networks it's able to capture a lot of the details and the nooks and crannies of this manifold in that sense it's able to create a lot crisper or it's able to model a lot more detail in the real data so let's explore what we can generate with this new data and this actually leads nicely into a lot of the recent advances that Gant's have enjoyed in the past couple of years and I'll talk about some results even in the past couple months that are really astonishing and that starts with this idea from earlier 2018 involving the progressive growth of Bret against so the idea here is that you want to iteratively build more and more detailed image generators so you start your generator by just predicting 4x4 images very coarse-grained images not detailed at all but when you start with this it's able to learn a representation a very coarse-grained representation of how to generate these coarse images once the generator has a good intuition on this coarse-grained representation you start progressively growing its dimensionality and start progressively adding new and new layers and increasing the spatial resolution of the generator this is good because it's able to stable the synthesis of the output and also it actually ends up speeding up training as well and it's able to create these high-resolution input outputs at the end 1000 by a thousand images that are very realistic again these weren't the images that I showed you I'll get to those in a second but these are still incredibly realistic images and here's some more examples of that same network producing those images the same team actually released another paper just a month ago where they create an extension of this work to a more complex architecture that builds on this work this progressive growing of gans to actually use what they call a style based approach where they're actually using the underlying style which you can think of as like a latent variable of the faces and using that as an intermediate mapping to automatically learn unsupervised separation of the high-level attributes such as the pose or like the subjects hair or skin color etc as a result this approach which you can see generated examples here these are not a real examples these are fake or generated examples the model is able to generate highly varied outputs that highlight that represent very realistic human faces so the idea behind style transfer is actually have the generator transfer styles from a source image which you can see on the top row so these are source images to a destination image which you can see on the right hand side and it's taking the features from the source image and applying it to the destination image and it's actually able to realize in certain cases that if you try and apply it a male's face to a female it actually realized that something is wrong and starts to add male features to this face it realizes that something's wrong so it starts to add a beard here even though there was no beard on this destination image and it's things like this that you're actually understanding that it's able to get some intuition about the underlying distribution of males and females males have beards facial hair whereas females typically don't and there are other examples here as well it's really remarkable work transforming skin color hair color even like patterns on the face to very fine-grained details in the in the image in the output image and finally one very last applications I'll touch on here is this notion of cycle gang which is the idea of having unpaired image to image translation which is very closely related to what we just discussed in the progressive growing of Gans through styles and what we want to do here is actually take a bunch of images in one domain and without having the corresponding image in the say in a different domain we want to just learn a generator to take an image in one domain generate a new image so take an image in Ex generate a new image and why following wise distribution and likewise take image of why created an image in X and the way they actually do this a really cool advancement of this paper was what they did was they actually created this cycle lost the cycle consistency that the network has to abide by where and if they create going from X to Y they then take that same generated output and go back from Y to X and check how close they are to the original input data and they enforce another supervised loss using that and that's why they call this approach cycle again because you're creating the cycle loss between the two generators and you also have these two discriminators that are trying to distinguish real from fake in each of those distributions and what you're able to do is actually transfer domains from unpaired images like horses on the left hand side and you're now taking that horse and making it into a zebra okay so this is just taking images of a lot of horses and a lot of zebras and without actually supervising how to go from horse to zebra it learns the underlying distribution of what's a horse what's a zebra so you can take a new image of a horse and make it look like a zebra and it's actually really interesting here because you might notice the obvious thing which is its adding stripes what I actually noticed here was even more interesting with it's changing the color of the grass which if you think about why is it doing this zebras are typically found in drier climates for example like in in Africa zebras are often found and the grass in these climates is probably not as green as the one that this horse is it so it's actually realizing not just the details of the horse going to the zebra stripes but also realizing the surroundings of the horse and transferring these details as well so finally I'll just summarize this lecture and conclude we covered two main techniques for generative modeling focusing first on variational autoencoders which are which we introduce as latent variable models here we try to learn a low dimensional input or sorry although dimensional latent space of our input so we can actually get some intuition and interpretation behind the underlying data distribution then we extended this idea into gans which is another form of variable models but one now trained between a minimax game using a generator and the discriminator to complete to create even more complex outputs or generated samples from the distributions and that's it thank you [Applause] 

today we're going to be discussing deep reinforcement learning which is actually one of a combination of disciplines between deep learning and the long-standing community of reinforcement learning which has been around for many decades now in machine learning and at a high level reinforcement learning provides us with a set of mathematical tools and methods for teaching agents how to actually go from perceiving the world which is the way we usually talk about deep learning or machine learning problems in the context of computer vision perception to actually go beyond this perception to actually act acting in the world and figuring out how to optimally act in that world and I'd like to start by showing a rather short but dramatic video of a trailer of the movie based on the alpha ghost story which you might have heard of just to give us an introduction to the power of these techniques go is the world's oldest continuously played board game it is one of the simplest and also most abstract beats me a professional player it go is a long-standing challenge of artificial intelligence [Music] everything we've ever tried in AI just falls over when you try the game of Go a number of possible configurations of the board is more than the number of atoms in the universe I'll forego found a way to learn how to play go so far alphago has beaten every challenge me giving it but we won't know its true strength until we play somebody who is at the top of the world like Lisa doll I'm not like no other is about to get underway in South Korea they said all is to go what Roger Federer is to tennis just the very thought of a machine playing a human because inherently intriguing the place is a madhouse welcome to the deep mind challenge for world is watching can Lisa doll find alphago's weakness whoa is there in fact a weakness the game kind of turned on its axis right now he's not confident thanks it's developing into a very very dangerous fight hold the phone Rena's left the room in the end he used about the pride I think something went wrong gasps man thank you he's got a plan here these ideas that are driving alphago are gonna drive our future this is it folks so for those of you interested that's actually a movie that came out about a year or two ago and it's available on Netflix now it's a rather dramatic depiction of the true story of alphago facing Lisa Dole but it's an incredibly powerful story at the same time because it really shows the impact that this algorithm had on the world and the press that it received as a result and hopefully by the end of this lecture you'll get a sense of the way that this this remarkable algorithm that Valve ago was trained and kind of going beyond that although will then give a lecture on some of the new frontiers of deep learning as well so let's start by actually talking about some of the classes of what we've seen in this lecture so far and comparing and seeing how reinforcement learning fits into those classes so first of all supervised learning is probably the most common thing that we've been dealing with in this class we're given data and labels and we're trying to learn this functional mapping to go from new data to a new to one of the existing labels in our training set and for an example we can do things like classification where I give you an image of an apple and the algorithm is tasked to determine that this is indeed an apple unsupervised learning is what we discussed yesterday and this deals with the problem where there's only data and no labels and comparing our problem with the Apple example here I give it another Apple and it's able to learn that this thing is like that other thing even though if it doesn't know exactly that these are apples but it's able to understand some underlying structure about these two objects now finally how does reinforcement learning fit into this paradigm reinforcement learning deals with something that we call state action pairs so it says pairs of both states in the environment that an agent receives as well as actions that it takes in that environment to execute and observe new States and the goal of reinforcement learning as opposed to supervised or unsupervised learning is to actually maximize the future rewards that it could see in any future time so to act optimally in in this environment such I can maximize all future rewards that it sees and going back to the Apple example if I show it this image of an apple the agent might now respond in a reinforcement learning setting by saying I should eat that thing because I've seen in the past that it helps me get nutrition and it helps keep me alive so again we don't know what this thing is it's not a supervised learning problem where we're explicitly telling you that this is an apple with nutritional value but it's learned over time that it gets reward from eating this and that should continue eating it in the future so our focus today in this class will be on reinforcement learning and seeing how we can build algorithms to operate in this state action or learning and perception paradigm so I've defined a couple concepts in that previous slide like an agent and environment action rewards that I didn't really define and I'd like to now start by going through simple example or a simple schematic where I clearly define all of those things so we can use them later in the lecture and go into greater greater levels of abstraction so the idea of reinforcement learning deals with the central component of reinforcement learning deals with an agent so an agent for example is like a drone that's making a delivery it could be also Super Mario that's trying to navigate a videogame the algorithm is the agent and in real life you are the agent okay so you're trying to build an algorithm or a machine learning work model that models that agent and the agent takes actions in some environment now the environment is like I said the place where the agent exists and it operates within it can send actions to that environment an action is simply just a possible move that the agent can make in that environment and your action is almost self-explanatory but I should note that the action that the agent executes at time T here I'm Dino Nia's little a of T can be chosen from usually a discrete subset of possible actions which we'll call capital a so in this first part of the lecture I'll focus on discrete action spaces where there's a limited number of possible actions you can imagine me telling it in a video game for example that the agent can go either left or right forwards or backwards pick up the block put down the block these are all possible actions that you could take in that environment now upon taking an action in the environment the agent will then receive some observation back from the environment these observations actually define how the agent interacts with the environment and it can include things like how the state changes so an example of a state is for example if you're the agent the state is what you see so it's the sensor inputs that you obtain it's your vision it's your sound your touch these are all parts of your state and basically it's a concrete situation that the agent finds itself at any given time so when it takes this action the environment responds with a new state that the agent is located and given that action and finally the last part part of this schematic is the reward that the environment responds with given that action so the reward is basically like a feedback that measures the success or failure of the given action taken in that environment for example in a video game when Mario touches a coin it gets an eight it gets a reward from any given state an agent sends an output in the form of actions to the environment and the environment returns the agents new state and an associated reward with that action rewards can be either immediate where you take an action and you immediately get a reward or they can be delayed in the context of delayed gratification where you may take an action today that you may not benefit until you may not benefit from until tomorrow or the day after tomorrow or even in some cases very long term into the future so building on this concept of rewards we can define this notion of a total reward that the agent obtains at any given time the total future reward rather as just the sum of all rewards from that time step into the future so for example if we're starting at time T we're looking at the reward that it obtains at time T plus the time to reward that obtains at time t + 1 + roared at t + 2 and so on there's a slight problem here and that's that if you are going to infinity if this summation is going to infinity so if you're looking at infinite time horizons with potentially infinite rewards this term capital R of T could also go to infinity and that's not a great property of mathematical equations so what we do is we introduce this notion of the discounted reward that's essentially where we discount future actions based on this discounting factor gamma it's just a number between zero and one where we're placing more weight on rewards obtained in the near-term future and placing less weight on it's placed in long term long term time steps from the current state so I guess before moving on any further I want to make sure that everyone understands all of these concepts present in this schematic because we're only going to be building on things from this point on and using these using this terminology to build higher and higher levels of that extraction so is this all clear to everyone yep go ahead the state changes immediate so usually you take an action and that's let's suppose in like self-driving cars if you want to train an agent to navigate in a self-driving car world you do it your action is a steering wheel angle and the next state is the next camera and put that that car sees it does not need to be related to the world so the rewards are basically just a scalar number that come back with each state but you could also imagine where there are no rewards and at a given time step you might not see a reward but you might see a reward way into the future so for example in in some games like for example pong you can imagine you've never get a reward until you either win or lose the game if you win the game you get a positive reward but if you lose the game you get a negative reward and all of the intermediate frames and actions that you take never result in any reward at all so now we're gonna take this notion of an agent collecting rewards in an environment and try to define what we call a queue function this is kind of a fundamental function that's gonna be the building block of one of the algorithms that we're gonna use in reinforcement learning in the first part of this lecture and just to reiterate again this this equation which you see on the top is the same equation as before it's saying the total reward that we can obtain from time T it's just a summation of the discounted rewards in the future ok and now we want to define this dysfunction which we're going to call the cue function and it's going to take as input two things one is the state and the action that the agent wants to execute at that state and we want that cue function to represent the expected total discounted reward that it could obtain in the future so now to give an example of this let's assume let's go back to the self-driving car example your plate you're placing your self-driving car on a position on the road that's your state and you want to know for any given action what is the total amount of reward future reward that that car can achieve by executing that action of course some actions will result in better Q values higher Q values because you're going to obtain if the road is straight obtaining a higher Q value might occur if you are taking actions corresponding to straight steering bowl angles but if you try to steer sharp to the right or sharp to the left your Q is going to sharply decrease on each end because these are undesirable actions so in a sense our Q value our Q function is telling us for any given action that the agent can make in a given state what is the what is the expected reward that it can obtain by executing that action okay so the key part of this problem in reinforcement learning is actually learning this function this is the hard thing so we want to learn this Q value function so given a state and given an input how can we compute that expected return of reward but ultimately what we need to actually act in the environment is a new function that I haven't defined it and that's called the policy function so here we're calling PI of s the policy and here the policy only takes as input just the state so it doesn't care about the action that the agent takes in fact it wants to output the desired action given any state so the agent obtains some state it perceives the world and ultimately you want your policy to output the optimal action to take given that state that's ultimately the goal of reinforcement learning you want to see a state and then know how to act in that state now the question I want to pose here is assuming we can learn this q function is there a way that we can now create or infer our policy function and I hope it's a little obvious here that the strategy we want to take is essentially like we want to try all possible actions that the agent can take in that given state and just find the maximum the one that results in the maximum reward and the ones that results in the maximum reward is just going to be the one that has the maximum Q value so what we're gonna define the policy function here as it's just the Arg max over all possible actions of that q value function so what that means just one more time is that we're gonna plug in all possible actions given the state into the Q value find the action that results in the highest possible total return in rewards and that's going to be the action that we take at that given state in deep reinforcement learning there are two main ways that we can try to learn policy functions the first way is actually by like I was alluding to before trying to first learn the q-value function so that's on the left hand side so we try and learn this cue function that goes from States and actions and then use that to infer a deterministic signal of which action to take given the state that we're currently in using this argument function our max function and that's like what we just saw another alternative approach that we'll discuss later in the class is using what's called as policy learning and here we don't care about explicitly modeling the cue function but instead we want to just have our output of our model be the policy that the agent should take so here the model that we're creating is not taking as input both the state and the action it's only taking as input the state and it's predicting a probability distribution which is PI over all possible actions probability distributions sum up to one they have some nice properties and then what we can do is we can actually just sample an action from that probability distribution in order to act in that state so like I said these are two different approaches for reinforcement learning two main approaches and the first part of the class will focus on value learning and then we'll come back to policy learning as a more general framework and more powerful framework we'll see that actually this is what alphago uses policy learning is what alphago uses and that's kind of what we'll end on and touch on how that works so before we get there let's keep going digger let's keep going deeper into the Q function so here's an example game that we'll consider this is the atari breakout game and the way it works is you're this agent you're this little pedal on the bottom and you can either choose to move left or right in the world at any given frame and there is this ball also in the world that's coming either towards you or away from you your job as the agent is to move your idle left and right to hit that ball and reflect it so that you can try to knock off a lot of these blocks on the top part of the screen every time you hit a block on the top part of the screen you get a reward if you don't hit a block you don't get a reward and if that ball passes your pedal without you hitting it you lose the game so your goal is to keep hitting that ball back onto the top of the board and breaking off as many as many of these colored blocks as possible each time getting a brand new reward and the point I want to make here is that understanding queue functions or understanding optimal queue values is actually a really tough problem and if I show you two possible example states and actions that an agent could take so for example here's it makes a pedal with a ball coming straight for the paddle down the agent can choose to stay where it is and basically just deflect that ball straight back up that's one possible state action pair another possible state action pair is where the ball is coming slightly at an angle towards the pedal the paddle can move slightly to the right hit that ball at an angle and just barely just barely knick it and send it ricocheting off into the screen and to the side of the screen rather now the question I have here is as a human which action state action pair do you think is more desirable to be in in this game which of you think it's a okay how about be interesting so actually you guys are much smarter than I anticipated or maybe you're just looking at the notes because the correct answer is B even in a slightly stochastic setting let's suppose you keep executing a and you keep hitting off these blocks in the middle of the screen you're kind of having a limited approach because every block that you knock off has to be this one that you explicitly aim towards and hit so here's an example of a policy executing something like a and it's hitting a lot of the blocks in the center of the screen not really hitting things on the side of the screen very often even though it's not going directly up and down it is targeting the center more than the side okay now I want to show you an alternative policy now it's B that's explicitly trying to hit the side of the paddle every time no matter where the ball is is trying to move away from the ball and then come back towards it so it hits the side and just barely hits the ball so I can send it ricocheting off into the corner of the screen and what you're gonna see is it's gonna basically be trying to create these gaps in the corner of the screen so on both left and right side so that the ball can get stuck in that gap and then start killing off a whole bunch of different blocks with one single action so here's an example it's gonna be trying to kill off those side blocks now it's going for the left side and once it breaks open now you can see it just starts dominating the game because it's ball is just getting stuck on that top platform and it's able to succeed much faster than the first than the first agent in agent a so to me at least this was not an intuitive action or an intuitive Q value to learn and for me I would have assumed that the safest action to take was actually a but through reinforcement learning we can learn more optimal actions than what might be immediately apparent to human operators so now let's bring this back to the context of deep learning and find out how we can use deep learning to actually model Q functions and estimate Q functions using training data and we can do this in one of two ways so the primary way or the primary model that's used is called a deep Q Network and this is essentially like I said a model that's trying to estimate a Q function so in this first in this first model that I'm showing here it takes as input a state and a possible action that you could execute at that state and the output is just the Q value it's just a scale or output and it's the neural network is basically predicting what is the estimated expected total reward that it can obtain given the state and this action then you want to train this network using mean squared error to produce the right answer given a lot of training data at a high level that's what's going on the problem with this approach is that if we want to use our policy now and we want our agent to act in this world we have to feed through the network a whole bunch of different actions at every time step to find the optimal queue value right so we have to for every possible action imagine we have a ton of actions in the previous example it's simple because we only have left and right as possible actions but let's suppose we have a ton of different actions for each action we have to feed in the state and that action compute the Q value do that for all actions now we have a whole bunch of Q values we take the maximum and use that action to act okay that's not great because it requires executing this network in a forward pass a total number of times that's equal to the total number of actions that the agent could take at that step another alternative is slightly reaper amat rising this problem still learning the Q value but now we input just the state and the network intrinsically will compute the Q value for each of the possible actions and since your action space is fixed and reinforcement learning in a lot of cases your output of the network is also fixed which means that it each time you input a state and the network is basically outputting and numbers where n is the dimensionality of your action space where each output corresponds to the Q value of executing that action now this is great because it means if we want to take an action given a state we simply feed in our state to the network it gives us back all these Q values we pick the maximum Q value and we use the wrote we use the action associated to that maximum Q value in both of these cases however we can actually train using mean squared error it's a fancy term a fancy version of mean squared error that I'll just quickly walk through so the right side is the predicted Q value that's actually the output of the neural network just to reiterate this takes as input the state and the action and this is what the network predicts you then want to minimize the error of that predicted Q value compared to the true or the target Q value which in this case is on the right hand side so the target Q value is what you actually observed when you took that action so when the agent takes an action it gets a reward that you can just record you store it in memory and you can also record the discounted reward that it receives in every action after that so that's the target return that's what you know it that's what you know the agent obtained that's a reward that they obtained given that action and you can use that to now have a regression problem over the predicted Q values and basically over time using back propagation it's just a normal feed-forward network we can train this loss function train this network according to this loss function to make our predicted Q value as close as possible to our desired or target Q values and just to show you some exciting results so when this first came out paper by deep mind showed that it could work in the context of Atari games and they wanted to present this general deep Q network this just learning through deep Q networks where they input the state of the game on the left-hand side pass it through a series of convolutional layers followed by nonlinear activation functions like rellis and propagating this information forward each at each time using convolutional layer activation function fully connected layer activation function and then finally at the output we have a list of n Q values where each Q value corresponds to the possible action that it could take and this is the exact same picture as I gave you before except now it's just for a specific game in Atari and one of the remarkable things that they showed was that this is an incredibly flexible algorithm because without changing anything about this algorithm but just deploying it in many different types of games you can get this network to perform above human level on a lot of different tasks in Atari so Atari is composed of a whole bunch of games which you can see on the x-axis so each bar here is a different game in Atari and things to the left of this vertical bar correspond to situations where the deep Q Network was able to outperform the level of a human operator in that game there are definitely situations in an Atari where that the DQ network was not able to outperform human level and typically what was noticed by a lot of researchers afterwards was that in situations or in games where we don't have a perfectly observable world where if I give you a state you can observe the like you can optimally observe the correct action to take in that state not all of these games are having that property and that's a very nice property to have so a lot of games have very sparse rewards for example this this game on the end montezuma's revenge is notorious for having extremely sparse rewards because it requires that the agent go through a ladder go to a different room and collect a key and then use that key to turn the knob or something like that and without randomly exploring or possibly seeing that sequence of states the agent would never get exposed to those cue values it could never learn that this was an optimal action to take if it just randomly explores the environment it's never going to actually see that possible action in that case it's never ever going to get any context of what the optimal action to take is in the context of breakout which I was showing you before where you have that paddle and the ball hitting the paddle and you're trying to break off all of these points on the top this is an example of a game where we have perfect information so if we know the direction of the ball we know the direction of what the position of our paddle and we see all of the points in the in the space we can correctly predict with like we we have an optimal solution at ma given state given what we see of where to move the paddle that kind of summarizes our topic of Q learning and I want to end on some of the downsides of Q learning it surpasses human level performance on a lot of simpler tasks but it also has trouble dealing with complexity it also can't handle action spaces which are continuous so if you think back to the way we defined the Q Network the deep Q Network we're outputting a q-value for each possible action it could take but imagine you're a self-driving car and the actions that you take are a continuous variable on your steering wheel angle it's the angle that the wheel should turn now you can't use cue learning because it requires an infinite number of outputs there are tricks that you can get around this with because you can just discretize your action space into very small bins and try to learn the Q value for each bin but of course the question is well how small do you want to make this the smaller you make these bins the harder learning becomes and just at its core the vanilla Q learning algorithm that I presented here is not well-suited for continuous action spaces and on another level they're not flexible to handle stochastic policies because we're basically sampling from this argument function we have our Q function and we just take the arc max to compute the best action that we can execute at any given time it does it means that we can't actually learn when our when our policies are stochastically computed so when the next state is maybe not deterministic but instead having some random component to it as well right so this is the point I mentioned about the continuous action space being actually a very important problem that might seem kind of trivial to deal with by just bending the solution bending the outputs but this is actually a really big problem in practice and to overcome this we're gonna consider a new class of reinforcement learning models called policy gradient models for training these algorithms so policy gradients is a slightly different twist on Q learning but at the foundation it's actually very different so let's recall Q learning so the deep Q network takes input the states and predicts a Q value for each possible action on the right hand side now in policy gradients we're gonna do something slightly different we're gonna take us and put the state but now we're gonna output a probability distribution over all possible actions again we're still considering the case of discrete action spaces but we'll see how we can easily extend the in ways that we couldn't do with q-learning to continuous action spaces as well let's stick with discrete action spaces for now just for simplicity so here pie of alpha sorry PI of AI for all I is just the probability that you should execute action I given the state that you see as an input and since this is the probability distribution it means that all of these outputs have to add up to one we can do this using softmax activation function and deep neural networks it's just enforces that the outputs are summing to one and again just to reiterate this PI a given s is the probability distribution of taking a given action given the state that we currently see and this is just fundamentally different than what we were doing before which was estimating a Q value which is saying what is the possible reward that I can obtain by executing this action and then using the maximum the Q value of maximum reward to execute that action so now we're directly learning the policy we're directly saying what is the correct action that I should take what is the probability that that action is a 1 whereas the probability that that action is a 2 and just execute the correct action so in some sense it's skipping a step from Q learning and Q learning you learn the Q function use the Q function to infer your policy and policy learning you just learn your policy directly so how do we train policy gradient learning essentially the way it works is we run a policy for a long time before we even start training we run multiple episodes or multiple rollouts of that policy a roll out is basically from start to end of a training session so we can define a roll out as basically from time 0 to time T where T is the end of some definition of a episode in that game so in the case of breakout capital T would be the time at which the ball passes the pad you miss it so this is the time when the episode ends and you miss the ball or it's the time at which you kill all of the points on the top and you have no other points to kill so now the game is over so you run your policy for a long time and then you get the reward after running that policy now in policy gradients all you want to do is increase the probability of actions that lead to high rewards and decrease the probability of actions that lead to low rewards it sounds simple it is simple let's just see how it's done by looking at the gradient which is where this this algorithm gets its name which is right here so let's walk through this let's walk through this algorithm a little more detail so we do a roll out for an episode given our policy so policy is defined by the neural network parametrized by the parameters theta we sample a bunch of episodes from that policy and each episode is basically just a collection of state action and reward pairs so we record all of those into memory then when we're ready to begin training all we do is compute this gradient right here and that gradient is the log likelihood of seeing a particular action given the state multiplied by the expected reward of that action sorry excuse me that's the expected discounted reward of that action at that time so let's try and parse what this means because this is really the entire policy gradient algorithm on this one line this is the key line here so let's really try and understand this line the green part is simply the log likelihood of obtaining or of outputting that action so let's suppose our action was very desirable it led to a good reward okay and that's just defined by we do our roll out on the top and we won the game at the end so all of these policies all of these actions should be enforced or reinforced and in this case we did and on the second line we did another episode which resulted in a loss all of these Paul should be discouraged in the future so when things result in positive rewards we multiply this is going to be a positive number and we're going to try and increase the log likelihood of seeing those actions again in the future so we want to tell the network essentially to update your parameters such that whatever you did that resulted in a good reward is gonna happen again in the future and to even greater probability so let's make sure that we definitely sample those things again because we got good rewards from them last time on the converse side if R is is negative or if it's zero if we didn't get any reward from it we want to make sure that we update our network now to change the parameters and make sure that we discourage any of the probabilities that we output on the previous time so we want to lower the log likelihood of executing those actions that resulted in negative rewards so now I'll talk a little bit about the game of Go and how we can use policy gradient learning combined with some fancy tricks that deepmind implemented in the game of alphago in the algorithm alphago and i think the core for those of you who aren't familiar with the game of Go it's an incredibly complex game with a massive state space there are more States than there are atoms in the universe and that's in the full version of the game where it's in 19 by 19 game and the idea of go is that you have a it's a two-player game black and white and the motivation or the goal is that you want to get more board territory than your opponent the state here is just the board of black and white cells and the action that you want to execute it's just a probability distribution over each possible cell that you need to put your next piece out that cell so you can have a you can train a network like we were defining before a policy based net network which takes us input an image of this board it's a 19 by 19 image where each pixel in that image corresponds to a cell in the board and the part of that network is going to be a probability distribution again it's a 19 by 19 probability distribution where each cell is now the probability that your next action should be placing a token on that cell on the board right so let's see how at a high level the alphago algorithm works they use a little trick here in the beginning which is they start by initializing their network by training it on a bunch of experts from sorry sorry they training it from a bunch of human experts on playing the game of go so they have humans play each other these are usually professional or very high quality high-level humans they record all of that training data and then they use that as input to train a supervised learning problem on that policy network so this again takes us input in action sorry it excuse me it takes us input the state of the board and it tries to maximize the probability of executing the action that the human output or sorry executing the action that the human executed okay so now this first step here is focusing on building a model using supervised learning to imitate what the humans how the humans played the game of go of course this is not going to surpass any human level performance because you're purely doing imitation learning so the next step of this algorithm is then to use that network which you trained using supervised learning and to pit it against itself in games of self play so you're gonna basically make two copies of this network and play one network against itself and use now reinforcement learning to achieve superhuman performance so now since its play against itself and not it's not receiving human input its able to discover new possible actions that the human may not have thought of that may result in even higher reward than before and finally the third step here is that you want to build another network at each time at each position on the board so it takes now the state of the board and tries to learn the value function and that's essentially very similar to the cue function except now it's outputting one output which is just the maximum Q function so over all actions this this value network is basically telling you how good of a board state is this so this gives us an intuition we're using neural networks now to give us an intuition as to what board states are desirable what support states may result in higher values or more probability of winning and you might notice that this is very closely related to the Q function this is telling us like I said at the core how desirable a given board state is and basically just by looking at the board state we want to determine is this a good board state or a bad sport state and the way that alphago uses this is it then uses this value network to understand which parts of the game it should focus more on so if it's at a particular state where it knows that this is not a good state to be in it knows that it shouldn't keep executing trials down the state so it's able to actually prune itself and have a bit of a more intelligent algorithm of learning by not executing all possible actions at every time step but by kind of building a smart tree of actions based on this heuristic learned by another network and the result is alphago which in 2016 beat lee sedol which is who's the top human player at go and this was the first time that an AI algorithm has beaten top human performers in the game of go this was really a groundbreaking moment it ties us back to that trailer that we saw at the beginning of class and finally just to summarize some really exciting new work from the same team that created alphago now they've released a new model called alpha zero which was published in science about one month ago and it's a general framework for learning self play models of board games so they show it being to outperform top model-based approaches and top human players in games of chess shogi and alphago so it's actually able to surpass the performance of alphago in just 40 hours and now this network alpha zero is called alpha zero because it requires no prior knowledge of human players it's learned entirely using reinforcement learning and self play and that's kind of the remarkable thinkers that without using any prior information at all now we've shown it's possible to one learn the possible or learn the likely sets of moves that human players would make and then the really interesting thing is that it then kind of starts to discard those moves in favor of even better moves that humans never really thought of making and the really interesting thing is if you follow this this evolution of the rewards going up in time if you stop it at any intermediate time especially on the early parts of so if you stop in the early training parts like for example right here and you look at the behavior of these agents they behave similar to top human players so they especially the execute the actions that they execute in the initial board states like right when the game starts they behave very similar to top human players but then as training continues and the agent starts to discover new and new more and more different advanced ways of executors starting the game it's actually able to create new policies that humans never even considered and now alpha zero is being used as almost a learning mechanism for top human performers on these new policies that can improve humans even more I think this is a really powerful technique because it shows reinforcement learning being used not just to pit humans against machines but also as a teaching mechanism for humans to discover new ways to execute optimal policies in some of these games and even going beyond games the ultimate goal of course is to create reinforcement learning agents that can act in the real world robotic reinforcement learning agents not just in simulated board games but in the real world with humans and help us learn in this world as well so that's all for reinforcement learning happy to take any questions or will hand it off Dava for the new frontiers of deep learning in the next part okay thank you [Applause] 

okay so welcome everyone to the final foundational lecture of this year's offering of six s-191 where we'll be taking kind of a step back from the architectures and the algorithms we've been exploring over the past two days to take a broader perspective on some of the limitations of deep learning and a couple of exciting new subfields that are emerging within deep learning so before we dive into that some very important announcements first and foremost we have amazing t-shirts for you that have arrived and we'll be distributing them today after lecture so we'll have them arranged sort of in the front by size and we'd first like to distribute two for credit registered students we should have plenty for everyone but then to registered listeners and then to all other guests and lesson and listeners so there should be more than enough we also have some shirts from Google as well as some other swag so it's gonna be really exciting and please do stick around for that so sort of where have we been and where are we going so following this lecture we have our final lab which is going to be on reinforcement learning and then tomorrow we have two extremely exciting guest lectures from Google and IBM as well as time for you to work on your final projects during the lab portion and on Friday we'll have one final guest lecture from Nvidia the project pitch competition as well as the judging and awards ceremony so I've we've received a lot of inquiries about the the final projects and specific logistics so I'd like to take a few minutes to to recap that so for those of you who are taking the course for credit right you have two options to fulfill your final requirement the first is a project proposal and here we're asking you to really pitch a novel deep learning architecture idea application and we've we've gotten a lot of questions about the size of the groups so groups of one are welcome but you will not be eligible to receive a prize if you are a group of one listeners are welcome to to present ideas and join groups in order to be eligible for a prize you must have a group of two to four people and your group must include at least one for credit registered student we're gonna give you three minutes to do your pitches and this link there's pretty detailed instructions for the the proposal on that link so last year we we only gave people one minute for their pitch which was really really short so by giving you three minutes we're really hoping that you have spent some time to think in-depth about your idea how it could work why it's compelling and we're going to have a panel of judges including judges from industry as well as Alexander myself and and other guest judges and our prizes are these three nvidia gpus as well as a set of four google homes sort of how the the prize Awards awarding is going to go is that top three teams will be awarded a GPU per team and the Google homes will be distributed within one team so if you have a team that has four people right and you're awarded the the Google home prize everyone will get a Google home if you have two each of you will get one and then the remaining two will be awarded to to the next best team okay so in terms of actually completing this we ask that you prepare slides for your for your pitch on Google slides so on Friday if you're participating you'll come down to the front present your pitch to the rest of the class and to our judges we ask that you please submit your groups on by today tonight at 10 p.m. there's this link leads to a Google sheet where you can sign up with your your team members names and a tentative like title for your for your project tomorrow's lab portion will be completely devoted to in-class work on the project then we ask that you submit your slides by midnight Thursday night so that we have everything ready and in order for Friday and the link for doing that is there and finally our our presentations will be on Friday so as was discussed in the first lecture the section second arguably most more boring option is to write a one-page review of a recent deep learning paper it can be either on you know deep learning fundamentals and theory or an interesting application of deep learning to a different domain that you may be interested in and this would be due Friday at the beginning of class by email to - intro to deep learning stuff mit.edu okay so tomorrow we're going to have two guest speakers the first is going to be where we're really really lucky and privileged to have her Fernanda Viegas she is an MIT alum and she's the co-director of Google's people and artificial intelligence Research Lab or pair and she's a world-class specialist on visualization techniques for machine learning and deep learning so it should be a really fun interactive a cool talk and we really hope to see everyone there the second talk will be given by Dimitri or dima crota from the MIT IBM Watson AI lab he's a physicist by training really exciting and fun guy and his research focuses on biologically plausible algorithms for training neural network so he'll he'll give some insight onto whether or not back propagation could actually be biologically plausible and if not what are some you know exciting new ideas about how learning could act we work in in the neuroscientific sense I guess then the lab portion is going to be devoted to work on the final projects we'll be here the TAS will be here you can brainstorm with us ask us questions you know work with your team you get the point finally Thursday we'll have our final guest lecture given by Yann Kouts from Nvidia who's a leader in computer vision and then we'll have the project proposal competition the awards as well as pizza celebration at the end ok so that is sort of the administrivia for for today and ideally the rest of the class so now let's start with with the technical content so on day one Alexandre showed this slide which sort of summarized how deep learning has revolutionised so many different research areas from autonomous vehicles to medicine and healthcare reinforcement learning generative modeling robotics and the list goes on and on and hopefully now through this series of 5 lectures you have a more concrete understanding of how and why deep learning is so well suited for these kinds of really complex tasks and how its enabled these advances across a multitude of disciplines and also you know so far we've primarily dealt with these algorithms that take as input some set of data in the form of signals sequences images or other sensory data to directly produce a decision at the as an output whether that's a prediction or an action as in the case of reinforcement learning and we've also seen ways in which we can go from sort of from decision to data in the context of generative modeling and to sample brand new data from the decision space in sort of this probabilistic setting more generally in in all these cases we've really been dealing with algorithms that are designed to do that are optimized to do well on Ingle tasks right but fail to think like humans or operate like humans sort of at a at a higher love order level of intelligence and to understand this in more detail we have to go back to a famous theorem in sort of the theory of neural networks which was presented in 1989 and generated quite the stir and this is called the universal approximation theorem and basically what it states is that a neural network with a single hidden layer is sufficient to approximate any arbitrary function any continuous function and in this class you know we've we've mostly been talking about deep models that use multiple layers but this theorem you know completely ignores this and says you just need one one neural layer and if you believe that any problem can be reduced sort of to a set of inputs and an output this means that there exists a neural network to solve any problem in the world right so long as you can define it using some continuous function and this may seem like an incredibly powerful result but if you look closely right there are two really big caveats to this first this this theorem makes no guarantee on the number of hidden units or the size of the hidden layer that's going to be required to solve you know your arbitrary problem and additionally it leaves open this question of how do we actually go about finding the weights to support whatever architecture that could be used to solve this problem it just claims that and actually proves that such an architecture exists but as we know from gradient descent and this idea of finding ways and sort of like a non convex landscape there's no guarantee that this this process of learning these weights would be anyway straightforward right and finally this theorem doesn't provide any guarantees that whatever model is that's learned would generalize well to other tasks and this theorem is is a perfect example of sort of the possible effects of overhype in AI and as a community I think we're all interested in sort of the state of deep learning and how we can use in that's probably a big motivation of why you're sitting in this lecture today but I think we we really need to be extremely careful in terms of how we market and advertise these algorithms so while the universal approximation theorem generated a lot of excitement when it first came out it also provided some false hope to the AI community that neural networks as they existed dirt could solve any problem in the world right and this overhype is extremely dangerous and historically there have actually been to quote-unquote AI winters where research in AI and neural networks specifically in in the second AI winter came to sort of a grinding halt and so this is why you know for the first portion of this lecture I'd like to focus on some of the limitations of these algorithms that we've learned about so far but also to take it a step further to touch on some really exciting new research that's looking to address these problems and limitations so first let's let's talk about limitations of deep learning and one of my favorite examples of a potential danger of deep neural networks comes from this paper from Google Google brain that was entitled understanding deep neural networks requires rethinking generalization and this paper really did something really simple but very powerful they took images from the imagenet dataset and you know their labels first four examples are shown here and what they did is that for every image in their data set they flipped a die write a K sided die where K is the number of possible classes that they were trying to consider in a classification problem and they use this result of this die roll to assign a brand you randomly sampled label to that image and this means that you know these new labels associated with each image were completely random with respect to what was actually present in the image and if you'll notice that these two examples of dogs ended up in this in this demonstration that I'm showing being mapped to different classes altogether so we're literally trying to randomize our labels entirely then what they did was that they tried to fit a deep neural network model to the sampled image net data ranging from either the the untouched original data with the original labels to data that they had reassigned the labels using this completely random sampling approach and then they tested the accuracy of their model on a test data set and as you may expect the accuracy of their models progressively decreased as the randomness in the training data set increased right but what was really interesting was when they tried was what happened when they looked at what happened in the training data set and this is what they found that no matter how much they randomized the labels the model was able to get 100% accuracy on the training set right because in training you know you're doing input label you know both right and this is a really powerful example because it shows once again in a similar way as the universal approximation theorem that deep neural Nets can perfectly fit to any function even if that function sort of is based on entirely random labels and to drive this point home we can understand neural networks simply as functional approximator x' and only universal function approximation theorem States is that neural networks are really really good at doing this so suppose you have this you know the set of training data we can learn you we can use a neural network to learn a maximum likelihood estimate of this training data and if we were to give the model a new data point shown here in this purple arrow we can use it to predict what the maximum likelihood estimate for that data point is going to be but if we extend the axis a bit left and right outside of the space of the training data that the network has seen what happens right there are no guarantees on what the training data will look like outside these bounds and this is a huge limitation that exists in modern deep neural networks and and in deep learning generally and so you know if you look here outside of these bounds that the network has been trained on we can't really know what our function looks like if the network has never seen data from those pieces before right so it's not going to do very well and this notion leaves really nicely into this idea of what's known as adversarial attacks on neural networks and the idea here is to take some example for example this this image of what you can see is a temple which a standard CNN trained on imagenet let's say can classify as a temple with you know 97 percent probability and then we can apply some perturbation to that image to generate what we call an adversarial example which to us looks completely similar to the original image right but if we were now to feed this adversarial example through that same CNN we can no longer recognize it as a temple you know and instead we predict okay this is an image of an ostrich right that makes no sense right so what's going on what is it about these perturbations and how are we generating them that we're able to fool the network in in this way so remember that normally during training when we train our network using gradient descent we have some like objective loss function J right that we're trying to optimize given a set of weights theta input data X and some output label Y and what we're asking is how does a small shift in the weights change our loss specifically how can we change our weights theta in some way to minimize this loss and when we train our networks to you know optimize this set of weights we're using a fixed input X and a fixed label Y and we're again reiterating trying to update our weights to minimize that loss with adversarial attacks we're asking a different problem how can we modify our input our input for example an image our input X in order to now increase the error in our networks prediction so we're trying to optimize over the input X right to perturb it in some way given a fixed set of weights theta and a fixed output Y and instead of minimizing the loss we're now trying to increase the loss to try to fool our network into making incorrect predictions and an extension of this idea was recently presented by a group of students here at MIT and they devised an algorithm for synthesizing a set of examples that would be adversarial over a diverse set of transformations like rotations or color changes and so the first thing that they demonstrated was that they were able to generate 2d images that were robust to noise transformations distortions other transformations but what was really really cool was that they actually showed that they could extend this idea to 3d objects and they actually used 3d printing to create actual physical adversarial objects and this was the first demonstration of Rosario examples that exist in the physical world so what they did in in this result shown here is that they 3d printed a set of turtles that were designed to be adversarial to a you know a given network and took images of those of those turtles and fed them in through the network and in the majority of cases right the network classifies these 3d turtles as rifles right and these these objects are designed to be adversarial they're designed to fool the network so this is pretty scary right and you know it opens a whole Pandora's box of how can we trick networks and and has some pretty severe implications for things like security and so these are just a couple of limitations that of neural networks that I've highlighted here you know as we've sort of touched on throughout this course they're very data hungry it's computationally intensive to train them they can be fooled by adversarial examples they can be subject to algorithmic bias they're relatively poor at representing uncertainty then a big point is this this question of interpretability right are known networks just black boxes that you can't peer into and sort of in the ml and AI community people tend to fall you know in sort of two camps one camp saying interpret interpret ability of neural networks matters a lot it's something that we should devote a lot of energy and thought into and others that very strongly argue that oh no we should not really concern ourselves with interpretability what's more important is you know generating these architectures that perform really really well on a task of interest and in going from limitations to sort of new frontiers in emerging areas in deep learning research I like to focus on these two sort of of points highlighted here the first is the notion of understanding uncertainty and the second is ways in which we can move past building models that are optimized for a single task to actually learning how to build a model capable of solving not one but many different problems so the first sort of new frontier is this field called Bayesian deep learning and so if we consider again right the very simple problem of image classification what we've learned so far is has been about modeling probabilities over a fixed number of classes so if if we are to train a model to predict you know dogs versus cats we output some probability that an input image is either a dog or a cat but I'd like to draw a distinction between a probability and this notion of uncertainty or confidence so if we were to feed in in the image of a horse into this network for example we would still output a probability of being dog or cat because right probabilities need to sum to one but the model may even even if it's more saying that it's more likely that this image of is of a horse it may be more uncertain in terms of you know its confidence in that prediction and there's this whole field of Bayesian deep learning that looks at modeling and understanding uncertainty in deep neural networks and sort of this gets into a lot of of statistics but the key idea is that Bayesian neural networks are trying to rather than learn a set of weights they're trying to learn a distribution over the possible weights right given some input data X and some output labels Y and to actually parameterize this problem they use Bayes rule which is a fundamental you know law from from probability theory but in practice this what's called this posterior distribution of the likelihood the probe a set of weights given input and output is computationally intractable and so instead of we can't learn this distribution directly so what we can do is find ways to approximate this posterior distribution through different types of sampling operations and one example of such a sampling approach is to use the principle of dropout which was introduced in the first lecture to actually obtain an estimate of the network's uncertainty so if we look at what this may look like for a network that's composed of convolutional layers consisting of like two dimensional feature Maps what how we can use dropout to SMA uncertainty by performing stochastic passes through the network and each time we make a pass through the network we sample each of these sets of weights right these filter maps according to some drop out mask that these are either zeros or ones meaning will will keep these weights highlighted in blue and will discard these weights highlight highlighted and white to generate this stochastic sample of our original filters and from this these passes what we can actually obtain is an estimate of sort of the expected value of the output labels given the input the mean as well as this variance term which provides an uncertainty estimate and this is useful in understanding the uncertainty of the model in making a prediction and one application of this type of approach is shown here in the context of depth estimation so given some input image we train a network to predict the depth of the pixels present in that image we also asked it okay give us an estimate of your uncertainty in making that prediction and we when we visualize that what you can see is that the model is more uncertain in this sort of edge here which makes sense if you look back at this original input that the edge is sort of at this point where those two cars are overlapping and so you can imagine that the model may have more difficulty in estimating the pixels that line that edge the depth of the pixels that line that edge furthermore if you remember from from yesterday's lecture I showed this video which is worked from the same group at Cambridge where they trained a convolutional base convolutional neural network based architecture on three tasks simultaneously semantic segmentation depth estimation and instant segmentation and what we really focused on yesterday was how this segmentation result was much crisper and cleaner from there this group's previous result from one year prior but what we didn't talk about was how they're actually achieving this improvement and what they're doing is they're using uncertainty by training their network on these three different tasks simultaneously what they're able to achieve is to use the uncertainty estimates from two of two tasks to improve the accuracy of the third task and this is used to regularize the the network and improve its generalization in one domain such as segmentation and this is just another example of their results and as you can see right each of these semantic segmentation instant segmentation and depth estimation seemed pretty crisp and clean when compared to the input image so the second exciting area of new research that I'd like to highlight is this idea of learning to learn and to understand why this may be useful and why you may want to build out algorithms that can learn to learn right well first like to reiterate that most most neural networks today are optimized for a single task and as models get more and more complex you know they increasingly require expert knowledge in terms of engineering them and building them and deploying them and hopefully you've gotten a taste of that knowledge through this course so this can be kind of a bottleneck right because you know there are so many different settings where deep learning may be useful but only so many deep learning researchers and engineers right so why can't we build a learning algorithm that actually learns which model is most well suited for an arbitrary set of data and an arbitrary task and Google asked this question a few years ago and it turns out that we can do this and this is the idea behind this this this concept of Auto ml which stands for sort of automatic machine learning automatically learning how to create new machine learning models for a particular problem and in the original auto ml which was proposed by Google uses a reinforcement learning framework and how it works is is the following they have sort of this like agent environment structure that Alexander introduced where they have a first Network the controller which in this case is a RNN that proposes a child model architecture right in terms of the parameters of that model which can then be trained and evaluated for its performance on a particular task and feedback on how well that child model does you know your tasks of interest is then used to inform the controller on how to improve its proposals for the next round in terms of okay what is the updated child network that I'm going to propose and this process is repeated thousands of times you know iteratively generating new architectures testing them and giving that feedback back to the controller to learn from and eventually the controller is going to learn to assign high probability to sort of areas of the architecture space that achieve better accuracy on that desired task and low probability to those architectures that don't perform well so how does this agent work as I mentioned it's an RNN controller that sort of at the macro scale considers different layers in the proposed generated network and at the internal level of each candidate layer it predicts different what are known as hyper parameters that define the architecture of that layer so for example if we're trying to generate a child CNN we may want to predict you know the number of different filters of a layer the the dimensionality of those filters the destryed that we're going to you know slide our filter patch over during the convolution operation all parameters associated with convolutional layers so then if we consider the the other network in this picture the child network what is it what is it doing to to reiterate this is a network that's generated by another neural network that's why it's called the child right and what we can do is we can take this this child network that's sampled from the RNN train it on a desire task right with the desired data set and evaluate its accuracy and after we do this we can then go back to our RNN controller update it right based on how the child met work performed after training and now the RNN parent can learn to create an even better child model right so this is a really powerful idea and what does it mean for us in practice well Google has now put the service on the cloud Google being Google right so that you can go in provide the auto Amell system a data set and a set of metrics that you wanted to optimize over and they will use parent RNN controllers to generate a candidate child Network that's designed to train optimally on your data set for your task right and this end result is this new child network that it gives back to you spawned from this rnm controller which you can then go and deploy on your data set right this is a pretty big deal right and it sort of gets that sort of this deeper question right they've demonstrated that we can create these AI systems that can generate new AI specifically designed to solve Mazar tasks right and this significantly reduces sort of the difficulties that machine learning engineers face in terms of optimizing a network architecture for for a different task right and this sort of gets at the heart of the question that Alexander proposed at the beginning of this course right this notion of generalized artificial intelligence and we spoke about a bit about what it means to be intelligent right loosely speaking the sense of taking in information using it to inform a future decision and as humans our learning pipeline is not restricted to solving only specific defined tasks how we learn one task can impact you know what we do on something something completely unrelated completely separate right and in order to reach sort of that same level with AI we really need to build systems that can not only learn single tasks but can improve their own learning and their reasoning so as to be able to generalize well two sets of related and dependent tasks so I'll leave you with this thought and I encourage you to to continue to discuss and think about these ideas amongst yourselves internally through introspection and also we're happy to chat and I think I can speak for the TAS in saying that they're happy to chat as well so that concludes you know the series of lectures from Alexander and I and we'll have our three guest lecturers over the next couple of days and then we'll have the final lab on reinforcement learning thank you [Applause] 

all right so thank you for the invitation it's really exciting to take part of my afternoon to come over here and talk to you all about some of the work that we're doing as Ava said I co-lead a couple of things at Google one is this team called the big picture group and a lot of our work centers around data visualization in the last three years or so it's been very much focused on data visualization for a machine-learning so you'll get to hear a lot about what we do today I also could lead the pair initiative which is stands for people plus AI research and there we have a bunch of different kinds of things that we do as part of this initiative we produce open source software hopefully some of which you are familiar with tensorflow is anyone tensorflow yes yes yes I'm seeing some noddings here we also put out educational materials so some of this actually starts to overlap with the data visualization for machine learning we've noticed that even as we at Google try to retrain our own engineering workforce to use machine learning it turns out that if you have simulations and data visualization front ends to play with some of these systems you just speed up learning by like a factor of 10 instead of going back to the command line to learn some of the basics so we can talk a little bit about that today as well all right so the first thing kind of let's start at the beginning with the data right why might you want to visualize your training data and what's special about your training data one of the things that we started saying at Google is that whenever you start working with machine learning and you you look at your results and maybe your results are not coming out as strongly as you would wish there is this innate intuition or this you're like itching to debug you're a machine learning model right one of the things we always say is before you even debug your model debug your data and that becomes it's it's incredibly important I'm sure you're talking about a lot of that here in the class as well problem is we don't have good tools to do that today so we're starting to build us at Google and other places everybody's starting to think about what might be interesting tools for you to start to debug your training data the data that your system is going to rely on so we started to approach this from a data visualization perspective so what I have here is a screenshot of the CFR ten data set everybody familiar with that data set very simple data set of images ten classes that's all and so what I'm going to demo for you right now is a visualization we created it's called facets and facets is a visualization right now I'm visualizing Safari ten and all I'm doing as you can tell is I'm just looking at all the pictures these are not all it's it's a sample of cifra ten of the pictures in each class and I can very easily zoom in and I can see what the pictures look like I can zoom out and even as simple as this is it already starts to give me a window into what's happening with my data one of the things I can see right away is that the heels are different right something you may expect so the the top row here is airplane and you can see that it has a ton more blue than say bird or a cat or deer and so this ship that may be expected right because I take pictures of airplanes against the sky and I take pictures of ships against the water okay but then I can start to play some interesting games I can say show me the same data set within each category but now I want you to arrange this in terms of hue distribution okay so now I can start to see okay these are little histograms of my pictures per category per hue and voila what are the bluest of the blue airplanes or there might be some interesting blue birds here okay these are also against the sky that's interesting let me see you I have some Reds on airplanes okay interesting just a very easy organic way to start looking at my data I can also look for things like okay what are the classes of in my data set that have the the most if you will kind of equal distribution of hues you get to things like ships and trucks and and so forth and you have these bulges of kind of earthy tones for the animals right which makes sense I can also start playing other games so I can come here and say okay now give me a confusion matrix of my data and again the confusion matrix is going to pair each one of these rows is what the system thinks my image is versus each one of the columns is what the humans have hand labeled my image to be so the good news to me is that right away I can see that the diagonal is the most populated set of cells that's good because that's where my system and the humans agree right and another interesting thing I can see here right away is that I have a couple of cells that are highly populated I have this cell here ton of pictures and I have this other cell here ton of pictures and sure enough these are my system is getting confused between dogs and cats okay so that already might start to give me some hints as to what I might want to debug about my system maybe I need to give it more images of cats and dogs so it does better on those classes now keep this in mind this is C far 10 this is kind of a hello world data set of machine learning everybody looks at this data set everybody benchmarks against this data set it's been looked at by thousands of people right so we started visualizing this and we're like oh okay so my system is very sure that these are cat these are not cats so let's see what the cats are that my system is very short are not cats interesting okay so I can see how like this guy here has a lot big ears maybe it's getting confused about that all right but then we started seeing some interesting things I challenge you to find something in the bottom rows that is indeed not a cat anyone a frog yes look at this this was hand labelled cat but my system is a hundred percent sure it's a frog and I have to give it to it I think it's a frog I don't think it's a cat at all but why does that matter it matters because just by giving you the ability to look at your data you can start to see actual mistakes in your data right and again this is a data set that thousands of researchers and practitioners use and benchmark again benchmark against and nobody knew that there were a couple of mistakes there interesting things right so again just creating a the ability to look at your data very easily can buy you can get rid of a lot of headache for you so that's that's facets and we open sourced facets so facets is available for anyone to use and it's been downloaded a bunch of times you don't have to use it for machine learning it's a visualization tool for any kind of faceted data set so you can use it for anything you want and here at MIT people have started using it so joy for instance at the Media Lab was using facets to look at racial bias racial and gender bias in facial recognition facial recognition systems from industry and she was using the visualization to make the point that the category of women of color where the was the category that these system would consistently do worse at alright I'm gonna I'm if I have time I'm gonna come back to the what if tool the only thing I'll say about this tool is that it's another open-source tool that we created that allows you to probe a machine learning model as a black box and to probe for things like machine learning fairness different concepts of ML fairness if we have time we'll come back to this alright so we just talked about the importance of looking at your data now let's talk a little bit about what can you learn when you're looking at your model sort of how is your model understanding the world how is it making sense of very very high dimensional data so I'm gonna walk you through a very quick exercise in high dimensionality so a warm-up exercise M nest yes M nest everybody familiar with omnis data set of machine learning and i'm going i'm going to start pulling us over to the world of visualization again so think about each one of these images as a very high dimensional vector how might I think about that all I'm going to do is I'm going to count the pixels and depending on the color of the pixel I'm gonna give it a value from zero to one one if it's all white zero if it's black something in the middle if it's a gray I end up with a vector boom i transformed an image into math that's great now I have a great way to compare these images alright so then we created a visualization to actually look at this so this is another tool that's open sourced by pear and this is called the embedding projector and now what we're doing here we're looking at M mist in high dimensional space okay so this is a 3d projection of of the Emnes numbers okay all I did here since I have ground truth I colored the numbers by the ground truth okay and the projection I'm using here is T's me it's a nonlinear projection it does really well in keeping track of local clusters it doesn't do very well at all in understanding the global structure of things but locally it's very good so what do we have here the good news is that we have clusters that's a good starting point and we have space between clusters so that's really good because it tells me that somehow these categories are being pulled apart from each other we also have so let's start playing with this tool it lets me rotate in many different ways I can zoom in and out to look more carefully at things I can also click on things so basically just so you know what's going on here I'm setting up a demo using the same tool that shows you how a visualization like this can work in a real system in a large system the system I was showing you before the the data was showing you before it was eminent right very easy toy example what I'm showing you now and this is it's taking its time to get to the T Snee projection it's calculating T's needs calculating all the clusters in this data set and I want to be able to interact with it that's why I'm taking my time and I'll talk to you while this is working in the background okay so we're gonna let this do its thing it's gonna run in the background I'm gonna try to get it not to distract us completely meanwhile let's talk about interpretability in a real world scenario so a couple years ago Google came out with something called a multilingual neural net Translate system what what does that mean it means that up until that moment whenever you wanted to use neural nets to translate between two languages you had to train a model on a pair of languages so I trained a model between English and French French English I trained a different model for English and Japanese Japanese English okay so it was all based on pairs for the first time Google came up with an architecture they'll let you ingest multiple languages and translate high-quality translations into multiple languages okay and that was that was a revelation it was it was quite a feat and so that's called the multilingual translation models there's another thing that was interesting in what these systems were doing somehow they were able to do what we call zero shot translation okay and so what that means is that imagine I have a system that is that is ingesting a bunch of sentences that translate between English and Japanese Japanese in English and it's also ingesting a bunch of sentences that translate between glish and Korea and back and forth okay somehow this system was able to also do Japanese to Korean and Korean to Japanese without ever having seen a single example of a sentence that goes straight from one to the next okay from Japanese to Korean so one of the challenges one of the big unknowns is that the people building these systems they weren't sure how is the system being able to do this with high quality translation high quality translation is extremely hard to get is extremely announced and so how is it doing that oh this was the data that we decided to visualize to actually start answering that question okay it turns out that between the encoder and the decoder there is an attention vector in the middle and this is the data that we were that we were going to visualize all right the question in these researchers Minds these are the people who were building the Translate system was imagine I have space with multiple languages an embedding space a very high dimensional space of language what were these multilingual systems doing were they doing something like what's here on the Left where they would resolve the space by keeping all the English in one corner keeping all the Japanese on the other corner and keeping all the Korean in another corner and then just mapping between these spaces for translations or where's the system doing something more like what you see on the right where it's way more messy but they're bringing together these multiple languages okay and so this why does this matter why does it matter if it's one way or another way it matters because if what the system was doing was mixing up the different languages in the same high dimensional space it had a very specific implication which meant it the system didn't care as much that one string said for instance home and another string said cazza it knew that these two strings had the same semantic meaning so if it was co-locating these very different strings from very different languages in the same high dimensional space it meant that it was paying attention to actual the semantics the actual meaning of the words and that matters because that then tells us that these are the first indications of a universal language okay and so but we had no way of knowing so we decided to visualize and how might we visualize this imagine I have a sentence that says something like the stratosphere extends from 10 kilometers to 50 kilometers in altitude what does that look like in high dimensional space it looks like a bunch of points where some points are going to be closer together like the 10 and the 15 maybe because they're both numbers okay so again location has a specific meaning here in high dimensional space and then what we do with the visualization is we connect these dots and so that is my sentence in high dimensional space it's the path of my sentence okay so now I have my sentence in English what does this look like when I have to set to when I have a translation does it look like this where I have my English sentence in one corner and I have the Portuguese version of that sentence on another corner or does it look more like something like this okay and so this is what we're trying to find out so the visualization for this let me see is actually this here so what I'm showing here again we're back in the embedding projector and I'm visualizing a multi-dimensional tea lingual system that takes in English Japanese and Korean I am coloring each one of those languages by a different color so English is blue Korean is red Japanese is yellow okay my question to you is as messy as this looks do you see a big neighborhood of red and a big neighborhood of blue and a big neighborhood of yellow no right so that right there was our very first indication that something really interesting was happening because we started to see clusters that had the three colors together okay and so the thing that I was hoping to show you but I don't know that I'm gonna be able to show you let me see if I can oh maybe maybe I'll show you like this so if I do a search for stratosphere well it it just did a bunch of stuff here that is not helpful ah I'm gonna try to drag it all the way here do you see this little cluster here the stratosphere with the altitude between 10 and 50 kilometers okay ignore all of this junk here this is not supposed to be there but this is what I care about if I look at this cluster okay and I look at the nearest neighbors these are the nearest neighbors here that I'm mousing over the three languages are here okay and my nearest neighbors for that ain't for that sentence in English both in Japanese and in Korea are all in that cluster alright so that in fact did you see I just clicked out and you see the cluster has three colors here so that was our indication that this was actually the beginning of a universal language this system was actually being able to coalesce the three spaces of three different languages in terms of semantic meaning now keep this image in mind and I want to show you a sister image of the image a visualization of a sister system this system here it's the same kind of visualization but this is a system that takes in English Portuguese and Spanish okay what do you see that's different here separated right I see a big neighborhood of red there and so we saw this and we're like wait wait wait a second we thought there were indications of a universal language going on what is that doing there by itself so we did a statistical analysis for quality of translation and we found out that the sentences in this cluster the was being pulled apart they were the translation translations were all bad low-quality translations okay so what does that tell us it tells us that the geometry of the space matters that if you have a multilingual system kind of in the case Google had if your system looks like this that's bad news that means that your system is not being able to cluster in the right way and it's having difficulties and it's going to do a bad job with translations okay so again visualization here not only to give you a sense of the landscape of what's happening these massively high dimensional spaces but also giving you a clue of how to start debugging your system maybe you need more training data in in Portuguese maybe it's something else but you have a problem okay so this was really exciting because it was the first time anyone could see kind of like an MRI version of this neuro net and the fact that it was functioning at the super-high level it was functioning for a language a couple of other things I wanted to show the same visualization can let us to understand things about our own language that I think are very interesting so this is again the embedding projector and you can one of the things I didn't demo today is not only do you have these projections like T Snead that you just saw or PCA you also can create custom projections you can say let's look at the English language so imagine each one of those dots is a word in the English language okay and now what I want to understand is the notion of biases in our language okay you may be familiar with papers like this one about word embeddings and the fact that you can do you can do calculations in in the direction of certain vectors so you can say oh look you know in word embedding space if I take the direction between China and Beijing if I take that direction and I follow roughly that direction from Russia I'm going to end up in Moscow okay if I follow that direction for Japan I'm going to end up in Tokyo and so on and so forth so there are these meaningful directions in wording bedding spaces that are incredibly powerful so now let's use that to start visualizing language what I did here is I went to the embedding projector that you just saw and I looked for the word book okay and then I filtered all the words by the hundred top nearest neighbors to book so I have words like biography and publishing and manuscript all the nearest neighbors to book but then I said okay now I want to see these words along the following projection I want to give an axis that goes from old to new so the closest the more to the left of where it is the closest to old it is so let's see some of the words I have poem manuscript author story okay closest to new here on the right I have company publications theory mind creation volume okay okay interesting now let's look at a different set of words my ex is now is man to woman and my focal word my anchor word is engineer and all of its nearest neighbors so I can tell already the engineer is closer to man than it is to woman by the way the y-axis doesn't mean anything it's random just pay attention to the X location okay so closer to man I have engineers engineering electrical electronics I have a really interesting word here at the bottom but I can't look at the bottom I can't point Michael Michael is one of the top nearest neighbors to engineer that's crazy okay and then close to woman I have journalist lawyer officer diplomat writer poet okay let's keep going my new anchor word is math between men and woman next to men computational arithmetic flash computation physics astronomical so forth next to a woman psychology instance procedural art educational library and so forth I changed my axes now it goes from Christian to on the left to Jew on the right and my anchor where it is moral so close to Christian Christian beliefs religion faith religious closest to Jew intellectual psychological serious educational sexual Liberty ethical so forth okay why does this matter it matters because it starts to it's to me this is kind of like the cannery on the mine about this is just language the data set here remember we were talking about training data be incredibly important word to vac which is this data set it's from hundreds of thousands of publications it's just how we use language in the real world okay so when you were training your machine learning systems based on language one thing to keep in mind is the fact that you're training on a biased data set and so being aware of these biases can actually be incredibly important and hopefully being able to use tools like visualization tools starts to give you a way of even becoming aware acknowledging under and trying hopefully trying to mitigate some of these effects I think I'm gonna stop here and open up for questions thank you any questions ah that's an awesome question if if if you had a word that had multiple meanings in say a projection like this it would probably show up in the middle which would not be very illuminating another thing that happens on the embedding projection projector is that you may have a word that is very very strongly so let's take let's talk about a concrete example the word Bank okay the word Bank can be a financial institution it can also be a river bank right if my corpus of data that I trained on is on financial news there is going to be a very high pole towards financial institution vocabulary right and I will see that bias in my data set hopefully what also shows up if my data set is broad enough is that when I highlight that word in the visualization here I was filtering but imagine I have the entire language okay let's say I highlight the word Bank it definitely lights up all of the financial institutional vocabulary that's highly connected to that but it also lights up stuff like River elsewhere in the visualization and that is one indication where you're like oh interesting there are these disparate very separate clusters here maybe there's something there maybe that's one indication that you have these multiple words with multiple meanings so part of what we were trying to understand was what mode at all it was in we had no clue we were literally like if it separates that's fine if it doesn't separate that's interesting literally we were like just give me a telescope so I can look into this world and see what it looks like the system was working really well so it was more of a curiosity of trying to understand how is it working so well what is it doing that it can get to these very high quality translations from no training data on specific pairs right it was more that kind of question than the oh we think it should do this or we think it should do that it was literally like what is it even doing that it works this way once we got to that set of questions another set of questions that came up was like well here's a system that's not working so well why is it not working so well and so that was the point that I was trying to make when I showed Olek this system between English Japanese and Korean that was working really well that looked like this okay now let's look at a system that was not working so well and it's a sister system okay here's a clue about why maybe it was not working as well does that make sense yeah sure in terms of biases oh yeah I think one thing that would be wonderful that I don't know that anyone has done yet is exactly what I let me know if I'm understanding your question correctly I think it would be wonderful to compare embedding spaces of different languages right it could be that in English we have a set of biases that maybe don't exist in Turkish I don't know but and even even trying to understand for instance what are the nearest neighbors of a given word in English versus nearest neighbors of a given word in French how much does that set of how do those embedding spaces differ I think that would be incredibly interesting one specific if we're talking about translation one very complex issue that touches translation word embeddings and and biases is what happens what used to happen now Google started addressing this what used to happen when you went to Google Translate and you said something about a doctor and a nurse the doctor is coming and the nurse and the nurse will see you okay so when you were to translate between languages that have gender like English two languages that don't have gender like Turkish it turns out that in so imagine it going the other way around imagine I have that sentence the doctor is coming to see you in Turkish and I am translating that into English on Google Translate Google Translate would translate it into he's coming the doctor is coming to see you he will look at your exams there was an assumption that it was a he if the word been used was nurse there would be an assumption in English that the pronoun following nurse would be she okay even though the original the source language had no notion of gender whatsoever attached to any of those words right and so Google had to actively proactively work out a solution where it realizes it's coming from a non gendered language to one that has biases in terms of professions and gender and give users a solution like oh the doctor is coming to see you he /she will talk to you next or something like that so you yeah these are the kinds of biases you want to be aware of and try to mitigate work around a very interesting point because part of what we're trying to do is we're trying to understand what is the intent on the user side so as a user I want a translation to my problem which is I see a sentence in Turkish and I have no idea what it means in English if all you're giving me is the top hit from a statistical distribution you're giving me a very specific answer you're not giving me all the possible answers to my translation problem as a user as a user it is important for me to know that the translation to that sentence could be he but it could be she right or do I want a distribution do I want you to decide for me what the translation is again like if I am if I don't know the language I'm translating from right I want to understand what are the possibilities in my own language I can understand so that therein lies the dilemma it really depends on what do you think what do you think the user the users intent is and how do you work with that but yeah to your point absolutely again we're back to the fact that the data is skewed absolutely it is skewed right and to your point it reflects a certain reality thank you 

