[NOISE] Okay. Hey, everyone. Um, morning. Welcome to CS230, Deep Learning. Um, so many of you know that, um, Deep Learning these days is the latest hardest area of computer science or AI. Uh, arguably, Deep Learning is the latest hardest area of, you know, all of human activity, uh, uh, maybe. Um, but this is called CS230 Deep Learning where we hope that we can help you understand the state of the art and become experts at building and applying Deep Learning systems. Um, unlike many Stanford classes, this class will be more interactive than, than others, uh, because in this class we offer in the flipped classroom format where we'll ask you to watch a lot of the videos at home, uh, uh, a lot of the deeplearning.AI content hosted on Coursera, thus preserving the classroom in discussion section time for much deeper discussions. Um, so to get started, let me, let me first, uh, introduce our teaching team. So, the co-instructors are Kian Katanforosh who are actually, uh, uh, one of the co-creators of the, uh, Deep Learning specialization, the deeplearning.ai content that we're using in this class. Um, and the, uh, the rest of the teaching team, uh, Swati Dubei is the class coordinator and she has been working with me and others on coordinating, uh, I guess CS230 and also CS229 and CS229A to, to make all of these classes run well and let you have a relatively smooth, you know, uh, uh, experience. Younes Mourri is the course adviser and he's also worked closely with Kian and me in creating a lot of the online contents that you use and Younes is also, um, head TA, uh, for CS229A, which some of you may also be taking. And then, we have two co-head TA's, Aarti Bagul, who's worked on machine learning research for a long time and Abhijeet who's still traveling back I think, and also a large team of TA's that, uh, I think about half of our TA's in CS230 had previously TA this course and, uh, their expertise spans everything from applying machine learning to problems in health care, applying machine learning or applying deep learning to problems in, um, uh, in robotics, to problems in computational biology to problems in, um, so, so I hope that as you work on your projects this quarter, as CS230 you'll be able to, um, get a lot of great advice, uh, uh, and help and mentorship from all of the TA's as well. Um, so the plan for today is, uh, I was going to spend maybe the, uh, little bit of time, uh, sharing with you what's happening in Deep Learning, why, you know, why Deep Learning is taking off and how this might affect your careers. Uh, and then in the second half, I have, Kian will takeover and talk a bit more about the projects you work on in this class and not just the final term project, but you know, the little machine translation system you build, the face recognition system you build, your art generation system you built and all of the, uh, many pretty cool machine learning and deep learning applications that you get to build throughout the course of this quarter, uh, and, and also share a view the, the, the detailed logistics for the plan for, for the class. Okay? Um, so I think that's, um, uh, let's see. All right. I'm gonna just use the whiteboard for this part. [NOISE] Cool. So, um, [NOISE] you know, deep learning, right, you know, it seems like the media still can't stop talking about it. Uh, and, um, it turns out that a lot of the ideas of deep learning happened around for several decades, right? The, the basic ideas of deep learning happened around for decades. So, why is deep learning suddenly taking off now? Why is it, quote, Coming out of nowhere? or what- whatever people say. Um, I think that the main reason that deep learning has been taking off, uh, and why, you know, suddenly all of you hopefully will be the, do really, um, powerful things with it much more effectively than two or three years ago is the following. Um, for a lot of, over the last couple of decades with the digitalization of society, we've just collected more and more data. Uh, so for example, all of us spend a lot more time on our computers and smartphones now and whenever you do things on a phone, uh, you know, that creates data, right? Uh, uh, and, and, um, and, um, what used to be represented through pieces of paper is now much more likely to be a digital record as well. So you're, if you go take an X-ray, uh, as at, at least in the United States, uh, less than some other kind, in developing economies, but basically the United States, there's a much higher chance now that your X-ray in the hospital is a digital image rather than physical piece of film. Or, um, if you order a new marker, right? There's a much higher chance that the fact that you ordered a marker, you know, off a website is now represented as a digital record compared to, uh, 10 years ago when the state of the global supply chain. Actually if you order, uh, if you order 10,000 markers, um, there's a much higher chance, you know, 10 years ago that the fact that you place that order was stored on a piece of paper. There's someone scribbled saying, Hey, ship 10,000 markers to Stanford. But now there's much more likely to be a digital record. And so the fact that, um, so many pieces of paper are now digital has created data. And, uh, for a lot of application areas, the amount of data has sort of, you know, exploded over the last 20 years. But what we found was that, um, if you look at more traditional learning algorithms, traditional machine learning algorithms, the performance of most of them would plateau, uh, even as you feed it more and more data. So, by traditional learning algorithms, I mean logistic regressions, support vector machines, you know, maybe decision trees depending on influential details, uh, and it was as if our older learning algorithms didn't know what to do if all the data you can now feed it. Um, but what we start to define several years ago, was if you train a small neural network, right? Its performance may look like that. If you train a medium, neural net, its performance may look like that. And if you train a very large neural net, you know, the performance kinda keeps on getting better and better, uh, up to some, usually up to some theoretical limit called base error rate, which you'll learn about later this quarter, but where performance can never exceed a 100 percent, but, but sometimes, uh, uh, sometimes there's some seeding of the performance. But onset, we've been able to measure on many, many problems, uh, with not yet, I think that across machine learning and deep learning broadly, I think we've not yet hit the limits of scale and by scale I mean, um, the amount of data you can throw at the problem that is still useful for the problem as well as, um, the size of the neural networks. And I think, um, you know, GPU computing, uh, uh, was a large part of how we were able to go from training small to medium to now training very large neural networks. And once upon a time, I think, um, you know, the first, actually I think a lot of the early work on, uh, training neural networks on GPUs, which is done here at Stanford, right? Crew there using cruder to the train neural networks. But, um, what use to be, you know, one thing, one lessons we've learned over and over in computing is that, uh, what yesterday's supercomputer is today's, uh, uh, you know, processor on your, on your smartwatch, right? And so what used to be an amount of computation that was accessible only to, you know, large research labs in Stanford. They could spend a $100,000 on GPUs. Today, you could rent that on a- on a cloud relatively inexpensively. And so the availability of relatively large neural network training capabilities has allowed really students, really almost everyone. Many people, not, not many, many people, um, to have enough access to computational power to train what, uh, large enough neural networks to drive very high acc- levels of accuracy for a lot of applications, right? Um, and it turns out that, uh, uh, if you look broadly across AI, you know, I, I think the mass media, right? Newspapers, you know, reporters use the term AI. I think, uh, uh, within, within academia or within the industry, we tend to say machine learning and deep learning. Uh, uh, but i- if you look broadly across AI, it turns out that AI, um, has many, many tools that's beyond machine learning, that's even beyond deep learning. And if any of you take, you know, CS221, right, Stanford's AI class, great class, uh, uh, you'll learn about a lot of these other tools of AI. But the reason that deep learning is so valuable today is that if you look across many of the tools of AI, let's say [NOISE], you know, there's a deep learning [NOISE] /machine learning, um, and, and, and again, some of you know, uh, neural networks and deep learning mean almost exactly the same thing, right? It's just that, um, as, you know, as we start to see deep learning rise over the last several years, we found that, uh, deep learning was just a much more, um, attractive brand. And so, you know, and so, so that's the brand that took off. Uh, uh, but, um, if you look at, if you, if you take an AI class, you look broadly across the portfolio of tools you have in AI. Um, I think that, you know, our often use deep learning and machine learning, um, are sometimes also use their probabilistic graphical model, right? We shall learn about it in CS- CS228, also a great class. Um, sometimes I use a planning algorithm, you know, when I'm working on self-driving car, right? You need a motion planning algorithm. You need various planning algorithms. Uh, sometimes I use the search algorithm. Uh, sometimes I use knowledge representation, it's very easy. This one is the technologies, uh, specifically knowledge graphs is one of the technologies that is widely used in industry, but I think often under appreciated in our academia. Um, uh, i- if you do a web search, you know, web search engine, pulls up a hotel and then there's a room prices and where there's WiFi, where there's swimming pool, that's actually a knowledge graph or a knowledge representation knowledge graph, but it's, uh, so it's actually used by many companies. These large databases, but this is, this is actually maybe under-appreciated in academia, um, or sometimes even game theory, [NOISE] right? So, if you learn about AI, there's a very large portfolio of many different tools you will see. But what has happened over the last [NOISE] several years [NOISE] is, um, [NOISE] if you go to a conference on probabilistic graphical models, right? [NOISE] If this is time and this is a performance A [NOISE]. You'll see that, you know, every year, um, probabilistic graphical models work a little bit better than the year before. In, in regard to the UAI conference, Uncertainty in AI conference. Maybe the- one of the leading conferences, maybe the leading one, I'm not sure on PGM's, you'll see there every year, you know, researchers publish papers that are better than the year before. The state- the, the, the field is steadily marching forward. Um, same for planning, if we go with triple AI or something, you'll see, you know, a few. There's advancing, search algorithms are getting better, uh, knowledge representation algorithms getting better, game theory algorithms are getting better. And so the, the, the field of AI marches forward, um, across all of these different disciplines. But the one that has taken off, you know, in- in- incredibly quickly is deep learning, machine learning. And I think a lot of this progress was, uh, initially driven, uh, by scale. Scale of data and scale of computation. The fact that we can now get tons of data throw it into a giant neural network and get good performance. But, uh, more recently has been also driven by, um, uh, the positive feedback loop of, um, seeing early traction in deep learning thus causing a lot more people to do research in deep learning algorithms. And so there's been tons of algorithmic innovation in deep learning in the last several years and you hear a lot about other algorithms that were, you know, relatively recently invented in this class as well. All right. Um, and so really, I think that the initially the twin forces of a scale data scale computation, but now the triple forces have also a lot of algorithmic innovation and massive investment, um, is continuing to make deep learning, uh, make tremendous progress. And so in CS230, uh, because of the, uh, uh, you know, I- I- I think our two main goals. Uh, the first is to, uh, um, have you, uh, become experts in the deep learning algorithms. Have you- have you learned the state of the art? Have you- have you- have you have deep technical knowledge on, um, the state of the art in deep learning. Um, and second is to give you the know how to apply these algorithms to whatever problems you want to work on. So, uh, one of the things I've learned- so I think, you know, actually some- some of you guys know my history, right? So, you know, I worked at Stanford for a long time. Then, um, I started as leading the Google Brain Team which, uh, did a lot of projects at Google and I think the Google Brain Team the, you know, built from scratch was arguably the leading force for, um, helping Google go from what was already a great Internet company into today a great AI company. Um, and then, uh, did something similar, Baidu in China or its Chine- you know, which was headquartered in China which kind of helped Baidu go from al- also what was already a great company into today. You know, many people say China's is greatest AI company. Um, and I think through work on many projects at Google, many projects at Baidu. And now leading Landing AI helping many companies on many projects and running around to different companies and see many different machine learning projects they have. I think I've been fortunate to learn a lot of lessons, um, not just about the technical aspects of machine learning, but about the practical know-how aspects of machine learning. And, um, if you, uh, uh, and- and I think that, uh, what you can learn from, um, you know, the Internet or from a purely academic sources or from reading research papers is a lot of the technical aspects of machine learning and deep learning. But, uh, there are a lot of other practical aspects of how to get these algorithms to work that, um, I actually do not know of any other academic course that- that kind of goes into great depth [NOISE] teaching it. Uh, there might be one but I'm- I'm- I'm not sure. But one of the things that, um, uh, uh, we hope to do in this class is to not just give you the tools but also give you the know-how and how to make it work, right? And I think, you know, I actually spent a lot of time thinking about, uh, so actually, uh, uh, late last night, I actually stayed up very late last night reading this new book by, um, John Osahalts on, ah, software architecture. Right. And I think that, um, there's a huge difference between, you know, a junior software engineer and a senior software engineer. Maybe everyone understands the C++ and the Python and the Java Syntax. Yeah, you can get that from- from, uh, from, you know, you just figure out, "Hey, this is how C++ works. This is how Java works. This is how Python NumPy works.". But, um, it's often the high-level judgment decisions, of how you architect the system. Uh, uh, what abstractions do you use? How do you define interfaces? That defines a difference between a really good software engineer versus, you know, a less experienced software engineer. It's not understanding C++ Syntax. Um, and I think in the same way, uh, today there are lots of ways for you to learn the technical tools of machine learning and deep learning. And you will learn that in this class. You know, you'll learn how to train the neural network. You learn the latest optimization algorithms. You, uh, understand deeply what a conv net is. What are, uh, recurrent neural network is. What LSTM is. You- you understand what intention model as you- you learn all of these things in great detail. You work on projects in computer vision, natural language processing, speech and so on. Um, but I think one other thing that is relatively unique, uh, to this class, um, uh, and- and- and to the- I guess the- the- the things you see, uh, on, uh, the deeplearning.ai Coursera websites as well as the things we do in class, is trying to give you the practical know-how so that when you're building a machine learning system, you can be very efficient in, uh, deciding things like, should we collect more data or not, right? And the answer is not always "Yes". Uh, I think- I think, um, with- I think that many of us try to convey the message that having more data is good, right? And that's actually true. More data pretty much never hurts, but I think the message of big data has also been over-hyped and sometimes it's actually not worth your while to go and collect more data. All right. Uh, but- so when you're working on machine learning project and if you are either do it by yourself or leading a team, your abilities to make a good judgment decision about, should you spend another week collecting more data, or should you spend another week searching for hyperparameters, or tuning parameters in your neural network. That's the type of decision that if you make it correctly, can easily make your team 2X or 3X or maybe 10X more efficient. And so one thing we hope to do in this class is to more systematically impart to you this- this type of knowledge. All right. And so I think, um, uh, e- even today, um, I, you know, actually- I actually visited lots of, uh, machine learning teams around Silicon Valley and around the world and I kinda see what they're doing and [NOISE] um, you know, recently I visited a company that had a team of, uh, 30 people trying to build a learning algorithm and the team of about 30 people was working on a learning algorithm for about three months. Right. And- and they had not yet managed to get it to work. So they're basically, you know, uh, you know, not succeeded after 3 months. Um, one of my colleagues [OVERLAPPING] have the datasets. [OVERLAPPING] Oh, Kian? Kian, you're broadcasting. [LAUGHTER] Don't say anything bad. [LAUGHTER] All right. [LAUGHTER] All right. Um, so, uh, one of my colleagues, um, took the dataset home and spent one weekend working on it. [LAUGHTER] Let's see what you're doing now. [NOISE] All right. Cool. Um, and- and- and one of my colleagues, uh, uh, working on this problem for one long weekend, he worked over a long weekend for three days, was able to build a machine learning system that outperformed what this group of 30 people have been able to do after about three months. So, what's that? That's like a, uh, I don't know, that's more than a 10x difference in- in speed, right? And- and a lot of the differences between the great machine learning teams versus less experienced ones is actually not just do you know how to, you know, implement, uh, um, uh, it's not just do you know how to implement an LSTM, right, in- in Tensorflow flow or Keras or whatever? You have to know that but it's actually other things as well. And I think, um, Kian and I and the teaching team are looking forward to trying to systematically impart to you a lot of this know-how so that, uh, when- hopefully someday when you're all leading a team of machine learning engineers or- or deep learning engineers that you could help direct the team's efforts more efficiently. Um, and oh, and actually if any of you are interested, uh, uh, one of the things I've been- actually, how many of you have heard of Machine Learning Yearning? Machine Learning Yearning? Wow, almost none of you. Okay. Interesting. Um, so this is, uh, if this is your first Machine Learning class, this may be too advanced for you. But if you've had a little bit of other machine learning background, um, Machine Learning Yearning is a book that I've been- I've been working on. It's still in draft form, but, um, uh, if any of you want a better Machine Learning Yearning, it's, uh, my attempts to try to turn- gather best principles for turning machine learning from a black art into systematic engineering discipline. And so, uh, if you go to this website, uh, uh, you know, this website will send you- actually I just finished the last- just finished the whole book draft last weekend. Uh, uh, and so email allows students. If you want a copy, go to the website and enter your email address and I'll make sure that, you know, when we send out the book- actually it might be later today, I'm not sure. That we'll- we'll- that you'll get a copy of the book draft as well. I tend to write books and then just post them on the Internet for free. So you could- but then here, we just email them out to people, uh, so you can- you can- you can get it if you go to the website. Um, and I think this will- and- and I think this class will talk a lot about a lot of the other principles of Machine Learning Yearning, but give you much more practice as well than- than just reading a book might. Um, so [NOISE] let's see. Okay. So, um, Kian will give a greater overview of what we'll cover in this class but, uh, uh, one of principles I've learned as well is that, you know, it- it- so I think, um, uh, actually some of you know my background, right? I- it's a, you know, co-founded Coursera, worked in education for a long time. So I spent a long time really thinking a lot about education and I think CS230 represents, you know, Kian and my- and our teaching teams, uh, uh, really best attempt to deliver a great, uh, on-campus deep learning course. Um, and so, oh, interesting. [LAUGHTER] Um- Um, and so the format of this class is, um, what's called the flipped classroom class. And what that means is that, so, you know, and I think, uh, I've taught on SCPD for a long time, right? For, you know, many, many years I guess. And I found that, uh, even for classes, uh, like CS229 or other Stanford courses, often students end up, you know, uh, watching videos at home. Uh, and, and I think with the flipped classroom what we realized was if many students are watching videos of these lectures at home anyway, um, why don't we spend a lot of effort to produce higher-quality videos, uh, that you can watch, that are more time-efficient for you to watch at home. Um, and so our, our team, uh, created videos, uh, deeplearning.ai created, you know, kind of the best videos we knew how to create it on deep learning, uh, that, uh, are now hosted on Coursera. And so, with- I, I actually think that it'll be, uh, uh, quite time-efficient for you to watch those videos, um, do the online programming exercises, do the online, uh, uh, quizzes. And what that does is it preserves the class time of both the weekly sessions that we meet right here on Wednesdays, as well as the TA discussion sections on Fridays for much deeper interactions and for much deeper discussions. And so, um, the format of the class is that we ask you to, uh, you know, do the online content, uh, created by deeplearning.ai hosted on Coursera, and then in class, uh, both the meetings with Kian and me- I think Kian and I will split the sessions roughly 50-50, uh, as well as for the deeper small group discussion sections you have with the TAs. That lets you spend much more time interacting with the TAs, uh, interacting with Kian and me, and going deeper into the material than just the- than, than the- than the, um, uh, than the- than the online content, uh, by itself. And, uh, that will also give us more opportunities to give you, um, advanced material, uh, that goes beyond what's hosted online, as well as, um, uh, uh, give you additional practice with these concepts, right? Um, and so let's see. Yeah. And so, um, I've also finished up with, uh, uh, two more thoughts, and then I'll hand it over to Kian. Um, I think, you know, uh, machine learning, deep learning, AI, whatever, it's changing a lot of industries, right? I, I think, you know, I think AI is the new electricity. Uh, much as the rise of electricity, uh, about a 100 years ago, starting with the United States, transform every industry, uh, really, you know. The rise of electricity transformed agriculture because finally we have refrigeration, right? That transformed agriculture. It transformed healthcare. I- imagine going to a hospital today that has no electricity, and how do you- how do you even do that, right? Without computers, medical devices, how you even run a healthcare system. Trans- transformed communications through telecom, through the telegraph initially, even now. So, much of communications really needs electricity, but electricity transformed every major industry. And I think machine learning and deep learning has reached a level of maturity where we see a surprisingly clear path for it to also transform pretty much every industry. And I hope that through, um, this class, uh, after these next 10 weeks that all of you will be well-qualified to go into these different industries and help transform them as well. Um, and I think, you know, after this class, I hope that you'll be well-qualified to, like, get a job in some of the big shiny tech companies that have, uh, a large AI teams. Um, I think a lot of the most exciting work to be done today is still, is to go into the less shiny industries that do not yet have, um, AI and machine learning yet and to take it to those areas. Actually, on the way in, I was chatting with a student, um, that works in cosmology who was commenting, was that you? No, sorry. Who has it? So- oh, at the back, who was commenting that cosmology needs more machine learning, right? And, and, and maybe he'll be the one to take a lot of the ideas from deep learning into cosmology because I think even outside the shiny tech areas like- and, and maybe since I hope it would play a role in their AI transformation of two large web-search companies I'm like done transforming Internet search companies. And I think that- but I think- and I think it's great that we have those great AI teams like Google Brain, Baidu AI group, other large tech companies have great AI teams and that's wonderful. I think a lot of the important work to be done, that I hope many of you will do, is to take AI to healthcare, take AI to computational biology, take AI to civil engineering, take AI to the mechanical engineering. I think all of this is worth doing. Um, just like electricity didn't have one killer app, it's useful for a lot of things. And I think, uh, uh, many of you will go out after this class and execute many exciting projects, both in tech companies and in, you know, other areas that, that- like, like, like cosmology, right? Uh, or other areas that were not traditionally considered, um, CS areas. Um, so, uh, just wrap up with, uh, uh, uh, two last thoughts. Um, I think that, uh, one of the things that excites me these days is I'm hoping, uh, you know, I, I wanna share with you one of the lessons I learned, right? Uh, watching the rise of AI in multiple companies and spent lot of time thinking about, you know, what is it that makes a great AI company. And one of the lessons I learned, um, was really, uh, hearing Jeff Bezos speak about what is it that makes for an internet company, right? And I think a lot of lessons, um, that we learned with the rise of the Internet will be useful, you know, and Internet was maybe one of the last major technological ways of disruption. And just as it has a great time to start working on the Internet maybe 20 years ago. I think today is a great time to start working on AI or deep learning. And so, is there a way to turn on the lights on this side as well? Do I- do I control that? [NOISE] Oh, thank you. Oh, thanks again. Great. So, so, I wanna show you one of the lessons I learned. Really, spend lot of time trying to understand the rise of the Internet because they will be useful to many of you as you navigate the rise of machine learning AI in your upcoming careers as well. Which is, um, one of the lessons I learned was, uh, you can take your favorite shopping mall and build a website for the shopping mall. That does not turn your shopping mall into an Internet company, right? So, you know, uh, like my wife, like Stanford Shopping Center, uh, uh, and I, I, I, and Stanford Shopping Center has a website. But even if, you know, a great shopping mall sells stuff on a website, there's a huge difference between a shopping mall with a website compared to a true Internet company like an Amazon or, and, and whatever. So, what's the difference? Um, about five, six, actually, six, six, seven years ago, I was chatting with the CEO of a very large American retailer. And, uh, at that time, he and his CIO were saying to me, they were saying, "Look, Andrew, we have a website, we sell things on the website, Amazon has a website, Amazon sells things on the website, it's the same thing." But of course it's not. And today, this particular large American retailer's, you know, future existence is actually a little bit in question partly, [NOISE] partly because of Amazon. Um, so one of the lessons I learned, um, uh, uh, really variances with Jeff Bezos is that what defines an Internet company is not just whether you have a website, instead it is, have you organized your team or your company to do the things that the Internet lets you do really well? For example, Internet teams, uh, engage in pervasive A/B testing, right? We, we, we know that we can launch two versions of the website and just see which one works better, and so we learn much faster. Whereas a traditional shopping mall, you can't launch two shopping malls in two parallel universes and see which one works better. So, you just- it's just- it's much harder to do that. Um, we tend to have short shipping times, right? You can ship a new product every day or every week, and so you learn much faster. Whereas a traditional shopping mall may, uh, redesign the shopping mall once per, once every three months, right? Uh, and we actually organize our teams differently. Um, we tend to push decision-making down to the engineers or engineers and product managers. Uh, because in the traditional shopping mall, you know, things kind of move slower, and maybe the CEO says something, and then everyone just does what the CEO says and that's fine. But in the Internet era, we learned that, um, the technology and the users are so complicated that, uh, only the engineers and the product managers, for those who don't know what that is, uh, own, are close enough to their technology, to the algorithms and users to make good decisions. And so, we tend to push decision-making power in Internet companies down to the engineers or engineers and product managers. And you have to do that in the Internet era because that's how you organize a company or organize a team to do the things the Internet lets you do really well, right? So, I think that was the rise of the Internet era. Um, I think with the rise of the AI era or AI machine learning or deep learning, whatever you wanna call it, um, we're learning that if you have, you know, a traditional company plus a few neural networks that does not by itself turn the company to an AI company, right? And I think what will define the great AI teams of the future, um, uh, will be, do you know how to organize your own work and organize your team's work to do the things that modern, you know, machine learning and deep learning and other AI things lets you do really well? Um, and I think, um, having met AI teams at Google and Baidu I'm a bit biased I think, you know, Google and Baidu are great and ahead of many other companies in thinking this through. But I think even the best companies in the world haven't completely figured out what are the principles by which to organize AI teams. But I think some of them will be that, um, we tend to, um, uh, I think that AI teams tend to be very good at strategic data acquisition. And so, you see AI companies or AI teams, even, even, uh, you know, do things that may not seem like it makes sense and why do these companies have all these free products, that don't make any money? Well, some of it is to acquire data that you can monetize through other ways, right? Uh, through advertising or through learning about users. And so, uh, there are a lot of data acquisition strategies that at the surface level may not make sense, but actually do make sense if you understand how this can be married with deep learning algorithms to create value elsewhere. Um, and I think that, uh, uh, AI companies tend to, um, organize data differently. AI teams tend to be very good at putting our data together. I think before the rise of deep learning, many companies have fragmented data warehouses, where if you have a big company, if you have 50 different databases, you know, in 50 different divisions, it's actually very difficult for an engineer to look at all this data and put it together and to train a learning algorithm to do something valuable. So, the leading AI companies, tend to have unified data warehouses. And I guess and I, I know we have a large home audience, the SCPD or other home audience here. So, if any of you work in large tech companies, you know, this is something that, that many companies are investing in today, to lay the foundation for learning algorithms. We tend to be very good at spotting pervasive automation opportunities,. Which is very good at spotting opportunities, where you could instead of having people do a task, have a deep learning algorithm do a task or have a different AI algorithm do a task. Um, and we also have a [NOISE] new job descriptions, which I don't have time to talk about. But just as with the rise of the Internet, we started creating a lot of new roles for engineers, right? I, I think, actually once upon a time, the world was simple and there was just a Software Engineering title. But as technology gotten- got more complicated, we started to specialize. So that's why, you know, with the Internet, we have front-end and back-end, mobile, right? And then we have, you know, and then we're increasingly other roles, right QA, DevOps, IT with increased specialization of knowledge. And so, with the rise of machine learning, we're starting the creation of new roles like, machine-learning engineer, resource- Machine Learning Research Scientist, er, and and our Product Managers in IT has also behave differently than Product Managers in tech companies. And so one of the, um, things we'll revisit a few times throughout this quarter is- and I don't mean to corporate, I know that many of you are- some of the SCPD audience or online audience already working in a company, many of you when you graduate from Stanford will end up maybe starting your own company or joining an existing company, but I think that's solving a lot of these questions, of how to organize your teams effectively in the AI era, will help you do more valuable work. And I think to, to, to make one more analogy, you know, I think that, one of the things I hope Kian and I will share with you throughout this quarter is, just as in the software engineering world, it took us a long time to figure out what is Agile development, right? or what are the pros and cons of, you know, Waterfall model versus Agile? Or, how do you- what is a Scrum process, right? Or is code review a good idea? It seems a good idea to me, right? But these, these practices after, after programming languages were created or invented or whatever, we still had to figure all these ways to help individuals and teams write software effectively. And so, if you worked in, you know, high-performing corporate, industrial AI teams using these software engineering practices, everything; code review, to Agile, to, to, to whatever, you know, you know that having a team work effectively to write software is more than everyone knowing C++ syntax, everyone doing Python syntax. And I think in the machine learning world, we're still in the process of inventing these types of processes. What is the strong, what is the Agile development, what's the equivalent of code review for developing machine learning algorithms and I think probably this class more than more than, this class and Machine Learning Yearning more than any other visuals I'm aware of right now. I think we'll try to systematically teach you these tools so that you don't just are able to derive a learning algorithm and, and implement a learning algorithm but that you're actually, you know, very effective in terms of how you go about building these systems. So last thing before I pass it to Kian is, the other question that I've been asked I guess several times this week now, that I'll just preemptively answer is, so, there are multiple machine learning classes going on at Stanford this quarter. So, the other frequently asked question is, which of these classes should you take? So let me just address that preemptively before someone asks me because I've been asked twice already in the other two classes this quarter. So, I think, actually what, what has happened over the last several years in Stanford is, the demand for machine learning education has, you know, been rising dramatically because the majority of CS PhD applicants to Stanford you, are applying to do work in machine learning or applying to do work in AI. And I think all of you can kinda of see that, there's such a shortage of machine learning engineers, right? And then there's a little bit of, and I think that shortage will continue for a long time. So I think many people see that. If you become expert in machine learning, there will be great opportunities for you to do meaningful work on campus, to take machine learning to comp-bio or- or cosmology or mechanical or do great research on campus as well as graduate from Stanford and do very unique work. When I wander around Silicon Valley, I feel like there are so many ideas for great machine learning projects that exactly zero people seem to be working on because there just aren't enough machine learning people in the world right now. So, by learning these skills, you could- you have many opportunities to be the first one to do something very exciting and meaningful, right. And, and you probably read in the newspapers about how much money machine learning people make, I'm actually much less more, I actually find that, I hope a lot you make a lot of money. Perhaps you you personally don't find that that, you know, as, as exciting. I think that, every time there's a major technological disruption, it gives us an opportunity to remake large parts of the world. And I hope that, some of you go improve the healthcare system, improve educational system, maybe, you know, see if we can help preserve the smooth functioning of democracy around the world. I think that it, it really your unique skills in deep learning would give you opportunities to do that. I think hopefully very meaningful work. But because of this massive, massive rise in demand for machine learning education, there are, so for longtime, CS229 machine learning was the core machine learning class at Stanford. And then CS230 is actually the newest, new creation I think. And the other class that we're involved in, that Eunice and I are involved in this quarter is CS229A. So, um, so if you are trying to decide which of these classes to take, um, I think, I think that these classes are little bit like Pokemon, right? You really should collect them all [LAUGHTER] [NOISE] But, but, but I think, we've been trying to design these classes to actually teach different things and not have too much overlap. Uh, uh, And so, there is- uh, so I have seen students take two classes at the same time and that's actually fine. There's not, the degree of overlap is fine, that you actually learn different things, if you take any two of these classes at the same time. CS229 is machine learning, is the most mathematical of these classes and we go much more, CS229 goes much more into the mathematical derivations of the algorithms. CS229A is applied machine learning, is much less mathematical but spends a bit more time on the practical aspects. Is actually the easiest on ramp to machine learning as well as the least mathematical of these classes. CS230 is somewhere in between. It's, it's a bit more, is more mathematical than CS229A, less mathematical than CS230. But where CS230 focuses on is on deep learning, which is just one small subset of machine learning but it is the hardest subset of machine learning. Whereas there are a lot of other machine learning algorithms, from your PCA, K-means recommender systems, support vector machines that are also very useful, that I use, you know in my work quite frequently, that we don't teach in CS230 but then it's taught in CS229 and CS229A. Where, so- so the unique things about CS230 is, it focuses on deep learning. So, I don't know, if you want to list deep learning on your resume, I guess maybe this is the easiest way to do it, I don't know after. Again it's not what I tend to optimize for but- but and I think CS230 goes the deepest in the practical know-how in how to apply these algorithms. And so, uh, and I, I want to set expectations accurately as well, right? So, what I don't want, is for you guys to complain in the other quarter that, you know, there wasn't enough math because that's actually not the point. What has happened in the last decade is, the amount of math you need to be a great machine learning person has actually decreased, I think. Uh, and I wanted to do less math in CS230 but spend more time teaching you the practical know-how of how to actually apply these algorithms, right? So, um, yeah. and I think 229A is probably the easiest of this class, that's the most technical, this is the most, most hands-on applied, you do a lot of projects on different, different topics, right? And I think these courses are often the foundation or some subset of these are often the foundational course as you would say. Because if you say, learn deep learning, so common sequence for our students is that, you know, learn the foundations of machine learning, uh, or uh, machine learning or deep learning. So, you have the foundation, uh, first before you go, which then often sets you up to later, go deeper into computer vision or natural language processing or robotics or deep reinforcement learning. And so common sequencing that, common tactic that Stanford students take is to use these as the foundation. You'll see a bit of everything from computer vision, natural language processing, speech recognition, you will touch a little bit on self-driving cars. But that gives you the foundation to then decide, do you want to go deeper into natural language processing or robotics or reinforcement learning or computer vision or something else. So these are common sequencing of classes that students take. Okay. So, um, look forward to spending this quarter with you. Let me just check where there are any quick questions and then I'll hand it over to Kian. Yeah, go for it. What's the third bullet in the AI era? Oh, what is the third bullet in the AI era? Decision-making by engineers and product managers. We'll be pushing decision-making, I wrote the decision making by engineers there, but really engineers and product managers. AI era I'm sorry. Oh, AI era, a pervasive auto, sorry, pervasive automation. Yeah, please So, uh, when you are talking bout AI being the next like electricity, um, I was wondering like, so- so to say that like uh, well like so far, you, like wha- what are the most, like the most meaningful successes of machine learning, that you think have happened already? So all of you are using learning algorithms probably dozens of times a day maybe even hundreds of times a day without knowing it. Right, every time you use a web search engine, there's a learning algorithm that's improving the quality of search results. There's also a learning algorithm trying to show you the most relevant ads and this helps those companies actually make a lot of money. Uh, every time, it turns out that um, uh, actually both Google and Baidu have publicly said that over 10 percent of searches on mobile are through voice search, uh, and so I think it's great that you can now talk [NOISE] to your cell phone rather than type on a tiny little keyboard if you're gonna [NOISE] do a- do a web search on mobile. Uh, if you go to, you know, websites like Amazon or Netflix or, uh, uh, uh, there are learning algorithms recommending more relevant movies or more relevant products to you. Uh, every time you use your credit cards, uh, there's a learning algorithm uh, trying to probably- for almost all companies I'm aware of, there's a learning algorithm trying to figure out if it's you using your credit card or if it's been stolen so they should- so they should, you know, disallow the- see if it's a fraudulent transaction or not. Uh, every time you open up your email uh, the only reason email is even usable is because of your spam filter which is because of learning algorithm that works much better now than, than, than before uh, I don't know, uh,uh, I,I yeah- so, so there's uh, I, I think, uh, you know, one of the amazing things about AI and machine learning is, I love it when it disappears in the background. Right, yo- yo- you use your-, you know, you use these algorithms. You boot up your map application then it finds the shortest route for you to drive from here to there, and there's a learning algorithm predicting what traffic would be like on highway 101 one hour from now. But you don't even need to think that there was a learning algorithm trying to figure out what traffic would be like one hour in the future. Seems pretty magical. Right, that, you know, tha- tha- that you could just use it. There is this, we could build all of these wonderful products and systems that helps people but abstract away a lot of details. So that's the present and I think in the future, near future, uh, most of my PhD students, my- my- most of my research group peers work on machine learning for healthcare. I think that will have significant in roads, you know, my, my, my team at Landing AI spending a lot of time with a lot of industries from manufacturing to agriculture. So all the things, uh, I'm excited about machine learning for education, uh, get people precise tutor help people with recommended precise content. There is fascinating research done here at Stanford by Chris Peach and a few others on using learning algorithms to give people feedback on coding homework assignments. Uh, I, th- so- so- sorry there are so many examples of machine learning I could talk for quite some time. Yeah. One last question I hand over Kian so. Yeah, go ahead. So will lectures be very different from what is on Coursera? Um, let's see. So the, uh, so the format of the class is that you watch, uh, videos created by deeplearning.ai and hosted on Coursera. So you'll see me a lot there. But in addition Kian and I, will be having lectures here in this classroom every Wednesday, and that will be, you know, completely new material that is not online anywhere. At least right now. Yeah- yeah. And then also the- I think that, tha- that the point- the point of that the flip classroom thing really is some of the things is really more time efficient for you to just learn online. So there's the online content. But what that does is it leaves this classroom time for us to not deliver the same lecture year after year but to get- try to get- spend time to get to know you and we have more time answering your questions and also give you more in course practice on these things. Right, so there is the Coursera DIY Coursera content. But what we do in CS230 is to augment to give you a much deeper practice, more advanced examples, some more deeper mathematical derivations, and more practice so you- so you deepen your knowledge of that. And with that, let me hand it over to Kian [NOISE]. Yeah, I'm gonna get back at him by making noise while he's talking. Yeah okay [LAUGHTER]. It's OK. Okay. Thanks Andrew. Hi everyone. Uh, I'm Kian. We're excited to have you here today. Those of you who are in class but also those of you who are SCPD students. Uh, we wanted to take a little more time to explain a little bit about the course logistics, what this course is about and also what it is to be a CS230 student in fall 2018. So, the course online is structured into five chapters or sub courses let's say. Uh, what we will teach you first is what is a neuron. You need to know that. After understanding what a neuron is you're going to be layers with these neurons. You're then going to stack these layers on top of each other to build a network that can be small or deep, uh, this is the first course. Unfortunately, it's not enough to deploy a network. Uh, just- just building, uh, neural network is not enough to get it to work. So in the second course, we're going to teach you the methods that are used to tune this network in order to improve their performances. This is the second part. Um, as Andrew mentioned, one thing we're really, uh, putting a huge emphasis on, uh, in CS230 is the industrial applications and how the industry works in AI. So the third course is going to help you understand how to strategize your project that you'll do to for the quarter, but also in general how do AI teams work? You can have an algorithm, you have to identify why does the algorithm work, why does it not work, and if it doesn't work, what are the parts that you should improve inside the algorithm? The two last courses, course fours-, ah, course four and five are focusing on two fields that are defined by two types of algorithms. First, Convolution Neural Networks that have been proven to work very well on imaging, um, or videos. And on the other hand sequence models that include also recurrent neural networks that are applied a lot in natural language processing or speech recognition. So you're going to see all that from the online perspective. Um, we use a specific notation in CS230. So when I will say C2M3, it refers to course two module three. So the third module of improving deep neural networks. Okay. And, I'd like everyone to go on the website CS230 syllabus after the class to look at all the syllabus for the quarter, check when the midterm is and when the final poster presentation is. Um, the schedule is posted there. Um, so check it out and we're going to use the Coursera platform as you know. So on Coursera, uh, you will receive an invite on your Stanford email and you should have received it already for course one, um, in order to access the platform. From the platform you will be able to watch videos, do quizzes, and do programming assignments. And every time we finish one of these courses, so C1 has four modules. When you're at C1 M4, you will receive a new invite to access C2 and so on. Okay. Inside CS230 we're going to use Piazza as a class forum for you to interact with the TAs and with the instructors. Uh, you can post privately or publicly depending on the matter. Okay. So let's see what it is to be One week in the life of a CS230 student. So we're going to do 10 times that over this- the fall quarter. So, what is one module? In a module, you will watch about, uh, 10 videos on Coursera, which will be about one hour and a half. Uh, you will do quizzes after watching the videos. This is going to take you about 20 minutes per module, and finally, you will complete programming assignments, which are on Jupyter Notebooks. You will get cells to test your code and also submit your code directly on the Coursera platform. In one week of class in Stanford, here, we will have two modules, usually. On top of these two modules, you will come to lecture for a one hour and a half in-class lecture on an advanced topic that is not taught online. And after that, uh, you will have TA sections on Fridays that are around one hour, and it's a good chance for you to meet other students for your projects, and also to interact with the TAs directly. Um, finally, we have also, uh, personalized mentorship this quarter, where, uh, every one of you will meet 15 minutes per week with the TA in order to check in on your projects, and gives you the next steps. So, we put a huge emphasis on the project in this class, and we want you-, you will see it later to build, to- to decide of your teams by this Friday in order to get started as soon as possible. Uh, next week, you will have your first mentorship meeting with the TAs, okay? It's gonna be fun. Uh, assignments and quizzes, uh, that are part of modules are due every Wednesday at 11:00 AM, so 30 minutes before class, so you can come to class with everything done and understand it. Uh, and do not follow the deadlines displayed on the Coursera platform, follow the deadlines posted on the CS230 website. The reason the deadlines are different is, because we wanna allow you to have late days, and Coursera was not built for late days, so we, we put the deadlines later on, on Coursera to allow you to submit even if you, you wanna use a late day. Does that make sense? Okay. So we are also using a, a kind of interactive, uh,- this, this is gonna start course two. We, we will use, uh, an interactive, uh, tool that is called Mentimeter, uh, to check in attendance in class and also, for you to answer some interactive questions. So, it's gonna start, uh, next, next week. Sorry, not course two. Uh, regarding the grading formula, uh, here it is. So, you have a small part on attendance, that is two percent of the final grade, eight percent uh, on quizzes, 25 percent on programming assignments, and, uh, big part on the midterm, and on the final projects. Uh, so this is posted on the website, if you want to check it. Uh, attendance is taken for in-class lectures for, uh, 15 minutes TA meetings and for the TA sections on Friday. You can have a bonus, and we've had students very active on, on Piazza, answered questions to other students, which was great, and they got a bonus, so I encourage you to, to do the same. Maybe we don't need TAs and instructors anyway. Okay. So, I- I wanted to take a little more time to go over, uh, some of the programming assignments that you're going to do this quarter uh, so that you, you know where you're going. Uh, in about three weeks from now, you're going to be able to translate these pictures here in the numbers that they corresponds to in, in sign languages, so it's sign language trans- translation from images to, uh, the output, uh, signification. Um, you're going to build a convolutional neural network, uh, and the first logistic regression and then a convolutional neural network in order to solve this problem. Uh, little later uh, you're going to be a, a Deep Learning engineer in a house that is not too far from here, called the Happy House. So, there is only one rule in this house, and the rule is that no sad person should enter the house, should avoid that. And because you're the only Deep Learning engineer that has the knowledge, you're given this task, which is don't let these sad people in, just let happy people in, and you're going to build the network, uh, that will run on a camera, that is in front of the house, and that is going to let people in or not. And unfortunately, some people will not get in, and other people will, will get in because they- they're happy, and you will save the Happy House at the end of the assignment, hopefully. Uh, this is, uh, one of the, the, the applications of, of deep learning I- that I personally prefer. It's called, uh, object detection. You, you might have heard of it, so this is running real time, and that- that's what is very impressive. You're going to work on, uh, Deep Learning architecture called YOLO v2, and YOLO v2 is an object detection algorithm that runs real-time, and is able to detect 9,000 objects, as fast as that. So, it's, it's really, really impressive. You have a few links here if you want to check the paper already, but maybe you will need, uh, some weeks to understand it well. Okay? [NOISE] Yeah, actually we have a- we can even run it directly on my computer, I think. It's going to be fun. [NOISE] We can run it. So here, you see it's running live on this computer, and so you see that if I move, it will find out that I move, so I cannot escape. Yeah, here it is. Okay. [NOISE] Okay, a few other projects, uh, one- two weeks from now, you will build an optimal goalkeeper shoot prediction. So, in soccer, you're a goalkeeper, and you want to decide where you should shoot the ball in order to make it land on one of your teammates. You're going to find, uh, what's the exact line on the field which tells the goalkeeper where to shoot, two weeks from now. About, um, i- in the- in the fourth course, uh, convolutional neural network, you're going to work on car detection, so this is a, a bigger image. Uh, this is exactly the programming assignment, so you're going to work on the autonomous driving application, that is finding cars, finding stop signs, finding lights, finding pedestrians and all the objects that are related to road features, okay? This is pretty cool, and you will generate these images yourself. So, these are pictures taken from a camera put in the front of, uh, of a, a car and was, was generated by Drive.ai. You will have a face recognition system that is going to first do face verification. Is this person, is this person the right person, but also face recognition, who is this person? Which is a little more complex. We're going to go over that together, uh, both online and in lecture. Art generation, some of you have heard of this, uh, it's an algorithm called Neural Style Transfer. And again, we usually put the, the papers at the bottom of the slides in case you want to check in yourself, for your projects. Uh, but this is a problem where you give the content image, which is the Golden Gate Bridge, and a style image which is an image that was painted usually by someone or an image from which you want to extract this style. This algorithm is going to generate a new image, is go- going to mix the contents of the first image with the style of the second image. Music generation, which is super fun, you're going to generate jazz music, um, in the fifth course, um, sequence models. You're going in the same course also generate texts by giving a huge corpus written by Shakespeare along time ago of poems. You're going to teach the algorithm to, to generate poems as if it was written by Shakespeare. So, you can even write the first sentence and is going to continue. Emojifier, you, you all have smartphones and I guess you notice that when you write a sentence on your smartphone, uh, it usually tells you what you should put next and sometimes it's an emoji. You're going to do this part. You're going to implement the algorithm that takes an input sentence and tells you what's the emoji that, that should come after it. Machine translation is a- is one of the application that has been tremendously performing well, uh, with deep learning. You're going to implement not a full machine translation from one language to another, but a similar task that is as exciting, which is, uh, changing human-readable dates to machine-readable dates. So, you know, let's say you're, you're, you're, you're filling in a form and you're typing a date. The, the entity that, that gathers this data will have a hard time convert all these dates into a specific format. You're going to implement the algorithm that is going to take all these different dates in different formats and generate the right formats, translated to human, from human-readable to machine-readable dates. Finally, trigger word detection that I also love and, and some of you have, have seen us build this algorithm, uh, a year ago I believe, which was- which Eunice and, and Andrew and I have, have worked on. Um, trigger word detection is the problem of detecting a single word. So, you know, you, you probably have, uh, objects from big, uh, companies that detect the voice and activate themselves under a trigger word. You're going to build this algorithm for the trigger word activate and many more projects that you will see. Now, these are the things that you will all build in this course. Every one of you will build it through programming assignments. But you also have to choose your own projects to work on throughout the course. And these are examples of projects that CS230 students have, have built in the past and which have wor- worked very well. One is coloring black and white pictures using a neural network into the color representation of these pictures. So, it's pretty cool because we can now watch, uh, movies that were, that were filmed in the 1930s or 1950s or I don't know when, uh, in color, which is super cool. Predicting a price of an object from a picture. So, this was a great project in the first iteration of CS230 where you give it a bike and the neural network guesses how much is the bike. So, if you wanna sell stuff you don't know how much, you just give it then you sell it at the, at the price. Uh, the student had actually implemented an algorithm to see which features of the bike are related to the price. So, it was super fun to see if it's the steering wheel or if it's the wheels or if it's the body of the bike that's makes this bike expensive according to the algorithm, and many more. So, last quarter, specifically, we had a lot of projects, uh, in physics and, uh, and astrophysics and chemical engineering and mechanics, which was great. Uh, some examples are detecting earthquake precursors signals with a sequence model. Um, predicting the atom energy-based on the atomic structure of an atom. So, you have, you have, for instance, softwares that run, that are really computationally expensive that look at the atomic structure of an atom and will output the energy of this atom. This takes a long time. These students have tried to make it a three second problem by running a neural network to find the energy of the atom. So, you have a bunch of problem across industries. So, healthcare, cancer, Parkinson, Alzheimer detection, we've had a lot of this. We've had brain tumor segmentation. Segmentation is a problem of on an image, classify every pixel. Tell me which pixel corresponds to the tumor, for example. So, we- we're really excited, uh, to see what you guys are going to build at the end of this quarter. And that's why we want you to build your teams very quickly, get started, because the project is what you should be proud of at the end of the quarter. We hope that you guys will come at the poster session proud of your poster, proud of the final project that you sent us and you can talk about it in the ten next years or 20 next years, hopefully. And I guess Andrew can, can, can confirm that CS229 students from the few past years have done projects that are amazing today and have been featured, uh, around the world in- as a researcher or, or industrial project. So, to sum up, in this course you will build a wide range of applications. Uh, it's very applied. There is some math but less than CS229 more than CS229A. Uh, and you have access to personalized mentorship thanks to the amazing TA team and the instructors. Um, and finally, we'll have, uh, to build a ten week long, uh, project. So, now we, we get to the serious thing. What is, uh, what we are up to this week. So, at the end of every lecture, you'll have one slide that's gonna remind you what you have to do for next week. Uh, next Wednesday, 11:00 AM. So, create your Coursera accounts based on the invite that you receive. If you didn't receive an invite, uh, Send it as a private posts on Piazza, we will send it again. Finish the two first modules of course one C1M1 and C1M2. It corresponds to two quizzes and two programming assignments and around 20 videos, okay, which are listed here. And for Friday, it means two days from now, uh, by the end of the day, uh, find project teammates and, uh, fill in the form to tell us who are your teammates. It's going to help us, uh, find you a mentor. Um, finally, there is a TA section also this Friday, no project mentorship. It will start next week, uh, but we, we will see you on Friday. Uh, I'm going to take a few questions if you have about. Yes? Are these slides also available to us? Yeah. These slides are going to be posted, uh, at the end of this class. So, the TA sections, we're going to have a large range of TA section on Friday. So, there's going to be, basically, every time you're going to be assigned to one of them and if you wanna move, you can send an email as a pro- um, uh, a Piazza post privately to ask to be moved to another section. So like how big is the team? How big is the team? Usually, it's from one to three students. Exceptionally, we, we would accept, uh, four students if the project is challenging enough, yeah. Yes. Can we combine the project with other classes as well? So, uh, it is possible to combine the project with other classes and it's been done in the past. Uh, what we want is you to, to give a project and a poster that, that is framed as CS230 wants it to be framed. And you discuss with us, in order for us to validate if you can merge this project with another class because it requires to have deep learning, of course. You- you're not supposed to combine this project with something that doesn't have deep learning at all. Okay. One more question. [NOISE] I think on Coursera you can retake quizzes. I think on Coursera you can retake quizzes and can we retake the quizzes in this class? So, you can, you can retake the quizzes as much as you want on Coursera. Uh, we will consider the last submitted quiz for this class. Okay. So, you can resubmit if you didn't get full way, yeah. Okay. Thanks guys and see you on Friday. 

Hello everyone? Welcome to the second lecture for CS230. So as I, I said earlier, uh, you can go on menti.com, uh, from your smartphones or your computers, and enter this code, 845709. Uh, we will use this tool for interactive questions during the lecture and we will also use it to, to track attendance. Uh, I'll add it at the end of the lecture, but, uh, if you have time do it now. [NOISE] Let's start the lecture, while you guys are doing that. Okay. So today's lecture is going to be about deep learning intuition, and the goal is to give you a systematic way to think about projects, everything related to deep learning. It includes how to collect your data, how to label your data, how to choose an architecture, but also how to design a proper loss function to optimize. So all of these decisions are decisions you're going to have to do, during your projects. And we'll try to give you here an overview of, uh, this systematic way of thinking for different projects. It's going to be high level, more than other lectures, but we hope it gives you a good start for your project. We will start with the ten minute recap on, uh, what you've seen in the two first, in the first week, uh, about neural networks. So as you know you can think of, uh, machine learning, deep learning in general, as modeling a function that takes an input that can be an image, a speech, a natural language, or a CSV file, give it to a box and get an output that can be classification. Is it a cat, zero, is, is there a cat on this image, output one, or is there no cat on this image, output zero? And I think a good way to remember what is a model is to define it as architecture plus parameters. Architecture is, uh, the design that you choose. So logistic regression is the first one you've seen. You will see shallow neural networks, deep neural networks, then you will see convolutional neural networks, and record neural networks. So these are all types of architectures and you can choose to make them deeper or shallower. Parameters are the core parts. They're the numbers that make your function take this cat as inputs and convert it to an output. So these are millions of numbers, and the goal of machine learning deep learning, is to find all these numbers. So we're all, uh, trying hard to find numbers basically, millions of numbers in matrices. If you give this cat and you forward propagate it, so we propagate it through the model to get an output. You will have to compare this output to the ground truth. Uh, the function used to do so is called the loss function. You've seen an example of a loss function this week. That is the logistic loss function. Uh, we will see more or loss functions, uh, later on. Uh, Computing the gradient of this loss function, is going to tell you how much should I move my parameters in order to update, uh, in, in order to make the loss go down. So in order to make this function recognize cats better than before. We do that many, many times, until you find the right parameters to plug in your architecture, you can then give your cats and get an output. What is very interesting and deep learning is that many things can change. You can change the input. We talked about natural language speech, structured and unstructured data in general. You can change the output, uh, It can be a classification algorithm, it can be a multi-class algorithm. I can ask you, give me the breed of the cats, instead of asking you give me just the cat, which makes the problem more complicated. It can also be a regression problem. I, I give you the cat and I ask you give me the age of the cat, which is much more complicated again. Does that make sense? Okay. Another thing that can change is the architecture, we talked about it earlier. And finally, the loss function. I think the last is function is something that, that people struggle with to understand what loss function to, to choose, uh, for a specific project and we're going to put a huge emphasis on that today. Okay. And, of course, in the architecture you can change the activation functions, in this optimization loop you can choose a specific optimizers. We're going to see in about three weeks, all the optimizers that can be Adam, stochastic gradient descent, batch gradient descent, RMSprop and momentum. And finally, all the hyper parameters. What is the learning rate of this loop? What is the batch that I'm using for my optimization? We are going to see all that together, but there's a bunch of things that can change in this scheme. Any questions on that, in general? So far so good. Okay. So let's take the first architecture that we've seen together, Logistic Regression. As you know, an image in computer science can be represented by a 3D matrix. Each matrix represents a certain color. RGB, red, green, blue. We can take all these numbers from these 3D matrix and put it in a vector. We flatten it in order to give it to our logistic regression. We forward propagate it. We multiply it by w, which is, our parameter and b, which is our bias. Give it to sigmoid function, get an output. If the network is trained properly, we should get a number that is more than 0.5 here to tell us that there is a cat in this image. So this is the basic scheme. Now, uh, my question for you is, if I want to do the same thing but, uh, I want to have a classifier that can classify several animals. So on the image there could be a giraffe, there could be an elephant or there could be a cat. How would you modify this architecture? Yes? [NOISE] [inaudible] Yes, exactly. So that's a good point. We could add several units. So several neurons, one for each animal and we will call it, multi-logistic regression. So it could be something like that. So we have a fully connection here, before we were all, all the inputs were connected to this neuron, and now we added two neurons. And each neuron is going to be responsible for one animal. How do we know which neuron is responsible for which animal? Is the network going to figure it out on its own, or do we have to help it? [NOISE] [inaudible] Exactly, the label is important. So what is going to tell your model this neuron should focus on cat, this neuron should focus on elephant, this neuron should focus on giraffe? Is the way you label your data. So how should we label this data, now, if we were to do this specific tasks. Any ideas? Yeah. Uh, [NOISE] One-hot vector. One-hot vector. Okay. So one-hot vector means, a vector with all zeros and one, one. Any other ideas? [NOISE] One, two, three. [NOISE] One, two, three. So I assume you, you say that each integer would correspond to a certain animal [NOISE]? Okay. Any other ideas? Modifying the loss function. Modifying the loss function. You mean, you want to put more weight on one animal, so you modify the loss function? Or what exactly- [NOISE] It was more like towards the one-hot encoding, but [inaudible] I see, with the one-hot encoding. So I agree with the one-hot encoding. I think there's a downside to the one-hot encoding. What is the downside of the one-hot encoding? [NOISE] [inaudible] Yes. So you're saying that the data without- if we have a lot of animals, the data- the labels only contain zero and one, one, so there's a huge imbalance there [NOISE]. I don't think that's an issue because these neurons are independent from each other right now. So yeah it, it could run into an issue if you have, uh, you have really a lot of animals, that's true. But there is another problem with it. The problem is that, do you think if you one, if you one-hot encode your labels, you would be able to detect an image with a giraffe and an elephant on the image? You will not be able to do so. You need a multi-hot encoding. So in this case, if there is a cat on image I will use a one-hot. I would say zero, one, zero as my label. But if I have a dog and a cat on the image I would say, one, one, zero. Okay. The one-hot encoding works very well when you have the constraint of having only one animal per image. And in this case, you would not use an activation function called Sigmoid, you would use another one, which is? [NOISE] Softmax. Softmax, yeah. The Softmax function, we're going to see it together. And for those of you who took 229, you've probably heard of it. Okay. So what I wanted to explain here is, the way you choose your labeling is very important and it's a decision you should make, prior to start the project. Okay. In terms of notation, uh, In, In this class we're going to use the following. A, square bracket one will denote all the activations of the first layer. So the square brackets would, would denote the layer and the lower script will denote the, their index of the neuron in the layer. Okay? And of course you can stack these neurons on top of each other to make the, the network more complex, depending on the task you're solving. Okay. Now, the concept I wanted to introduce in this recap was the concept of encoding. Uh, you probably- some of you have probably seen this image before. If you have, uh, a network that is not too shallow, you will notice that what the first neurons see are very precise representations of the data. So there are pixel level representations of the data. X3i is probably, one of the three channels of the 3D matrix, just one number. So what this neuron sees, is going to be a pixel level representation of the image. Okay? What this neuron sees, the second layer, the one in the hidden layer, is going to see the representation outputted by all the neurons in the first layer. These are going to be more high level, more complex. Because the first neurons will see pixels, they are going to output a little more detailed information, like, I found an edge here, I found an edge there, and so on. Give it to the second layer. The second layer is going to see more complex information and is going to give it to the third layer, which is going to assemble some high level complex features that could be eyes, nose, mouth, depending on what network you've been training. So this is an extraction of what's happening in each layer when the network was trained on, uh, face recognition. Yes. Um, doesn't this only apply to [inaudible] [NOISE] networks, because the combination [NOISE] [inaudible] does not necessarily, uh, [inaudible] . Yeah, yeah, yeah. So I think if I, here I give a you fully-connected network, but that's true. These type of visuals, ah, are more, ah, observed in convolutional neural networks because these are filters, but this happens also in this type of network, it's just harder to visualize. Okay. So, this is what we call an encoding. It means if I extract the information from this layer, so all the numbers that are coming out of these edges, I extract them, I will have a complex representation of my input data. If I extract the numbers that are at the end of the first layer, I will have a lower level representation of my data. That might be edges, okay? We're going to use this encoding, ah, throughout this lecture. Any questions on that? Okay. So let's build intuition on concrete applications. We're going to start, ah, with a short warm-up with the Day'n'Night classification, and then quickly move to Face verification and Face recognition. And after that, we'll do some Art generation and finish with a Trigger-word detection. If we have time, we'll, well talk about how to ship a model, which is shipping architecture plus parameters, okay, with an emphasis, as I said, on the architecture, the loss, the training strategy, to help you make decisions during your project. [NOISE] So, let's start with the first game. [NOISE] Ah, we're given an image and we have to build a network that tells us if the image is taken during the day, label zero, or was taken at night, label one. [NOISE] So, first question is, what dataset do we need to collect? Imaging captured. Um? Imaging that are captured during the day and during the night and it's labeled. Okay. Labeled images captured during the day and during the night. I agree, though probably, oh, yeah, let me ask the question. How many images? [LAUGHTER] That was wrong, actually. [LAUGHTER] How many images, like how do you get this number? [NOISE] Can someone give me an estimate of how many images you need in order to solve this problem, and explain how you get this estimate. A number that's similar to a number of parameters. You're saying a number similar to a number of parameters that you've in the network? Yeah. So I think it's better to think of it in the other way around. The network comes after, so you, right now, you don't know what networks you will use. So you cannot decide the number of data points based on your parameters. Later on, based on how your network is flexible, you can add more data, and a- ah, that's probably what you meant. But, at first, you want to get, you want to get the number. Yeah. More o- more images than pixels within an image? More images than pixels within an image. Ah, I- I don't think that, that, that's, that that has anything to do with the pixel within an image. You can have a very simple task, like, you have only images that are red and green, and you want to classify red and green. [NOISE] The image can be giant, you can have a lot of pixels, it's not gonna change the number of data points you need. Maybe images that have computation resources [inaudible]? Okay. So, you're talking about computation resources, so m- the more images we have, probably the more computation resources we will need, is that what you mean? Yeah, there is something like that. I think, in general, ah, you want to try to gauge the complexity of the task. So, let's say, we did a problem that was cat recognition. Detect if there is a cat on an image or not. In this problem, we remember that with 10,000 images, we managed to train a pretty good classifier. How do you compare this problem to the cat problem? You think it's easier or harder? I think it's easier. Easier. Yeah, I agree. That's probably easier. [NOISE] So in terms of complexity, these tasks looks less complex than the cat recognition task, so you would probably need less data. That's a rule of thumb. The second rule of thumb and why I get to this image is, what do we exactly want to do? Do we want to classify pictures that were taken outside, which seems even easier? Or do we want also the network to classify complicated pictures? What, what do I mean by complicated pictures? Inside your house. Um? Inside your house. So like, let's say, on a picture you have a window on the right side. A human would be able to say it's the day because I see the window, but for the network, it's going to take much longer to learn that, much longer than for pictures taken outside. What else? What are other complicated, okay, in the back. Uh, like dawn or twilights or edges, um- Dawn, twilight, sunrise, sunset, in general? It's complicated because you have to define it and you have to teach your network what, what does that mean, is it night or day. Okay. So, depending on what task you want to solve, it's going to tell you if you need more data or less data. I think, for this task, if you take outside pictures, 10,000 images is going to be enough, but if you want the network to detect indoor as well, you probably need 100,000 images or something. And this is based on comparing with projects you did in the past, so it's gonna come with experience. Now, as you know, when you have a dataset, you need to split it between train, validation, and test sets. Some of you have heard that. We are going to see it together even more. You need to train your network on a specific sets and test it on another one. How do you think you should split these 10,000 images? Um? 50-50 between train and test? 80-20. 80-20? I think we, we, we go more towards 80-20 because the test sets is made for analyze, to analyze if your network is doing well on real-world data or not. I think 2,000 images is enough to get that sense, probably, and you want to put complicated examples in this dataset as well, so I would go towards 80-20. And the bigger the datasets, the more I would put in the train set, so if I have one million images, I would put even more like, 98 percent, maybe, in the train set, and two percent to test my model, okay? Now, I wrote bias here. What do I mean by bias? You just have a correct, like, balance between classes. Yes. You need a correct balance between classes. You don't want to give 9,000 dark images and 1,000 day images. You want to balance between these two to teach your networks to recognize both classes. Okay. What should be the input of your network? Um? The pixel image. Yeah. So, this is an example of a pixel image. It's the Louvre Museum during the day. [NOISE] Harder question. What should be the resolution of this image, and why do we care? The more resolution [inaudible] [NOISE] Okay. That's great. So, you said, let me repeat for SCPD students as well, as low as you can, in order to achieve good results. Why do we want low resolution? It's because in terms of computation, it's going to be better. Remember, if I have a 32 by 32 image, how many pixels there are? If it's color, I have 32 times 32 times three. If I have 400 by 400, I have 400 by 400 by three. It's a lot more. So I want to minimize the resolution in order to still be able to achieve good performance. So what does it mean to still achieve good performance? How do I get this number? I'd continue with a similar resolution as opposed to the, uh, partial [inaudible]. Okay. Similar resolution as you expect the algorithm in real life to work on? Yeah. Probably, I agree. What else? What other rule of thumb can you use in order to choose this resolution? Perhaps, um, we compare it to the performance of the [inaudible] we can tell if it's there [inaudible]. Yeah. Great idea. Compare to human performance. So what I do, so there is one way to do it, which is the brute force way, I would say. We will train models on different resolutions and then compare the results, or you can be smart and use human performance as a comparison. So I would print this image or several images like these in different resolutions on paper. And I would go see humans and say classify those, classify those, and classify those. And I would compare human performance on all these three types of resolution, in order to decide what's the minimum resolution that I can use, in order to get perfect human performance. So by doing that, I got that 64 by 64 by three was enough resolution, for a human, to detect if an image is taken during the day or during the night. And this is a pretty small resolution in imaging, but it seems like a small, like an easy task. If you have to find a d- d- a breed of a cat, you probably need more because some cats are very, look very alike, and you need a high resolution to distinguish them, and maybe training for the human as well. I know only three breeds of cats so I wouldn't be able to do it anyway. What should be the output of the model? Labels about the image. Labels, so Y equals zero for day, Y equal one for night. I agree. What should be the last activation of the network? [NOISE] The last function? Sigmoid. Sigmoid. We saw that Sigmoid takes a number between plus infinity- minus infinity and plus infinity, puts it between zero and one so that we can interpret it as a probability. What architecture would you use? Fully-connected or convolutional. Fully-connected or convolutional. I think, later this quarter, you will see that convolutionals perform well in imaging, so we would directly use a convolutional, but I think a shallow network, fully-connected or convolutional, would do the job pretty well. You don't need a deep network because you gauge the complexity of this task. [NOISE] And what should be the loss function, finally? [NOISE] It could be, um, maximum number of functions like, uh, log-likelihood. Yeah. So, the log-likelihood. So, it's also called the logistic class, that's the on you're talking about [NOISE]. So, the way you get this number and you'll prove it in CS 229. We're not going to prove it here. But basically, you interpret your data in a probabilistic way and you take the maximum likelihood estimation of the data which gives you this formula, for those of you who did the math behind. You can ask in office hours, TA is going to help you understand it more properly. Okay. And of course, this means that if y equals zero, we want y hat the prediction to be close to zero. If y equal one we want y hat the prediction to be close to one. Okay. So, this was the warm up. Now we're going to delve into Face verification. Any you question on day and night classification. Yes. You said that you increase the data without the percentage that changes so you have a kind of [inaudible]. So, your- the question is about how you choose the size of the test set versus the train set. In general, you would first say how many images do I need or data points in order to be able to understand what my model do in the real world. This can depend on the task. Like if I talk about- if I- if I tell you about speech recognition, you want to figure out if your model is doing well for all accents in the world. So, your test set might be very big and very distributed. In this case, you might have a few examples that are during the day, few during the night and a few at dawn, on sunset, sunrise and also indoor. Three of those is going to give you a number. So, there's no good number. There is like you have to gauge it. Okay one more question. How do you chose that loss function [inaudible]? Yeah, that's a good question. So, how do you choose the loss function? We're going to see in the next, uh, in the next slides how to choose loss functions but for this one specifically, you choose this one because it- it- it's a, it's a convex function for classification problem. It's easier to optimize than other loss functions. So, there is a proof but- but I will not go over it here. If you know L1 loss, that compares Y to Y hat this one is harder to optimize for a classification problem, we would use it for regression problems. Okay. [NOISE] So, our new gain is the school wants to use face verification to validate student IDs in facilities like the gym. So, you know, when you enter the gym, you swipe your ID and then, uh, I guess the person sees your face on the screen based on this ID and looks at your face in real and comparison let's say. So, now we want to put a camera and have you swipe and the camera is going to compare this image to the image in the database. Does that make sense? To let you in or not. So, what's- what dataset do we need to solve this problem? What should we collect? Yeah. Okay. Between the ID and the image. Yeah, so probably schools have databases because when you enter the school you submit your image and you also are given a card, an ID. So, you have this mapping. Okay. What else do we need? So, pictures of every student labeled with their names, that's what you say. So, this is a picture of Bertrand. This is a picture when he was younger. And that's the one he gave to the school when he arrived. What should be the input of our model? Is it this picture? More photos of him. More photos of him. More photos of him. I'm asking just like the input of the model. Like we probably need more photos of him as well but what's- what's going to be the image we give to the model? Exactly the person standing for verification. Exactly, the person standing in front of the camera when entering the gym. So, this is the entrance of the gym and Bertrand is trying to enter the gym. So, it's him. Okay. What should be the resolution? Those of you who have done projects in imaging, what do you think should be the resolution? 256 by 256. 256 by 256, any other idea more precisely. I think in general [NOISE] you will go over 400, so 400 by 400. What's the reason? Why do we need 64 for- for day and night and 400 for face verification? The video takes different shapes. Yeah. There's more details to detect. So, like distance between the eyes probably, size of the nose, mouth, uh, general- general features of the face. These are harder to detect for a 64 by 64 image. And you can test it, you can go outside and show two pictures of people that look like each other and ask people can you differentiate those two person or not. And you'll see that with less than that sometimes it's- people are struggling. Is color important? Is color important. That's a good question. We should have talked about it in day and night actually. Is color important. Because if you remove the color, you basically divide by three the number of pixels, right? So, if we could do it without color, we would do it without color. In this case, color is going to be important because, uh, probably you want your camera to work in, uh, different settings, day and night as well. So, the luminosity is different, the brightness and also we all have different colors and we need to all be detected, compared to each other. Yeah. I might go somewhere in an island and come back, uh, you know, full of color but, uh, but I still want to be able to access the gym. Uh, output. What should be the output? The question on the resolution, is that a minimum resolution or is that like a- I think if you have mo- in unlimited computational power, you would take more resolution but that's a trade-off between computation and resolution. So, output is going to be one, if it's you and zero if it's not you in which case they will not let you in. Okay. Now, uh, the question is what architecture should we use to solve this problem now that we collected the data set of mapping between student IDs and images. The question is how do you know how many images you need to train the network- The question is- [OVERLAPPING] [inaudible]. How do you know how many, many images you need to train the network. You don't know, you can find an estimate. It's going to depend on your architecture. But in general, uh, the more complex a task, the more data you will need. And we will see something called error analysis in about four weeks which is once your network works, you're going to give it a lot of examples. Detect which examples are misclassified by your network and you're going to add more of these in the training set. So, you're going to boost your datasets. Okay. Talking about the architecture. If I ask you, what's the easiest way to compare two images, what would you do? Like these two images, the database image and the input image. Some sort of hash. Some sort of hash, what do you mean by that. Taking the input run, uh, set a specific function on it and then there. Okay. Take an- take this, run it into a specific function, take this run it into a specific function and compare the two values. That's great. That's a good idea. And the more basic one is just compute the distance, uh, between the pixels. Just compute the distance between the pixels and you get if it's the same person or not. Unfortunately, it doesn't work and a few reasons are the background lighting can be different. And so if I do this minus this, this pixel which is let's say dark is going to have a value of zero, this pixel which is white is going to have a value of 255, the distance is gigantic but it's still the same person. Is a problem. Person can wear makeup, can grow a beard, can be younger on a picture, the ID can be outdated. So, it doesn't work to just compare these two pictures together, we need to find a function that we will apply these- these- these two images to and will give us a more- a better representation of the image. So, that's what we're going to do now. What we're going to do is that will encode information, use the encoding that we talked about of the picture in the vector. So, we want a vector that would represent teachers like distance between eyes, nose, mouth, color, all these type of stuff, hair,, uh, in a vector. So, this is the picture of left Bertrand from the ID. We will run it to a network and we hopefully can find a good encoding of this network. Then we will run the picture of Bertrand at the facility run it in the deep network, get another vector. And hopefully if we train the network properly, these two vector should be close to each other. Lets say we have a threshold that is 0.5, 0.4 is the distance between these two. Is less than the threshold. So, I would say Bertrand is the right person. Is you. Does this scheme make cha- make sense. What is the 1.28 d represent? What does the 1.28 d vector represent. The real question is can I say that the third entry corresponds to something specific? It's complicated to say but depending on what network you choose and the training process you choose, it will give you a different network, a different vector. So, that's what we're going to talk about now. The question is how do I know that this vector is good? Like right now, if I take a random network, I give my image to it, is going to output a random vector. This vector is not going to contain any useful information. I want to make sure that this information is useful and that's how I will design my loss function. Okay. So, just to recap, we gather all students faces encoding in a database once we have this and given a new picture, we compute the distance between- between the new picture and all the vectors in the database if we find a match. Oh sorry. We compare this vector of the input image with the vector corresponding to the ID image. If it's small, we consider that is the same person. Okay. Now talking about the loss and the training to figure out is this vector corresponds to something meaningful. First, we need more data because we need our model to understand in general the features of the face and a university that has a 1000 students is probably not going to be enough to have 1000 image in order to push a model to understand all the features of the face. Instead we will go online find open datasets with millions of pictures of faces and help the model learn from these faces to then use it inside the facility. There was a question in the back. Why couldn't [inaudible] work out like we did with the, like the- [inaudible] but every student is uh, one? That's another option. So the question is why can we tell continues the one-hot encoding. We could build a classifier that has n output neurons, n corresponding to the number of students in the school and you take an image you run it to the network is going to tell you which student it is. What's the issue with that? Every year students enter the school you will have to modify your network every year because you have more students and you need a higher output vector, a larger output vector. You- we don't wanna retrain all the time our networks. Okay, so what's- what, what we really want, if, if we wanna put it in words, is that's uh, oh, there's a mistake here. What we really want is, if I give you two pictures of the same person, I want to similar encoding, I want the vector to be similar. If I give you two pictures of different persons, I want different encodings, I want the vector to be very different and we are going to rely on these two assumptions and these two thoughts in order to generate uh, our loss function by giving it triplets, triplets means three pictures: one that we call anchor, that is the person, a person, one that we call positive, that is the same person as the anchor but a different picture of that person and the third one that we call negative, that is a picture of someone else. And now what we wanna do is to minimize the encoding distance between the anchor and the positive and maximize the encoding distance between the anchor of, and the negative. Does, the- these two thoughts makes sense? So now my question for you is, what should be the loss function? What should be the loss function, so please go on menti and enter the code and there are three options here A, B and C,choose which of these you think should be the right loss function to use for this problem. Uh, you have it on your phone as well, like issue, yeah, it's small on the screen but you can see it on, on its cutoff? It's better here? [NOISE] We can't see the URL [inaudible]. It's too small. [NOISE] A45709, can you see it on your phone? So by Enc of A, I mean the encoding vector of the anchor, by Enc of P, I mean the encoding vector of the positive image after you run them through the network. [NOISE] Okay 30 more seconds. [NOISE] Okay. I- 20 more seconds. Okay let's see what we have. Okay. So, two-thirds of the people think that's, that it's the first answer A, so I, I read it for everyone, the loss is equal to the L2 distance between the encoding of A and the encoding of P minus the L2 distance between the encoding of A and the encoding of N. So, someone who has answered this, do you wanna give uh, an explanation? Yes. We're are trying to minimize the first difference between N the positive and we're trying to maximize difference between A and the negative let me subtract, so the [inaudible]. Yes, that's correct. So what you said I repeat it [NOISE] for [inaudible] students. We wanna maximize the distance between the encoding of A and the equity of the negative, that's why we have the minus sign here, because we want the loss to go down and to go down we put a minus sign and we maximize this term and on the other hand we wanna minimize the other term because it's a positive term, okay so I agree with answer. Okay, that was the first time you use this tool, it's gonna be quicker next time. Okay, so we have uh, we have uh, figure out what's the loss function should be and now thinking about it. Now that we designed our loss function, we're able to use an optimization algorithm, run an image in the network, sorry run, run three images in the network, like that. Gets three outputs encoding of A, encoding of P, encoding of N, compute the loss, take the gradient of the loss and update the parameters in order to minimize the loss. Hopefully after doing that many times we would get an encoding that represents features of the face because the network will have to figure out who are the same people, who are different people. Does it make sense? This is called the triplet loss. And I cheated a little bit in the, in the quiz, I didn't write this alpha. The true loss function contains a small alpha, you know why? Yes? So we don't have negative loss? [NOISE] Yeah that- that's not exactly the role of the alpha, in order to not have negative loss what, what you can do is to use a maximum of the loss and zero and train on the maximum of the loss and zero but there is another reason why we have this alpha. Yes? [inaudible] to have uh, difference between like false negative and false positive like which one do you prefer? Which one do you prefer based on false negative and false negative, no i- it- it's not about that. So sometimes you have an alpha in loss function to put a weight on some classes but this is an additional alpha, it's not a multiplicative alpha. So, it has nothing to do with that. Yeah? To penalize large weight. To penalize [NOISE] large weight, so you're talking about [NOISE] penalization. If we had weights in this formula next to the alpha like alpha times the norm of the weights, this would be regularization, but here this term doesn't penalize weight. [inaudible]. It's not gonna affect the gradient, it's not gonna affect, it's not gonna affect the weights, but the reason we have it here is because let's say the encoding function is uh, let's say the encoding function is just a function zero. What we are going to have is that we're going to have encoding of A equals zero minus zero and here zero minus zero and so we will have basically a perfect loss of zero uh, and we still didn't train our network, we just learned the function null. So this alpha is called the Margin and it pushes your network to learn something meaningful in order to, to stabili- stabilize itself on, on zeros. Okay? [NOISE] [inaudible]? Yeah, so it also has to do with the initializations but because we didn't talk about initialization yet we only saw zero initialization, I think in concentration to- together. Another way to, to, to avoid uh, the networks to stabilize or to, to become stable on zero is to change the initialization scheme and in two weeks we're going to see difference initialization schemes together. [NOISE] Yeah? [inaudible]. [NOISE] So, the question is how do we know that this network is going to be robust to rotations of the image, or scaling of the image, or translation of the image? We know it's because in the dataset, we are going to give let's say your picture and your picture scales, and we're going to tell the network this is the same person. So, the network will have to learn that the scale doesn't mean it's not the same person. You have to learn this feature. Okay. One more question and then we move on. In front, yes. So why is it starting at zero a problem? Can't we just make it negatives loss value? Yeah. That's a good question. Why is it a problem to, to, to stay at- to stabilize at zero? It's because its common to keep then the loss function positive, and in the paper that you can find, this FaceNet paper, they don't train exactly this loss, they train the maximum of this loss and zero. Yeah. Okay. So you train and you get the right function. Now, let's make the problem a little more complicated. What we did so far was face verification, we're going to do face recognition. What's the difference? The difference is there is no more ID. So now you just have a camera in the facility, you enter, the camera looks at you and find you. How would you design this new network? Yes, in the back. [inaudible] you've added in an element now of recognition as well, because now before you'd sort of stand in front of it and it new that every picture had a face, now it needs to detect the face. Okay. So you're saying maybe we need to add an element to the pipeline that is a diction- detection element. That's true in general for face recognition. Uh, let's say you have a picture that is quite big. You want to use the first network that identifies the face, like finds it on the picture, detects it, and then crop the face and give it to another network, that's true. That could also be used in verification as well. Yeah. [inaudible] because they are taking more and more time to go through all the faces in your database. Great. So the difference maybe with what you're saying is maybe we can use a verification algorithm that you've trained. But instead of looking one-to-one comparison we look at one to N comparison. So we have the pictures of all the students in the database. What we can do is run all these database pictures in the model, get a vector that represents them, right? We get the vectors. Now, you enter the facility, we get your picture, we run it through the model, we get your vector and we can compare this vector to all the vectors in the database to identify you. What's the complexity of this? It's the number of students. You have for every prediction to go over the whole database. And a common network like model that you can use to do that is K-Nearest Neighbors. So, of course, if you have only one picture per students, it's not going to be very precise. But if you collect three pictures per students and you run a two nearest neighbors algorithm, it will decide that if the two pictures are the same it's likely that this person is the same as the two person on the picture. Okay? Now, let's make it a little more complicated. You probably saw that on your, on your phones, uh, sometimes you take a picture and it recognizes that it's, uh, your grandmother or your grandfather or your mother and father. Uh, what's happening behind is that there's some clustering happening. It means we have a bunch of images and we wanna cluster them together. So this is also another algorithm that you see in CS229 and CS229A, which is K-Means algorithm. And this is a clustering algorithm by taking all the vectors that we have in the database. We can find, uh- Let's say, sorry, you have a- you have a your phone, you have thousands of pictures of let's say 20 different people. What you want is to cluster all the pictures of the same person separately. What you will do is that you will encode all the pictures in vectors, and then you will run a clus- clustering algorithm like K-means in order to cluster those into groups. These are the vectors that look like each other, these are the vectors that look like each other. Okay? And then you can simply give folders to the users with all the pictures of your mom, all the pictures of your dad and so on. How to, uh, define the K in this case. Sometimes like obviously all the people [inaudible]. Good question. How- how do you define the K? So someone has an idea actually. [inaudible]. Yeah. So one- one way is to, as you said, to try different values, trainer clustering algorithm and look at a certain loss you defined how small it is. There's actually an algorithm called X-means, that is used- X-means, you might search for that if you want- to find, uh, to find the K. There is also a method called the Elbow Method and that you want to search for as well to figure out the K. Okay. And, as you said, maybe we need just to detect the face first and then crop and give it to the algorithm. One more question on, on face verification and connection. So would you also use the, like factor of [inaudible]. Sorry, can you- can you repeat louder? Do you also need to use that vector that you trained for [inaudible]? Do you need to use the vector that you trained for classification? Um, sorry, I do, I do not understand. So you mean could- Yeah. So is the vector after you've changed the [inaudible]? Oh, so where is the encoding coming from? That's what you mean in, in the network? Yeah. Okay. Good question. So you have a deep network and you want to decide where should you take the encoding from. In this case, the more complex the task, the deeper you would go. But for face verification, what you want and you know it as a human, you want to know features like, uh, distance between eyes, nose and stuff, and so you have to go deeper. You need the first layers to figure out the edges, gives the edges to the second layer, the secondary to figure out the nose, the eyes, give it to the third layer, the third layer to figure out the distances between the eyes, the distance in between the ears. So you would go deeper and get the encoding deeper because you know that you want high level features. Okay. Art generation, given a picture and make it look beautiful. As usual, data. What do we need? A little complicated because we have to define what beautiful is. [NOISE] So data some beautiful pictures? I don't know, maybe my concept of beautiful is different than yours. [NOISE] A certain style that we want. Data in the certain style that we want. That's a good point. So we might say that beautiful means paintings, like paintings are usually beautiful. So you want to have a, that kind of a style. Yeah, that's true. So let's say we have any data that we, we want. What we're going to do and the way we define this problem is let's take an image that we call the content image, and here again you have the Louvre Museum. And let's take an image that we call the style image, and this is a painting that we find beautiful. What we want is to generate an image that looks like it's the content of the content image, but painted by the painter of the style image. So this style image is Claude Monet and here we have the Louvre painted by Claude Monet, even if, uh, he was dead when this pyramid was created. So that's our goal and this is what we would call art generation. There are other methods, but this is one. So how do we do that? What architectures do we need? And please try to use what we've seen in the past two applications together. [NOISE] What training scheme, what application, what, what architecture [NOISE]. No one wants to try? Yes. [inaudible] Yeah. [inaudible] So you're saying we, we give- we take some style images, we give it as input to a network and the network outputs yes or no, like one or zero? So do we want to generate? We wanna generate an image, yes. Okay. So given [inaudible]. Okay. Yes, probably. So what you're proposing is we get an image, that is the content image, and we have a network that is the style, style network, which will s- style this image and we will get the content but styled version of the content. Right. So it will take certain features of that style and use this to change the output. [NOISE] Yeah. So we use certain features of the style and change this style according to what the network is. So this is actually done. This is one method. That's not the one we will see today. But [NOISE], uh, the issue with this method, which is a small issue, is that you have to train your network to learn one style. Network learns one style, you give the content, it gives you the constant with the specific style of the model. What we want to do is to have no model that is restricted to a specific style. I wanna be able to give a painting of Picasso and get this picture painted by Picasso. So the difference here is that we're not, we're not going to learn parameters of a network like we did for face verification or for day and night classification. We're going to learn an image. So, you remember when we talked about backpropagation of the gradient to the parameters, were not going to do that. We're going to backpropagate all the way back to the image. Let's see how it works. So, first, we have to understand what content means and what's style means. To do that, we're going to use encoding. We're going to, to, to, to use the ideas that we talked about later. Giving the content image to a network that is very good will allow us to extract some information about the content of this image. We specifically saw together that earlier layers will detect the edges. The edges are usually a good representation of the content of the image. So I might have a very good network, give my content image, extract the information from the first layer, this information is going to be the content of the image. Now, the question is how do I get the style? I wanna give my style image and find a way to extract the style. That's what we're going to learn later in this course, it's a technique called Gram matrix. And the important thing to remember is that, the style is non-localized information. If I show you, uh, the, the pictures in the previous slide, oh sorry, here, you see that in the generated picture, although on the style image there was a tree on the left side, there's no tree on the generated image. It means when I extracted the style, I just extracted non-localized information. What's the technique that Claude Monet has used to paint? I didn't want to extract these tree that was on the style image, don't want the content. Okay. So we're going to take a network that understands images very well, and they're common online. You can find ImageNet classific- classification networks online, that were trained to recognize more than thousand- thousands of objects. This network is going to understand basically anything you give it. If I give it the Louvre Museum it's going to find all the edges very easily, it's going to figure out that there is- it's during the day, it's going to figure out their buildings on the sides and all the features of the image because it was trained for months on thousands of classes. Let's say we have this network, we give our content image to it and we extract information from the first few layers. This information we call it contents C, content of the content image, does that make sense? Now, I give the style image and I will use another method that is called the Gram matrix to extract style S style of the style image, okay? And now the question is; what should be the loss function? So let's go on Menti. So same code as usual, just open it. If you wanna repeat- you can repeat the code if you want, 845709, and these are the three proposals for the loss function. So reminder, content C means content of the content image, style S means style of the style image, style G means style of the generated image, content G means content of the generated image. Take like a minute. It's too small? Oh, the code, up, 845709. So why do need to have an image in that class? You don't actually need to classify an image on [inaudible] So why do you need to use ImageNet, [inaudible] ? Why- so just repeating the question, why do we need to use ImageNet? Because we, we don't really need to classify an image and it's going to waste time. Uh, the reason we use ImageNet is because ImageNet understands our pictures. So if, if you give the content image to a network that doesn't understand pictures very well, you're not going to get their edges very well. So you want a network-. I don't care about the classification of the pictures. You don't care about the classification output, you just cut the network in the middle, extract the layers in the middle. Okay. Let's see what the answers are according to you guys. So if we are getting style- style of it, you are not training anything, right? So yeah, I repeat, we're not training anything here. We're getting a model that exists and we use this model. But we are going to talk about the training after. Okay. Someone who has answered the second, uh, question and I, I will read it out loud, the loss is the L2 difference between the style of the style image and the generated style, plus, the L2 distance between the gener- the generators content and the contents content. Yeah. We want to maximize both the [inaudible]. [NOISE] So yeah, we wanna minimize both terms here. So we want the content of the content image to look like the content of the generated image, so we wanna minimize the L2 distance of these two. And the reason we use a plus is because we also wanna minimize the difference of styles between the generated and the style image. So you see we don't have any terms that says style of the content image minus style of the generated image is minimized. This is the loss we want. Okay, up now. Okay. So just going over the architecture again. So the loss function we're going to use will be the one we saw. And so one thing that I want to emphasize here is we're not training the network, there is no parameter that we train. The parameters are in the ImageNet classification network, we use them we don't train them. What we will train is the image. So you get an image and you start with white noise, you run this image through the classification network but you don't care about the classification of this image. ImageNet is going to give a random class to this image, totally random. Um, instead, you will extract content G and style G, okay? So from this image, you run it and you extract information from this network using the same techniques that you've used to extract content C and style S. So content C and style S you have it, you have it. You're able to compute the loss function because now you have the four terms of the loss function. You compute the derivatives, instead of stopping in the network, you go all the way back to the pixels of the image and you decide how much should I move the pixels in order to make this loss go down, and you do that many times. You do that many times. And the more you do that, the more this is going to look like the content of the content image, and the style of the style image. Yeah, one question. So for each new example of content and style images you need to do a new training like this? Yeah. So the downside of this network, is although it has the flexibility to work with any style, any content, every time you wanna generate an image you have to do this training loop. While the other network that you talked about doesn't need that because the model is trained to, to convert the content to a style, you just give it and cool. Do you have to train the network on many, kind of like Monet images or you only need to do those kind of like Monet? Which network you talk about, this network? Yes. Yeah. So do we need to train this network on Monet images? Usually not. This network is trained on millions of images. It's basically seen everything you can imagine. Yeah. So you only need to give one art piece to it and then it will be able to back-propagate properly into any [inaudible]. Uh, what you mean back-propagate properly. Here you're not training the network. You are getting this image. Computing the backpropagation and going back to the image, only updating the image you don't update the network. Where does the rps-? It comes from Content C and style S, it comes from the Stye S. So, the loss function you bake- the baseline is you have Content C and style S because you've chosen a content picture and a style picture and now every, at every step you will find the new Content G and Style G. Backpropagates updates, [NOISE] give it again get the new Content G and Style G, update again and so on. [NOISE] No, the, the art never touches with- just one time. The art image just touches onetime the neural network you can, you extract style S and then that's all, you don't use it again. Okay let's do one more question here. Why do you start white noise instead of the content or the style? Good question. Why do you start with white noise instead of the content or the style? Actually do you think it's better to start with the content or the style? Probably the style. Probably the style? I think probably the content because uh, the, the edges at least look like the content is going to to help you- your network converge quicker. Yeah, that's true you don't have to start with white noise, in generally the baseline is start with white noise so that anything can happen, if you give it the content to start with is going to have a bias towards the content but if you train longer. Okay one more question and then we can move on. So this style and content [inaudible]. ImageNet doesn't understand what's content and style but ImageNet finds the edges on the image and so you can give the content image and extract the few first layers to get information about them because when it was trained on classification, it needed to find the edges. To find that a dog is a dog, you first need to find the edges of the dog so it's, it's trained to do so and for the style, it's complicated to understand the style but the network finds all the features on the image and then we use of post-processing technique that is called the Gram matrix in order to extract what we call style. It's basically ah, a cross-correlation of all the features of the network. We will learn it together later on. Okay, let's move on to the next application because we don't have too much time. So this is the one I prefer, ah, given a 10 second audio speech detect the word activate, so you know we talked about trigger word detection, there are many companies that have this wake word thing where you have a device at home and when you say you're certain word it activates itself. So here is the same thing for the word activate. What data do we need? Do we need a lot or not? Probably a lot because there are many accents and one thing that is counter-intuitive is that, if two humans, like let's say, let's say two- two women speak as a human you would say these voices are, are pretty similar, right? You can detect the word. What the network sees is a list of numbers that are totally different from one person to another because the frequencies we use in our voices are totally different from each other. So the numbers are very different although as a human we feel that it's very similar. So we need a lot of 10 seconds audio clips, that's it. What should be the distribution? It should contain as many accents as you can, as many, uh, female-male voices, uh, kid-adults uh, and so on. What should be the input of the network? It should be a 10 second audio clip that we can represent like that. The 10 second audio clip is going to contain some positive words, in green. Positive word is activate and it's also going to contain negative words in pink like kitchen, lion, whatever, words that are not activate and we want only to detect the positive word. What should be the sample rate? Again same question you would test on humans, ah, you would, you would, you would also talk to an expert in speech recognition  to know what's the best sample rate to use for speech processing, what should be the output? Any the ideas? [NOISE] Okay, yeah any other? Classification, yes no. Classification, yes no. So zero or one. Actually let's make a test, let- let's do a test. So we have three audio speech here, speech one, speech two, speech three. I don't know if we have the sound here. Do we have the sound? [NOISE] Maybe we have it now, okay let's try. [FOREIGN] So this is labeled one [LAUGHTER] Nobody speaks Italian in the, in the, in the room, second-one. [FOREIGN] Okay what's the wake word? Has anybody found what was the, the trigger word? We need more data. We need more. [LAUGHTER] So you know what's funny is, to be this is the right scheme to label, like it's definitely possible but it seems that even for humans this labeling scheme is super hard. We're not able to, to find what's, what's happening, like I don't know, even if I did this slide I don't even remember. No kidding. Now let's try something else, Okay? So now we have a different labeling scheme that tells us also where the wake word is happening. Let's hear it again. [FOREIGN] Okay, what's the trigger word? Pomeriggio. Pomeriggio means uh, afternoon in Italian. Okay. So you see wha- what I, I am trying to illustrate is uh, compare the human to the computer and you will get what's the right labeling scheme to use and of course the labeling scheme here is going to be better for the model rather than the first one and we just proved it. Uh, the, the important thing is to know that the first one would also work, we just need a ton of data. We need a lot more data to make the first labeling scheme work than we need for the second one, does that make sense? So yeah, we will use something like that. [inaudible] . [NOISE] Good question, actually this is not the best labeling scheme. As you said, should the one come before or after the word was said? What do you guys think? Before? After. After, yeah. You will see that uh, recurrent neural networks are going basically to look at uh, the data just as human do, like temporarily from the beginning to the end. In this case you need to hear the word in order to detect it, so you're going to put the one right after the word was said. Another issue that we have with this is that there are too many zeros, it's highly unbalanced so the network is pushed to always predict zeros. So what we do as a hack, and there's a lot of hacks like that happening in papers if you read them. We're going to add several ones after the word was say, I would add 20 ones, basically, okay? So this is our labeling scheme now. What should be the last activation of our network? [NOISE] Sigmoid function, yeah, sigmoid but sequential. For every time step you would use a sigmoid to output zero or one, basically. Don't worry if you don't understand spe- specifically what networks were using, you're going to learn it in a few weeks. So the architecture should, should be like a recurrent neural network, probably. Uh, convolutional neural networks might work as well, we'll see it later on in the course and the loss function should be the same as before but we should make a sequential. For every time step we should use a loss function like that and we should sum them over all the timestamp. Sounds good? So, another insights on these projects- I'll take it after- is what was critical to the success of this project. I think there are two things that are really critical when you when you build such a project. The first one is, to have a straight strategic data acquisition pipeline, so let's talk more about that. We said that our data should be 10 second audio clips that contain positive and negative words from many different access. How would you collect this data? [NOISE] That's right. [NOISE] You say you pay people to give you 10 seconds of their voice? [LAUGHTER] [inaudible] I think you, you can take your phone, go around campus and that's actually how we did it, we took our phones, we went around campus and we got some audio recordings. So one way to do it is that, to go and get 10 seconds audio recordings from different before with a large distribution of access and then what do you do? You label? You label by hands? That's one method, is it long or short? Is it quick or not? It's super slow, yeah. [inaudible] Oh, subtitles in movies. Uh, that's a good idea actually. You could like based on the licensing of the movie. [LAUGHTER] You could like, ah, take an audio from a movie and you get the subtitles and you are looking for activate. And every time the subtitle say, "Activate", you could label your data. That's super fun. That's super good actually. You could label automatically using that. Yet. So, that's a good idea. I think there's another way to do it that is closer to that which is we're going to collect three databases. The first one is going to be the positive word database, the second one is going to be the negative word database, the third one is going to be the background noise database. So, I take the background, 10 seconds. I insert randomly from one to three negative words and I insert randomly from one to three positive words making sure it doesn't overlap with a negative word. Okay? What's the main advantage of this method? Programmatic generation of samples. Yeah, programmatic generation of samples and automated labeling. I can label. I know where I inserted my positive words. [NOISE] So, I just add ones where I inserted it. I can generate millions of data examples like that just because I found the right strategy to, to create data. You see the difference between the two methods. The one where you have to go out and collect data and the one where you just go out, collect positive words, negative words, and then find background noise on YouTube or wherever you have the right license to use. It's, it's a big difference and this can make, [NOISE] can make your company succeed compared to another company. It's very common. All right. So, I would go on campus, take one second audio clips of positive words, put it in the database in green. Take one second audio clips of negative words of the same people as well, put it in the pink database and get background noise from anywhere I can find it's very cheap and then create the synthetic data, label it automatically. And you know, with like five positive words, five negative words, five backgrounds, you can create a lot of data points. Okay. So, this is an important technique that you might want to think about in your projects. The second thing that is important for the success of such a project is the architecture search and hyperparameter tuning. So, all of you, you will have complicated projects where you would be lost regarding the cur- architecture to use at first. It's a complicated process to find the architecture but you, you should not give up. And the first thing I would say is talk to the experts. So, let me tell you the story of this project. Um, first I, I started like looking at the literature and figuring out what network I could use for this project. And I ended up using that for, for the beginning part. I use a Fourier transform to extract features from the speech. Who's familiar with spectrograms or Fourier transforms? So, for the others, think about audio speech as a 1D signal. But every 1D signal can be decomposed into a sum of sines and cosines with a specific frequency and amplitude for each of these. And so, I can convert a 1D signal into a matrix for- with, with, with basically [NOISE] with basically one axis that is the frequency, one axis that is the time, going from, going from 0 to 10 seconds. And I will get the value of all the, the amplitude of this frequency. So, maybe this one is a strong frequency, this one is a strong frequency, this one is a low one and so on. For every time step. This is the spectrogram of an audio speech. You're going to learn a little bit more about that. So, after I got the spectrogram which is better than the 1D signal for the network, I would use an LSTM which is a recurrent neural network and add a sigmoid layer after it to get probabilities between zero and one. I will threshold them, everything more than 0.5 I will consider that it's a one everything less it's a zero. I tried for a long time fitting this network on the data, it didn't work. But one day I was working on campus and I, I, I, I found a friend that was an expert in speech recognition. He's worked a lot on all these problems and he exactly knew that this was not going to work. He could told me- he could have told me. So, he told me, "There's several issues with this network. The first one is your hyperparameters in the Fourier transform, they're wrong. Go on my GitHub, you will find what hyperparameters I used for this Fourier transform. You will find specifically what sample rate, what window size, what frequencies I used." So, that was better. Then he said, "One issue is that your recurrent neural network is too big. It's super hard to train. Instead, you should reduce it." So, I've used- so, he told me to use a convolution to reduce the number of time steps of my audio clip. You will learn about all these layers later. Ah, and also use batch Nor which is a specific type of layer that, that makes the training easier. And finally, you get your sigmoid layer and you output zeros and ones. But because the outputs time-steps is smaller than the input, you have to expand it. So, you need an expansion algorithm, just a script that expands every zero in two zeros. Let's say every one in two ones and so on. And now I get another architecture that I managed to train within a day. And this was all because I was lucky enough to find the experts and get advice from this person. So, I think you will run into the same problems as I run into during your projects. The important thing is spend more time figuring out who's the expert and who can tell you the answer rather than trying out random things. I think this is a- an important thing to think about. Okay. So, don't give up and also use our analysis which we are going to see later. Ah, we have two more minutes. So, I'm not gonna go over this one. I'm just going to talk about it quickly. There is another way to solve way chord detection. And the other way is to use the triplet loss algorithm. Instead of using anchor positive and negative faces, you can use audio speech of one second. Anchor is the word activate. Positive is other word activate said differently and negative is another word. You will train your network to encode activates in a certain vector and then compare the distance between vectors to figure out if activate is present or not. Okay. We have about two more minutes. So, I'm going to [NOISE] Oh, sorry. My bad [LAUGHTER] just on me [LAUGHTER]. Ah, just to finish, ah, with two more slides. Ah, now that you've seen some loss function, I want to show you another one and I want you to tell me what application does these beautiful loss correspond to. This one of the most beautiful loss I- I've seen in my life. [LAUGHTER] So, someone can tell me what's the application, what problem are we trying to solve if we use this loss function? Speech recognition. Speech recognition, no. It's not the case. Good trial. Yes. Regression. Regression. That's true. It's a regression problem but it's a specific regression problem. Bounding box. Good. Bounding box the object detection. This is object detection. So, I, I put the paper here you can check it out but how do you know that it's object detection? I've done it before. Oh, you've done it before. [LAUGHTER] Okay. So, this is the loss function of a network called YOLO. And the reason you can find out its bounding boxes is because if you look at the first term, you would see that it's comparing x to true x predicted x to pre- to true x, predicted y to true y. This is the center of a bounding box, xy. Second term is W and H. W and H stands for width and height of a bounding box. And it's trying to minimize the distance between the true bounding box and the predicted bounding box basically. The third term has an iden- indicator function with objects. It's saying, "If there is an object, you should have a high probability of objectness." The fourth term is saying that if there is no object, you should have a lower probability of objectness. And finally the final term is telling you you have to find the class that is in this box. Is it a cat? Is the dog? Is it an elephant? Is whatever. So, this is an object detection loss function. Actually do you know why, why you will have a square root here? [NOISE] [inaudible] that. The reason we have the square root is because you want to penalize more errors on small bounding boxes rather than big bounding boxes. So, if I give you an image of a human like that and a cat like this, you can have- So, this box the one inside is the ground truth, is very tight box. This one same and the box that are predicted are the predictions. So, these are the predictions and the other ones are the ground truth. What is interesting is that a two pixel error on this cats is much more important than the two pixel error on this human because the box is smaller. So, that's why you use a square root to penalize more the errors on small boxes than on big boxes. Okay. And finally the final slide, okay. Let's go over that. So, just recalling what we have for next week. Ah, you have two modules to complete for next Wednesday, ah, which are C1M3 with the following quiz and the following programming assignments, C1M4 with one quiz and two programming assignments. You're going to build your first deep neural network. This is all going to be on the web- it's already on the website and we'll publish the slides now. Ah, you have TA project mentorship that is mandatory this week. So, TA project mentorships are mandatory this week to start the week before the project proposal, the week before the project- no after the project proposal, after the project milestone and before the final project submission. Okay. And Friday TA sections, you're going to do some neural style transfer and R generation, ah, filling the AWS form. I don't know if it's been done yet. We're, we're going to try to give you some credits, ah, for your projects with GPUs. [NOISE] Okay. Thanks guys. 

All right. Hi everyone. Okay, I guess we're live. Ah, so as- as- Aarti was saying, please enter your SUNetID. Ah, we can bring this up again at the end of class today. We'll just take another like, what 20 seconds? And then we'll- we'll go on to the main discussion. [NOISE] All right. So, um, what I want to discuss with you today is, um, ah, what I'm going to call full cycle deep learning applications, right? Um, [NOISE] and so, um, I think this Sunday, uh, you'll be submitting your proposals for the, ah, class projects you do this quarter. And, um, in most of the, uh, in- in a lot of what you learn about the machine learning projects, you learn how to build machine learning models. Um, what I want to do today is share with you the bigger context of how a machine learning model, you know, how a neural network you might train, uh, fits in the context of a bigger project. Uh, so what are all the steps, right? Just as if you're writing a software product, you know, you take another classes, then you, uh, what happened? That, uh, they teach you how to build a website for example. What is that? Um, but to build a product requires more than just building a website, right? So, what are the- what are the other things you need to do to actually do a successful software project? And in this case, to do a successful machine learning application. Um, and so, uh, let's see. So- so, yeah. Test, test, is the audio on? Test. Could you turn up the audio? Yeah, how is this? No? Can't here me now. Hey. Oh, I think I'm broadcasting. I hear myself great. [LAUGHTER] Okay, you can hear me now. Great. Thank you. All right. Thank you. All right. So, what I want to do is share with you, um, full cycle machine learning. Not just how to, uh, you learn a lot about how to build deep learning models but how does that fit in a bigger project, right? Just as if you're taking the class on building a website, you know, then great, you know how the code of a website, that's really valuable. But what are all the things you need to do to make a successful website? Or to build a- build a project that involves launching a website or mobile app or whatever. Um, so as- as you plan for your, um, class project proposals, uh, due this Sunday, uh, if you're doing an application project that fits in the context of a bigger application, um, also keep all these steps in mind, right? So, um, you know, these are what I think of as the steps of an ML project, or really maybe- maybe not fast project but maybe a serious machine learning application, right? And I think, oh, no I built a lot of machine learning products over several years. So, some of these are also things that I wish I had known, you know, many years ago. Um, one, does this kind of- maybe kind of obvious but, you know, select a problem. And let's say for the sake of simplicity data, you use supervised learning, right. It- it turns out for the CS 230 class projects I think, uh, more than 50 percent of the class projects tend to use supervised learning. There are also other projects that use- end up using GANs which we'll talk about later this quarter or other things. But I think, you know, let's say you use supervised learning to- to build interesting application. Um, and- and I think for today, I'm going to use as a running example of building a, um, building a- a voice-activated device, right? So, you know, uh, no, actually, how- how many of you have like a smart speaker in your home? Like a voice-activated device in your home? You know, the- in- in the US? Well, not that many of you, interesting. Okay cool. Yeah, so, I think, uh, you know the- the Amazon Echos, Google Homes, the Apple Siris or the- the- in- in China, my- one of former team's built Baidu DeurOS. Ah, ah but let's say for the sake of argument that you want to build a voice-activated device. And I'm going to use as a running example. Um, and so in order to build a voice-activated device, and- and again I'm not going to use any of the commercial brands like Alexa or OK Google or Hey Siri or I guess in China it was a Hello [FOREIGN] which means kind of roughly hello little du. Um, but let's use a more neutral word which as I see you wanted to build a device that your responsive word activate. Um, and you're actually going to implement this as a problem set later this quarter. Um, but so you want to build a yeah. Is it possible to [inaudible]. Okay. No. Volume up. Uh, uh, let's see how that- okay is this better? No? Yes? No? This is better? Okay, cool. Thank you. Look at how ironic, talk about speech recognition and the volume isn't high enough. Okay. um, so let's say you want- well, [LAUGHTER] let me know about comes off again and thank you. Okay. Um, so let's say you want to build a voice-activated device. So, the key components, the key machine learning, deep learning component is going to be a learning algorithm that takes as input an audio clip and, uh, outputs, um, did it detect what's sometimes called the trigger word. Did I go soft again? Okay, this is okay, great. All right and- and O plus Y, you know zero one, they did detect their trigger word such as Alexa, or OK Google or Hey Siri, or Hello [inaudible] or, um, or activate or whatever wake word or trigger word, right? Um, and so step one is, uh, select a problem. Um, and then, in order to train a learning algorithm, you need to get labeled data if you're applying supervised learning. And then you design a model, use back prop or some of the other algorithms you learned about momentum, Adam, so you know, this optimization algorithms gradient descend, to train the model. Um, and then maybe you test it on your test set. And then you deploy it meaning you start selling these smart speakers and, you know, putting them into hopefully into your users homes. Um, and then you have to maintain the system. I'll talk about this later as well. Uh, and- and this is not chronological but, ah, one thing that's often done, but I- I want to talk about it at the end instead is not really step eight. It's a QA which is, uh, quality assurance which is an ongoing process, right? And so, um, one, uh, let's see. So, as you- so if you want to build a product, if you want to sell a machine learning product, these are maybe some of the key steps you need to work on. Um, some observations, when you train a model, training a model is often a very iterative process. So, every time we train the machine learning model, you'll find that, you know, I can almost guarantee whatever you do, it will not work. At least not the first time, right? And so you'll find that even though I've written these, uh, sequence of steps, when you train a model, you undergo, no- that neural network architecture didn't work. and need to increase the number of hidden units or change the regularization or switch to RNN or switch to a totally different architecture. And sometimes you train a model and go nope, that didn't work. Um, I need to get more data, right? And so this is often a very iterative process we are cycling through, um, uh, the several different steps here. Um, and then I think, ah, one distinction that you have not yet learned about in the Coursera- in the deeplearning.ai Coursera videos is how to split up the data into train, dev, and tests. So, I am going to simplify those details for now. But just as a- a foreshadowing I guess of what, um, you learn later in the- in the, uh, deeplearning.ai Coursera videos is how to take a data set, you have trained into- excuse me, into a training set, um, ah, into a set that you actually test cross-validate using during development called the dev set or development set or [inaudible] cross validation set. That was a separate test set. So, you'll learn about this later. But I'm just simplifying a little bit, um, for today. Okay. So, um, so I think, um, the first thing I want to do is ask you a question, right? So, we're going to talk through many of these steps. And when it turns out that, um, what a lot of machine learning classes do and- and do a good job teaching is focusing on maybe these three steps, or maybe these four steps, right? And what I want to do today is spend more time, so this is the heart of machine learning, how do you build a great model. Uh, and what I want to do today is spend more time talking about step one, uh, and six and seven, and then just a little bit of time talking about the core of this because you kind of need to do the other steps as well when you want build a deep learning product or build a machine learning application. Okay. Um, so let's talk about discussion question. Um, I'm actually curious. Uh, if you're selecting a, um, project to work on, uh, uh, what are the- actually so I- don't- don't answer this yet. I'll- I'll tell you what the question I'm going to ask is, um, which is- [NOISE] All right. Uh, what properties make for a good candidate deep learning project? But don't answer yet, right? So I, I wanna say a few more things before, before I invite you to answer, which is that, um, all of you, for the last few days, I hope, have been thinking about what project you wanna do for this class. And what I wanna do is just discuss some properties of what are good projects to work on and what are maybe not good projects to work on, okay? And, and think of this as your chance to give your classmates advice, right? What are the things your classmates should think about if they can't decide this is a good project to work on, okay? Um, and so, what I wanna do for today is, ah, use, um, this voice-activated thing as a mo- as a, as a motivating example. And, you know, th- there's actually one project I, uh, uh, was working on, uh, the, the, actually, I thought, there were, actually, there's one project I thought of working on but decided not to work on, ah, and that, that, that's a voice-activated device. So, it turns out that [NOISE] , um, these voice-activated devices like Echo, Google Homes, and so on, they are taking off quite rapidly in the US and around the world. Um, it turns out that one of the, you know, significant pain points of these devices is the need to, um, configure it, right? To set it up for Wi-Fi. So, um, I've done a lot of work on speech recognition, you know, ah, a hotel, did a lot of work on group speech system, I led the Baidu speech system. So I've been published papers on speech recognition, and I have a, I have one of these devices in my home, right? Um, well, a- actually, Amazon, I have an Amazon Echo in my living room. Um, but even to this day, I have configured exactly one light bulb to be hooked up, to be controlled by my Echo, [LAUGHTER] because the, the seller process, not blaming any country, it's just difficult to hook up, you know, a Wi-Fi enabled light bulb and then to set it up so that your s- smart speaker or whatever, as in, say, you know, "Smart device, turn off the lamp." So, I, I have one light bulb in my living room where, that I can [LAUGHTER] turn on and off and that's it, right? [LAUGHTER] Ah, even as a speech researcher. So, [LAUGHTER] um, maybe that's even a bad example. Um, so one, one, one application that I think, ah, that the, I, I was actually searching because you're working on is to build a, um, embedded device that you can sell to lamp makers, so that, I don't know where you buy your lamps from, but, you know, I have a few lamps from IKEA, or a few lamps from wherever. But you can buy a desk lamp, [NOISE] so that when you buy the desk lamp, there's already a built-in microphone, so that without needing to connect this thing to Wi-Fi, you know, and say, "Hey here's a $20 desk lamp, um, put them on your desk," and you can go home and say, "Desk lamp, turn on, or desk lamp, turn off." Uh, then, I think that will help a lot more users get voice-activated devices into their home. And it's actually not clear to me, if you want to turn on a desk lamp, it's actually not clear to me that you want to turn to a smart speaker and say, "Hey, Smart Speaker, please turn on that lamp over there," right? I- it, it, maybe it feels more natural to just talk directly to a desk lamp and tell it to turn on and turn off. Um, and so, ah, a- also for what it's worth, someone we're friends now with evaluated this, we actually thought that this could be a r- reasonable business, to build embedded devices, to sell to lamp makers or other device makers so that they can sell their own voice-activated devices without needing this complicated Wi-Fi setup process. Um, and so to do this, you would need to build a learning algorithm, and have it run on an embedded device, then it just inputs an audio clip and outputs, you know, whenever it detects the, the, the wake word. And instead of a wake word being "Activate," the wake word would be a "Lamp, turn on" or "Lamp, turn off." You need two wake words or trigger words, one to turn it on, one to turn it off, right? Oh, and, and, and, and I think just the other thing that, um, I think would make this work, ah, is, um, ah, ah, to, ah, to give these devices names. So, if you have five lamps, or two lamps, you, you need an wait index into these different desk lamps. So, um, let's say you decide for your project, you know, to have a little switch here, so this lamp could be called John, [NOISE] or Mary, or Bob, or Alice, like a four-way switch. So that's depending on where you set this four-way switch, you can say, you know, "John," right? "Turn on," right? O- or d- if, if you decide to call this lamp John, I guess you could give some other names so you don't have any lamp by the same name, okay? Um, so, what I'm gonna do is use as a, ah, motivating example on this as a possible project. Oh, and, and I'm not working on this. If any of you want to build a startup doing this, go for it, I, I, this is not net, [NOISE] well, I, I felt my teams and I, we had better ideas, so we wanted to do other things in this but I actually don't see anything wrong with this. I think this actually could be a reasonable thing to pursue as well, but I'm not doing it. So you're all very welcome to, if you want, okay? [NOISE] Um, so now, the question I want to pose to you [NOISE] is, ah, when you're brainstorming project ideas, you know, like, this idea, or some other idea, um, what are the things you would want to watch out for? Wha- what are the properties that you would want to be true in order for you to feel good proposing this as a, as a CS 230 Project, right? So why don't you take a minute and, and write this down. I think, ah, uh, uh, yeah. Well, wha- wha- what if, if you're asking a friend if, if a friend is asking you, "What are the things I should look at to see if something is [NOISE] a good project?" What would you, what would you recommend to them? So, fe- few, just write down a few key words, and then we'll see what people say. And then, and then, I'll tell you what I tend to look out for when I'm selecting projects. And I have a list of, ah, five points. Might take like, I don't know, like, two minutes to- That's not activated. Yeah. Oh. Sorry. This is not activated? Um, uh, you're not able to answer this up, enter answers? Okay. [NOISE] Let me test the Internet access. Just, [NOISE] just checking it. Yeah. I'm connected to the Internet. Uh, [NOISE] Aarti, any ideas? [NOISE] It was just activated. Oh, I see. Okay. All right. Let me try that. [NOISE] I'll just. Turn it on. [OVERLAPPING] Oh, it's working now? Okay. Thank you. [OVERLAPPING] Maybe I'll turn off to do it, but it keeps getting turning off. Okay. Yes. Thank you. [NOISE] Yeah. Thanks. [NOISE] So, if you take, like, two minutes to enter, and I think, I think I can figure this and let you enter multiple answers. Let me just take two minutes. [NOISE] All right, another one minute. [NOISE] All right, 30 seconds. Okay, three, two, one. Well, maybe in the hindsight that wasn't the best visualization. Can people see this? [NOISE] Um nevermind, trying to see if- all right so, data novelty, loss of data, some of these are very small, human doable, number of examples, do one, two ones [inaudible] algorithm, new industry of fields, uh, clear objective, practical useful, huh, oh, finishing time, uh, [LAUGHTER]. Host real life problem, useful hasn't been done [inaudible] tractable. Yeah. Generalization [inaudible] Um, let me make some comments on these. I think I, this is, this is pretty good, um. I have a list of five bullet points and maybe I just share of you my list of five, uh, um, [NOISE] which is I'm, just some things to encourage you to pay attention to, um, you know. This, this may or may not be the best criteria, but interests I think, well, interests plus, uh, I just hopefully you work on something that you're actually interested in, um. And then I think, uh, uh, right, data availability which many of you cited is a good criteria or, uh, wanted the ways that, um, Stanford class projects sometimes do not go well, is its students spend a month trying to collect data and after month have not yet found the data and then- and then, you know, and then there's uh, and then there's a lot of wasted time. Um, one thing that I would encourage you to consider as well is, uh, domain knowledge, um. And I think that if you are a biologist and have unique knowledge into some aspect of biology through which you want to apply Machine learning, that will actually let you do a very interesting project, right. That- that is is actually difficult for others to do. Um, and I think, uh, more generally as, as a advice for navigating your careers, right. So, you know, i- its interesting because you know, Machine learning, Deep learning, there's so much, there's so many people wanting to jump into Machine learning and Deep learning, um, actually I'll give you an example. So, I sometimes talk to, uh, uh, doctors, Radiology students, uh, uh, including, you know, like Stanford and other universities Radiology students that want to learn about Machine learning, right? Because they hear about, you know, Deep learning, maybe someday affecting radiologist jobs, and so they want to be part of Deep learning. And so my career advice to them is usually to not forget everything they learned as a doctor and try to, you know, do Machine learning 101 from scratch and just forget everything they learned as a doctor and just become a CS major. I think that- that path can work, but I think where radiologists could do the most unique work, uh, that allows them to make a most unique contribution, is that they use their domain knowledge of healthcare radiology and do something in Machine learning applied to radiology, right, uh, and so, was [LAUGHTER] all right. [LAUGHTER] How, how many- how many millennials are there in this class? [LAUGHTER] What is that me, me, me, me, me thing? [LAUGHTER] All right. All right. This is really wrong. Yeah. [LAUGHTER] I- I think it's because this is a word cloud. So they count word frequency, right? The money thing. I don't know, I have very mixed feelings about that [LAUGHTER]. All right. Um, but I think as, [inaudible] I actually know that some of you are taking, you know, Deep learning because you work in a different discipline and you want to do something in this hot, new, exciting thing of Machine learning and I think, uh, whatever this major you're in, if your domain knowledge about some other area, you know, Education, Civil Engineering, Biology, Law. Um, taking Deep learning allows you to do very unique work applying Machine learning to your domain, right, um, [NOISE] uh. Let's see. Um, I think that, uh, uh, um, [NOISE] um, I think, well, I call the utility but several of you mentioned as well, something that has a positive impact that actually helps other people, uh, uh, uh, and, and I- I- I don't know money could be an aspect of utility, but maybe not, the most inspiring one uh, and then I think, um, [NOISE] and I think one of the biggest challenges we face in the industry today is still, frankly, is actually good judgment on feasibility, um. So today, I still see too many, uh, leaders, sometimes CEOs of large companies that stand on stage and announce to the whole world, you know, we're gonna do this Machine learning project, to do this by this deadline and then 20 minutes later, I talk to their engineers and the engineers say nope, no way, not happening what the [LAUGHTER] CEO just finally says [LAUGHTER] whole engineering [inaudible] is not doing that and knows that it's impossible. So, I think one of the biggest challenges is actually feasibility um, in fact, I actually know that's a- a- a you know, I was chatting with Aarti about, uh, the, the, um, uh TA office hours and I know that, uh, uh there been lot of, you know, a lot of you have been, uh, thinking about applying, uh, end-to-end Deep learning, right? You know. Can you input any x and output any y and do that accurately and sometimes it's possible and sometimes it's not, and it still takes, uh, relatively deep, deep judgment about what neural networks can and cannot do with a certain amount of data that you may or may not be able to acquire in order to do some of these things, right? Um, so- so, I think throughout this quarter, you gain much deeper judgment as well on on what is feasible, I guess. [NOISE] This is pretty interesting. I once know, uh, uh, uh I- I- I knew a CEO of a ve- very, of a large company that once told his team, um, uh, he actually gave this team these instructions. He said, uh, I wish they assume that AI can do anything, uh, and- and- and uh, uh, I think tha- tha- that had an interesting effect, I guess. Uh, uh, uh, yeah. Cool. All right. So, I think step one, um, was select a project, I hope this is [inaudible] projects, try to keep some of those things in mind, um. Step two is get data, right, [NOISE] um. And so, uh, what I want you to do, uh, and I'm going to pose the second question and then have some you discuss this. Let's say that you're actually working on this, you know, smarts voice-activated embedded devices thing, right? So let's say that you and your friends wanna build a start up so train a deep learning algorithm to detect, you know, phrases like John turned on or Mary turn off or Bob turn off or whatever, uh, to sell to device makers so that they can have low voice [NOISE] embedded voice detection chip. It doesn't require a complicated Wi-Fi setup process, right? So let's see one of- let's see how she wanna do this. So, you need to collect some data in order to start training a learning algorithm, all right? So, uh, the second question I would pose to you is, uh, uh, uh, two- a question in two parts uh, but, but, uh, have you answer it all, all at the same time which is, uh, in how many, how many days, let's say you actually proposed this for your CS 230 project this Sunday and then you start work on it, you know, like on Monday. Are you guys start work on it today before the proposal? But, uh, how many days would you spend collecting data? Uh, and how would you collect data? Okay? And I think, um, I'm actually how, how many of you have participated in Engineering Scrum? If you know what that means? Okay a few of you uh, those who have an industry. Okay, all right so engineering estimation, when you estimate how long a project takes one of the common practices is use a Fibonacci sequence to estimate how long a project will take, right? And so Fibonacci sequence one, one, two, three, five, eight,13 and so on. And this roughly powers of two- but doesn't grow as fast as powers to Fibonacci numbers are cool, right. For so- uh, uh, so, so what I want you to do is just finish after configuration right. When our, um, speech bubbles, okay. Yeah, that's good. All right. So what I'd like you to do is in the text answer, uh, I really write two things, one is write a number, how many days do you think you spent on collecting data? You and your teammates if you're actually doing this project. Uh, and then how, how would you go about collecting the data? Okay? So once you take good, another two minutes, uh, to write in an answer. [NOISE] Oh I'm sorry. This heavy load, now still not activated. So then try to hit, uh, interesting. That is not helpful. All right, that is definitely not helpful. Um. All right, let's do this, write down your answer on a piece of paper first and take two minutes [inaudible 29:10]. So the two questions are, are how many days? Uh, pick a number from a Fibonacci sequence and, uh, are you going to use it nothing? Oh, okay, uh, let's swap out my computer for Aarti's. Oh actually, yeah oh, if Aarti's computers working, actually go ahead. Sorry. Okay I can just present. Yeah, yeah let's, let's plug in your laptop? Shall we? So you just use your laptop. [NOISE] See, sure. Yeah. Yeah. Doesn't say I wonder if there's a network problem or web browser problem. Uh, I started using uh, Firefox recently in addition to Chrome and Safari and that was Firefox, I've tried with other web browsers later. Right, cool. Okay, okay, thank you. Thanks all to you. [NOISE]. All right. I can maybe, yeah maybe people then take another minute from now, just extend the time a bit to end. All right. Now the 10 seconds. All right, cool. Let's see, uh, show people's answers, okay? Right. Well, three hundred sixty-five So, there's a, there's a, there's a lot of variance in the answers, right? Uh, [LAUGHTER] Download from online depends on what data you want. It turns out well. So if you're trying to find data or phrases like John turn on that, that, that data doesn't exist online. Uh, it turns out that we're trying to find audio clips at the web activate. There are some websites with, uh, single words pronounce but those- but uh, not a lot of audio clips session so the trigger world where the wake word is the word activate. Uh, there are some websites we can download like, maybe 10 audio clips of a few people saying activate but it's quite hard to find hundreds of examples of different people saying the word activate. Um. Five days it falls from the sky. [LAUGHTER] All right, so, let me suggest, um, uh- let me suggest that you guys discuss with each other in small groups, uh, what you think would be the best strategy? How many days we find collecting the data and how we decide collecting data? Try convince people next to you on that. Uh, and, and before I ask you to start discussing, I wanna leave you with one thought which is, um, how long do you think will take you to train your first model? Right. And so if it take you a day to train your first model or two days? Uh, do you want to spend x time collecting data and then spend let's say, you know, I'll know to [inaudible] the deep learning thing, train a model, it might take a couple of days, right? Especially if you've download open source packages, so, so the amount of time needed to collect data as x followed by two days to train your first model, what do you think x should be the amount of time? Once you go spend like two minutes to discuss with each other and see if you can, can the, the, the answers are- there is very large variance right once you guys discuss, if you actually, if, if the people sitting next to you are your project partners, why should you discuss with them how, how many days you think you should spend collecting data and how you collect the data? Okay? Why don't you  take two minutes to discuss a little. [NOISE] All right. [NOISE] All right guys. So, [NOISE] wow, all right guys. [LAUGHTER] Hey guys. So, [NOISE] all right. A lot of exciting discussion. So, actually how, how many of you, how many of the groups wound up on the, on the low end? How many of you, you know, convinced each other that maybe it should be, like, three days or less? Oh, just a few of you. How come? Some, some- someone, someone say why? Why is it, why, why? Because you wanted us to see if the algorithm works first. So, we need some direction to just test to see if the algorithm is even reaching some sort of good benchmark before we then go and collect the next data set. Cool, yeah, right. I guess a little bit of data to test how the algorithm works before you even go and collect the next data set, all right? Cool. And did anyone had a, had a high-end, like a 13 days or more?Yes. Very few. How come? Anyone, actually anyone, anyone with insights you want to share with the whole class if you, what, what were you all discussing so excitedly? [LAUGHTER]. Yeah, go ahead. So, um, depending on domain knowledge, uh, maybe a connection can take a long time especially for this problem, like, based on that one idea which we discussed in previous class, we were thinking like, we could use like, movie clips and like, some typos to like, generate sound like, [inaudible] And that will take like time to like mine data, um, [inaudible] [NOISE], Yeah, yeah, yeah, right. Yeah. So, there are accompanying systems to look at subtitle, uh, uh, videos, right? Uh, uh, like, uh, YouTube videos with captions or something and, and if there's, uh, appropriately, creative commons data there you can use. Yeah. So, let me, let me tell you my bias. I, I- I'll just tell you what I would do if I was working on this project. Well, as, well, one caveat, I haven't done so much work in speech recognition previously, right? This is my first project. Um, I would probably spend 1-2 days collecting data [NOISE], kind of, on the short end, right? And I think that, you know, one of the, and, and, and one of the reasons is that Machine Learning, kind of that circle I drew up there is actually a very iterative process where, um, until you try it, you, you almost never know what's actually going to be hard about the problem, right? And so, um, so if I was doing this project, I'll just tell you honestly what I would do. Like, again, I've actually thought about this project a bunch, right? Including, you know, trying to validate market acceptance and so on. But, um, but which is that, um, I would get a cheap microphone, a user or, or user built-in laptop microphone or buy a microphone off, you know, buy a microphone off Amazon or something and go around say, go around Stanford campus or go to your friends and have them just say, "Hey, do you mind saying into this microphone the word activate or John, turn on or whatever," and collect a bunch of data that way. Um, and then, uh, uh, and with one or two days, um, you should be able to collect at least hundreds of examples, uh, and that might be enough of the data set to start training a rudimentary learning algorithm to get going. Because if you have not yet worked on this problem before, it turns out to be very difficult to know what's going to be hard about the problem. So, is what's gonna be hard, um, highly accented speakers, right? Uh, or is what's gonna be hard background noise, um, or is what's gonna be hard, you know, confusing turn on with turn off? You hear John turn and then [NOISE] But when you build a new Machine Learning system, it's very difficult to know what's hard and what's easy about the problem, uh, or or is what's gonna be difficult that far-field, which is, um, the technical term for if the microphone is very far away, right? So, it turns out that, you know, if, if we turn on the, um, microphone on my laptop now for example, um, the, the laptop, which is what, like three meters away from me, uh, will be hearing voice directly from my mouth as well as voice bouncing off the walls. [NOISE] So, there's a lot of reverberation in this room and so that makes speech recognition harder. You, me are so good at processing out reverberant sounds, reverberations that you almost don't notice it, but it may, i- it actually the, but the learning algorithm will have, sometimes has problems with reverberations, right? Or echos bouncing off the hard walls of this room. Um, and so depending on what your learning algorithm has trouble with, um, you will then want to go back to collect very different types of data or explore very different types of algorithms. Well, the problem is that sometimes is because just the volume is just too soft, in which case, you know, maybe you need to do something else and normalize all your volumes, or buy a more sensitive microphone or something. So, it turns out that when building most Machine Learning applications, unless you've experienced working on it. So, I- I've actually worked on this problem before, so I have a sense of what's hard and what's easy. But when you work on a new project for the first time, it's very difficult to know what's hard and what's easy. And so my advice to most teams is, um, rather than [NOISE] spending say 20 days to collect data and then two days to collect model, to train a model, and it's often by training a model and then seeing what are the examples it gets wrong, when does the algorithm fail? That, that, that's your feedback to either collect more data or redesign the model, right? Or try something else. Um, and if you can shrink the data collection period down to be more comparable to how long you end up taking to train your model, then you can start iterating much more rapidly on, on actually improving your model, right? Uh, and uh, so maybe one rule of thumb I actually tend to recommend for most class projects is I know, if i- maybe if you need to spend a week, up to a week to collect data, you know, maybe that's okay, but if you can get it going even more quickly. Uh, uh, I would even maybe more strongly recommend that, um, and there have been so few examples in my life where the first time I trained a learning algorithm it worked, right? I- it like, pretty much never happens. Yeah, I, I, I, i- it happened once about a year ago and I was so surprised, I still remember that one time. [LAUGHTER] Uh, and so what, so, so Machine Learning development is often a very iterative process and by quickly collecting data set. And, and often data sets are collected through sweat and hard work, right? And so, I, I would literally, you know, and actually, well, I've actually done a long long speech. And to get it going quickly, I would probably just, um, uh, have myself or my team members run around and find people and ask them to speak into a microphone and record audio clips that way. Um, and then only when you validate that you need a bigger data set would you go to a more complicated things, like set up an Amazon Mechanical Turk thing, right? To crowdsource, which I've also done actually, right? I also had very large data sets collected off Amazon Mechanical Turk, but only in a later stage of the project where you understand what you really need, right? Um, so as you, as you start working on your class projects, maybe, maybe keep that, keep that in mind. So, now, um, [NOISE] so, one other tip that, uh, Machine Learning researchers, on average, we tend to be terrible at this, um, um, but I'll give this advice anyway, is when you're going through this process, getting data, design a model, uh, a literature search would be very helpful, you know, so see what other, see what algorithms others are using for this problem. It turns out the literature is actually quite immature. There isn't a convergence of, uh, like a world standard set, sta- standard algorithms for trigger word detection in literature right now, which people are still making up algorithms. So, if you, if you do a little survey, you find that to be the case, but you're training initial model. Um, and in most Machine Learning applications, you go through this process multiple times. So, one tip that I would recommend you do is, uh, keep clear notes, um, on the experiments you've run. Right. Because they'll often be as you train the model, you see oh this model works great on American accent of speakers, but not on British accent of speakers. Right. I- I- I was born in the UK, sometimes I use British accents for example. If you are from a different part of the world, you think of different global accents but since I'm from the UK, I'm- I'm just gonna pick on British accents I guess. Keep clear notes on the experiments run because what happens in every machine learning project is after awhile you have trained 30 models and then you and your team members are going, "Oh yeah, we tried that idea two weeks ago did it work and if you have clear notes from when you actually did that work two weeks ago, then you can refer back rather than have to rerun the experiments." Um, the other thing that some groups do is, um, have a spreadsheet that keeps track of what's the learning rate you use? What's the number of hidden units? What's this? What's this? What's this? Or, or, or, or cheaper than a, than a text document so that which will make it easier to refer back to, to know someone's you tried earlier. Um, this is one piece of commonly-given advice. This is one of those things that every machine learning person knows we should do this but on average, we're very bad at doing this but, but, but, but, but you could I don't know. But at the times, I've managed to keep good notes so that you save them all the time right to try to remember what exactly you tried two weeks ago. Okay. So, a lot of this class will be on this process of how to get data, develop the train data test the design the model, train the model. Eventually, test them all then iterate. Okay. So, a lot this classes on this. So, I wanna jump ahead. So, when you have a good enough model and you want to deploy it. Okay. So, step six, I guess is deployment. Now, um, um, this is er, uh, um, one of the reasons I want to step through this example going through a concrete example is I find that when you're learning about machine learning for the first time is often seeing, you know, what- what my team says called war stories kinda of stories or projects that, that, that others have built before that often provides the best learning experience. I think like I have built speech recognition systems. It took me like a year or two years for me to do it. So, I'm trying to so rather than, you know, having you spend two years of your life building speech systems and can summarize a war story. Right. To tell you what the process is like, I'm hoping that these concrete examples of what building these systems are like in, you know, large corporations that can help you accelerate your learnings without needing to get two years of on-the-job experience. You can just secure the salient points. Okay. Now, if you're deploying a system like this, one of the things, um, and this is actually true. This is actually a real phenomenon for deploying speech systems is you have the audio clip, you have a neural network, and then you know this will output zero, one and the neural networks that work well, will tend to be relatively large, right relatively large model. Larger than hidden units relatively high complexity. And, um, if you have some of the smart speakers in your home, um, you recognize that a lot of them are Edge devices as opposed to purely cloud computation, right? So we all know what the cloud is. Um, uh, and what an Edge device is. And Edge device is a smart speaker that's in your home or the cell phone in your wallet. So, Edge devices are you know the things that are close to the data as opposed to the cloud which is a giant service we have in our data centers, right? So, um, because of network latency, er, er, and, and, and because of privacy a lot of these computations are done on Edge devices like a smart speaker in your home or, er, er, like er, I guess Hey Siri or Okay Google can wake up your cell phone, right? And so, Edge devices have much lower computational budgets and much lower power budgets, limited battery life, much less powerful processes than we have in our cloud data centers. And so, it turns out that se- se serving up a very large neural network is quite difficult. Right? It's very difficult for, you know, a low-power, inexpensive microprocessor sitting on a smart speaking in your living room to run a very large neural network with a lot of hidden units and a lot parameters. And so, what is often done is to actually do this. Which is to input an audio clip and then have a much simpler algorithm. Uh, figure out if, you know, anyone is even talking, right? Because so the smart speaker, you know, in my living room hears silence most of the day, right? Because usually, just no one at home, right? There's no no voice. And then only if it hears, you know, someone talking then feed it to the big neural network that you've trained and ramp-up, use a larger power budget in order to classify 01. Okay. Um, this component goes by many different names, um, in- in reasonably standard terminology but not totally standard terminology. The literature I'm gonna call this VAD. For- Voice Activity Detection. [NOISE] Right. Um, it turns out that voice activity detection, there's a standard component is in many different speech recognition systems if you are using a cellphone for example, VAD is a component that tries to figure out if anyone's even talking because if they think no one is talking then there's no need to encode the audio and try to transmit the audio, right? Aarti, could you? Okay. Yeah. [LAUGHTER] Yeah. Um, and so, uh, so the next question I want to ask you and, and I- I- I thought this is timely because, um, uh, well there's a couple options, right? Option one is to build a non-machine learning-based VAD system, Voice Activity Detection system which is just, you know, see if the volume of the audio your smart speaker is recording is greater than epsilon. So, the silence just forget it. And option two is train a small neural network, um, to recognize on, on, on human speech, right? And so, uh, my next question to you is, um, if you're working on this project, would you pick option one or would you pick option two? Right? As you, as you, as you move toward- oh sorry and I think, um, a small neural network. So, a small neural network or in some cases, I've seen people use a small support vector machine as well for those who don't know what that is. A small model can be run with a low computational budget. There's a much simpler problem to detect if someone is talking than to recognize a word they said so you can actually do this, you know, but this with reasonable accuracy with a small neural network but if you actually work on this project for CS 230 which would you try first? So, could we come to the next question? Yes. Okay. I cant figure out how to do that. Oh oh sure. Okay. Yeah, yeah, yeah, you can let them start answering I guess and then while you figure out the projection. Cool. I will just keep unlocking it periodically. Are people able to vote? No, there are no votes yet. Well, I guess you write so much code, you have a shortcut to go so you're coding environment on your laptop. Great. [LAUGHTER]. Yeah. That way? All right cool. Great thank you. [NOISE] Oh wow! Great. Right. You're answering quickly. Another like 20 seconds if that's enough time to give the answers. [NOISE] All right cool. Um, that's fascinating. There's a lot of disagreement in this class. Um, people must say why, why would you choose option one, why would you option two? And then I- I- I- I- I have a very strong point of view on what I would do, right? Um, but, but I'm curious why, uh, why option one and why option two. Go ahead. Option one is easy to debug in the environment. Easy to debug. Anything else? Either option one or option two? Option one is simple. Option one is simple. Anything else? Option two is not an environment. Option two is not the environment. [LAUGHTER]. I was thinking the option one, right? If you've a dog in your house and he barks and it can activate. So, when you have option two, you can probably kind of already like simplify the problem but it does not, is another way to I mean, activates the machine but doesn't by itself. [NOISE] That's why if it's talking to. Yeah, if the dog barking. Option two, it would be much better. Yeah, yeah, someth- something kinda goes noise. Cool. All right. And just two of you and in each-. Option two, they're making noise from people like whispering? Like, what if someone is whispering? Huh? What if someone is whispering? What if someone's whispering? [inaudible]. Yeah, yeah, go. Yeah, go ahead, have you. [inaudible] I think it will pick up the background noise. Yeah, cool, yeah. And in a noisy place like, you know, I have a friend who's, uh, lives actually listening to the train station. So, right, so. option one will cover a lot of things [NOISE], which is possible. Yeah. Whichever option you take has to be running constantly. So, you want something that is very cheap, so it seems like option one is better than [inaudible]. Yeah, whether we pick has to be constan- constantly somehow to be cheap, low power, low conflict. So, le- let me show you some of the pros and cons. Um, uh, so, um, uh, I think, yeah, there are pros and cons in option one and option two was why there's so, so, so many votes were both options. I perceived we'll choose option one. Um, but, but let me just, let's just discuss the pros and cons, right? I think that's, um, uh, option one, um, forces just a few lines of codes. Is, is yes, maybe options two isn't that complicated, but option one is even simpler. And I think that, um, uh, actually maybe I would say if I hadn't worked on this problem before I will choose option one. Uh, but since I have experience in speech recognition eventually I know you need option two. But that's because I, I because I've worked on this problem before. But if it's your first time working on the speech activation problem, I would encourage you on average to try to really simple quick and direct solutions and go ahead and, um, so let's see how long would it take to implement this. Right? I would say like 10 minutes, five minutes, I don't know, right? How long would it take to implement that? i don't know. Like what? Four hours? One day? I, I, I don't really know actually. Right. Um, le- let me just write one day, I'm, I'm not quite sure. Right, but if, um, option one can be implemented in 10 minutes then I would encourage you to do that. And go ahead and put the smart speaker in your home or in your potential users homes. And only when you find out that the dog barking is a problem or the train on the railway station or whatever is a problem, then go back and invest more in fixing it, right? And, in fact, um, It's true that maybe it's annoying if the dog barking keeps on waking up the system. But maybe that's okay because if the large neural network then screens out all the dog barking then the overall performance systems actually just fine. And, uh, and, and, and you now have a much simpler system. Right? But, uh, but, but it, it turns out that, um, the reason, uh, you might need to go to option two eventually is because there are some homes in noisy environments. Uh, uh, you know, there's constant background noise, and so that will keep the large neural network running a little bit too frequently, so, so if you have a large engineering budget, you know, so, so, some of the smart speaker teams are over hundreds of engineers that are working on it. So, if you have hundreds of engineers work on it, totally option two will perform better. But if your strap a startup team, a scrappy startup team with few of you working on a class project, you know, the evidence that you need that level of complexity is not that high, and I would really do that first and, and you start to gather evidence that you really should make the investment to build more complex system before actually making the investments of days or, e- er, and eventually I think this is one day to build your first prototype, right, and then eventually you'll be, you'll be more complicated. Um, it turns out that, um, the other reason [NOISE] , um, the other huge advantage of the simple method is the following. Um, and this is one of the frankly, this is one of the, this is actually one of the big problems and big weaknesses of Machine Learning algorithms in deep learning algorithms, which is what happens is, uh, um, when you build a system and you ship, ship a product, the data will change, right, and so, um, I'm gonna simplify the example a little bit. But, you know, I, I know Stanford is very cosmopolitan, this Palo Alto is very cosmopolitan. See, you collect data in this region, you get accents from people all over the world, right, because, because that's Stanford or that's Palo Alto. But, but just to simplify the example a little bit, [NOISE] um, let's say that you train [NOISE] on, uh, US accents, right, uh, but you know, for some reason, uh, uh, when you ship a product, maybe it sells really well in the UK and you start getting data [NOISE] with a UK or with British accents. Right? So, one of the biggest problems you face in practical deployment of Machine Learning systems is that, the data you train on is not gonna be the data you need to perform well on. Um, and, and I'm gonna share with you some practical ideas for how to solve this, but this is one of those practical realities and practical weaknesses of Machine Learning, That is actually not talked about much in academia, uh, uh because it turns out that the data sets we have in academia are not set up well for researchers to study and publish papers on this. I think we can start new Machine Learning benchmarks in the future. But there's one of those problems that is actually kind of under appreciated in academic literature. Uh, but that [NOISE] is a problem facing many, many practical deployments of Machine Learning algorithms. Um, and, uh, and so, more generally, the problem is one of data changing, right, and, uh, you might have new classes of users with new accents or you might train a lot on, uh, the, maybe you get data from even Stanford users and maybe Stanford is not too noisy or Stanford has a certain, you know, types of characteristics. When you ship it to another city or another country that's much more noisy, uh, you know, different background noise. [NOISE] Right? Or, uh, you start manufacturing the smart speaker and to lower the costs of the speaker, they swap it out, they swap out the high-end microphone that you use from your laptop to collect the data, for a lower-end microphone, [NOISE] right? It's very common thing done in, you know, well, done in manufacturing, right? If you could use a cheaper microphone why not, right, and, and often to human ears, The sound sounds just fine on a cheaper microphone, but if you train your learning algorithm using your, you know, I guess yeah, well, I use a Mac, but a Mac has a pretty decent microphones. If you train the data using all you collect from a Mac and then eventually has a different microphone, it may not generalize well. So, one of the challenges of, um, Machine Learning is that you often develop a system on one data set, and then when you ship a product, something about the world changes, uh, and, and, and your system needs to perform on a very different type [NOISE] of data than what you had trained on. Um, and so, [NOISE], Um, and so, what will happen is after you deploy the model, uh, the world may change and you often end up going back to get more data, redesign the model right and, and, and I guess, er, and this is, this is, uh, uh, the maintenance of the Machine Learning model. Uh, I wanna give some other examples. A web search, right, uh, this happens all the time, at multiple web search engine which is, uh, you train a neural network or you train a system to, to give relevant web search results. But then something about the world changes. You know, for example there's a major political event, some new person is elected president of some foreign country or there's a major scandal or just the Internet changes, right, or, or there's a, a, a, actually what, what happens in China is that new words getting invented all the time. Uh, uh, in, in, in Chin- the Chin- uh, says that, by of what they Google and Baidu, but the Chinese language is more fluid than the English language, and so new worst get invented all the time. And so, the language changes, and so whether we are trained just isn't working as long as it used to, right. Um, or, or, or maybe a different company, suddenly shuts off, you know, their entire website to your search index because they don't want you, uh, indexing their website and so it's like the Internet changes and what have you done doesn't work anymore. Um, or, um, uh, self-driving. Okay. [NOISE]. Uh, it turns out that you build a self-driving car in California, and then you try to deploy these vehicles in Texas, um, you know, it turns out traffic lights and Texas look very different than traffic lights in, in, in California. So, a model trained on California, Texas on, so a neural network trained to recognize, uh, California traffic lights actually doesn't work very well on Texas traffic lights. Right? Uh, I'm trying to remember which way around to this, but I think California and Texas have a different distribution of horizontal versus vertical traffic lights for example, there's actually, as humans don't notice as you go, oh yeah, red, yellow, green, but the learning algorithm doesn't actually generalize that well if you go to a different location. You've go to a foreign country, again, traffic lights, signage, the lane markings all change, um. Or I guess, well, one example I was working on earlier this week, right? Uh, manufacturing, right, landing AI, working on inspection of parts in factories, um, and so if you are doing visual inspection [NOISE] in the factory, and the factory starts making a new component, you know, they were making this model of cell phone, but cell phones turn over quickly, and so, but in a few months later they're making a different type of cell phone or something weird happens in the manufacturing process or the lighting changes and new type of defect. So the world changes, right? And, um, so, oh, she got locked again? [NOISE] What I'd like to do is, um, actually revisit the previous question in light of this, uh, the world changes, phenomenon, right? [NOISE] Which is, let's say you've collected a lot of data with American accented speakers. Um, and then, you know, we'll ship the product to the UK. And then, um, and then for some reason you find that you have all these British accent speakers, right, trying to use your smart speaker. So, between these two algorithms, the non machine-learning approach, which is a set the threshold versus trained in neural network. Which system do you think will be more robust for VAD, voice activity detection? [NOISE] All right, let's take like another, I don't know, 40 seconds? [NOISE] All right. Yeah. Interesting. Do people want to comment? Well, more, more, more people voted for a non-ML. Does  someone want to explain why. Yeah. Go ahead. [inaudible] Yeah. You're right. So we say, for the VAD voice-activated section, uh, if you just measure the volume, then it doesn't really depend on, on the absolute, right? So, non ML might be more robust. Anyone else? [NOISE]. All right. So, again I'm going to show you about. So, it turns out that, um, if you train a small neural network, uh, to, um, uh, you know, American accented speech, uh, there's a bigger chance, that your neural network, because it's so clever, right? They'll learn to recognize American speech, [NOISE] and have a harder time generalizing to British accented speech. Do I make sense? And so, one of the things that I've seen a lot of teams, uh, where-where's, so, so, one way that non ML thing could fail to generalize, would be that British speakers are systematically, you know, louder or softer than American speakers, right? So, you know, I know. I don't know if Americans stereo typically are louder or less loud than British, but, but, you know, but if, but if American and British speakers one, one country just has louder voices as softer voices, there maybe, the threshold you set won't generalize well, but that seems unlikely. I, I don't see that being realistically. But, um, but if you train your neural network a lot, parameters, then, uh, it's more likely that the neural network will pick up on some idiosyncrasy of American accents, to decide the [NOISE] salaries even speaking, um, and thus maybe less robust in generalizing into British accented speech, right? And, another way to think about this, is if you imagine to take it even further example, imagine that, um, you're using VAD for a totally different language than, than English, right? Where, um, take a different language, you know, Chinese or Hindi or Spanish or something where the sounds are really different. If you train a VAD system to detect, you know, English, it may not allow work for detecting Spanish or Chinese or French or, or some other language. Um. And so, do you think of British accents as uh, as somewhere on the spectrum. Not a foreign language by any means, but just more different, than, I think the, um, non ML system is, is, is more likely to be robust, right? And so, one of the lessons that too many, that, that a lot of machine learning teams learn the hard way is uh, um, if you don't need to use a learning algorithm for something, if you can hand code a simple rule, like, if volume greater than 0.01, do this or that, uh, those rules are can more robust. Um, and, the, one of the reasons we use learning algorithms is when we can't hand code something, like, I don't know how to hand code something to detect a cat, or detect the car on the road, or detect the person. So, use learning algorithms with those. But if there is such an encoded rule, that actually does pretty well, you find that it is more robust to shift into data and will often generalize better. And uh, if any of you take a, uh, uh, we talk a bit this, about this little bit in CS 229, I think CS 229 talks about this as well, but this particular observation is backed up by very rigorous learning theory. And the learning theory, is basically that the fewer parameters you have, uh, if you still do well on your training set. If you can have model with very few parameters, that does well on your training set, you would generalize better, right. So, there's this very rigorous machine learning theory that basically says that, and in the case of the non machine-learning approach there's maybe one parameter which is what's the threshold for epsilon. And that's worked well enough for your training set, then you are severe generalizing even when the data changes is, um, much higher, right? Um. Now, the last question, um, I want to pose for discussion today is, um, when when discussing deployments, uh, oh, and, and so, one of the lessons of deployment is, that's just the way the world works. Yeah. Pretty machinery system deployed, the world will usually change and you often end up collecting data and having integrated and maybe improve the model, right? And you have fixed a model for British speakers around you. Um. So, we talked about edge deployments, as well as cloud deployments, right? And so, um, ignoring issues of user privacy and latency, which are super important, but for purposes of question let's, let's, let's put aside issues of user privacy and network latency. Um, if you need to maintain the model, so it means there's means of updating the model, right? Even as the world changes. [NOISE]. Um, oh sorry I miss, I miss typed. It should read, does a cloud or edge deployment make maintenance easier, not of, right? Uh, once you, once you just enter a one word answer, and, and why? [NOISE] And so, maintenance is when the world changes, something changes. So you need to update the learning model to take advant- take care of this British accent. So, which type of deployment makes it easier? Let's take like, yeah, another two minutes to enter your answers. [NOISE] All right. Another like 50 seconds. All right. Cool. Now, see what people wrote. [NOISE] Wow. Cool. Great. All right. Almost everyone is saying Clouds. Most people are saying cloud. Almost. All right. Cool. Great. And, and just to summarize, I think there are actually two reasons why. Most people said it's easier to push updates, that's part of it. I think the other part of it, is that if all the data that is at the edge. If all the data is processed, you know, in the user's home and another constant cloud, then even if you have all of these unhappy British accents it uses, you may not even find out, right? You're sitting in the company headquarters, you have all these users and mysterious things, you know, seem to be not using your device maybe because they're unsatisfied with it, but if the data isn't coming into your servers in the cloud, then you might not even find anything about it. Now, there's serious issues about user privacy, uh, uh, as well as security, right? So, so, so, please if you haven't bought the product, please be respectful of that, uh, uh and then take, take, take care of that in a very thoughtful and respectful way of users. But um, [NOISE] if, uh, first if, um, so this is the cloud, right? If you have a lot of edge devices and all the data is processed there. Um, you won't even know what your users are doing. And they're happy or unhappy. You just don't know. But, if some of the data in a stream to your servers in the cloud, and if the user privacy with really fiscious good user consent tells you for what you're doing with the data, but if you take care of that in a, in a, in a, in a, in a reasonable and sound way, if you're able to examine some of the data, then you can at least figure out that gee looks like analyzing the data, there are these people at this axon, there's background noise that, um, uh is uh, giving it bad product experience and you can also maybe have the data, so you can gather the data from the edge, to feed back to your model, right? So, so it let's you detect that something's gone wrong, um, unless you have the data to retrain the model, so solve that British accent problem, we can retrain the model for more British accented speech, and then finally let's you push the model back up. All right. So, [NOISE] first, it let's you detect what's going on, two, it gives you data for training, and three, unless you more easily push the model backup. Uh, push, push, push the new model to production, to deployment, okay? Um, and this is also why, uh, even if your computation needs to run on the edge. If um, you could in a way respectful of user privacy and it's transparent at the how you use data, if you can get even a small sample of data, or have a few volunteer users send you some data back to the cloud, that will greatly increase your ability to detect there's something's gone wrong, as well as maybe give you some data to retrain the model. Uh, so even if you can only do, so that push updates, right, this, this will, this will help greatly, okay? Um. All right. So, finally, one last comment I think uh, one, one, one last challenge is uh, in a lot of machine learning systems, you're not done at deployment. There's a constant ongoing maintenance process. And I think, one of the processes, uh, you know, AI teams are getting better and as well, setting up QA, to make sure that we update the model, you don't break something. So I think, QA and large companies, quality assurance processes is kind of testers. And I think the way you test machine [NOISE] learning algorithms is different in the way you test traditional software, because the performance of machine learning algorithms is often measured in a statistical way, right? So it doesn't work and it doesn't work. It, it, it neither works nor doesn't work. Instead it works, you know, 95% of the time or something, and so, lot of companies are evolving the QA processes to have this type of statistical testing to make sure that even if you change the model, you do a push update, it still works, you know, 95 or 99% of the time or something rather than so, so, so putting in place new QA test processes as well. Okay? All right. Hope that was helpful stepping through what the full arc of a machine learning project will look like. Uh,will, will later this quarter or in course three as well as in later, lectures are present, [NOISE] we'll keep talking about machine learning strategy and how they make decisions things. So, let's break for today. 

Okay. Let's get started, guys. So welcome to lecture number 4. Um, today we will go over two topics that are not discussed, uh, in the Coursera videos. Uh, you've been learning C2M1 and C2M2, if I'm not mistaking. So you've learned about, uh, what, uh, an initialization is, how to tune neural networks, what tests validation and train sets are. Today, we're going to go a little further, uh, you should have the background to understand 80 percent of this, uh, lecture. There is gonna be 20 percent that I want you to look back after you've seen the BatchNorm videos for those of you who haven't seen them. So we split the lecture in two parts, and I put back the attendance code at the, at the very end of the lecture so don't worry. Ah, one topic is attacking, ah, neural networks, ah, with adversarial examples. Ah, the second one is generative adversarial networks. [NOISE] And although these two topics have a common word which is adversarial, they are two separate topics. You will understand why it's called adversarial in both cases. So let's get started with adversarial examples. And in 2013, ah, Christian Szegedy and his team have, uh, published a paper called Intriguing Properties of Neural Networks. What they noticed is that neural networks, neural networks have kind of a blind spot, the spots, uh, for which several machine learning including the state of the art ones that you will learn about, ah, VGG 16-19 inception, uh, networks and resi- re-residual networks are vulnerable to something called adversarial examples. These adversarial examples you're going to learn what it is, in three parts. First, by explaining how these examples in the context of images can attack a network in their blind spots, and, and make the network classify these images as something totally wrong. How to defend against these type of examples, and why are networks vulnerable to these type of examples. This is a little bit more theoretical, and we're going to go over it on the board. The, the papers that are listed on the bottom are the two big papers that, that started this field of research. So I would advise you to go and, and read them because we have only one hour-and-a-half to go over two big topics, um, in, in, in deep learning and, ah, we will not have the time to go into details of everything. Okay. So let's set up the goal. The goal is like, is that given a pre-trained network. So a network trained on ImageNet on 1,000 classes, millions of images. Ah, find an input image that is not an iguana, so it doesn't look like the animal iguana. A batch will be classified by the network as an iguana. We will call this an adversarial example if we manage to find it. Okay. Yeah, one question. Ah, what was the magic code for those that came in this late? Uh, let me- so 284889, let me write it down on the board so that you can- Thank you. Can you guys see? [NOISE] Okay. Let's move on. So we have a network pre-trained on image and it's a very good network. Ah, what I want is to fool it by giving it an image that doesn't look like an iguana but is classified as an iguana. So if I give it a cat image to start with. The network is obviously going to give me a vector of probabilities that has the maximum probability for a cat, because it's a good network. And you can guess what's the output layer of this network, it's probably a softmax, so classification network. Now what I want is to find an image x that is going to be classified as an iguana by the network. Okay. Does the, the, the setting makes sense to everyone? Okay. Now as usual, uh, this ma- this, this might remind you of what we've seen together about neural style transfer. You remember the, the art generation thing, where we wanted to generate an image based on the content of the first image and the style of another image. And in that problem, the main difference with classic supervised learning was that we fixed the parameters of the network, which was also pre-trained, and we back propagate the error of the loss all the way back to the input image to update the pixels, so that it looks like the content of the content image and the style of the style image. The first thing we did is that we rephrased the problem. We, we try to, to, to phrase what exactly we want. So wha- what would you say is a sentence that defines our last function let's say. Any ideas? Okay. Complicated. Yep. An image that provides minimum cost. An image that provides minimum cost. Okay. What's the cost you're talking about? Cost of the, the difference between the expected iguana and non-expected iguana. Expected iguana and non-expected iguana. Wha- what do you mean exactly by that? So if we're sort of going back in the training session, we're trying to train it on an image and we wanted to think that [NOISE] this is a cat and iguana. Yeah. Okay. So you want, ah, this image to minimize a certain loss function, and the loss function would be the distance metric between the output you're looking for and the output you want. Okay. Yeah. So I would say, we want to find x, the image, such that y hat of x, which is the result of the forward propagation of x in the network is equal to y-iguana, which is a one-hot vector with the one at the position of iguana. Does that make sense? So now based on that we define our loss function, which is can be an L2 loss, can be an L1 loss, can be a cross-entropy in practice. Ah, this one, ah, works better. So you see that minimizing this loss function, would lead our image x to be outputted as an iguana by the network. Does that makes sense? And then the process is very similar to neural style transfer, where we will optimize the image iteratively. So we will start with x, we will forward propagate it, compute the loss function that we just defined. And remember, we're not training the network, right? We'll just take the, the derivative of the loss function all the way back to the inputs, and update the input using a gradient descent algorithm until we get something that is classified as iguana. Yeah, any question on that? But this doesn't necessarily mean that the x that you get in- Okay. So you mentioned that it doesn't guarantee that x is loo- going to look like something. The only thing is guaranteeing is that this x will be classified as an iguana if we train properly. [NOISE] We will, we will talk about that now. Er, another question in the back I thought. Yeah. For the last question we miss the one that for logistic regression. Oh yeah, it could be binary cross en- it could be cross entropy. Yeah. So in this case not binary cross entropy because we have a, uh, uh, uh, a vector of, of n classes, where it could have been cross-entropy. Okay. So yeah that's true. We- are we guaranteed that the forged image x, this one, i- is going to look like an iguana? Who thinks it's going to look like an iguana? If you- who thinks it's not going to look like an iguana? Okay. Majority of people. So can someone tell me why i- it's not going to look like an iguana? [NOISE]. [inaudible] making a vector through a vector. Okay. So you're saying, uh, the loss function is unconstrained, is very unconstrained, so we didn't put any constraints on what the image should look like. That's true. Actually, the answer to this question is, it depends. We don't know. Maybe it looks like an iguana or maybe it doesn't. But in terms of probabilities, it's high chance that it doesn't look like an iguana. So the reason is here. Let's say this is our space of input images. And the interesting thing is that even if as a human on a daily basis we deal with images of the real world. So like, I mean, if you look at a TV, uh, that is totally buggy, you see pixels, random pixels, but in other contexts, we usually see real-world distribution images. A network is deterministic, it means it takes an image. Any input image that fits the, the first layer would, would be- would produce an output, right. So this is the whole space of input images that the network can see. Um, this is the space of real images, it's a lot smaller. Can someone tell me what's the size of the, the, the space of possible input images for a network? [NOISE]. Infinite. Huh? Sorry. Infinite. Infinite? Yeah. Uh, It's not infinite. It's, it's a lot but not- [NOISE] It's the number of the pixels to the power of the number of things it could be. Okay. Uh, yeah, there is an idea here. Someone there? I also said the same thing with just number of possible pixel permutations. Yeah, that's true. So more precisely- you would start with how many pixel values are there? There are 255, 256 pixel values, and then what's the size of an image? Let's say 64 by 64 by 3, and your results would give you 256, so you fix the first pixel, 256 possible value, then the second one can be anything else, then the third one can be anything else, and you end up with a very big number. So this is a huge number. And the space of real images is here. Now if we had to plot the space of im- of images classified as an iguana, it would be something like that. Right. And you see that there is a small overlap between the space of real images and the space of images classified by- as an iguana by the network. And this is where we probably are not. We're probably in the green part that is not overlapping with the red part, because we didn't constrain our optimization problem. Does that make sense? Okay. Now we're going to constrain it a little bit more, because in practice, these type of attacks are not too dangerous because as a, as a human we would see that the pictures look like garbage. The dangerous attack is if the picture looks like a cat, but the network sees it as an iguana and a human see it as a cat. Can someone think of, uh, of like malicious applications of that? [NOISE] Face recognition, it could show a face of- you, you could show your, your, picture of your face, it pushed the network [NOISE] to think it's a face of someone else. What else? Yeah. Breaking CAPTCHAs and breaking like against bot detection. Yeah. Breaking CAPTCHAs. If you know what the output, what output you want you can force the network to think that these CAPTCHA, uh, thi- this input CAPTCHA is the output it's looking for. Or in general, I would say like social medias, uh, if someone is malicious and wants to put, uh, violent content online, there is- all these companies have algorithms to check for this violent content. If people can use adversarial examples that look still violent, but are not detected as violent by the algorithms using this methodology, they could still publish their violent pictures. Uh, think about self-driving cars. A stop sign that looks like a stop sign for everyone, but when the self-driving car sees it, it's not a stop sign. So these are malicious applications of adversarial examples, and there are a lot more. Okay. And in fact, the picture we generated previously would look like that. It's nothing special. So now let's constrain our problem a little bit more. We're going to say we want the picture to look like a cat but be classified as an iguana. Okay. So now say we have our neural network. If we give it a cat it's going to predict that it's a cat. What we want is still give it a cat but predict that it's an iguana. Okay. I, I go quickly over that because it's very similar to what we did before, so I just plucked, I just put back what we had on the previous slide. Okay, exactly the same thing. Now, the way we rephrase our problem will be a little different. Instead of saying we want only y hat of x equals y- iguana, we have another constraint. What's the other constraint? The picture x should be closer to a picture of a cat. So we want x equal or very close to x-cat. And in terms of loss function, what it does is that it adds another term which is going to decide how x should be close to x-cat. If we minimize this loss now, we should have an image that looks like a cat because of the second term, and that is predicted as an iguana because of the first term. Does that makes sense? So we're just building up our loss functions, and I guess you guys are very familiar with this type of thought process now. Okay, and same process, we optimize until we hopefully get the cat. Now our question is, what should be the initial image we start with? We didn't talk about that in the previous example [NOISE].Yeah. White noise? White noise. Yeah, possibly white noise. Any other, uh, proposals? Maybe a cat. A cat? Yeah, which cat? The [inaudible] [NOISE]. I don't know. Probably the cat that we put in the loss function, right? Because it's- is the closest one to what we want to get. So if we want to have a fast process, we'd better start with exactly this cat, which is the one we put in our loss function here, right? If we put another cat, it's going to be a little longer because we have to change the pixel of the other cat to look like this cat. That's what we told our loss function. If we start with white noise, it will take even longer because we have to change the pixels all the way so that it looks real and then it looks like a cat that we defined here. So yeah, the best thing would be probably to start with the picture of the cat. Does that makes sense? And then move the pixels so that this term is also minimized. Yeah. So when you write that loss function, it seems like you are implicitly saying that what a human sees as a cat will just be like minimizing the RMSE error to the actual cat picture, right? Yeah? Is that- I mean, I thought that RMSE error was actually a really bad way to gauge whether or not a human, like saw two images as similar. Yeah. This is, this is empirical, the fact that we use that type of, of loss function. But in practice, it could have been any distance between X and X cat, and any distance between Y hat and Y cat, yeah, and Y iguana, sorry. Yes. So when you say X cat is [inaudible] just one specific cat. Yeah. [inaudible]. Exactly. I can't think of a way of making a constrained, like a complex loss function that takes a bunch of cats. And then it puts like something like a minimum of it, right? The minimum distance between [inaudible] Can we just look at this wide [inaudible] probability of like 0.55 probability of iguana and cat and then try to [inaudible] I'm not sure about the second method. But just to repeat the point you mentioned, is that here we had to choose a cat. It means the X cat is actually an image of a cat. So what if we don't know what the cat should look like, we just want a random cat to come out and be classified as an iguana. We're going to see uh,  generating networks after which can be used to do that type of stuff. But, uh, but for the second part of the question, I'm not sure what the optimization process would look like. Okay, let's move on? So yeah, it's probably a good idea to start with the cat image that we specified in the loss function. Okay. And so then we have an image of a cat that originally was classified as 92 percent cat and we modified a few pixels. So you can see that this image looks a little blurry. So by doing this modification, the network will think it's an iguana. Okay? And sometimes this modification can be very slight and we can even not be able to notice it. Sounds good. Now, let's add something else to this, uh, to this, uh, to this, uh, draft. We add a third set which is the space of images that look real to a human. So that's interesting because the, the space of images that look real to a human is actually bigger than space- than the space of real images. An example is this one. This is probably an image that looks real to human, but it's not an image that we could see in, in the daily life because of these slight pixel changes. Okay? So these are the space of dangerous adversarial examples. They looked real to human but they're not actually real. They might be used to fool a model. Okay. Now let's see a video, uh, by Kurakin et al, uh, on real-world example of adversarial examples. So for those who cannot read, they're taking, uh, a camera which, which classify- which has a classifier. And the classifier classifies the first part as a library and the second image that is that- the same as a prison. So the second image has slight different pixels but it's hard to see for a human. Same here. So the, the the classifier on the phone classifies the first image as a washer with 52 percent accuracy, confidence, and the second one as a door mat. Yeah. So yeah, this is, uh, a small example of- of what can, what can be done. Okay. Now let's go, we've seen how to generate these adversarial examples. It's an optimization process. We will see, uh, what are the type of attacks that we can lead and what are defenses against these adversarial examples. So we would usually, uh, split the attacks into two parts. non-targeted attacks and targeted attacks. So non-targeted attacks mean that we just want outputs, we just want to find an adversarial example that is going to fool the model. While targeted attack is we want to force this adversarial example to be output- to output a specific class that we chose. These are two different type of attacks that, that are widely discussed in, in the research. Knowledge of the attacker is something very important. For those of you who did some crypto, you know that we talk about white-box attacks, black-box attacks. So one interesting thing is that, uh, a black-box attack- a white-box attack is when you have access to a network. So we have our image in pre-train- in pre-trained network. We have fully access to, to all the parameters and, and the gradients. So it's probably an easier attack. Right? We can, we can back-propagate all the way back to the image and update the image, like we did. A black-box attack is when the model is probably encrypted or something like that, so that we don't have access to its parameters, activations, and, uh, architecture. So the question is how do we attack in black-box attack if we cannot back-propagate because we don't have access to the layers? Any ideas? Yeah. Numerical gradient. Numerical gradient. Yeah, good idea. So you know you will trick the image a little bit and you will see how it changes the loss. Looking at these you can, you can do have an estimate of the numerical gradient. Even if the model is a black-box model. This assumes that you can query the model, right? You can query it. What if you cannot even query the model or you can query it one time only, it's to send the adversarial example. How would you do that? So this becomes more complicated. So, there is ve- a very complex property of these adversarial examples is that they're highly transferable. It means I have a model here that is, uh, an animal classifier, okay? I don't have access to it. I cannot even query it. I still wanna fool it. What I'm going to do is that I'm going to build my own animal classifier, forge an adversarial example on it. It's highly likely that it's going to be an adversarial example for the other one as well. So, this is called transferability, and it's still a, uh, research topic, okay? We're trying to understand why this happens and, uh, also, uh, how to defend against that. You know, maybe a defense against that is to, is to- we're going to see it after, I'm not gonna say it now, sorry. Uh, does that make sense or no, this transferability? Probably is because two animal classifiers look at the same features in images, right? And maybe these pixels that are play- we're playing with are changing also the output of the other network. Let's go over some kind of defenses. So, one solution to defend against these adversarial networks is to create a safet- Safety Net. A Safety Net is what? Is, uh, a net that- like a firewall, you would put it before your network. Every image that comes in will be classified as fake like forged or real by the network and you only take those which are real and no- not adversarial. Does that make sense? So, you could- you could- you could say that, okay, but we can also build an adversarial network that, that fools this network, right? Just we need black-box or white-box, we can just create an adversarial net- example for this network. It's true. But the issue is that now we have two constraints. We have to fool the first one and the second one at the same time. You know, maybe if you fool the first one, there is a chance that the second one is going to be fooled. We don't know, okay? It just makes it more complex. There is no good defense at this point to- to- to all type of adversarial examples. This is an option that people are researching for. So, the paper is here if you want to check it out. Can you guys think of another solution? [NOISE]. I've got one. Yeah. Just like multiple in terms of loss functions [inaudible] adversarial examples loss functions and train them. Train on multiple loss functions of different networks? Yes. So, you're talking about ensembling. Maybe we can- maybe we can create five networks to do our tasks, and it's highly unlikely that the adversarial example is going to fool the five networks the same way, right? Any other idea? Yes. Uh, generate adversarial examples and train on them. Exactly. Generate adversarial examples and train on those, okay? So, you will generate a cat image that is adversarial. So, some pixels have been changed to fool a network. You will label it as the human sees it. So as a cat because you want the network to still see that as a cat and you will train on those. The downside of that is that it's very costly. We've seen that generating adversarial examples is super costly and also we don't know if it can generalize to other adversarial examples. Maybe we are going to overfit to the ones we have. So, it is another optimization problem. Now, another solution is to train on adversarial examples at the same time as we train on- on normal examples. So, look at this loss function. This loss function, the loss mu is a sum of two loss functions. One is the classic loss function we would use. So, let's say, cross entropy in the case of a- of a classification and the second one is the same loss function but we give it the adversarial version of x. So, what's the complexity of that at every gradient descent step? For every iteration of our gradient descent, we're going to have to iterate enough to forge an adversarial example at every step, right? Because we have x, what we wanna do is forward propagate x through the network to compute the first term, generate x adversarial with the optimization process and forward propagate it to calculate the second term and then back propagate over the weights of the network. This is super costly as well and is very similar to what you said, it's just online just all the time, okay? So, what is interesting is we're going to delve a little more. There's another technique called logit pairing, I just put it here. We're not going to talk about it. There is the paper here if you want to check it. It's another way to do adversarial training. Uh, but what I would like to talk about is more, from a theoretical perspective, why are neural network vulnerable to adversarial examples? So, let's, let's do some, some work on the board. Yeah, one question. Let's say, uh, so, when you want to expose the [inaudible] probably look like a cat, all right? So, you expect to be able to [inaudible] can't you just [inaudible] denoise it [inaudible]? Denoising is also a method that's interesting, but you- so the thing is that it's just like in crypto, every time you come up with a defense, someone will come up with an attack and it's a race between humans, you know. So, this is the same type of problem. And security problems are open-ended. Okay. So, let's go over, uh, something interesting that is more on the ins- on  the intuition side of adversarial examples. So, let me- let me write down something. Uh, so, one question we ask ourselves is why do adversarial example exist? What's the reason? And Ian Goodfellow and- and his team have came up with explaining- with the- one of the seminal papers of adversarial examples, where they argue that although many people in the past have- have attributed this existence of adversarial examples to high non-lineari- non-linearities of neural networks and overfitting. So, because we over-fit to a specific dataset, we actually don't understand what cats are. We just understanding what, what we've been trained on. Uh, they argue that it's actually the linear parts of networks that is the cause of the existence of adversarial examples. So, let's see why. And the example I'm gonna- I'm gonna look at is linear regression. So, together we've seen logistic regression. Linear regression is basically the same thing without the sigmoid. So, before the sigmoid, we have y-hat equals wx plus b. So, the forward propagation of our network is going to be y-hat equals wx plus b, okay? And our first example is going to be a six-dimensional input. Okay. We have a neuron here, but the neuron doesn't have any activation because we're in linear regression. So here what happens is simply w x plus b. Okay? And then we get y-hats. And we probably use an L1 or L2 loss because it's a regression problem to, uh, to train this network. Now let's look at our first example. Our first example where, uh, where it's- where we trained our network. So network has been trained- sorry. Network has been trained and converged to w equals one, three, minus one, two, two, three. This is w. And you know, like, because we defined x to be a vector of size 6, a column vector, w has to be a row vector of size 6. So the network converge to this value of w and b equals 0. So now, we're going to look at these inputs. We're giving a new input to the network. And the net- th- the input is going to be one, minus one, two, zero, three, minus two. Okay. So I'm going to forward propagate this to get y-hat equals wx plus b. [NOISE]. And this value is going to be 1 times 1 minus 3 minus 2 plus 0 plus 6 minus 6. If I didn't make a mistake, up, up, 2 minus 3 plus, okay. [NOISE] Okay. And so we- we- we basically get minus 4. And so this is the- the- the first- the first example that was propagated. Now, the question is [NOISE] how to change x into x-star such that y-hat changes radically but x-star is close to x? So this is basically a problem of adversarial examples. Can we find an example that is very close to x but radically- radically changes the output of our network? And we're trying to build intuition on- on adversarial neural networks. So the interesting part is to- is to identify how we should modify x. And the intuition comes from the derivative. If you take the derivative of y-hat with respect to x, you know that the definition of this term is- is like correlated to the impact on y-hat of small changes of x, right? How- what's the impact of small changes of x to- on- on the output? And if you compute it, what do you get? W. W? Everybody agrees? What's- what's the shape of this thing? Shape of that is the same as shape of x. So it should be w-transpose. Remember, derivative of a scalar with respect to a vector is the shape of the vector. Okay. Now it's interesting to- to see this because if we compute x-star to be, let's say, x plus a small perturbation like, I will call it, perturbation value. Can you write bigger? Yeah. Sorry. And can you see the top one? Yeah. You said yes or no? Yes. Okay. [NOISE]. So what if x-star equals x plus epsilon times w-transpose? You know, and this epsilon, I will call it value of the perturbation. Now, if we forward propagate x-star, it means we do y-hat-star equals w x-star plus b, would be zero at this point. We're going to get w x plus epsilon w times w-transpose. And w times w-transpose is a dot product, right? So this is the same as w-squared. So what is interesting? It's interesting because the- the smart part was that this term is always going to be positive. It means we- we moved a little bit x because we can make this change little by changing epsilon to a small value. But it's going to push y-hat to a larger value for sure. You know? And if I had a minus here instead of a plus, it will push y-hat to a smaller value. And the- the interesting thing is, now, if we compute x-star to be x plus epsilon times w-transpose, and we take epsilon to be a small value like, let's say, 0.2. You can make the calculation. What we get is- is this. So 1, minus 1, 2, 0, 3, minus 2, plus 0.2 times 1, 0.2 times 3, minus 0.2, plus 0.4, plus 0.4, and plus 0.6. So if you look at that, all the positive values have been pushed on the right. You agree? And all the negative values- uh, sorry, sorry. No, that's my bad. No, no, that's not it. So let- let's finish the calculation and I'll give the insight after. 1.2, minus 0.4, 1.8, 0.4, 3.4, and minus 1.4. So this is our x-star that we hope to be adversarial. Okay. Let's compute y-hat-star to see what happens. It's w x-star plus b, which is zero. So what we get when we multiply w by x-star is 1.2- [NOISE] 1.2 minus 1.2, minus 1.8 plus 0.8 plus 6.8 and minus 4.2. [NOISE], which I believe is going to give us 0.5. All right. So we see that a very slight change in x-star has pushed y-hat from minus four to 0.5. And so a few things we want to notice here. [NOISE]. So insights on this- on this small example. The first one is that, uh, if W is large, then X star is not similar to X, right? The larger the W, the less X star is- is likely to be like X. And specifically, if one entry of W is very large, XI, the pixel corresponding to this entry is going to be very different from XI star. Um, if W is large, X star is going to be different than X. So what we're going to do is that we are going to take sign- sign of W instead of taking W. What's the reason why we do that? Because the interesting part is the sign of- of the W. It means, if we play correctly with the sign of W, we will always push the X, this term WX star in the positive side. Because every entry here, this multiplication is going to give us a positive number, right? And the second insight is that as X grows in dimension, the impact of plus epsilon sign of W increases. Does that make sense? So the impact of sign of W on Y hat increases. And so what's interesting to notice is that we can keep epsilon as small as possible. It means X and X star will be very similar but as we grow in dimension, we're going to get more term in this, a lot more term. And the change in Y hat is going to grow and grow and grow and grow and grow. And so the one reason why adversarial examples exist for images is because the dimension is very high, 64 by 64 by three. So we can make epsilon very small and take the sign of W, we will still get Y hat to be far from the original value that it had. Does that make sense? Yeah. Do you guys have any questions on that? So epsilon doesn't grow with the dimension, but its impact of this term increases with the dimension. [NOISE] Okay. [NOISE]. The one hot encoder changes what into what? So you have the input image cat, right? Yeah. It puts it right between these two that gives [inaudible]. Okay. So you like- you try to unadversarially [inaudible] the cat? Yeah. Yeah. I- I don't know if that had been done. I don't think that has been done. So you're talking about taking an encoder that takes the adversarial example, convert it into a normal image of the cat and then give the cat. Yeah. Maybe yeah. I don't know. So it's a topic of research. Uh, okay, let's move on because we don't have too much time. So just to conclude, what we're going to count as a general way to generate adversarial examples is this formula. [NOISE] This is going to be a fast way to generate adversarial example. So this method is called the fas- Fast Gradient Sign Method. So basically what we're doing is that we can- we- we are linearizing the cost function in- in the proximity of, uh, the parameters. And we're saying that what's applied to linear networks here is going to also apply for this general formula for deeper networks. So we're pushing the pixel images in one direction that is going to impact highly the output, okay? So that's the intuition behind it. Now you might say that, okay, we did this example on the linear network, but neural networks are not linear, they are highly non-linear. In fact, if you look where the research has been going for the past few years, we are trying to linearize all the behaviors of these neural networks. With ReLU for example, or with Xavier initialization. All that type of methods, even the sigmoid, when we train on sigmoid, we do all we can to put sigmoid in the linear regime, because we want fast training. Okay? And one last thing that I'll mention for adversarial examples is if I have a network like this. [NOISE] So fully connected with three-dimensional inputs, up, yeah. And then one here and then the output. What's interesting is computing the chain rule on- on- on this neuron, will give you that derivative of the loss function with respect to, let's say, X is equal to the derivative of the loss function with respect to Z one, one. Here times derivative of Z one, one with respect to X. Let's say we're- we're going- we're going, there is actually a summation here. But anyway. Uh, just let me illustrate the point. Uh, what we're- what we're saying is that- what we're- what we try to do with neural networks is to have this gradient be high. Because if this gradient is not high, we're not able to train the parameters of this neuron and we need this gradient to be high. Because if you want to do the same thing with the- with W one, one, which is the parameter related to this neuron, you would need to go to this chain rule. Correct? So we need this gradient to be high. And if this gradient is high, the gradient with respect to the input is also going to be high. Because you use the same gradient in the chain rule. So networks that are- that have high gradients and that are operating in the linear regime are even more, uh, vulnerable to adversarial examples because of this observation. So any question on, on adversarial examples? Before we move on, I think we don't have time and I would like to, to go over the, the GANs with you guys. So let's move on to GANs. I'll stick around to answer questions on that part. So the general question we're asking now is, uh, do neural networks understand the data? Because we've seen that some, some data points look like they would be real, uh, but the neural networks don't understand it. So more generally, uh, can we build generated networks that can mimic the real-world distribution of images? Let's say, and this is what we will call generative adversarial networks. We'll start by motivating it, and then we look at something called the minimax game between two networks, a generator and a discriminator, that are going to help each other improve, and finally we'll see that GANs are hard to train, uh, we'll see some tips to train them, and finally, go over some nice results and methods to evaluate GANs, okay? So, uh, the motivation behind generative adversarial networks is to handle computers with an understanding of our world, okay? So by, by that we mean that we want to collect a lot of data, use it to train a model that can generate images that look like they're real even if they're not, so a dog that has never existed can be generated by this network. Um, and finally, uh, the number of parameters of the model, uh, is smaller than the amount of data, we already talked about that, and this is the intuition behind why a generated network can exist. Is because there is too much data in the world, any images count as data for generating the network, and there are not enough parameters to mimic this data. You know, you have- the network needs to understand the salient features of the dataset, because it doesn't have enough parameters to overfit everything. So let's talk about probability distributions. So these are samples from real images that have been taken, and if you plot this real data distribution in a 2-D map, uh, it would look like something like that. I made it up, but this is the image space similar to what we talked about in adversarial networks, and this green shape is the space of real-world images. Now, uh, if you train a generator and generate some images that look like this, and these images come from StackGAN, uh, from Zhang et al. Uh, this distribution, if the generator is not good, is not going to match the real world distribution. So our goal here is to do something so that the red distribution matches the real-world distribution, then to train the network so that it realizes what we want. So this is our generator and it's what counts, is what, what we want to train ultimately. We want to give it, let's say, a random number or a random latent code of 100 dimension scalar numbers, and we want it to output an image. But of course because it's not trained initially, it's going to output a random image, looks like something like that random pixels. Now, this image doesn't look very good. What we want is these images to look like generated images that are very similar to the real world. So how are we going to help this generator train? It's not like what we did in classic supervised learning, because we don't have, uh, we don't really have inputs and labels, you know, there is no label. We could maybe give it an image of a cat and ask it to output another cat, but we want the network to be able to output things that don't exist, things that we've never seen. Right. So we want the network to understand what a cat is but not overfit to the cat we give it. So the way we're going to do it is through a small game between these network called the generator G, and another network called the discriminator D. Let's, let's look at how it works. We have a database of real images, and we're going to start with this distribution on the bottom, which is the real-world data distribution, is the distribution of the images in this database. Now our generator has this distribution initially, it means the pixels that you see here probably follow a distribution that doesn't match the real world. We'll define the discriminator D, and the goal of the discriminator will be to detect if an image is real or not. So we're going to give several images to this discriminator, sometimes we will give it generated images, and sometimes we will give it real-world images. What we want is that this discriminator is a binary classifier that outputs one if the image is real and zero if the image was generated, okay? So let's say we give it x coming from the generated image is going to give us zero, because we want the discriminator to detect that x was actually G of z. If the image came from our database of real images, we want the discriminator to say one. So it seems like the discriminator would be easy to train, right? It's just a binary classification. We can define a loss function. That is the binary cross entropy. And the good thing is we can have as many label as we want, like it's, it's unsupervised but a little bit supervised, you know, we have this database and we label it all as one, it's just this image exists, let's label them as one for discriminator, and everything that comes out of the generator let's label it as zero for discriminator. So basically, data is not costly at all in this point. The way we will train is that we will backpropagate the gradient to the discriminator to train the discriminator, using a binary cross entropy. But what we ultimately want is to train the generator, that's what we want. At the end, we were not going to use the discriminator, we just want to generate images. So we are going to direct the gradient to go back to the generator. And why does this gradient can go back to the generator? The reason is that x is G of z, it means we can backpropagate the gradient all the way back to the input of the discriminator. But this input depends on the input of the generator if the image was generated. So we can also backpropagate and direct the gradient to the generator. Does it make sense? There is a direct relation between z and the loss function, in the case where the image was generated. If the image was real, then the generator couldn't get the gradient, because x doesn't depend on z or on the features and parameters of the generator. Okay? So we would run an algorithm such as Adam, um, simultaneously on two minibatches, one for the true data and from, from generated data. Does this scheme makes sense to everyone? Yeah, one question? So you said there was two minibatches, you're not mixing two and generating it together. So there's many methods of, your question is about mixing the minibatches. Usually we would use, uh, we would, we would use one minibatch for the real data and one minibatch for the fake data. But in, in practice, you can try other things. Yeah. So there are many methods that are being tried to train GANs property. We're going to delve a little more into the details of that when we will see the loss functions. So we hope that the probability distributions will match at the end, and if it matches, we're going to just take the generator and generate images, normally it should be able to generate images that look real, [NOISE] that looked like they came from this distribution. Okay? Sounds good? So now let's talk more about the training procedure and try to figure out what the loss functions should be in this case. What should be the cost of the discriminator? Assuming, assuming we give two minibatches, one for real data, so real images, and one for generated data that come from G [NOISE]. Yes. The same basic loss function we use for every binary classes, right? The same basic loss function we use from binary class- for binary class case. It's true we're going to tweak it a tiny bit, but it's the same idea. So this is what it can look like. We're going to call it JD, cost function of the discriminator. It has two terms. What does the first term say? What does the second term say? And you can recognize the binary cross-entropy here. [NOISE]. The only difference is that we have a label that is Y_real and a label that is Y_generated. In practice, Y_real and Y_generated are always going to be set to values. We know that Y_generated is zero and we know that Y_real is one. So we can just remove these two terms because they're both equal to one. The first term is telling us this should correctly label real data as one, the cross-entropy term. The first term of a binary cross-entropy. The second term is going to tell us, D should correctly labeled generated data as zero. So the difference with classic cross-entropy we've seen is that, this summation is the summation over the real mini-batch. And the summation on the second cross-entropy is a summation on generated mini-batch. Does that makes sense? So we both want the D to correctly identify real data, and also correctly identify fake data. That's why we have two terms. Now, what about the generator? What do you think should be the cost function of the generator? Yes. So just about that cost function. If I've been putting data that's from the generator, I won't run the first pass because I don't have a, uh, a Y_real if I have the- an input that's coming in from the generator. Yeah. Exactly. It's about half of this. Yeah. But in your batch, we have had, like, a certain number of real example, a certain number of generated examples. The generated examples have no impact on the first cross-entropy, and same for the real examples on the second cross-entropy. Any other questions? Okay. So coming back to the cross- to the- to the cost of the generator. What should it be? This is a tiny bit complicated. Let's move- let's move on because we don't have too much time. The cost of the generator basically should say that G should try to prove D. [NOISE] The goal is to for G to generate real samples. And in order to generate real samples, we want to fool D. If G managed to fool D and D is very good, it means G is very good, right? The problem is that it's a game. Because if D is bad and G fools D, it doesn't mean that G is good. Because G- because D is bad, it doesn't detect very well the real versus fake examples. We want D to go up to- to be very good and G to go up at the same time. Until the equilibrium is reached at a certain point where D will always output one-half, like, random probabilities because it cannot distinguish the samples coming from G versus the real samples. So this cost function is basically saying, uh, for generated images, we want D to classify them as one. That's what it's saying. We want to fool D, okay? Yeah. One question. Uh, just a little bit of a side question, um, I can kind of see- so if you're implementing this, I can kind of see how you would, uh, you know, implement for D, but how would you implement for D as if you're actually implementing this? Um, is there- has there been a module to dot train this because it's not immediately obvious how you do this setup? So, you know, like, if you- if, if you're using- so how to implement that? If you're using a deep learning framework, you've been building a graph, right? And at the end of your graph, you've been building your cost function D that is very close to a binary cross-entropy. Uh, what you're going to just do is to define a node that is going to be minus the cost function of D. It's going- every time you are going to call the function J of G, it's going to run the graph that you define for J of D and run, uh, an in- an opposition operation- an oppositive operation. Yeah. So now you have two different cost functions. How can they propagate gradients back the same way? These are two different cost functions. Propagate gradients back the same way? Yeah. We're not going to propagate the same way. We are going to- to returning [OVERLAPPING] to a minus sign for the grad- for the generator. So, you know, you- you- you backpropagate on the- on the- on- on D. And when you backpropagate on G, you would flip- you would flip the sign. That's all we do. The same thing with the sign flipped. In terms of implementation it's just, uh, another operation. Okay. Now, let's look at someth- something interesting is that this, uh, log- logarithm. Let's look at [NOISE] at the graph of the logarithm. So I'm going to plot against the axes, axis G, oh sorry, D of G of z. So what does this mean? This axis is the output of D when given a generated example, G of z. It's going to be between zero and one because it's a probability. D is a binary classifier with a sigmoid, uh, output probably. Um, if we plot logarithm of X. So, like, this type of thing. This would be log of D of G of z. Does it makes sense? That's the logarithm function. Um, if I plot minus that, minus that. So let me- let me plot minus logarithm of G of D of z or, or let me- let me do something else. Let me plot logarithm of minus D of G of z. This is it. Do, do you guys agree? Now, what I'm going to do is that I'm going to plot another function that is this one. That is logarithm of one minus D of G of z, okay? So the question is, right now, what we're doing is that we're saying the, the cost function of the generator is logarithm of 1 minus D of G of z. So it looks like this, right? It looks like this one. [NOISE] What's the issue with this one? What do you think is the issue with this cost function looking at it like that? It goes to negative infinity? Sorry. It goes to negative infinity? Can you say it louder? I mean, it go- goes to negative in- infinity. It goes to negative infinity in, in one, that's what you mean? Yeah. Yeah. And so the, the consequence of that is that the gradient here is going to be very large, the closer we go to one. But the closer we are to zero, the lower is the gradient. And it's the reverse phenomenon for this lo- logarithm. The gradient is very high, and very high I mean in absolute value. Very high when we're close to zero, but it's very low when we go close to one, okay? So which loss function do you think would be better? A loss function that looks like this one or a loss function that looks like this one to train our generator? The broader question is where are we early in the training? Are we close to here or are we close to there? What does it mean to be close there? Close to one? [NOISE]. You're fooling the network. Hmm? You're fooling the network. You're fooling the network. It means that D thinks that generated, uh, samples are real. They're here. This place is the contrary. D thinks that generated samples are fake. It means, correctly finds out that they're fake. Early on, we're generally here. Because the discriminator is better than the generator. Generator outputs garbage at the beginning, and it's very easy for the discriminator to figure out that it's fake because this garbage looks very different from real world data. So early on, we're here. So which function is the best one to- to- to- to- to be our cost? [inaudible]. Yeah. So probably, this one is better. So we have to use a mathematical trick to change this into that. Right. And the mathematical trick is pretty standard. Right now, we're minimizing something that is in log of one minus X. We can say that doing so is the same as maximizing something that is in log of X. Do you agree? Simple flip. I mean, max flip. And we can also say that it is the same as minimizing something in minus log of X. Does it make sense? So we are going to use this mathematical trick to convert our function that is a saturating cost, we would say, into a non-saturating cost that is going to look more like this. Let's see what it looks like. So to sum up, our cost function currently looks like that. It's a saturating cost. Because early on, the gradients are small. We cannot train G. We're going to do a flip that I just talked about on the board, and converts this into another function that is a non-saturating cost. Okay. Yeah. Well, actually, yeah. So the reason it's the blue one is like that is because I added a minus sign here. So I'm flipping this. Okay? And it's the same thing, it's just the- the sign of the gradient that is going to be different. Like that, the gradient is high at the beginning and low at the end. That makes sense? [NOISE] So we're going to do the- use this flip. And so we have a new training procedure now where J of D didn't change but J of G changed. We have a minus sign here and instead of the log of one minus D of G of Z, we have the log of G, uh, D of G of Z. Does that make sense to everyone? Good. And actually, so this is a fun thing if you- if you check this paper which is really cool, our GANs created a large, study of many, many different GANs. It shows what people have tried. And you can see that people have tried all types of loss to make GANs trainable. So it looks- it looks complicated here. But actually, the MM GAN is the first one we saw together. It's the mini-max loss function. The second one is the non-saturating one that we just see. So you see between the first two. The only difference is that on the generator, we gets the log of one minus D of X hat becoming log- minus log of D of X hat. Okay. Now, another trick to train GANs is to use the fact that, uh, a non-saturating, uh, to use the fact that D is usually easier to train than G. But as D improves, G can improve. If D doesn't improve, G cannot improve. So you can see the- the- the- the performance of D as an upper bound to what G can achieve. Because of that, we will usually train D more time than we will train G. So we will basically train for num_iteration, K times D, one times G. K times D, one times G, and so on. So that the discriminator becomes better than the- the generator can catch up. Better than can catch up, and so on. That make sense? There's also methods to use like different learning rates for D and G to take this into account, to train faster the discriminator. Okay. Uh, because we don't have too much time, I'm going to skip the BatchNorm with GANs. We are going to sit probably next week, uh, together after you guys have seen the BatchNorm videos. Okay. It's cool. So just to sum up. Some- some tips to train GANs is to modify the cost function. We've seen one modification, there are many more. Uh, keeping D up-to-date with respect to G. So updating D more than you update G using Virtual BatchNorm which is a derivate of BatchNorm, so it's a different type of BatchNorm that is used here. And something called one-sided la- label smoothing that I'm not going to talk about it today because we don't have time. So let's see some nice result now, and that's the funniest part. Um, so some of you have worked with word embeddings, and you- you might know that word embeddings are vectors that can encode the meaning of a word. And you can compute operations sometimes on these- on these words. So if you take, um, if you take king minus queen, it should be equal to man minus woman. Operations like that. That's happened in the space of encoding. So here's the thing. You can use a generator to generate faces, and the paper is listed on the bottom here. So you give a code that is a random code and it will give you an image of a- a face. You can give it a second code, it's going to give you a second image that is different from the first one because the code was different. You can give it a third one, it's going to give you a third fa- third face. The fun part is, if you take code one minus code two plus code three. So basically, image of a man with glasses minus image of a man plus image of a woman will give you an image of a woman with glasses. So [OVERLAPPING]. So this is interesting because it means that linear operation in the latent space of codes have impact directly on the image space. Okay. Let's look at something even better. So you can use GANs for image generation. Of course, these are very nice samples. You see that sometimes, GANs have problem with- with the- [LAUGHTER] I don't know. I don't think that's a dog. But- but- but the- but these are StackGAN++ is a- is a very impressive GAN that has generated- that has been state of the art for a long time. Okay. So let's see something fun. Something called image-to-image translation. So, uh, actually, the- the- the project winners last quarter in Spring was a project dealing with exactly that. Generating satellite images based on the map image. So given the map image, generate the satellite image using a GAN. So you see that instead of giving a latent code that was 100 dimensional, you could give a very detailed code. The code can be this image. Right? And you have to find a way to constrain your network in a certain- with- in a certain way to push it to output exactly the satellite image that corresponded to this map image. There are many other results that are fun. Converting zebras to- horses to zebras and zebras to horses. Um, and apples to oranges and oranges to apple. So let's do a- a case study together. Let's say our goal is to convert horses to zebras on images and vice versa. Can you tell me what data we need? Let's go quickly so that we have some time. Horses and zebras? Yeah. Horses and zebras. Do you need per images? You know, like, do you need to have the same image of a horse as a zebra? No. Yeah. So the problem is, uh, okay, we could have labeled images, you know, like uh, a horse and its, uh, zebra doppelganger in the same position. Uh, and we could train a network to take one and output the other. Unfortunately, we don't- not- every horse has a doppelganger that is a zebra, so we cannot do that. Uh, so instead, we're going to do unpaired, uh, unpaired generative adversarial networks. It means we have a database of horses and a database of zebras. But these are different horses and different zebras. They're not one-to-one- there's no one-to-one mapping between them. There's no mapping at all. What architecture do you wanna use? GAN? Nice. [LAUGHTER] GAN, not a [inaudible]. Okay. So let's see about the architecture and the cost. So I'm going over it very quickly because it's a- it's a very fun GAN with- it's called CycleGAN. So the way we are going to work it out is we have a horse called capital H. We want to generate the zebra version of this horse, right. So we give it to a generator that we call G1. You can call it H2Z, like horse to zebra. It should give us this horse H as a zebra, right. And in fact, if we're training a GAN, we need a discriminator. So, we will add a discriminator that is going to be a binary classifier to tell us if this image outputted by Generator 1 is real or not. So this discriminator is going to take in some images of zebras, probably, or-yeah, zebras or horses [NOISE], and it's going to also take the generated images and going to see if which one is fake which one is real. On the other hand, we're going to do- and the vice versa is very important. We need to enforce the fact that this horse G1 of H should be the same horse as H. In order to do that, we're going to create another gen- generator which is going to take the generated image, and generate back the input image. And this is where we will be able to enforce the constraints that G2 of G1 of H should be equal to H. Do you see why this loop is super important? Because if we don't have this loop, we don't have a constraint on the fact that the horse should be the- the zebra should be the horse as a zebra, the same horse as H. So we'll do that and we have a second discriminator to decide if this image is real. This is one step, H2Z. Another state might be Z2H where we start with zebra, give it to Generator 2, ge nerate the horse version of the zebra. Discriminate, generate back the zebra version of the zebra and discriminate. Does that makes sense? So this is the general pattern using CycleGANs. And what I'd like to go over is what loss should we minimize in order to enforce the fact that we want the horse to be converted to a zebra that is the same as the horse. Can someone give me the terms that we need? Someone wants to give it a try? Go for it. Two minutes. Yes. So you want to make sure that the picture in the end that is of the zebra that you started off with, matches the zebra that you started it with or the horse that you started off matches the horse that you had originally. Okay. But at the same time, you also need to have Discriminator 2 identifying that the image is a real zebra or a real horse- Yeah. -because you don't want it to just sort of input in the sample image and it output back to you the sample image. Yeah, correct. So I think you'd want to add the output of the cost function for Discriminator 2 to the cost that you get at for comparing the starting images. Okay, that's correct. So you're saying we need the classic cost functions that we've seen previously, plus another one that is the matching between H and G2 of G1 of H, and Z and G1 of G2 of Z. Yes. Correct. So we have all these terms. One term to train G1, which is the classic term we've seen, differentiate real images from generated images. G1 is what? Same. We are using the non-saturating costs on generating images. Same for D2. Same for G2. These are classics. The one we need to add to all of this is the cycle costs which is the distance between this term, G2 of G1 of H and H, and the same thing for zebras. Does that make sense? So you have the intuition to build that type of loss. We just sum everything and it gives us the cost function we're looking for. Yeah. Can we use the same, uh, D1 as D2? It's the same [inaudible] recognized [inaudible] Oh, the same cost function for D1 and D2? Yeah. Could you use the same- So, the, the- you could but it's not going to work that well. I think- So I think there's a- there's a tiny mistake here, is that, uh, the Zi here, the small Zi should be small Hi, and the small Hi on top should be a small Zi. Because the Discriminator 1 is going to receive generated samples that look like zebras because it came out of G1. So you want the real database to- that you give it to to be zebras as well. To force- to force the generator one to output things that look like zebras, and vice versa for the second one. Okay? And this my favorite. So you can convert the ramen to a face and back to a ramen. [LAUGHTER] It's the most fun application I found. It's from Naritomi et al, and Takuya Tako. So it's Japanese research lab are working hard to, to, to do face2ramen [LAUGHTER]. And actually, in two- in two to three weeks, you will learn, um, object detection, you know, to detect faces. And if you learn that, maybe you can start a project to like detect the face and then replace it by a ramen. [LAUGHTER] Because I don't know, this is also a funny, funny work by Naritomi. Okay. Oh, this is a super cool application as well. So let's look at that. Okay. So we have- so this model is a conditional GAN that was conditioned on learning, um, learning edges and generating cats based on the edges. So I'm gonna- I'm gonna to try to draw a cat. [LAUGHTER] Okay, sorry. I cannot see [LAUGHTER]. Again, I'm not a good drawer- [LAUGHTER]. It's a cat. Okay. It's going to download the model. I hope it's gonna work. [LAUGHTER] Okay. Yeah, I, I don't think it worked, but it's supposed to work. So you can generate cats based on, on edges and you can do it for different things. You can do it for a shoe. So all these models have been trained for that. Okay. Yeah, I have a question. Yes, go for it. [NOISE] So, so for this model, would you have the specific things for the things that you want it to generate? Like two things, so cats and shoes in this case? Uh, sorry. Can you repeat? Is it generalizable or do you have to train it specifically for the domains? You have to train it specifically for the domain. So like these models are different models that have [NOISE] been trained. Okay. Okay. I'm looking for my presentation, [NOISE] I missed it. The presentation disappeared. Okay. Another application is super resolution. You can give a lower resolution image and generate the super resolution version of it using GANs. And this is pretty cool because you can get, uh, a high resolution image, down-sample it, and use this as the minimax game, you know. [NOISE] Like you have the high resolution version of the lower ver- ver- lower-resolution image. Um, other applications can be privacy-preserving. So some people have been working on- you know in medical- uh, in the medical space privacy is a huge issue. You cannot share a dataset among hospitals, among medical teams it's common, so people have been looking at generating a dataset that looks like a medical dataset. If you train a model on this dataset, it's going to give you the same type of parameters than the other one, but this dataset is anonymized. So they can share the anonymized data with each other and train their model on that, without being able to access, uh, the information of the patient and who it is. Um, manufacturing is important as well, so GANs can generate, um, very specific, uh, objects that can replace bones for humans, personalized to, to the human body. So same for dental. If you lose the teeth, uh, the, the technician can take a picture and decide what's the, the crown should look like. The GAN can generate it. Um, another topic is how to evaluate GANs, you know. Um, you might say we can just look at the images and see if they look real and it will give us an idea if the GAN is working well. In fact, this is hard because maybe the images you're looking at are over-fitting images from the real samples you gave to the- to the- to the discriminator. Uh, so how do you check that? It's very complicated. So human annotation is a big one, where you would, uh, [NOISE] you would build a software, push it on the cloud and people all around the world are gonna select which images look generated, which images look not generated to see if a human can, can, can compare your GAN to real-world data, and how your GAN performs. So it would look like that. A web app indicates which image is fake, which image is real. You can- you can do different experiments like you can show very quickly an image for a fraction of a second and ask them was it real or not, or you can give them unlimited time. Different experiments can be led. Uh, there is another one that is more scalable because human annotation is very painful. You know, every time you train a GAN, you want to do that to verify if the GAN is working well. It takes a lot of time. So instead of using humans, why don't we use a very good network that is good at classification. In fact, in fact, the Inception network is a tremendous network that does classification. We're going to give our image samples to this Inception network and see what the network thinks of this image. Does it think that it's a dog or not? Does it look like a dog for the network or not? And we can scale it and make it very quick. And there is a Inception score that, that we can talk next week about when we'll have time. Uh, it measures the quality of the samples and also it measures the diversity of the sample. I'll go over it next week, hopefully. Uh, there is another distance that is very popular, uh, that has been growingly popular recently called the Frechet Inception Distance. And I, I- I'll advise you to check some of these paper if you're more interested in it for, for your projects. So just to end, um, for next Wednesday, we'll have, uh, C2 and three and also the whole C3 modules. [NOISE] Uh, you'll have three quizzes. Be careful, these two quiz, C3M1 and C3M2 are longer than ca- than normal quizzes. They're like wide case studies, so take your time, and go over it, um, and you have one programming assignment. Uh, make sure you understand the BatchNorm videos, so that we can go over the virtual BatchNorm hopefully next week together. Um, and hands-on section this Friday, uh, you will receive your project proposal as soon as possible, uh, and meet with your project TAs to go over the proposal and to make decisions regarding the next steps for your projects. Uh, I'll stick around in case you have any questions. Okay. Thanks, guys. 

Thanks for being here for Lecture 5, uh, of CS230. Um, today we have, uh, the chance to, to host, uh, a guest speaker Pranav Rajpurkar, who is a PhD student, um, in computer science advised by, uh, Professor Andrew Ng and Professor Percy Liang. So Pranav is, uh, [NOISE] is working on, um, AI and high impact projects, uh, specifically related to health care and natural language processing. And today he's going to, to present, um, an overview of AI for healthcare and he's going to dig into some projects he has led, uh, through case studies. So, uh, don't hesitate to interact, I think we have a lot to learn from Pranav, and, um, he's really an industry expert for AI for health care, um, and I, I lent you the mic Pranav. Thanks for being here. Thanks, Kian. Thanks for inviting me. Uh, can you hear me at the back, is the mic on? All right, fantastic. Well really glad to be here. Um, so I wanna cover three things today. The first is give you a sort of broad overview of what AI applications in healthcare look like. [NOISE] The second is bring you three case studies from the lab that I'm in, ah, as demonstrations of AI and health care research. [NOISE] And then finally, ah, some ways that you can get involved if you're interested in applying AI to high impact problems in health care or if you're from a healthcare background as well. Let's start with the first [NOISE]. So one way we can decompose the kinds of things AI can do in healthcare is by trying to formulate levels of questions that we can ask from data. At the lowest level are what are descriptive questions. Here, we're really trying to get at what happened. Then there are diagnostic questions where we're asking why did it happen? If a patient had chest pains, I took their, [NOISE] x-ray, what does that chest x-ray show? If they have palpitations, what does their ECG show? Then there are predictive problems. Here, I care about asking about the future, what's going to happen in the next six months? [NOISE] And then at the highest level, are prescriptive problems. Here, I'm really trying to ask, okay, I know this is the patient, this is the symptoms they're coming in with, this is how, ah, their trajectory will look like in terms of, um, in terms of things that may happen that they're at risk of, what should I do? And this is the real action point. And that's I would say the, the gold mine but, ah, to get there requires a lot of data and a lot of steps. And we'll talk a little bit more about that. So in CS230 you're all well aware, ah, of the paradigm shift of, of deep learning. And if we look at machine-learning in, um, healthcare literature, we see that it has a very similar pattern is that we had this feature extraction engineer who was responsible for, um, getting from the input to a set of features that a classifier can understand, and the deep learning paradigm is to combine feature extraction and the classification into one step, ah, by automatically extracting features, which is cool. Here's what I think will be the next paradigm, ah, shift for AI in healthcare, but also more generally. Is, ah, we still have a deep learning engineer up here, ah, that's you, that's me, ah, that are designing the networks, that are making decisions like a convolutional neural network is the best architecture for this problem, this specific type of architecture. There's an RNN and CNN and whatever NN you can throw on there. But, what if we could just replace out the ML engineer as well? Ah, and I find this quite funny, because everyone, you know, in AI for healthcare, a question that I get asked a lot is, are we going to replace doctors with all these AI solutions? And nobody actually realizes that we might replace machine-learning engineers faster than we might replace doctors if this, er, is to be the case. And a lot of research is, ah, developing algorithms that can automatically learn architectures, some of which you might go through in this class. Great. So that's the general overview. Now I wanna talk about three case studies in the lab [NOISE] of AI being applied to different problems. And because healthcare is so broad I thought I'd focus in on one narrow vertical, and let us go deep on that and that's medical imaging. So I've chosen three problems, um, and one of them is a 1D problem, the second is a 2D problem is, and the third is a, is a t- 3D problem, so I thought we could- we can walk through [NOISE] all the different kinds of data here. Ah, so this is some work that was done early last year in the lab where we showed that we were able to detect arrhythmias at the level of cardiologists. Um, so arrhythmias are an important problem, affect millions of people, ah, this has especially come to light recently with the, ah, devices, like the Apple Watch which now have a ECG monitoring. Ah, and, ah, the thing about this is that sometimes you might have symptoms and know that you have arrhythmias, but other times, ah, you may not have symptoms and still have arrhythmias that can be addressed with, ah, with, ah, if, if you were to do an ECG. An ECG's test is basically showing the heart's electrical activity over time. The electrodes are attached to the skin, safe tests and it takes over a few minutes, and this is what it looks like when you're hooked up to all the different electrodes. Ah, so this test is often done for a few minutes in the hospital, [NOISE] and the finding is basically that, uh, in a few minutes you can't really capture a person's abnormal heart rhythms. So let's send them home for 24 to 48 hours with a Holter monitor, and let's see what we can find. Um, there are more recent devices such as the Zio Patch, which let, um, let patients be monitored for up to two weeks, and [NOISE] it's, it's quite convenient. You can use it in the shower or while you're sleeping, so you really can capture, ah, a lot of what- what's happening in the hearts, uh, uh, ECG activity. But if we look at the amount of data that's generated in two weeks, it's 1.6 million heartbeats. That's a lot, and there are very few doctors who'd be willing to go through two weeks of ECG reading for each of their patients. And this really motivates why we need automated interpretation here. But automated detection comes with its challenges. One of them is, you know, you have in the hospital several electrodes, and in more recent devices we have just one. Ah, and the way one can think of several electrodes is sort of th- the electrical activity of the heart is 3D, and, um, each one of the electrodes is giving a different 2D perspective into the 3D perspective, um, but now that we'll have only one lead, we only have one of these perspectives available. Ah, and the second one is that the differences between the heart rhythms are very subtle. This is what a cardiac cycle looks like, and when we're looking at, ah, [NOISE] arrhythmias or abnormal heart rhythms, ah, one's going to look at the substructures within the cycle and then, ah, this structure between cycles as well. And the differences are, are quite subtle. So when we started working on this problem, oh, maybe I should share this story. So, uh, we started working on this problem and then it was, uh, me, my, my collaborator Awni and, uh, and Professor Ng. And one of the things that he, that he mentioned we should do he said, "Let's just go out and read ECG books and let's do the exercises." And if you're in med school, there are these, uh, books where, where you can, uh, where you can learn about ECG interpretation and then there are several exercises that you can do to test yourselves. Uh, so I went to the med school library, uh, you know, they have those, uh, uh, hand-cranked, uh, shelves at the bottom so you have to move them and then grab my book. And then we went for two weeks and did, uh, learned, so did go through two books and learned ECG interpretation and it was pretty challenging. And if we looked at previous literature to this, I think they were, sort of, drawing upon some domain knowledge here and that here we are looking at waves. How can we extract specific features from waves that doctors are also looking at? So there was a lot of feature engineering going on. And if you're familiar with wavelet transforms, they were the, sort of, um, they were the most common approach, uh, with a lot of, sort of, like, different mother wavelets etc., etc., pre-processing band pass filters. So everything you can imagine doing with signals was done. And then you fed it into your SVM and you called it a day. With deep learning, we can change things up a bit. So on the left, we have an ECG signal and on the right as just, uh, three heart rhythms, we're gonna call them A, B, and C. And we're gonna learn a mapping to go straight from the input to the output. And here's how we're gonna break it out. We're gonna say that every label, labels the same amount of the signal. So if we had four labels and the ECG would be split into these four, sort of, this rhythm is labeling this part. And then we're gonna use a deep neural network. So we built a 1D convolutional neural network, uh, which runs over the time dimension of the input. Because remember, we're getting one scalar, uh, over, over time. And then this architecture is 34 layers deep. Um, so I thought I'd talk a little bit about the architecture. Have you seen ResNets before? Okay. Uh, so should I go into this? I think you can go- they are going to learn about this next, next week. Okay. Cool. Here's my one-minute spiel of ResNet then, is that you're going deeper in terms of the number of layers that you're having in a network. You should be able to represent a larger set of functions. But when we look at the training error for these very deep networks, what we find is that it's worse than a smaller network. Now this is not the validation error, this is the training error. That means even with the ability to represent a more complex function, we aren't able to represent the training data. So the motivating idea of residual networks is to say, "Hey, let's add shortcuts within the network so as to minimize the distance from the error signal to each of my layers." Uh, this is just mapped to say the same thing. So further work on ResNet showed that, okay, we have the shortcut connection, how should we make information flow through it the best? And, uh, the finding was basically that anything you, you add to the shortcut, to the highway, think of these as stop signs or, um, or, or signals on a highway. And it's basically saying the fastest way on the highway is to not have anything but addition on it. [NOISE] And then there were a few advancements on top of that, um, like adding dropout and increasing the number of filters in the convolutional neural network, um, that we also add it to this network. Okay. So that's the convolutional neural network. Let's talk a little bit about data. So one thing that was cool about this project was that we got to partner up with a- uh, with a start-up that manufactures these hardware patches and we got data off of patients who were wearing these patches for up to two weeks. And this was from around 30,000 patients. Um, and this is 600 times bigger than the largest data set that, that was out there before. And for each of these ECG signals, what happens is that each of them is annotated by a clinical ECG expert who says, "Here's where rhythm A starts and here is where it ends, so let's mark the whole ECG that way." Obviously, very time intensive but a good data source. And then we had a test set as well. And here we used, um, here we used a committee of cardiologists. So they'd get together sit in a room and decide, okay, we disagree on the specific point, let's try to, let's try to discuss which one of us is right or what this rhythm actually is. So they arrive at a ground truth after discussion. And then we can, of course, test cardiologists as well. And the way we do this, is we have them do it individually. So there's not the same set that did the ground truth, there's a different set of cardiologists coming in one at a time, you tell me what's going on here and we're gonna test you. [NOISE] So when we compared the performance of our algorithm to cardiologists, uh, we found that we were able to surpass them, um, on the F1 metrics. This is precision and recall. And when we looked at where the mistakes were made, uh, we can see that sort of the, the biggest mistake was between distinguishing two rhythms, which look very, very similar, um, but actually don't have a difference in, um, in treatment. Here's another case where the model is not making a mistake which the experts are making. Um, and turns out this is a costing mistake. This is saying a benign heart rhythm, uh, or what experts thought was a benign heart rhythm, was actually a pretty serious one. Um, so that's, that's one beauty of automation, is that we're able to catch these, um, catch these misdiagnosis. [NOISE] Um, here are three hard blocks, uh, which are clinically relevant to catch on which the model outperforms the experts, and on atrial fibrillation which is probably the most common serious arrhythmia the same pulse. So one of the things that's neat about this application and a lot of applications in healthcare, is what automation, uh, with deep learning, machine learning enables, is for us to be able to continuously monitor patients. And this is not something we've been able to do before, so a lot of even science of understanding how patients, uh, risks factors, uh, what they are, or how they change hasn't been done before. And this is an exciting opportunity to be able to advance science as well. And the Apple Watch has recently, um, released a- their ECG monitoring. Um, and it'll be exciting to see what new things we can find out about the health of our hearts from, uh, from these inventions. Okay. So that was our first- yeah, a question. Uh, how big of a problem did you find, uh, data privacy should be in confidence among this dataset, which is awesome, by the way. Yeah. Uh, so I repeat the question, how, uh, how difficult was it it to, uh, to, uh, sort of, um, deal with data privacy and sort of keep patients', [NOISE] uh, pri- information private. So in, in this case, we do not- we had completely de-identified data, so it was just, um, someone's ECG signal without any extra information about their, uh, clinical records or anything like that. So it's, it's very, it's very de-identified. Um, sorry. I guess I had to ask that how did you get approval for that and were there problems in getting approval? Um, because, um, you know, there's a lot of teachers that have concerns. So did you have to like get it like signed off by some credible authority or, what were the obstacles of getting that to be there? Oh, sure. And I think we can, we can take this question offline as well. But one of the beauties of working at Stanford is that there's a lot of, uh, industry research collaborations and, uh, we have great infrastructure to be able to work with that. Uh, so which brings me on to my, uh, second case study. Sorry, yeah go for it. [inaudible] [NOISE] [inaudible] So how do you actually find that? That's a good question. So just to repeat the question, how did we define the gold standard when we have experts setting the gold standard? Uh, so here's how we did it. So one, one way to come up with a gold standard is to say, okay, if we looked at what a consensus would say, what would they say? And so we got three cardiologists in a room to set the gold standard, and then to compare the performance of experts. Uh, these were individuals who were separate from those groups of cardiologists, who sat in another room and said what they thought of the, um, of the ECG signals. So that way there's, there's some disagreement where the gold standard's set by the committee. [NOISE] Great. So here we looked at how we can detect pneumonia off of chest X-rays. Uh, so pneumonia is an infection that affects, uh, millions in the, uh, US. Uh, its big global burden is actually in, um, in kids. Uh, so that's where it's really useful to be able to, uh, detect that automatically and well. So to detect pneumonia, there's a chest X-ray exam. Uh, and chest X-rays are the most common, uh, imaging procedure, uh, with two billion chest X-rays done per year. And the way abnormalities are detected in chest X-rays is they present as areas of increased density. Uh, so where things should appear dark, they appear brighter or vice versa. And here's what characteristically pneumonia looks like, where it's like a fluffy cloud. Uh, but this is an oversimplification, of course, because, uh, pneumonia is when the alveoli fill up with pus, but the alveoli can fill up with a lot of other things as well, which lead to very different interpretations and diagnoses for the patients and treatment for the patient. So it's quite confusing which is why radiologists trained for years to be able to do this. Um, the setup is we'll take an input image of someone's chest X-ray and output the binary label 01 which indicates the presence or the absence of pneumonia. And here we use a 2D convolutional neural network, uh, which is pre- pre-trained on ImageNet. Okay. So we looked at shortcut connections earlier and, uh, DenseNets had this, um, idea to take shortcut connections to the extreme. It says what happens if we connect every layer to every other layer instead of just connecting sort of one- instead of having just one shortcut which is what ResNet had. And, uh, DenseNet beat the previous state of the art, um, and has generally lower error and fewer parameters on the ImageNet challenge. So that's what we used. Uh, for the dataset, when we started working on this project, uh, which was around October of last year, uh, there was this large dataset that was released by the NIH of 100,000 chest X-rays. And this was the largest public dataset at the time. And here each X-ray is annotated with up to 14 different pathologies. And the way this annotation works is there is an NLP system which reads a report, and then outputs for each of several pathologies, whether there is a mention, whether there is a negation like not pneumonia, for instance, um, and then annotates accordingly. And then for our test set, uh, we had four radiologists here at Stanford, independently annotate and tell us what they thought was going on in those X-rays. So one of the questions that comes up often in medical imaging, is we have, we have a model, we have several experts, but we don't really have a ground truth. And we don't have a ground truth for several reasons sometimes. One of them is just that it's difficult to tell whether someone had pneumonia or not without additional information like their clinical record, or even once you gave them antibio- antibiotics, did they get treated? Uh, so really one way to evaluate whether a, uh, model is better than a radiologists or as well- doing, as well as the radiologist, is by saying, do they agree with other experts similarly? So that's what we use here, that's the idea. We say, okay, let's have one of the radiologists be the, the, the prediction model we're evaluating. And let's set another radiologist to be ground truth. And now we're gonna compute the F1-score once, uh, change the ground truth, do it the second time, change it again, third, and then also use the model as the ground truth and do it again. And we can use a very symmetric evaluation scheme. But this time having the model be evaluated against each of the four experts. So we do that and then we get a score for both of them. Well, for all of the experts and for the model. And we showed in our work that we were able to, uh, do better than the average radiologist at this task. Um, two ways to extend this in the future is to be able to look at patient history as well, and look at, uh, lateral radiographs and be able to improve upon this diagnosis. Um, at the time at which we released our work, um, on all 14 pathologies, we were able to outperform the previous state of the art. Okay. So model interpretation. Model interpretation. Yes, there's a question. Going back to that slide that you had on future work. So one more slide, please, yeah. So, uh, if you have pneumonia and you present it to the doctor and you have like fever, you're coughing, your ribs hurt from coughing too much, can't sleep, all of those issues, that's not included in the model. So I think my question is that if you go to a dataset and you're trying to determine does this person have pneumonia or not? Like that's one thing but you don't- it's not that you just don't have that data, but you're not looking at other images, let's say does that person have cancer, or does that person have like, other infections of the body because they feel cold [inaudible]. And so all those are, um, images that you're not really looking at. So let's say in a tough situation, so the obvious situation isn't really giving much to do it, right? But in a tough situation that you get a patient that has a fever and he's coughing violently, you don't know if it's cancer or pneumonia or Lachlan disease. Then how do you- how do you get your algorithm to work in that condition? And also if you're not including all those other cases, then it's not just that, what's the use of it? But, like you know what I'm saying? Yeah. Well, so I'm trying to keep this technical since it's a technical class. What, is there a neural network architecture that you would use to be able to solve problem number one? Is it multi-task learning? Is it like- Sure, sure. Okay, so let me try to boil those sort of sets of questions down. Uh, so one is patients are coming in, we're not getting access to their, uh, clinical histories. So how are we able to make this determination at all? So one thing is that when we're training the algorithm, we're training the algorithm on, on, uh, pathologies extracted from radiology reports. And these radiology reports are written with understanding a full clinical history and understanding of, uh, sort of what the patient presented with in terms of symptoms as well. So we're training the model on, on these radiology reports, which had access to more information. And the second is that the utility of this is not as much in being able to compare a patient's x-rays day-to-day, as much as there is a new patient, uh, with a set of symptoms, and can we identify things from their chest X-rays? Which brings us to model interpretation. So if you were a end user from model, um, oh I- so when I was back in undergrad, um, and I was in the lab, we were working on autonomous cars. Um, and I thought about this a lot, how many of you've been in an autonomous car? How many of you would trust being in an autonomous car? [NOISE] [LAUGHTER] All right. Cool. Yeah, I thought about this as well. Would I trust being in an autonomous car? And I thought it would be pretty sweet if the algorithm that was, that was in the car would tell me whatever decision it was going to make in advance. I know that's not possible at high speeds, so that, you know, just in case I disagreed with a particular decision, uh, I could say, no abort. And, uh, and have the model sort of, you know, uh, remake its decision. And I think the same holds true in healthcare as well. Though one advantage that happens in healthcare is rather than having to make decisions within seconds like in the case of the autonomous car, there is often a larger time frame like minutes or hours that we have. And, and here it's, it's useful to be able to inform the clinician that's treating the patient to say, hey, here's what my model thought and why. So here's the technique we use for that class activation maps which you may cover in another lecture. Uh, so I'll just, I'll just leave it at saying that there are ways of being able to look at what parts of the image are most evident of a particular pathology, uh, to generate these, these heat-maps. Uh, so here's a heat map that's generated for, uh, pneumonia. So this X-ray has pneumonia and I can, ah, and, and, um, uh, and the algorithm in the red is able to highlight the areas where it thought was most problematic for that. Uh, here's one in which ab- it's able to do a collapsed right lung. Here's one in which able- it's able to find a small cancer. And here the goal is to be able to improve healthcare delivery, where, um, in the developed world, one of the things that's useful for is to be able to prioritize the workflow, make sure the radiologists are getting to the patients most in need of care before ones whose X-rays look more normal. Uh, but the second part which I'm, ah, quite excited about is to increase the access of medical imaging expertise globally. Uh, where right now, the, the World Health Organization estimates that about two-thirds of the world's population does not have access to diagnostics. Um, and so we thought, "Hey, wouldn't it be cool if we just made an app that was able to, uh, allow users to upload images of their, um, of X-rays and be able to give its diagnosis?" Ah, so this is still in the works, so I'll show you what we've got running locally. And so here, I'm presented with a screen that asks me to upload an X-ray. And so I have, I have several X-rays here, um, and I'm gonna pick the one that says, ah, cardiomegaly. So cardiomegaly refers to the enlargement of the heart. [NOISE] So I uploaded it, now it's running, the model is running in the back end. And within a couple of seconds, it's outputted its diagnoses on the right. So you'll see the 14 pathologies that the model is trained on being listed, and then next to them a bar. Uh, and at the top of this list is cardiomegaly, which is, um, what this patient has the, the heart is sort of extending out. And if I hover on cardiomegaly, I can see that the probability, ah, is displayed on there. And now we talked about interpretation. How do I believe that this model is actually looking at the heart rather than looking at something else? And so if I click on it, um, I get the class activation map for this, which shows that indeed it is focused on the heart, uh, to be able to, um, and, and is looking at the right thing. So I guess you can say the algorithms- heart's in the right place. [LAUGHTER] Cool. Uh, but I thought- so this is an image that I got from the, the dataset that we were using NIH. But it's pretty cool if an algorithm is able to generalize to populations beyond. And so I thought what we'd do is we could just look up, um, [NOISE] look up an image of cardiomegaly, uh, and download it and just see if our model's able to- this one looks pretty large, so is this. I don't want an annotated one. All right. That's good. So we can do that, save it, desktop. And now we can upload it here. And it's already re-done its thing and on the top is cardiomegaly once again. [NOISE] So it's able to generalize to- and there's the highlight. So it's able to generalize to populations beyond just the ones it was trained on. So I'm very excited by that. And what I got even more excited by is, uh, we're thinking of deploying this out in, um, out in different parts of the world, and when we got an image, uh, that showed how X-rays are read in, uh, this hospital that we're working with in Africa, uh, this is what we saw. And so the idea that one could snap a picture and upload it, seems- and get a diagnosis seems very powerful. Um, so the third case study I wanna take you through is, um, being able to look at MR. So we've talked about 1D, a 1D setup where we had an ECG signal. We've talked about a 2D set-up with an X-ray. How many of you thinking of working on a 3D problem for your project? Okay. A few. Well, that's good. Cool. Um, so here we looked at knee MR. So MR's of the knee is the standard of care to evaluate knee disorders, and more MR examinations are performed on the knee than any other part of the body. Um, and the question that we sought out to answer was, can we identify knee abnormalities? Um, two of the most common ones include an ACL tear and a meniscal tear at the level of radiologists. Now with the 3D problem, one thing that we have that we don't have in a 2D setting is the ability the he loo- to look at the same, same thing from different angles. And so when radiologists do this diagnosis, they look at three views; the sagittal, the coronal, and the axial, which are, [NOISE] which are three ways of, uh, looking through, uh, the 3D structure of the knee. And in an MR you get different types of series, ah, based on the magnetic fields, and so here are three different, um, series that are, that are used. And what we're gonna do is output for a particular knee MR examination, the probability that it's abnormal, the probability of an ACL tear, and the probability of a meniscal tear. The important thing to recognize here is this not a multiclass problem, and that I could have both types of tears and it's a multi-label problem. [NOISE] So we're gonna train a, uh, convolutional neural network for every view-pathology pair. So that's nine convolutional networks, and then combine them together, uh, using a logistic regression. So here's what each convolutional neural network looks like. I have a bunch of slices within a view. I'm gonna pass each of them to a feature extractor, I'm gonna get an output probability. So we had 1,400 knee MR exams from, uh, the Stanford Medical Center, and, uh, we tested on 120 of them, where the majority vote of three, uh, subspecialty radiologists established the, the ground truth. And we found that we did pretty well on, on the three tasks, and had the model be able to pick up the different abnormalities pretty well. And one can extend these, these methods of interpretive- interpretability, uh, to, uh, to 3D, 3D inputs as well. So that's what we did here. Okay. So, uh, I, I saw this, I saw this cartoon a few, a few weeks ago and I thought it was, it was pretty funny, uh, which is a lot of machine learning engineers think, uh, that they don't need to externally validate, which is to find out how my model works on, uh, works on data that's not my- where my original data set came from, so there's, uh, there's a difference in, in distributions. Uh, but it's really quite, uh, exciting when a model does generalize to, to datasets that it's not seen before. And so we got this dataset that's, that's public from a hospital in Croatia. And here's how it was different. So it was a different, it was a different kind of series, with different magnetic properties. Uh, it's a different scanner and it was a different institution in a different country. And we asked, "Okay, what happens when we run this model off-the-shelf that was trained on Stanford data but tested on that kind of data?" And we found that it did relatively well without any training at all. [NOISE] But then when we trained on it, we found that we were able to outperform the previous lead best-reported result on the dataset. So there's still some work to be done in being able to generalize, um, sort of my network here that was trained on my data to be able to work on datasets from different institutions, different countries as well, but we're making some steps along that way, it remains a very open problem for taking. [NOISE] And it, and it is a very [inaudible] in hospital that has some sort of the [inaudible] Yeah. So we did the best we could in terms of processing. So we had- so one of the preprocessing steps that's important is being able to, uh, get the mean of the, of the input data to be as close to the mean of the input data that you trained on. Uh, so that was one preprocessing step we tried, but we were trying to minimize that to say out of the box, how would this work? If we had never seen this data before, how would it work on that population? So one big topic in across, uh, a lot of applied fields is asking the question, okay, we're talking about models working automatically autonomously, how would these models work in- when working together with experts in different fields? And here we asked that questions about radiologists and about imaging models. Would it be possible to be able to boost the performance if the model and the radiologists work together? And so that's really the set-up. A radiologist with model, is that better than the radiologists by themselves? And here's how we set it up. We set- let's have experts read the same case twice separated by a certain set of weeks, um, and then see how they would perform on the same set of cases. And what we found, that we were able to increase the performance generally with a signifant- significant increase in specificity for ACL tears. That means if someone- if a patient came in, uh, without a, uh, without an ACL tear, I'd be able to, uh, find it better. So in the future- yes, question? Would you have any bias, the opinion of the radiologist, or is that the intended thing that you wanna kind of bias in the opinion for that it actually looks at the patient's health? Yeah. So that's a good question, and I, and I think how- so, uh, sort of automation bias captures a lot of this, and that once we have sort of models working with, um, experts together, can we expect that the experts will sort of take it less seriously because that's, that's a big concern, and start relying on what the model says and says, "I won't even look at this exam. I'm just gonna trust what the model says blindly." Um, that's absolutely possible in a very open area of research. Some of the ways that people have tried to address it is to say, "You know what I'm gonna do from time to time? I'm gonna pass in an exam to the radiologist for which I'm gonna to flip the answer and I'll know the right one. And if they get that wrong, I'll alert them, that you're relying too much on the model, uh, stop." Uh, but there are a lot of more sophisticated ways to go about addressing automated bias. And as far as I know, it's a very open field of research, especially as we're getting into deep learning assistance. And one utility of this is to say basically that the set of patients don't need a follow-up, let's not send them for unnecessary surgery. Great. So I shared, uh, three case studies from the lab. The final thing I wanna do is to talk a little bit about how you can get involved if you're interested in applications of AI to healthcare. Uh, so the first is, uh, the ability for you to just get your hands dirty with, uh, datasets and, and be able to try out your own model. So we have, uh, from our lab, released, uh, the MURA dataset, which is a large dataset of, uh, uh, bone X-rays, and the task is to be able to tell if it's, um, if the X-rays are normal or not, and they come from different, um, parts of the- of the upper body, um, and that's- that's what the dataset X-rays look like. And this is a pretty interesting setup because you have more than one view, uh, so more than one angle for the same body part, for the same study, for the same patient, and the goal is to be able to combine these well, uh, into convolutional neural network and, and be able to output the probability of an abnormality. And one of the interesting things here for transfer learning as well is, do you wanna train the models differently per body part or do you wanna train them, uh, train the same model for body parts or combine certain models? Uh, so a lot of design decisions there. And this is what trai- some trained models look like. This is a model baseline that we released that's able to identify a fracture here and a piece of hardware on the right. Um, and you can download the dataset off our website. So if you Google, uh, MURA dataset or go on our website stanfordmlgroup.github.io, you should be able to find it. Um, the second way to get involved is through the AI for Healthcare Bootcamp, which is a two-quarter long program that our lab runs, um, which provides, um, students coming out of, uh, classes like 230, an opportunity to get, uh, involved in research. And here's, uh, students receive training from, uh, PhD students in the lab and medical school faculty, um, to work on structured products over two quarters. Um, and if you have a background in sort of, uh, AI, which you do, uh, then you're encouraged to apply. And we're working on a wide set of problems across radiology, uh, EHR, public health, and pathology right now. Um, this is what the lab looks like. We have a lot of fun. Um, and the applications for the bootcamp starting in the winter are now open. So the early applications deadline is November 23rd, and you can go on this link and, um, and, and apply. Uh, so that's my time. Thank you so much for having me and thanks for having me, Kian. [APPLAUSE] [NOISE] Let me set up the microphone. [NOISE]. Do you wanna take one or two questions? Yes, I'll take a couple of questions. All right. Let me ask- I'll ask a question about the privacy concerns, uh, and further other ethics concerns. What about compensation for the medical experts that you're potentially putting out of business, uh, with the free tool like the one that you're, you're developing or, you know, in just in general? Because their, their knowledge is being used to train these models, it's not free. Uh-huh. Yeah. So the question was we're having these, uh, automated AI models trained with the knowledge of medical experts, um, and what are ways in which we're thinking of compensating these medical experts, uh, right now or in the future when we have, uh, possibly automated models? Um, I think a lot of people are thinking about these problems and working on them, uh, right now. There are a variety of approaches, uh, that people are thinking about in terms of economic incentives and there's a lot of, uh, fear about sort of will AI actually work with or augment experts in whatever field they're working on. I don't have a great, uh, silver bullet for this, uh, but I know there's, there's a lot of work going on in there. [NOISE] I just wanted to know, um, when you're looking through, uh, MRIs, we should have- looking at four or five category of issues like we used there, one of them is the most likely. Uh, it's possible that a human looking at it could point out something that was not being looked at by the AI model at that time? Yeah. So how do you address it? Yeah. That's a great question. So the- just to repeat the question, it's, uh, we ha- we're looking at MR exams and we're saying for these three pathologies, we're able to output the probabilities, what happens if there's another pathology that we haven't looked at? Uh, so I have a couple of answers for that. The first is that one of the- one of the categories here was simply to tell whether it was normal or abnormal. So the idea here is that the abnormality class will capture a lot of different pathologies there, at least the ones seen at Stanford. Uh, but it's often the case that we're building for one particular pathology, and then there's obviously a, um, a burden on the, the model and the model developers to be able to convey, "Hey look, our algorithm model only does this and you really need to watch out for everything else that the model doesn't cover." Maybe that's the- unless there's one more question? No. All right. That's the last question we'll take then. Thank you once again. Thanks, man. [APPLAUSE] So now you've got, you've got the, the perspective. Is the microphone working? Yeah. Now you've got the perspective of an AI researcher working in healthcare. Now you are going to be the AI researcher, researcher working in healthcare. We're going to go over a case study, and that is targeted at skin disease. So, you know, uh, in order to detect skin disease, sometimes you take pictures, microscopic pictures of cells on your skin, and then analyze those pictures. So that's what we're going to talk about today. So let me talk about the problem statement. You're a deep learning engineer and you've been chosen by a group of healthcare practitioners, uh, to determine which parts of a microscopic image corresponds to a cell. Okay. So here is how, how it looks like. Um, on the, the, the black and white, it's not a black and white image, it's a color image but looks black and white. The input image is the, the one that is closer to me, um, and the yellow, um, one is the ground truth that has been labeled by a doctor, let's say. So what you're trying to do is to segment the cells on this image, and we didn't talk about segmentation yet or a little bit. Segmentation is, uh, is about producing, uh, value- a class for each of the pixels in our image. So in this case, each pixel would correspond to either no cell or cell, zero or one. And once we output a matrix of zero's and one's telling us which pixels corresponded to a cell, we should get hopefully a mask like the yellow mask that I overlapped with the input image. Does that make sense? Yeah. Isn't there a third category that's the boundary, because in the colored image, the yellow one you don't have the boundaries for the cell. Yeah, we'll talk about the boundary later. But right now, assume it's a binary segmentation, so zero and one, no cell and cell. Okay? So uh, it's going to be very interactive, uh, and I think we're going to use Menti for several question and group you guys into groups of three. So here are other examples of images that were segmented with a mask. Now, doctors have collected 100,000 images coming from microscopes, but the images come from three different microscopes. There is a type A, type B, and type C microscope, and the data is splitted between these three as 50 percent for type A, 25 percent for type B, 25 percent for type C. Um, the first question I'll have for you is, given that the doctors want to be able to use your algorithm on images from the microscope of type C, this microscope is the latest one, it's the one that is going to be used widely in the field, and they want your, your network to work on this one. How would you split your dataset into train, dev, and test set? That's the question. And please group in teams of two or three and discuss it for a minute, uh, on how you would split this dataset. [NOISE] [OVERLAPPING] You can start going on Menti and, and write down your answers as well. [NOISE]. [OVERLAPPING] Okay. So take, uh, 30 seconds to input your, your insights on, on Menti. You can do one per team, and we'll start going over some of the answers here. Okay. Dev test least split C train on A plus B, 20k in train, 2.5 in dev and test. Training 80 all A, all B, 5KC dev, 10KC test 10KC. 95-5 where test and dev is from population we care about. I think these are good answers. I think there's no perfect answer to that, but two things to take into consideration. You have a lot of data so you probably wanna split it into 95-5 closer to that than to 60-20-20. And most importantly, you want to have C images in the des- dev and test set to have the same distribution among these two. That's what you've seen in the third course, uh, and we would prefer to have actually C images in the train set. You wanted your algorithm to have C images. So I would say a very good answer is, is this one. 95-5 where the 5-5 are exclusively from C, and you also have C images in the 90 percent of training images. Any other insights on that? Whatever is- yeah. [NOISE] How do we type that, like microscopes A and B dataset doesn't have some, like, hidden, you know, feature that will mess up the training. Yeah. So, there is much more thing we didn't talk about here. One is how do we know what's the distribution of microscope A images and microscopy B images versus microscope C. Do they look like each other? If they do, all good. If they don't, how can we, how can we make sure the model doesn't get bad hints from these two distributions. Uh, another thing is data, data augmentation. We could augment this dataset as well, and try to get as much as C-distribution images as possible. We're going to talk about that. Okay. Split has to roughly be 95-5 not 60-20-20, distribution of dev and test sets has to be the same, containing images from C, and there also- should also be C image in the training set. Now, talking about data augmentation. Uh, do you think you can augment these data? And if yes, give only three distinct method you would use. If no, explic- explicate- er, explain why you cannot. You wanna take 30 seconds to talk about it with your neighbors? Yeah. [OVERLAPPING] Okay. [NOISE]. Okay. Guys, let's go over some of the answers. So rotation, zoom, blur. I think looking at the images that we have from the cells, this might work very well. Uh, rotation, zoom, blur, translation, uh, combination of those, stretch, symmetry, like, probably a lot of those work. One follow-up question that I'll have is, can you, can someone give an example of, uh, a task where data augmentation might hurt the model rather than helping it. [NOISE] Yeah. If I wanna overfit on the test set. If you want to overfit on the test set. Can you be more precise? [NOISE]. Like, and you don't wanna generalize too much. Oh, you don't want your model to generalize too much? Okay. [NOISE] Yeah, that, there, there are some cases where you don't want the mo- model to generalize too much, especially, you know, doing encoding, but any, any other ideas? You're doing like face detection, you wouldn't want the face to be, like, upside down or, like, either side. I see. So if you do face detection, you probably don't want the face to the upside down, although we never know depending on the use. [LAUGHTER] but, uh, it's, it's not gonna help much if the camera is always like that and it's filming humans that are not upside down. Any, but I don't think it's gonna hurt the model. It's probably going to not help the model, I guess. Yeah. Anything [NOISE] if you like, stretch the image then that will be inaccurate. Yeah, good point. So, there are, there are algorithms like maybe, you know, FlowNet. It's an algorithm that- that's used for, uh, on videos to detect the speed of a car, let's say. Uh, if you stretch the images, you, probably you cannot detect the speed of the car anymore. Any other examples? [NOISE] Yeah. Character recognition. Character recognition I think is a good example. So, let's say, you're, you're trying to detect [NOISE] what this is and you do symmetry flip and you get that, you know, like you- you're, you're labeling as B everything that was D and as D everything that was B. For nine and six it's the same story. So these data augmentations are actually hurting the model because you don't relabel when you data, when you augment your data, all right? Okay. [NOISE] Okay. So yeah, many augmentation methods are possible: cropping, adding random noise, um, changing contrasts. I think data augmentation is super important. I remember a story of, um, of a company that was working on, uh, self-driving cars and, and also, uh, virtual assistants in cars, you know what, like, this type of interaction you have with someone in your car, a virtual assistant, and they noticed that the speech recognition system [NOISE] was actually not working well when the car was going backwards. Like, no idea why, like, why. It just doesn't seem related to the speech recognition system of the car. And they test it out and they, they, they looked and they figured out that people, uh, were putting their hands in the passenger seat looking back and talking to the virtual assistant. And because the microphone was in the front, the voice was very different when you were talking to, to, to the back of the car rather than the front of the car. And so they used data augmentation in order to augment their current data. They didn't have data on that type of, of people talking to the back of the car. So by augmenting smartly, you can change the voices so that they look like they were used by someone who was talking to the back of the car and that solved the problem. Okay. Um, small question. Uh, we can do it quickly. What is the mathematical relation between nx and ny? So remember we have an RGB image [NOISE] and we can, we can flatten it into a vector of size nx, and the output is a mask of size ny. What's the relationship between nx and ny? Someone wants to go for it? [NOISE] They're equal. They're equal. Who thinks they're equal? Who thinks they are not equal and why? [NOISE] Based on why, because you have RGB on this side and you just have one color [inaudible]. Exactly. Ny would be 3nx, uh, sorry, nx would be 3ny because you have RGB images and for each RGB pixel, you would have one output zero or one. Okay. That was a question on one of the midterms. It was a complicated question. Uh, what's the last activation of your network? Sigmoid. You want probably an output zero and one. Uh, and if you had several classes, so later on we will see we can also segment per disease, then you would have a softmax. Uh, what loss function should we use? [NOISE] I'm gonna give it to you to go quickly because we don't have too much time. You're going to use, uh, [NOISE] a binary cross-entropy loss over all the output and the entries of, of the output of your network. Does that makes sense? So always think, that thinking through the loss function is interesting. [NOISE] Okay. So you, you have a first try and, and you've coded your own neural network that you've, uh, that you've named model M1, M1 and you've trained it for 1000 epochs. It doesn't end up performing well. So it looks like that. You give it the input image through the model and get an output that is expected to be the following one but it's not. So one of your friends tells you about transfer learning and they, they, they tell you about another labeled data set of one million microscope images, that have been labeled for skin disease classification, which are very similar to those you wanna work with from microscope C. So a model M2 has already been trained by another research lab on these new data sets on a 10-class disease classification. And so here is an example of input/output of the model. You have an input image that probably looks very similar to the ones you're working on. The network has a certain number of layers and a softmax classification at the end that gives you the probability distribution over the disease that seems to correspond to this image. So they're not doing segmentation anymore, right? They're doing classification. Okay. So the question here is going to be, you want to perform transfer learning from M2 to M1, what are the hyper parameters that you will have to tune? It's more difficult than it looks like. So think about it, discuss with your neighbors for a minute. Try to figure out what are the hyper parameters involved in this transfer learning process. [NOISE] Okay, take 15 more seconds to wrap it up. [NOISE] Okay. Let's see what you guys have. Learning rates. It is a hyperparameter. I don't know if it's specific to the, to the transfer learning, weights of the last layers. So I don't think that's, ah, a hyperparameter. Weights are parameters. New cost function for additional output layers. I think that's a hyper- the choice of the loss you might count it as a parameter. I don't think it's specifically related to transfer learning. You will have to train with the loss you've used on your model M1. Number of new layers, yeah, weights of the new, another hyperparameter. Okay. Last one or two in the layers of M2. So do we train, what do we fine tune? There's a lot about layers actually. Size of added layers, not sure. [LAUGHTER] Okay, let- let's, let's go over it together because it seems that there's a lot of different answers here. Um, [NOISE] I'm trying to write it down here. So let's say we have, we have the model M2. [NOISE] Is it big enough for the back? We have the model M2, and so we give it an input image. [NOISE] Okay input. [NOISE] And the model M2 gives us a probability distribution, softmax. So we have a softmax here. [NOISE] Yo- you will agree that we probably don't need the softmax layer. We don't want it, we want to do some segmentation. So one thing we have to choose is, how much of this pre-trained network? Because it's a pre-trained network. How much of this network do we keep? Let's say, we keep these layers. Because they probably know the inherent salient features of the dataset like the edges of th- the cells that we're very interested in. So we take it. So we have it here. [NOISE] And you agree that here we have a first hyper-parameter. That is L. The number of layers from M2 that we take. Now, what other hyperparameters do we have to choose? This is L. We probably have to add a certain number of layers here, in order to produce our segmentation. So there's probably another hyperparameter. [NOISE] Which is L_0. How many layers do I stack on top of this one? And remember, these layers are pre-trained. [NOISE] But these ones are randomly initialized. [NOISE] That makes sense. So two hyperparameters. Anyone sees a third one? [NOISE] The third one comes when you decide to train this new network. You have the input image. [NOISE] Give it to the network. Get the output segmentation mask. Segmentation mask let's say seg mask. [NOISE] And what you have to decide is how many of these layers will I freeze? How many of the pre-train layers I freeze? Probably, if, if you have a small dataset, you prefer keeping the features that are here freezing them, and focusing on retraining the last few layers. So there is another hyperparameter which is, how much of this will I freeze L_f. What does it mean to freeze? It means during training, I don't train these layers. I assume that they've been seeing a lot of data already. They understand very well the edges and less complex features of the data. I'm going to use my new- my small dataset to train the last layers. So three hyperparameters. [NOISE] L, L_0, and L_f. Does that makes sense? [NOISE] Okay. So this is for transfer learning. So it looks more complicated than the question- the question was more complicated than it looked like. Okay. Let's move, where am I? Okay. Let's go over another question. Okay. So this, we did it. Now it's interesting because, ah, here we have an input image, and in the middle, we have the output that the doctor would like. But on the right, you have the output of your algorithm. So you see that there is a difference, between what they want and what we're producing. And it goes back to someone mentioned it earlier. There is a problem here. How do you think you can correct the model, and/or the dataset to satisfy the doctor's requests? So the issue with, with this image is that, they want to be able to separate the cells among them, and they cannot do it based on your algorithm, its still a little hard. There is, there is something to add. So can someone come up with the answer. Or do you want to explain actually you mentioned one of the answers so that we, we can finish this slide, yeah? Ah, you wanna add boundaries because now it looks like you could have like three cells on the bottom left blurring in together. And so if you answer adding boundaries, it makes the cells more well-defined. Good answer. So one way is when you label your datasets, originally you labeled with zeros and ones, for every pixel. Now, instead you will label with three classes,  zero, one or boundary. Like let's say zero, one, two, for boundary or even the best method I would say is that for each pixel, for each input pixel, the output will be [NOISE] the corresponding- okay, this one is not good. [NOISE] The corresponding label, like this is a cell picture. [NOISE] P of cell, P of boundary [NOISE] and P of no cell. What you will do is that instead of having a sigmoid activation you will use a softmax activation. Okay, and the softmax will be for a pixel. Um, one other way to do that, if it still doesn't work, doesn't work even if you labeled the boundaries. What is another way to do that? You relabel your datasets by taking into account the boundaries. The model still doesn't perform well. I think it's all about the weighting of the loss function. It's likely that the number of pixels that are boundaries are going to be fewer than the number of pixels that are cells or no cells. So the network will be biased towards predicting cell or no cell. Instead, what you can do is, when you compute your loss function, your loss function should have three terms. One, binary cross-entropy let's say for no cell, one for cell, [NOISE] and one for boundary. [NOISE] Okay, and this is going to be summed over, i equals 1 to n_i. The whole output pixel values. What you can do is to attribute a coefficient to each of those; alpha, beta or one. And by tweaking these coefficients, if you put a very high- a very low number here and there, it means you're telling your model to focus on the boundary. You telling th- the model that if you miss the boundary, it's a huge penalty. We want you to train by figuring out all the boundaries. That's another trick that you could use. One question on that. Yeah. When you say you're relabeling your dataset you- so like you do that manually or is that [inaudible] Good question. What do I mean by rela- relabeling your dataset? This, this- last Friday's section has been be labeling bounding boxes, you know, for the YOLO algorithm. So the same tools are available for segmentation where you have an image, and you would draw the different lines. Ah, in practice, if the more- if the tool that you were using, the line used will just count as a cell, everything including the line with, with- everything inside what you draw. Plus the boundary we count as cell and the rest has no cell, it's just a line of code to make it different. The line you drew will count as boundary. Everything inside will count as cell, and everything outside will count as no cell. So it's the way you use your labeling tool. That's all. So do we make alpha and beta static or do we make alpha and beta variable parameters like we fit in other, you know, in-between. I think it's not learnable parameters. It's more hyperparameters to tune. [NOISE] So the same way you tune lambda for your regularization, you would tune alpha and beta. So when you make a distinction like if that becomes an attention mechanism, how do you combine those two terms? So this is not an attention mechanism because it's just a training trick. I would say. You cannot know, ah, how much attention we tell you for each image, how much the model is looking at this part versus that part. This is not going to tell you that, it's just a training trick. [NOISE] What's the advantage to doing it this way as opposed to like object detection like detecting each cell? So the question is what's the advantage of doing segmentation rather than detection? Yeah. Yeah, so detection means you want to output a bounding box. If you output the bounding box, what you could do is output the bounding box, crop it out, and then analyze the cell and try to find the contour of the cell. But if you want to separate the cells, if you want to be very precise, segmentation is going to work well. If you want to be very fast, bounding boxes would work better, I think that's the general way. Segmentation is not working as fast as the YOLO algorithm works for object detection. Yeah. I would say that. But it's more- much more precise. Okay. So modify the datasets in order to label the boundaries, on top of that you can change the loss function to give more weight to boundaries or penalize false positives. Okay. Ah, we have one more slide I think. Ah, so let's go over it. So now th- the doctors, they give you a new dataset that contain images similar to the previous ones. Uh, the difference is that each linked image now is labeled with zero and one. Zero meaning, there are no cancer cells on that image, and one means there is at least a cancer cell on this image. So we're not doing segmentation anymore. It's a binary classification; image, cancer or no cancer. Okay. So you easily build the state-of-the-art model because you're you're a very strong person in classification, uh, and you achieve 99 percent accuracy. The doctors are super happy, and they ask you to explain the network's prediction. So given an image classified as one, how can you figure out based on which cell the model predicts one? So Pranav talked a little bit about that. There are other methods that you should be able to figure out right now. Even if you don't know class activation maps. [NOISE] So to sum it up. [NOISE] We have an image, [NOISE] input image, [NOISE] put it in your new network that is a binary classifier. [NOISE] And the network says one. You wanna figure out why the network says one, based on which pixels, what do you do? Visualize the weights. [NOISE] Visualize the weights. Uh, what do you visualize in the weights? The edges. So I think visualizing the weights, uh, is not related to the input. The weights are not gonna change based on the input. So here you wanna know why this input led to one. So it's not about the weights. [NOISE] Do you mark the gradients on each pixel of the input and see which one [inaudible]. Good idea. So you know, after you get the one here, this is Y hat, basically. It's not exactly one, let's say it's 0.7 probability. What you gotta remember is that this number derivative of Y hat with respect to X, is what? It's a matrix of shapes same as X, you know, it's a matrix. And each entry of the matrix is telling you how much moving this pixel influences Y hat. Do you agree? So the top left number here is telling you how much X1 is impacting Y hat. Is it or not? Maybe it's not. If you have a cat detector and the cat is here, you can change this, this pixel is never gonna change anything. So the value here is going to be very small, closer to zero. Let's assume the cancer cell is here, you will see high number in this part of the matrix because these, these pi- these are the pixel that if we move them, it will change Y hat. Does it make sense? So quick way to interpret your network. It doesn't- it's not too, too good, like, you're not gonna have tremendous results. But you should see these pixels have higher derivative values than the others. Okay. That's one way. And then we will see in two weeks, uh, how to interpret neural networks, visualizing the weights included and all the other methods. Okay. So gradient with respect to- your model detects cancer cells from the test set images with 99 percent accuracy, while a doctor would on average perform 97 percent on the same task. Is this possible or not? Who thinks it's possible to have a network that achieves more accuracy on the test set than the doctor? Okay. Can someone, can someone say why? Do you have an explanation? You can look at complex things that possibly you didn't get from your training. Okay, the network probably looks at complex things that doctor didn't see, they didn't see. That's what you're saying. Possibly. I think there is a more rigorous explanation. Human error is an approximation for base error but we don't have to know what it is so theoretically we can get better. Yeah. So here we're talking about base error, human level performance and all that stuff. That's when you should see it. So one thing is that there are many concepts that you will see in course three that are actually implemented in the industry. But it's, it's not because you know them that you're going to understand that it's time to use them and that's what we want you to get to. Like, now, when I ask you this question, you have to talk- think about base error, human level accuracy and so on. So the question that you should ask here is; what was the data set labeled? Wh- what were the labels coming from? If the data set was labeled by individual doctors, I think that looks weird. Like, if it was labeled by individual doctors, I think it's very weird that the model performs better on the test set than what doctors have labeled, because- simply because the labels are wrong, three percent of the time on average the labels are wrong. So you're, you're te- teaching wrong things to your model three percent of the time. So it's surprising that it gets better, could happen, but surprising. But if every single image of the data that has been labeled by a group of doctors as Pranav talked about it, then, the average accuracy of this group of doctor is probably higher than one doctor. Maybe it's 99 percent, in which case it makes sense that the model can beat one doctor. Does it make sense? So you have base error, you're trying to approximate with, with, like, the best error you can achieve. So regrouping- grouping a cluster of doctors, probably better than one doctor. This is your human level performance and then you should be able to beat one doctor. Okay. [NOISE] So you want to build a pipeline that goes from image taken by the front of your car to steering direction for autonomous driving. What you could do, is that you could send this image to a car detector, that detects all the cars, a pedestrian detector, that detects all the pedestrians. And then you can give it to a path planner, let's say, that plans the path and outputs the steering direction, let's say. So it's not end-to-end. End-to-end would be, I have an input image and I give it an ou- I want this output. So a few other disadvantages of this is, is, uh, something can go wrong anywhere in the model, you know. How do you know which part of the model went wrong? Can you tell me which part? I give you [NOISE] an image, the steering direction is wrong. Why? Yes. Look at the different components and try to isolate it. Good idea, looking at the different components. So what you can do is look what happens here and there. Loo- look what's happening here and there. You think, based on this image, the car detector worked well or not? You can check it out. Do you think the pedestrian detector works well or not? You can check it out. If there is something wrong here, it's probably one of these two items. It doesn't mean this one is good, it just means that these two items are wrong. How do you check that this one is good? You can label ground-truth images and give them here as input to this one, and figure out if it's figuring out the steering direction or not. If it is, it seems that the path planner is working well. If it is not, it means there's a problem here. Now, what if every single component seemed to work properly, like let's say these two work properly, but there is still a problem. It might be because what you selected as a human was wrong. The path planner cannot detect, cannot get the steering direction correct based on only the pedestrians and the car detecti- and, and the cars, probably need the stop signs and stuff like that as well, you know. And so because you made hand engineering choices here, your model might go wrong. That's another thing. And another advantage of, of, uh, of this type of pipeline is that data is probably easier to find out at end for every algorithm, rather than the, for the whole end-to-end pipeline. If you want to collect data for the entire pipeline, you would need to take a car, put a camera in the front, like, like, build a, kind of, steering wheel angle detector that will measure your steering wheel at every step, while you're driving. So you need to drive everywhere, basically, with a car that has this feature. It's pretty hard. You need a lot of data, a lot of roads. While this one, you can collect data of images anywhere and label it, uh, and label the pedestrians on it. You can detect cars by the same process, okay? So these choices also depend on what data can you access easily or what data is harder to acquire. Any questions on that? You're going to learn about convolutional neural networks now. We're going to get fun with a lot of imaging. You have a quiz and two programming assignments for the first module. Second module same. Midterm, next Friday, not this one. Everything up to C4M2 will be included in the midterm. So up to the videos you're watching this week. Includes TA sections and a next one- and every in-class lecture including next Wednesday. And this Friday you have a TA section. Any questions on that? Okay. See you next week, guys. 

All right. Hey, everyone. Welcome back. Um, this is, people can hear me okay? All right. So if as usual, you can take a second to enter your, uh, SU ID so we know who's here. Um, so today's lecture will be a Choose Your Own Adventure lecture, um. So I think, you know, by now you've learned a lot about the, um, technical aspects of building learning algorithms and then, in the third course, uh, in the third set of modules, you saw some of the principles for debugging Learning Algorithms and how best to use these tools, um, in order to be efficient in how you build a machine learning application. What I want to do today is a step through with you, a moderately complicated machine learning application, and, um, throughout all of today's lecture, I'm gonna, you know, step you through a scenario and then ask you to kind of choose your own adventure. Because if you have to work on this project, what are you gonna do, right? Um, and to give you more of that practice in the next, um, what, hour and a bit that we have, uh, on thinking through machine learning strategy, um. And you know I've, I've seen in so many projects, uh, there are, there are sometimes things that a less strategically sophisticated team will take a year to do, but if you're actually very strategic and very sophisticated in deciding what you will do next, right, how to drive a project forward, I've seen many times that what a different team will take to do, maybe you could do it in a month or two, right. And, you know, if you're trying to, um, I don't know, write a research paper, or build a business, or build a product, the, the, the ability to drive a machine learning project quickly gives you a huge advantage and just, you know, you're making much more efficient use of your life as well, right. Um, so, in, for today I'd like to, er, er, I'm gonna pose a scenario, pose a machine learning application and say, all right, I mean you are the CEO of this project, what are you going to do next? So, but I'd like to have today's meeting be quite interactive as well. So, can I get people to sit in groups of two, and ideally three or so, maybe plus minus one and try to sit next to someone that you don't work with all the time. Er, so, so, if you're sitting, sitting next to your best friend, I'm glad your best friend is in the class with you, but go sit with someone else because I think, um- I've done this multiple times and the discussion's actually richer if you talk to someone that you don't know super well. So actually take a second, introduce yourself and, and, and, and just greet your neighbor, I guess. So, the example I wanna go through today is actually a continuation of the example I described briefly, uh, uh, in the last lecture I taught, uh, Building a Speech Recognition System, right. So, remember I briefly motivated this trigger word, wake word or trigger word detection system last time where, you know, uh, right. I, I, I, I, I actually have both an Amazon Echo and a Google Home, uh, but you know, it's it's a lot of work to configure these things to turn on and off the light bulbs, um, and so if you can build a chip, uh, to sell to say a lamp maker, to recognize uh, phrases like, you know- let's say we call the lamp Robert, right, um. Then you can recognize phases like Robert, turn on, right? Robert turn off, and you have a little switch to give this thing different names, you can call it Robert, or, or Rainier or Alice or something. So you can also have Rainier, turn on, Rainier, turn off. Just give, give your a lamp a name and just say Hey, Robert, turn on, right? So, rather than detecting different names and turn on and turn off, I'm just gonna focus on- just for the technical discussion, I'm just going to focus on the phrase Robert turn on, er, but it's kind of the same problem we need to solve like, four times to give it two names or to turn on and turn off. So, I'm gonna abbreviate Robert turn on as RTO, right, so if you wanna call your name Robert and, um, tell your lamp to turn on, um. I think I was inspired by one Isaac Asimov, wrote these um, robotic novel series and, and all his robots' name started with R. So, maybe R is Robot turn on, um. And so, uh, let's see. So, let's say that, um, you are the new CEO of a small startup with, you know, three persons, uh, and your goal is to build an, um, is to build a circuit or actually, your goal is to build a learning algorithm, um, that can recognize this phrase, Robert turn on, er, so that when someone, you know, buys this lamp and they say Robert turn on, then the lamp can turn on, right? And just focusing on the task of building, and then, then, you know, to, to be CEO of the startup, it means you need to do a lot of things, right? You need to figure out how to do the embedded circuitry, you figure out who are the lamp makers, etc. So there's all that stuff, but for today, let's just focus on the machine learning aspect of it, um. And so my first question to you is very open-ended, is- but and this is the life of a CEO, right? You wake up one day and you've just got to decide what to do, um. But so my first question to you is the open-ended question is, you're the CEO, uh, you're gonna shop at work uh, you know, tomorrow in your startup office and you want to build a learning algorithm to detect the phrase Robert turn on for this application, right? So, um, so my question is, what are you gonna do, right? So take a, take a minute to, uh, answer that by yourself first. Uh, don't, don't discuss with your neighbor yet, but you know, you're gonna shop in your office and, and then you're gonna start working on these engineering problems of building a neural network to do this so, uh, and, and do this as yourself, right? Don't, don't, don't pretend the- yeah, this hypothetical whatever, er, er, er, er, startup CEO with $10 billion to spend or whatever. Just do it. Just say yeah, I- but I don't think this is a terrible startup idea. I, I, I, I, this is not the best idea but I think this could work. So you're actually welcome to do this. But let's say you decide to do this and you go into your office tomorrow, right, what do you do, right? Why don't you take, um, why don't you take let's say two minutes to enter an answer, then we can, then we can discuss. [NOISE] In fact, I think, uh, yeah, yes, I, one thing I really like about the answer was actually the read exist- read existing literature part, right, um. In fact when you start a new project, um, uh, uh, and I think, um, uh, when you start doing a new project like that, assuming you've not worked with trigger word detection before, you know, reading research papers or reading code on GitHub, or reading blog posts on this problem is actually a very good way to quickly level up your knowledge, um. And I think that, you know it, it, it turns out that, uh, in terms of your, uh, exploration strategy, right, um, I want to describe to you how I read research papers, um, uh, which is- so this is, um, not a good way to review the literature which is if the x-axis is time and the vertical axis is research papers, what some people will do is find the first research paper and read that until it's done. And then go and find a second research paper and read that until it's done, and then go and find the third research paper and this- because this is a very sequential way of reading research papers and I find that the more strategic way to to go through these resources, everything ranging from blog posts, um, lots of good Medium articles that explain things, right, uh, research papers, um, right, GitHub. Is if you use a parallel exploration process where- This, this is actually what it feels like when I'm doing research on the web. I'm trying to learn about a new field I'm not that experienced in. Right, so I've just done a lot of work on trigger word detection. But if I hadn't worked on this before, then I would probably find you know, three papers. So again x-axis is time and vertical axis as different papers. And um, you know read a few papers, kind of in parallel at a surface level and skim them. And based on that, you might decide to read that one in greater detail, and then to add other papers that you start skimming and maybe find another one that you want to read in great detail, and then to gradually add new papers to your reading list and read some to completion and some not to completion. Um, you, I, I was actually chatting with um, ah, ah, one of my friends, uh, a for- former student at Berkeley, who mentioned that he was wanting to learn about a new topic. And he, he was, he told me he's compiling a reading list of 200 research papers that he wanted to read. That sounds like a lot. You rarely read 200 papers. But I think if you read 10 papers, you have a basic understanding. If you read 50, you have a pretty decent understanding. If you read like 100, I think you have a very good understanding, uh, uh, of, of, of a few but often this is [NOISE] time well spent. I guess. Um and ah, some other tips, again this is, I'm- I'm really thinking if you really are a CEO of this startup and this is what you wanna do, what advice would I give you? Um, ah, ah when you're reading papers ah, other thing to realize, one is that uh, some papers don't make sense. Right. And this fine. Uh, uh, even I read some papers I would just go nope. I don't think that makes sense. Uh, and, and it's not that uncommon for us to uh, find papers from a decade ago and we learned that half of it was great and the other half of it you know was really talking about things that were not that important. Right, so it's okay. Uh, uh, authors you know, usually papers are technically accurate but often what they thought was important like maybe an author thought that using batch number was really important for this problem. But it just turns out not to be the case. That, that happens a lot. That happens sometimes. [NOISE] Um, and I think the other tactic that I see Stanford students sometimes not use enough is uh, talking to experts including contacting the authors. So when I read a paper [NOISE] um, uh, I don't, I don't bother the authors unless I've actually like tried to figure it out myself. Right. But if you actually spend some time trying to understand the paper and if it really doesn't make sense to you uh, uh, uh, it's, it's, it's okay to email the authors and see if they respond. And [NOISE] and people are busy. Maybe there's a 50 percent chance of respond. And that's okay because it takes you five minutes to write an email and there's a 50 percent chance to get back to you. That could be time pretty well spent. Uh, uh, but, but don't, don't, don't bother people unless you've tried to do your own work. I, I usually get a lot of emails from you know high-school students tha- that do not feel like they've done their own work an- and I just write, and then yes. So just don't, don't, don't bother people unless you've actually tried to do your own work. [LAUGHTER] Um [NOISE] cool. So, after um, [NOISE] looking at the literature ah, and having a base maybe downloading an open source implementation or getting a sense of the avenue you wanna try. Oh and it, it turns out the, the trigger word detection literature is actually one literature where there isn't consensus on. This is a good algorithm, this a bad algorithm. Right. Despite all the trigger word, wake word detection systems that you know some of you may use already uh, there, there, there isn't actually consensus in the, in, in, in the research for me today on like, this is the best avenue to try. Um, but so let's say that um, you've read some papers, downloaded some open-source implementations and now you want to start training your first system. Right. And last time we talked about this we talked a little bit about how much time you will spend to collect data, and, and you know, we said you sp- spend a small amount of time. Spend like a day, or maybe two days at most. So collect your first data sets and start training up a model. Um, but my next question to you is what data would you collect? Right. Um, in particular what train dev test data [NOISE] would you collect? So you've decided on an initial neural network architecture and you want to [NOISE] train something to recognize this phrase, Robert turn on. Uh, I think this uh, probably I don't think it's possible to download the data set. I don't think anyone has collected a data set with the words Robert turn on and posted it on the Internet. So you have to collect your own data for this particular trigger phrase that you wanna use. But um, you know as CEO of this startup trying to build a neural net to detect the phrase "Robert turn on," um what data do you collect? Right. So why don't you take, why don't you again, take on um, let's say three minutes to write an answer to this. Yeah. I think this is an interesting one. Um, Robert turn on over and over and then data augmentation. [NOISE] Um, data augmentation is one of those techniques that um, uh, is a way to reduce the variance in your learning algorithm because you're generating more data. And ah, having worked on this problem I happen to know data augmentation works uh, is very useful for this problem. But if you didn't already know that fact this is one of the things I would probably not do right away, because I would train a quick and dirty system validate that you really have a high variance problem before investing the effort into data augmentation. So data augment is one of those techniques that so you know, like it never hurts here or it rarely hurts, usually helps but I don't bother to make that investment unless you have collected the evidence that you actually have a high variance problem and that this is actually a good use of your time. Right. Yeah. [NOISE] I think this, this one actually, this is actually nice. So um, uh, record everyone at startup saying, Robert turn on 100 times. So rea- really nice thing about that. You get it done really quickly. Um, uh, when I'm working with teams um, I actually think in terms of hours, in terms of how long it take us to do something. So this one you can probably do in like 30 minutes. Right so you get your data set collected in 30 minutes and get going. Uh, or, or, or you run around Stanford and just ask you know friends or strangers to speak into your uh, uh, laptop microphone. You could spend a few hours to get a much bigger data set than possibly at startup. I probably do that. Probably I should go and collect data in several hours rather than only spend 30 minutes. But this is actually pretty interesting as well because it lets you get it done really quickly. That makes sense? Right. [NOISE] So um, yeah. So let me actually uh, uh, share some more concrete advice. Right. And, and I think I should someti- sometime back um, to, to prepare a homework problem that you'll see later in this course. Ken and Eunice and I, we're actually you know building the system partially to, to, to create a homework right, that, that, that you'll see later in this course. So this is like a [NOISE]. Uh, this trigger word thing is a nice learning example that we're using in a few points throughout this course. Um, so here's one thing you can do. Uh, and this, this is actually what's um, [NOISE] what we did. Right. Which is uh, collect [NOISE] um, well simplifying a little bit um, [NOISE] collect a hundred examples [NOISE] of uh, uh, 10-second audio clips. Right? And so, uh, it turns out once you grab a hold of someone, uh, and ask them to speak into your microphone, you know, you can keep them for, um, three seconds which is how long it takes to say, Robert turn on or you can keep them for ten seconds which they're actually very willing to spend the extra seven seconds with you. Right? Um, but so if this is ten seconds of audio data, you know, so this is ten seconds of audio, right, and, and audio is just patterns of, uh, little changes in air pressure. Right. So, if you plot audio, the reason it looks like this waveform. It's just, uh, the, the way you're hearing my voice is you know, my voice or the speakers are creating very rapid changes in air pressure and your ear measures those very rapid changes in air pressure, interprets the sound and so a microphone, uh, is a sensitive device for recording these very very high frequency changes in air pressure and these plots that you see in audio is just, what is the air pressure at different moments in time. Right? But so given a, um, uh, ten second clip like this. If this is a three-second section, where they said, Robert turn on, then what you would like to do is to build a desk lamp say, that can sit here and the lamp is turned off, turned off, turn off, turned off, turned off, turned off. And at the moment they finish saying, Robert turn on, yeah, you turn it on. So this is, uh, output label y really, right?. And then, and then it's not detecting the phrase. So- so what you want to do for the trigger word system is, um, at you know, pretty much the moment they finished saying Robert turn on, uh, you once your learning algorithm to output a one, that's your target label y saying, "Yeah, I just heard this trigger word." Uh, and for all other times, you wanted to output zero, right? Cause- because, uh, and the one is when you decide to turn on the lamp at that moment in time. Right? So to collect the data set, um, here's something you can do, which is collect 100 audio clips, of 10 seconds each. And you know, when I'm prioritizing my work or my team's work, I will really, you know, look at these numbers and think okay, let's say- let's say you actually if you are doing it. Let's say you are running around Stanford and you want to collect 100 audio clips, uh, uh, maybe 10 people, 10 clips per person or maybe 100 different people. Um, I will actually estimate, you know, if you go to Stanford cafeteria uh, how long does it take to get one person? You can probably get one person every minute or two, if you go to a busy place on, on like the Stanford cafeteria. So you could probably get this done in like, uh, 100 to 200 minutes, like, two or three hours right? It's not that bad. So you get this done quite quickly. Um, and so and, and let's say you collect 100 audio clips and actually for the, for the, for the purposes of, uh, today let's say, you collect 100 audio clips to use for training, 25 for your dev set, um, [NOISE] and zero for the test set. Right. That's actually not that uncommon if you're building a new product to just not have a test set because your goal is, uh, to build something that convinces, you know, just early prototyping phases of a project, sometimes they don't bother with a test set. If, if, it goes to publish a paper then of course you need a rigorously collected test set. But if you're just building a product, then you don't need a rigorous evaluation sometimes you can just get started without dealing with a test set, right? So it's pretty easy to get started. Um, and then. [NOISE] All right. So taking that audio clip from above, um, one thing you can do to turn this into a supervised learning problem, uh, is to take- so the, the phrase Robert turn on can be said in less than three seconds. So let's say you take three seconds as the duration of audio. Right? So we can do is, uh, clip out. So let's say here was when Robert turn on was said. So what you can do is, uh, oh, right,  and the target label is zero, zero, zero, zero, zero, zero, zero one, zero, zero, zero, zero. Um, what you can do is then clip out different audio clips of three seconds. So here's one audio clip, you can take that audio clip. This is X and the target label is zero because, because was Robert turn on was not said [NOISE]. Um, and you can take on- on this audio clip at different random, each clipped, three second clip and that clip also has a target label 0. Um, and you know, for this one right? Which is a three-second clip that can, that-that, that ends at, uh, on, the last part of the on sound you would have a target label of 1. Right? So, and- and when- when you learn about sequence models, RNNs, you learn a better method than this explicit clipping. But for now let's say you take these, um, audio clips and turn it into, uh, three- so take a 10 second clip and by clipping out ran- different windows you can take your, um, let's say 100, uh- uh, clips. And because for each ten second clip you can take different windows, you could turn this into let say, uh, 3,000 [NOISE] training examples. Right? So here I took a ten second clip and - and [NOISE] show, you know, took three different three second windows, but you take 30 three second windows, then each 10 second audio clip becomes 30 examples. And now, you've turned a problem into a binary classification problem where you need to train a neural network that inputs a three-second clip and labels it as either zero or one. Right? This make sense? And so this is an example of, uh, the- the more complex, uh, pipelines you might have, if you're building a learning algorithm. So take a continuous, you know, audio detection problem and turn it into a binary classification problem which you've learned how to build various neural networks for. Right? And again, when you learn about RNNs, you learn about other ways to process sequence data or temporal data. Okay. So, um, oh, go ahead. [inaudible] uh is that manually getting the data? [NOISE] Uh is this manually getting, yes, I would, yeah, actually, if you have 100 examples, uh, it's not that hard to just listen to it on your laptop or some audio playing software to figure out when- when they finished saying, uh, Robert turn on. And then at that moment to put a one in the target label right? Because this is really when you want the lamp to turn on, right? Makes sense? Cool. So, um, any other questions? Actually, feel free to ask clarifying questions, yeah, go ahead. I, I wonder if this is gonna cause a problem but  ones are too sparse. Oh, sure, let me get back to that. Anything else? All right. Is there a specific reason why you only train them on a few seconds instead of ten seconds, [inaudible] [inaudible] [NOISE] I see, yeah, why do we do three seconds, or four or five seconds then there's another hyperparameter to contend. So I think. uh, [NOISE] I don't know, you have say it really slowly to take a few seconds is this, right? This is Robert turn on. And so again is this design choice? [LAUGHTER] Um, yeah,  all right. So, so, um, let's say you do this, feed it to a supervised learning algorithm, train a neural network, um, and let's say that when you classify this, ah, when, when you run this algorithm, you end up with 99.5 percent accuracy [NOISE] right? Um, uh, but you find that the algorithm has zero detections. [NOISE] Right? Um, and, and what I mean is that whatever audio you give it, it just output zero all the time. So the algorithm just says, nope, I never heard the phrase "Robert turn on", you know. So, so, uh, so, uh, so, so my question to you is, you know- and by the way, the reason I'm going through these scenarios is, um, I found that, uh, a good way to gain good intuitions and, and to become good at making these decisions, is these are the decisions that a project leader, ri ght, or a tech leader or a CEO needs to make. These are actually like pretty much exactly the decisions you need to make. And I find that, um, one of the ways to gain this type of experience is you, you know, find a job with a good AI team and work with them for five years, right? And then you actually live through this and you see what they do. But instead of needing you to go and spend five years to see ten examples of this, I'm trying to step you through maybe one example in, in one hour. So, so instead of, uh, you know, gaining this experience through work experience, which is great, but takes many, many years [LAUGHTER] or many, many months hoping to, you know, let's just put you in the position of making these decisions. You can learn from them much faster, right? Um, so- and all the examples I'm giving are actually completely realistic, right? They're either exactly or very similar to things I have seen in, in actual, you know, very real projects. So question is your learning algorithm gives this result, 95 percent accuracy, zero detections, what do you do? Let me mention some of- some of the answers I really liked. I think that, uh, um, you know, I- when I think of building learning algorithms, the process is often specify a dev set and/or test set that measure what you care about. And then you don't always have to do it, but it's good hygiene. It's just- it is, uh, sharpens the clarity of your thinking, right? If you have a very clear specification of the problem. And I think one insight out of this is that if your dev set is really out of whack, right? Because it's so unbalanced, that accuracy in your dev set doesn't translate to what you actually care about. Because, you know, presumably, this is 99.5 percent accurate on the dev set as well. But this performance is terrible. So it's doing great on a dev set, on your accuracy metric, but giving you terrible performance. So I think of it as good hygiene. You know, this is kind of good sound practice, uh, to, to just specify, make sure you at least have a dev set and evaluation metric that corresponds more closely to what you care about. So making the dev set more balanced, uh, equal numbers of positive and negative would, would be a good step toward that. Um, and then I think, um, uh, you could also, um- there are a few people that talked about, um, give the higher weights to the positive examples, right? So, you know, um, uh, one way to do this is to resample your training and your dev sets to make them more proportionate in terms of maybe closer to a balanced ratio positive negative examples. That'd be okay. The other way to not do resampling, we'd just give the positive examples a greater weight, right? Um, I would probably resample. Um, another thing you could do, um, uh, you know, in the, in the interests of, um, speed, even if it's not the mathematically most, most sound thing to do, is to change the target labels to be a bunch of ones after that. And this is a hack, this is not formally rigorous. But if you've implemented the rest of this code already, this might be a reasonable, you know, a little bit hacky thing to do. But this is- this, this, this might work well enough. Right? I would- I might not- I don't know if I would want to try to write an academic research paper with this method, maybe you can get away with it. But this little thing that I think if you tried to publish a paper with this, academic reviewers might raise their eyebrows and say maybe, you know, maybe this is okay. But I think if you want something quick and dirty, that just works. I think, uh, uh, labeling the ones, changing a bunch of labels to be ones so that's, say, a clip here, right? Uh, that ends just a little bit after Robert turn on, that's still labeled one, that'll be pretty reasonable. But this will be saying that, uh, for anywhere [NOISE] within maybe a 0.5 second period after Robert turn on finished, it's okay to turn on the light anytime within that period. That you kind of wanna be turning on the light. Turning on the lamp, you know, say within half a second, right, after Robert turn on is- has been said. And this would be a not- this would be a way to just get more labels of ones in there. All right, that makes sense? Yeah? With like rebalancing your data sets with [inaudible] , how does that translate to when you deploy this, you're not going to see Robert turn on as much, right? Like one out of 1,000 might be reflected, but what do we expect to see? Yeah, yeah. Right. So, um, I think that, uh- how do we put it? Um, so if you actually- yes, so uh, this, this is sort of a dev set and evaluation measure kind of question, right? So, uh, one of the- a couple of the metrics that people often use, uh, when actually working on this, is, uh, when someone says Robert turn on, what is the chance that it actually wakes up, or the lamp turns on? And then the second is, if no one is saying anything to the lamp, you know, how often does it randomly turn on by itself without you having said anything? So those are the two metrics people actually use. And, and sometimes you also try to combine them in a single number evaluation metric or something. Uh, but I think that, um, uh, you could identify the data sets and measure both of these things. And then, and then hopefully find a way to combine them into a single real number, which I think- yeah. And I think one of the ways you talked about in the, in the videos as well as, right? Does that make sense? Uh, yeah. But I think- I think, uh, so the question is really, um, uh, on what is it that satisfies a user need, right? And, uh, and just one, one thing about, uh, the straightforward way of rebalancing, is that if you don't do this then your whole data set, just has very few positive examples, right? Um, and so if you throw away all the negative examples, so that you cut down the number of negative examples until you have exactly equal numbers of positives and negatives, you've actually thrown away a lot of negative examples. This makes sense? And so one, one problem with the straightforward way of rebalancing, is that, you know, in your audio clip, in your test 10 second clip that we collected by running around Stanford, um, you have one example of Robert turn on. And so if you want exactly per, perfectly balanced positive and negative, it means that you're allowed to only clip out one negative example out of this. You can say, that's negative and that's a positive. And you can't clip out more negative examples from this, right? So, so if you use, uh- if you insist on a perfect rebalance, you're actually throwing away a lot of negative examples that, that could be helpful for the learning algorithm. Great. [NOISE] Um, so all right. [NOISE] So, um, you know, a lot of the workflow of, uh, building learning algorithms is, um, uh, building learning algorithms feels more like debugging, right? Because what happens in a typical machine learning workflow is you implement something and it doesn't work. So you figure out what is the problem, so fix that, uh, like rebalancing or reweighting or adding more ones. And so that fixes the current problem. And then after fixing the current problem, which, which is the one we just solved, say, you then come across a new problem and you have to solve that. And you fix that problem, you click somewhere else, another new problem. So I find that, uh, the workflow of, um, when I'm working on a machine learning project, it often feels more like software debugging than software development, right? Because you're often trying to figure out what doesn't work and you're trying to fix that. And after you fix that problem, then another bug surfaces and you squash that, and you do that, and another, and you kind of keep doing that until the algorithm works. So if I keep talking about, you know, your algorithm doesn't work, what do you do next, right? That, that's kind of the theme of today's presentation. But that, that is what the workflow- That is what your day-to-day work of developing a learning algorithm is usually like because it's like, it doesn't work, and you fix it. It still doesn't work, then you fix that, and it still doesn't work, you fix it. And you do that enough times until it works, right? That, that is actually what often working on a learning algorithm works- look, looks like. All right. So let's say you fix that problem, um, and you conclude, uh, through doing error analysis, that your algorithm is overfitting, right? So you know, you've- you've added a lot more ones, so the dataset is a little bit more balanced. So let's just add a bunch of ones like I did on that previous board, so let's just add a lot of ones here, so the dataset isn't as unbalanced. And um, [NOISE] let's see if- [NOISE], um, right, okay, good. Let's say that- sorry [NOISE] , see, too many pages of notes here. Okay, good. So let's say that you find that it achieves now 98 percent accuracy on training and 50 percent accuracy on the dev set, right? So very large gap between your training and your dev set performance, and so a clear sign of overfitting. And so I think of one of the earlier questions, someone talked about data augmentation, and so we have this clear sign of overfitting, this is a good time to consider data augmentation, right? And then- and so let's say you go ahead and do data augmentation. So for audio, this is how you could do data augmentation, which is, um, collect a bunch of background audio. You know, so I guess if you're trying to build a lab that might go into people's homes, then you could go into your friends' homes and, you know, with their permission, record, right? What the background sound of their home looks like. You know, maybe there are people talking in the background, maybe the TV on in the background. Well, whatever goes on in people- people's homes. Um, and then, it turns out that if you take a, um, say, a one second clip, of Robert turn on, or RTO, and you add that to a background clip, then you can synthesize an audio clip of what it sounds like in your friend's house if someone were to suddenly pop up and say Robert turn on against the background sound of your friend's house, right? And- and it turns out that, um, if you want to make the system robust, so actually, for example, have a- I know- I- I- I actually know someone that lives, unfortunately, close near to a train station and so their house actually has a lot of train station noise from the Caltrain. And so what you can do to make your system more robust is also take a clip of say train noise, right? Like Caltrain noise, and if you take that noise and take, uh, in this case, let's say one-second, one-second of a three-second clip of someone saying Robert turn on and you synthesized that on top of the train in the background, then what you end up with is a 10 second clip of someone saying Robert turn on against the noisy train in the background type of noise. All right. And so in order to do data augmentation or data synthesis, you can take some one-second clips of people saying Robert turn on in a quiet background and then take some one second clip of people saying random words, right? Let's say, you know, cardinal, right? Since you're Stanford, and synthesize this against the train noise background and then you would have, in this case, you would have what sounds like train noise, train noise, train noise, Robert turn on, train noise, train noise, train noise, train noise, cardinal, train noise, and so on right? And then you could generate the labels now as zeros there, ones there and then zeros there, right? Because if this is what it actually sounded like in a user's home, then you want the lamp to turn on after Robert turn on but not after these random words. So you can pick different random words. Great. Um, so let's see. Great. So um, what I'd like you to do is evaluate three different possible ways, um, to collect noisy data, right? To- to- to collect this type of background data, right? Um, and so what I'd like you to do for the next question is let's say you and your team, you know, have, ah, brainstormed. Um, brainstormed a few different ways to collect  this type of background noise data and let's say you've decided that you would like to collect 10 hours of background noise data, right? Okay? So I'm going to present to you three options. One is um, you know, run around Stanford and place microfilms around Stanford or in their friend's homes, do this with consent and don't- don't, you know, California as you- you- you're not supposed to- don't record people without their knowledge and consent, right? Second is download clips online. It turns out if you go to YouTube there are these like 10 hour long clips of, you know, rain noise or cars driving around. So you actually- and again, if you do that, find something that's Creative Commons and sort of appropriately licensed, right? Another thing you could do is, ah, use Mechanical Turk. Amazon Mechanical Turk. Where you can have people all around the world be paid, you know, modest amounts of money to submit audio clips, right? So for the next exercise what I want you to do because, um, and I want that this exercise of discipline which is, what I want you to do is, um, I want you to estimate. Let's see. What time is it now? Okay, it's 12:30 PM right now. What I want you to do is write down three numbers in the next exercise to estimate if you were to do this, you know, let's say you were to go do this right now, right? By what time will you have finished if you were to do option one? What time would you finish if you were to do option two? What time would you finish if you were to do option three? If your goal is to collect 10 hours of data through one of these mechanisms. Does that make sense? So it's 12:30 PM now. So what I'd like you to do is just write down three numbers. First number is what time is it, what time would it be by the time you collected 10 hours of data, you know, from around Stanford. What time would it be right- and if you could do this in- so if you think you would do it by tonight, then write 09:00 PM. If you think it'll do, if you think it will take you one week, that write the date one week from now, right? Whatever it is. But just write down three numbers of these three activities, okay? Why don't you do this one relatively quickly? Can people do this in like maybe a minute and a half? All right, cool. This is interesting, um. Yeah, whether people are, actually this is a surprisingly large variability. I'll mention one thing that um, surprised me. Um, I'll give you my own assessment. I think that uh, you know when I'm leading startup teams we tend to be very scrappy, right? And so I think that um, if the goal is to collect 10 hours of data, if you have three friends who have a laptop you can collect three hours of data per hour because you got three recordings going in parallel. So if I were doing this with say two other friends you know, I bet, I bet we could get this done by tonight, right? Because if you need nine hours of data that's each person needs to collect three hours of data and you run around Stanford and you keep the microphones running, I bet, I bet I could get this done by 6:00 PM right maybe, maybe even earlier I don't know. Download clips online uh, is actually, I don't know, it's actually an interesting one, maybe be about the same time. Um, it turns out one tricky thing about downloading clips online is that um, uh, I think a lot of the you, the- there are people that um, have trouble sleeping at night so they listen to highway noise or whatever. And so there are these you know 20 hours of highway clips highway noise on YouTube that you can find. But I, I don't know how those clips are generated. And I suspect a lot of them loop, right? Meaning it's the same one hour played over and over. So I actually think it's harder than, than, than one like, yes they get 10 hours of um [NOISE] non-repetitive data and it's one of those things you know, if I take an hour of highway sound and loop it you can't tell the difference because all highway sound sounds the same. I just can't tell one minute of highway sound from another one but if you have one hour of highway sound looped 10 times, the learning algorithm will actually perform much less well than if you have 10 hours of fresh highway sound. So this I would actually have a harder time doing. I think I've probably I, I, I would prob- if I were doing this I, because of these problems I would probably budget until sometime tomorrow, right? May- may- maybe, maybe 9:00 PM or something. Maybe that's doable. I'm not sure. Um, the one surprise to me was some people thought they could do this by tonight. Uh, I, a- again I've used Amazon Mechanical Turk is actually a huge process to set up Amazon Mechanical Turk, get people on board. Um, and especially to get them on microphone. Uh, uh, I don't know if you implement something on Flash that can speak in their web browser. [NOISE] And then Flash isn't supported, it's, it's, actually, it's actually not that easy to get a lot of Turkers to do this and the global supply of [NOISE] Turkers is also unlimited. So I would, if I were doing this I would probably [NOISE] I don't know maybe a week or something, right? Hard to say, I'm not sure um. But so the specific opinion isn't that important but I want you to go through this exercise because this is how um, efficient startup teams should you know, brainstorm a list of things and then you all figure out how long you think it'll take to do these things and I think uh, you can have a debate about how high quality the data is, I think you can get very high quality data from this and from this. Uh, [NOISE] I, I, I just don't trust a lot of those online audio sources. Uh, but this is really fast and you can get pretty high-quality data. I would probably do this to collect the background sound to get going, right? But I think that part of their workflow I see of you know, fast-moving teams is pretty much exactly what you did. Which one, what is that the exercise of brainstorming the lists of options and then really estimating or what time can we get this done and then use that to pick an option, right? Um, and then I wanna just mention one last thing uh, which is that [NOISE] these differences matter, right? Um, you know I've actually built, I've built a lot systems, built a lot of machine learning systems. But um, oh and, and I think by the way if you do everything we just described and you'll see this later in a problem set. Uh, you can actually with this set of ideas pretty much this set of ideas that we just went through today. You can actually build, build a, build a pretty decent trigger word detection system or wake registering word detection system. In fact we'll oscillate through pretty much this in a later homework exercise. But now you know when you get to that homework exercise when you do RNNs uh, you know how you could come up with this sort of process yourself if, if you didn't already know how to make these types of choices. Yeah, question Just one question. Uh, I conduct my research [inaudible] how my micro affect my result but at the beginning it seems like it's not important like my micro [inaudible] So when I download it, it, my, what mess my result but what, what you have to think about it? Yeah so my advice wha- [NOISE] , does the microphone affect your result, right? [OVERLAPPING] My, my advice is would be to uh, get something going quick and dirty and then develop a data set, right, with the actual types of data you can develop on your real microphone and then see if there is a problem and it may be er, different microphones do have different characteristics. And if it is a problem then go back and think about how you collect data that's more representative how you test. I wanna mention one more quick thing, you were handed class surveys and wants to do something real quick which is um, I wanna tell you why these things really matter which is um, if this is uh, performance, right? Uh, uh, let's say, actually let's say error. And um, this is time, right? And if this is today and you're the CEO of a startup remember that's, that's what we're doing in this lesson. And this is six months from now, but this is 12 months from now. Um, you know maybe if a competitor, actually maybe, maybe I don't know. [LAUGHTER] Maybe because we've talked about this so much in this class maybe two of you in this class are  gonna build a startup and be a competitor. Um, but over time most machine learning teams, you know the error actually goes down over time as you work on problems, very good and this is what I see in tons of practical projects. You know, we work on project, improve the system, and the error actually goes down over time as you work on this over the next 12 months say but if you're really a CEO of a startup doing this [NOISE] and it turns out that it's the startups that have the discipline to constantly be the most efficient. Um, don't do something that takes you two days. If you can get a similar result in one day. The difference is not that you're one day slower, the difference is that you are 2x faster, right? And then, then having that mindset if we take this whole chart and compress it on the horizontal axis. Um then you [NOISE] want to be this startup that you know makes the same amount of process in six months instead of 12 months, right? And because uh, if you're able to do this then your startup will actually perform much better in the marketplace. Assuming your accuracy's important which it seems to be for wake word. And so don't think of this as saving you a day here and there think of this as making your team twice as fast. And that's the difference between this level of performance and that level of performance. So that's why when I'm, you know building teams to and executing these projects I tend to be pretty obsessive about uh, making sure we're very efficient in exploring the options and [NOISE] don't wait till tomorrow to collect data of dubious quality when you have a better idea of collecting data by today because the difference is not that you wasted 12 hours, the difference you are twice as slow as a company, right? So I think uh, so hopefully through this example on your ongoing experiences throughout this course it can help you continue to get better at this. Right. Um, last thing we want to do [NOISE] was uh, we're about halfway through the course and go ahead um, we went to hand out a survey um, an anonymous survey uh, to get some feedback from you about this class. And whenever we get these surveys uh, we end up uh, uh, thanks to previous generations of students' feedback. We've already been gradually making class better. So I think Ken and I actually read all of these questions ourselves and try to find ways to take your feedback to improve the class so if you can take you know, five minutes um, for this survey and you can hand it in just drop it off anonymously up here in front. Uh, we're very grateful for your suggestions. Okay? So um, I think if you haven't entered your ID yet uh, you can still do so but uh, that's it for today. So please fill out the survey and anonymously just drop it off back in front then we'll wrap up. Okay, thank you. 

Hi everyone. Uh, welcome to lecture number seven. Um, so, up to now, uh, I believe, can you hear me in the back? Is it easy? Okay. So, in the last set of module that you've seen, you've learned about convolutional neural networks and how they can be applied to imaging, notably. Uh, you've played with different types of layers including pooling, max pooling, average pooling, and convolutional layers. You've also seen some classification, uh, with the most classic algorithms, uh, all the way up to Inception and, and ResNets. Uh, and then you jumped into advanced application like object detection with YOLO, uh, and the Fast R-CNN, Faster R-CNN series with an optional video. And finally, uh, face recognition and neural style transfer that we talked a little bit about in the past lectures. So, today, we are going to build on top of everything you've seen in this set of modules, to try to delve into the neural networks and interpret them. Because you, you, you notice after seeing, uh, the set of modules up to now that a lot of, uh, improvements of the neural networks are based on trial and error. So, we try something, uh, we do hyperparameter search, sometimes the model improves, sometimes it doesn't. We use a validation set to find the right set of methods that would make our model improve. It's not satisfactory from a scientific standpoint, so people are also searching how can we find, uh, an effective way to improve our neural networks, not only with trial and error, but with theory that goes into the network and visualizations. So, today, we will focus on that. We first, uh, we'll see three methods, saliency maps, occlusion sensitivity, and class activation maps, which are used to kind of understand what was the decision process of the network. Given this output, how can we map back the output decision on the input space to see which part of the inputs were discriminative for this output. And later on, we will delve even more in details into the network by looking at intermediate layers, what happens at an activation level, at a layer level, and at a network level with another set of methods, gradient ascent class model visualization, dataset search, and deconvolution. We will spend some time on the deconvolution because it's, uh, it's a cool, it's a cool type of, uh, mathematical operation to know and it will give you more intuition on how the convolution works from a mathematical perspective. Uh, if we have time, we'll go over a fun application called Deep Dream, um, which is super cool visuals for some of you who know it. Okay? Let's go. Menti code is on the board, if you guys need to, to sign up. So, uh, as usual, we'll go over some context, trial information and small case studies, so don't hesitate to participate. So, you've built an animal classifier for a pet shop, um, and you gave it to them. It's, it's super good. It's been trained, uh, on ImageNet plus some other data. And what, what is a little worrying is that the pet shop is a little reluctant to use your network, because they don't understand the decision process of the model. So, how can you quickly show that the model is actually looking at a specific animal, let's say a cat, if I give it an input that is a cat. We've seen that together, one time, everybody remembers? So, I'll go quickly. Uh, you have a network, here is a dog given as an input to a CNN. The CNN assuming the constraint is that there is one animal per image was trained with a Softmax output layer and we get a probability distribution over all animals, iguana, dog, car, uh, cat and crab. And what we want is to take the derivative of the score of dog and backpropagate it to the input to know which parts of the inputs were discriminative for this score of dog. Does that make sense? Everybody remembers this? And so, the interesting part is that this value is the same shape as x. So, it's the size of the input. It's a matrix of numbers. If the numbers are large in absolute value, it means the pixels corresponding to these locations had an impact on the score of dog. Okay? What do you think the score of dog is? Is it the output probability or no? What- wha- wha- what do I mean by s of dog? [NOISE] Yeah? Score of the dog? It's the score of the dog, yeah. But is it, uh, 0.85, that's what I mean? [NOISE] No, there are actually formulas used to compute the 0.85, going to the softmax [inaudible] Yes. So, it's the, it's the score that is pre-softmax. It's the score that comes before the softmax. So, as a reminder, here's a softmax layer and this is how it could be presented. So, you get as a vector, that is a set of scores that are not necessarily probabilities, they are just scores between minus infinity and plus infinity. You give them to the softmax and the softmax, what it's going to do is that it's going to output a vector where the sum of all the probabilities in this vector are going to sum up to one. Okay? And so, the issue is if instead of using the derivative of what we called Y hat last time, we use the score of dog, we will get a better representation here. The reason is in order to maximize this number, score of dog divided by the sum of the score of al- all animals, or like maybe I, I should write exponential of score of dog divided by sum of exponential of the score of all animals. One way is to minimize the su- the scores of all the other animals rather than maximizing the score of dog. So, you see, so maybe moving a certain pixel will minimize the score of fish. And so, this pixel will have a high influence on Y hat, the general output of the network. But it actually doesn't have an influence on the score of dog one layer before. Does it make sense? So, that's why we would use, uh, the scores pre-softmax instead of using the scores post-softmax that are the probabilities. Okay. And what's fun is here you cannot see that, the slides are online if you wanna- if you wanna look at it on your computers. But you have some of the pixels that are roughly the same positions as the dog is on the input image that are stronger. So, we see some white pixels here. And this can be used to segment the- the dog probably. So, you could use a simple thresholding to find where the dog was based on this pixel, uh, pixel derivative, the pixel score map. It doesn't work too- too well in practice, so we have better methods to do segmentation, but this can be done as well. So, this is what is called saliency maps, and it's a common technique to quickly, uh, visualize, uh, what the network is looking at. In practice, we will use other methods. So, here's another contextual story. Now you've built the animal classifier, they're still a little scared, but you wanna prove that the model is actually looking at the input image at the right position. You don't need to be quick but you have to be very precise. [NOISE] Yeah? So, going back from the last slide, is the saliency map that edge detection, one pixel border? No, the saliency map is literally distinct here. Okay. It's the values of the de- the derivative. Oh, okay. So, it's like the gradient's at [inaudible] So, you- you take the score of dog, you backpropagate the gradient all the way to the inputs, it gives you a matrix that's exactly the same size as the x. And you use- you use like a specific color scheme to see which pixels are the strongest. Perfect, thank you. Okay. So, here we have our CNN. The dog is forward propagated and you get a score of, uh, probability score for the dog. Now, you want a method that is more precise than the previous one but not necessarily too fast. And this one, we've talked about it a little bit, it's occlusion sensitivity. So, the idea here is to put a gray square on the dog here. And we propagate this image with the gray square at this position through the CNN. What we get is another probability distribution that is probably similar to the one we had before, because the gray square doesn't seem to impact too much of the image. At-, uh, at least from a human perspective, we still see a dog, right? So, the score of dog might be high, 83 percent probably. What we can say, is that we can build a probability map corresponding to the class dog and ha- and we will write down on this map how confident is the network if the gray square is at a specific location. So, for our first location, it seems that the network is very confident, so let's put a red square here. Now, I'm going to move the gray square a little bit. I'm shifting it just as we do for a convolution and I'm going to send again this new image in the network. It's going to give me a new probability distribution output and the score of dog might change. So, looking at the score of dog, I'm going to say, okay, the network is still very confident that there is a dog here, and I continue. I shift it again, here same, network's still very confident that there is a dog. Now, I shift the, the, the square, um, vertically down, and I see that partial, that the- the face of the dog is partially occluded. Probability of dog will probably go down, because the network cannot see one eye of the dog. It's not confident that there's a dog anymore. So, probably, the confidence of the network went down. I'm going to put a, a square that is tending to be blue, and I continue. I shift it again and here we don't see the dog face anymore. So, probably the network might, might classify this as a chair, right? Because the chair is more obvious than the dog now. And so, the probability score of dog might go down. So, I'm gonna put a blue square here and we're going to continue. Here, we don't see the tail of the dog, it's still fine, the network is pretty confident, and so on. And what I will look at now is this probability map which tells me roughly where the dog is. So, here we used a pretty big filter compared to the size of the image. The smaller the, sorry, the pretty big gray square, the smaller the gray square, the more precise this probability map is going to be. Does that make sense? So, this is, if you have time, if you can, you can take your time with the pet shop to explain them, uh, what's happening, you would do that. Yeah? Would you ever, in an occlusion type of situation have an increase in the probability not just a decrease, say, you removed the noise from the picture? We will see that in the next slide. That's correct. So let's see more examples. Here, we have three classes and these, these, these images has been- have been generated by Matthew Zeiler and Rob Fergus. This paper, Visualizing and Understanding Convolutional Networks, is one of the seminal paper that has led the research in in visualizing and interpreting neural networks. So, I'd advise you to take a look at it, and we will refer to it a lot of time in this lecture. So, now we have three examples. One is a pomeranian, which is this type of cute dog, a car wheel, which is the true class of the second image, and an Afghan Hound, which is this type of dog here on the last image. So, if you do the same thing as we did before that's what you would see. So, just to clarify, here we see a blue color. It means when the gray square was positioned here or centered at this location, the network was less confident that the true class was pomeranian. And in fact, if you look at the paper they explained that when a gray square was here, the confidence of pomeranian went down because the conference, because the confidence of tennis ball went up. And in fact, the pomeranian dog has a tennis ball in the mouth. And another interesting thing to notice is on the last picture here. You see that there is a, a red color on the top left of the image. And this is you exactly as what- as what you mentioned Adam is that, when the square was on the face of the human, the network was much more confident that the true cla- that the true class was the dog. Because you removed a lot of meaningful information for the network which has the face of the human. And similarly, if you put the square on the dog, the true class that the network was outputting was human probably, does make sense? Okay. So, this is called occlusion sensitivity, and it's the second method that you now have seen for interpreting where the network looks at on an input. So, let's move to class activation maps. So, I don't know if you remember, but two weeks ago, Pranav when he discussed the techniques that he has used in healthcare, he explained that you get a- he did a chest x-ray. And he manages to, to tell the doctor where the network is looking at when predicting a certain disease based on his chest X-ray, right? You remember that? So, this was done through class activation maps, and that's what we're going to see now. So, one important thing to notice is that we discussed that classification networks seem to have a very good localization ability, and we can see it with the two methods that we previously discussed. Same thing, for those of you who have read the yellow paper, that you've studied in this set of modules. The YOLOv2 algorithm has first been trained on classification, because classification has a lot of data, a lot more than object detection. Has been trained on classification, builds a very good localization ability and then has been fine-tuned, and retrained on object detection datasets. Okay. And so the core idea of class activation map is to show that CNNs have a very good localization ability even if they were trained only on image level labels. So, we have this network. There is a very classic network used for classification. We give it a kid and a dog. Uh, this class activation map is coming from MIT, the MIT lab with Bolei Zhou et al in 2016. And forward propagate this image of a kid with a dog through the network which has some CONV, ReLU, MAX POOL, classic series of layers, several of them. And at the end, you usually flatten the last output volume of the CONV, and run it through several fully connected layer which are going to play the role of the classifier, and send it to a softmax, and get the probability output. Now, what we're going to do is that we are going to prove that this CNN is generalizing to localization. So, we're going to convert this same network in another network. And the part which is going to change is only the last part. The downside of using flattened plus fully connected is that you lose all spatial information, right? You have a volume that has spatial information, although it's been going through some max pooling, so it's been down sampled and you lost some part of the spatial localization. Flattening kills it, you flatten it you run it through a fully connected layer, and then it's over. You- it-s, it's super hard to find out where the activation was corresponds to on the input space. So, instead of using flattened plus fully-connected, we're going to use global average pooling. We're going to explain what it is. A fully connected softmax layer and get the probability output. And we're going to show that now this network can be trained very quickly because we just need to train one layer, the fully connected here, and can show where the network looks at. The same as the previous network. So, let's talk about it more in detail. Assume this was the last CONV layer of our network, and it outputs a volume, a volume that is sized to simplify four by four by six. So, six filters were used in the last CONV. And so we have six feature maps now. Does that makes sense? I'm going to convert this using a gla- global average pooling to just a vector of six values. What is global average pooling? It's just taking these feature maps. Each of them averaging them into one number. So, now instead of having a four by four by six volume, I have a one by one by six volume, but we can call it a vector. Does that make sense? So, what's interesting is that this number, actually holds the information of the whole feature map that came before in, one number being averaged over it. I'm going to put these in a vector, and I'm going to call them activations. As usual a_1, a_2, a_3, a_4, a_5, a_6. As I said, I'm going to train a fully-connected layer here with the softmax activation, and the outputs are going to be the probabilities. So, what is interesting about that? It's that the feature maps here as you know will contain some visual patterns. So, if I look at the first feature map, I can plot it here, so these are the values. And of course, this one is much more granular than four by four. It's not a four by four it's much more numbers. But this- you can say that this is the feature map, and it seems that the activations have found something here. There was a visual pattern in the inputs that activated the feature map, and the filters which generated this feature map here in this location. Same for the second one, there's probably two objects or two patterns that activated the filters that generated this feature map, and so on. So we have six of those. And after I've trained my fully connected layers here- my fully connected layer, I look at the score of dog. Score of dog is 91 percent. What I can do is to know this 91 percent, how much did it come from these feature maps? And how can I know it? It's because now I have a direct mapping using the weights. I know that the weight number one here, this edge you see it, is how much the score was dependent on the orange feature map? Does that makes sense? The second weight, if you look at the green edge, is the weights that has multiplied this feature map to give birth to the outputs of a dog. So, this weight is telling me how much this feature map the green one has influence on the output. Does that makes sense? So, now what I can do is to sum all of these, a weighted sum of all these feature maps. And if I just do this weighted sum, I will get another feature map. Something like that. And you notice that, this one seems to be highly influenced by the green one, the green feature map, yeah. It means probably the weight here was higher. It probably means that the second feature of the last CONV was the one that was looking at the dog. Does that make sense? Okay. And then, once I get this feature map, this feature map is not the size of the input image, right? It's the size of the height and width of the output of the last CONV. So, the only thing I'm going to do is, I am going to up sample it back simply, so that it fits the size of the input image, and I'm going to overlay it on the input image to get my class activation map. The reason it's called class activation map is because this feature map is dependent on the class you're talking about. If I was using, uh, let's say I was using car here, if I was using car, the weights would have been different, right? Look at the edges that connect the first activation to the activation of the previous layer. These weights are different. So, if I sum all of these feature maps I'm going to get something else. Does that make sense? So, this is class activation maps. And in fact, there is a dog here and there is a human there. And what you can notice is, probably if I look at the class of human the weights number one might be very high, because it seems that this visual pattern that activated the first feature map was the face of the kid. Okay. So, what is super cool is that you can get your network, and just change the last few layers into global average pooling plus a softmax fully connected layer. And you can do that, and visualize very well. It requires a small fine tuning. Yeah. So are these like saliency maps, but for the activation? It's a different vocabulary, I would use saliency maps for the backpropagation up to the pixels and class activation maps related to one class [NOISE]. Uh, it's not the backpropagation at all, it's just an up sampling to the, to the input space based on the feature maps of the last CONV layer. So it's mostly just examining the weights and sort of doing like a max operation on a, on them, not so much that different from backpropagation. Yes. Good [NOISE]. Any other questions on class activation maps? Yes. Does taking the average not kill the spatial information? Yeah. That's a good question. So, taking the average, does it kill the spatial information? So, let me, let me write down the formula here. This is the score that we're interested in, let's say dog plus C. What you could say is that this score is a sum of K equals one to six of WK, which is the, the weight that, that connects the output activation to the previous layer, times what's times A of the previous layer. Uh, let's say we, we, we use a notation that is like K is the Kth feature map and IJ is the location and I sum that over the locations. Can you see in the back? Roughly? So, what I'm saying is that here, I have my global average pooling that happened here and I can divide it by the certain number, so divided by 16, four by four. Okay. I can switch the two sums, so I can say that this thing is a sum over IJ the locations times sum over K equals one to six of what, WK times AK, so the activations of the Kth feature map in position a- IJ and times the normalization, 116. Does it make sense? Does this makes sense? So I, I still have the, the, the location, I still moved, I still moved the sum around and what I could do is to say that this thing is the score in location IJ of the class activation map, is a class score for this location IJ and I'm summing it over all locations. So, just by flipping what the average pooling was doing over the locations, I can say that my weighting, using my weights, all the activation in a specific location for all the feature maps, I can get the score of this position in regards to the final output. Does that makes sense? So, we- we're not losing the spatial information. [NOISE] The reason we're not losing it is because we know, we know what the feature maps are. Right. We know what they are and we know that they've been averaged exactly, so we exactly can map it back. Were you giving only one way to each [inaudible]. Yeah. Because we, we assume that each filter that generated these feature maps detects one, one specific thing. So, like if, if this is the feature map it means assuming the filter was detecting dog, that we're going to see just, just something here meaning that there is a dog here and if there was a dog on the lower part of the image we would also have strong activations in these parts. [NOISE] I, I, I say if you wanna see more of the map behind it, check the paper, but this is the intuition behind it. You can flip the summations using the global average pooling and show that you keep the spatial information. The thing is you do the global average pooling, but you don't lose the feature maps because you know where they were from the output of the count, right? So, you're not, you're not deleting this information. That makes sense? Yeah. So, the summation of, uh, the activation is K divided by 16 is instead of taking the average, right, for that [inaudible]. Yeah. [NOISE] Okay, let's move on and watch a cool video on how act- class activation maps work. This video was from Kyle McDonald. And it's, uh, it's live so it's very quick. So, you can see that the network is looking at this speed boat. Okay. So now, the three methods we've seen are methods that are roughly mapping back the output to the input space and helping us visualize which parts of the inputs were the most discriminative to lead to this output and the decision of the network. Now we're going to try to delve more into details in the, in the, in the intermediate layers of the network and try to interpret how does the network see our world, not necessarily related to a specific input, but in general. Okay. So, the pet shop now trusts your model because you- you've used occlusion sensitivity, saliency maps, and class activation maps to show that the model is looking at the right place, uh, but they got a little scared when you did that. And they asked you to explain what the model thinks a dog is. So, you have this trained convolutional neural network and you have an output probability. Yeah, let me take one in the back. Yeah. Um, what are some good ways to visualize like non-image data? Non-image data that's a, that's a good question. It's actually, so the reason we're seeing images was mo- most of the resources being [NOISE] focusing on images, um, if you look at let's say time series data. So, either speech or natural language, the main way to realize those is, uh, with the attention method, uh, are you familiar with that? So, in the next set of modules that you're going to start this week and you will just study in the next two weeks you will see a visualization method called attention models, which will tell you which part of a sentence was important, let's say to output a number like assuming you're doing machine translation. You know some languages, they don't have a direct one to one mapping. It means I might say, uh, I love cats, but in another language maybe [NOISE] this same sentence will be cats I love or something near that, its fit. And you want an [NOISE] attention model to se- to show you that the cat was referring to the second. I think it's, it's, it's okay. Okay, sorry guys [NOISE]. [NOISE] So, going back to the presentation. Now, we're going to delve into- inside the network. And so the new thing is the pet shop is a little scared and asked you to explain what the network thinks a dog is. What's the representation of dog for the network? So, here, we're going to use a method that we've already seen together called gradient ascent, which is defining an objective, that is technically the score of the dog, minus a regularization term. What the regularization term is doing, is it's- it's saying that x should look natural. It's not necessarily L2 regularization, it can be something else, and we- we will discuss it in the next slide, but don't think about it right now. What we will do is we will compute the back-propagation of this objective function all the way back to the input, and perform gradient ascent to find the image that maximizes the score of the dog. So, it's an iterative process, takes longer than the class activation map. And we repeat the process, forward propagate x, compute the objective, back-propagate, and update the pixels and so on. You guys are familiar with that? So let's see what- what we can visualize doing that. So, actually, if you take an image net- classification network, and you perform this on the classes of goose or ostrich or kit fox, husky, dalmatians, you can see what the network is looking at or what the network thinks that dalmatian is. So, for the dalmatian, you can see some- some black dots on a white background, somehow, but these are- are still quite hard to interpret. It's not super easy to see and even worse here on the screen, better on your computers. But you can see a fox, some here, you can see orange color for the fox. It means that pushing the pixels to an orange color would actually lead to a higher score of the kit fox in the output. If you use a better regularization than L2, you might get better pictures. So, this is for flamingo, this is for pelican, and this is for hartebeest. So, a few things that are interesting to see, is that in order to maximize the score of flamingo, what the network visualized is many flamingos. It means that's 10 flamingo leads to a higher score of the class flamingo than one flamingo for the network. Talking about regularization, what does L2 regularization say? It says that for visualizing, we don't want to have extreme values of pixel. It doesn't help much to have one pixel with an extreme value, one pixel with a low value and so on. So, we're going to regularize all the pixels so that all the values are around each other, and then we can re-scale it between zero and 20- 255 if you want. One thing to notice is that the gradient ascent process doesn't constrain the inputs to be between zero and 255. You can go to plus infinity potentially, while an image is stored with numbers between zero and 255, so you might want to clip that as well. This is another type of regularization. One thing that led to beautiful pictures was what Jason Yosinski and his team did is, they forward propagated an image, computed the score, computed the objective function, back-propagated, updated the pixels, and blurred them, blurred the picture. Because what- what is not useful for visualizing, is if you have high frequency variation between pixels, it doesn't have to visualize, if you have many pixels close to each other that have many different values. Instead, you want to have a smooth transition among pixels, and this is another type of regularization called Gaussian blurring. Okay? So, this method actually makes a lot of sense in- in- in scientific terms. You're- you're maximizing an objective function that gives you what the network sees as flamingo, which would maximize the score of flamingo. So, we call it also class model visualization. Yes? So, does a more realistic class model, visualization correspond to a more accurate model? [NOISE] Um, does a more realistic class model visualization correspond to a more accurate. So, it's hard to map the accuracy of the model based on this visualization, but it's a good way to validate that the network is looking at the right thing. Yeah. We're going to- to see more of this later. I think the most interesting part is actually on this slide is, we- we did it for the class score, but we could have done it with any activation. So, let's say I stop in the middle of the network, and I define my objective function to be this activation. I'm going to back propagate and find the input that we maximize this activation. It will tell me what is this activation. What does this activation fire for? So, that's even more interesting I think than looking at the inputs and then the output. Does that make sense? That we could do it on any activation? Yep. [NOISE] Any questions on that? [NOISE] Okay. So, now, we're going to do another trick which is data-set search. It's actually one of the most useful, I think. Not fast, but very useful. So, the pet shop loved the previous technique, and asks if there are other alternatives to- to show what- what an activation in the middle of a network is thinking. You take an image, forward propagate it to the network, get your output. Now, what you're going to do is select a feature map, let's say this one, where at this layer, and the feature map is of size five by five by 256. It means that the CONV layer here had 256 filters, right? You are going to look at these feature maps and select probably, uh, yeah, what you're going to do is select one of the feature maps, okay? We select one out of 256 feature maps, and we're going to learn- run a lot of data, forward propagate it through the network, and look which data points have had the maximum activation of this feature map. So, let's say we do it with the first feature map. We notice that these are the top five images that really fired this feature map, like high activations on the feature map. What it tells us, is that probably this feature map is detecting shirts. Could do the same thing, let's say we take the second feature map, and we look which data points have maximized the activations of this feature map, out of a lot of data. And we see that this is what we got, the top five images. Probably means that the other feature map seems to be activated when seeing edges. So, the second one is much more likely to appear earlier in the network obviously than later. So, one thing that you may ask is, do these images seem cropped? Like I don't think that this was an image in the data-set, it's probably a subpart of the image. What do you think this crop corresponds to? [NOISE] Any idea how we cropped the image, and why these are cropped? [NOISE] Like, why- why didn't I show you the full images? How was I able to show you the cropped? [NOISE]. [inaudible] and so that anything outside is not [inaudible] That's correct. So, let's say we pick an activation, an activation in the network. This activation for a convolutional neural network oftentime doesn't see the entire input image. Right? Doesn't see it. What it sees is a subspace of the input's image. Does that make sense? So, let's look at another slide. Here, we have a picture of units, 64 by 64 by 3. It's our input. We run it through a five-layer ConvNet. And now, we get an encoding volume that is much smaller in height and width, but bigger in depth. If I tell you what this activation is seeing. If you map it back, you look at the stride and the filter size you've used, you could say that this is the part that this filter is seeing. This- this-, uh, this activation is seeing. It means the pixel that was up there had no influence on this activation, and it makes sense when you think of it. You're- you're- the- the easiest way to think about it is looking at the- the top picks, the- the- the top entry on the encoding volume, top-left entry. You have the input image, you put a filter here. This filter gives you one number, right? This number, this activation only depends on this part of the image, but then if you add a convolution after it, it will take more filters. And so, the deeper you go, the more part of the image the activation will see. So, if you look at an activation in layer 10, it will see much- a much larger part of the input than an activation in layer one. That makes sense? So, that's why- that's why probably the pictures that I showed here, these ones are very small part cro- crops, small crops of the image, which means the activation I was talking about here is probably earlier in the network. It sees a much smaller part of the input. [inaudible]  [NOISE] Yeah, yeah. So, what you look at it which activation was maximum. You look at this one and then you match this one back to crop. Does that make sense? Okay, so here's units again, up and same, this one would correspond more in the center of the image. This intuition makes sense? Okay cool. So, let's talk about deconvolution now. This is gonna be the hardest part of the lecture, but probably helping with- with more intuition on deconvolution. You remember that? That was the generative adversarial networks scheme. And we said that giving a code to the generator, the generator is able to output an image. So, there is something happening here that we didn't talk about. Is how can we start with a 100 dimensional vector and output a 64 by 64 by 3 image? That seems weird. We could use, you might say, a fully connected layer with a lot of neurons, right, to up-sample. In practice, this is one method, another one is to use a deconvolution network. So, convolutions will encode the information in a smaller volume in height and width deeper in- in depth, while the deconvolution will do the reverse. It will up-sample the height and width of an image. So, that would be useful in this case. Another case where it would be useful is segmentation. You remember our case studies, uh, for segmentation life cell, microscopic images of cells. Give it to a convolution network. It's going to encode it. So, it's going to lower the height and width. The interesting thing about this encoding in the middle is that it holds a lot of meaningful information. But what we want ultimately, is to get a segmentation mask, and the segmentation mask in height and width has to be the same size as the pixel image. So we need a deconvolution network to up-sample it. So, deconvolution are used in these cases. Today the case we're going to talk about is visualization. Remember the gradient ascent method we talked about. We define an objective function by choosing an activation in the middle of the network, and we want the objective to be equal to this activation to find the input image that maximizes its activation through an iterative process. Now, we don't want to use an iterative process. We want to use a reconstruction of this activation directly in the input space by one backward path. So, let's say I select this feature map out of the max pool, 255, sorry, 5 by 5 by 256. What I'm going to do is, I'm going to identify the max activation of this feature map. Here it is. It's this one, third column second row. I'm going to set all the others to zero. Just this one I keep it, because it seems that this one has detected something. Don't wanna talk about the others. I'm going to try to reconstruct in the input space what this activation has fired for. So, I'm going to compute the reverse mathematical operation of pooling, ReLU, and convolution. I will unpool, I will un-ReLU, let's say, doesn't ex- this word doesn't exist, so don't use it. But un-ReLU and deconv. And I will do it several times because this activation went through several of them. So I will do it again and again until I see, oh, this specific activation that I selected in the feature map fired because it saw the ears of the dog. And as you see, this image is cropped again. It's not the entire image, it's just the part that the activation has seen. And if you look at where the activation is located on the feature map, it makes sense that this is the part that corresponds to it. So now, the higher level intuition is this. We are going to delve into it and see what do we mean by unpool, what do we mean by Un-reLU, and what do we mean by de-conv. Okay. Yes. So, if we had [inaudible]. Would we have just gotten a reconstruction of the whole image? So, the difference is, you mean if we don't zero out all the activations? It shows that this reconstruction would be messier. It would be more messy. [NOISE] Yeah. Doesn't, doesn't necessarily mean you will not get the full image, because probably the other activations probably didn't even fire, means they didn't detect anything else. It's just that it's gonna- it's gonna add some noise to this reconstruction. Okay, so let's talk about deconvolution a little bit on the board. [NOISE] So, to start with deconvolution, and you, you guys can take notes if you want. We are going to spend about 20 minutes on the board now to discuss deconvolution, okay? [NOISE] To understand the deconvolution, we first need to understand deconvolution. We've seen it, uh, from a computer science perspective, but actually, what we are going to do here is we are going to frame deconvolution as a simple matrix vector mathematical operation. You're going to see that it's actually possible. So let's start with a 1D conv. For the 1D convolution, I will take an input x which is of size 12, x1, x2, x3, x4, x5, x6, x7, x8. So, 8 plus 2 padding, which gives me the 12 that I mentioned. So, the input is a one-dimensional vector which has padding of two on both sides. I will give it to a layer that will be a 1D conv. And this layer would have only one filter. And the filter size will be four. We will also use a stride equal to two. [NOISE] So, my first question is, what's the size of the output? Can you guys compute it on your- on your notepads and, and tell me what's the size of the output. [NOISE]. Input size 12, [NOISE] filter of size four, stride of two, padding of two. Five, yeah I heard you, yeah. So, remember use nx, sorry, ny equals nx minus f plus 2p divided by stride and you will get five. So, what I'm going to get is Y1, Y2, Y3, Y4, Y5. [NOISE] So, I'm going to focus on this specific convolution for now. And I'm going to show now that we can define it as, as a mathematical operation between a matrix and a vector. So, the way to do it is, I guess the easiest way is to write the system of equation that is underlying here. What is Y1? Y1 is the filter applied to the four first values here. This makes sense? So, if I define my filter as being y W1, W2, W3, and W4, what I'm gonna get is that Y1 equals W1 times zero plus W2 times zero plus W3 times x1 plus W4 times x2. This makes sense? Just the convolution, elementwise operation, and then sum all of it. Y2 is going to be same thing, but with a stride of two, going two down. So, it's going to give me W1 times x1 plus W2 times x2 plus W3 times x3 plus W4 times x4. Correct? Everybody is following? No. Same thing. We will do it for all the y's until Y5, and we know that Y5 is elementwise operation between the filter and the four last number here, summing them. So, it will give me W1 times x7 plus W2 times x8 plus zero plus W3 times zero plus W4 time zero. [NOISE] Okay. Now what we're going to do is to try to write down y as the matrix vector operation between w and x. We need to find what this w matrix is. And looking at this system of equation, it seems that it's not impossible. So let's try to do it. I will write my Y vector here, Y_1, Y_2, Y_3, Y_4, Y_5. And I will write my matrix here and my vector x here. So first question is, what do you think will be the shape of this w matrix? Um? 5 by 12. 5 by 12. Correct. We know that this is 5 by 1, this is 12 by 1, so of course w is going to be 5 by 12. Right? So, now, let's try to fill it in 0, 0, x_1, x_2, x_3, blah, blah, blah, x8, 0, 0. Can you guys see in the back or no? Yeah? Okay. Cool. Ah, so, I'm going to fill in this matrix regarding this system of equation. I know that the Y1 would be w_1 times 0, w_2 times 0, w_3 times x_1, w_4 times x_2. So this vector is going to multiply the first row here. So I just have to place my ws here. w_1 will come here, multiply 0, w_2 will come here, w_3 would come here, and w_4 would come here. And all the rest would be filled in with 0s, right? I don't want any more multiplications. How about the second row of this matrix? I know that Y_2 has to be equal to this dot product with this row. And I know that it's going to give me w_1x_1 plus w_2x_2 plus w_3x_3. x_1 is the third input on this vector, third- third entry. So, I would need to shift what I had in the previous row with a stride of two, it will give me that. That makes sense? So if I use the dot product of this row with that, I should get the second equation up there. And so on and you understand what happens, right? This pattern will just shift with the stride of two on the side. So, I would get zeros here and I will get my w_1, w_2, w_3, w_4 and then zeros. And all the way down here. And all the way down here, what I will get is w_4, w_3, w_2, w_1 and zeros. So the only thing I wanna mention here is that the convolution operation as you see can be framed as a simple matrix times a vector. Yes. So why did you have zeros in- on the right side of the top row, in the left side, that's when multiplying the- [NOISE] For the top row, why the zeros are on the right side? Yes. Because I don't want Y hat- Y_1 to be dependent on x_3 to x_8. So I want these to be zero multiplicate priors. Okay. Oh, because of the stride and the window size. Okay. Thank you. So why is this important for the intuition behind the deconvolution and the existence of the deconvolution? It's because if we manage to write down y equal wx, we probably can write down x equal w minus one, y if w is an invertible matrix and this is going to, to be our deconvolution. And in fact, what's the, what's the shape of this new matrix? 12 by 5. Um? 12 by 5. Yes. 12 by 5. We have 12 by 1 on one side, 5 by 1 on the other, it has to be 12 by 5. So it's flipped compared to w. So one thing we're going to do here is we're going to make an assumption. First assumption is that w is an invertible matrix. And on top of that, we're going to make a stronger assumption which is that w is an orthogonal matrix. And without going into the details here, same as when we proved Xavier initialization in sections, we made some assumptions that are not always true. This assumption is not going to be always true. One, one intuition that you can have is, if I'm using a filter that is, ah, assume the filter is an edge detector. So like, ah, plus one, zero, zero, minus one. In this case, the matrix would be orthogonal. Why? A matrix that is orthogonal means that if I take two of the columns here, I dot-product them together, it should give me zero. Same with the rows, you can see it. So, what's interesting is that, ah, if the stride was four, there will be no overlap between these two rows. It would give me an orthogonal matrix. Here a stride is two but if I replace this w_1 by minus one, zero, zero, plus one, so plus one, zero, zero, minus one and minus, plus one, zero, zero, minus one, you can see that the dot product would be zero. The zeros will multiply the ones and the ones will multiply the zeros, it gives me a zero dot product. So, this is a case where it works. Practices doesn't always work. The reason we're making this assumption is because we wanna make a reconstruction, right? So, we wanna be able to have this w minus one, this, this, this invert and the reconstruction is not going to be exact. But at, at first-order approximation, we can assume that the reconstruction will still be useful to us, even if this assumption is not always true. In the case where w is orthogonal, I know that the inverse of w is w transpose. Or another way to write it, is that for orthogonal matrices, w transpose times w is the identity matrix. So, what it tells me is that x is going to be w transpose time y, times y. So, let's see what we get from that. Let me write down the Menti code. So, let's say now we have our x and we wanna regenerate our, or we will have our y and we want to generate our x using this method. So, I would, what I would write is to understand the 1D deconv. We can use the following illustrations, where we have x here, which is zero, zero, x_1, x_2, x_3, all the way down to x_8. Okay? And I will have my w matrix here, w transpose and my Y vector, Y_1, Y_2, Y_3, Y_4, and Y_5 here. And so, I know that this matrix will be the transpose of the one I have here, right? So, I can just write down the transpose. The transpose will be w_1, w_2, w_3, w_4. Okay? I will shift it down with a stride of two and so on. [NOISE] And this whole thing will be W Transpose. So, th- the small issue here is that this in practice is not- is going to be very similar to a convolution, but because, uh, but it's going to be a tiny little different interval of implementation. Another question I might ask is, how can we do the same thing with the same pattern as we have here? It means the stride is going from left to right, instead of going from up to down. I'm going to introduce that with a technique called sub-pixel convolution. And for those of you who read papers and segmentation in visualization, oftentimes this is a type of convolution that is used for reconstruction. So, let's see how it works. I just wanna do the same operation, but instead of doing it with a strike going from up to down, I want to do it from a strike going from left to right. O- one, one thing you wanna, you wanna notice here, is that, uh, the two lines that I wrote here are cropped. And the reason is because we're using a padded input. Here, we will just crop the two top lines. And same for the two last lines. They will be cropped. Look at that. W1 will multiply Y1, and this one will multiply Y2 and so on. So, this dot product will give me W1 times Y1, but I don't want that to happen because I wanna get the padded zero here. So, I will just crop that. In this matrix it's actually going to be smaller than it seems, and is going to generate my X1 through X8 and then I will pad the top values and the bottom values. Okay, just the height. So, let's look at the sub-pixel convolution. I have my input. And I will do something quite fun. I will perform a sub-pixel operation on Y. What does it mean? I will insert zeros almost everywhere. I will insert them, and I will get 0, 0, Y1, 0, Y2, 0, Y3, 0, Y4, 0, Y5 and 0, 0. Even more, one more 0 here, one more 0 here. So, this vector is just the vector Y with some zeros inserted around it and also in the middle between the elements of Y. Now, why is that interesting? It's interesting because I can now write down my convolution by flipping my weight. [NOISE] So, let me explain a little bit what happened here. What we wanted is, in order to be able to efficiently compute the deconvolution the same way as we've learned to compute the convolution. We wanted to have the weights scattered from left to right with a stride moving from left to right. What we did, is that we used a sub-pixel version of Y by inserting zeros in the middle, and we divided the stride by two. So, instead of having a stride of two as we had in our convolution, we have a stride of one in our deconvolution. So, notice that I shift my weights from one at every step, when I move from one row to another. Second thing is, I flipped my weights. I flipped my weights. So, instead of having W1, W2, W3, W4, now I have W4, W3, W2, W1. And what you could see is looking at that, first, look at this row, the first row that is not cropped. The result of the dot product of this row with this vector is going to be Y1 times W3, plus Y2 times W1. Yeah? Now, let's look what happened here. I look at my first row here, the dot product of this first row with my Y here is going to be- sorry, sorry, we- these two are cropped as well. And same here. So, looking at my first non-cropped row here as a dot product with this vector what I get is W3 times Y1, plus W2- sorry, plus W1 times Y2. So, exactly the same thing as I got there. So, these two operations are exactly the same operations. They're the same thing. You get the same results two different way of doing it. One, is using a weird operation with strides going from top to bottom. And the second one is exactly a convolution. This is a convolution. Convolution plus flipped weights, insertion of zeros for the sub-pixel version of Y. And on top of that, padding here and there. So, this was the hardest part. Okay? Does it give you more intuition on the convolution here? You know now how convolution can be framed as a mathematical operation between a matrix and a vector. And you know also that under these assumptions, the way we will deconvolve is just by flipping our weights, dividing the stride by two, and inserting zeros. If we just do that, we're deconvolving. For propagating the convolution, the following way you wanna deconvolve, just flip all the weights, insert zeros sub-pixel, and finally divide the stride. And that's the de-convolution. So, super complex thing to understand but this is the intuition behind it. Now, let, let's try to have an intuition of how it would work in two-dimension. Uh, let me write it down. The sub-pixel convolution, we already have that [inaudible] [NOISE] Why do we use that? Yeah. Because in terms of implementation this is the same as what we've been using here. It's, it's very similar, while this one is another implementation. So, you could do both the same, is the same operation. But in practice this one is easier to understand because it, it's exactly the same operation of the convolution, with flipped weights, insertion of zeros and divided stride. That's why I wanted to show that. Yeah. So, uh, what, what happens when, uh, the assumption [OVERLAPPING]. When the ass- assumption doesn't hold? Yeah. So, oftentimes the assumption doesn't hold, but what we want is to be able to see a reconstruction. And if we use this method we will still see a reconstruction. Practice if we had really W minus one, the reconstruction would be much better. But we don't. So, uh, let me go over the 2D, uh, the 2D example. We are going to go a little over time because we have two hours technically for- one hour and 50 minutes, and uh, and let me go over the 2D example. And then we will answer this question on why we need to make this assumption. So, here is the interpretation of the 2D deconvolution. Let me write it down here. [NOISE] The intuition behind the 2D deconv is, I get my inputs. Which is five by five, and this I call it x. I forward propagate it using a filter of size two-by-two, in a conv layer, and a stride of two. This is my convolution. What I get. So, if you do five minus two, plus the padding which is zero, divided by two, plus one, oh, I forgot the plus one actually here, plus one and you floor it. So- so, five minus two divided by two gives you, uh, three divided by two plus one. Um, no actually it will give you three by three, yeah, three by three. A y of three by three. That's what you get. And now, this you call it y. What you're going to do here, is you're going to deconvolve y. In order to deconvolve y, in order to deconvolve it, you're going to use a stride of one. And what we said is that we need to divide this stride by two, right? So, we need a stride of one, and the filter will be the same, two-by-two. And you remember that what we've seen, is that the filter is the same. It's just that it's going to be flipped. So, you will use a filter of two-by-two, but flipped. And now, what do we get? We hope to get a five-by-five input, which is going to be our reconstructed x, five-by-five input. And the way we're going to do it, is this is the intuition behind it. Yeah. Is it up two by two? [NOISE]. Five minus two divided by two. Yeah, it's two by two. Okay. Up two by two. Thanks . [OVERLAPPING]. Two by two. Five-by-five here. That's what we hope to reconstruct. The way we will do it, is we will take the filter, s is two by two. We will put it here. And we will multiply all the weights of this filter by y11. All the weights will be multiplied by y11. So, I will get four values here, which are going to be w4 y111, w3 y111 and so on. Now, I will shift this with a stride of one. And I will put my filter again here. And I will multiply all the entries by y12 and so on. And you see that this entry has an overlap. So, it will, it will be updated at every step of the convolution. It's not like what happened in the forward pass. So, this is the intuition behind the 2D convolution. 3D, same thing. You have, uh, a volume here. So, your filter is going to be a volume. What you're going to do is you're going to put the volume here, multiply by y11 and so on. And then if you have a second filter, you would put it again on top of it and multiply by y11 all the weights of the filter and so on. It's a little complicated, but this is the intuition behind deconvolution. Okay, let's get back to the lecture. I'm going to take one question here if you guys need clarification. [NOISE] Don't worry if you don't understand deconvolution truly. The important part is that you get the intuition here and you understand how we do it. So, let me make a comment. [NOISE] Why do we need to make this assumption and do we need to make it? [NOISE] When we want to reconstruct [NOISE] like we're doing here in the visualization, we need to make this assumption because we don't want to retrain weights for the deconvolutional network. What we know is that the activation we selected here on the feature map is- has gone through the entire pipeline of the ConvNet. So, to reconstruct, we need to use the weights that we already have in the ConvNet. We need to pass them to the deconvolution and reconstruct. If we're doing the segmentation, like we talked about for the live cell we don't need to do this assumption. We're just saying that this is a procedure that is a deconvolution, and we will train the weights of the deconvolution. So, there is no need to make this assumption, it's just we have a technique that is dividing the stride by one and inserting zeros and then beam, we retrain the weights and we get an output that is an upsampled version of the input that was given to it. So, there's two use cases. One where you use the weights and one where you don't. In this case, we don't want to retrain, we wanna use the weights. So let's see. Let's see a- a version more visual of the upsampling. So, we do the sub-pixel image. This is my image, four by four, I insert zeros and I pad it, I get a nine by nine image. I have my filter like that. And this filter will convolve. I will- it will convolve over the input, so I will place it on my input, and at every step I will perform a convolution up. I will get a value here. The value is blue because as you can see the weights that affected the output were only the blue weights. I would use a stride of one beam. Now, the weights that affect my input are the green ones and so on. And I would just convolve as I do usually, and so on. And now one step down. I see that the weights that are impacting my input are the purple ones. So, I would put a purple square here and so on. So, I just do the convolution like that. And so on, so one thing that is interesting here is that the values that are blue in my out six by six output, were generated only using the blue values of the filter, the blue weights in the filter. The ones that are green were only used-were only generated using the green values of my filter. So, actually this subsample- sub-pixel convolution or deconvolution could have been done with four convolutions, with the blue weights, green weights, purple weights and yellow weights. And then, just- just replace such that the adjustments would be the output. Just put the output of each of these conv and mix them to give out a six by six output. Only thing you need to know we have an input four by four and we get an output six by six. That's what we wanted. We wanted to upsample the image. We can retrain the weights or use the transposed version of them. So, let's see what happens now. We understood what, uh, what deconv was doing. So, we're able to deconv. What we need to do is also to unpool and to unReLU. Fortunately, it's easier than the deconv. So, we're not gonna do board work anymore. So, let's see how unpool works. If I give you this, uh, inputs to the pooling- to a max pooling layer. The output is obviously going to be this one, 42 is the maximum of these four numbers. Assuming we're using a two-by-two filter with stride of two, vertically and horizontally, 12 is the maximum of the green numbers, six is the maximum of the red numbers and seven the- the orange ones. Now, question. I give you back the outputs and I tell you, give me the input. Can you give me the input or no? No. No, why- why? [NOISE] You only keep the maximum. So, you- you lost all the other numbers. I don't know anymore the zero, one and minus one that were the red numbers here because they didn't pass through the maximum. So, max pool is not invertible, from a mathematical perspective. What we can do is approximate its invert. How can we do that? [NOISE]. Spread it out. Spread it out. That's a good point, we could spread out the six among the four values. That would be an approximation. A better way if we manage to cache some values, is to cache something we call the switches. We cache the values of the maximum, using a matrix that is very easy to score, of zeros and ones. And we pass it to the unpooling. And now we can approximate the inverts, because we know where 6 was, we know where 12 was, we know where 42 was and 7 was. But it's still not invertible because we- we lost all the other numbers. Think about maxpool back propagation. It's exactly the same thing. These numbers 0, 1, -1. They had no impact on the loss function at the end, because they didn't pass through the forward propagation. So, actually with the switches you can have the exact back propagation, we know that the other values are going to be zeros, because they didn't affect the loss during the forward propagation. That- that make sense? Okay. So, this is maxpooling, unpooling, unmaxpooling. And we can use it with the switches. We can approximate it. Why not just cache the whole regional matrix? Yeah, why don't we just cache the whole region there. We could- could cache the entire thing. But in terms of back- for back propagation in terms of efficiency we will just use this switching because it's enough. But not for unpooling though. Yeah, yeah, for unpooling you're right, we could cache everything. But then it's cheating, like you- you kept it, it's like, just give it back. Okay. So now, we know how [NOISE] unpooling works. Let's look at the ReLU. So, what we need to do, in fact, is to pass the switches and the filters back to the unpooling deconv in order to reconstruct. Switches are the matrix of zeros and ones indicating where the maximums were, and filters are the filters that I will transpose under this assumption on the board. Okay. And so on and so on, and I get my reconstruction. I just need to explain the ReLU now. I give you this input to ReLU and I forward propagate it. What do we get? All the negative numbers are going to be equalized to zero, and the others are going to be kept. Now, let's say I'm doing a backpropagation [NOISE] through ReLU. What do I get if I give you that? This is the gradients that are coming back, and I'm asking you what are the gradients after the ReLU during the backpropagation? [NOISE] How does the ReLU behave in backprop? [NOISE]. Zeros? [NOISE] Which ones are zero? Um, the negative. The negative are zeroes? Do you agree? The negatives in this yellow matrix are going to be zeros during the backprop. Are you guys sure? [NOISE] Think always about what was the influence of the input on the loss function and you will find out what was the backpropagation. Look at this number. This number here, -2. Did this number have, the fact that it was -2, did it have any influence on the loss function? No, it could have been -10, it could have been -20. It's not gonna impact the loss function. So, what do you think should be the number here? Zero. Zero. Even if the number that is coming back, the gradient is 10. So, what do you think should be the ReLU backward output? [NOISE] Same idea as max-pooling. What we need to do is to remember the switches. Remember which of these values had an impact on the loss. We pass the switches, all these values here that are kind of a y, you know this is a y. All these ones had no impact on the loss function. So, when you backpropagate, their gradient should be set to zero, doesn't matter to update them. It's not gonna make the loss go down. So, these are all zeros and the rest they just pass. Why do they pass with the same value? Because ReLU for positive numbers was 1. So, this number 1 here that passed the ReLU during the forward propagation, it was not modified. Its gradient is going to be 1. That makes sense? So this is ReLU backward. Now, in this reconstruction method, we're not going to use ReLU backward. We're going to use something we call ReLU DeconvNet let's say. The reason we're not, the intuition between why we're not using ReLU backward is because what we're interested in is to know which pixels of the input positively affected the, the activation that we're talking of. So, what we're going to do is that we're just going to do a ReLU. We're just going to do a ReLU backward. Another reason is when we reconstruct, we wanna have the minimum influence from the forward propagation because we don't really want our reconstruction to depend on the forward propagation. We would like our reconstruction to be unbiased and just look at this activation, reconstruct what happened. So, that's what you're going to use. Again, this is a hack that has been found through trial and error and it's not going to be scientifically viable all the time. Okay. So now, we can do everything and we can reconstruct and find out what was this activation corresponds to. It took time to understand it, but it's super fast to do now, It's just one pass, not iterative. We could do it with every layer. So, let's say we do it with the first block of conv, ReLU, maxpool. I go here. I choose an activation. I, I, I, I find the maximum activation. I set all the others to 0. I unpool, ReLU, deconv and I find out the reconstruction. This specific activation was looking at edges like that. So, let's delve into the fun and see how we can visualize inside, what's happening inside the network. So, all the visualization we're going to see now can be found in Matthew Zeiler's and Rob Fergus' paper Visualizing and Understanding Convolution Networks. I'm going to explain what they correspond to, but check, check out their papers if you want to understand more into detail. So, what happens here is that on, on the top left, you have nine pictures. These are the cropped pictures of the data set that activated the first filter of the first layer maximum. So, we have a first filter on the first layer and we run all the data sets and we recorded what are the main pictures that activate this filter. These were the main ones. And we did the same thing for all the filters of the first layer and there are nine times nine of them. There are a lot of them, I think. In the bottom here you have the filters, which are the weights that were plotted. Just take the filter, plot the weights. This is th- this is important only for the first layer. When you go deeper into your network, the filter itself cannot be interpreted. It's super hard to understand it. Here, because the weights are directly multiplying the pixels, the first layer weights can be interpretable. And in fact, you see that the, let's look at the third one, the third filter here on the first row. The third filter has weights that are kind of diagonal, like one of the diagonals. And in fact if you look at the datas that maximized these filters' activation, the feature map corresponding to this filter, they're all like cropped images that correspond to diagonals. That's what happens. Now, the, the deeper we go, the more fun we have. So let's go. Results on a validation set of 50,000 images. What's happened here is they took 50,000 images, they forward propagated to the network. They recorded which image is the maximum, the one that's maximized the activation of the feature map corresponding to the first filter of layer two, second filter and so on for all the filters. Let's look at one of them. We can see that's, okay, we have a circle on this one. It means that this, the filter gener- which generated the feature map corresponding, uh, [NOISE] to this has been activated through probably a wheel or something like that. So, that the image of the wheel was the one that maximized the activation of this one and then we use the deconv method to reconstruct it. Any questions on that? Yeah. What if the activation function is not ReLU [inaudible]. Good question, yeah. What if the activation function is not ReLU? In practice, you would just use a backward to reconstruct if it's [inaudible]. You would use the same, the same type of method and you would try to approximate the reconstruction. Okay, let's go a little deeper. So now, same layer two, forward propagate all the images of the data set, find the nine images that are the maximum activate- that lead to the maximum activation of the first filter. These are plotted on top here. What you can see is like for this filter, that is the sixth row first filter, features are more invariant to small changes. So, this filter actually was activated to many different types of circles, spirals, whirls, and so it's, it's still activated although the circles were different sized. Can go even deeper up third layer. What's interesting is that the deeper you go, the more complexity you see. So, at the beginning we were seeing only edges, now we see much more complex figures. You can see a face here, in this- in this entry. It means that this filter activated for when it sees this- when it has seen a data point that had this face, then we reconstructed it, cropped it on the face. Uh, the face is kind of red, it means that the more red it was, the more activation it led to. And same top nine for layer three. So, these are the nine images that actually led to the face. These are the nine images that maximize the, the, the activation of the feature map corresponding to that filter and so on. So, here is a very funny. [inaudible]  [NOISE]. Can you stand up? [NOISE]. And realization layers, we can switch back and forth between showing the actual activations and showing images synthesized to produce high activation. So, he's- he's giving his own image to the network right now. By the time we get to the fifth convolutional layer, the features being computed represent abstract concepts. So, these are the gradients I said. [OVERLAPPING] For example, this neuron seems to respond to faces. We can further investigate this neuron by showing a few different types of information. First, we can artificially create optimized images using new regularization techniques that are described in [OVERLAPPING]. Our paper, the one we talked about. These synthetic images show that this neuron fires in response to a face. [OVERLAPPING] It also taught the images from the training set to activate this neuron the most as well as pixels from those images most responsible for the high activations computed via the deconvolution. And this is the deconvolutionary substance. This feature responds to multiple faces in different locations. And by looking at the deconv, we can see that it would respond more strongly if we had even darker eyes and rosier lips. We can also confirm that it cares about the head and shoulders, but ignores the arms and torso. We can even see that it fires to some extent for cat faces. Using back-prop or deconv, we can see that this unit depends most strongly on a couple of units in the previous layer conv4, and about a dozen or so in conv3. So they're trying to track back track where- which neurons led to [OVERLAPPING]. So, let's look at another neuron on this screen. So, what is this unit doing? From the top nine images, we may conclude that it fires for different types of clothing, but examining the synthetic images shows that it may be detecting not clothing per se, but wrinkles. In the live plot, we can see that it's activated by my shirt and smoothing out half of my shirt causes that half of the activations to decrease. Finally, here's another interesting neuron. This one has learned to look for printed text in a variety of sizes, colors, and fonts. This is pretty cool because we never asked the network to look for wrinkles or text or faces. The only labels we provided were at the very last layer. So, the only reason the network learned features like texts and faces in the middle was to support final decisions at that last layer. For example, the text detector may provide good evidence that a rectangle is in fact a book seen on edge and detecting many books next to each other might be a good way of detecting a bookcase, which was one of the categories we trained the net to recognize. In this video, we've shown some of the features of the DeepViz toolbox and a few of the things we've learned by using it. You can download it. Yeah, so they had a toolbox, which is exactly what you visualize here, and you could test the toolbox on your model, takes time to- to get- get it to run, but- but if you want to visualize all the neurons, it's very helpful. Okay. So, uh, let's go quickly. We'll spend about three minutes on the optional Deep Dream one because it's fun. And yeah, feel free- feel free to jump in and ask questions. So, the Deep Dream one is, uh, is implemented by Google, and, uh, the page- the- the blog post is by Alexander Mordvintsev. The idea here is to generate parts using this knowledge of visualization and how they do that is quite interesting. They would take an input, forward propagate it to the network and at a specific layer that we call the- the green layer, then pick activation and set the gradient to be equal to this activation. The gradient at this layer and then we back propagated the gradients to the input. So, earlier what we did is that we defined a new objective function, that was equal to an activation and we tried to maximize this objective function. Here, they- they're doing it even stronger. They take the activations and they set the gradients to be equal to the activations. And so the stronger the activation, the stronger it's going to become later on, and so on and so on, and so on. So, they are trying to see what the network is activating for and in- increase even this activation. So, forward propagate the image, set the gradient of the dreaming layer to be equal to its activation, but back propagate all the way back to the inputs and update the pixel of the image. Do that several time and every time the activations will change. So, you have to set again the new activations to be the- the- the gradients of the green layer and back propagate, and ultimately, you will see things happening. So, it's hard to see here on the screen, but you would have a pig appearing here. You'd have like a tree somewhere there, and some animals, and a lot of animals are going to start appearing in this cloud. It's interesting because it means, let's say, you see this cloud here? If the network thought that this cloud looked a little bit like a dog, so one of the- the feature maps was- which would be generated by the filter that detects dog would activate itself a little bit. Because we set the gradient to be equal to the activation, it's going to increase the appearance of the dog in the image and so on. And then you will see a dog appearing after a few iterations. So, it's quite fun and if you zoom you see that type of thing. So, you see a pig-snail, it's kind of a pig with the snail carapace. Camel-bird, dog- dog-fish. I'd advise you to like look at this on the slides rather than on the screen, but it's quite fun. And same, like if you give that type of image, you would see that- because the network thought there was like a tower a little bit, you will increase the network's confidence in the fact that there is a tower by changing the- the image and the tower would come out. And so on, it's quite cool. Uh, yeah and if you're dreaming lower layers, obviously you will see edges happening or patterns coming. Because then the lower layers seem to detect an edge and then you will increase its confidence on its edge so it will- it will create an edge on the image. This is a fun video I have, Deep Dream on a video. [MUSIC]. So, everything that the metric thinks is something it knows with the information it appears to be. [MUSIC] And what's funny is that there is so many animals in the video. And the reason is [MUSIC]. Gets too trippy, I'm going to stop it. [LAUGHTER] So, one- one insight that is fun about it is, uh, if the network and this is not only for Deep Dream, it's also for- it's mostly for gradient ascents. Let's say we have an output score of a dumbbell, and we define our objective function to be the dumbbell score, and we try to find the image that maximizes a dumbbell when we'll see something like that. What's interesting is that the network thinks that the dumbbell is the hand with the dumbbell. Not only the dumbbell. And you can see it here, you see the hands. And the reason is it has never seen a dumbbell alone. So, probably in ImageNet there is no picture of a dumbbell alone in a corner and labeled as dumbbell. But instead, it's usually a human trying to push hard. Okay. So, just to summarize what we've learned today, we are now able to answer all the following questions. What part of the input is responsible for the output beam, occlusion sensitivity, class activation maps seem to be the best way to go. What is the role of a given neuron feature layer? Deconvolve, reconstruct, search in a dataset? What are the top images and do gradient ascents? Check- can we check what the network focuses on? Occlusion sensitivity, saliency map, class activation maps? How does the network see our world? I would say gradient descent, maybe Deep Dream is cool stuff. And then what are the- the implication and- and use cases of these visualizations? Uh, you can use saliency maps to segment, it's not very useful given the new methods we have. But the deconvolution that we've seen together is widely used for segmentation and reconstruction. Also for generating adversarial networks to generate images in parts sometimes. Uh, these visualizations are also helpful to detect if some of the neurons in your network are dead. So, let's say you have a network and you use the toolbox and you see that whatever the input image you give, some feature maps are always dark. It means that the feature that generated this feature map by convoluting over the inputs probably never detected anything. So, it's not being even trained. That's a type of insight you can get. Okay, thanks guys. Sorry we went over time. [NOISE] 

Okay. Hey, everyone, looks like we're on. So as usual, if you have not yet, um, please enter your SUID so that we know you're here in this room. Um, so actually, can you hear me okay at the back? Is it okay? Oh, yes, is the volume okay at the back? All right. No one's responding. Yes, okay. All right. [LAUGHTER] Thank you. Okay. So, um, what I want to do today is, um, share with you two things. You know, we're approaching the end of quarter. Uh, I hope you guys are looking forward to, to the Thanksgiving break, um, next week. Um, actually and I guess we have a lot of home viewers, but those us- those of you that are viewing this from outside California, know that we're all feeling really bad air here in California. So I hope, if you're somebody watching at home you have better air wherever you are. Um, uh, but, uh, what I hope to do today is give you some advice that will set you up for the future, uh, so if even beyond the conclusion of CS230. And in particular, what I want to do today is, um, share with you some advice on how to read research papers, uh, because, you know, deep learning is evolving fast enough that even though you've learned a lot of foundations of deep learning and learned a lot of tips and tricks and probably know better than many practitioners how to actually get deep, deep learning algorithms to work already. Uh, when you're working on specific applications whether in computer vision or natural language processing or speech recognition or something else, um, for you to be able to efficiently figure out the academic literature on key parts of, uh, the, the deep learning world, will help you keep on developing and, you know, staying on top of ideas even as they evolve over the next several years or maybe decade. So first thing I wanna do is, uh, give you advice on how, uh, when say, when I'm trying to master a new body of literature, how I go about that and hope that those techniques would be useful to help you be more efficient in how you read research papers. And then a second thing is, in previous offerings of this course, one request from a lot of students was just advice for navigating a career in machine learning. And so in the second half of today, I want to share some thoughts with you on that. Okay, so it turns out that- so I guess two topics reading research papers, right? Um, and, uh, then second career advice in machine learning. So it turns out that, uh, you know, reading research papers is one of those things that a lot of P- PhD students learn by osmosis, right? Meaning that if you're a PhD student and you see, you know, a few professors or see other PhD students do certain things, then you might try to pick it up by osmosis. But I hope today to accelerate your efficiency in how you acquire knowledge yourself from the, uh, from the a- academic literature, right? And so let's say that this is the area you want to become good at, let's say you want to build that, um, speech recognition, right? Let's turn this off now. Let's say you want to build that, um, speech recognition system that we talked about, the Robert turn on and the desk lamp. All right. Um, this is what I've read- this is the sequence of steps I recommend you take, uh, which is first: [NOISE] compile lists of papers and the- and by papers, I mean, both research papers often posted on arXiv,  onto the Internet, but also plus Medium posts, um, [NOISE] you know, what maybe some occasional GitHub post although those are rarer. But whatever texts or learning resources you have. And then, um, what I usually do is end up skipping around the list. All right. So if I'm trying to master a new body of knowledge, say you want to learn the most speech recognition systems, this is what it feels like to read a set of papers, which is maybe you initially start off with five papers and if on the horizontal axis, I plot, you know, 0 percent to 100 percent read/understood, right? The way it feels like reading these papers is often read, you know, ten percent of each paper or try to quickly skim and understand each of these papers. And if based on that you decide that paper number two is a dud, right, other, other, other authors even cite it and say boy they, they sure got it wrong or you read it, and it just doesn't make sense. Then go ahead and forget it. And, uh, as you skip around to different papers, uh, you might decide that paper three is a really seminal one and then spend a lot of time to go ahead and read and understand the whole thing. And based on that, you might then find a sixth paper from the citations and read that and go back and flesh out your understanding on paper four. And then find a paper seven and go and read that all the way to the conclusion. Um, but this is what it feels like as you, you know, assemble a list of papers and skip around and try to, uh, um, master a body of literature around some topic that you want to learn. And I think, um, some rough guidelines, you know, if you read 15 to 20 papers I think you have a basic understanding of an- of an area like, maybe good enough to do some work, apply some algorithms. Um, if you read, um, 50 to 100 papers in an area like speech recognition and, and kind of understand a lot of it, then that's probably enough to give you a very good understanding of an area, right? You might, know- I'm, I'm always careful about when I say you're mastering a subject but you read 50 to 100 papers on speech recognition, you have a very good understanding of speech recognition. Or if you're interested in say domain adaptation, right? By the time you've read 50 or 100 papers, you have a very good understanding of, of a subject like that. But if you read 5 to 20 papers, it's probably enough for you to implement it but maybe not, not sure this is enough for you to do research or be really at the cutting edge but these are maybe some guidelines for the volume of reading you should aspire to if you want to pick up a new area. I'll take one of the subjects in CS230 and go more deeply into it, right? Um, now [NOISE] how do you read one paper? And, um, I hope most of you brought your laptops. So what I'm gonna do is describe to you how I read one paper, and then after that I'm actually going to ask all of you to, you know, download the paper online and just take, I don't know, uh, uh, take, take a few minutes to read a paper right here in class and see how far you can get understanding of a research paper in just minutes right, right here in class. Okay. So when reading one paper. So the, the, the bad way to read the paper is to go from the first word until the last word, right? This is a bad way to- when you read a paper like this. Oh, and by the way, actually here, I'll tell you what my real life is like. So, um, I actually pretty much everywhere I go, whenever I backpack this is my actual folder. I don't want to show- this is my actual folder of unread papers. So pretty much everywhere I go, I actually have a paper, you know, a stack of papers is on my personal reading list. This is actually my real life. I didn't bring this to show you. This is in my backpack all the time. Ah, and I think that- these days on my team at Landing AI and Deeplearning.ai, I personally lead a reading group where I lead a discussion about two papers a week. Uh, but to select two papers, that means I need to read like five or six papers a week to select two, you know, to present and discuss at the Landing AI and Deeplearning.ai meeting group. So this is my real life, right? And how I try to stay on top of the literature and, and I have a- I'm doing a lot. If I can find the time, if I can find the time to read a couple of papers a week, hopefully all of you can too. Uh, but when I'm reading a paper, uh, this is, this is how I recommend you go about it which is, do- do- don't go for the first word and read until the last word, uh, instead, uh, take multiple passes through the paper [NOISE]. Right? Um, and so, you know, step one is, uh, [NOISE] read the title, [NOISE] the abstract, um, [NOISE] and also the figures. Um, especially in Deep Learning, there are a lot of research papers where sort of the entire paper is summarized in one or two figures in the figure caption. So, um, so sometimes, just by reading the title, abstract and, you know, the key neural network architecture figure that just describes what the whole papers are, and maybe one or two of the experiments section. You can sometimes get a very good sense of what the whole paper is about without, you know, hardly reading any of the texts in the paper itself, right? Tha- tha- that's the first pass. Um, second pass, I would tend to read more carefully, um, [NOISE] the intro, the conclusions, um, look carefully at all the figures again, [NOISE] and then skim, um, the rest, and you know, um, I- I don't know how many of you have published academic papers, but, uh, when people publish academic papers, um, part of, you know, the publication process is, uh, convincing the reviewers that your paper is worthy for acceptance. And so what you find is that the abstract, intro and conclusion is often when the authors try to summarize their work really, really carefully, uh, to make a case, to make a very clear case to the reviewers as to why, you know, they think their paper should be accepted for publication. And so because of that, you know, maybe slightly not, slightly unusual incentive, the intro and conclusion and abstract often give a very clear summary of what's the paper actually about. Um, and depending on, [NOISE] um, and again, just to be, you know, b- bluntly honest with you guys, um, the related work section is useful if you want, sometimes is useful if you want to- to understand related work and figure out what's- what are the most important works in the papers. But the first time you read this, you might skim or even skip, skim the related work section. It turns out, unless you're already familiar with the literature, if this is a body of work that you're not that familiar with, the related work section is sometimes almost impossible to understand. Uh, and again, since I'm being very honest with you guys, sometimes, related work section is when the authors try to cite everyone that could possibly be reviewing the paper and to make them feel good, uh, uh, and then hopefully accept the paper. And so related work sessions is sometimes written in funny ways, right? Um, and then, uh, [NOISE] step 3, I would often read the paper, but, um, [NOISE] just skip the math [NOISE], right? Um, and four, read the whole thing, uh, but skip parts that don't make sense, [NOISE] right? You know, um, I think that, uh, one thing that's happened many times in the research is that, I mean, the papers will tend to be cutting edge research, and so when, uh, we publish things, we sometimes don't know what's really important and what's not important, right? So there are- there are many examples of- of well known, highly cited research papers where some of it was just great stuff and some of it, you know, turned out to be unimportant. But at the time the paper was written, the authors did not know, every- no one on the planet knew what was important and what was not important. And maybe one example. Um, the LeNet-5 paper, right? Sample paper by Yann LeCun. Part of it was phenomenal, just established a lot of the foundations of ConvNets. And so it's, uh, one of the most incredibly influential papers. But you go back and read that paper, an- another sort of, whole half of the paper was about other stuff, right? Transducers and so on that is much less used. And so- and so it's fine if you read a paper and some of it doesn't make sense because it's not that unusual, or sometimes it just happens that, um, great research means we're publishing things at the boundaries of our knowledge and sometimes, ah, uh, the stuff you see, you know, we'll realize five years in the future that that wasn't the most important thing after all, right? Or that- what was the key part of the algorithm, maybe it wasn't what the authors thought. And so sometimes the past papers don't make sense. It's okay to skim it initially and move on, right? Uh, uh, unless you're trying to do a pe- unless you're trying to do deep research and really need to master it, then go ahead and spend more time. But if you're trying to get through a lot of papers, then, you know, then- then it's just prioritizing your time, okay? Um, and so, ah, just a few last things and then I'll ask you to practice this yourself with a paper, right? Um, you know, I think that when you've read and understood the paper, um, [NOISE] these are questions to try to keep in mind. And when you read a paper in a few minutes, maybe try to answer these questions: what do the authors try to accomplish? And what I hope to do in a few minutes is ask you to, uh, download a paper off the Internet, read it, and then, um, try to answer these questions and discuss your answer to these questions with- with- with your peers, right? With others in the class, okay? Um, what were the key elements, [NOISE] what can you use yourself, and um, [NOISE] okay? So I think if you can answer these questions, hopefully that will reflect that you have a pretty good understanding of the paper, okay? Um, and so what I would like you to do is, um, pull up your laptop and then so you- there- there's actually a- so I think on the, uh, ConvNet videos, right? On, um, the- the different AI ConvNet videos on Coursera, you learned a bit about, um, ah, well, various neural network architectures up to ResNets. And it turns out that there's another, uh, follow-on piece of work that maybe builds on some of the ideas of ResNets, which is called DenseNets. So, what I'd like you to do is, um, oh, and- and so what I'd like you to do is actually try this. [NOISE] And when I'm reading a paper, [NOISE] again, in the earlier stages, don't get stuck on the math, just go ahead and skim the math, and read the English text where you get through faster. Ah, and maybe one of the principles is, go from the very efficient high information content first, and then go to the harder material later, right? That's why often I just skim the math and I don't- if I don't understand some of the equation just move on, and then only later go back and, and really try to figure out the math more carefully, okay? So what I'd like you to do is take on a- I want you to take, um, uh, uh, wonder if, uh, let's- let's- let's try, let's- let's- have you take seven minutes. I was thinking maybe one- one minute per page is quite fast and, um, [NOISE] search for this paper, [NOISE] Densely Connected Convolutional Neural Net- Networks, by Gao Huang et al, okay? I want you guys to take out your laptops, uh, search for this paper, er, download it. You should find this on arXiv, um, A-R-X-I-V, right? And, uh, and this is also, so sometimes we also call this Dense Nets, I guess. And go ahead and, uh, take, why don't you take like seven minutes to read this paper and I'll let you know when the time is passed, and then after that time, um, I'd like you to, you know, discuss with your, with, with the others, right, what, wha- what you think are the answers, especially the first two. Because the other two you can leave for later. Why don't you go ahead and take a few minutes to do that now, and then I'll let you know when, um, sort of like, seven minutes have passed and then you can discuss your answers to these with your friends, okay? [NOISE] All right guys. So, um, anyone with any thoughts or insights, surprises, or thoughts from this? So, now you've spent 11 minutes on this paper, right? Seven minutes reading, four minutes discussing. It was a very, very short period of time, but any, any thoughts? What do you think of the paper? Come on, you-all, you-all just spent a lot of time sitting around, discussing with each other. Wha- wha- what did people think about the time you spent trying to read the paper? Actually, did you feel you, how, actually, r- raise your hand if you feel, you know, you've kind of understood the main concepts in the paper just a bit. Okay, yeah, like, two-thirds of you, many of you. And, actually, what did you think of the figures? Wow, people are really less energetic today than usual  [inaudible] So I think this is one of those papers where the, the paper is almost entirely summarized in figures one and two, all right. I think if you [inaudible] um, if you look at Figure One and the caption there and Figure Two on page three and the caption there and understand those two figures, those really convey, you know, 80 percent of the idea of the paper, right? Um, and I think that, uh, um, couple of other tips. So, um, it turns out that as you read these papers with practice, you do get faster. So, um, for example, Table One, uh, on page four, right, the, you know, this mess of the table on top. This is a pretty common format or a format like this is how a lot of authors use to describe a network architecture, especially in computer vision. So one of the things you find as well is that, um, the first time you see something like Table One it just looks really complicated. But by the time you've read a few papers in a similar format, you will look at Table One and go, "Oh, yeah, got it." You know, this is, this is, this is the DenseNet-121 versus DenseNet-169 architecture, and you will more quickly pick up those things. And so another thing you'll find is that, um, reading these papers actually gets better with practice, because you see different authors use different ways or similar ways of expressing themselves, and you get used to that. You'll actually be faster and faster at, uh, implementing these, um, at, at, at understanding these ideas. And I think, I know these days when I'm reading a paper like this, it maybe takes me about half an hour to, to feel like, and I, I know I gave you guys seven minutes when I thought I would need maybe half an hour to figure out a paper like this. Uh, um, uh, and I think, uh, for a more c- uh, I find that, uh, it's not unusual for people relatively new to machine learning to need maybe an hour to kind of, you know, really understand a paper like this. Um, and then I know I'm pretty experienced in machine learning, so I'm down to maybe half an hour for a paper like this, maybe even 20 minutes if it's a really easy one. But there are some outliers, so I have some colleagues who sometimes stumble across a really difficult paper. You need to chase down all the references and learn a lot of other stuff. So sometimes you come across a paper that takes you three or four hours or even longer to really understand it, but, uh, but I think depending on how much time you want to spend per week reading papers, um, you could actually learn, you know, learn a lot, right, um, uh, doing what you just did by maybe spending half an hour per paper, an hour a paper rather than seven minutes, right? Um, so, all right. I feel like, uh, yeah, and so, I, I think it's great, and, and, and notice that I've actually not said anything about the content of this paper, right? So whatever you guys just learned, that was all you. I had nothing to do with it. So, yeah, like you have the ability to go and learn this stuff by yourself. You don't need me anymore, right? [LAUGHTER] Um, so just the last few comments. Um, let's see. So the other things I get asked, questions I get is, uh, you know, where, where do you go? The deep learning field evolves so rapidly. So where, where do you go, uh, to? So if you're trying to master a new body of knowledge, definitely do web searches, and there are often good blog posts on, you know, here are the most important papers in speech recognition. There are lots of great resources there. And then the other thing you, I don't know, a lot of people try, want to do is try to keep up with the state of the art   of deep learning even as it's evolving rapidly. And so, um, I, I- I'll just tell you where I go to keep up with, um, you know, discussions, announcements. And surprisingly, Twitter is becoming an impo- surprisingly important place for researchers to find out about, um, new things. Um, there's an ML Subreddit, it is actually pretty good. Um, lot of noise, but many important pieces of work do get mentioned there. Uh, some of the top machine-learning con- conferences are NIPS, ICML, and ICLR, right? And so whenever these conferences come around, take a look and glance throughout these, the titles, see if there's something that interests you. And then I think I'm, I'm fortunate I guess to have, um, friends, you know, uh, both colleagues here at Stanford as well as colleagues in several other teams I work with that, um, uh, that just tell me when there's a cool paper, I guess. But I think with, here within Stanford or among your workplace, for those of you taking this at SCPD, you can form a community that shares interesting papers. So a lot of the groups I have on Slack and we regularly Slack each other or send, send each other, uh, text messages on the Slack messaging system, where you find interesting papers, and tha- tha- that's been great for me actually. Um, yeah, oh, and, and, and Twitter, let's see. Kian is, I follow Kian, you can follow him too. Uh, This is me, Andrew Y Ng, right? Um, I probably don't Slack up papers as often as I do. But if you look at, and you can also look at who we follow, and there are a lot of good researchers, uh, that, that will share all these things online. Oh, and, um, there, there are, there's a bunch of people that also use a website called Arxiv Sanity. Um, I don't as much sometimes, um, but there's lots of resources like that, all right? Um. All right. Cool. So just two last tips for how to read papers and get good at this. Um, so to more deeply understand the paper, uh, some of the papers will have math in it. Uh, and, actually, if you read the, I don't know, you all learned about Batch Norm, right? In the second module's videos. If you read the Batch Norm paper, it's actually one of the harder papers to read. There's a lot of math in the derivation of Batch Norm but there are papers like that. And if you want to make sure you understand the math here's what I would recommend, which is, read through it, take detailed notes and then see if you can re-derive it from scratch. So if you want to deeply understand the math of an algorithm from like, you know, Batch Norm or the details of back-prop or something the good practice. And I think a lot of sort of a theory- theoretical science and mathematics Ph.D  students will use a practice like this. You just go ahead and read the paper. Make sure you understand it and then to make sure you really, really understand it put, put, put aside the results and try to re-derive the math yourself from scratch. And if you can start from a blank piece of paper and re-derive one of these algorithms from scratch, then that's a good sign that you really understood it. When I was a Ph. D student I did this a lot, right? That you know I would read a textbook or read a paper or something and then put aside whatever I read and see if I could re-derive it from scratch starting from a blank piece of paper as only if I could do that, and I would you know feel like yep, I think I understand this piece of math. And it turns out if you want to do this type of math yourself is your ability to derive this type of math, re-derive this type of math, that gives you the ability to generalize, to derive new novel pieces of math yourself. So I think I actually learned a lot of math, for several machine learning by doing this. And just by re-deriving other people's work that allowed me to learn how to derive my own novel algorithms. And actually sometimes you go to the art galleries, right? They go to the Smithsonian. You see these art students, you know, sitting on the floor copying the great artworks, the great paintings you know, painted by the masters centuries ago. And so I think just as today there are art students sitting in or the de Young Museum or whatever or and I was at the Getty Museum in LA a few months ago. You actually see these art students you know, copying the work of the masters. And I think a lot of the ways that you want to become good at the math of machine learning yourself, this is a good way to do it. It's time-consuming but then you can become good at it that way. And same thing for codes, right? I think the simple lightweight version one of learning would be to download and run the open source code if you can find it, and a deeper way to learn this material is to re-implement it from scratch. Right, it is easy to download an open sourcing and run it and say ooh, it works. But if you can re-implement one of these algorithms from scratch then that's a strong sign that you've really understood this algorithm. Okay? Um, alright. And then longer term advice. Right. You know, for user keep on learning and keep on getting better and better, the more important thing is for you to learn steadily not for you to have a focus intense activity you know, like over Thanksgiving you read 50 papers over Thanksgiving and then you're done for the rest your life. It doesn't work like that, right? And I think you're actually much better off reading two or three papers a week for the next year than you know, cramming everything right over, over one long weekend or something. Actually in education we actually know that spaced repetition works better than cramming so the same same thing, same body of learning. If you learn a bit every week and space it out you actually have much better long-term retention than if you try to cram everything in short-term so there's, there's a very solid result that we know from pedagogy and how the human brain works. So, so if you're able to- so so again the way I, my life is my backpack. I just always have a few papers with me. And I find that I can, I read almost everything on the tablet. Almost everything on iPad, but I find that research papers one of the things where the ability to flip between pages and skim I still find more efficient on paper. So I read almost nothing on paper these days except for research papers, but that's just me. Your mileage may vary. Maybe something else will work better for you. Okay? All right. So let's see, that's it for reading research papers, I hope that while you're in CS230, you know, if some of you find some cool papers or if you go further for the DenseNet paper and find an interesting result there. Go ahead and post on Piazza if any of you want to start a reading group of other friends here at Stanford you know, encourage you to look around class, find, find, find a group here on campus or with among your CS230 classmates or your work colleagues. For those of you taking this on SCPD so that you can all keep studying the literature and learning and helping each other along. Okay? So that's it for reading papers. The second thing we're gonna do today is just give some longer-term advice on navigating a career in machine learning, right? Any questions about this before I move on? Okay. Cool. All right. But I hope that was useful. Some of this I wish I had known when I was a first-year PhD student but c'est la vie. Alright. Let's see. Can we turn on the lights please? Alright. So kind of in response to requests from early- students in earlier versions of the class, before we, you know as we approach the end of the quarter, want to give some advice to how to navigate a career in machine learning, right? So today machine learning there are so many options to do, so many exciting things. So how do you, you know, what do you want to do? So I'm going to assume that most of you will want to do one of two things, right? At some point you know you want to get the job, right? Maybe a job that does work in machine learning and including a faculty position for those of you who want to be a professor. But I guess eventually most people end up with a job I think I guess there are other alternatives but but and some of you want to go on to more advanced graduate studies although even after you get your PhD at some point, most people do get a job after the PhD. And by job I mean either in a big company, you know, or a or a startup, right? But regardless of the details of this, I think- I hope most of you want to do important work. Okay. So what I'd like to do today is break, you know, this into, how do you find a job or join a Ph.D program or whatever that lets you do important work. And I want to break this discussion into two steps. One is just how do you get a position? How do you get that job offer or how do you get that offer of admission to the Ph.D program or admission to the master's program or whatever you wanna do. And then two is selecting a position. Between going to this university versus that university or between taking on the job in this company versus that company. What are the ones that will tend to set you up for success, for long-term personal success and career success? And really I hope that, by the way, I hope that all of these are just tactics to let you do important work right and this, I hope that's what you want to do. So you know, what do recruiters look for? And I think just to keep the language simpler I'm going to pretend that, I'm just gonna talk about finding a job. And but a lot of that very similar things apply for  PhD programs is just instead of saying recruiters I would say admissions committees right then it's actually some of this is, but let me just focus on the job scenario. So most recruiters look for technical skills. So for example, there are a lot of machine learning interviews that will ask you questions like, you know, where would you use gradient descent or batch gradient descent or stochastic gradient descent, you know, descent and what happens when the mean batch size is too large or too small, right? So there are companies, many companies today asking questions like that in the interview process. Or can you explain difference between an LCM and GIGO and when would you use GIGO? So you really get questions like that in many job interviews today. And so recruiters looking for ML skills as well as, and so you will often be quizzed on ML skills as well as your coding ability, right? And then beyond your- and I think Silicon Valley's become quite good at giving people the assessments to test for real skill in machine learning engineering and in software engineering. And then the other thing that recruiters will look for, that many recruiters will look for is meaningful work. And in particular, um, uh, you know, there are some candidates that apply for jobs that have very, um, theoreticals. They're very academic skills meaning you can answer all the quiz questions about, you know, what is Batch Norm? Can you derive the [inaudible] for this? But unless you've actually shown that you can apply this in a meaningful setting, it's harder to convince a company or a recruiter that you know not just the theory, but that you know how to actually make this stuff work. And so, um, having done meaningful work using machine learning is a very strong, is a very desirable candidate, I think, to a lot of companies. Kind of work experience. And I think really, whether you've done, whether you've done something meaningful, um, reassures that, you know, that you can actually do work, right? There's not just you can answer quiz questions, that you know how to implement learning algorithms that work. Um, and, and maybe, um, uh, yeah, right. Um, and then many recruiters actually look for your ability to keep on learning new skills and stay on top of machine learning even as it evolves as well. Okay. And so a very common pattern for the, um, successful, you know, AI engineers, say, machine learning engineers, would be the following, where if on the horizontal axis, I plot different areas. So, you might learn about machine learning. Learn about deep learning. Learn about probabilistic graphical models. Learn about NLP. Learn about computer vision and so on for other areas of AI and machine learning. Um, and if the vertical area of the vertical axis is depth, uh, a lot of all the strongest candidates for jobs are, um, T-shaped individuals. Meaning that you have a broad understanding of many different topics in the AI machine learning, and very deep understanding in, you know, maybe at least one area. Maybe more than one area. Um, and so I think by taking CS230 and doing the things that you're doing here, hopefully you're acquiring a deeper understanding of one of these areas of deep learning in particular. Um, but the other thing that even, you know, deepens your knowledge in one area will be the projects you work on. Um, the open source contributions you make, right. Uh, whether or not you've done research. Um, and maybe whether or not you've done an internship. Right? Okay. And I think these two elements, you know, broad area of skills, and then also going deeper to do a meaningful project in deep learning. Or, um, work with a Stanford professor, right? And do a meaningful research project, or make some contribution to open-source. Publish it on GitHub, and then let us use it. These are the things that let you deepen your knowledge and, and convince recruiters that you both have the broad technical skills, and when called on you're able to apply these in a, in a, in a meaningful way to an important problem, right? And in fact, um, the way we design CS230 is actually a microcosm of this. Where, um, you know, you learned about neural nets. Um, then about topics like Batch Norm, ConvNets, sequence models, right? I'm just gonna say RNNs. So, actually you've a breadth within the field of deep learning. And then what happens is, well, then, and the reason I want you to work on the project is so that you can pick one of these areas. And maybe go deep and build a meaningful project in one of these areas, which will, which will, and it's not just about making a resume look good, right? It's about giving you the practical experience to make sure you actually know how to make these things work, um, uh, and give you the learning. To make sure that you actually know how to make a CNN work, to make a RNN work. All right. And then of course it stands many students also list their projects on their resumes obviously. Um, so, I think the um, let's see. The- the- the- failure modes. The things, bad ways to navigate your career. Um, there are some students they just do this, right? There are some Stanford students that just take class, after class, after class, after class, and go equally in depth in a huge range of areas. And this is not terrible. You can actually still got a job uh, uh you still get. Sometimes you can even get into some Ph.D. programs like this with all the depth, but this is not the best way to navigate your career. All right? So, there are some Stanford students who's- that takes tons of classes. You can get a good GPA doing that, but do nothing else. And this is not terrible, but this is- this is not- this is not great. It's not as good as the alternative. Um, there's one other thing I've seen Stanford students do which is, uh, just try to do that, right? But you just try to jump in on day one, and go really really deep in one area. And again, um, this has its own challenges, I guess. You know, one, one, one failure mode, one mode is actually not great. As sometimes you actually get, um, undergrad freshmen at Stanford that have not yet learned a lot about coding, or software engineering, or machine learning, and try to jump into research projects right away. This turns out not to be very efficient because it turns out Stanford classes are, your online courses or Stanford classes are a very efficient way for you to learn about the broad range of areas. And after that going deeper and getting experience in one vertical area then deepens your knowledge. It makes so you know how to actually make those ideas work. Uh, so I do see sometimes unfortunately, you know, som- some Stanford freshmen join us already knowing how to code and have implemented, you know, some learning algorithms, but some students that do not yet have much experience try to jump into research projects right away. And that turns out not to be very productive for the student or for the research group because until you've taken the classes and mastered the basics it's difficult to understand what's really going on in the advanced projects, right? Um, so I would, I, I would say this is actually worse than that, right? This is, this is actually okay. This is actually pretty bad. It is I, I, I would not do this for your career, right? Yeah. Probably not. Yeah. Um, and then the other not-so-great mode that you see some Stanford students do is get a lot of breadth, and then do a tiny project here. Do a tiny project there. Do a tiny project there. Do a tiny project there. And you end up with ten tiny projects, but no one or two really sec- significant projects. So again, this is not terrible, but, you know, beyond a certain point, by the way recruiters are not impressed by volume, right? So, having done 10 lame projects is actually not impressive. Not nearly as impressive as doing one great project or two great projects. And again, there's more to life than impressing recruiters, but recruiters are very rational. And the reason recruiters are less impressed by someone who's profile looks like this is because they're actually probably factually less skilled and less able at doing good work in machine learning compared to someone that, that has done a substantive project and knows what it takes to see, see the whole thing through. Does that make sense? So, when I say you'd have recruiters more or less impressed is because they're actually quite rational, in terms of, uh, trying to understand how good you are at um, uh, at, at, doing important work, at building meaningful AI systems. Makes sense? Um, and so in terms of building up both the horizontal piece and vertical piece, uh, this is what I recommend. Um, to build a horizontal piece, a lot of this is about building foundational skills. And, um, it turns out coursework is a very efficient way to do this. Uh, you know, in, in, in these courses, right, you know various instructors like us, but many other Stanford professors, um, put a lot of work into organizing the content to make it efficient for you to learn this material. Um, and then also reading research papers which we just talked about. Having a community will help you. Um, and then that is often, uh, building a more deep and, um, relevant project, right? And, and, and the pro- projects have to be relevant. So, you know, if you want to build a career machine learning, build a career in AI. Hopefully, the project is something that's relevant to CS, or machine learning, or AI deep learning. Um, I do see, I don't know, for some reason, I feel like, uh, a surprisingly large number of Stanford students I know are in the Stanford dance crew, and they spend a lot of time on that which is fine. If you enjoy dancing, go have fun. Don't, don't, you know, you, you don't need to work all the time. So, go join the dance crew, or go on the overseas exchange program. And go hang out in London and have fun, but those things do not as directly contribute to this, right? Yeah. I know, I think, I think, in an earlier version of this presentation, you know, students walked away, saying ha, you know, Andrew says we should not have fun we should work all the time and that's not the goal [LAUGHTER]. Um, All right. There is one. All right. Um, you know, there is the uh, Saturday morning problem which all of you will face. Right? Which is every week, uh, including this week on Saturday morning you have a choice. Um, you can, uh, read a paper [LAUGHTER] or work on research or work on open source or, I don't know what people do, or you can watch TV or something, [LAUGHTER] right? Um, and you will face this choice, like, maybe every Saturday, you know, for the rest of your life or for all Saturdays in the rest of your life. And, um, you know, you can build out that foundation skills, go deep or go have fun, and you should have fun, all right? Just for the record. But one of the problems that a lot of people face is that, um, even if you spend all Saturday and all Sunday reading a research paper, um, you know, the following Monday, or maybe spend all Saturday and Sunday working hard, it turns out that the following Monday, you're not that much better at deep learning. Is like, yeah, you work really hard. So you read five papers, you know, great. Uh, but if you work on a research group the professor or your manager if you're in a company, they have no idea how hard you work. So there's no one to come by and say ''Oh, good job working so hard all weekend.'' So no one knows these sacrifices you make all weekend to study or code open source, just no one knows. So there's almost no short-term reward to doing these things. Um, but they see- and, and, and, and whereas there might be short-term rewards for doing other things, right? Um, uh, but the secret to this is that it's not about reading papers really, really hard for one Saturday morning or for all Saturday once and it being done. The secret to this is to do this consistently, um, you know, for years, um, or at least a month. And it turns out that if you read, um, two papers a week, and you do that for a year then you have read 50 papers after a year and you will be much better at deep learning after that, right? I mean when you read, you have read 100 papers in the year if you read two papers a week. And so for you to be successful is much less about the intense burst of effort you put in over one weekend. It's much more about whether you can find a little bit of time every week to read a few papers or contribute to open source or take some online courses, uh, but- and if you do that you know every week for six months or do that every week for a year, you will actually learn a lot about these fields and be much better off, and be much more capable at deep learning and machine learning or whatever, right? Um, yeah. So, um, yeah, and yeah she- my wife and I actually do not own a TV. [LAUGHTER] For what it's worth. Okay, but again, if you own one go ahead. Make sure- don't, don't drive yourself crazy. There's a healthy work-life integration as well. All right. So, um, so I hope that doing these things more is not about finding a job, it's about doing these things and make you more capable as a machine learning person, so that you have the power to go out and implement stuff that matters, right? To do stuff that actually, do, do work that matters. Well the second thing we'll chat about is selecting a job, right? And it's actually really interesting. Um, I, uh, gave this part of presentation, um, last year, uh, actually sorry earlier this year and shortly after that presentation, um, there was a student in the class that was already in a company who emailed me saying, "Boy Andrew, I wish you'd told me this before I accepted my current job." Um, and so [LAUGHTER] let's see. Hopefully this will be useful to you. Um, so it turns out that ,um, uh, you know, I, so when you're- at some point you're deciding, you know, what Ph.D program do you want to apply for, what companies you want to apply for a job at and, um, I can tell you what, uh, so if you want to keep learning new things, um, I think one of the biggest predictors of your success will be whether or not you're working with great people and projects, right? And in particular, um, you know, there are these fascinating results from, uh, what are these, I wanna say from the social sciences showing that, um, if your closest friends, if your five closest friends or ten closest friends are all smokers, there's a much higher chance you become a smoker as well, right? And if your five or 10 close friends are, uh, um, you know, overweight, there's a much high chance you'd do the same or- and conversely if there's a, you know, so I think that if your five closest friends work really hard, read a lot of research papers, care about their work, learning and making themselves better, then there's actually a very good chance that you will be, that they'll influence you that way as well. So we're all human. We're all influenced by the people around us, right? And so, um, I think that- and I've been fortunate, I've taught at Stanford for a long time now, so I've been fortunate to have seen a lot of students go from Stanford to various careers and because I've seen how many hundreds or maybe thousands of Stanford students, that I knew right, when they were still Stanford students, go on to separate jobs. I saw many of them have amazing careers. Um, I saw, you know, a few have, like, okay careers, um, that I think over time I've learned to pattern match what is predictive of your future success after you leave Stanford and I'll share with you some of those patterns, share with you some of those patterns as you navigate your career. And it's just there's so many options in machine learning today that its's kind of tragic if you don't, you know, navigate to hopefully maximize your chance of being one of the people that gets to do fun and important work that helps others. Um, so when selecting a position, um, I would advise you to focus on the team, um, [NOISE] you interact with and by team I mean, you know, somewhere between 10 to 30 persons, right, maybe up to 50, right? Um, because it turns out that if yo- there will be some group of people. Maybe 10 to 30 people, maybe 50 people that you interact with quite closely and these will be appears in the people that will influence you the most, right? Um, because if you join a company with 10,000 people, you will not interact with all 10,000 people. There will be a core of 10 or 30 or 50 people that you interact with the most, and it's those people how much they know, how much they teach you, how hard working they are, whether they're learning themselves that will influence you the most, rather than all these other hypothetical 10,000 people in a giant company. Um, and of these people, one of the ones that will influence you the most is your manager, all right? So make sure you meet your manager and get to know them and make sure they're someone you want to work with. Um, and in particular, I would recommend focusing on these things and not on the brand, um, of the company. Because it turns out that the brand of the company you work with is actually not that correlated. Yeah maybe there's a very weak correlation, but it's actually not that correlated with what your personal experience would be like if that makes sense, right? Um, and so, um, [NOISE] and by the way, again, just full disclosure. I'm one of the- I have a research group here at Stanford, right? My research group at Stanford is one of the better known research groups in the world but just don't join us because you think we are well-known, right? It's just not a good reason to join us for the brand. Instead, if you want to work with someone, meet the people and evaluate the individuals, or look at the people and see if you think these are people you can learn from and work with, and are good people, makes sense? [NOISE] So, um, in today's world there are a lot of companies, um, recruiting Stanford students. So let me give you some advice. This piece I only give because many years- well I'll just give you advice. So sometimes, there are giant companies with let's say, uh, 50,000 people, right? And I'm not thinking of any one specific company. If you're trying to guess what company I'm thinking of, there is no one specific company I'm thinking of but this pattern matches, uh, to many large companies. But maybe there's a giant company with, you know, 50,000 people, right? And, um, let's say that they have a 300 person, right, AI team, um, it turns out that if you look at the work of the 300 persons in the AI team and if they send you a job offer to join the 300 person AI team, that might be pretty good, right? Since this may be the group, you know, whose work you hear about, they publish papers, you read them on the news. Um and so if you've got a job offer to work with this group, that might be pretty good or even better would be sometimes even within the 30 person AI team it's actually difficult to tell what's good and what's not. There is often a lot of variance even with this, what's even better would be if you get a job offer to join a 30 person team. So you actually know who's your manager, who are your peers, who you're working with. And if you think these are 30 great people you can learn from, that could be a great job offer. The failure mode that unfortunately I've seen, um, several Stanford students go down and it's actually this is a true story. There was once, uh, several years ago there's a Stanford student I knew that I thought was a great guy, right? You know, I knew his work, he was coding machine learning algorithms. I thought he was very sharp and did very good work, uh, working with some of my Ph.D students. He got a job offer from one of these giant companies with- that has a great AI group. Um, and his offer wasn't to go to the AI group, his offer was to, um, join us and then we'll assign you to a team. So this particular student, that was a Stanford student that I know about and care about, um, he wound up being assigned to a really boring Java back end payments team and, uh, so after he accepted the job offer, he wound up being assigned to a, you know, back-end- and I apologize. I know you work on Java back-end payment process systems I think they're great [LAUGHTER] but the student was assigned to that team and he was really bored and so, um, I think that this was a student whose career- I personally saw his career rising, while he was at Stanford and after he went to this, you know, frankly not very interesting team, I saw his career plateau, um, and after about a year and a half he resigned from this company after wasting a year and a half of his life and missing out really on a year and a half of this very exciting growth of AI machine learning, right? So it was very unfortunate. Um, uh, and it was actually after I told this story, um, last time I taught this class earlier this year that actually someone from, um, actually it was from the same big company [LAUGHTER] he found me and said, "Boy, Andrew I wish you'd told me the story earlier, because this is exactly what happened to me, at the same big company [LAUGHTER]. Now, I wanna share with you, uh, a different, um, so- so I would just be careful about rotation programs as well. You know, when the company is trying to recruit you, if a company refuses to tell you what project you work on, who's your manager, exactly what team you're joining, I personally do not find those job offers that attractive because if they can't, you know, if they refuse to tell you what team you're gonna work with, well chances are, right, telling you the answer will not make the job attractive to you. That's why they're not telling you. So I'd just be very careful. And sometimes rotation programs sound good on paper but it is really, you know, well we'll figure out where to send you later. So, I feel like I've seen some students go into rotation programs that sound good on paper, that sound like a good idea but just as you wouldn't- after you graduate from Stanford, would you wanna do four internships and then apply for a job? That would be a weird thing to do. So, sometimes rotation programs are yeah, come and do four internships and then we'll let you apply for a job and see where we wanna send you. It could be a job at back end payment processing system, right? So, um, so so just just be cautious about the marketing of rotation programs. Um uh, and again, if you do if but if- but if what they say is do rotation and then you join this team, then you can look at this team and say yep, that's a great team. I wanna do rotation but then I would go and work with this team and and these are the 30 people I'll work with. So that could be great. But do a rotation and then we can send you anywhere in this giant company, that I would just be very careful about. Um, now on the flip side, there are some companies, I'm not gonna mention any companies, but there are some companies with you know, not as glamorous, not as- not as like cool brands, and maybe this is a, I don't know, 10,000 person company or 1,000 or 50,000 person or whatever. Let's say 10,000 person company. I have seen many companies that are not super well-known in the AI world, they are not in the news all the time, but they may have a very elite team of 100 people doing great work in machine learning, right? And there are definitely companies whose brands are not you know, the first companies you think of when you think of big AI companies that sometimes have a really really great 10 person or 50 person or 100 person team that works on learning algorithms. And even if the overall brand or the overall company, you know, isn't as like, is a little bit sucky. If you manage to track down this team and if you have a job offer to join this elite team in a much bigger company, you could actually learn a lot from these people and do important work. You know, one of the things about Silicon Valley is that uh, the brand of your resume matters less and less, right? Less than never before. I mean, I guess, I think the exception of the Stanford brand, you totally want the Stanford brand in your resume but with that exception, but really you know,  Silicon Valley is becoming really good. Sili- the world, right? Has become really good at evaluating people for your genuine technical skills and your genuine capability and less for your brand and so, I would recommend that instead of trying to get the best stamps of approval on your resume to go and take the positions that let you have the best learning experiences and also allows you to do the most important work and that is really shaped by the you know, 30 or 50 people you work with and not by the overall brand of the company you work with, right? So the variance across um uh-so there's a huge variance across teams within one company and that variance is actually pretty bigger or might be bigger than the variance across different companies, does that make sense? Since I would, and if a company refuses to tell you what team you would join, I would seriously consider just, you know, doing something- if you have a better option, I would, I would do something else. Um, and then finally, um, yeah and- and so really I- again I guess I don't wanna name these companies but you know think of some of the large retailers or some of the large healthcare systems or there are a lot of companies that are not well known in the AI world but that I've met their AI teams and I think they're great. And so if you're able to find those jobs and meet their people you can actually get very exciting jobs in there. All right but of course, for the giant companies with elite AI teams, you can join that elite AI team, right? That's also- that's also great. I'm a bit biased since I use to lead some of these elite AI teams. So- so I think those teams are great but the loss of some teams in a, um, ah, yeah. All right. Um, lastly, you know, just general advice, this is how I really live my life. I tend to choose the things to work on that will allow you to learn the most and you know, try to do important work, right? So, you know especially if you're relatively early in your career, whatever you learn in your career will pay off for a long time and so um, uh and so joining the teams that are working with a great set of 10 or 30 or 50 teammates will let you learn a lot, and then also, you know, hopefully, I mean, yeah and- and just don't- don't don't join a like a cigarette company and hope you know, give more people cancer or stuff like that. Just don't- don't do this. Don't- don't do stuff like that. But if you can do meaningful work that helps other people and do important work and also learn a lot on the way, hopefully you can find positions like that, right? That let you set- set yourself up for long-term success but also do work that you think matters and that, and that helps other people. All right. Um, any questions while we wrap up? Yeah. [NOISE] I have a question about important work, what are some topics that you think you would include as important [inaudible]? What's important? You know, I don't know. Um, I think one of the most meaningful things to do in life is called [inaudible]. Either advance the human condition or help other people. But the thing is, I'm nervous. I don't wanna name one or two things because the world needs a lot of people who work on a lot of different things. So, the world's not gonna function if everyone works on computational biology. I think comp-bio is great but it's actually good that, where people work on comp-bio, my Ph.D students like you know, many work on the outside to healthcare. My team at Landing AI does a lot of work on the AI applied to manufacturing, to agriculture, to some health care and some other industries. Um,uh, I actually especially the California fire is burning you know, I actually think that there's important work to be done in AI and climate change, uh, um, but I think that there's a lot of important work in a lot of industries. Right, I actually think that, you know, I should think that the next wave of AI, excuse me I should say machine learning, is we've already um, transformed a lot of the tech world, right? So, you know, yeah, I mean we've already helped a lot of the Silicon Valley tech world become good at AI and that's great, right? Helped build a couple of the teams that wound up doing this, right? Google Brain, how Google become good at deep learning, the Baidu I grew up with, hope I do become, you know, good at one of the greatest AI companies in the world, certainly in China, and I'm very happy that between me and some of my friends in the industry we've made a lot of good AI companies. I think part of the next phase for the evolution of machine learning is for it to go into not just the tech companies like the, you know, like the Google and Baidu which I helped as well as Facebook, Microsoft which I had nothing to do with as well as what else AirBnB, Pinterest, Uber, right? All these are great companies. I hope they'll all embrace AI. But I think some of the most exciting work to be done still has also looked outside the tech industry and to look at all the sometimes called traditional industries that do not have shiny tech things because I think the value creation there as surprise you could implement there may be even bigger than if you, you know, uh, uh yeah. I'll mention one interesting thing, one thing I noticed is a lot of large tech companies all work on the same problems, right? So everyone works on machine translation, everyone works on speech recognition, face detection, and click-through rate and part of me feels like this is great because it means there's a lot of progress in machine translation and that's great. We do want progress in machine translation. Though sometimes when you look at other industries. Um, so, you know, when you look at manufacturing or um, some of the medical devices that you're looking at or sometimes on on these farms hanging out with farmers on, on, on. If you like, in my own work with my teams where sometimes we're stumbling across brand new research problems that the big tech companies do not see and have not yet learned to frame. So, I find one of the most exciting challenges is actually to be constantly on the cutting edge. Looking at these types of problems there's a different cutting edge than the cutting edge of the big tech companies. So, I think some of you will join the big tech companies and that's great. We need more AI in the big companies, in the tech companies, but I think a lot of exciting work to do in AI is also outside where we traditionally consider tech, right? All right. It's 10 to, it's 12:50. So, hope- I hope this was helpful and let's- let's break for today. Have a, have a great Thanksgiving everyone and we'll see you in a couple of weeks. 

Hi everyone and welcome [NOISE] to Lecture 9 uh, for CS230. Uh, today we are going to discuss an advanced topic uh, that will be kind of the, the marriage between deep learning and another field of AI which is reinforcement learning, and we will see a practical uh, application in how deep learning methods can be plugged in another family of algorithm. So, it's interesting because deep learning methods and deep neural networks have been shown to be very good uh, function approximators. Essentially, that's what they are. We're giving them data so that they can approximate a function. There are a lot of different fields which require these function approximators, and deep learning methods can be plugged in all these methods. This is one of these examples. So, we'll first uh, motivate uh, the, the setting of reinforcement learning. Why do we need reinforcement learning? Why cannot, what wh- why can't we use deep learning methods to solve everything? There is some set of methods that we cannot solve with deep learning and reinforcement learning meth- uh, re-reinforcement learning applications are examples of that. Uh, we will see an example uh, to introduce an algorithm, a re- reinforcement learning algorithm called Q-Learning, and we will add deep learning to this algorithm and make it Deep Q-Learning. [NOISE] Uh, as we've seen with uh, generative adversarial networks and also deep neural networks, most models are hard to train. We've had, we had to come up with Xavier initialization, with dropout, with batch norm, and myriads, myriads of methods to make these deep neural networks trained. In GANs, we had to use methods as well in order to train GANs, and tricks and hacks. So, here we will see some of the tips and tricks to train Deep Q-Learning, which is a reinforcement learning algorithm. [NOISE] And at the end, we will have a guest speaker coming uh, to talk about advanced topics which are mostly research, which combine deep learning and reinforcement learning. Sounds good? Okay. Let's go. [NOISE] So, deep reinforcement learning is a very recent field I would say. Although both fields are, are- reinforcement learning have, has existed for a long time. Uh, only recently, it's been shown that using deep learning as a way to approximate the functions that play a big role in reinforcement learning algorithms has worked a lot. So, one example is AlphaGo and uh, you probably all have heard of it. It's Google DeepMind's AlphaGo has uh, beaten world champions in a game called the game of Go, which is a [NOISE] very, a very old strategy old game, and the one on the right here um, or on, on your right, human level control through deep reinforcement learning is also uh DeepMind, Google's DeepMind paper that came out and hit the headlines on the front page of Nature, which is uh, one of the leading uh, multidisciplinary peer review journals uh, in the world. And they've shown that with deep learning plugged in a reinforcement learning setting, they can train an agent that beats human level in a variety of games and in fact, these are Atari games. So, they've shown actually that their algorithm, the same algorithm reproduced for a large number of games, can beat humans on all of these games. Most of these games. Not all of these games. So, these are two examples, although they use different sub techniques of uh, reinforcement learning. they both include some deep learning aspect in it. And today we will mostly talk about the human level controlled through deep reinforcement learning or so-called deep Q network, presented in this paper. So, let's, let's start with, with motivating reinforcement learning using the, the AlphaGo setting. Um, this is a board of Go and the picture comes from DeepMind's blog. Uh, so Go you can think of it as a strategy game, where you have a grid that is up to 19 by 19 and you have two players. One player has white stones and one player has black stones, and at every step in the game, you can position a stone on the, on the board. On one of the grid cross. The goal is to surround your opponent, so to maximize your territory by surrounding your opponent. And it's a very complex game for different reasons. Uh, one reason is that you have to be you, you cannot be shortsighted in this game. You have to have a long-term strategy. Another reason is that the board is so big. It's much bigger than a chessboard, right? Chessboard is eight by eight. So, let me ask you a question. If you had to solve or build an agent that solves this game and beats humans or plays very well at least with deep learning methods that you've seen so far, how would you do that? [NOISE] Someone wants to try. [NOISE] So, let's say you have a, you have to collect the data set because in, in classic supervised learning, we need a data set with x and y. What do you think would be your x and y? [NOISE] Yeah. The game board is the input, the output probability of victory in that position. Okay. Input is game board, and output is probability of victory in that position. So, that's, that's a good one I think. Input output. What's the issue with that one? [NOISE] So, yeah. [inaudible] Yeah. It's super hard to represent what the probability of winning is from this board. Even, like nobody can tell. Even if I ask [NOISE] an experienced human to come and tell us what's the probability of black winning in this or white winning [NOISE] in, in this setting, they wouldn't be able to tell then [NOISE]. So, this is a little more complicated. Any other ideas of data sets? Yeah. [NOISE] [inaudible]. Okay. Good point. So, we could have the grid like this one and then this is the input, and the output would be the move, the next action taken by probably a professional player. So, we would just watch professional players playing and we would record their moves, and we will build a data set of what is a professional move, and we hope that our network using these input/outputs, will at some point learn how the professional players play and given an input, a state of the board, would be able to decide of the next move. What's the issue with that? Yeah. [inaudible] Yes. [NOISE] We need a whole lot of data. Why? And- and you, you said it. You- you said because, uh, we need basically to represent all types of positions of the board, all states. So, if, if you were actually. Let's- let's do that. If we were to compute the number of possible states, uh, of this board, what would it be? [NOISE] It's a 19 by 19 board. [NOISE]. Remember what we did with adversarial examples. We did it for pixels, right? [NOISE] Now, we're doing it for the board. So, what's- the, the question first is. Yeah, You wanna try? Uh, three to the 19 squared. Yeah. Three to the power 19 [NOISE] times 19. Or 19 squared. Yeah. So, why is it that? [NOISE] So, why is it? Is it this? Each spot can have [inaudible] uh, have no stone a white stone or a black. Yeah. Each spot and there are 19 times 19 spots, can have three state basically. No stone, white stone, or black stone. But this is the all possible state. This is about 10 to the 170. So, it's super, super big. So, we can probably not get even close to that by observing professional players. First because we don't have enough professional players and because, uh, we are humans and we don't have infinite life. So, the professional players can not play forever. They might get tired as well. Uh, but, so one issue is the state space is too big. Another one is that the ground truth probably would be wrong. It's not because you're a professional player that you will play the best move every time, right? Every player has their own strategy. So, the ground truth we're, we're having here is not necessarily true, and our network might, might not be able to beat these human players. What we are looking into here is an algorithm that beats humans. Okay. Second one, too many states in the game as you mentioned, and third one we will likely not generalize. The reason we will not generalize is because in classic supervised learning, we're looking for patterns. If I ask you to build an algorithm to detect cats versus dogs, it will look for what the pattern of a cat is versus what the pattern of the dog is in, and in the convolutional filters, we will learn that. In this case, it's about a strategy. It's not about a pattern. So, you have to understand the process of winning this game in order to make the next move. You cannot generalize if you don't understand this process of long-term strategy. So, we have to incorporate that, and that's where RL comes into place. RL is reinforcement learning, a method that would- could be described with one sentence as automatically learning to make good sequences of decisions. So, it's about the long-term. It's not about the short-term, and we would use it generally when we have delayed labels, like in this game. The label that you mentioned at the beginning was probability of victory. This is a long-term label. We cannot get this label now but over time, the closer we get to the end, the better we are at seeing the victory or not, and it's for to make sequences of decisions. So, we make a move then the opponent makes a move. Then we make another move, and all the decisions of these moves are correlated with each other. Like you have to plan in, in advance. When you are human you do that, when you play chess, when you play Go [NOISE]. So examples of RL applications can be robotics and it's still a research topic how deep RL can change robotics, but think about having a robot walking from here and you wanna send it there. You'll want to send the robot there. What you're teaching the robot is if you get there it's good, right? It's good. You achieve the task, but I cannot give you the probability of getting there at every point. I can help you out by giving you a reward when you arrive there and let you trial and error. So, the robot will try and randomly initialized, the robot would just fall down at the first, at first. Gets a negative reward. Then, repeats. This time the robot knows that it shouldn't fall down. It shouldn't go down. You should probably go this way. So, through trial and error and reward on the long-term, the- the robot is supposed to learn this pattern. Another one is games and that's the one we will see today. Uh, games can be represented as, as, as a set of reward for reinforcement learning algorithm. So, this is where you win, this is where you lose. Let the algorithm play and figure out what winning means and what losing means, until it learns. Okay. The problem with using deep learning is that the algorithm will not learn cause this reward is too long-term. So, we're using reinforcement learning, and finally advertisements. So, a lot of advertisements, um, are real-time bidding. So, you wanna know given a budget when you wanna invest this budget, and this is a long-term strategy planning as well, that reinforcement learning can help with. Okay. [NOISE] So, this was the motivation of reinforcement learning. We're going to jump to a concrete example that is a super vanilla example to understand Q-Learning. So, let's start with this game or environment. So, we call that an environment generally and it has several states. In this case five states. So, we have these states and we can define rewards, which are the following. So, let's see what is our goal in this game. We define it as maximize the return or the reward on the long-term. And what is the reward is the, the numbers that you have here, that were defined by a human. So, this is where the human defines the reward. Now, what's the game? The game has five states. State one is a trash can and has a reward of plus two. State two is a starting state, initial state, and we assume that we would start in the initial state with the plastic bottle in our hand. The goal would be to throw this plastic bottle in a can. [NOISE] If it hits the trash can, we get +2. If we get to state five, we get to the recycle bin, and we can get +10. Super important application. State four has a chocolate. So, what happens is if you go to state four, you get a reward of 1 because you can eat the chocolate, and you can also throw the, the chocolate in the, in- in the, in the recycle bin hopefully. Does this setting makes sense? So, these states are of three types. One is the starting state initial which is brown. The [NOISE] normal state which is not a starting neither, neither a starting nor a, an ending state, and it's gray. And the blue states are terminal states. So, if we get to the terminal state, we end up a game or an episode let's say. Does the setting makes sense? Okay, and you have two, two possible actions. You have to move. Either you go on the left or you go on the right. An additional rule will, will add is that the garbage collector will come in three minutes, and every step takes you one minute. So, you cannot spend more than three minutes in this game. In other words, you cannot stay at the chocolate and eat chocolate forever. You have to, to move at some point. Okay. So, one question I'd have is how do you define the long-term return? Because we said we want a long-term return. We don't want, we don't care about short-term returns. [NOISE] What do you think is a good way to define the long-term return here? [NOISE] Yeah. Sum of the terminal state. The sum of the terminal states. No, the sum of how many points you have when you reach the terminal state. The sum of how many points you have when you reach a terminal state. So, let's say I'm in this state two, I have zero reward right now. If I reach the terminal state on the, on the, on your left, the +2. I get +2 reward and I finish the game. If I go on the right instead and I reached the +10. You are saying that the long-term return can be all the sum of the rewards I got to get there, so +11. So, this is one way to define the long-term return. Any other ideas? [NOISE] [inaudible] reward. Yeah, we probably want to incorporate the time-steps and reduce the reward as, as time passes and in fact this would be called a discounted return versus what you said would be called a return. Here we'll use a discounted return in and it has several advantages, some are mathematical because the return you describe which is not discounted might not converge. It might go up to plus infinity. This discounted return will converge with an appropriate discount. So intuitively also, why is the discounted return intuitive, it's because time is always an important factor in our decision-making. People prefer cash now then cash in 10 years, right? Or similarly, you can consider that the robot has a limited life expectancy, like it has a battery and loses battery every time it moves. So you want to take into account this discount of if I can eat chocolate close, I go for it because I know that if the chocolate is too far, I might not get there because I'm losing some battery, some energy for example. So this is the discounted return. Now, if we take gamma equals one which means we have no discount, the best strategy to follow in the settings seems to be to go to the left or to go to the right starting in the initial state two. Right. And the reason is, it's a simple computation. On one side I get +2, on the other side I get +11. What if my discount was 0.1? Which one would be better? Yeah, the left would be better, directly to plus. And the reason is because we compute in our mind. We just do 0 plus 0.1 times 1, which gives us 0.1, plus 0.1 squared times 10. And it's less than 2. We know it. Okay. So now we're going to assume that the discount is 0.9. And it's a very common discount to use in reinforcement learning and we use a discounted return. So the general question here and it's the core of reinforcement learning in this case of Q-Learning is, what do we want to learn? And this is really, really, think of it as a human, what would you like to learn? What are the numbers you need to have in order to be able to make decisions really quickly, assuming you had a lot more states than that and actions? Any ideas of what we want to learn? What would help our decision-making? Optimal action at each state. Yeah. That's exactly what we want to learn. For a given state, tell me the actions that I can take. And for that I need to have a score for all the actions in every state. In order to store the scores, we need a matrix, right? So, this is our matrix. We will call it a Q table. It's going to be of shape, number of states times number of actions. If I have this matrix of scores and the scores are correct, I'm in state three, I can look on the third row of this matrix and look what's the maximum value I have, is it the first one or the second one? If it's the first one, I go to the left, if it's the second one that is maximum, I go to the right. This is what we would like to have. Does that make sense, this Q table? So now, let's try to build a Q table for this example. If you had to build it, you would first think of it as a tree. Oh and by the way, every entry of this Q table tells you how good it is to take this action in that state. State corresponding to the row, action corresponding to the column. So now, how do we get there? We can build a tree. And that's, that's similar to what we would do in our mind. We start in S2. In S2, we have two options. Either we go to S1, we get 2, or we go to S3 and we get 0. From S2 we- from S1, we cannot go anywhere, it's a terminal state. But from S3, we can go to S2, and get 0 by going back, or we can go to S4 and get 1. That makes sense? From S4, same. We can get 0 by going back to S3 or we can go to S5 and get +10. Now, here I just have my immediate reward for every state. What I would like to compute is the discounted return for all the state because ultimately, what should lead my decision-making in a state is, if I take this action, I get to a new state. What's the maximum reward I can get from there in the future? Not just the reward I get in that state. If I take the other action, I get to another state. What's the maximum reward I could get from that state? Not just the immediate reward that I get from going to that state. So what we will do- we can do it together. Let's say we want to compute the value of, of the actions from S3, from S3 going right to left. From S3, I can either go to S4 or S2. Going to S4, I know that the immediate reward was 1, and I know that from S4, I can get +10. This is the maximum I can get. So I can discount this 10 multiplied by 0.9, 10 times 0.9 gives us 9, + 1 which was the immediate reward gives us 9, gives us, gives us 10. So 10 is the score that we give to the action go right from state S3. Now, what if we do it from one step before S2? From S2, I know that I can go to S3, and to S3 I get zero reward. So the immediate reward is zero. But I know that from S3, I can get 10 reward ultimately on the long-term. I need to discount this reward from one step. So I multiply this 10 by 0.9 and I get 0 plus 0.9 times 10 which gives me 9. So now, in state two going right will give us a long-term reward of 9. Makes sense? And you do the same thing. You can copy back that going from S4 to S3 will give you 0 plus the maximum, you can get from S3 which was 10 discounted by 0.9, or you can do it from S2. From S2, I can go left and get +2, or I can go right, and get 9. And the immediate reward would be 9, would be 0. And I will discount the 9 by 0.9 and get 8.1. So that's the process we would do to compute that. And you see that it's an iterative algorithm. I will just copy back all these values in my matrix. And now, if I'm in state two, I can clearly say that the best action seems to go, seems to say go to the left because the long-term discounted reward is 9, while the long-term discounted reward for going to the right is 2. And I'm done. That's Q-Learning. I solved the problem. I had, I had a problem statement. I found a matrix that tells me in every state what action I should take. I'm done. So, why do we need deep learning? That's the question we will try to answer. So the best strategy to follow with 0.9 is still right, right, right, and the way I see it is, I just look at my matrix at every step. And I follow always the maximum of my row. So, from state two, 9 is the maximum, so I go right. From state three, 10 is the maximum so I still go right. And from state four, 10 is the maximum, so I go right again. So I take the maximum over all the actions in a specific state. Okay. Now, one interesting thing to follow is that when you do this iterative algorithm, at some point, it should converge. And ours converged to some values that represent the discounted rewards for every state and action. There is an equation that this Q-function follows. And we know that the optimal Q-function follows this equation. The one we have here follows this equation. This equation is called the Bellman equation. And it has two terms. One is R and one is discount times the maximum of the Q scores over all the actions. So, how does that make sense? Given that you're in a state S, you want to know the score of going, of taking action a in the state. The score should be the reward that you get by going there, plus the discount times the maximum you can get in the future. That's actually what we used in the iteration. Does this Bellman equation make sense? Okay. So remember this is going to be very important in Q-learning, this Bellman equation. It's the equation that is satisfied by the optimal Q table or Q-function. And if you try out all these entries, you will see that it follows this equation. So when Q didn't, is not optimal, it's not following this equation yet. We would like you to follow this equation. Another point of vocabulary in reinforcement learning is a policy. Policies denoted P sometimes or mu. And- sorry. Pi, pi of S is equal to argmax over the actions of the optimal Q that you have. What it means it is exactly our decision process. It's even that we are in state S. We look at all the columns of the state S in our Q table, we take the maximum. And this is what pi of S is telling us. It's telling us, "This is the action you should take." So, pi, our policy is our decision-making. Okay. It tells us what's the best strategy to follow in a given state. Any questions so far? Okay, and so I have a question for you. Why is deep learning helpful? Yes? The number of states is- is large, is way too large to store. Yeah. That's very easy. Number of states is way too large to store a table like that. So like, if you have a small number of states and number of action, then easily you can you use the Q-table. You can add every state. Looking into the Q-table is super quick, and find out what you should do. But ultimately, this Q-table will get bigger and bigger depending on the application, right? And the number of states for Go is 10 to the power 170 approximately, which means that this matrix should have a number of rows equal to 10 with 170 zeros after it. You-you know what I mean. It's very big. And number of actions is also going to be bigger. In Go, you can place your action everywhere on the board that is available of course. Okay. So, many- way too many states and actions. So, we would need to come up with maybe a function approximator that can give us the action based on the state, instead of having to store this matrix. That's where deep learning will come. So, just to recap these first 30 minutes. In terms of vocabulary, we learned what an environment is. It's the- it's the general game definition. An agent is the thing we're trying to train, the decision-maker. A state, an action, reward, total return, a discount factor. The Q-table which is the matrix of entries representing how good it is to take action A in state S, a policy which is our decision-making function, telling us what's the best strategy to apply in a state. and Bellman equation which is satisfied by the optimal Q- table. Now, we will tweak this Q-table into a Q-function. And that's where we- we shift from Q-learning to deep Q-learning. So, find a Q-function to replace the Q-table. Okay? So, this is the setting. We have our problem statement. We have our Q-table. We want to change it into a function approximator that will be our neural network. Does that make sense how deep learning comes into reinforcement learning here? So now, we take a state as input, forward propagate it in the deep network, and get an output which is an action- an action score. For all the actions. It makes sense to have an output layer that is the size of the number of actions because we don't wanna- we don't wanna give an action as input and the state as input, and get the score for this action taken in this state. Instead, we can be much quicker. We can just give the state as inputs, get all the distribution of scores over the output, and we just select the maximum of this vector, which will tell us which action is best. So if- if we're in state two let's say here. We are in state two and we forward propagate state two, we get two values which are the scores of going left and right from state two. We can select the maximum of those and it will give us our action. The question is how to train this network? We know how to train it. We've been learning it for nine weeks. Compute the loss, back propagate. Can you guys think of some issues that, that make this setting different from a classic supervised learning setting? The reward change is dynamic. Yes? The reward change is dynamic. The reward change is dynamic. So, the reward doesn't change. The reward is set. You define it at the beginning. It doesn't change dynamically. But I think what you meant is that the Q-scores change dynamically. Yeah. That's true. The Q-scores change dynamically. But that's- that's probably okay because our network changed. Our network is now the Q-score. So, when we update the parameters of the network, it updates the Q-scores. What's-what's another issue that we might have? No labels. No labels. Remember in supervised learning, you need labels to train your network. What are the labels, here? [NOISE]. And don't say compute the Q-table, use them as labels. It's not gonna work. [NOISE]. Okay. So, that's the main issue that makes this problem very different from classic supervised learning. So, let's see how- how deep learning can be tweaked a little. And we want you to see these techniques because they- they're helpful when you read a variety of research papers. We have our network given a state gives us two scores that represent actions for going left and right from the state. The loss function that we will define, is it a classification problem or a regression problem? [NOISE] Regression problem because the Q-score doesn't have to be a probability between zero and one. It's just a score that you want to give. And that should look- that should me- meet the long-term discounted reward. In fact, the loss function we can use is the L2 loss function, y minus the Q-score squared. So, let's say we do it for the Q going to the right. The question is, what is y? What is the target for this Q? And remember what I copied on the top of the slide is the Bellman equation. We know that the optimal Q should follow this equation. We know it. The problem is that this equation depends on its own Q. You know like, you have Q on both sides of the equation. It means if you set the label to be r plus gamma times the max of Q stars, then when you will back propagate, you will also have a derivative here. Let me- let me go into the details. Let's define a target value. Let's assume that going, uh, left is better than going right at this point in time. So, we initialize the network randomly. We forward propagate state two in the network, and the Q-score for left is more than the Q-score for right. So, that's the action we will take at this point is going left. Let's define our target y as the reward you get when you go left immediate, plus gamma times the maximum of all the Q values you get from the next step. So, let me spend a little more time on that because it's a little complicated. I'm in s. I move to s next using a move on the left? I get immediate reward r, and I also get a new state s prime, s next. I can forward propagate this state in the network and understand what is the maximum I can get from this state. Take the maximum value and plug it in here. So, this is hopefully what the optimal Q should follow. It's a proxy to a good label. It means we know that the Bellman equation tells us the best Q satisfies this equation. But in fact, this equation is not true yet because the true equation we have Q star here, not Q. Q star which is the optimal Q. What we hope is that if we use this proxy as our label, and we learn the difference between where we are now in this proxy, we can then update the proxy, get closer to the optimality. Train again, update the proxy, get closer to optimality, train again, and so on. Our only hope is that these will converge. So, does it make sense how this is different from deep learning? The labels are moving. They're not static labels. We define a label to be a best guess of what would be the best Q-function we have. Then we compute the loss of where the Q-function is right now compared to this. We back propagate so that the Q-function gets closer to our best guess. Then, now that we have a better Q-function, we can have a better guess. So, we make a better guess, and we fix this guess. And now, we compute the difference between this Q-function that we have and our best guess. We back propagate up. We get to our best guess. We can update our best guess again. And we hope that doing that iteratively will end with the convergence and a Q-function that will be very close to satisfy the Bellman equation, the optimal Bellman equation. Does it make sense? This is the most complicated part of Q-learning. Yeah? So, are you saying we generate [inaudible] of the Q-function? We generate the output of the network, we get the Q function, we compare it to the Q, the best Q function that we think is- it is. What is the best Q function? The one that satisfies the Bellman equation. But we're never actually given the Bellman equation. We don't but we- we guess it based on the Q we have. Okay. So basically, when you have Q you can compute this Bellman equation and it will give you some values. These values are probably closer to where you want to get, to where you- from where you are now. Where you are now is- is further from this optimality, and you want to reduce this gap by, by- like to close the gap, you back propagate. Yes? Is there possibility for this will diverge? So, the question is, is there a possibility for this to diverge? So, this is a broader discussion that would take a full lecture to prove. So, I put a paper here from Fran- Francisco Melo which proves the convergence of this algorithm so, it converges, and in fact, it converges because we're using a lot of tips and tricks that we will see later, but if you want to see the math behind it and it's a, it's a full lecture of proof, I invite you to look at this simple ah proof for convergence of the Bellman equation. Okay. Okay. So, this is the case where our left score is higher than right score, and we have two terms in our target, immediate reward for taking action left and also discounted maximum future reward when you are in state S, S next. Okay. The- the, the tricky part is that let's say, we we compute that we can do it, we have everything, we have everything to compute our target, we have R which is defined by the, by the human at the beginning, and we can also get this number because we know that if we take action left we can then get S next, and we forward propagate S next in the network. We take the maximum output and it's this. So, we have everything in this, in this equation. The problem now is if I plug this and my Q score in my loss function and I ask you to back propagate. Back propagation is what W equals W minus alpha times the derivative of the loss function with respect to W, the parameters of the network. Which term will have a non-zero value? Obviously, the second term Q of S go to the left will have a non-zero value because it depends on the parameters of the network W, but Y will also have a non-zero value. Because you have Q here. So, how do you handle that? You actually get a feedback loop in this back propagation that makes the network unstable. What we do is that we consider this fixed, we will consider this Q fixed. The Q that is our target is going to be fixed for many iteration. Let's say, a million or a 100,000 iteration until we get close to there, and our gradient is small then we will update it and we'll fix it. So, we actually have two networks in parallel, one that is fixed and one that is not fixed. Okay. And the second case is similar. If the Q score to go on the right was more than the Q score to go on the left, we would define our targets as immediate reward of going to the right plus gamma times the maximum Q score we get, if we're in the state that we will be in the next state and take the best action. Does this makes sense? It's the most complicated part of Q-learning. This is the hard part to understand. So, immediate reward to go to the right and discounted maximum future reward when you're in state S next, going to the right. So, this is whole fix for back prop. So, no derivative. If we do that then no problem, Y is just a number. We come back to our original supervised learning setting. Y is a number and we compute the loss and we back propagate, no difference. Okay. So, compute dL- dL over dW and update W using stochastic gradient descent methods. RMS prop Adam whatever you guys want. So, let's go over this, this full DQN, deep Q-network implementation, and this slide is a pseudocode to help you understand how this entire algorithm works. We will actually plug in many methods in this, in this pseudocode. So, please focus right now, and- and if you understand this, you understand the entire rest of the lecture. We initialize our Q-network parameters just as we initialize a network in deep learning, we loop over episodes. So, let's define an episode to be one game like going from start to end to a terminal state, is one episode. We can also define episode sometimes to be many states like Breakout which is the game with the paddle, usually is 20 points. The first player to get 20 points finishes the game. So, episode will be 20 points. Once your looping over episode starts from an initial state S. In our case, it's only one initial state which is state two and loop over time steps. Forward propagate S state two in the Q-network, execute action A which has the maximum Q-score, observe a immediate reward R and the next step S prime. Compute target Y and to compute Y we know that, we need to take S prime, forward propagate it in the network again, and then, compute the loss function, update the parameters with gradient descent. Does this loop make sense? It's very close to what we do in general. The only difference would be this part like we compute target Y using a double forward propagation. So, with forward propagation, we forward propagate two times in each loop. Do you guys have any questions on, on this pseudocode? Okay. So, we will now see a concrete application of a Deep Q-Network. So, this was the theoretical part. Now, we are going to the practical part which is going to be more fun. So, let's look at this game, it's called Breakout. The goal when you play Breakout is to destroy all the bricks without having the ball pass the line on the bottom. So, we have a paddle and our decisions can be idle, stay, stay where you are, move the paddle to the right or move the paddle to the left. Right? And this demo, and you have the credits on the bottom of the slide, uh, shows that after training Breakout using Q-learning they gets a super intelligent agent which figures out a trick to finish the game very quickly. So, actually even like good players didn't know this trick, professional players know this trick, but, uh, in Breakout you can actually try to dig a tunnel to get on the other side of the bricks, and then, you will destroy all the bricks super quickly from top to bottom instead of bottom-up. What's super interesting is that the network figured out this on its own without human supervision, and this is the kind of thing we want. Remember, if we were to use input, the Go boards and output professional players. We will not figure out that type of stuff most of the time. So, my question is, what's the input of the Q-network in this setting? Our goal is to destroy all the bricks. So, play Breakout. What should be the input? [NOISE] Try something. [inaudible] position of bricks. Position of the paddle, position of the bricks. What else? Ball position. Okay. Yeah I agree. So, this is what we will call a feature representation. It means when you're in an environment you can extract some features, right? And these are examples of features. Giving the position of the ball is one feature, giving the position of the bricks, another feature, giving the position of the paddle, another feature. Which are good features for this game, but if you want to get the entire information you'd better do [NOISE] something else. Yeah. The pixels? You don't want any human supervision. You don't wanna put features you- you just. Okay. Take the pixels, take the game, you can control the paddle, take the pixel. Oh, yeah. This is a good input to the Q-network. What's the output? I said it earlier. Probably the output of the network will be 3 Q values representing the action going left, going right and staying idle in a specific state. That is the input of the network. So, given a pixel image we want to predict Q scores for the three possible actions. Now, what's the issue with that? Do you think that would work or not? Can someone think of something going wrong here? Looking at the inputs. [NOISE] Okay. I'm gonna help you. If I give-yeah, you wanna try? [inaudible]. Oh yeah, good point. Based on this image, you cannot know if the ball is going up or down. Actually, it's super hard because the, the action you take highly depends on whether the ball is going up or down, right? If the ball is going down, and even if the ball is going down, you don't even know which direct- direction it's going down. So, there is a problem here definitely. There is not enough information to make a decision on the action to take. And if it's hard for us, it's gonna be hard for the network. So, what's a hack to, to prevent that? Is to take successive frames. So, instead of one frame, we can take four frames-successive frames. And here, the same setting as we had before but we see that the ball is going up. We see which direction is going up, and we know what action we should take because we know the slope of the ball and also, uh, also if it's going up or down. That make sense? Okay. So, this is called the preprocessing. Given a state, compute a function Phi of S. That gives you the history of this state which is the four-sequence of four last frames. What other preprocessing can we do? And this is something I want you to be quick. Like, we we learned it together in deep learning, input preprocessing. Remember that second lecture where the question was what resolution should we use? Remember, you have a cat recognition, what resolution would you wanna use? Here same thing. If we can reduce the size of the inputs, let's do it. If we don't need all that information, let's do it. For example, do you think the colors are important? Very minor. I don't think they're important. So, maybe we can gray scale everything. That removes three chan- that converts three channels into one channel, which is amazing in terms of computation. What else? I think we can crop a lot of these. Like maybe there's a line here we don't need to make any decision. We don't need the scores maybe. So actually, there's some games where the score is important for decision-making. The example is football like, or soccer. Uh, when you're- when you're winning 1-0, you you'd better if you're playing against a strong team defend like, get back and defend to keep this 1-0. So, the score is actually important in the decision-making process. And in fact, uh, there are famous coach in football which have this technique called park the bus, where you just put all your team in front of the goal once you have scored a goal. So, this is an example. So, here there is no park the bus. But, uh, we can definitely get rid of the score, which removes some pixels and reduces the number of computations, and we can reduce to grayscale. One important thing to be careful about when you reduce to grayscale is that grayscale is a dimensionality reduction technique. It means you- you lose information. But you know, if you have three channels and you reduce everything in one channel, sometimes you would have different color pixels which will end up with the same grayscale value depending on the grayscale that you use. And it's been seen that you lose some information sometimes. So, let's say the ball and some bricks have the same grayscale value, then you would not differentiate them. Or let's say the paddle and the background have the same grayscale value, then you would not differentiate them. So, you have to be careful of that type of stuff. And there's other methods that do grayscale in other ways like luminance. So, we have our Phi of S which is this, which is this uh, input to the Q network, and the deep Q network architecture is going to be a convolutional neural network because we're working with images. So, we forward propagate that, this is the architecture from Mnih, Kavukcuoglu, Silver at al from Deep Mind. CONV ReLU, CONV ReLU, CONV ReLU, two fully connected layers and you get your Q-scores. And we get back to our training loop. So, what do we need to change in our training loop here? Is we said that one frame is not enough. So, we preprocess all the frames. So, the initial state is converted to Phi of s. The forward propagation state is Phi of s and so on. So, everywhere we had s or s prime, we convert to Phi of s or Phi of s prime which gives us the history. Now, there are a lot more techniques that we can plug in here, and we will see three more. One is keeping track of the terminal state. In this loop, we should keep track of the terminal state because we said if we reach a terminal state, we want to end the loop, break the loop. Now, the reason is because the y function. So basically, we have to add, create a Boolean to detect the terminal state before looping through the time steps. And inside the loop, we wanna check if the new s prime we are going to is a terminal state. If it's a terminal state, then I can stop this loop and go back, play another episode. So, play another, start at another starting state, and continue my game. Now, this y target that we compute is different if we are in a terminal state or not. Because if we're in a terminal state, there is no reason to have a discounted long-term reward. There's nothing behind that terminal state. So, if we're in a terminal state, we just set it to the immediate reward and we break. If we're not in a terminal state, then we would add this discounted future reward. Any questions on that? Yep, another issue that we're seeing this and which makes, uh, this reinforcement learning setting super different from the classic supervised learning setting is that we only train on what we explore. It means I'm starting in a state s. I compute, I forward propagate this Phi of s in my network. I get my vector of Q-values. I select the best Q-value, the largest. I get a new state because I can move now from state s to s prime. So, I have a transition from s take action A, get s prime or Phi of s. Take action A, get Phi of s prime. Now, this is, is what I will use to train my network. I can forward propagate Phi of s prime again in the network, and get my y target. Compare my y to my Q and then backpropagate. The issue is I may never explore this state transition again. Maybe I will never get there anymore. It's super different from what we do in supervised learning where you have a dataset, and your dataset can be used many times. With batch gradient descent or with any gradient descent algorithm. One epoch, you see all the data points. So, if you do two epochs, you see every data point two times. If you do 10 epochs, you see every data points three times or 10 times. So, it means that every data point can be used several time to train your algorithm in classic deep learning that we've seen together. In this case, it doesn't seem possible because we only train when we explore, and we might never get back there. Especially because the training will be influenced by where we go. So, maybe there are some places where we will never go because while we train and while we learn, it will, it will kind of direct our decision process and we will never train on some parts of the game. So, this is why we have other techniques to keep this training stable. One is called experience replay. So, as I said, here's what we're currently doing. We have Phi of s, forward propagate, get a. From taking action a, we observe an immediate reward r, and a new state Phi of s prime. Then from Phi of s prime, we can take a new action a prime, observe a new reward r prime, and the new state Phi of s prime prime, and so on. And each of these is called a state transition, and can be used to train. This is one experience leads to one iteration of gradient descent. E1, E2, E3, Experience 1, Experience 2, Experience 3. And the training will be trained on Experience 1 then trained on Experience 2 then trained on Experience 3. What we're doing with experience replay is the following. We will observe experience 1 because we start in a state, we take an action. We see another state and a reward and this is called experience 1. We will create a replay memory. You can think of it as a data structure in computer science and you will place this Experience 1 tuple in this replay memory. Then from there, we will experience Experience 2. We'll put Experience 2 in the replay memory. Same with Experience 3, put it in the replay memory and so on. Now, during training, what we will do is we will first train on Experience 1 because it's the only experience we have so far. Next step, instead of training on E 2, we will train on a sample from E 1 and E 2. It means we will take one out of the replay memory and use this one for training. But we will still continue to experiment something else and we will sample from there. And at every step, the replay memory will become bigger and bigger and while we train, we will not necessarily train on the step we explore, we will train on a sample which is the replay memory plus the new state we explored. Why is it good is because E 1 as you see can be useful many times in the training and maybe one was a critical state. Like it was a very important data point to learn or a Q function and so on and so on. Does the replay memory makes sense? So, several advantages. One is data efficiency. We can use data many times. Don't have to use one data point only one time. Another very important advantage of experience replay is that if you don't use experience replay, you have a lot of correlation between the successive data points. So, let's say the ball is on the bottom right here, and the ball is going to the top left. For the next 10 data points, the ball is always going to go to the top left. And it means the action you can take, is always the same. It actually doesn't matter a lot because the ball is going up. But most likely you wanna follow where the ball is going. So, the action will be to go towards the ball for 10 actions in a row. And then the ball would bounce on the wall and on the top and go back down here, down to the bottom left-to the bottom right. What will happen if your paddle is here is that, for 10 steps in a row you will send your paddle on the right. Remember what we said when we, when we ask the question if you have to train a cat versus dog classifier with batches of images of cats, batches of images of dog, train first on the cats then trains on the dogs, then trains on the cats, then trains on the dogs. We will not converge because your network will be super biased towards predicting cat after seeing 10 images of cat. Super biased with predicting dogs when it sees 10 images of dog. That what's happening here. So, you wanna de-correlate all these experiences. You want to be able to take one experience, take another one that has nothing to do with it and so on. This is what experience replay does. And the third one, is that the third one is that you're basically trading computation and memory against exploration. Exploration is super costly. The state-space might be super big, but you know you have enough computation probably, you can have a lot of computation and you have memory space, let's use an experience replay. Okay. So, let's add experience replay to our code here. The transition resulting from this part, is added to the experience to the replay memory D and will not necessarily be used in the iteration space. So, what's happening is before we propagate phi of S, we get, we observe a reward and an action. And this action leads to a state phi of S prime. This is an experience. Instead of training on this experience, I'm just going to take it, put it in the replay memory. Add experience to replay memory. And what I will train on is not this experience, it's a sampled random mini-batch of transition from the replay memory. So, you see, we're exploring but we're not training on what we explore. We're training on the replay memory, but the replay memory is dynamic. It changes. And update using the sample transitions. So, the sample transition from the replay memory will be used to do the update. That's the hack. Now, another hack we want, the last hack we want to talk about is exploration versus exploitation. So, as a human, and let's say you're commuting to Stanford every day and you know the road you're commuting at. You know it. You always take the same road and you're biased towards taking this road. Why? Because the first time you took it it went well. And the more you take it, the more you learn about it not that it's good to know the tricks of how to drive fast but but like you know the tricks, you know that this, this, this light is going to be green at that moment and so on. So, you, you, you build a very good expertise in this road, super expert. But maybe there's another road that you don't wanna try that is better. You just don't try it because you're focused on that road. You're doing exploitation. You exploit what you already know. Exploration would be- okay let's do it. I'm gonna try another road today, I might get late to the course but maybe I will have a good discovery and I would like this road and I will take it later on. There is a trade off between these two because the RL algorithm is going to figure out some strategies that are super good. And will try to do local search in these to get better and better. But you might have another minimum that is better than this one and you don't explore it. Using the algorithm we currently have, there's no trade-off between exploitation and exploration. We're almost doing only exploitation. So, how to incentivize this exploration. Do you guys have an idea? So, right now, when we're in a state S, we're forward propagating the states, for all states in the network and we take the action that is the best action always. So we're exploiting. We're exploiting what we already know. We take the best action. Instead of taking this best action, what can we do? Yeah. Monte Carlo sampling. Monte Carlo sampling, good point. Another one, you wanted to try something else? Could have a parameter that's the ratio times you take the best action versus exploring another action. Okay. Take a hyper-parameter that tells you when you can explore, when you can exploit. Is that what you mean? Yeah, that's a good point. So, I think that that's a solution. You can take a hyper-parameter that is a probability telling you with this probability explore, otherwise with one minus this probability exploit. That's what, that's what we're going to do. So, let's look why exploration versus exploitation doesn't work. We're in initial state one, S1. And we have three options. Either we go using action A1 to S2 and we get a reward of zero, or we go to action use action 2, get to S3 and get reward of 1 or use action 3 and go to S4, and get a reward of 1,000. So, this is obviously where we wanna go. We wanna go to S4 because it has the maximum reward. And we don't need to do much computation in our head. It's simple, there is no discount, it's direct. Just after initializing the Q-network, you get the following Q-values. Forward propagate S1 in the Q-network and get 0.5 for taking action 1, 0.4 for taking action 2, 0.3 for taking action 3. So, this is obviously not good but our network, it was randomly initialized. What it's telling us is that 0.5 is the maximum. So, we should take action 1. So, let's go. Take action 1, observe S 2. You observe a reward of 0. Our target because it's a terminal state is only equal to the reward. There is no additional term. So, we want our target to match our Q. Our target is zero. So, Q should match zero. So, we train and we get the Q that should be zero. Does that makes sense? Now, we do another round of iteration. We look we are in S1, we get back to the beginning of the episode we see that our Q function tells us that action two is the best. Because 0.4 is the maximum value. It means go to S3. I go to S3, I observe reward of 1. What does it mean? It's a terminal state. So, my target is 1. Y equals 1. I want the Q to match my Y. So, my Q should be 1. Now, I continue third step up. Q function says go to A2. I go to A2 nothing happens. I already matched the reward. Four step go to A2, you see what happens? We will never go there. We'll never get there because we're not exploring. So, instead of doing that, what we are saying is that five percent of the time, take your random action to explore and 95 percent of the time follow your exploitation. Okay. So, that's where we add it. With probability epsilon, the hyper-parameter take random action A, otherwise do what we were doing before, exploit. Does that make sense? Okay, cool. So, now we plugged in all these tricks in our pseudo code and this is our new pseudo code. So, we have to initialize a replay memory which we did not have to do earlier. In blue, you can find the replay memory added lines. In orange, you can find the added lines for checking the terminal state and in purple, you can find the added lines related to epsilon-greedy, exploration versus exploitation. And finally in bold, the pre-processing. Any questions on that? So, that's, that's we wanted to see a variant of how deep learning can be used in a setting that is not necessarily classic supervised learning setting. Can you see that the main advantage of deep learning in this case is it's a good function approximator? Your convolutional neural network can extract a lot of information from the pixels that we're not able to get with other networks. Okay. So, let, let's see what we have here. We have our super Atari bot that's gonna dig a tunnel, and it's going to destroy all the bricks super quickly. It's good to see that after building it. So, this is work from DeepMind's team, and you can find this video on YouTube. Okay, another thing I wanted to say quickly is what's the difference between with and without human knowledge? You will see a lot of people-a lot of papers mentioning that this algorithm was trained with human learned knowledge, or this algorithm was trained without any human in the loop. Why is human knowledge very important? Like, think about it. Just playing one game as a human and teaching that to the algorithm will help the algorithm a lot. When the algorithm sees this game, what it sees is pixels. What do we see when we see that game? We see that there is a key here. We know the key is usually a good thing. So, we have a lot of context, right? As a human. We know I'm probably gonna go for the key. I'm not gonna go for this-this thing, no. Uh, same, ladder. What is the ladder? We-we directly identify that the ladder is something we can go up and down. We identify that this rope is probably something I can use to jump from one side to the other. So as a human, there is a lot more background information that we have even without knowing it-without realizing it. So, there's a huge difference between algorithms trained with human in the loop and without human in the loop. This game is actually Montezuma's Revenge. The DQN algorithm when the paper came out on-on the nature-on nature- in Nature the-the second-the second version of the paper. They showed that they-they beat human on 49 games that are the same type of games I-as Breakout. But this one was the hardest one. So, they couldn't beat human on this one. And the reason was because there's a lot of information and also the game has-is very long. So in order-it's called Montezuma's Revenge and. I think Ramtin Keramati is going to talk about it a little later. But in order to get to win this game, you have to go through a lot of different stages, and it's super long. So, it's super hard for the-the-the algorithm to explore all the state space. Okay. So, that slide I will show you a few more games that-that the DeepMind team has solved. Pong is one. SeaQuest is another one, and Space Invaders that you might know which-which is probably the-the most famous of the three. I hope you know. Okay. So, that said, I'm gonna hand in the microphone to-we're lucky to have an RL expert. So,  Ramtin Keramati is a fourth year PhD student, uh, in RL working with Professor Brunskill at Stanford. And he will tell us a little bit about his experience and he will show us some advanced applications of deep learning in RL, and how these plug in together. Thank you. Thanks Kian for that introduction. Okay. Can everyone hear me now? Right, good. Cool. Okay first, I have like, 8-9 minutes. You have more. I have more? Yes. Okay. Great, okay first question. After seeing that lecture so far like, how many are you-of you are thinking that RL is actually cool? Like, honestly. That's like, oh that's a lot. Okay. [LAUGHTER] That's a lot. Okay. My hope is after showing you some other advanced topics here, then the percentage is gonna even increase. So, let's [LAUGHTER] let's see. Uh, it's almost impossible to talk about it like, advancements in RL recently without mentioning Alpha Go I think  right now wrote down on a table that it's almost 10 to the uh, power of 170 different configuration of the board. And that's roughly more than-I mean that's more than the estimated number of atoms in the universe. So, one traditional al-algorithm that before like deep learning and stuff like that. Was like tree search in RL, which is basically go exhaustively search all the-all the possible actions that you can take, and then take the best one. In that situation Alpha Go that's all-all almost impossible. So, what they do that's also a paper from DeepMind is that they train a neural network for-for that. They kind of merge the tree search with-with deep learning, a neural network that they have. They have two kinds of networks. One is called the value network. Value network is basically consuming this image-image of a board and telling you what's the probability that if you can win in this situation. So, if the value is higher, then the probability of winning is higher. Oh, how does it help you-help you in the case that if you wanna search for the action, you don't have to go until the end of the game because the end of the game is a lot of steps, and it's almost impossible to go to the end of the game in all these simulations. So, that helps you to understand what's the value of each game like, beforehand? Like, after 48 step or 58 step if you're gonna win that game or-or if you're gonna lose that game? There's another network of the policy network which helps us to take action. But I think the most interesting thing of the Alpha Go is that it's trained from scratch. So, it trains from nothing, and they have a tree called self play that-there is two AI playing with each other. The best one I-replicate the best-the best one you keep it fixed, and I have another one that is trying to beat the previous version of itself. And after it can beat the previous version of itself like, reliably many times, then I replace this again for the previous one. And then I adjust it. So, this is a training curve of like itself-a self play of the Alpha Go as you see. And it takes a lot of compute. So, that's kind of crazy. But finally they beat the human. Okay. Another type of algorithm, and this is like, the whole different class of algorithm called policy gradients. Uh-. We have developed an algorithm called trust region policy optimization. Can I stop that? [LAUGHTER]. This method was abled when locomotion controllers for [OVERLAPPING]. Can you mute the sound please? Okay, great. So, policy gradient algorithm. [LAUGHTER]. Well, what I can do is restart this from here. Uh- No. That is not. Doesn't work. Okay. Okay. So, here like in the DQN that you've seen, uh, you-you came and like, compute the Q-value of each state. And then what you have done is that you take the argmax of this with respect to action and then you choose the action that you want to choose, right? But what you care at the end of the day is the action which is the mapping from a state to action, which we call it a policy, right? So, what you care at the end of the day is actually the policy. Like, what action should they take? Is not really Q value itself, right? So, this class of- class of methods that is called policy gradients, is trying to directly optimize for the policy. So, rather than updating the Q function, I compute the gradient of my policy. I update my policy network again, and again, and again. So, let's see these videos. So, this is like this guy, that is trying to reach the pink uh, ball over there, and sometimes like gets hit by the some external forces. And this is called uh, a raster algorithm, call it like PPO. This is a policy gradient. I try to reach for that ball. So, I think that you've heard of, ah, OpenAI like five, like the bot that is playing DOTA. So, this is like, completely like, PPO algorithm. And they have like, a lot of compute showing that, and I guess I have the numbers here. There is 180 years of play in one day. This is how much compute they have. Uh, so that's fine. There's another even funnier video. Its called Competitive Self-Play. Again, the same idea with policy gradient. Inside you put two agents in front of each other, and they just try to beat each other. And if they beat each other, they get a reward. The most interesting part is that- for example in that game, the purpose is just to-to pull the other one out, right? But they understand some emerging behavior which is if- for us human makes sense but for them to learn out of nothing is kind of cool. [NOISE] So there's like one risk here that when they're playing, oh, this, uh, this guy's trying to kick the ball inside. One, one risk here is to overfit. [LAUGHTER] That's also cool. [LAUGHTER] Oh, yeah. One technical point before we move, one technical point here is that here, wait, where is the, no, the next one. Okay. Here that two, our two agent are p- p- playing with each other, and we are just updating the person with the best other agent, like previously, we are doing like a self-play, is that you overfit to the actual agent that you have in front of you. So, uh, the agent in front of you is powerful, but you might overfit to this, and if I, uh, put the agent that is not that powerful but is using this simple trick that the powerful agent, like, never uses, then you might just l- lose the game, right? So, one trick here to make it more stable is that rather than playing against only one agent, you'd alternate between different version of the agent itself, so it all, like, learns all this skill together. It doesn't overfit to this level. So, there's another, uh, thing called like, meta learning. Meta learning is a whole different algorithm again, [NOISE] and the, and the purpose is that a lot of tasks are like similar to each other, right? For example, walking to left and walking to right are like walking in the front direction, they're like same tasks, essentially. [NOISE] So, the point is, rather than training on a single task which is like go left or go right, you train on a distribution of tasks that are similar to each other. [NOISE] And then the idea is that, for each specific task, I should learn with like, uh, very few gradient steps, so very few updates should be enough for me. So, if I learn, okay, play this video, it's like, at the beginning, this agent has been trained with meta learning before, doesn't know how to move, but just look at the number of gradient steps, like, after two or three gradient steps, it totally knows how to move. That's, th- that's normally takes a lot of the steps to train, [NOISE] but that's only because of the meta learning approach that we've used here. [NOISE] Meta learning is also cool, I mean, uh, the algorithm is from Berkeley, Chelsea Finn, which is now also coming to Stanford. It's called Model-Agnostic Meta-Learning. [NOISE] So, all right. Another point this, uh, very interesting game, Montezuma's Revenge, that Kian talked. How much time do we have? [inaudible]. All right. Uh, yeah. So, you've seen, uh, exploration-exploitation dilemma, right? So it's, it's, it's bad if you don't ex- explore, you gonna fail many times. So, if you do the exploration with the scheme that you just saw like epsilon-greedy, this is a map of the Montezuma game, and you gonna see all different moves of that game, if you do like, exploration randomly. And, uh, the game, I think, has, like, 21 or 20 something different rooms that is hard to reach. [NOISE] So, there's this recent paper I think from Google Brain from Marc Bellemare and team. It's called Unifying the Count-based Metas for Exploration. Exploration's essentially a very hard challenge, mostly in the situation that the reward is a sparse. For exactly, in this game, the first reward that you get is when you reach the key, right? [NOISE] And from t- top to here, it's almost like 200 a steps, and g- getting the number of actions after 200 steps exactly right by, like, random exploration, is almost impossible, so you're never gonna do that. [NOISE] What, uh, a very interesting trick here is that you're kind of keeping counts on how many times you visited a state, [NOISE] and then if you visit a state, that is, s- uh, [NOISE] that has like, uh, fewer counts, then you give a reward to the agent, so we call it the intrinsic reward. So, it kind of makes the- Let's change your mic really quick. [NOISE] [LAUGHTER] Right here, I keep it. [NOISE] S- so, it changes the, [NOISE] looking for the reward is [inaudible] environment is also, intense you up, like it, it has the instant reach because you go and search around because you gotta increase the counts of the state that it has never seen before. So, this gets the agent to actually explore and look more, so it just [NOISE] goes down usually like different rooms and stuff like that. [NOISE] So, these identities, and this game is interesting, if you search this, there's a lot of people that recently are trying to solve the game, as it includes research on that Montezuma's is one of the game, and it's just f- fun also to see the agent play. Any question on that? [NOISE] [inaudible] [LAUGHTER] Any question? [NOISE] Well, I- There is also [NOISE] another interesting [NOISE] point that would be just fun to know about. It's called imitation learning. Imitation learning is the case that, well, I mean, RL agents, so sometimes you don't know the reward, like, for example, the Atari games, their reward is like, very well-defined, right? If I get the key, I get the reward, that's just obvious, but sometimes, like, defining the reward is hard. For example, when the car, like the blue one, wanna drive in a, in some highway, what is the definition of the reward, right? So, we don't have a clear definition of that. But, on the other hand, you have a person, like you have human expert that can drive for us, and then this is, "Oh, this is the right way of driving," right? So, in this situation, we have something called imitation learning that you try to mimic the behavior of a expert. [NOISE] So, not exactly copying this, because if we copy this and then you show us it completely different states, then we don't know what to do, but from now, we learn. And this is like, for example, and there is a paper that called Generative Adversarial Imitation Learning, which was, like, from Stefano's group here at Stanford, and that was also interesting. [NOISE] Well, I think that's advanced topic. If you have any questions, I'm here. Kian. [NOISE] Question? [NOISE] No? [NOISE] Okay. Sorry. Just, uh, for, for, uh, next week, so there is no assignment. We have not finished at C-5 and you know about sequence models now. Uh, we all need to take a lot of time for this project. The project's the big part of this because this has no, has, um, [NOISE] there's gonna be, um, [NOISE] project team mentorship. And this Friday, we'll have these sections with reading research papers. We go over the, the, object detection YOLO and YOLO v2 papers from Redmon et al. Okay. See you guys. Thank you. 

So hello everyone, and welcome for the last lecture of CS230 Deep Learning. So it's been ten weeks and you've been, you've been studying deep learning all around starting with fully connected networks, uh, understanding how to boost these networks and make them better and then, uh, using recurrent neural networks in the last part as- and convolutional neural networks in the fourth part, uh, to build models for imaging and text and other applications. So today's class wrap-up and, uh, the lecture might be, uh, slightly, um, shorter than usual. But we're going to go over a small case study on conversational assistants to start with which is an industrial topic. Um, we will do a small quiz competition with Menti, and the fastest person who has the best answer will win 400 hours of GPU credits on Amazon. [LAUGHTER] So you guys can, can start, can start working on it. Um, we will see some class project advice because you guys have about two weeks, uh, less than two weeks before to post your presentation, and the final project due date. We'll also go over some of the next steps after CS230, what have our students done over the past year and what we think are good next steps and closing remarks to finish. Oh by the way, if you have a Clicker with, with batch would you please, can you bring it to me. Um, okay. So let's get started with how to build a chatbot to help students find or/and enroll in the right course. So this is going to be a pretty simple case of a chatbot because chatbots and convertional- conversational assistants in general have been very hard to build and have been an industrial topic. There are some places where academia has helped, uh, the chatbot improvement. And here we're going to see how we can take all our algorithms, what we've learned in this class and plug it in in a conversational setting. That sounds good? So let me give you an example, students might write to the chatbot, "Hi, I want to enroll in CS106A for Winter 2019 to learn coding." The chatbot can answer, "For sure. I just enrolled you." So that would be one goal of the chatbot. A second example might be finding information about classes, "Hi, what are the undergraduate level history classes offered in spring 2019?" Then the chatbot can get back to the students and say, "Here's the list of history classes offered in Spring 2019." So we're making a small assumption here. We're building a chatbot for a very restricted area. In general, and a lot of time, chatbots which work very well are super goal-oriented or transactional and the state of possible utterances or requests from users is small, smaller than what you could expect in other industrial settings. So here we're making the assumption that the students will only try to find information about a course, or will try to enroll in the course. So I want you guys to, to pair in groups of two or three and try to come up with ideas of what methods that we've seen together can be used in order to implement such a chatbot. Okay? So take a minute [NOISE] introduce yourself to your mates and, and try to figure out which methods can be leveraged in this case. Okay. Let's see what we have here. RNNs for natural language processing, yeah, transfer learning. LSTM to pick out important words from inputs based on those input triggers, output, some predefined information from storage. Yeah. So this seems to, to say that there's going to be one learning part where we need to have probably recurrent neural networks helping out and one other knowledge base or storage part where we can retrieve some information. We're going to see that. Some attention models is- showed that today a lot of, uh, natural language processing models are built with attention models. [NOISE] RNN for speech recognition and speech generation for. So we didn't talk about the speech part. So far we assumed that the conversational assistant is text-based, uh, but later on we will see what happens if we wanna add speech to it. Fancy methods. Oh, reinforcement learning for making decisions about responses. That's interesting. So why do you guys think we, we would need reinforcement learning? Yes. It allows you to have certain [inaudible] context in different states and we'll also have like a value associated with it, you said it's very goal-oriented. [inaudible] Yeah, that's good. So just to repeat, it's important to keep a notion of context and also we have a sequence of utterances from the user and, uh, the conversational assistant and probably the outcome of the conversation would come far along the way and not at every step. So that's true. Uh, Reinforcement learning has been, uh, a research topic for conversational assistants as well and oftentimes we will try to learn a policy for the chatbot which given a state will tell us what action to take next. This can be done using Q-Learning which is the method we've seen together, or sometimes with policy gradients. Okay. Word encoding, sorry word embedding probably. Okay. Cool. So I agree there's many ways to, to plug in a deep learning algorithm in, in this chatbot setting, we're going to see a few of them. First, I'd like to introduce some vocabulary which is commonly used when talking about conversational assistants, conversational assistants. An utterance is, you can think of it as a user input. So if I say the student utterance, it's the sentence that was written by the students for the chatbot. The assistant utterance is the one coming from the chatbot side. The intent denotes the intention of the user. So in our case, we will have two intents, which is very limited. The user either wants to find information from, for a course or the user wants to, uh, enroll in a class. These are two different intentions that are probably to be detected early on in the conversation. And then you have something called slots. Slots, uh, are used to gather multiple information from the user on a specific intent that a user has. So let's say the student wants to enroll in a class. In order to enroll the student in a class, you need to fill in several slots. You need to understand probably which class the student is talking about, which quarter the student wants to enroll in the class [NOISE] which year is the student talking about and eventually you want to know the  SUid of the students. But probably we can assume that the SUid is already encoded in a conversation on the environment we're in. So these are three vocabulary and we're also going to talk about turns for conversational assistance. So, so single turn, uh, conversation is when there is just a user utterance and a response and multi-turn is when there is several user utterances, hence, uh, conversational assistant utterances. And you understand that multi- multi-utterance conversations are harder to understand because we need to track context. Our assumption today will be that we work in an environment with limited intents and slots. It means we can define two intents and for each of these two intents, there are several slots that we wanna fill in, it's going to make our life easier. Of course in practice you can have multi- myriads of intents and slots and you- the, the task becomes more complicated when you have more of those. So my first question will be, how to detect the intent based on, uh, the user utterance. Can you talk about what kind of data set you need to build in order to train the model to detect the intent? [NOISE] [NOISE] Or what type of network you wanna use. [NOISE] There's not a single good answer, so go for it if you wanna brainstorm [NOISE]. So, I, I, I think there's, there's going to be two options obviously because we have a, we have a sequence coming in, which is the user input. We might want to use a recurrent neural network [NOISE] to encode long-term dependencies or we might want to use a convolutional network. Actually, uh, convolutional networks have some benefits that recurrent neural networks don't have and they, they might work better for example, if the intent we're looking for is always encoded in a small number of words somewhere in the input sequence, because you will have a filter scanning that and the filter can detect the intent. So, if you have a filter that was trained in order to detect the intent, inform, another filter trained to detect the intent enroll, then these two filters will detect the word enroll or the word I'm looking for, and so on, in order to detect the intent, okay? In terms of data, what you probably need is pairs of user utterances along with the intent of the user. So, you will need to label the datasets like this one with X and input, I want to, so it's padded, I want to enroll in CS106A for Winter 2019 to learn coding. And this, you will label it as enroll. And notice that enroll here is a function. [NOISE] So the, the label is [NOISE] actually noted as a function. And the reason is because we can call this function in order to retrieve information. And another example is, hi, what are the undergraduate level history classes offered in Spring 2019? And this will be labeled as inform. So it's probably a two-class classification or three classes if you wanna add a third class that corresponds to other intent. A user might want to use this chatbot for another intent that the chatbot wasn't built for. So, these are the classes, enroll and inform. And what's interesting is that if we identify that the intent of the user is enroll, we probably want to call an API or to request information from another server. And in this case, it might be Axess because the platform we use to enroll in classes is Axess. And then, to retrieve information in order to help the user about their classes, we can probably call Explorecourses, assuming that these, these services have APIs, these services have APIs. Does that make sense? And now, the interesting part is that the enroll function might request some input that you have to identify. Those will be the slots, same for the inform function, okay? So, we could train a sequence classifier, either convolutional or record. And this, we're not going to go into the details, you've learned it in the sequence models class. How to detect the slots now. So in terms of data, it's going to look very similar to the previous one, but we will have a sequence to sequence problem now where the user utterance will be, uh, a sequence of words and the slots tag will also be a sequence. So, for example, show me the Tuesday the 5th of December flights from Paris to Kuala Lumpur, if you were to build conversational assistants for flights booking, then the label you wanna have is probably something like that. It doesn't have to be exactly this, but y denotes zero. For some of the words, the sequences B-DAY I-DAY O O B-DEP B-ARR I-ARR. What do you think these correspond to and why do we need that? [NOISE] We've probably, you've probably seen that in, in the sections a few weeks back. [NOISE] So, why do we denote these labels in a certain format? [NOISE]. Uh-huh. It helps you identify the slot tag. And like, one of them is departure, arrival, arrival, and then the other one, for day and possible [inaudible] [NOISE] Yeah. Yeah, correct. So, [NOISE] uh, I agree with what you said for day, day, departure, arrival, arrival. So, these words are encoding day, departure, and arrival. How about the B, and the I, and the O? Yeah, someone has an idea? Yeah. Beginning, beginning the [inaudible] one word. Yeah, exactly. B, B denotes beginning while I denotes in or inside, and O out or output, general. So, what happens here is that sometimes you would have a slot which might be filled by several words and not a single word, and you wanna be able to detect this entire chunk. It's called chunking. Uh, so you would use a special encoding in order to identify if this word is the beginning of a word that you wanna fill in the slots or is the end, or inside, or out of the word you wanna fill in the slot. And then, day, departure, and arrival are three possible slots that we wanna fill in in order to be able to book the flight. If you don't receive these slots, you might wanna have your chatbot request the slots later. Okay. So, another example in, uh, and classes here can be day, departure, arrival, class, like, do you want to travel in eco or business, uh, number of passengers that you wanna have on your flight. Uh, if we were, uh, for our chatbot here, it would be, hi, I want to enroll in CS106A for Winter 2019 to learn coding, and we will encode it by B, beginning of the code of the class, beginning of the quarter, and beginning of the year. [NOISE] That would be a possible encoding. And then you will train, uh, using, ah, probably a recurrent neural network, an algorithm to predict all these tags. That makes sense? So, now we have already two models that are running our chatbots. One that is for the intent and one that is for the tags. What do you think about joint training? Do you think it's something we could do here? And what do I mean by joint training? [NOISE] Yeah. Training out all of the different codes, like training for in fact quarter year and class, rather than training in separate networks for each of those, like the joint element of the training. Not training for different codes, no. I was talking more about training for different tasks. So, intent and intent for enrolling, intent for, uh, intent and, and uh, and slots tagging. [NOISE] Because here we have one intent classifier which takes an input sequence and outputs a single class, and we have a slot tagger which takes the same input, exactly the same input and tags every single word in the sequence. [NOISE] So, probably we can use joint training in order to train one network that might be able to do both. And this network would be jointly trained with two different loss functions. One for the intent and one for the slots. It's usually helpful to jointly train two networks especially in the earlier layers because you end up learning the same type of feature. [NOISE] That's, that's interesting for natural language processing. Yeah. How do you do the joint loss function for it? Does it calculate both losses independently and sum them together or is there a trade off between training networks versus training thoughts. So the question is how would you describe the loss function in this joint training? You would actually sum two loss functions. The two loss functions you were using you would just sum them and hope that's, uh, the back propagation will train actually both networks. And the networks will probably have a common base and then would be separated after. So let's say you have a first LSTM layer that encodes some information about, uh, your user utterance, then this will give, uh, will give its output to two different networks, which wil- will be trained separately, okay? And class- these here are codes for the class, quarter, year, and SUID, assuming SUID is already in the environment, we will not need to request it. So can you tell me how to acquire this data now that we've seen it? So take- take about a minute to discuss with your mates, uh, how to acquire that type of data and then answer on Menti. [NOISE] Okay. So let's go over some of the answers. [MUSIC] Mechanical Turk, have people manually collect, annotate the data. That's true. So as we discussed earlier in the quarter, this would be the method which is probably the more rigorous, uh, when it's applied with a specific, uh, labeling process and data collection process. It will take more time. So you would have to build, uh, a UI, uh, user interface for them to be able to label all this data, which is not trivial in general. Amazon Mechanical Turk, pay large number of Stanford students, that works. Have a human chat assistance service user and enter the data in, hand label data. Yeah, I think you can start with hand labeling probably. Can auto-generate some data by substituting date, courses, quarter and other tags. Oh, that's a good idea. So who wrote that? Someone wants to comment, yeah. I think you can already have the annotations having something like a base at the end of the annotation [inaudible]. Yeah, that's a good idea. So I repeat for the SCPD students. Um, we already have a bunch of possible dates. We can easily find a list of dates. You've done it in one assignment, right? Uh, where you were using neural machine translation to transfer for human-readable dates to machine-readable dates. So we have datasets of dates, so we could use that. Uh, we also have a list of courses that we can probably find on ExploreCourses. We know that there are not too many quarters and- and we ave- have probably it's a basis for any other tag like list of possible SUIDs or- or like seven figures, something like that. So all numbers of seven figures hopefully. Um, and then we can have sentences with like blank spots where we insert these and we can generate a lot of data using this insertion scheme automated, and every time we insert, we can label. We're going to see that. Um, I like this idea as well. Use a part of speech tagger, identity recognition model to identify examples requests that are found elsewhere. So one thing we discussed in section is that you have available models to do part of speech tagging. Right. So why don't we use them? These are trained really well. And we could give our user utterances that we collected online, uh, and tag them automatically using these good models. Of course, it's not gonna be perfect, but we can at least get started with that and leverage a model that someone else has built to tag and label our dataset. Okay. Good ideas here. So let's see the data generation process which is the most strategic to start with I would say. Uh, we would have, talking about the- the flight booking virtual assistant. We would have a database of all the departure locations. So whatever, uh, Paris, London, uh, Kuala Lumpur and a lot of arrivals as well. So these are a list of cities that have airports probably in the world. And we will have a list of way to write days and also class, business, eco, eco plus, premium, I don't know, whatever you want, uh, and user utterances. And then what we will do is that, we will pull a user utterance from the database such as this one. I would like to book a flight from dep to arrival for, uh, in- in- in business class, let's say in class for this day. And then we can plug-in from dataset randomly the slots. Does that makes sense? We can generate a lot of data using this process. So this user utterance can be augmented in virtually tens or hundreds of different combinations. So that's one way to augment your dataset automatically and label it, but you also need hand labeled data because, uh, you don't want your model to overfit to this specific type of user utterances, okay? And so- so same for our virtual assistant for the- for- for the university. Hi, I want to enroll in code for a quarter year and then we can insert from the database the quarter, the year and the code of different classes, so that we can train our network on that. Does this date augmentation makes sense? So these are common tricks you would see in- in various papers and this is an example of one of them. Okay. So we can label automatically when inserting and we can train a sequence-to-sequence model in order to fill in the slots. Okay. So let's go on Menti and start the competition, which is the- the most fun. Okay. So let's get back to- to -to where we were. We have a chatbot that is able to answer, for sure, I just enrolled you. The way it does that is that, it receives the user utterance. I want to enroll in CS106A, Winter 2019 to learn coding. It identifies the intent of the user using a sequence classifier, same type of network as you've built for the Emojify assignment. And then it also runs another algorithm, which will fill in the slots and here we have all the slots needed. We have the code for the class, we have the quarter and we have the year, the student ID is implicitly given. So we're able to enroll- to enroll the students by calling access with all the slots. Done. Now let's make it a little more complicated. Let's say the students say, hi, I want to enroll in CS106A to- to learn coding. So the difference between this utterance and the previous one, example one, is that you don't have all the slots. You identify, uh, with your slot's tagger that CS106A is the code of a class, but you don't know the quarter, you don't know the year. So you probably want your chatbot to get back to the- to the student and say, for which quarter would you like to enroll, right? And the, the student would hopefully say, Winter 2019 or Winter, and then you have to ask for the year 2019. And finally you can say, "For sure, I just enrolled you." So we're not making any assumption here on natural language generation. You've worked on a Shakespeare assignment where you generate Shakespeare-like sentences. In fact, a good chatbot would have this feature of generating language. But for our purpose which you can just hard-code that when you're able to enroll the students, you just say, I just enrolled you. When you are able to retrieve information from the student, you would just write, here is some information and you would plug in whatever the ExploreCourses API sent back in a JSON, okay? So here the idea is, this student utterance cannot be understood without context. There is no way to understand Winter 2019 if you don't have a context management system. Does that make sense? So we want to build that context management system. And then the question is how to handle context. So there is a- there is many- there are many ways to do that, and people are still searching for the best ways. One way is to handle it with reinforcement learning, as we mentioned earlier. Another way which is quite intuitive and, and closer to what we've seen together in sequence model, uh, in the module, in the module five is, uh, this type of architecture, which is- which is taken from Chen et al., End-to-End Memory Networks with Knowledge Carryover for Multi-turn Spoken Language Understanding. So now you're able to understand what multi-turn means and end-to-end memory networks. So what happens here, just to describe it, is we will save all the history utterances. It means from the beginning of the conversation, we will record all the utterances and messages exchanged between the user and the, the assistant. We will keep it in a storage that we will call history utterances. C is the current utterance. So let's say the student says Winter 2019. This is the utterance of the, the student at this point. We will run this C, uh, and of course like it's, it's- this utterance will be run into an RNN and we will get back an encoding of the sentence. So there is all the, like, word embedding stuff that I don't describe but you guys are used to it. So we use word embeddings we run into, uh, we run it to an RNN and we get back the encoding of the user utterance and this encoding will then be compared to what we have in memory. So all the user utterances that we had in memory, are also going to be running in an RNN, that will encode their information in vectors. These vectors are going to be put in a memory representation and RU will be directly inner product, we, we will have an inner product from RU with all the memories and this pooled into a softmax will give us a vector of attention, that you guys should be used to now. A knowledge attention distribution, telling us what's the relation, where should we put our attention in the memory for this specific utterance. Does that make sense? So simple inner product softmax gives us a series of weights here, okay? Then we get a weighted sum of all these attention weights multiplied by the memory and it gives us a vector that encodes the relevance of the memory regarding our current utterance. This is then summed and run into a simple matrix multiplication to get an output vector which will be run in a slot stacking sequence and usually it's experimental but they pass also the current utterance to the RNN tagger, and the RNN tagger comes up with a slot tagging. So using that you can understand that Winter 2019 is actually the tagger for the slot's quarter and year, because you have this memory network. Does that makes sense? So this is another type of attention models you want to use and these memory networks can be used to manage some contexts for the slots tagger. Okay. So just to recap, we have our example, "Hi I want to enroll in a class," and we detect the intent, which is enroll. We also detect that there's some slots missing because we know, we know that the enroll function needs the quarter, the year and the class in order to be able to be called. So we have to ask for those. So we probably hard coded the fact that if you don't have the quarter, the year and the class, you probably want to first ask for the class, or the quarter, or the year. Then you can, you can get back to the person by asking, "Which class do you want to enroll in?" The person would get back to you, you will use your memory network to understand that CS230 is the slots for the enroll intent, you would fill it in. So now we have our intent with the class equals CS230, and we have our slots quarter and year which are to be filled, the chatbot gets back, "For which quarter?" and hopefully the student gives you the year at the same time and you can fill in the slots and then you're enrolled in CS230 for Winter 2019. Yeah. Should be Spring. Should be Spring, yeah [LAUGHTER]. This chatbot is not trained very well. [LAUGHTER] Okay. Any questions on that? So this is a very simple case of a, of a conversational assistant. Just to give you some ideas. There's some papers listed in the presentation that you can go to in order to get more advanced, uh, research insights. Uh, but the idea here is that we are limited to a specific intent, to two specific intents and a few slots. What do you think we would need if we didn't restrict ourselves to specific intents and slots? [NOISE] Very complicated, yeah. One industrial way to do it is to use a knowledge graph. What it means is let's say you're an e-commerce platform, you probably have, from your platform, a knowledge graph of, of all the items on the platform with connections among them, like let's say color of let's say you have a shoe, a shoe is a slot that might be the object for the intent I want to buy something, right? The shoe can have several attributes like color or size or men or women, like gender and all these are connected together in a ge- in a, in a, in a, in a gigantic knowledge graph. And you will follow the path of this knowledge graph following some probability- probabilities. So when we detect the intent of the user which is, buy something, we could identify the object, I wanna buy a shoe, and then based on our knowledge graph, it says that the next question that we should ask or the next slots that we need to fill is, uh, which brand do you want your shoe to be? And so the knowledge graph is going to tell you with 60 percent probability, go to brand and ask about the brand. Once you're there what other information you need in order to be able to retrieve five results for the user to review, and so on. So the knowledge graph is something industrial that can be used in order to have multiple intents, multiple slots for every intent. Okay. And at the end we can make an API call here. With CS230 quarter, Winter, 2019, quarter, Winter, year 2019, and the SUid. Okay. Another question I'll, I'll had for you I- I've had for you, I, I have for you is, uh, how to evaluate the performance of a chatbot. What do you think of that? So the- there are common ways to [NOISE] to, to evaluate several part of your pipeline like, how is your slot tagger doing? How is your intent classifier doing. You can use metrics such as precision recall, F1 score, which is a mix of both, uh, and report those in order to compare how this module is doing for the chatbot. But ultimately you want to understand how good is your chatbot overall. So some experiments are done and this is a paper of a deep reinforcement learning chatbot, built in 2017 by the MILA, uh, Serban et al. and what they did is that they used Mechanical Turk in order to evaluate their chatbot and also built a scoring system for their reinforcement learning chatbot. So I'm reading for you the instructions. Uh, you will be presented with a conversation between two speakers, Speaker A and B. You will also be presented with four potential responses from one of the speakers for this dialogue and the task is for you to rate each of, uh, the responses between one inappropriate, doesn't make sense, to five, uh, highly appropriate and interesting based on how appropriate the response is, to continue the conversation, three is neutral and, uh, if two responses are equally appropriate, you should give them the same score and if you see a response that is not in English, please give it one score. So here's what happens, uh, from a user perspective. You will have a conversation, you need to work on your English. Why do you say tha- that about me? Uh, well your English is very poor. So this is a conversation. And then the response 1 is, but English is my native language. Response 2 is, what are the reasons come to mind? Response 3 is, here is a funny fact. Go is the shortest complete sentence in the English language and then the fourth response is bye doggie [LAUGHTER]. So obviously you have to, you have to score, uh, you have to score these, uh, these responses according to what you think how relevant they are. And then, um, and then this course will be used either for the scoring system of the deep reinforcement learning chatbot or it can be used to evaluate how good is your chatbot compared to other chatbots. So maybe each of these responses come from a different model. Does that makes sense? So these are a few ways. Then another way, uh, which is asking for the opinion of the user on, uh, different, uh, responses. So let's say, you- you're a user and, um, you're- you are comparing two chatbots. You can give your opinion on which one you think is more natural. And you would ask a lot of users to do that to compare two or three chatbots together and also compare them to natural language from human. And then by doing a lot of, uh, Mean Opinion Score, uh, experiments, you can evaluate which chatbots are better than the others, just comparing them one to one. Okay. Now getting back to one thing that a student mentioned earlier is what if we want to have a vocal assistant? So right now our assistant is not vocal, it's just text. What other things do we need to build in order to make it a vocal assistant? We're not going to go into the details but roughly you would need a speech-to-text system, which will take the voice of a user, convert it into a text, and this as you've seen in the sequence model class, has different step in the pipeline uh, and the speech-to-text. So- and text-to-speech. That takes the text from the chatbot and converts it into a voice. So that's how you have like virtual assistants talking to us, it's because they have a text-to-speech system running. And these are three papers, the first one is Deep Speech 2 from Baidu's team, uh, which built an end-to-end speech recognition in English and Mandarin; and the two others are text-to-speech synthesis. So one came up in February 2018 which is the Tacotron 2 and the second one is WaveNet which is a very popular generative models, and these are- these are far beyond the scope of the class uh, but, uh you can study them in other classes at Stanford which are more specific to speech. Uh, okay, class project advice; so this Friday we are going to go over uh, again the rubrics of what we look at when we- we create projects and here is the list of things we would look at, uh. So make sure you have a very good problem description. When you read papers you see that there is a very good abstract. We expect you to give us a very good abstract so that when we read it, we get a good understanding of the paper. Hyperparameter tuning; always report what you do. You don't need to- to be very exhaustive but- but you can just tell us what hyperparameters you've been choosing and which ones you've been testing and why they didn't work. Um, the writing- we look for typos. This is common in- in the grading scheme. Typos, uh, uh, clear language. So review it, peer review your paper. Explanation of choice and decision; this is a very important part. We expect you to explain uh, the decisions you're making. So we don't want you to tell us, "I've taken- I've made that decision" just without explaining. But rather tell us, "there is this paper that mentioned that, this architecture worked well on that specific task. Uh I've tried three architectures here are my hyperparameters and results. That's why I'm going to- I'm going to dig more into that one" and so on. Data cleaning and preprocessing, if applicable to your pro- project, explain it, uh, how much code you wrote on your own; it's important to us, and please submit your GitHub privately to the TAs. When you submit your project, it's going to make it easier for us to review the code. Um, insights and discussions include; the next steps. What would you have done if you had more time? Uh, and also interpret your results. Don't just give results without explanation but rather try to extract information from these results, and it can also drive your next steps explanation. Uh, results are important but, if you don't have the results you expected, it's fine. We will look at how much work you've done and some tasks are very complicated. We don't expect you to beat state of the art on every single task. Some of you are going to beat state of the art hopefully, but those of you who didn't; still report all your results and explain why it didn't work. Uh, give references and also penalty for more than five pages. So if you're working on a- on a theoretical project you can add additional pages as appendix. You can also add appendix for your project. But the core has to be five pages. And for the final poster presentation which will happen not this Friday; next one, uh, we will ask you to pitch your project in three minutes. So not everyone in the group has to talk but at least one person has to talk, an- and we prefer if several of you talk in the project, but you have three minutes to pitch your projects. So prepare a pitch in advance and you will have two minutes of questions from the TA, which are also part of the grading. Okay? Finally, what's next after CS230? So there's a ton of classes at Stanford. We're in a good learning environment, which is- which is super. Uh, next steps can be in the university classes, you can take natural language processing and, uh, computer vision but also [NOISE] classes from different departments. Uh, deep generative models is a good way to learn about text-to-speech. For example or GANs. Probabilistic graphical models is also a very important class in the CS department. Of course if you haven't taken it yet CS 229; machine learning, or CS 229A; applied machine learning, are the go to- to learn machine learning. Reinforcement learning is a class where you can- you can delve more into Q-learning, policy gradients, and all these methods uh, that sometime use deep learning. So we're going to publish that list in case you want to check it. But these are examples of classes you can take and of course there are other classes that are not mentioned here, that might be relevant to pursue your learning in deep- deep learning and machine learning. Okay. That said, I'm going to- to give the microphone to Andrew. For closing remarks and- and yeah good luck on your projects. So we'll see you on Friday for the discussion sections and next week for the final project. Do you have a microphone? Thanks, Kian. So, all right. Here we are, at the end of this class. Nearly at the end this class. Um, you know, the NeurIPS conference is taking place right now. Formerly the NIPS Conference, but they renamed to NeurIPS, and I remember it was ten years ago that um- at that time a PhD student [inaudible] presented the paper; workshop paper at NIPS telling people, "hey, consider using GPUs, and CUDA there which is a new thing that NVidia had just published to train neural networks." We've done that work on a GPU server, that Ian Goodfellow the creator of GANs had built in his dorm room when he was an undergraduate at Stanford. So our first GPU server was built in a Stanford undergrad's dorm room. Um, and I remember sitting down with Jeff Hinton at the food court and saying, "hey check out this CUDA thing" and Jeff say, "oh, but GPU programming is really hard". But then- but then, but oh, maybe this CUDA thing looks promising. Um, and I tell this story because I want you to know as Stanford students that your work can matter, right? When Ian Goodfellow built that GPU server in his dorm room, I had no idea if he realized that a decade later you know someone would be winning several hundred hours of AWS credits to train bigger deep learning algorithms. But I think at Stanford- here at Stanford University, we're very much at the heart of the technology world. I think Silicon Valley is here to a large part because Stanford University is here, and um, we live in a world where with the superpowers that you now have, you have a lot of opportunities to do new and exciting work. Which may or may not seem like it will matter in the short run. Maybe even seem consequential in the short run. Because certainly you have a huge impact in the long run. Actually, a couple of weekends ago, so my wife- we roast coffee beans at home, right? My wife buys raw coffee beans and then we actually roast them, and Carol; my wife, tends to roast them in this really cheap popcorn popper that we have right now. So I don't have- I don't know how much coffee you guys drink, I drink a lot of coffee. And so you know- so Carol buys these green coffee beans, she puts them in this cheap popcorn popper; which is made for popping popcorn, not made for roasting coffee beans. This is one of the standard cheap ways to roast coffee beans and I love my wife. I drink the coffee she makes but sometimes she burns the coffee beans. So I- I found this article on the Internet from a former student that had written an article on how they use machine learning to roast- to - to optimize the roasting of coffee beans, um, and so I forwarded it to Carol, um. She wasn't very happy about that. [LAUGHTER], um. And I raised this as another [LAUGHTER] example of how, uh, uh, all of you, um- you know, I would never have thought of applying machine learning to roasting coffee beans. It's just- I mean, you know, I like my coffee but it had never occurred to me to do that. But someone taking a machine learning class, like you guys are, go ahead and come up with a better way of roasting coffee beans using learning algorithms. Um, and again, I think- I don't know if this particular person who wrote this blogpost was thinking of building a business out of it. I, I don't know. There might be a business there, there might not, or it might be just a fun personal hobby. I actually don't know. Um, but all of you with these skills have that opportunity. And then, um, again, earlier this week, uh, was it Monday night? Um, a group of us, uh, we, we were actually in the Gates Building, um, where a bunch of students actually from the AI for Healthcare Bootcamp that, that Kian alluded to, you know, we're going over some of the final projects for the, for the students in the AI for Healthcare Bootcamp, um, were, were working on. And I think, and I think I actually met several people including, Aarti right? When she first participated in a much earlier version of that AI for Heathcare Bootcamp. So you, you can ask Aarti about it if you're interested. But they're- um, one of the, um, master's students who was working with Ph.D student Pranav Rajpurkar that I think you guys met in this class. He was demoing an app where, um, you could pull up an X-ray film, uh, and take a picture with your cell phone, um, upload the picture to a website, um, and have a website read the X-ray and suggest a diagnosis for a, for a patient. Um, most of the planet today has insufficient access to radiology services. Uh, there are many countries where it costs you three months of salary, um, to go and get an X-ray taken, and then maybe try to find a radiologist to read it. But most of the planet, um- billions of people on this planet do not have sufficient services- radiology services. And, um, while the Stanford students in the AI for Healthcare Bootcamp is doing a research project- Actually, you were a co-author on the CheXNet paper, weren't you, Aarti? Yeah, right. Yes. Aarti was a shared co-author on, on, one of these papers. Um, it is again maybe work done here at Stanford that, you know, is taking the first steps toward maybe- if we can improve the deep learning algorithms, pass regulatory hurdles, you know, prove safety, maybe that type of work is happening here at Stanford. Doing AI for Healthcare, maybe that will have a transformative effect on how healthcare is run, um, all around the world. So, um, the skills that you guys now have, uh, uh, are a very unique set of skills. There are not that many people on the planet today that can apply learning algorithms and deep learning algorithms the way that you can. And you can tell a lot of the ideas you learn in this class were, you know, invented in the last year or two. So there's just not yet been time for these ideas to even become widespread. And if I look at a lot of the most pressing problems facing society, be it a lack of access to healthcare or, um- Scientists spend a lot of time thinking about climate change. Um, uh, and I think if you look at the, the, the- can we improve access to education. Can we just make whole society run more efficiently. Um, I think that all of you have the skills to do very unique projects. Um, and I hope that as you graduate from this class, I'm sure some of you will build great businesses, maybe make a lot of money. That's great. And, and I hope that all of you will also take the unique skills you have to work on projects that matter the most to other people, that, that help other people. Um, because if one of you does not take your skills to do something meaningful, then there's probably some very meaningful project that just no one is working on. Because I think the number of meaningful projects, um, I think actually greatly exceeds the number of people in the world today that are skilled at deep learning. Which is why all of you have a unique opportunity to take these algorithms that you now know about, to apply to anything from, I don't know, developing novel chatbots, to improving healthcare, to- I guess my team are learning AIs. Improving manufacturing, agriculture, also some healthcare, to maybe helping with climate change, to helping with global education, uh, and, and any other problems that, that really matter. So I hope, I hope maybe- uh, I hope that all of you go on, um, to, to do work that matters. Um, and then one last story, um, you know, a few- jus- just a few months ago now, I got to drive a tractor, right? It was very big, a little bit scary. It feels like a bigger machine than I should be qualified to drive. Um, it's, it's a huge tractor. And, and it turns out that when you drive a tractor- so it turns out when you drive a normal car, you know, it's really clear which way is up on your steering wheel, right? You point the steering wheel up and your, your car drives forward. Uh, for the tractor that I got to drive, this huge tractor, it turns out that, uh, this giant steering wheel and to drive straight, the giant steering wheel was just oriented at a weird angle. And to turn right, you turn it clockwise. To turn left, you turn it anticlockwise, and, and that was that, right? So it was a lot of fun. Um, and maybe in addition to, uh- and, and it was just fun, you know [LAUGHTER]. I drove a tractor, made a u-turn, drove back to where I started, did not hit anyone, you know, there was no accident. And then I climbed down off this giant tractor. Um, and maybe I tell that story because, uh, uh, I hope that even while you are doing this important, uh, uh, uh, maybe beneficial to other people, these types of work, I hope, I hope you also have fun. I, I think that I feel really privileged that as a Machine Learning Engineer, um, I some days, I get to go drive a tractor, right? And, and I hope that, um- and one of the most exciting things, um, you know, I, I, I feel like, um, uh, a lot of the best- a lot of biggest untapped opportunities for AI, lie outside the software industry. Um, I'm very proud of the work that helped to, you know, leading the Google Brain team, leading AI Baidu. And I think more people should do that type of work. Um, and I think that, um, here in Silicon Valley, many of you will get jobs in the tech sector, and that's great. We need more people to do that. And I also think that if you look at all of human activity, the majority of human activity is actually outside the software industry. The majority of global GDP growth, global GDP, uh, uh, is actually outside the software industry. And I would just urge you, as you're considering what is the most meaningful work, to consider software industry, but also look outside the software industry. Because I think really the biggest untapped opportunities for AI lie outside- I think lie outside the software industry. And, um, and we can't have everyone doing the same thing, right? That's actually not a healthy plan, and if everyone, you know, works on improve web search or improve- or, or, or even improve healthcare. I, I- and I think we need a world where all of you have these skills, share these skills, teach other people what you've learned, and go out to do this work that hopefully affects the software industry, affects other industries, affects for-profit, non-profit, affects government. Um, but use these AI capabilities to lift up the whole human race, right? Um, and then, finally, um, uh, the last thing I want to say on behalf of Kian and me, and the whole teaching team, is, um, I wanted to thank you for your hard work on this class. Uh, I know that, you know, watching the videos, uh, doing the homeworks on the website, uh, meeting with the TAs, uh, going to the section, uh, um, you know, that many of you have put a lot of work in this class. And it wasn't so long ago, I guess, when I was a student. Um, you know, staying at home, doing this homework or trying to derive that math thing. I, I also take some online classes myself, so it's actually not so long ago that, you know, I was sitting at a computer much like you trying to watch some Coursera videos, and then click on this, click on that, and answer things online. Uh, and, and, I, I, I appreciate. Uh Kian and I, and the whole teaching team appreciate all the hard work you've put into this. Um, and I hope also that you got a lot out of your hard work and that you will take these rare and unique skills you now have to go on. And, and when you graduate from Stanford- or for the- or for the home viewers, I guess, uh, for the- uh, for the home viewers, as well as the, uh, in-classroom viewers. That you take these rare skills that you now have and go on to do work that matters, and go on to do work that helps other people. So with that, um, I look forward to seeing, um, all of your projects at the poster session. Uh, uh, and I apologize in advance, we won't be able to really get a deep understanding in three minutes, but don't worry. We do read your project reports. Uh, but I look forward to seeing- uh, I hope you're looking forward also to seeing everyone else's work at the poster session. Uh, but with that, let me just say on behalf of the- Kian and me, and the whole teaching team, uh, thank you all very much. Thank you. [APPLAUSE] 

