Transcript,Keywords
"um hi everybody um welcome to stanford cs224 n also known as ling 284 natural language processing with deep learning i'm christopher manning and i'm the main instructor for this class so what we hope to do today is to dive right in so i'm going to spend about 10 minutes talking about the course and then we're going to get straight into content for reasons i'll explain in a minute in a minute so we'll talk about human language and word meaning i'll then introduce the ideas of the word to vec algorithm for learning word meaning and then going from there we'll kind of concretely work through how you can work out objective function gradients with respect to the word deveck algorithm and say a teeny bit about how optimization works and then right at the end of the class i then want to spend a little bit of time giving you a sense of how these word vectors work and what you can do with them so really the key learning for today is i want to give you a sense of how amazing deep learning word vectors are so we have this really surprising result that word meaning can be represented not perfectly but really rather well by a large vector of real numbers and you know that's sort of in a way a common place of the last decade of deep learning but it flies in the face of thousands of years of tradition and it's really rather an unexpected result to start focusing on okay so um quickly what do we hope to teach in this course so we've got three primary goals um the first is to teach you the foundations i a good deep understanding of the effect of modern methods for deep learning applied to nlp so we are going to start with and go through the basics and then go on to key methods that are used in nlp recurrent networks attention transformers and things like that we want to do something more than just that we'd also like to give you some sense of a big picture understanding of human languages and what are the reasons for why they're actually quite difficult to understand and produce even though humans seem to do it easily now obviously if you really want to learn a lot about this topic you should enroll in and go and start doing some classes in the linguistics department but nevertheless for a lot of you this is the only human language content you'll see during your master's degree or whatever and so we do hope to spend a bit of time on that starting today and then finally we want to give you an understanding of an ability to build systems in pi torch for some of the major problems in nlp so we'll look at learning word meanings dependency parsing machine translation question answering let's dive in to human language once upon a time i had a lot longer introduction that gave lots of examples about human how human languages can be misunderstood and complex i'll show a few of those um examples in later lectures but since right for today we're going to be focused on word meaning i thought i'd just give um one example which comes from a very nice xkcd cartoon and um that isn't sort of about some of the sort of syntactic ambiguities of sentences but instead it's really emphasizing the important point that language is a social system constructed and interpreted by people and that's part of how and it changes as people decide to adapt its construction and that's part of the reason why human languages are great as an adaptive system for human beings but difficult as a system or our computers to understand to this day so in this conversation um between the two women one says anyway i could care less and the other says i think you mean you couldn't care less saying you could care less implies you care at least some amount and the other one says i don't know where these unbelievably complicated brains drifting through a void trying in vain to connect with one another by blindly fleeing words out into the darkness every choice of phrasing spelling and tone and timing carries countless signals and contexts and subtexts and more and every listener interprets those signals in their own way language isn't a formal system language is glorious chaos you can never know for sure what any words will mean to anyone all you can do is try to get better at guessing how your words affect people so you can have a chance of finding the ones that will make them feel something like what you want them to feel everything else is pointless i assume you're giving me tips on how you interpret words because you want me to feel less alone if so then thank you that means a lot but if you're just running my sentences past some mental checklist so you can show off how well you know it then i could care less okay so that's ultimately um what our goal is is to how to do a better job at building um computational systems um that try to get better at guessing how their words will affect other people and what other people are meaning by the words that they choose to say so an interesting thing about human language is it is a system that was constructed by human beings um and it's a system that was constructed you know relatively recently in some sense so in discussions of artificial intelligence um a lot of the time um people focus a lot on human brains and the neurons buzzing by and this intelligence um that's meant to be inside people's heads but i just wanted to focus for a moment on the role of language there's actually you know this is kind of controversial but you know it's not necessarily the case that humans are much more intelligent than some of the higher apes like chimpanzees of bonobos right so chimpanzees and bonobos have been shown to be able to use pools to make plans and in fact chimps have much better short-term memory than human beings do so relative to that if you look through the history of life on earth human beings develop language really recently um how recently we kind of actually don't know because you know there's no fossils that say okay here's a language speaker um but you know most people estimate that language arose for human beings sort of you know somewhere in the range of a hundred thousand to a million years ago okay that's a while ago but compared to the process of evolution of life on earth that's kind of um blinking an eyelid um but that powerfulness communication between human beings quickly set off our ascendancy over other creatures um so it's kind of interesting that the ultimate power turned out not to be have been poisonous fangs or being super fast or super big but having the ability to communicate with other members of your tribe it was much more recently again that humans developed writing which allowed knowledge to be communicated across distances of time and space and so that's only about 5 000 years old the power of writing so in just a few thousand years the ability to preserve and share knowledge took us from the bronze age to the smartphones and tablets of today so a key question for artificial intelligence and human computer interaction is how to get computers to be able to understand the information conveyed in human languages simultaneously artificial intelligence requires computers with the knowledge of people fortunately now our ai systems might be able to benefit from a virtuous cycle we need knowledge to understand language and people well but it's also the case that a lot of that knowledge is contained in language spread out across the books and web pages of the world and that's one of the things we're going to look at in this course is how that we can sort of build on that virtuous cycle a lot of progress has already been made and i just want to very quickly um give a sense of that so in the last decade or so and especially in the last few years with newer methods of machine translation we're now in a space where machine translation really works moderately well so again from the history of the world this is just amazing right for thousands of years learning other people's languages was a human task which required a lot of effort and concentration but now we're in a world where you could just hop on your web browser and think oh i wonder what the news is in kenya today and you can head off over to a kenyan website and you can see something like this and you can go huh and you can then ask google um to translate it for you um from swahili and you know the translation isn't quite perfect but it's you know it's reasonably good so the newspaper tuko has been informed that local government minister lingsan and his transport counterparts city me died within two separate hours so you know within two separate hours is kind of awkward but essentially we're doing pretty well at getting the information out of this page and so um that's quite amazing um the single biggest development in nlp for the last year certainly in the popular media media was gpt um which was a huge new model that was released by open ai um what gpt 3 is about and why it's great is actually a bit subtle and so i can't really go through all the details of this here but it's exciting because it seems like it's the first step on the path to what we might call universal models where you can train up one extremely large model on something like that library picture i showed before and it just has knowledge of the world knowledge of human languages knowledge of how to do tasks and then you can apply it to do all sorts of things so no longer are we building a model to detect spam and then a model to detect pornography and then a model to detect um whatever foreign language content and just building all these separate supervised classifiers for every different task we've now just built up a model that understands so exactly what it does is it just predicts following words so on the left it's being told to write um about elon musk in the style of dr seuss and it started off with some text and then it's generating more text and the way it generates more text is literally by just predicting one word at a time following words come to complete its text but this has a very powerful facility because what you can do with 3 is you can give it a couple of examples of what you'd like it to do so i can give it some text and say i broke the window change it into a question what did i break i gracefully saved the day i changed it into a question what did i gracefully save so this prompt um tells gpt 3 what i'm wanting it to do and so then if i give it another statement like i gave john flowers i can then say gpt-3 predict what words come next and it'll follow my prompt and produce who did i give flowers to or i can say i gave her a rose and a guitar and it will follow the idea of the pattern and do who did i give a rose and a guitar to and actually this one model can then do an amazing range of things including many there's quite surprising to do at all to give just one example of that another thing that you can do is get it to translate human language sentences into sql so this can make it much easier to do cs145 so having given it a couple of examples of sql translation of human language text which i'm this time not showing because it won't fit on my slide i can then give it a sentence like how many users have signed up since the start of 2020 and it turns it into sql or i can give it another query what is the average number of influences each user subscribe to and again it then converts that into sql so gpt gpt-3 knows a lot about the meaning of language and the meaning of other things like sql and can fluently manipulate it okay so that leads us straight into this top meaning and how do we represent the meaning of a word well what is meaning well we could look up something like the webster dictionary and say okay the idea is represented by a word the idea that a person wants to express by using words signs etc um those webster's dictionary definitions really focused on the word idea somehow but this is pretty close to the commonest way that linguists think about meaning so that they think of word meaning as being appearing between a a word which is a signifier or symbol and the thing that it signifies the signified thing which is an idea or thing so that the meaning of the word chair is the set of things that are chairs and that's referred to as denotational semantics a term that's also used and similarly applied for the semantics of programming languages this model isn't very deeply implementable like how do i go from the idea that okay chair means the set of chairs in the world just something i can manipulate meaning within my computers so traditionally the way that meaning has normally been handled in natural language processing systems is to make use of resources like dictionaries and thesauri in particular a popular one is wordnet which organized words and terms into both synonym sets words that can mean the same thing and hypernyms which correspond to is a relationships um and so for the is a relationships you know we can kind of look at the hyponyms of panda and panda is a kind of proceed whatever those are like i guess that's probably with red pandas um which is a kind of carnivore which is a kind of placenta which is kind of mammal and you sort of head up this um hyponym um hierarchy so wordnet has been a greater resource for nlp but it's also been highly deficient so it lacks a lot of nuance so for example in word net proficient is listed as a synonym for good but you know maybe that's sometimes true but it seems like in a lot of context it's not true and you mean something rather different when you say proficient versus good um it's limited as a human constructed um thesaurus so in particular there's lots of words and lots of uses of words that just aren't there including you know anything um that is you know sort of more current terminology like um wicked is there for the wicked witch but not for more modern colloquial uses um ninja certainly isn't there for the kind of description some people make of programmers and it's kind of impossible to keep up to date um so it requires a lot of human labor but even when you have that you know it has a sets of synonyms but doesn't really have a good sense of words that means something similar so um fantastic and great means something similar without really being synonyms and so this idea of meaning similarity is something that would be really useful to make progress on and where deep learning models excel okay so what's the problem with a lot of traditional nlp well the problem with a lot of traditional nlp is that words are regarded as discrete symbols so we have symbols like hotel conference motel our words which in deep learning speak we refer to as a localist representation and that's because if you in statistical or machine learning systems want to represent these symbols that each of them is a separate thing so the standard way of representing them and this is what you do in something like a statistical model if you're building a logistic regression model with words as features is that you represent them as one hot vectors so you have a dimension for each different word so maybe like in my example here are my representations as vectors for motel and hotel um and so that means that we have to have huge vectors corresponding to the number of words in our vocabulary so the kind of if you had a high school english dictionary it probably had about 250 000 words in it um but there are many many more words in the language really so maybe we at least want to have a 500 000 um dimensional vector to be able to cope with that okay um but the bigger even bigger problem with the discrete symbols is that we don't have this notion of word relationships and similarity so for example in web search if a user searches for seattle motel we'd also like to match on documents containing seattle hotel but our problem is we've got these one-hot vectors for the different words and so in a formal mathematical sense these two vectors are orthogonal that there's no natural notion of similarity between them whatsoever well there are some things that we could do but try and do about that and people did do about that um in you know before 2010 we could say hey we could use word net synonyms and we count things that list the synonyms is similar anyway or hey maybe we could somehow build up representations of words that have meaning overlap and people did all of those things but they tended to fail badly from incompleteness so instead what i want to introduce today is the modern deep learning method of doing that where we encode similarity in a real value vector themselves so how do we go about doing that okay and the way we do that is by exploiting this idea called distributional semantics so the idea of distributional semantics is again something that when you first see it maybe feels a little bit crazy because rather than having something like denotational semantics what we're now going to do is say that a word's meaning is going to be given by the words that frequently appear close to it jr firth was a british linguist from the middle of last century and one of his pithy slogans that everyone quotes at this moment is you shall know a word by the company it keeps and so this idea that you can represent a sense for words meaning as a notion of what context it appears in has been a very successful idea one of the most successful ideas that's used throughout statistical and deep learning nlp it's actually an interesting idea um more philosophically so that there are kind of interesting connections for example in wittgenstein's later writings he became enamored of a use theory of meaning and this is a sin in some sense a use theory of meaning but whether you know it's the ultimate theory of semantics it's actually still pretty controversial but it proves to be an extremely computational sense of semantics which has just led to it being used everywhere very successfully um in deep learning systems so when a word appears in a text it has a context which are the set of words that appear and so for a particular word my example here is banking we'll find a bunch of places where banking occurs in texts and we'll collect the sort of nearby words as context words and we'll see say that those words that are appearing that kind of muddy brown color around banking that those context words will in some sense represent the meaning of the word banking um while i'm here let me just mention one distinction that will come up regularly when we're talking about a word um in our natural language processing class we sort of have two senses of word which are referred to as types and tokens so there's a particular instance for word so there's in the first example government debt problems turning turning into banking crises there's banking there and that's a token of the word banking but then i've collected a bunch of instances of quote unquote the word banking and when i say the word banking and a bunch of examples of it i'm then treating banking as a type which refers to you know the uses and meaning the word banking has across instances okay so um what are we going to do with these distributional models of language well what we want to do is we're going based on looking at the words that occur in context as vectors that we want to build up a dense real valued vector for each word that in some sense represents the meaning of that word and the way it all represent the meaning of that word is that this vector will be useful for predicting other words that occur in the context of this um so in this example to keep it manageable on the slide vectors are only eight dimensional um but in reality we use considerably bigger vectors so a very common size is actually 300 dimensional vectors okay so for each word that's a word type we're going to have a word vector these are also used with other names they're referred to as newer word representations or for a reason they'll become clearer on the next slide they're referred to as word embeddings so these are now a distributed representation not a localist representation because the meaning of the word banking is spread over all 300 dimensions of the vector okay these are called word embeddings because effectively when we have a whole bunch of words these representations place them all in a high dimensional vector space and so they're embedded into that space now unfortunately human beings are very bad at looking at 300 dimensional vector spaces or even eight dimensional vector spaces so the only thing that i can really display to you here is a two-dimensional projection of that space now even that's useful um but it's also important to realize that when you're making a two-dimensional projection of a 300 dimensional space you're losing almost in all the information in that space and a lot of things will be crushed together that don't actually deserve to be better um so here's um my word embeddings of course you can't see any of those at all but if i zoom in and then i zoom in further what you'll already see is that the representations we've learned distributionally do a just a good job at grouping together similar um words so in this sort of overall picture i can zoom into one part of the space is actually the part that's up here in this view of it um and it's got words for countries so not only are countries generally grouped together um even the sort of particular subgroupings of countries make a certain amount of sense and down here we then have nationality words if we go to another part of the space we can see different kind of words so here are verbs and we have ones like come and go are very similar um saying and thinking words say think expect a kind of similar and by nearby over in the bottom right we have sort of verbal auxiliaries and copulas so have had hairs um forms of the verb to be and certain contentful verbs are similar to copula verbs because they describe states you know he remained angry he became angry and so they're actually then grouped close together to the word the verb to be so there's a lot of interesting structure um in this space um that then represents the meaning of words so the algorithm i'm going to introduce now is one that's called word to vec which was um introduced by tamash mikulov and colleagues in 2013 as a framework for learning word vectors and it's sort of a simple and easy to understand place to start so the idea is we have a a lot of text from somewhere which we commonly refer to as a corpus of text corpus is just the latin word for body so it's a body of text and so then we choose a fixed vocabulary which will typically be large but nevertheless truncated so we get rid of some of the really rare words so we might say vocabulary size of four hundred thousand and we then create for ourselves a vector for each word okay so then what we do is we want to work out what's a good vector um to for each word and the really interesting thing is that we can learn these word vectors from just a big pile of text by doing this distributional similarity task of being able to predict well what words occur in the context of other words so in particular we're going to iterate through words in the text and so at any moment we have a center word um c and context words outside of it which we'll call o and then based on the current word vectors we're going to calculate the probability of a context word occurring given the center word according to our current model but then we know that certain words did actually occur in the context of that center word and so what we want to do is then keep adjusting the word vectors to maximize the probability that's assigned to words that actually occur in the context of the center word as we proceed through these texts so to start to make that a bit more concrete this is what we're doing um so we have a piece of text we choose our center word which is here into and then we say well if a model of predicting the probability of context words given the center word and this model will come to in a minute but it's defined in terms of our word vectors so let's see what probability it gives to the words that actually occurred in the to the context of this word it gives them some probability but maybe be nice if the probability of the sign was higher so then how can we change our word vectors to raise those probabilities and so we'll do some calculations with into being the center word and then we'll just go on to the next word and then we'll do the same kind of calculations and keep on chunking so the big question then is well what are we doing for working out the probability of a word occurring in the context of the center word and so that's the central part of what we develop as the word object so this is the overall model that we want to use so for each position in our corpus our body of text we want to predict context words within a window of fixed size m given the center word wj and we want to become good at doing that so we want to give high probability to words that occur in the context and so what we're going to do is we're going to work out what's formally the data likelihood as to how good a job we do at predicting words in the context of other words and so formally that likelihood is going to be defined in terms of our word vectors so they're the parameters of our model and it's going to be calculated as taking the product of using each word as the center word and then the product of each word and a window around that of the probability of predicting that context word in the center word and so to learn this model we're going to have an objective function sometimes also called a cost or a loss that we want to optimize and essentially what we want to do is we want to maximize the likelihood of the context we see around center words but following standard practice we slightly fiddle that because rather than dealing with products it's easier to deal with sums and so we work with log likelihood and once we take log likelihood all of our products turn into sums we also work with the average log likelihood so we've got a one on t term here for the number of words in the corpus and finally for no particular reason we like to minimize our objective function rather than maximizing it so we stick a minus sign in there and so then by minimizing this objective function j of theta that comes maximizing our predictive accuracy okay so that's the setup but we still haven't made any progress in how do we calculate the probability of a word occurring in the context given the center word and so the way we're actually going to do that is we have vector representations for each word and we're going to work out the probability simply in terms of the word vectors now at this point there's a little technical point we're actually going to give to each word two word vectors one word vector for when it's used as the center word and a different word vector when it's used as a context word um this is done because it just simplifies the math and the optimization so it seems a little bit ugly but actually makes building word vectors a lot easier and really we can come back to that and discuss it um later but that's what it is and so then once we have these word vectors um the equation that we're going to use for giving the probability of a context word appearing given the center word is that we're going to calculate it using the expression in the middle bottom of my slide so let's sort of pull that apart just a little bit more um so what we have here with this expression is so for a particular center word and a particular context word o we're going to look up the vector representation of each word so they're u of o and v of c and so then we're simply going to take the dot product of those two vectors so dot product is a natural measure for similarity between words because in any particular dimension uh positive you'll get some a component that adds to the dot products um if both are negative it'll add a lot to the dot product some if one's positive and one's negative um it'll subtract from this similarity measure um if both of them are zero it won't change the similarity so it sort of seems a sort of plausible idea to just take a dot product and thinking well if two words have a larger dot product that means they're more similar and so then after that we're sort of really doing nothing more than okay we want to use dot products to represent word similarity and now let's do the dumbest thing that we know how to turn this into a probability distribution well what do we do well firstly well taking a dot product of two vectors that might come out as positive or negative but well if we want to have probabilities we can't have negative probabilities so a simple way to avoid negative probabilities is to exponentiate them because then we know everything is positive and so then we're always getting a positive number in the numerator but for probabilities we also want to have the numbers add up to one so we have a probability distribution so we're just normalizing in the obvious way where we divide through by the sum of the numerator quantity for each different word and the vocabulary and so then necessarily that gives us a probability distribution so all the rest of that that i was just talking through what we're using there is what's called the softmax function so the softmax function will take any rn vector and turn it into things between um zero to one um and so we can take numbers and put them through this soft max and turn them into probability distribution right so the name comes from the fact that it's sort of like a max um so because of the fact that we exponentiate that really emphasizes the big contents in the different dimensions of calculating similarities so most of the probability goes to the most similar things um and it's called soft because well it doesn't do that absolutely it'll still give some probability to everything that's in the slightest bit similar i mean on the other hand it's a slightly weird name because you know max normally takes a set of things and just returns one the biggest of them whereas the softmax is taking a set of numbers and is scaling them but is returning the whole probability distribution okay so now we have all the pieces of our model and so how do we make our word vectors well the idea of what we want to do is we want to fiddle our word vectors in such a way that we minimize our loss i that we maximize the probability of the words that we actually saw in the context of the center word and so theta the theta represents all of our model parameters in one very long vector so for our model here the only parameters are our word vectors so we have for each word two vectors it's context vector and its center vector and each of those is a d dimensional vector where d might be 300 and we have v many words so we end up with this big huge vector which is 2 v long which if you have a 500 000 vocab times the 300 dimensional the time um small method i can do in my head but it's got millions and millions of parameters so i've got millions and millions of parameters and we somehow want to fiddle them all to maximize the prediction of context words and so the way we're going to do that then is we use calculus so what we want to do is take that math that we've seen previously and say huh well with this objective function um we can work out derivatives and so we can work out where the gradient is so how we can walk downhill to minimize loss so we're at some point and we can figure out what what is downhill and we can then progressively walk downhill and improve our model and so what our job is going to be is to compute all of those vector gradients okay um so at this point i then want to kind of um show a little bit more as to how we can actually um do that and a couple more slides here but maybe i'll just try and  things again and move to my interactive whiteboard what we wanted to do right so we had our overall we had our overall j theta that we were wanting to minimize our average neglog likelihood so that was the minus one on t of the sum of t equals one to big t which was our text length and then we were going through the words in each context so we're doing j between m words on each side um except itself um and then what we wanted to do was in the side there we were then we were working out the log probability of the context word at that position um given the word that's in the center position t and so then we converted that into um our word vectors by saying that the probability of o given c is going to be expressed as the um this soft max of the dot product okay and so now what we want to do is work out the gradient the direction of downhill for this last gen and so the way we're doing that is we're working out the partial derivative of this expression with respect to every parameter in the model and all the parameters in the model are the components the dimensions of the word vectors of every word and so we have um the center word vectors and the outside word vectors so here um i'm just going to do the center word vectors but on homework on a future homework assignment 2 the outside word vectors will show up and they're kind of similar so what we're doing is we're working out the partial derivative with respect to our center word vector which is you know maybe a 300 dimensional word vector of this probability of o give c um and since we're using log probabilities of the log of this probability of o given c of this x of u of o t v c over my writing will get worse and worse sorry um i've already made a mistake haven't i the sum um the sum of w equals one to the vocabulary of the expert uwt vc okay um well at this point things start off pretty easy so what we have here is something that's log of a over b so that's easy we can turn this into log a minus log b but before i go further i'll just make a comment at this point um you know so at this point um my audience divides on into right there are some people in the audience um for which maybe a lot of people [Music] ah this is um really elementary math i've seen this a million times before and he isn't even explaining it very well um and if you're in that group well feel free to look at your email or on the newspaper or whatever else is best suited to you but i think there are also um other people in the class who oh the last time i saw calculus was when i was in high school for which that's not the case and so i wanted to spend a few minutes um going through this a bit concretely so that to try and get over the idea that you know even though most of deep learning and even word vector learning seems like magic that it's not really magic um it's really just doing math and one of the things we hope is that you do actually understand this math that's being done so i'll keep along and do a bit more of it okay so then what we have is sort of use this way of writing the log and so then we can say that that expression above equals the partial derivatives with of vc of the log of the numerator log x u o t v c minus um the partial derivative of the log of of the denominator so that's then the sum of w equals 1 to v of the x of u w t v c okay so at that point i have um my numerator here and my former denominator there um so at that point there are starts the first part is the numerator part so the numerator part is really really easy so um we have here that log and x but just inverses of each other so they just go away so that becomes the derivative with respect to vc of just what's left behind which is you u0 dot product and with vc okay um and so the thing to be aware of is you know we're still doing this multivariate calculus so what we have here is calculus with respect to a vector like hopefully you saw some of in math 51 or some other place not um high school um single variable calculus on the other hand um you know to the extent you and half remember some of this stuff most of the time you can just do fi perfectly well by thinking about what happens um with one dimension at a time and it generalizes the multivariable calculus so if about um all that you remember of calculus is that d dx of a x equals a really it's the same thing that we're going to be using here that here we have the the outside word dot producted with the vc well at the end of the day that's going to have terms of sort of u0 component 1 times the center word component 1 plus u um zero component two plus um this were component two and so we're sort of using this bit over here and so what we're going to be getting out is the u0 and u01 and the u0 2 so this will be all that is left with respect to vc1 when we take its derivative with respect to vc1 and this term will be the only thing left when we take the derivative with respect to the variable um vc2 so the end result of taking the vector derivative of u0 dot product and with vc is simply going to be u0 okay great so that's progress so then at that point we go on and we say oh damn we still have the the denominator and that um slightly more complex but not so bad so then we want to take the partial derivatives with respect to vc of the log of the denominator okay and so then at this point um the one tool that we need to know and remember is how to use the chain rule so the chain rule is when you're wanting to work out um of having derivatives of compositions of functions so we have f of g of whatever x but here it's going to be vc and so we want to say okay what we have here is we're working out a composition of functions so here's our f um and here is our x which is g of v c actually maybe i shouldn't call it x um oops maybe i was it's probably better to call it z or something um okay so when we then want to work out um the chain rule well what do we do we take the derivative of f at the point z and so at that point we have to actually remember something we have to remember that the derivative of log is the one on x function so this is going to be equal to the 1 on x for z so that's then going to be 1 over the sum of w equals 1 to v of x of u t v c multiplied by the derivative of the inner function so so the derivative of um the part that is remaining i hope i'm getting this right the sum of oh and there's one trick here at this point we do want to have a change of index so we want to say the sum of x equals 1 to v of x of u of x v c since we can get into trouble if we don't change that variable um to be using a different one okay so at that point we're making some progress but we still want to work out the derivative of this and so what we want to do is apply the chain rule once more so now here's our f and in here is our new z equals g of vc and so we then sort of repeat over so we can move the um derivative inside uh some always so we're then taking the derivative of this and so then the derivative of x is itself okay so we're going to just have x of u x t v c times um there's the sum of x equals 1 to v times the derivative um of u x t v c okay and so then this is what we've worked out before we can just rewrite as ux okay so we're now making progress um so if we start putting all of that together what we have is um the derivative or the partial derivatives with vc of this log probability right we have the numerator which was just u0 um minus um we then had the sum of the numerator sum over x equals 1 to v of x u x t dc times u of x then that was multiplied by our first term that came from the one on x which gives you the sum of w equals one to v of the x of u w t v c and this is the fact that we change the variables um became important and so by just sort of rewriting that a little um we can get that that equals u0 minus um the sum of v equals oh sorry x all right x equals 1 to v of this x view of x t v c over the sum of w equals 1 to v of x u w t v c times u of x and so at that point this sort of interesting thing has happened that um we've ended up getting straight back exactly the soft max formula probability that we saw when we started um we can just rewrite that more conveniently as saying this equals u0 minus the sum over x equals 1 to v of the probability of x given c times ux and so what we have at that moment is this thing here is an expectation and so this is an an average over all the context vectors weighted by their probability according to the model and so it's always the case with these softmax style models that what you get out for the derivatives is you get obs the observed um minus the expected so our model is good if our model on average predicts exactly um the word vector that we actually see and so we're going to try and adjust the parameters of our model so it does that much of all um now i mean we try and make it do it as much as possible i mean of course as you'll find you can never get close right you know if i just say to you okay the word is croissant which words are going to um occur in the context of croissant i mean you can't answer that there are all sorts of sentences that you could say that involve the word croissant so actually our particular probability estimates are going to be kind of small but nevertheless we want to sort of fiddle our word vectors to try and make those estimates as high as we possibly can so i've gone on about this stuff um a bit but haven't actually sort of shown you any of what actually happened sorry i just want to quickly um show you a bit of that as to what actually happens with word vectors um so here's a simple little ipython notebook which is also what you'll be using for assignment one only um so in the first cell i import a bunch of stuff um so we've got numpy for our vectors matplotlib plotting off it learns kind of your machine learning um swiss army knife gensim is a package that you may well not have seen before it's a package that's often used for word vectors it's not really used for deep learning so this is the only time you'll see it in the class but if you just want a good package for working with word vectors and some other application it's a good one to know about okay so then in my second cell here i'm loading a particular set of word vectors so these are our glove word vectors that we made at stanford in 2014 and i'm loading a hundred dimensional word vectors um so that things are a little bit quicker for me um while i'm doing things here sort of do this model of bread and croissant um well what i've just got here is um word vectors so i just wanted to sort of um show you that there are um word vectors hmm well maybe i should have loaded those word vectors in advance hmm let's see oh okay well i'm in business um okay so right so here are my word vectors for um bread and croissant and while and seeing that maybe these two words are a bit similar so both of them are negative in the first dimension positive and the second negative in the third positive and the fourth negative and the fifth so it sort of looks like they might have a fair bit of dot product which is kind of what we want because bread and croissant are kind of similar um but what we can do is actually ask the model and these are gen sim functions now you know what are the most similar words so i can ask for croissant um what are the most similar words to that and it will tell me it's things like brioche baguette focaccia so that's pretty good pudding is perhaps a little bit more questionable we can say most similar to the usa and it says canada america usa with periods united states that's pretty good most similar to banana um i get out coconuts mangoes bananas sort of fairly tropical very great um now before finishing though i want to show you something slightly more than just similarity which is one of the amazing things that people observed with these word vectors and that was to say you can actually sort of do arithmetic in this vector space that makes sense and so in particular people suggested this analogy task and so the idea of the analogy task is you should be able to start with a word like king and you should be able to subtract out a male component from it add back in a woman component and then you should be able to ask well what word is over here and what you'd like is that the word over there is queen um and so um this sort of little bit of so we're going to do that um with this sort of same most similar function which is actually more so as well as having positive words you can ask for most similar negative words and you might wonder what's most negatively similar to a banana and you might be thinking oh it's um i don't know um some kind of meat or something actually that by itself isn't very useful because when you could just ask for most negatively similar things you tend to get crazy strings that were found in the data set that you don't know what they mean if anything but if we put the two together we can use the most similar function with positives and negatives to do analogies so we're going to say we want a positive king we want to subtract out negatively man we want to then add in positively woman and find out what's most similar to this point in the space so my analogy function does that precisely that by taking a couple of most similar ones and then subtracting out um the negative one and so we can try out this analogy function so i can do the analogy i show in the picture with man as to king as woman is fight sorry i'm not saying this right um yeah man is the king as woman is too oh sorry i haven't done my cells um okay man is the king as a woman as the queen so um that's great and that's um works well i mean and you can do it the sort of other way around king is to man as queen as to woman um if this only worked for that one freakish example um you maybe um wouldn't be very impressed but you know it actually turns out like it's not perfect but you can do all sorts of fun analogies with this and they actually work so you know i could ask for something like an analogy um oh here's a good one australia is to be uh as france is to what and you can think about what you think the answer that one should be and it comes out as champagne which is pretty good or i could ask for something like analogy pencil is to sketching as camera is to what um and it says photographing um you can also do the analogies with people um at this point i have to point out that this data was um and the model was built in 2014 so you can't ask anything about um donald trump in it well you can trump is in there but not as president but i could ask something like analogy obama is to clinton as reagan is to what and you can think of what you think is the right analogy there the analogy it returns is nixon so i guess that depends on what you think of bill clinton as to whether you think that was a good analogy or not you can also do sort of linguistic analogies with it so you can do something like analogy tall is to tallest as long is to what and it does longest so it really just sort of knows a lot about the meaning behavior of words and you know i think when these um methods were first developed and hopefully still for you that you know people were just gobsmacked about how well this actually worked at capturing of words and so these word vectors then went everywhere as a new representation that was so powerful for working out word meaning and so that's our starting point for this class and we'll say a bit more about them next time and they're also the basis of what you're looking at for the first assignment can i ask a quick question about the distinction between the two vectors per word yes um my understanding is that there can be several context words per uh word in the vocabulary like word in the vocabulary um but then if there's only two vectors i kind of i thought the distinction between the two is that one it's like the actual word and one's like the context word but the multiple context words like how do you how do you pick just two then well so we're doing every one of them right so like um maybe i won't turn back on the screen share but you know we were doing in the objective function there was a sum over you so you've got you know this big corpus of text right so you're taking a sum over every word which is it appearing as the center word and then inside that there's a second sum which is for each word in the context so you are going to count each word as a context word and so then for one particular term of that objective function you've got a particular context word and a particular um center word but you're then sort of summing over different context words for each center of the word and then you're summing over all of the the decisions of different center words and and to say um a little just a sentence more about having two vectors i mean you know in some sense it's an ugly detail but it's was done to make things sort of simple and fast so you know if you look at the math carefully if you sort of treated this two vectors as the same so if you use the same vectors for center and context and you say okay let's work out the derivatives things get uglier and the reason that they get uglier is it's okay when i'm iterating over all the choices of um context word oh my god sometimes the context word is going to be the same as the center word and so that messes with working out um my derivatives whereas by taking them as separate vectors that never happens so it's easy um but the kind of interesting thing is you know saying that you have these two different representations sort of just ends up really sort of doing no harm and my wave my hands argument for that is you know since we're kind of moving through each position the corpus one by one you know something a word that is the center word at one moment is going to be a context word at the next moment and the word that was the context word is going to have become the center word so you're sort of doing the um the computation both ways in each case and so you should be able to convince yourself that the two representations for the word end up being very similar and they do not not identical for technical reasons at the ends of documents and things like that but very very similar and so effectively you tend to get two very similar representations for each word and we just average them and call that the word vector and so when we use word vectors we just have one vector for each word that makes sense thank you i have a question purely of curiosity so we started when we projected the vectors the word vectors onto the 2d surface we saw like little clusters of words that are similar to each other and then later on we saw that um with the analogies thing we kind of see that there's these directional vectors that sort of indicate like the ruler of or the ceo of something like that and so i'm wondering is there are there relationships between those relational vectors themselves such as like is the um the ruler of vector sort of similar to the ceo of vector which is very different from like is makes a good sandwich with vector is there any research on that that's a good question um how will you stump me already in the first lecture ah i mean that yeah i can't actually think of a piece of research and so i'm not sure i have a confident and i'm not sure i have a confident answer i mean it seems like that's a really easy thing to check um with how much you have one of these sets of um word vectors that it seems um like and for any relationship that is represented well enough by word you should be able to see if it comes out kind of similar um huh i mean i'm not sure we can we can look and see yeah that's totally okay just just curious i'm sorry i missed the last little bit to your answer to first question so when you wanted to collapse two vectors for the same word did you say you usually take the average um different people have done different things but the most common practice is after you've uh you know there's still a bit more i have to cover about running word divec that we didn't really get through today so i still got a bit more work to do on thursday but you know once you've run your word to vec algorithm and you you sort of your output is two vectors for each word and kind of like when it's center and when it's context and so typically people just average those two vectors and say okay that's the representation of the word croissant and that's what appears in the sort of word vectors file like the one i loaded that makes sense thank you oh thanks so my question is if a word have two different meanings or multiple different meanings can we still represent it as a same single vector yes that's a very good question um and actually there is some content on that in thursday's lecture so i can say more about that um but yeah the first reaction is you kind of should be scared because um something i've said nothing about at all is you know most words especially short common words have lots of meaning so if you have a word like star that can be astronomical object or it can be you know a film star a hollywood star or it can be something like the gold stars that you've got in elementary school and we're just taking all those uses of the word star and collapsing them together into one word vector um and you might think that's really crazy and bad um but actually turns out to work rather well um maybe i won't go through all of that um right now because there is actually stuff on that on thursday's lecture oh i see i think you can put ahead of the slides for next time oh wait i know this let's see [Music] is do we look at how to implement or do we look at like the stack of like something like alexa or something provide speech to uh context actions in this course was it just primarily uh understanding so this is an unusual con an unusual quarter but for this quarter there's a very clear answer which is um this quarter um there's also a speech class being taught which is cs 224 s um a speech class being taught by andrew mars and you know this is a class that's been more regularly offered sometimes it's only been offered every third year but it's being offered right now so if what you want to do is learn about speech recognition and learn about sort of methods for building dialogue systems you should do cs224 yes so you know for this class in general um the vast bulk of this class is working with text and doing various kinds of text analysis and understanding so we do tasks like some of the ones i mentioned we do machine translation um we do question answering um we look at how to pass this structure of sentences and things like that you know in other years i sometimes say a little bit about speech um but since this quarter there's a whole different class that's focused on speech that seem a little bit silly i guess you have the the part of partnering with your audience [Music] more on speech i'm now getting a bad echo i'm not sure if that's my fault or your fault but anyway um anyway answer yeah so the speech class does a mix of stuff so i mean the sort of pure speech problems classically have been um doing speech recognition so going from a speech signal to text and doing text-to-speech going from text to us a speech signal and both of those are problems which are now normally done including by the cell phone that sits in your pocket using neural networks and so it covers both of those but then between that the class covers quite a bit and in particular it starts off with looking at building dialogue system so this is sort of something like alexa google assistant siri as to well assuming you have a speech recognition a text-to-speech system um then you do have text in and text out what are the kind of ways that people go about building um um dialogue systems like the ones that i just mentioned um i actually had a question so i think there was some people in the chat noticing that the uh like opposites were really near to each other which was kind of odd but i was also wondering um what about like positive and negative uh valence or like affect um is that captured well in this type of model or is it like not captured well like well like with the opposites how those weren't really yeah so the short answer is for both of those and so this is a good question a good observation and the short answer is no both of those are captured really really badly i mean there's there's a definition um oh you know when i say really really badly i mean what i mean is if that's what you want to focus on um you've got problems i mean it's not that the algorithm doesn't work so precisely what you find is that you know antonyms generally occur in very similar topics because you know whether it's um saying you know john is really tall or john is really short or that movie was fantastic or that movie was terrible right you get antonyms occurring in the same context so because of that their vectors are very similar and similarly for sort of affect and sentiment based words well like um great and terrible example their contexts are similar um they're for um that if you're just learning this kind of predict words and context models um that no that's not captured now that's not the end of the story i mean you know absolutely people wanted to use neural networks for sentiment and other kinds of sort of connotation effect and there are very good ways of doing that but somehow you have to do something more than simply predicting words in context because that's not sufficient to um capture that dimension um more on that later adjectives too like very basic adjectives like so and like not because those would like appear in like similar context right what was your first example before not uh like so this is so cool so that that's actually a good question as well so yeah so there are these very common words that are commonly referred to as function words by linguists which you know includes ones like um so and not but other ones like and and prepositions like you know two and on um you sort of might suspect that the word vectors for those don't work out very well because they occur in all kinds of different contexts and they're not very distinct from each other in many cases and to a first approximation i think that's true and part of why i didn't use those as examples in my slides yeah but you know at the end of the day we do build up vector representations of those words too and you'll see in a few lectures time when we start building what we call language models that actually they do do a great job in those words as well i mean to explain what i'm meaning there i mean you know another feature of the word to vect model is that actually ignore the position of words right so it said i'm going to predict every word around the center word but you know i'm predicting it in the same way i'm not predicting differently the word before me or versus the word after me or the word two away in either direction right they're all just predicted the same by that one um probability function and so if that's all you've got that sort of destroys your ability to do a good job at um capturing these sort of common more grammatical words like so not an and but we build slightly different models that are more sensitive to the structure of sentences and then we start doing a good job on those too okay thank you i had a question about the characterization of word to fact um because i i which was slightly different from how it was presented in the microwave so are these like two complementary reasons yeah so i i've still got more to say so i'm stay tuned thursday um for more stuff on word vectors um you know so word to back is kind of a framework for building word vectors and that there are sort of several variant precise algorithms within the framework and you know one of them is how whether you're predicting the context words or whether you're predicting the center word so the model i showed was predicting the context words so it was the skip gram model but then there's sort of a detail of how in particular do you do the optimization and what i presented was the sort of easiest way to do it which is naive optimization with the equation the soft max equation for word vectors um it turns out that that naive optimization is sort of ex needlessly expensive and people have come up with um a faster ways of doing it in particular um the commonest thing you see is what's called skip gram with negative sampling and the negative sampling is then sort of a much more efficient way to estimate things and i'll mention that on thursday right okay thank you who's asking for more information about how word vectors are constructed uh beyond the summary of random initialization and then gradient based uh iterative upgrade optimization yeah um so i sort of will do a bit more connecting this together um in the thursday lecture i guess this sort of only so much one can fit in the first class um but the pic the picture is essentially the picture i showed the pieces of so to learn word vectors you start off by having a vector for each word type both for context and outside and those vectors you initialize randomly so that you just put small little numbers that are randomly generated in each vector component and that's just your starting point and so from there on you're using an iterative algorithm where you're progressively updating those word vectors so they do a better job at predicting which words appear in the context of other words and the way that we're going to do that is by using um the gradients that i was sort of starting to show how to calculate and then you know once you have a gradient you can walk in the opposite direction of the gradient and you're then walking downhill i you're minimizing your loss and we're going to sort of do lots of that until our word vectors get as good as possible so you know um it's really all math but in some sense you know word vector learning is sort of miraculous since you do literally just start off with completely random word vectors and run this algorithm of predicting words for a long time and out of nothing emerges these word vectors that represent meaning well 
","['', '', '', 'human language and word meaning', 'word to vec algorithm', 'word vector learning', 'objective function gradients', 'deep learning word vectors', 'amazing deep learning word vectors', 'human languages', 'social system constructed by human beings', 'xkcd cartoon', 'syntactic ambiguities of sentences', 'language is a glorious chaos', 'computers understand human language', 'artificial intelligence', 'human brain', 'chimpanzees', 'bonobos', 'short-term memory', 'ascendancy over other creatures', 'communication between human beings', '']"
"okay so what are we going to do for today so the main content for today is to um go through sort of more stuff about word vectors including touching on word sensors and then introducing the notion of neural network classifiers um so our biggest goal is that by the end of today's class you should feel like you could confidently look at one of the word embeddings papers such as the google word to vect paper or the glove paper or sanjiv aurora's paper that we'll come to later and feel like yeah i can understand this i know what they're doing and it makes sense so let's go back to where we were um so this was sort of introducing this model of word devec and third line your idea was that we started with random word vectors and then we're going to sort of it we have a big corpus of text and we're going to iterate through each word in the whole corpus and for each position we're going to try and predict what words surround this our center word and we're going to do that with a probability distribution that's defined in terms of the dot product between the word vectors for the center word and the context words um and so that will give a probability estimate of a word appearing in the context of into well actual words did occur in the context of into on this occasion so what we're going to want to do is sort of make it more likely that turning problems banking and crises will turn up in the context of into and so that's learning updating the word vectors so they can predict actual surrounding words better um and then the thing that's almost magical is that doing no more than this simple algorithm this allows us to learn word vectors that capture well word similarity and meaningful directions in a word space so more precisely right for this model the only parameters of this model are the word vectors so we have outside word vectors and center word vectors for each word and then we're taking their dot product um to get a probability well we get taking a dot product to get a score of how likely a particular outside word is to occur with the center word and then we're using the soft max transformation to convert those scores into probabilities as i discussed last time and i kind of come back to at the end this time a couple of things to note um this model is what we call an nlp a bag of words models so bag of words models are models that don't actually pay any attention to word order or position it doesn't matter if you're next to the center word or a bit further away on the left or right the probability estimate would be the same and that seems like a very crude model of language that will offend any linguist and it is a very crude model of language and we'll move on to better models of language as we go on but even that crude model of language is enough to learn quite a lot of the probability sorry quite a lot about the properties of words and then the second note is well with this model we want it to give reasonably high probabilities to the words that do occur in the context of the center word at least if they do so at all often but obviously lots of different words can occur so we're not talking about probabilities like 0.3 and 0.5 we're more likely going to be talking about probabilities like 0.01 and numbers like that well how do we achieve that and well the way that the word defect model achieves this and this is the learning phase of the model is to place words that are similar in meaning close to each other in this high dimensional vector space so again you can't read this one but if we scroll into this one we see lots of words that are similar and meaning group close together in the space so here are days of the week like tuesday thursday sunday and also christmas over what else do we have we have samsung and nokia this is a diagram i made quite a few years ago so that's when nokia was still an important maker of cell phones we have various sort of fields like mathematics and economics over here so we group words that are similar in meaning actually one more note i wanted to make on this i mean again this is a two-dimensional picture which is all i can show you on a slide um and it's done with the principal components projection that you'll also use in the assignment um something important to remember but hard to remember is that high dimensional spaces have very different properties to the two dimensional spaces that we can look at and so in particular a word a vector can be close to many other things in a high dimensional space but close to them on different dimensions okay so i've mentioned doing learning so the next question is well how do we um learn good word vectors and this was the bit that i didn't quite hook up at the end of last class so for a while in the last i said calculus and we have to work out um the gradient of the loss function with respect to the parameters that will allow us to make progress um but i didn't sort of altogether put that together so what we're going to do is um we start off with random word vectors we initialize them to small numbers near zero in each dimension we've defined our loss function j which we looked at last time and then we're going to use a gradient descent algorithm which is an iterative iterative algorithm that learns to maximize j of theta by changing theta and so the idea of this algorithm is that from the current values of theta you calculate the gradient j of theta and then what you're going to do is make a small step in the direction of the negative gradient so the gradient is pointing upwards and we're taking a small step in the direction of the negative of the gradient to gradually move down towards the minimum and so one of the parameters of neural nets that you can fiddle in your software package is what is the step size so if you take a really really itsy bitsy step it might take you a long time to minimize the function you do a lot of wasted computation on the other hand if your step size is much too big well then you can actually diverge and start going to worse places or even if you are going downhill a little bit that what's going to happen is you're then going to end up bouncing back and forth and it'll take you much longer to get to the minimum okay in this picture i have a beautiful quadratic and it's easy to minimize it something that you might know about neural networks is that in general they're not convex so you could think that this is just all going to go alright um but the truth is and practice life works out to be okay but i think i won't get into that more right now and come back to that um in the later class so this is our gradient descent so we have the current values of the parameters theta we then walk a little bit in the negative direction of the gradient using our learning rate or step size alpha and that gives us new parameter values where that means that you know these are vectors but for each individual parameter we are updating it a little bit by working out the partial derivative of j with respect to that parameter so that's the simple gradient descent algorithm nobody uses it and you shouldn't use it the problem is that our j is a function of all windows in the corpus remember we're doing this sum over every center word in the entire corpus and we'll often have billions of words in the corpus so actually working out j of theta or the gradient of j of theta would be extremely extremely expensive because we have to iterate over our entire corpus so you'd wait a very long time before you made a single gradient update and so optimization be extremely slow and so basically a hundred percent of the time in neural network land we don't use gradient descent we instead use what's called stochastic gradient descent and stochastic gradient descent is a very simple modification of this so rather than working out an estimate of the gradient based on the entire corpus you simply take one center word or a small batch like 32 center words and you work out an estimate of the gradient based on them now that estimate of the gradient will be noisy and bad because you've only looked at a small fraction of the corpus rather than the whole corpus but nevertheless you can use that estimate of the gradient to update your theta parameters in exactly the same way and so this is the algorithm that we can do and so then if we have a billion word corpus um we can if we do it on each center word we can make a billion updates to the parameters we pass through the corpus once rather than only making one more accurate update to the parameters at once you've been through the corpus so overall we can learn several orders of magnitude more quickly and so this is the algorithm that you'll be using everywhere including um you know right right from the beginning from our assignments um again just an extra comment of more complicated stuff we'll come back to all right this is the gradient descent is a sort of performance hack it lets you learn much more quickly it turns out it's not only a performance hack neural nets have some quite counter-intuitive um properties and actually the fact that stochastic gradient descent is kind of noisy and bounces around as it does its thing it actually means that in complex networks it learns better um solutions than if you were to run plain gradient descent very slowly so you can both compute much more quickly and do a better job okay one final note on running stochastic gradients with word vectors this is kind of an aside but something to note is that if we're doing a stochastic gradient update based on one window then actually in that window we'll have seen almost none of our parameters because if we have a window of something like five words to either side of the center word we've seen at most 11 distinct word types so we will have gradient information for those 11 words but the other 100 000 odd words now vocabulary will have no gradient update information so this will be a very very sparse gradient update so if you're only thinking math you can just have your entire gradient and use the equation that i showed before but if you're thinking systems optimization then you'd want to think well actually i only want to update the parameters for a few words and there have to be and there are much more efficient ways that i could do that um and so um here's so this is another aside will be useful for the assignment so i will say it up until now when i presented word vectors i presented them as column vectors and that makes the most sense if you think about it as a piece of math whereas actually in all common deep learning packages including pytorch that we're using word vectors are actually represented as row vectors and if you remember back to the representation of matrices and cs107 or something like that um that you'll know that that's then obviously efficient um for representing words because then you can access an entire word vector as a contiguous range of memory different if you're in fortran anyway so actually our word vectors will be row vectors when you look at those um inside pi torch okay now i wanted to say a bit more about the word to vec algorithm family um and also um what you're going to do in homework 2. um so if you're still meant to be working on homework 1 which remembers um to next tuesday that really actually with today's content we're starting into homework two and i'll kind of go through the first part of homework two today and this other stuff you need to know for homework two so i mentioned briefly the idea that we have two separate vectors for each word type the center vector and the outside vectors and we just average them both at the end they're similar but not identical for multiple reasons including the random initialization and the stochastic gradient descent um you can implement a word defect algorithm with just one vector per word and actually if you do it works slightly better but it makes the algorithm much more complicated and the reason for that is sometimes you'll have the same word type as the center word and the context word and that means that when you're doing your calculus at that point you've then got this sort of messy case that just for that word you're getting an x squared term oh sorry a dot product you're getting a dot product of x dot x term which makes it sort of much messier to work out and so that's why we use this sort of simple optimization of having two vectors per word okay so for the word to vect model as introduced in the miklov at our paper in 2013 it wasn't really just one algorithm it was a family of algorithms so there are two basic model variants one was called the skip gram model which is the one that i've explained to you that predicted for outside words position independent given the centre word in a bag of words style model the other one was called the continuous bag of words model sibo and in this one you predict the center word from a bag of context words both of these give similar results the skipgram one is more natural in various ways so it's sort of normally the one that people have um gravitated to in subsequent work um but then as to how you train this model um what i've presented so far is the naive softmax equation which is a simple but relatively expensive training method and so that isn't really what they suggest using in your paper in the paper they suggest using a method that's called negative sampling so an acronym you'll see sometimes is sgns which means skip grams negative sampling so let me just um say a little bit um about what this is but actually doing the script gram model with negative sampling is the part of homework too so you'll get to know this model well so the point is that if you use this naive softmax you know even though people commonly do use this naive softmax in various neural net models that working out the denominator is pretty expensive and that's because you have to iterate over every word in the vocabulary and work out these dot products so if you have a hundred thousand word um vocabulary you have to do a hundred thousand dot products um to work out the denominator and that seems a little bit of a shame and so instead of that the idea of negative sampling is where instead of using this soft max we're going to train binary logistic regression models for both the troop the true pair of center word and the context word versus noise pairs where we keep the true center word and we just randomly sample words from the vocabulary so as presented in the paper the idea is like this so overall what we want to optimize is still an average of the loss for each particular center word but for when we're working out the loss for each particular center word we're going to work out um sorry the loss for each particular center word and each particular window we're going to take the dot product as before of the center word and the outside word and that's sort of the main quantity but now instead of using that inside the softmax we're going to put it through the logistic function which is sometimes also often also called the sigmoid function the name logistic is more precise so that's this function here so the logistic function is a handy function that will map any real number to a probability between zero and one open interval so basically if the dot product is large the logistic of the dot product will be virtually one okay so we want this to be large and then what we'd like is on average we'd like the dot product between the center word and words that we just chose randomly i.e they most likely didn't actually occur in the context of the center word to be small and there's just one little trick of how this is done which is this sigmoid function is symmetric and so if um we want this probability to be small we can take the negative of the dot product so we're wanting it to be over here that the product the dot product of a random word in the center word is a negative number and so then we're going to take the negation of that and then again once we put that through the sigmoid we'd like a big number okay so the way they're presenting things they're actually maximizing this quantity but if i go back to making it a bit more similar to the way we had written things we've worked with minimizing the negative log likelihood um so it it looks like this so we're taking the negative log likelihood of this the sigmoid of the dot product um again negative log likelihood we're using the same negator dot product through the sigmoid and then we're going to work out this quantity for a handful of brand number we k negative samples um and how likely they are to sample word depends on their probability and where this loss function is going to be minimized given this negation by making these dot products large and these dot products um smalling negative so they're just then one other trick that they use actually there's more than one other trick that's used in the word defect paper to get it to perform well but i'll only mention one of their other tricks here um when they sample the words they don't simply just sample the words based on their um probability of occurrence in the corpus or uniformly what they do is they start with what we call the unigram distribution of words so that is how often words actually occur in our big corpus so if you have a billion word corpus and a particular word occurred 90 times in it you're taking 90 divided by a billion and so that's the unigram probability of the word but what they then do is that they take that to the three-quarters power and the effect of that three-quarters power which is then re-normalized to make a probability distribution with z kind of like we saw last time with the soft max by taking the three-quarters power that has the effect of dampening the difference between common and rare words um so that less frequent words are sampled somewhat more often but still not nearly as much as they would be if you just use something like a uniform distribution over the vocabulary okay so that's basically um everything to say about the basics of how we have this very simple neural network algorithm word deveck and how we can train it and learn word vectors so for the next bit what i want to do is step back a bit and say well here's an algorithm that i've shown you that works great um what else could we have done and what can we say about that um and the first thing that you might think about is well here's this funny iterative algorithm to give you word vectors um you know if we have a lot of words and a corpus it seems like a more obvious thing that we could do is just look at the counts of how words occur with each other and build a matrix of counts uh co-occurrence matrix so here's the idea of a co-occurrence matrix so i've got a teeny little corpus i like deep learning i like nlp i enjoy flying and i can define a window size i made my window simply size one to make it easy to fill in my matrix symmetric just like our word to back algorithm and so then the counts in these cells are simply how often things that co-occur in the window of size one so i like occurs twice so we get twos in these cells because it's symmetric deep learning occurs one so we get one here and lots of other things occur zero so we can build up a co-occurrence matrix like this and well these actually give us a representation of words as co-occurrence vectors so i can take the word i with either a row or a column vector since it's symmetric and say okay my representation of the word i is this row vector and that is a representation of the word i and i think you can maybe convince yourself that to the extent that words have similar meaning and usage you'd sort of expect them to have somewhat similar vectors right so if i had the word u as well on a larger corpus you might expect i and u to have similar vectors because i like you like i enjoy you and joy um you'd see the same kinds of possibilities hey chris could you keep looking to answer some questions sure all right so we got some questions from negative uh sort of the negative stamping sampling slides um in particular um what's like can you give some intuition for negative sampling what is the negative sampling doing and why do we uh only take one positive example those are two questions that could be answered in tandem okay um that's a good question okay i'll try and give more intuition so is to work out something like what the softmax did in a much more efficient way um so in the soft max well you wanted to give high probability to the in predicting the context a context word that actually did appear with the center word um and well the way you do that is by having the dot product between those two words be as big as possible and part of how but you know you're going to be sort of it's more than that because in the denominator you're also working out the dot product with every other word in the vocabulary so as well as wanting the dot product with the actual word that you see in the context to be big you maximize your likelihood by making the dot products of other words that weren't in the context smaller because that's shrinking your denominator and therefore um you've got a bigger number coming out and you're maximizing the loss so even for the softmax the general thing that you want to do to maximize it is have dot product with words action the context big dot product with words not in the context be small to the extent possible and obviously you have to average this as best you can over all kinds of different contexts because sometimes different words appear in different contexts obviously so um so the negative sampling is a way of therefore trying to maximize the same objective now you know for you only you only have one positive term because you're actually wanting to use the actual data um so you're not waiting wanting to invent data so for working out the entire j we do do work this quantity out for every center word and every context word so you know we are iterating over the different words in the context window and then we're moving through positions in the corpus so we're doing different vcs so you know gradually we do this but for one particular center word and one particular context word we only have one real piece of data that's positive so that's all we use because we don't know what other words should be counted as positive words now for the negative words you could just sample one negative word and that would probably work but if you want a sort of a slightly better more stable sense of okay we'd like to in general have other words have low probability it seems like you might be able to get better more stable results if you instead say let's have 10 or 15 sample negative words and indeed that's been found to be true but and for the negative words well it's easy to sample any number of random words you want and at that point it's kind of a probabilistic argument the words that you're sampling might not be actually bad words to appear in the context they might actually be other words that are in the context but 99.9 of the time they will be unlikely words to occur in the context and so they're good ones to use and yes you only sample 10 or 15 of them but that's enough to make progress because the center word is going to turn up on other occasions and when it does you'll sample different words over here so that you gradually sample different parts of the space and start to learn we had this co-occurrence matrix and it gives a representation of words as co-occurrence vectors and just one more note on that i mean there are actually two ways that people have commonly made these co-occurrence matrices one corresponds to what we've seen already that you use a window around a word which is similar to word to vec and that allows you to capture some locality and some of the sort of syntactic and semantic proximity that's more fine-grained the other way these co-matrix diseases have often made is that normally documents have some structure whether it's paragraphs or um just actual web pages sort of sized documents so you can just make the your window size a paragraph or a whole web page and count co-occurrence in those and this is the kind of method that's often been used in information retrieval in methods like latent semantic analysis okay so the question then is are these kind of count word vectors good things to use well people have used them they're not terrible but they have certain problems the kind of problems that they have uh well firstly they're huge though very sparse so this is back where i said before if we had a vocabulary of half a million words when then we have a half a million dimensional vector for each word which is much much bigger than the word vectors that we typically use um and it also means that because we have these very high dimensional vectors um that we have a lot of sparsity and a lot of randomness so the results that you get tend to be noisier and less robust depending on what particular stuff was in the corpus and so in general people have found that you can get much better results by working with low dimensional vectors so then the idea is we can store the most of the important information about the distribution of words in the context of other words in a fixed small number of dimensions giving a dense vector and in practice the dimensionality of the vectors that are used are normally somewhere between 25 and a thousand and so at that point we need to use two we need to use some way to reduce the dimensionality of our count co occurrence vectors so if you have a good memory from a linear algebra class you hopefully saw singular value decomposition and it has various mathematical properties um that i'm not going to talk about here of single singular value projection giving you an optimal way under a certain definition of optimality of producing a reduced dimensionality matrix that maximally or sorry pair of matrices that maximally well lets you recover the original matrix but the idea of the singular value decomposition is you can take any matrix such as our count matrix and you can decompose that into three matrices u a diagonal matrix sigma and a v transpose matrix um and this works for any shape now in these matrices some parts of it are never used because since this matrix is rectangular there's nothing over here and so this part of the the transpose matrix gets ignored but if you're wanting to get smaller dimensional representations what you do is take advantage of the fact that the singular values inside the diagonal sigma matrix are ordered from largest down to smallest so what we can do is just delete out more of the matrix of the delete out some singular values which effectively means that in this product sum of u and sum of v is also not used and so then as a result of that we're getting lower dimensional representations um for our words if we're wanting to have word vectors which still do as good as possible a job within the given dimensionality of enabling you to recover the original co-occurrence matrix so from a linear algebra background um this is the obvious thing to use so how does that work um well if you just build a raw count co-occurrence matrix and run svd on that and try and use those as word vectors it actually works poorly and it works poorly because if you get into the mathematical assumptions of svd you're expecting to have these normally distributed errors and what you're getting with word counts looked not at all like something's normal you didn't because you have exceedingly common words like arthur and and and you have a very large number of rare words so that doesn't work very well but you actually get something that works a lot better if you scale the counts in the cells so to deal with this problem of extremely frequent words there are some things we can do we could just take the log of the raw counts we could kind of cap the maximum count we could throw away the function words and any of these kind of ideas let you build then have a co-occurrence matrix that you get more useful word vectors from running something like svd and indeed these kind of models were explored um in the 1990s and in the 2000s and in particular um doug rhody explored a number of these ideas as is how to improve the co-occurrence matrix in a model that he built that was called kohl's and you know actually in his kohl's model he observed the fact that you could get the same kind of linear components that have semantic components that we saw yesterday when talking about analogies so for example this is a figure from his paper and you can see that we seem to have a meaning component going from a verb to the person who does the verb so drive to drive a swim to swimmer teach the teacher marry to priest and that these vector components are not perfectly but are roughly parallel and roughly the same size and so we have a meaning component there that we could add on to another word just like we did for previously for analogies we could say drivers to driver as mari is to what and we'd add on this screen vector component which is roughly the same as this one and we'd say oh priest so that this space could actually get some word vectors analogies right as well and so that seemed really interesting to us around the time word to vec came out of wanting to understand better what the iterative updating algorithm of word deveck did and how it related to these more linear algebra based methods that have been explored in the couple of decades previously and so for the next bit i want to tell you a little bit about the glove algorithm which was an algorithm for word vectors that was made by jeffrey pennington richard socher and me in 2014 and so the starting point of this was to try to connect together the linear algebra based methods on co-occurrence matrices like lsa and coles with the models like skip grand sibo and their other friends which were iterative neural updating algorithms so on the one hand you know the linear algebra methods actually seemed like they had advantages for fast training and efficient usage of statistics but although there had been work on capturing word similarities with them by and large the results weren't as good perhaps because of disproportionate importance given to large accounts in the main conversely um the models um the the neural models it seems like if you're just doing these gradient updates on windows you're somehow inefficiently using statistics versus a co-occurrence matrix but on the other hand it's actually easier to scale to a very large corpus by trading time for space and but at that time it seemed like the newer methods just worked better for people that they generated improved performance on many tasks not just on word similarity and that they could capture complex patterns such as the analogies that went beyond word similarity and so what we wanted to do was understand a bit more as to what do you what properties do you need to have this analogies work out as i showed last time and so what we realized was that if you'd like to do have these sort of vector subtractions um and additions work for an analogy the property that you um want is for meaning components so a meaning component is something like going from male to female queen to king or going from its age and truck to driver um that those meaning components should be represented as ratios of co-occurrence probabilities so here's an example that shows that okay so suppose the meaning component that we want to get out is the spectrum from solid to gas as in physics well you'd think that you can get at the solid part of it perhaps by saying does the word co-occur with ice and the word solid occurs with ice so that looks hopeful and gas doesn't occur with ice much so that looks hopeful but the problem is the word water will also occur a lot with ice and if you just take some other random word like the word random it probably doesn't occur with ice much in contrast if you look at words co-occurring with steam solid won't occur with steam much but gas will but water will again and random will be small so to get out the meaning component we want of going from gas to solid what's actually really useful is to look at the ratio of these co-occurrence probabilities because then we get a spectrum of large to small between solid and gas whereas for water in a random word it basically cancels out and gives you one um i just wrote these numbers in but if you count them up in a large corpus it is basically what you get so here are actual co-occurrence probabilities and that for water and my random word which was fashion here these are approximately one um whereas for the ratio of probability of co-occurrence of solid with ice or steam is about ten and four guess it's about a tenth so how can we capture these ratios of coeconos probabilities as linear meaning components so that in our word vector space we can just add and subtract linear meaning components well it seems like the way we can achieve that is if we build a log by linear model so that the dot product between two word vectors attempts to approximate the log of the probability of co-occurrence so if you do that you then get this property that the difference between two vectors its similarity to another word corresponds to the log of the probability ratio shown on the previous slide so the glove model wanted to try and um unify the thinking between the co-occurrence matrix models and the neural models by being in some way similar to a newer model but actually calculated on top of a current matrix count so we had an explicit loss function and our explicit loss function is that we wanted the dot product to be similar to the log of the co-occurrence we actually added in some bias terms here but i'll ignore those for the moment and we wanted to not have very common words dominate and so we kept the effect of high word counts using this f function that's shown here and then we could optimize this j function directly on the co-occurrence count matrix so that gave us fast training scalable to huge corpora um and so this algorithm worked very well so if you ask if you run this algorithm ask what are the nearest words to frog you get frogs toad and then you get some complicated words but it turns out they are all frogs um until you get down to lizards so latoya's that lovely tree frog there um and so this actually seemed to work out pretty well how well did it work out um to discuss that a bit more i now want to say something about how do we evaluate word vectors are we good for up to there for questions we've got some questions uh what do you mean by an inefficient use of statistics as a con for skip gram well what i mean is that you know for word to vac you're just you know looking at one center word at a time and generating a few negative samples and so it sort of seems like doing something always precise there whereas if you're doing uh optimization algorithm on the whole matrix at once well you actually know everything about the matrix at once you're not just looking at what words what other words occurred in this one context of the center word you've got the entire vector of co-occurrence accounts for the center word and another word and so therefore you can much more efficiently and less noisily work out how to minimize your loss okay i'll go on okay so i've sort of said look at these word vectors they're great and i sort of showed you a few things at the end of the last class which argued hey these are great um you know they work out these analogies um they show similarity and things like this um we want to make this a bit more precise and indeed for natural language processing as in other areas of machine learning a big part of what people are doing is working out good ways to evaluate knowledge that things have so how can we really evaluate word vectors so in general for nlp evaluation people talk about two ways of evaluation intrinsic and extrinsic so an intrinsic evaluation means that you evaluate directly on the specific or intermediate subtasks that you've been working on so i want a measure where i can directly score how good my word vectors are and normally intrinsic evaluations are fast to compute they helped you to understand the component you've been working on but often simply trying to optimize that component may or may not have a very big good effect on the overall system that you're trying to build um so people have also also been very interested in extrinsic evaluations so an extrinsic evaluation is that you take some real task of interest to human beings whether that's web search or machine translation or something like that and you say your goal is to actually improve performance on that task well that's a real proof that this is doing something useful so it in some ways it's just clearly better but on the other hand it also has some disadvantages it takes a lot longer to evaluate on an extrinsic task because it's a much bigger system and sometimes you know when you change things it's unclear whether the fact that the numbers went down was because you now have worse word vectors or whether it's just somehow the other components of the system interacted better with your old word vectors and if you change the other components as well things would get better again so in some ways it can sometimes be muddier to see if you're making progress but i'll touch on both of these methods here um so for intrinsic evaluation of word vectors one way um which we mentioned last time was this word vector analogy so we could simply give our models a big collection of word vector analogy problems so we could say man is the woman as king is the what and ask the model to find the word that is closest using that sort of word analogy computation and hope that what comes out there is queen and so that's something people have done and have worked out an accuracy score of how often that you are right at this point i should just mention one little trick of these word vector analogies that everyone uses but not everyone talks about a lot in the first instance i mean there's a little trick which you can find in the sim code if you look at it that when it does manage to woman as king is to what something that could often happen is that actually the word once you do your pluses and your minuses that the word that will actually be closest is still king so the way people always do this is that they don't allow one of the three input words um in the selection process so you're choosing the nearest word that isn't one of what words um okay so since um here is showing results from the glove vectors um so the glove factors have this strong linear component property just like i showed before um for um coal so this is for the male female dimension and so because of this you'd expect in a lot of cases that word analogies would work because i can take the vector difference of man and woman and then if i add that vector difference onto brother i expect to get to sister and king queen and from any of these examples but of course they may not always work right because if i start from emperor it's sort of on a more of a lean and so it might turn out that i get countess or duchess coming out instead you can do this for various different relations so different semantic relations so these sort of word vectors actually learn quite a bit of just world knowledge um so here's the company ceo or this is the company ceo around 2010 to 2014 when the data was taken from word vectors and they as well as semantic things or pragmatic things like this they also learn syntactic things so here are vectors for positive comparative and superlative forms of adjectives and you can see those also um move and roughly linear components um so um the word to vect people built a data set of analogies so you could evaluate different models on the accuracy of their analogies and so here's how you can do this and this gives some numbers so there are semantic and syntactic analogies i'll just look at the totals okay so what i said before is if you just use unscaled co-occurrence counts and pass them through an svd things work terribly and you see that there you only get 7.3 but then as i also pointed out if you do some scaling you can actually get svd to of a scaled count matrix to work reasonably well so this spdl is similar to the kohl's model and now we're getting up to 60.1 which actually isn't a bad score right so you can actually do a decent job without a neural network um and then here are the two variants of the um word to vect model and here are our results from the glove model and of course at the time 2014 we took this as absolute proof that our model was better and our more efficient use of statistics was really working in our favor um with seven years of retrospect i think that's kind of not really true it turns out i think the main part of why we scored better is that we actually had better data and so there's a bit of evidence about that on this next slide here so this looks at the semantic syntactic and overall performance on word analogies of glove models that were trained on different subsets of data so in particular the two on the left are trained on wikipedia and you can see that training on wikipedia makes you do really well on semantic analogies which maybe makes sense because wikipedia just tells you a lot of semantic facts i mean that's kind of what encyclopedias do and so one of the big advantages we actually had was that wikipedia that the glove model was partly trained on wikipedia as well as other texts whereas the word to vect model that was released was trained exclusively on google news so newswire data and if you only train on a smallish amount of newswire data you can see that for the semantics it's it's just not as good as even a one-quarter of the size amount of wikipedia data though if you get a lot of data you can compensate for that so here on the the right hand did you then have common crawl web data and so once there's a lot of web data so now 42 billion words um you're then starting to add good scores again from the semantic side um the graph on the right then shows how well do you do as you increase the vector dimension and so what you can see there is you know 25 dimensional vectors aren't very good they go up to sort of 50 and then 100 and so 100 dimensional vectors already work reasonably well so that's why i used hundred dimensional vectors when i showed my example in class yet is the sweet spare too long load and working reasonably well but you still get significant gains for 200 and it's somewhat to 300 so at least back around so 2013 to 15 everyone sort of gravitated to the fact that 300 dimensional vectors is the sweet spot um so almost frequently if you look through the best known sets of word vectors that include the word divec vectors and the glove vectors that usually what you get is 300 dimensional word vectors um that's not the only intrinsic evaluation you can do another intrinsic evaluation you can do is see how these models model human judgments of word similarity um so psychologists for several decades have actually taken human judgments a word similarity where literally you're asking people for pairs of words like professor and doctor to give them a similarity score that's sort of being measured as some continuous quantity giving you a score between say 0 and ten um and so there are human judgments which are then averaged over multiple human judgments as to how similar different words are so tiger and cat is pretty similar um computer and internet is pretty similar plane and car is less similar stock and cd aren't very similar at all but stock and jaguar even less similar so we could then say for the our models do they have the same similarity judgments and in particular we can measure a correlation coefficient of whether they give the same ordering of similarity judgments and so then we can get data for that and so there are various different data sets of word similarities and we can score different models as to how well they do on similarities and again you see here that plain svds and works comparatively better here for similarities than it did for analogies you know it's not great but it's now not completely terrible because we no longer need that linear property but again scaled svds work a lot better word deveck works a bit better than that and we got some of the same kind of minor advantages from the glove model hey chris sorry to interrupt a lot of the students were asking if you could re-explain the objective function for the glove model and also what log bilinear means okay uh sure okay here is here is my here is my um objective function the right if i go so one slide before that right so the property that we want is that we want the dot product um to represent the log probability of co-occurrence so um and that's then gives me my tricky log bilinear so the buy is that there's sort of the wi and the wj so that there are sort of two linear things and it's linear in each one of them so this is sort of like having and rather than having a sort of an ax where you just have something that's linear in x and a is a constant it's bilinear because we have the w i w j and there's linear in both of them and that's then related to the log of a probability and so that gives us the log by linear model and so since we since we'd like these things to be equal what we're doing here if you ignore these two center terms is that we're wanting to say the difference between these two is as small as possible so we're taking this difference and we're squaring it so it's always positive and we want that squared term to be as small as possible and you know that's 90 percent of it and you can basically stop there but the other bit that's in here is a lot of the time when you're building models um rather than simply having sort of an ax model it seems useful to have a bias term which can move things up and down for the word in general and so we add it into the model bias term so that there's a bias term for both words so if in general probabilities are high for a certain word this bias term can model that and for the other word this bias term then model it okay so now i'll pop back and after um oh actually i just saw someone said why multiplying by the f of sorry i did skip that last term um okay the why modifying by this f of x i j so this last bit was to scale things depending on the frequency of a word because you want to pay more attention to words that are more common or word pairs that are more common because you know if you think about it um in word divect terms you're seeing if things have a co-occurrence account of 50 versus 3 you want to do a better job at modeling the co-occurrence of the things that occurred together 50 times and so you want to consider in the count of co-occurrence but then the argument is that that actually leads you astray when you have extremely common words like function words and so effectively you paid more attention to words that co-occurred together up until a certain point and then the curve just went flat so it didn't matter if it was an extremely extremely common word so then um for extrinsic word vector evaluation so at this point you're now wanting to sort of say well can we embed our word vectors in some end user task and do they help and do different word vectors work better or worse than other word vectors so this is something that we'll see a lot of later in the class i mean in particular when you get on to doing assignment three that assignment three you get to build dependency parsers and you can then use word vectors in the dependency parser and see how much they help we don't actually make you test out different sets of word vectors but you could um here's just one example of this to give you a sense so the task of named entity recognition is going through a piece of text and identifying mentions of a person name or an organization name like a company or a location and so if you have good word vectors um do they help you do named entity recognition better and the answer to that is yes so if one starts off with a model that simply has discrete features so it uses word identity as features you can build a pretty good named entity model doing that but if you add into it word vectors you get a better representation of the meaning of words and so that you can have the numbers go up quite a bit and then you can compare different models to see how much gain they give you in terms of this extrinsic task so skipping ahead this was a question that i was asked after class which was word senses because so far we've had just one word sorry for one particular string we've got some string house and we're going to say for each of those strings there's a word vector and if you think about it a bit more that seems like it's very weird because actually most words um especially common words and especially words that have existed for a long time actually have many meanings which are very different so how could that be captured if you only have one word vector for the word because you can't actually capture the fact that you've got different meanings for the word because your meaning for the word is just one point in space one vector and so as an example of that here's the word pipe now it's actually but it is an old germanic word well what kind of means does the word pike have um so you can maybe just think for a minute and think um what were meanings the word pike has and it actually turns out you know it has a lot of different meanings so so perhaps the most basic meaning is um if you did fantasy games or something medieval weapons um a sharp pointed staff there's a pike um but there's a kind of a fish that has a similar elongated shape that's a pike um it was used for railroad um lines maybe that usage isn't used much anymore but it certainly still survives in referring to roads so this is like when you have turnpikes we have expressions where pike means the future like coming down the pike it's a position in diving that divers do a pike those are all now nooses they're also verbal uses so you can pike somebody with your pike you know different usages might have different currency in australia you can also use pike to mean that you pull out of doing something like i reckon he's going to pike i don't think that usage is used in america but lots of meanings and actually for words that are commoner if you start thinking words like line or field i mean they just have even more meanings than this so what are we actually doing with just one vector for a word and well one way you could go is to say okay up until now what we've done is crazy pike has and other words have all of these different meanings so maybe what we should do is have different word vectors for the different meanings of pike so we'd have one word vector for the medieval pointy weapon another word vector for the kind of fish another word vector for the kind of road so that they then be word sense vectors and you can do that i mean actually we were working on that in the early 2010s actually even before word to vect came out so this picture is a little bit small to see but what we were doing was for words we work clustering instances of a word hoping that those clusters so clustering the word tokens hoping those clusters that were similar represented sensors and then for the clusters of word tokens we were sort of treating them like they were separate words and learning a word vector for each and you know basically that actually works so in green we have two senses for the word bank and so there's one sense for the word bank that's over here where it's close to words like banking finance transaction and laundering and then we have another sense for the word bank over here whereas close to words like plateau boundary gap territory which is the riverbank sense of the word bank um and for the word jaguar that's in purple um well jq has a number of sensors and so we have those as well so this sense down here is um close to hunter so that's the sort of big game animal sense of um jaguar up the top here is being shown close to luxury and convertibles this is the jaguar car sense um then jaguar here is near string um keyboard and words like that so jaguar is the name of a kind of keyboard um and then this final jaguar over here is close to software and microsoft and then if you're old enough you'll remember that there was an old version of mac os so it's called jaguar um so that's then the computer sense so basically this does work and we can learn word vectors for different sensors of a word but actually this isn't the majority way that things have been gone in practice and there are kind of a couple of reasons for that i mean one is just simplicity if you do this it's kind of complex because you first of all have to learn word senses and then start learning word vectors in terms of the word senses but the other reason is although this model of having word sensors um is traditional it's what you see in dictionaries it's commonly what's being used in natural language processing i mean it tends to be imperfect in its own way because we're trying to take all the uses of the word pike and sort of cut them up into key different sensors where the difference is kind of overlapping and it's often not clear which ones to count as distinct so for example here right a railroad line and a type of road well sort of that's the same sense of pike it's just that they're different forms of transportation and so you know that this could be you know a type of transportation line and cover both of them so it's always sort of very unclear how you cut word meaning into different sensors and indeed if you look at different dictionaries everyone does it differently so um it actually turns out that in practice you can do rather well by simply having one word vector per word type and what happens if you do that well what you find is that what you learn as a word vector is what gets referred to in fancy talk as a super superposition of the dif of the word vectors for the different senses of a word um where the word superposition means no more or less than a weighted sum so our the vector that we learned for pike will be a weighted average of the vectors that you would have learned for the medieval weapon sense plus the fish sense plus the road sense plus whatever other senses that you have where the weighting that's given to these different sense vectors corresponds to the frequencies of use of the different sensors so we end up with the word um the vector for pike um being a kind of an average vector and so if you're um if you're say okay you've just added up several different vectors into an average you might think that that's kind of useless because you know you've lost the real meanings of the word and you've just got some kind of funny average vector that's in between them but actually it turns out that if you use this average vector in applications it tends to sort of self-disambiguate because if you say is the word pike similar to the word for fish well part of this vector represents fish the fish sense of pike and so in those components it'll be kind of similar to the fish vector and so yes you'll say the um substantia there's substantial similarity whereas if in another um piece of text that says you know the men were aimed were armed with pikes and lancers or pikes and mesas or whatever other medieval weapons you remember well actually some of that meaning is in the pike vector as well and so it'll say yeah there's good similarity with mace and staff and words like that as well and in fact we can work out which sense of pike is intended by just sort of seeing which components are similar to other words that are used in the same context and indeed there's actually a much more surprising result than that and this is a result that's um jews are sanjiv aurora tanguma who is now on our stanford faculty and others in 2018 and that's the following result which i'm not actually going to explain but um so if you think that the vector for pike is just a sum of the vectors for the different sensors well it should be you'd think that it's just completely impossible to reconstruct the sense vectors from the vector for um the word type because normally if i say i've got two numbers the sum of them is 17 you just have no information as to what my two numbers are right you can't resolve it um and even worse if i tell you i've got three numbers and they sum to 17 but it turns out that when we have these high dimensional vector spaces that things are so sparse in those high dimensional vector spaces that you can use ideas from sparse coding to actually separate out the different sensors providing they're relatively common so they show in their paper that you can start with the vector of say pike and actually separate out components of that vector that correspond to different sensors of the word pike and so here's an example at the bottom of this slide which is for the word it's separated out that vector into five different sensors and so there's one sense it's close to the words trousers blouse waistcoat so this is the sort of clothing sense of tie another sensors is close to wires cables wiring electrical so that's the sort of the thai sense of attire used in the electrical staff and then we have sort of scoreline goals equalizer so this is the sporting game sense of tie this one also seems to in a different way evokes sporting game sense of tie and then there's finally this one here maybe my music is just really bad maybe it's because you get ties and music when you tie notes together i guess so you get these different senses out of it 
","['', 'word vectors', 'word embedding', 'neural network classifiers', 'bag-of-words model', 'gradient descent', 'stochastic gradient descent', 'word2vec', 'google word2vec', 'glove', 'sanjiv aurora', 'co-occurrence matrix', 'word similarity', 'semantic similarity', 'intrinsic evaluation', 'human judgments', 'polysemy', 'word sense disambiguation', 'sparse coding', 'high dimensional vector space', '']"
"hi everyone i'll get started okay so we're now i'm back for the second week of cs224n um on natural language processing with deep learning okay so um for today's lecture what we're going to be looking at is all the math details of doing neural net learning first of all looking at how we can work out by hand um gradients for training neural networks and then looking at how it's done more algorithmically which is known as the back propagation algorithm and correspondingly for you guys um well i hope you remembered that you know one minute ago was when assignment one was due and everyone has handed that in if by some chance you haven't handed it in um really should hand it in as soon as possible best to preserve those late days for the harder assignments so i mean i actually forgot to mention we actually did make one change um for this year to make it a bit easier when occasionally people join the class a week late if you want to this year in the grading um assignment one can be discounted and we'll just use your other four assignments but if you've been in the class so far for that 98 percent of people well since assignment one is the easiest assignment again it's silly not to do it and have it as part of your grade okay so starting today we've put out assignment two and assignment two is all about making sure you really understand the math of neural networks and then the software that we use to do that math so this is going to be a bit of a tough week for some so for some people who are great on all their math and backgrounds um they'll feel like this is stuff they know well nothing very difficult but i know there are quite a few of you um who this lecture and week is the biggest struggle of the course we really do want people to actually have an understanding of what goes on in your network learning rather than viewing it as some kind of deep magic and i hope that some of the material we give today and that you read up on and use in the assignment will really give you more of a sense of what these neural networks are doing and how it is just math that's applied in the systematic large scale that works out the answers and that this will be valuable and giving you a deeper sense of what's going on but if this material seems very um scary and difficult you can take some refuge in the fact that there's fast light at the end of the tunnel since this is really the only lecture that's heavily going through the math details of neural networks after that we'll be kind of popping back up to a higher level and by and large after this week we'll be making use of software to do a lot of the complicated math for us um but nevertheless i hope this is valuable i'll go through everything quickly today but if this isn't stuff that you know backwards i really do encourage you um to you know work through it and get help as you need it so do come along to our office hours there are also a number of pieces of tutorial material given in the syllabus so there's both the lecture notes there's some materials from cs231 um in the list of readings the very top reading is uh some material put together by kevin clark a couple of years ago and actually that one's my favorite the presentation there fairly closely follows the presentation in this lecture of going through matrix calculus so you know personally i'd recommend starting with that one but there are four different ones you can choose from if one of them seems more helpful to you two other things on what's coming up um actually for thursday's lecture we make a big change and thursday's lecture is probably the most linguistic lecture of the whole class where we go through the details of dependency grammar and dependency parsing some people find that tough as well but at least it'll be tough in a different way and then one other really good opportunity is this friday we have our second tutorial at 10 a.m which is an introduction to pie torch which is the deep learning framework that we'll be using for the rest of the class once we've gone through these first two assignments where you um do things by yourself um so this is a great chance to get intro to pytorch they'll be really useful for later in the class okay um today's material is really all about sort of the math of neural networks but just to sort of introduce a setting where we can work through this i'm going to introduce a simple nlp task and a simple form of classifier that we can use for it so the task of named entity recognition is a very common basic nlp task and the goal of this is you're looking through pieces of text and you're wanting to label by labeling the words which words belong to entity categories like persons locations products dates times etc so for this piece of text last night paris hilton wowed in the sequin gown samuel quinn was arrested in the hilton hotel in paris in april 1989 the the some words are being labeled as named entities as shown um these two sentences don't actually belong together in the same article but i chose those two sentences to illustrate the basic point that it's not that you can just do this task by using a dictionary yes a dictionary is helpful to know that paris can possibly be a location but paris can also be a person name so you have to use context to get named entity recognition right okay well how might we do that with the neural network there are much more advanced ways of doing this but a simple yet already pretty good way of doing um named entity recognition within a simple neural net is to say well what we're going to do is use the word vectors that we've learned about and we're going to build up a context window of word vectors and then we're going to put those through a neural network layer and then feed it through a softmax classifier of the kind that we um sorry i said that wrong and then we're going to feed it through a logistic classifier of the kind that we saw when looking at negative sampling which is going to say for a particular entity type such as location is it high probability location or is it not a high probability location so for a sentence like the museums in paris are amazing to see what we're going to do is for each word say we're doing the word paris we're going to form a window around it say a plus or minus two word window and so for those five words we're going to get word vectors for them from the kind of word debacle glove word vectors we've learned and we're going to make a long vector out of the concatenation of those five word vectors so the word of interest is in the middle and then we're going to feed this vector to a classifier which is at the end going to have a probability of the word being a location and then we could have another classifier that says the probability of the word being a person name and so once we've done that we're then going to run it at the next position so we then say well is the word r a location and we'd feed a window of five words as then in paris are amazing too and put it through the same kind of classifier and so this is the classifier that we'll use so it's input will be this word window so if we have d dimensional word vectors this will be a 5d vector and then we're going to put it through a layer of a neural network so the layer of the neural network is going to multiply this vector by a matrix add on a bias vector and then put that through a non-linearity such as the soft max transformation that we've seen before and that will give us a hidden vector which might be of a smaller dimensionality such as this one here and so then with that hidden vector we're then going to take the dot product of it with an extra vector here here's u so we take u dot product h and so when we do that we're getting out a single number and that number can be any real number and so then finally we're going to put that number through a logistic transform of the same kind that we saw when doing negative sampling the logistic transform will take any real number and it will transform it into a probability that that word is a location so its output is the predicted probability of the word belonging to a particular class and so this could be our location classifier which could classify each word in a window as to what the probability is that it's a location word and so this little neural network here is the neural network i'm going to use today when going through some of the math but actually i'm going to make it even easier on myself i'm going to throw away the logistic function at the top and i'm really just going to work through the math of the bottom three quarters of this if you look at kevin clark's handout that i just mentioned he includes when he works through it also working through the logistic function and we also saw working through a softnext in the first lecture when i was working through some of the word today model okay um so the overall question we want to be able to answer is so here's our stochastic gradient descent equation that we have existing um parameters of our model and we want to update them based on our current loss which is at the j of theta so for getting our um loss here that the true answer as to whether a word is a location or not will be either you know one if it is a location or zero if it isn't our logistic classifier will return some number like um 0.9 and we'll use the distance away from what it should have been squared as our loss um so we work out a loss and then we're moving a little distance in the negative of the gradient which will be in changing our parameter estimates in such a way that they reduce the loss and so this is already being written in terms of a whole vector of parameters which is being updated as to a new vector of parameters but you can also think about it that for each individual parameter theta j that we're working out the partial derivative of the loss with respect to that parameter and then we're moving a little bit in the negative direction of that um that's going to give us a new value for parameter theta j and we're going to update all of the parameters of our model as we learn i mean in particular in contrast to what commonly happens in statistics we also we update not only the sort of parameters of our model that are sort of weights in the classifier but we also will update our data representation so we'll also be changing our word vectors as we learn okay so to build neural nets i.e to train neural nets based on data what we need is to be able to compute this gradient of the parameters so that we can then iteratively update the weights of the model and efficiently train a model that has good weights i.e that has high accuracy and so how can we do that um well what i'm going to talk about today is first of all um how you can do it by hand and so for doing it by hand this is basically a review of matrix calculus and that'll take quite a bit of the lecture and then after um we've talked about that for a while i'll then shift gears and introduce the back propagation algorithm which is the central technology for neural networks and that technology is essentially the efficient application of calculus on a large scale as we'll come to talking about soon so for computing gradients by hand what we're doing is matrix calculus so we're working with vectors and matrices and working out gradients and this can seem like pretty scary stuff and well to the extent that you're kind of scared and don't know what's going on one choice is to work out a non-vectorized gradient by just working out what the partial derivative is for one parameter at a time and i showed a little example of that in the first lecture but it's much much faster and more useful to actually be able to work with vectorized gradients and in some sense if you're not very confident this is kind of almost a leap of faith but it really is the case that multi-variable calculus is just like single variable calculus except you're using vectors and matrices so providing you remember some basics of single variable calculus you really should be able to do this stuff and get it to work out lots of other sources i've mentioned the notes you can also look at the textbook for math 51 or which also has quite a lot of material on this i know some of you have bad memories of math 51. um okay so let's go through this and see how it works from ramping up from the beginning so the beginning of calculus is you know we have a function with one input and one output f of x equals x cubed and so then its gradient is its slope right so that's its derivative so um its derivative is three 3x squared and the way to think about this is how much will the output change if we change the input a little bit right so what we're wanting to do in our neural net models is change what they output so that they do a better job of predicting the correct answers when we're doing supervised learning and so what we want to know is if we fiddle different parameters of the model how much of an effect will that have on the output because then we can choose how to fiddle them in the right way to move things down right so you know when we're saying that the derivative here is 3x squared well what we're saying is that if you're at x equals one if you fiddle the input a little bit the output will change three times as much three times one squared and it does so if i say what's the value at 1.01 it's about 1.03 it's changed three times as much and that's its slope but at x equals four um the derivative is 16 times 348 so if we fiddle the input a little it'll change 48 times as much and that's roughly what happens 4.01 cubed is is 64.48 now of course you know this is just sort of i'm showing it for a small fiddle but you know that's an approximation to the actual truth okay so um then we sort of ramp up to the more complex cases which are more reflective of what we do with neural networks so um if we have a function with one output and n inputs then we have a gradient so a gradient is a vector of partial derivatives with respect to each input so we've got n inputs x1 to xn and we're working out the partial derivative of f with respect to x1 the partial derivative of f with expected respect to x2 etc and we then get a vector of partial derivatives where each element of this vector is just like a simple derivative with respect to one variable okay so from that point we just keep on ramping up for what we do with neural networks so commonly when we have something like a layer in a neural network we'll have a function within inputs they'll be like our word vectors then we do something like multiply by a matrix and then we'll have m outputs so we have a function now which is taking n inputs and is producing m outputs so at this point um what we're calculating for the gradient is what's called a jacobian matrix so for m inputs and n outputs the jacobian is an m by n matrix of partial of every combination of partial derivatives so um i function f splits up into these different sub functions f1 through m fm which generate each of the m outputs and so then we're taking the partial derivative of f1 with respect to x1 through the partial derivative of f1 with respect to xn then heading down you know we make it up to the partial derivative of fm with respect to x1 etc so we have every possible partial derivative of an output variable with respect to one of the input variables okay so in simple calculus when you have a composition of one variable functions so that if you have um y equals x squared and then z equals three y um that's then z is a composition of two functions of well you're composing two functions to get z as a function of x then you can work out the derivative of z with respect to x and the way you do that is with the chain rule and so in the chain rule you multiply derivatives so dz dx equals dz dy times dydx so dzy is just 3 and dydx is 2x so we get 3 times 2x so that overall um the derivative here is 6x and since if we multiply this together we're really saying that z equals 3x squared um you should trivially be able to see again aha its derivative is 6x so that works okay um so once we move into vectors and matrices and jacobians um it's actually the same game so when we're working with those we can compose functions and work out their derivatives by simply multiplying jacobians so if we have start with an input x and then put it through the simplest form of neural network layer and say that z equals wx plus b so we multiply that the x vector by matrix w and then add on a bias vector b and then typically we'd put things through a non-linearity f so f could be a sigmoid function we'll then say h equals f of z so this is the composition of two functions in terms of um vectors and matrices so we can use jacobians and we can say the partial of h with respect to x is going to be the product um of the partial of h with respect to z and the partial of zero with respect to x and this all does work out so let's start going through some examples of how these things work slightly more concretely first just particular jacobians and then composing them together so one case we look at is the non-linearities that we put a vector through so this is something like putting a vector through the sigmoid function f um and so if we have an intermediate vector z and we turn into vector h by putting it through us a logistic function we can say what is dhdz um well for this um formally this is a function that has n inputs and n outputs so at the end of the day we're computing an n by n jacobian and so what that's meaning is the elements of this n by n jacobian are going to take the partial derivative of each output with respect to each input and well what is that going to be in this case well in this case because we're actually just computing element wise a transformation such as a logistic transform of each element zi like the second equation here if i equals j we've got something to compute whereas if i doesn't equal j um there's just the input has no influence on the output and so the derivative of zero so if i doesn't equal j we're going to get a zero and if i does equal j what then we're going to get the regular one variable derivative of the logistic function which if i remember correctly um you were asked to compute now i can't remember it's assigned one or assignment two but one of the two asks you to compute it um so our jacobian for this case looks like this we have a diagonal matrix with the um the derivatives of each element along the diagonal and everything else is zero okay so let's look at a couple of other jacobians um so if we are asking if we've got this w x plus b basic neural network layer and we're asking um for the gradient with respect to x then what we're going to have coming out is that that's actually going to be the matrix w so this is where what i hope you can do is look at the notes at home and work through [Music] this exactly and see that this is actually the right answer but this is the way in which if you just have faith and think this is a just like single variable calculus except i've now got vectors and matrices the answer you get is actually what you expected to get because this is just like um the derivative of ax plus b with respect to x where it's a so similarly if we take the partial derivative with respect to b of w x plus b we get out the identity matrix okay then one other jacobian that we mentioned um while in the first lecture while working through word to vac is if you have the dot product of two um vectors i that's a number that what you get coming out of that it so that the partial derivative of u t h with respect to u is h transpose and at this point there's some fine print that i'm going to come back to in a minute so this is the correct jacobian right because in this case um we have the dimension of h inputs and we have one output and so we want to have a row vector um but there's a little bit more to say on that that i'll come back to in about 20 slides um but this is the correct jacobian okay so if you are not familiar with these kind of jacobians do please look at some of the notes that are available and try and compute these in more detail element wise and convince yourself that they really are right but i'm going to assume these now and show you what happens when we actually then work out gradients for at least a mini little neural net okay so here is most of this um neural net i mean as i commented um that you know really we'd be working out the partial derivative of the loss j with respect to these variables but for the example i'm doing here i just i've locked that off to keep it a little simpler and more manageable for the lecture and so we're going to just work out the partial derivative of the score s which is a real number with respect to the different parameters of this model where the parameters of this model are going to be the w and the b and the u and also the input because we can update the weight vectors of the the word vectors of different words based on tuning them to better predict um the classification outputs that we desire so let's start off with a fairly easy one where we want to update um the bias vector b to have our system um classify better so to be able to do that what we want to work out is the partial derivatives of s with respect to b so we know how to put that into our stochastic gradient update for the b parameters okay so how do we go about doing these things so the first step is we want to sort of break things up into different functions of minimal complexity that compose together so in particular this neural net layer h equals f of w x plus b it's still a little bit complex so let's decompose that one further step so um we have the input x we then calculate the linear transformation z equals wx plus b and then um we put things through the sort of element-wise non-linearity h equals f of z and then we do the dot product with u and you know it's useful for working these things out you know split into pieces like this have straight what your different variables are and to know what the dimensionality of each of these variables is it's well worth just writing out the dimensionality of every variable and making sure that the answers that you're computing are of the right dimensionality so at this point though what we can see is that calculating s is the product of three sorry is the composition of three functions around x so for working out the partials of s with respect to b um it's the composition of the three functions shown on the left and so therefore the gradient of s with respect to b we're going to take um the product of these three partial derivatives okay so how do what do we that so um so you know we've got the s equals u t h so that's the sort of the top um corresponding partial derivative partial derivative of h with respect to z partial derivative of z with respect um to b which is the first one that we're working out okay so we want to work this out and if we're lucky we remember those jacobians i showed previously about the jacobian for a vector dot product the jacobian for the non-linearity and the jacobian for the simple linear transformation and so we can use those so for the partials of s with respect to h well that's going to be ut using the first one the partials of h with respect to z okay so that's the non-linearity and so that's going to be the matrix that's the diagonal matrix with the element wise derivative f prime of z and 0 elsewhere and then for the w x plus b when we're taking the partials with respect to b that's just the identity matrix so we can simplify that down a little the identity matrix disappears and since u t is a vector and this is a diagonal matrix we can rewrite this as ut hadamard product of f prime of z i think this is the first time i've used this little circle for hadamard product but it it's something that you'll see quite a bit in your network work since it's often used so when we have two vectors ut and this vector here sometimes you want to do an element-wise product so the output of this will be a vector where you've taken the first element of each and multiplied then the second element of each and multiplied them etc downwards and so that's called the hadamard product and it's what we're calculating as to calculate a vector which is the gradient of s with respect to b okay so that's good so we now have a gradient of s with respect to b and we could use that in our stochastic gradient but we don't stop there we also want to work out the gradient with respect to others of our parameters so we might want to next go on and work out the gradient of s with respect to w well we can use the chain rule just like we did before right so we've got the same product of functions and everything is going to be the same apart from me now taking um the derivatives with respect to w rather than b um so it's now going to be um the partial of s with respect to h h with respect to z and z with respect to w and the important thing to notice here and this leads into what we do with the backpropagation algorithm is wait a minute this is very similar to what we've already done so when we're working out the gradients of s with respect to b the first two terms were exactly the same it's only the last one that differs so to be able to build um or to train neural networks efficiently this is what happens all the time and it's absolutely essential that we use an algorithm that avoids repeated computation and so the idea we're going to develop is when we have this equation stack that there's sort of stuff that's above um where we compute z and we're going to be sort of that'll be the same each time and we want to compute something from that that we can then sort of feed downwards when working out the gradients with respect to w x or b and so we do that by defining delta which is delta is the partials composed that are above the linear transform and that's referred to as the local error signal it's what's being passed in from above to the linear transform and we've already computed the gradient of that in the preceding slides and so the final form of the partial of s with respect to b will be um delta times the remaining part and well we've seen that you know for um partial of s with respect to b um the partial of z with respect to b is just the identity so the end result was delta but in this time we're then going to have to work out the partial of z with respect to w and multiply that by delta so that's the part that we still haven't yet done so um and this is where things get in some sense a little bit hairier and so there's something that's important to explain um so you know what should we have for the jacobian of um dsdw well that's a function that has one output the output is just a score a real number and then it has n by m inputs so the jacobian is um a 1 by n by m matrix i a very long row vector but um that's correct math but it turns out that that's kind of bad for our neural networks because remember what we want to do with our neural networks is do stochastic gradient descent and we want to say theta nu equals theta old minus a small multiplier times the gradient and well actually the w matrix is an n by m matrix and so we couldn't actually do the subtraction if this gradient we calculate is just a huge row vector we'd like to have it as the same shape as the w matrix in neural network land when we do this um we depart from pure math at this point and we use what we call the shape convention so what we're going to say is um and you're meant to use this for answers in the assignment that the shape of the gradient we're always going to make to be the shape of the parameters and so therefore um the sdw we're also going to represent as an n by m matrix just like w and we're going to reshape the jacobian to place it into this matrix shape okay so if we want to place it into this matrix shape what do we what are we going to want to get for the sdw well we know that it's going to involve delta our local error signal and then we have to work out something for dz dw um well since c equals wx plus b you'd kind of expect that the answer should be x um and that's right so the answer um to dsdw is um going to be delta transpose times x transpose and so the form that we're getting for this derivative is going to be the product of the local error signal at that's in comes from above versus what we calculate from the local input x so that shouldn't yet be obvious why that is true so let me just go through in a bit more detail why that's true so when we want to work out um d s d w right it's sort of delta times dz dw where um what that's computing for z is wx plus b so let's just consider for a moment what the derivative is with respect to a single weight w i j so w i j might be w 2 3 that's shown in my little neural network here and so the first thing to notice is that w i j only contributes to zi so it's going into z2 which then computes h2 and it has no effect whatsoever on h1 okay so when we're working out um dzi dw i j it's going to be d w i x that sort of row that row of the matrix plus bi which means um that for we've got a kind of a sum of w i k times x k and then for this sum this is like one variable calculus that when we're taking the derivative of this with respect to w i j every term and this sum is going to be zero the derivative is going to be zero except for the one that involves w i j and then the derivative of that is just like a x with respect to a it's going to be x so you get x j out as the answer and so the end result of that is that when we're working out what we want is the answer is that we're going to um get that these columns where x1 is all that's left x2 is all that's left through xm is all that's left and then that's multiplied by the vectors of the local error signal from above and what we want to compute is this outer product matrix where we're getting the different combinations of the delta and the x and so we can get the n by m matrix that we'd like to have by our shape convention by taking delta transpose which is n by 1 times x transpose which is then 1 by m and then we get this outer product matrix um so like that's the kind of a hacky argument that i've made it's certainly a way of doing things that the dimensions work out and it sort of makes sense um there's a more detailed run through this that appears in lecture notes um and i encourage you to sort of also look at the more matty version of that here's a little bit more information about um the shape convention so well first of all one um more example of this so when you're working out the sdb that that comes out as a it's jacobian is a row vector um but similarly you know according to shape convention we want our gradient to be the same shape as b and b is a column vector so that's sort of again they're different shapes and you have to transpose one to get the other and so effectively what we have is a disagreement between the jacobian form so the jacobian form makes sense for you know calculus and math because if you want to have it like i claimed that matrix calculus is just like single variable calculus apart from using vectors and matrices you can just multiply together the partials that only works out if you're using jacobians but on the other hand if you want to do stochastic gradient descent and be able to sort of subtract off a piece of the gradient that only works if you have the same shape matrix for the gradient as you do for the original matrix and so this is a bit confusing but that's just the reality there are both of these um two things so the jacobian form is useful in doing the um calculus but for the answers in the assignment we want the answers um to be presented using the shape convention so that the gradient is shown in the same shape as the parameters and therefore you'll be able to it's the right shape for doing a gradient update by just subtracting a small amount of the gradient so for working through things there are then basically two choices one choice is to work through all the math using jacobians and then right at the end um to reshape following the shape convention to give the answer so that's what i did when i worked out dsdb we worked through it using jacobians we got an answer but it turned out to be a row vector and so well then we have to transpose it at the end to get it into the right shape for the shape convention um the alternative is um to always follow the shape convention um and that's kind of what i did when i was then working out dsdw i didn't fully use jacobians i said oh well when we work out whatever was dz dw let's work out what shape we want it to be and what to fill in the cells with and if you're sort of trying to do it um immediately with the shape convention it's a little bit more hacky in a way since you know you have to look at the dimensions for what you want and figure out when to transpose or to reshape the matrix to be at the right shape but the kind of informal reasoning that i gave is what you do and what works and you know one way of and there are sort of hints that you can use right that you know that your gradient should always be the same shape as your parameters and you know that the error message coming in will always have the same dimensionality as that hidden layer and you can sort of work it out always following the shape convention okay um so that is hey doing this is all matrix calculus so after pausing for breath for a second the rest of the lecture is then okay let's look at how our software trains neural networks using what's referred to as the back propagation the back propagation algorithm um so the short answer is you know basically we've already done it the rest of the lecture is easy um so you know essentially i've just shown you what the back propagation algorithm does um so the back propagation algorithm is judiciously taking and propagating derivatives using the matrix chain rule the rest of the back propagation algorithm is to say okay when we have these neural networks we have a lot of shared structure and shared derivatives so what we want to do is maximally efficiently reuse derivatives of higher layers when we're computing derivatives for lower layers so that we minimize computation and i already pointed that out in the first half but we want to systematically exploit that and so the way we do that in our computational systems is they construct computation graphs um so this maybe looks a little bit like what you saw in a compiler's class if you did one right that you're creating i'm i call it here computation graph but it's really a tree right so you're creating here this tree of computations in this case but in more general case it's some kind of directed graph of computations which has source nodes which are inputs either inputs like x or input parameters like w and b and it's interior nodes are operations and so then once we've constructed a graph and so this graph corresponds to exactly the example i did before right this was our little neural net that's in the top right and here's the corresponding computation graph of computing w x plus b put it through the sigmoid non-linearity f multiply the resulting dot product of the resulting vector with u gives us our output score s um okay so what we do to compute this is we pass along the edges the results of operations so this is w x then z then h and then our output is s and so the first thing we want to be able to do to compute with neural networks is to be able to compute for different inputs what the output is and so that's referred to as forward propagation and so we simply run this expression much like you'd standardly do in a compiler to compute the value of s and that's the forward propagation phase but the essential additional element of neural networks is that we then also want to be able to send back gradients which will tell us how to update the parameters of the model and so it's this ability um to send back gradients which gives us the ability for these models to learn once we have a loss function at the end we can work out how to change the parameters of the model so that they more accurately produce the desired output i they minimize the loss and so it's doing that part that then is called back propagation so we then once we forward propagated a value with our current parameters we then um head backwards reversing the direction of the arrows and pass along gradients down to the different parameters like b and w and u that we can use to change using stochastic gradient to send what the value of b is of what the value of w is so we start off with dsds which is just one and then we run our back propagation and we're using the sort of same kind of composition of jacobian so we have the sdh here and the sdz and we progressively pass back those gradients so we just need to work out how to efficiently and cleanly do this in a computational system and so let's sort of work through again a few of these cases so the general situation is um we have a particular node so a node is where some kind of operation like multiplication or a non-linearity happens and so the simplest case is that we've got one output and one input so we'll do that first so that's like h equals f of z so what we have is an upstream gradient um dsdh and what we want to do is compute the downstream gradient of dsdz and the way we're going to do that is say well for this function f it's a function it's got a derivative a gradient so what we want to do is work out that local gradient dhdz and then that gives us everything that we need to work out the sdz because that's precisely we're going to use the chain rule we're going to say the dsdz equals the product of the sdh times the hdz where this is again using jacobians okay so the general principle that we're going to use is the downstream gradient equals the upstream gradient times the local gradient okay sometimes it gets a little bit more complicated so we might have multiple inputs to a function so this is the matrix vector multiply so z equals wx okay when there are multiple inputs we still have an upstream gradient dsdz but what we're going to do is work out a local gradient with respect to each input so we have dz dw and dzdx and so then at that point it's exactly the same for each piece of it we're going to work out the downstream gradients the sdw and the sdx by using the chain rule with respect to the particular local gradient so um let's go through an example of this i mean this is kind of a silly example it's not really an example that looks like a typical neural net but it's sort of a simple example where we can show some of the components of what we do so what we're going to do is want to calculate f of x y z which is being calculated as x plus y times the max of y and z um and we've got you know particular values that we're starting off with x equals one y equals two and z equals zero so these are the current values of our parameters and so we can say okay well we want to build an expression tree for that here's our expression tree we're taking x plus y we're taking the max of y and z and then we're multiplying them and so our forward propagation phase is just to run this so we take the values of our parameters and we simply start to compute with them right so we have one two two zero um and we add them as three the max is two we multiply them and that gives us six okay so then at that point we then want to go and work out um how to do things um for back propagation and how these back propagation steps work and so the first part of that is sort of working out what our local gradients are going to be um so um so this is a here and this is x and y so d a d x since a equals x plus y is just going to be one and d a d y is also going to be one um then um for b equals the max of y z um so this is this max node so the local gradients for that is um it's going to depend on y b whether y is greater than z so d b d y is going to be one if and only if y is greater than z which it is at our particular point here so that's one and db dz is going to be one only if z is greater than y so for our particular values here that one is going to be zero um and then finally here we're calculating the product f equals a b um so for that um we're going to um wait sorry that slides along perfect okay so for the product um the derivative of f with respect to a is equal to b which is two and the derivative of f with respect to b is a equals three so that gives us all of the local gradients at each node and so then to run backpropagation we start with dfdf which is just one and then we're going to work out the downstream equals the upstream times the local okay so the local so when you have a product like this um note that sort of the gradients flip so we take upstream times the local which is 2 oops so the downstream is 2 on this side dfdb is three so we're taking upstream times local that gives us three um and so that gives us back propagates values to um the plus and max nodes and so then we continue along so for the max node um the local gradient dbdy equals one so we're going to take upstream is three so we tend to take three times one and that gives us three d b d z is zero because of the fact that z's value is not the max um so we're taking three times zero and saying the gradient there is zero so finally doing the plus node um the local gradients for both x and y there are one so we're just getting two times one in both cases and we're saying that the gradients there are two okay and so again at the end of the day um the interpretation here is that this is giving is this information as to if we wiggle the values of x y and z how much of a difference does it make to the output what is the slope the gradient with respect to the variable so what we've seen is that since z isn't the max of y and z if i change the value of z a little like if i make z 0.1 or minus 0.1 it makes no difference at all to what i compute as the output so therefore the gradient there is zero if i change the value of x a little then that is going to have an effect and it's going to affect the output by twice as much as the amount i change it oops right so and that's because um the dfdz equals two um so interestingly um so i mean we can basically work that out so if we imagine um making sort of x 2.1 well then what we'd calculate the max is to [Music] oh sorry sorry if we make x 1.1 we then get the max here is 2 and we get 1.1 plus 2 as 3.1 so we get 3.1 times 2 so that'd be about 6.2 so changing x by 0.1 has added 0.2 to the value of f um conversely for the value of y we find that the df d y equals 5 so what we do when we've got two things coming out here as i'll go through again in a moment is we're summing the gradients so again three plus two equals five and empirically that's what happens so if we consider fiddling the value of y a little let's say we make it a value of 2.1 then the prediction is they'll have five times as bigger an effect on the output value we compute and well what do we compute so we compute 1 plus 2.1 so that's 3.1 and we compute the max of um 2.1 and 0 as 2.1 so we'll take the product of 2.1 and 3.1 and i calculate that in advance since i can't really do this arithmetic in my head and the product of those two is 6.51 so it has gone up about by 0.5 so we've multiplied my fiddly at by 0.1 by five times to work out the magnitude of the effect of the output okay so for this stuff you know before i did the case of you know when we had one oops one in and one out here and multiple ends and one out here the case that i hadn't actually dealt with is the case of when you have multiple outward branches but that then turned up in the computation of y so once you have multiple outward branches what you're doing is your summing so that when you want to work out the dfdy you've got a local gradient you've got two upstream gradients and you're working it out with respect to each of them as in the chain rule and then you're summing them together to work out the impact at the end right so we also saw some of the other node intuitions which it's useful to have um doing this so when you have an addition um that distributes the upstream gradient to each of the things below it when you have max it's like a routing node so when you have max you have the upstream gradient and it goes to one of the branches below it and the rest of them get no gradient um when you then have a multiplication it has this effect of switching the gradient so if you're taking three by two um the gradient on the two side is three and on the three side is two and if you think about in terms of how much effect you get from when you're doing this sort of wiggling that totally makes sense right because if you're multiplying another number by three then any change here is going to be multiplied by 3 and vice versa okay so that so this is the kind of computation graph that we want to use to work out derivatives in an automated computational fashion um which is the basis of the back propagation algorithm but at that point that you know this is what we're doing but there's still you know one mistake that we can make it would be wrong for us to sort of say okay well first of all we want to work out the sdb so look we can start up here we can propagate our upstream errors work out local gradients upstream error local gradient and keep all the way down and get the dsdb down here okay next we want to do it for dsdw let's just run it all over again because if we did that we'd be doing repeated computation as i showed in the first half that this term is the same both times this term is the same both times this term is the same both times that only the bits at the end differ so what we want to do is avoid duplicated computation and compute all the gradients um that we're going to need um successively so that we only do them once and so that was analogous when i introduced this delta variable when we computed gradients by hand so starting off here from d um we starting off here with dsds is one we then want to one time compute gradient in the green here one time compute the gradient in green here that's all common work then we're going to take the local gradient um for dz db and multiply that by the upstream gradient to work out dsdb and then we're going to take the same upstream gradient and then um work out the local gradient here um and then sort of propagate that down to give us the sdw so the end result is we want to sort of systematically work to forward computation forward in the graph and backward computation back propagation backward in the graph in a way that we do things efficiently so this is the general form of the algorithm which works for an arbitrary computation graph so at the end of the day we've got a single scalar output z and then we have inputs and parameters which compute z and so once we have this computation graph and i added in this funky extra arrow here to make it a more general computation graph well we can always say that we can work out a starting point something that doesn't depend on anything so in this case both of these bottom two nodes don't depend on anything else so we can start with them and we can start to compute forward we can compute values for all of these sort of second row from the bottom nodes and then we're able to compute um the third lens up so we can have a topological sort of the nodes based on the dependencies in this directed graph and we can compute the value of each node given some subset of its predecessors which it depends on and so doing that is referred to as the forward propagation phase and gives us a computation of the scalar output z using our current parameters and our current inputs and so then after that we run back propagation so for back propagation we initialize the output gradient dz dz as one and then we visit nodes in the reverse order of the topological sort and we compute the gradients downward and so our recipe is that for each node as we head down we're going to compute the gradient of the node with respect to its successes and the things that it feeds into and how we compute that gradient is using this chain rule that we've looked at so this is sort of the generalized form of the chain rule where we have multiple outputs and so we're summing over the different outputs and then for each output we're computing the product of the upstream gradient and the local gradient with respect to that node and so we head downwards and we continue down in the reverse topological sort order and we work out um the gradient with respect to each variable in this graph and so it hopefully looks um kind of intuitive looking at this picture that if you think of it like this the big oak complexity of forward propagation and backward propagation is the same right in both cases you're doing a linear pass through all of these nodes and calculating values given predecessors and then values given successes i mean you have to do a little bit more work is um for working out the gradients sort of as shown by this chain rule but it's the same big o complexity so if somehow you're implementing stuff for yourself rather than relying on the software and you're calculating the gradiences of a different order of complexity of forward propagation it means that you're doing something wrong you're doing repeated work that you shouldn't have to do okay so this algorithm works for a completely arbitrary computation graph any directed acyclic graph you can apply this algorithm in general what we find is that we build neural networks that have a regular layer structure so we have things like a vector of inputs and then that's multiplied by a matrix it's transformed into another vector which might be multiplied by another matrix or summed with another matrix or something right so once we're using that kind of regular layer structure we can then parallelize the computation by working out the gradients in terms of jacobians of vectors and matrices and do things in parallel much more efficiently okay so doing this is then referred to as automatic differentiation and so essentially um if you know the computation graph you should be able to have your compute clever computer system work out um what the derivatives of everything is and then apply back propagation um to work out how to update the parameters and learn and there's actually a sort of an interesting um sort of thing of how history has gone backwards here which i'll just note um so some of you might be um familiar with symbolic um computation packages so those are things like mathematica so mathematica you can give it a symbolic form of a computation and then it can work out derivatives for you so it should be the case that if you give a complete symbolic form of a computation graph um then it should be able to work out all the derivatives for you and you never have to work out a derivative by hand whatsoever and that was actually attempted in a famous um deep learning library called fianno which came out of joshua bendio's group at the university of montreal that it had a compiler that did that kind of symbolic manipulation um but you know somehow that sort of proved um a little bit too too hard a road to follow i imagine it actually might come back again in the future and so for modern deep learning frameworks which includes both tensorflow or pi torch they do 90 percent of um this computation of automatic differentiation for you but they don't actually symbolically compute derivatives so for each particular node or layer of your deep learning system somebody either you or the person who wrote that layer has hand-written the local derivatives but then everything from that point on the sort of the taking doing the chain rule of combining upstream gradients with local gradients to work out downstream gradients that's then all being done automatically for back propagation on the computation graph and so that what that means is for a whole neural network you have a computation graph and it's going to have a forward pass and a backward pass and so for the forward pass you're topologically sorting the nodes based on their dependencies in the computation graph and then for each node you're running forward the forward computation on that node and then for backward propagation you are reversing the topological sort of the graph and then for each node in the graph you're running the backward propagation which is the little bit of backdrop the chain rule at that node and then the result of doing that is you have gradients for your inputs and parameters and so this is the overall software runs this for you and so what you want to do is then actually have stuff for particular nodes or layers in the graph so if i have a multiply gate it's going to have a forward algorithm which just computes that the output is x times y in terms of the two inputs and then i'm going to want to compute to tell it also how to calculate the local derivative so i want to say what is the local derivative so dl dx and the ldy in terms of the upstream gradient dldz and so i will then manually work out how to calculate that and normally what i have to do is i assume the forward pass is being run first and i'm going to shove into some local variables for my class the values that were used in the forward computation so as well as computing z equals x times y i'm going to sort of remember what x and y were so that then when i'm asked to compute the backward pass i'm then going to have implemented here um what we saw earlier of um that when it's x y you're going to sort of swap the y and the x um to work out the local gradients and so then i'm going to multiply those by the upstream gradient and i'm going to return i've just written it here as a sort of a little list but really it's going to be a numpy vector of the gradients okay um so that's um 98 of what i wanted to cover um today just um a couple of quick comments um left so um that can and should all be automated sometimes you want to just check if you're computing the right gradients and so the standard way of checking that you're computing the right gradients is to manually work out the gradient by doing a numeric calculation of the gradient and so um you can do that so you can work out what the derivative of x of f with respect to x should be by choosing some sort of small number like 10 to the minus 4 adding it to x subtracting it from x and then so the difference between these numbers is 2h dividing it through by 2h and you're simply working out the rise over the run which is the slope of that point with respect to x and that's an approximation of the gradient of f with respect to x at that value of x so this is so simple you can't make a mistake implementing that and so therefore you can use this to check um where your whether your gradient values are correct or not um this isn't something that you'd want to use much um because not only is it approximate but it's extremely slow because to work this out you have to run the forward computation for every parameter of the model so if you have a model with a million parameters you're now doing a million times as much work to run back prop as as you would do if you're actually using calculus so calculus is a good thing to know but it can be really useful to check that the right values are being calculated in the old days when we hand wrote everything this was kind of the key unit test that people used everywhere these days most of the time you're reusing layers that are built into pie torch or some other deep learning framework so it's much less needed but sometimes you're implementing your own layer and you really do want to check that things are implemented correctly there's a fine point in the way this is written if you saw this in sort of high school calculus class you would have seen rise over run of f of x plus h minus f of x divided by h it turns out that doing this two-sided estimate like this is much much more accurate than doing a one-sided estimate and so you're really much encouraged to use this approximation okay so at that point um we've mastered the core technology of neural nets um backpropagation is recursively and hence efficiently applying the chain rule along the computation graph with this sort of key step that downstream gradient equals upstream abstract the upstream gradient times local gradient and so for calculating with neural nets we do the forward pass to work out values with current parameters then run back propagation and work out the gradient of the loss and currently computed loss with respect to those parameters now to some extent um you know with modern deep learning frameworks you don't actually have to know how to do any of this right it's like it's the same as you don't have to know how to implement a c compiler you can just write c code and say gcc and it'll compile it and it'll run um the right stuff for you um and that's the kind of functionality you get from the pytorch framework so do come along to the pie torch tutorial this friday and get a sense about how easy it is to write new networks um using a framework like pytorch or tensorflow and you know it's so easy that's why you know high school students across the nation are now doing their science projects training deep learning systems because you don't actually have to understand very much debunk a few neural network layers together and set it computing on some data but you know we hope in this class that you actually are also learning how these things are implemented um so you have a deeper understanding of than that and you know it turns out that sometimes you need to have a deeper understanding so back propagation doesn't always work carefully perfectly and so understanding what it's really doing can be crucial to debugging things and so we'll actually see an example of that fairly soon when we start looking at recurrent models and some of the problems that they have which will require us to think a bit more deeply about what's happening in our gradient computations okay that's it for today 
","['', 'neural net learning', 'gradients for training neural networks', 'back propagation algorithm', 'assignment one', 'assignment two', 'named entity recognition', 'logistic classifier', 'word vectors', 'context window', 'softmax classifier', 'neural network layer', 'hidden vector', 'word embedding', 'negative sampling', 'chain rule', 'computational graph', 'backpropagation algorithm', 'pytorch', 'deep learning framework', '']"
"okay so for today um we're actually gonna take a bit of a change of pace from what the last couple of lectures or have been about and we're going to focus much more on linguistics and natural language processing and so in particular we're going to start looking at the topic of dependency parsing and so this is the plan of what to go about through today so i'm going to start out by going through some ideas that have been used with the syntactic structure of languages of constituency and dependency and introduce those and then focusing in more on dependency structure i'm then going to look at dependency grammars and dependency tree banks and then having done that we're then going to move back into thinking about how to build natural language processing systems and so i'm going to introduce the idea of transition based dependency parsing and then in particular having developed that idea i'm going to talk about a way to build a simple but highly effective neural dependency parser and so this simple highly effective neural dependency parser is essentially what we'll be asking you to build um in the third assignment so in some sense we're getting a little bit ahead of ourselves here because in week two of the class um we teach you how to do both assignments two and three um but all of this material will come in really useful before i get underway just a couple of announcements so if for a site again for assignment two you don't yet need to use the pi torch framework but now's a good time to work on getting pie torch installed for your python programming assignment three is in part also an introduction to using pytorch it's got a lot of scaffolding included in the assignment um but beyond that um this friday we've got a pie torch tutorial and thoroughly encourage you um to come along to that as well look for it under the zoom tab and um in the in the second half of the thursday of week four we have an explicit class that um partly focuses on the final projects and what the choices are for those but it's never too late to start thinking about the final project and what kind of things you want to do for the final project um so do um come meet with people there are so resources on the course pages about what different tas know about i've also talked to a number of people about final projects but clearly i can't talk to everybody so i encourage you to also be thinking about what you want to do for final projects okay so what i wanted to do today was introduce how people think about the structure of sentences um and to put structure on top of them to explain how human language conveys meaning and so our starting point for meaning and essentially what we've dealt with with word vectors up until now is we have words and words are obviously an important part of the meaning of human languages but for words in human languages um there's more that we can do with them in thinking about how to structure sentences so in particular the first most basic way that we think about words when we thinking about how sentences are structured is we give to them what's called a part of speech we can say that cat is a noun by is a preposition door is another noun cuddly is an adjective and then for the word the um if it was given a different part of speech if you saw any parts of speech in school it was probably you're told it was an article sometimes that is just put into the class of adjectives in modern linguistics and what you'll see in the resources that we use words like that are referred to as determiners and the idea is that there's a bunch of words includes art and but also other words like this and that um or even every which are words sort of occur at the beginning of something like the cuddly cat which have a determinative function of sort of picking out which cats that they're referring to and so we refer to those as determiners but it's not the case that when we want to communicate with language that we just have this word salad where we say a bunch of words we just say you know whatever leaking kitchen tap and um let the other person put it together we put words together in a particular way to express meanings and so therefore languages have larger units of putting meaning together and the question is how we represent and think about those now in modern work um in particular in modern united states linguistics or even what you see in computer science classes when thinking about formal languages the most common way to approach this is with the idea of context-free grammars which you see at least a little bit of in 103 if you've done 103 what a linguist would often refer to as free structure grammars and the idea there is to say well there are bigger units in languages that we refer to as phrases so something like the cuddly cat is a cat um with some other words modifying it and so we'll refer to that as a noun phrase um but then we have ways in which phrases can get larger um by building things and side phrases so the door here is also a noun phrase um but then we can build something bigger around it with the prepositions as a preposition and then we have a prepositional phrase and in general we can keep going so we can then make something like the cuddly cat by the door and then the door is a noun phrase the cuddly cat is a noun phrase by the door as a prepositional phrase but then when we put it all together the whole of this thing becomes a bigger noun phrase and so it's working with these ideas of nested phrases what in context-free grammar terms you'd refer to as non-terminals so noun phrase and prepositional phrase would be non-terminals in the context-free grammar we can build up a bigger structure of human languages so let's just do that for a little bit um to review what happens here so we start off saying okay you can say the cat and the dog and so those are noun phrases and so we want a rule that can explain those so we could say a noun phrase goes to determiner noun and then somewhere over the side we'd have a lexicon and in our lexicon we'd say that dog is a noun and cat is a noun and is a determiner and that is a determiner okay so then we notice you can do a bit more than that so you can say things like the large cat a barking dog so that suggests we can have a noun phrase after the determiner there can optionally be an adjective and then there's the noun and that can explain some things we can say but we can also say the cat by the door or a barking dog in a crate and so we can also put a prepositional phrase at the end and that's optional but you can combine it together with an adjective for the example i gave like a barking dog on the table and so that this grammar can handle that um so then we'll keep on and say um well actually you can use multiple adjectives so you can say a large barking dog or a large barking cuddly cat no maybe not well sentences like that so we have any number of adjectives which we can represent with a star what's referred to as the cleany star so that's good um oh but i forgot a bit actually um for by the door i have to have a rule for producing by the door so i also need a rule that's a prepositional phrase goes to a preposition followed by a noun phrase and so then i also have to have prepositions and that can be in or on or by okay and i can make other sentences of course with this as well like the large crate on the table or something like that or the large crate on the large table okay so i chug along and then well i could have something like talk to the cat and so now i need more stuff so talk is a verb and two is still looks like a preposition so i need to be able to make up something um with that as well okay so what i can do is say i can also have a rule for a verb phrase that goes to a verb and then after that for something like talk to the cat that it can take a prepositional phrase after it and then i can say that the verb goes to talk or walked um okay then i can pause then i can cover those sentences oops um okay so that's that's the end of what i have here um so in this sort of a way i'm hand writing a grammar so here is now i have this grammar and a lexicon and for the examples that i've written i've written down here um this grammar and this lexicon is sufficient um to pass these sort of fragments of showing expansion that i just wrote down i mean of course there's a lot more to english than what you see here right so if i have something like you know the cat walked behind the dog then i need some more grammar rules so it seems then i need a rule that says i can have a sentence that goes to a noun phrase followed by a verb phrase um and i can keep on um doing things of this sort um that's the um one question that um ruthanne asked was about um what the brackets mean and is the first np different from the second um so for this notation on the brackets here i mean this is actually a a common notation that's used in linguistics um it's sort of in some sense a little bit different to traditional computer science notation since the star is used in both to mean zero or more of something so you could have zero one two three four five adjectives somehow it's usual in linguistics that when you're using the star you also put parentheses around it um to mean it's optional so sort of parentheses and star are used together to mean any number of something when it's parentheses just by themselves that's then meaning zero or one um and then four um are these two noun phrases different no they're both noun phrase rules and so in our grammar we can have multiple rules that expand noun phrase in different ways um but you know actually in my example here my second rule because i wrote it quite generally it actually covers the first rule as well so actually at that point i can cross out this first rule because i don't actually need it in my grammar but in general you know you haven't a choice between writing multiple rules for noun phrase goes to categories which effectively gives you a disjunction or working out by various syntactic conventions how to compress them together okay um so that was what gets referred to in natural language processing as constituency grammars um where the standard form of constituency grammar is a context free grammar of the sort that i trust you saw at least a teeny bit of either in cs 103 or something like a programming languages compilers formal languages class there are other forms of grammars that also pick out constituency there are things like tria joining grammars but i'm i'm not going to really talk about any of those now what i actually want to present is a somewhat different way of looking at grammar which is referred to as um dependency grammar which puts a dependency structure over sentences now actually it's not that these two ways of looking at grammar have nothing to do with each other i mean there's a whole um formal theory about the relationships between different kinds of grammars and you can very precisely state relationships um and isomorphisms between different grammars of different kinds but on the surface these two kinds of grammars look sort of different and emphasize different things and for reasons of this sort of closeness to picking out relationships and sentences and their ease of use it turns out that in modern natural language processing starting i guess around 2000 so really in the last 20 years nlp people have really swung behind dependency grammars so if you look around now where people are using grammars in nlp by far the most common thing that's being used is dependency grammars um so i'm going to teach us today a bit about those and for what we're going to build um in assignment three is building using supervised learning and neural dependency parser so the idea of dependency grammar is that when we have a sentence what we're going to do is we're going to say for each word what other words um modify it so what we're going to do is when we say the large crate we're going to say okay well large is modifying crate and that is modifying crate in the kitchen that is modifying kitchen by the door that is modifying door and so i'm showing a modification that depends a dependency or an attachment relationship by drawing an arrow from the head to what's referred to in dependency grammar as the dependent the thing that modifies further specifies or attaches to the head um okay so that's the start of this um well another dependency um that is that well um in look in the large crate that where you're looking is in the large crate so you're going to want to have the large in the large crate as being a dependent of look and so that's also going to be a dependency relationship here um and then there's one final bit um that might seem a little bit confusing to people and that's actually when we have these prepositions um there are two ways that you can think that this might work so if it was something like look in the crate it seems like that is a dependent of crate but you could think that you want to say look in and it's in the crate and give this dependency relationship with the sort of preposition as sort of thinking of it as the head of what was before our prepositional phrase and that's a possible strategy in the dependency grammar but what i'm going to show you today and what you're going to use in the assignment is dependency grammars that follow the representation of universal dependencies and universal dependencies is a framework which actually is involved in creating which was set up to try and give a common dependency grammar over many different human languages and in the design decisions that were made in the context of designing universal dependencies what we decided was that for what in some languages you use prepositions lots of other languages make much more use of case markings so if you've seen something like german you've seen more case markings like genitive and dative cases and in other languages like latin or finnish lots of native american languages you have many more case markings again which cover most of the role of prepositions um so in universal dependencies essentially in the crate is treated like a case marked noun and so what we say is that the in is also a dependent of crate and then you're looking in the crate um so in the structure we adopt um in as a dependent of crate this inn is a dependent of kitchen this buy is a dependent of door and then we have these prepositional phrases in the kitchen by the door and we want to work out well what they modify well in the kitchen is modifying crate right because it's a crate in the kitchen so we're going to say that it's this piece is a dependent of crate um and then well what about by the door well it's not really meaning that's a kitchen by the door um and it's not meaning to look by the door again it's a crate by the door and so what we're going to have is the crate also has door as a dependent and so that gives us our full dependency structure of this sentence okay and so that's a teeny introduction to syntactic structure i'm going to say a bit more about it and give a few more examples but let me just for a moment sort of say a little bit about well why are we interested in syntactic structure why do we need to know the structure of sentences and this gets into how does human languages work so human languages can can communicate very complex ideas i mean in fact you know anything that humans know how to communicate to one another they communicate pretty much by using words so we can structure and communicate very complex ideas but we can't um communicate a really complex idea by one word we can't just you know choose a word like you know empathy and say it with a lot of meaning and say empathy and the other person is meant to understand everything about what that means right we have to compose a complex meaning that explains things by putting words together into bigger units and the syntax of a language allows us to put words together into bigger units um where we can build up and convey to other people a complex meaning and so then the listener doesn't get this syntactic structure right the syntactic structure of the sentence is hidden from the listener all the listener gets is a sequence of words one after another bang bang bang so the listener has to be able to do what i was just trying to do in this example that as the sequence of words comes in that the listener works out which words modify which other words and therefore can construct the structure of the sentence and hence the meaning of the sentence and so in the same way if we want to build clever neural net models that can understand the meaning of sentences those clever neural net models also have to understand what is this structure of the sentence so that they can interpret the language correctly and we'll go through some examples and see more of that okay so the fundamental point that we're going to sort of spend a bit more time on is that these choices of how you build up the structure of a language change the interpretation of the language and a human listener or equally a natural language understanding program has to make in a sort of probabilistic fashion choices as to which words modify i depend upon which other words so that they're coming up with the interpretation of the sentence that they think was intended by the person who said it okay so um to get a sense of this and how sentence structure is um interesting and difficult what i'm going to go through now is a few examples of different ambiguities that you find in natural language and i've got some funny examples some newspaper headlines but these are all real natural language ambiguities that you find throughout um natural language well at this point i should say this is where i'm being guilty of saying language but i'm meaning in english some of these ambiguities you find in lots of other languages as well but which ambiguities for syntactic structure partly depend on the details of the language so different languages have different syntactic instructions different word orders different amounts of word having different forms of words like case markings and so depending on those details there might be different ambiguities so here's one ambiguity which is one of the commonest ambiguities in english so san jose cops kill man with knife so this sentence has two meanings um either it's the san jose cops um who are killing a man and they're killing a man with a knife um and so that corresponds to a dependency structure where the san jose cops are the subject of killing the man is the object of killing and then the knife is then the instrument with which they're doing the killing so that the knife is an oblique modifier for the instrument of killing and so that's one possible structure for this sentence um but it's probably not the right one um so what it actually probably was was that it was a man with a knife and the san jose cops killed the man so that corresponds to the knife then being a noun modifier of the man and then kill is still killing the man so the man is the object of killing and the cops are still the subject and so whenever you have a prepositional phrase like this that's coming further on in a sentence there's a choice of how to interpret it it could be either interpreted as modifying a noun phrase that comes before it or it can be interpreted as modifying a verb that comes before it so systematically in english you get these prepositional phrase attachment ambiguities throughout all of our sentences but um you know to give two further observations on that you know the first observation is you know you encounter sentences um with prepositional phrase attachment um ambiguities every time you read a newspaper article every time you talk to somebody but most of the time you never notice them and that's because our human brains are incredibly good at considering the possible interpretations and going with the one that makes sense according to context the second comment as i said different human languages expose different ambiguities so for example this is an ambiguity that you normally don't get in chinese because in chinese prepositional phrases modifying a verb are normally placed before the verb and so therefore you don't standardly get this ambiguity but you know there are different other ambiguities that you find commonly in chinese sentences okay so this ambiguity you find everywhere because prepositional phrases are really common at the right ends of sentences so here's another one um scientists count whales from space so that gives us these two possible interpretations that there are whales from space and scientists are counting them um and then the other one is how the scientists are counting the whales is that they're counting them from space and they're using satellites to count the sales which is the correct interpretation um that the newspaper hopes that you're getting um and this problem gets much much more complex because many sentences in english have prepositional phrases all over the place um so here's the kind of boring sentence that you find in the financial you news the board approved its acquisition by royal trusco limited of toronto for 27 dollars a share at its monthly meeting and well if you look at the structure of this sentence what we find is you know here's a verb then here's the object noun phrase so we've got the object noun phrase here and then after that what do we find well we find a prepositional phrase another prepositional phrase another prepositional phrase and another prepositional phrase and how to attach each of these is then ambiguous so the basic rule of how you can attach them is you can attach them to things to the left providing you don't create crossing attachments so in principle by royal trusco limited could be attached to either approved or acquisition but in this case by royal trusco limited is that it's the acquirer so it's um a modifier of the acquisition okay so then we have of toronto so of toronto could be modifying raw trusco limited it could be modifying the acquisition or it can be modifying the approved and in this case the of toronto is telling you more about the company and so it's a modifier of royal trusco limited okay so then the next one is for 27 a share and that could be modifying toronto royal trusca limited the acquisition or the approving and well in this case um that's talking about the price of the acquisition so this one is mod go jumps back and this is now a prepositional phrase that's modifying the acquisition and then at the end at its um monthly meeting um well that's where the approval is happening um by the ver by the board so rather than any of these preceding four noun phrases at its monthly meeting is modifying the approval um and so it attaches right back there and this example is kind of too big and so i couldn't fit it in one line but as i think maybe you can see that you know none of these dependencies cross each other and they connect at different places ambiguously so because we can chain these prepositions like this and attach them at different places like this um human language sentences are actually extremely ambiguous um so the number if you have a sentence with um k prepositional phrases um at the end of it where here we have k equals four um the number of parses this sentence has the number of different ways you can make these attachments is given by the catalan numbers so the catalan numbers are an exponentially growing series which arises in many tree-like contexts so if you're doing something like triangulations of a polygon you get catalan numbers if you're doing triangulation and graphical models in cs228 you get catalan numbers but we don't need to worry about the details here the central point is this is an exponential series and so you're getting an exponential number of parsers in terms of the number of prepositional phrases um and so in general you know the number of parsers human languages have is exponential in their length which is kind of bad news um because if you're then trying to enumerate all the parsers it you might fear that you really have to do a ton of work the thing to notice about structures like these prepositional phrase attachment ambiguities is that there's nothing that resolves these ambiguities in terms of the structure of the sentence so if you've done something like looked at the kind of grammars that are used in compilers that the grammars used in compiling compilers for programming languages are mainly made to be unambiguous and to the extent that there are any ambiguities there are default rules that are used to say choose this one particular parse tree for your piece of a programming language and human languages just aren't like that they're globally ambiguous and the listening human is just meant to be smart enough to figure out what was intended so the analogy would be that you know in programming languages um when you're working out what does an else clause modify well you've got the answer that you can either look at um the curly braces to work out what the else clause modifies or if you're using python you look at the indentation and it tells you what the else clause modifies where by contrast for human languages um the it would be um just write down else something it doesn't matter how you do it you don't need parentheses you don't need indentation the human being will just figure out what the else clause is meant to pair up with okay lots of other forms of ambiguities in human languages so let's look at a few others another one that's very common over all sorts of languages is coordination scope ambiguities so here's a sentence shuttle veteran and long time nasa executive fred gregory appointed the board well this is an ambiguous sentence um there are two possible readings of this one reading is that there are two people there's a shuttle veteran and there's a long time nasa executive fred gregory and they were both appointed to the board two people and the other possibility is there's um someone named fred gregory who's a shuttle veteran and long-time lesser executive and they're appointed to the verb one person and these two interpretations again correspond to having different path structures so in one structure we've got a coordination of the shuttle veteran and the long time nasa executive fred gregory coordinated together in one case these are coordinated and then fred gregory specifies the name of the nasa executive um so it's then um specifying um who that executive is where the what in the other one the shuttle veteran and long time nasa executive all together is then something that is a modifier of um fred gregory okay so one time this is the unit that modifies fred gregory in the other one up here just long time nasa executive modifies fred gregory and then that's conjoined together with the shuttle veteran and so that also gives different interpretations um so this is a slightly reduced example of the i mean in um newspaper headlines tend to be more ambiguous than many other pieces of text because they're written in this shortened form to get things to fit and this is an especially shortened form where it's actually left out in the explicit conjunction but this headline says doctor no heart cognitive issues and this was after i guess one of trump it was after trump's first physical and while this was an ambiguity because there are two ways that you can read this you can either read this as saying doctor no heart and cognitive issues which gives you one interpretation instead of that the way we should read it um is that it's heart or cognitive and so then saying no heart or cognitive issues and we have a different narrower scope of the coordination and then we get a different reading okay um i want to give a couple more examples of different kinds of ambiguities another one you see quite a bit is when you have modifiers that are adjectives and adverbs um that there are different ways that you can have things modifying other things um this example is a little bit not safe for work but here goes students get first-hand job experience so this is an ambiguous sentence and again we can think of it as a syntactic ambiguity in terms of which things modify which other things so the nice polite way to render this sentence is that first is modifying hands so we've got first hand um it's job experience so job is a compound noun modifying experience and it's first-hand experience so ex first hand is then modifying experience and then get is the object of sorry first-hand job experience is the object of get and the students are the subject of get but if you have a smuttier mind you can interpret this a different way and in the alternative interpretation you then have hand going together with job um and the the first is then a modifier of experience um and job is still a modifier of experience and so then you get this different path structure and different interpretation there okay one more example in a way this example is similar to the previous one it's sort of having modifier pieces that can modify different things but rather than just being with individual adjectives or individual adverbs is then much larger units such as verb phrases can often have attachment ambiguities so this sentence headline is mutilated body washes up on rio beach to be used for olympic speech volleyball so we have this big verb phrase here of to be used for olympics beach volleyball and then again we have this attachment decision um that we could either say um that that big verb phrase is modifying i is attached to the rio beach or um we could say no no the to be used for olympic speech body volleyball that that is modifying the mutilated body and it's a body that's to be used for the olympic speech volleyball um which gives the funny reading yeah so i hope that's given you at least a little bit of a sense of how human language syntactic structure is complex ambiguous and to work out the intended interpretations you need to know something about that structure in terms of how much you need to understand i mean you know this isn't a linguistics class if you'd like to learn more about human language structure you can go off and do a syntax class but you know we're not really going to spend a lot of time working through language structure but there will be some questions on this in the assignment and so we're expecting that you can be at the level that you can have sort of some intuitions as to which words and phrases are modifying other words and phrases and therefore you could choose between two dependency analyses which one's correct okay um i've spent quite a bit of time on that um so better um keep going okay so the general idea is that knowing this sort of syntactic structure of a sentence can help us with semantic interpretation i mean as well as just generally saying we can understand language it's also used in many cases for simple practical forms of semantic extraction so people such as in biomedical informatics often want to get out particular relations such as protein protein interactions and well here's a sentence the results demonstrated that chi c interacts rhythmically with sas a chi a and kai b um and commonly that people can get out those kind of relationships by looking at patterns of dependency relations with particular verbs so for the interacts verb if you have a pattern of something being the subject and something else being the noun modifier of interacts well that's an interaction relationship but it gets a bit more complicated than that as in this example because often there are conjunctions so you also want to have another pattern where you have also interactions between the subject and the noun modifiers conjunct which will allow us to also find the chi a and kb examples okay um so i've sort of given an informal tour of dependency grammar to just try and uh quickly say a little bit more about formally what a dependency grammar is so in dependency syntax what we say is that the syntactic structure of a sentence consists of relations between pairs of words and it's a binary asymmetric relation i we draw arrows between pairs of words which we call dependencies now normally dependency grammars then type those grammatical relation type those arrows to express what kind of relation that there is and so that they have some kind of taxonomy of grammatical relation so we might have a subject grammatical relation a verbal auxiliary grammatical relation an oblique modifier grammatical relation we have some kind of typology of grammatical relations so and we refer to the arrow as going between the head is the head here and something that is a dependent of it so the subject of a verb is the dependent of the verb or when you have a noun modifier like our sort of cuddly cat we say that um cuddly is a dependent of cat and so cat is the head of cuddly cat and so normally dependencies like in these examples form a tree which is formal it so it's not just any graph with arrows we have an and graph which is connected a cyclic and has a single root so here's the root of the graph and so that gives us a dependency tree analysis dependency grammars have a really really long history so the famous first linguist um was panini um who wrote about the structure of sanskrit um and mainly he worked on the sound system of sanskrit and how sounds change in various contexts which what linguists call phonology and the different forms of sanskrit words sanskrit has rich morphology of inflecting nouns and verbs for different cases and forms but he also worked a little on the syntactic structure of sanskrit sentences and essentially what he proposed was a dependency grammar over sanskrit sentences and it turns out that sort of for most of recorded history when then when people have then um gone on and tried to put structures over human sentences um what they have used is dependency grammars um so there was a lot of work in the first millennium by arabic grammarians of trying to work out the grammar um the structure of sentences and effectively what they used was you know akinned what i've just presented as a dependency grammar so compared to you know 2500 years of history the ideas of having context-free grammars and having constituency grammars is actually a really really recent invention so it was really sort of in the middle of the 20th century that the ideas of um constituency grammar and context free grammars were developed first by wells in the 40s and then by noam chomsky in the early 50s leading to things like the chomsky hierarchy that you might see um cs103 or a formal languages class um so for modern work on dependency grammar using kind of the terminology and um notation that i've just introduced that's normally attributed to lucianteniere who was a french linguist um in around the sort of middle of the 20th century as well dependency grammar was widely used in the 20th century um in a number of places i mean in particular it tends to be sort of much more natural and easier to think about for languages that have a lot of different case markings or nouns like nominative accusative genitive data of instrumental kind of cases like you get in a language like latin or russian and a lot of those languages have much freer word order than english so the subject or object of you know in english the subject has to be before the verb the object has to be after the verb but lots of other languages have much freer word order and instead use different forms of nouns to show you what's the subject or the object of the sentence and dependency grammars can often seem much more natural for those kinds of languages dependency grammars were also prominent at the very beginnings of computational linguistics so one of the first people working computational linguistics in the us was david hayes so the professional society for computational linguistics is called the association for computational linguistics and he was actually one of the founders of the association for computational linguistics and he published in the early 1960s um an early perhaps the first dependency grammar paused sorry dependency parser okay um yeah a little teeny note just in case you see other things when when you have these arrows um you can draw them in either direction you can either draw arrows from the head or to the dependent or from the dependent to the head and actually different people have done one and the other right so the way tenure drew them was to draw them from the head to the dependent and we're following that convention but you know if you're looking at something that somebody else has written um with dependency arrows the first thing you have to work out is are they using the arrow heads at the heads or the dependents um now and not one other thing here is that we a sentence is seen as having the overall head word of the sentence which every other word of the sentence hangs off it's a common convention to add this sort of fake root to every sentence that then points um to the head word of the whole sentence here completed that just tends to make the algorithmic stuff easier because then you can say that every word of the sentence is dependent on precisely one other node where what you can be dependent on is either another word on the sentence or the fake root of the sentence and when we build our parsers we will introduce that fake root okay so um that's sort of dependency grammars and dependency structure i now want to get us back to natural language processing and starting to build parses for dependency grammars but before doing that i just want to say yeah where do we get our data from and that's actually an interesting story in some sense so the answer to that is well what we do is get human beings commonly linguists or other people who are actually interested in the structure of human sentences and we get them to sit around and hand pass sentences and give them dependency structures and we collect a lot of those parsers and we call that a tree bank and so um this is something that really only started happening in the late 80s and took off in a bigger way in the 90s until then no one had attempted to build tree banks lots of people had attempted to build parsers and it seemed like well if you want to build a parser the efficient way to do it is to start writing a grammar so you start writing some grammar rules and you start writing a lexicon with words and parts of speech and you sit around working on your grammar when i was a phd student one of my first summer jobs was spending the summer handwriting a grammar and it sort of seems like writing a grammar is more efficient because you're writing this one general thing that tells you the structure of a human language um but there's just been this massive sea change partly driven by the adoption of machine learning techniques where it's now seen as axiomatic that the way to make progress is to have annotated data namely here a tree bank that shows you the structure of sentences and so what i'm showing here is a teeny extract um from a universal dependencies tree bank and so that's what i mentioned earlier that this has been this effort to try and have a common dependency grammar representation that you can apply to lots of different human languages and so you can go over to this url and see that there's about 60 different languages at the moment which have universal dependencies tree banks um so why are tree banks good i mean it sort of seems like it's bad news if you have to have people sitting around for weeks and months hand passing sentences it seems a lot slower and actually a lot less useful than having somebody writing a grammar which just has um you know a much bigger multiplier factor in the utility of their effort it turns out that although that feel initial feeling seems sort of valid that in practice there's just a lot more you can do with the tree bank so why are tree banks um great you know one reason is the tree banks are highly reusable so typically when people have written grammars they've written grammars for you know one particular parser and the only thing it was ever used in is that one particular parser but when you build a tree bank that's just a use a useful data resource and people use it for all kinds of things so the well-known tree banks have been used by hundreds and hundreds of people and although all tree banks were initially built for the purposes of hey let's help natural language processing systems it turns out that people have actually been able to do lots of other things with tree banks so for example these days psycholinguists commonly use tree banks to get various kinds of statistics about data for thinking about psycholinguistic models linguists use tree banks for looking at patterns of different syntactic constructions that occur um that there's just been a lot of reuse of this data for all kinds of purposes but they have other advantages that i mentioned here you know when people are just sitting around saying oh what sentences are good they tend to only think of the core of language where lots of weird things happen in language and so if you actually just have some sentences and you have to go off and pass them then you actually have to deal with the totality of language um since you're parsing actual sentences you get statistics so you naturally get the kind of statistics that are useful to machine learning systems by constructing a tree bank where you don't get them for free if you hand write a grammar but then a final way which is perhaps the most important of all is you if you actually want to be able to do um science of building systems you need a way to evaluate these nlp systems i mean it seems hard to believe now that you know back in the 90s and 80s when people built nlp parsers it was literally the case um that the way they were evaluated was you said to your friend oh i've built this parser type in a sentence on the terminal and see what it gives you back it's pretty good hey um and that was just the way business was done um whereas what we'd like to know is well as i showed you earlier english sentences can have lots of different parsers commonly can this system choose the right parses for particular sentences and therefore have the basis of interpreting them as a human being would and while we can only systematically do that evaluation if we have a whole bunch of sentences that have been hand passed by humans with their correct interpretations so the rise of tree banks turned parser building into an empirical science where people could then compete rigorously on the basis of look my parser has two percent higher accuracy than your parser in choosing the correct parsers for sentences okay so well how do we build a parser once we've got dependencies so there's sort of a bunch of sources of information that you could hope to use so one source of information is looking at the words on either end of the dependency so discussing issues that seems a reasonable thing to say and so it's likely that issues could be the object of discussing whereas if it was some other word right if you're thinking of making you know outstanding the object of discussion discussing outstanding that doesn't sound right so that wouldn't be so good a second source of information is distance so most dependencies are relatively short distance some of them aren't some of long distance dependencies but they're relatively rare the vast majority of dependencies are nearby and another source of information is the intervening material so there are certain things that dependencies um rarely span so clauses and sentences are normally organized around verbs and so dependencies rarely span across intervening verbs we can also use punctuation and written language things like commas which can give some indication of the structure and so punctuation may also indicate bad places to have long distance dependencies over and there's one final source of information which is what's referred to as valency which is for a head what kind of information does it usually have around it so if you have a noun there are things that you just know about what kinds of dependents nouns normally have so it's common that it will have a determiner to the left the cat on the other hand um it's not going to be the case that there's a determiner to the right cat the that's just not what you get in english on the left you're also likely to have an adjectival modifier that's where he had cuddly but again it's not so likely you're going to have the adjectival modifier over on the right for cuddly so there are sort of facts about what things different kinds of words take on the left and the right and so that's the valency of the heads and that's also a useful source of information okay so what do we need to do using that information to build a parser well effectively what we do is have a sentence i'll give a talk tomorrow on your networks and what we have to do is say for every word in that sentence we have to choose some other word that it's a dependent of where one possibility is it's a dependent of root so we're giving it a structure where we're saying okay for this word i've decided that it's a dependent on on networks and then for this word it's also dependent on networks um and for this word it's a dependent on give so we're choosing um one for each word and there are usually a few constraints so only one word is a dependent of root we have a tree we don't want cycles so we don't want to say that word a is dependent on word b and word b is dependent on word a and then there's one final issue um which is um whether arrows can cross or not so in in this particular sentence we actually have these crossing dependencies you can see there i'll give a talk tomorrow on neural networks and this is the correct dependency parse for this sentence because what we have here is that it's a talk and it's a talk on neural network so the on neural networks modifies the talk um but which leads to these crossing dependencies i didn't have to say it like that i could have said i'll give a talk on your networks um tomorrow and then on your networks would be next to the talk so most of the time in languages dependencies are projective the things stay together so the dependencies have a kind of a nesting structure of the kind that you also see in context-free grammars but most languages have at least a few phenomena where you ended up with these ability for phrases to be split apart which lead to non-projective dependencies so in particular one of them in english is that you can take modifying phrases and clauses like the on neural networks here and shift them right towards the end of the sentence and get i'll give a talk tomorrow on neural networks and that then leads to non-projective sentences um so as a parse is projective if there are no crossing dependency arcs when the words are laid out in their linear order with all arcs above the words and if you have a dependency pass that corresponds to a context-free grammar tree um it actually has to be projective because context-free grammars necessarily have this sort of nested tree structure following the linear order but dependency grammars normally allow non-projective structures to account for displaced constituents and you can't easily get the semantics of certain constructions right without these non-projective dependencies so here's another example um in english with question formation with what's called preposition stranding so the sentence is who did bill buy the coffee from yesterday um there's another way i could have said this it's less natural in english but i could have said um from who did bill from who did bill buy the coffee yesterday in many languages of the world that's the only way you could have said it and when you do that from who is kept together and you have a projective pass for the sentence but english allows and indeed much prefers you to do what is referred to as preposition stranding where you move the who um but you just leave the preposition behind and so you get who did bill buy the coffee from yesterday and so then we're ending up with this non-projective dependency structure as i've shown there okay i'll come back to non-projectivity in a little bit how do we go about building dependency parsers well there are a whole bunch of ways um that you can build dependency parsers very quickly i'll just say a few names and i'll tell you about one of them so you can use dynamic programming methods to build dependency parsers so i i showed earlier that you can have an exponential number of parsers for a sentence and that sounds like really bad news for building a system well it turns out that you can be clever and you can work out a way to dynamic program finding that exponential number of parsers and then you can have an o n cubed algorithm so you could do that you can use graph algorithms and um and i'll say a bit about that later but that may spill into next time um so you can see since we're wanting to kind of connect up all the words into a tree using graph edges that you could think of doing that using using a minimum spanning tree algorithm of the sort that you hopefully saw in cf cs 161 and so that idea has been used for parsing constraint satisfaction ideas that you might have seen in cs221 have been used for dependency parsing but the way i'm going to show now is transition based parsing or sometimes referred to as deterministic dependency parsing and the idea of this is one's going to use a transition system so that's like shift reduce parsing if you've seen shift reduce parsing in something like a compiler's class or a formal languages class that shift and reduce our transition steps and so use a transition system to guide the construction of and so let me just explain about that so let's see um so this was an idea that was made prominent by joachim nivray who's a swedish computational linguist who introduced this idea of greedy transition based parsing so his idea is well what we're going to do for dependency parsing is we're going to part be able to pass sentences by having a set of transitions which are kind of like shift reduce parser and it's going to just work left to right bottom up and pass a sentence so we're going to say we have a stack sigma a buffer beta of the words that we have to process and we're going to build up a set of dependency arcs by using actions which are shift and reduce actions and putting those together this will give us the ability to put path structures over sentences and let me go through the details of this and this is a little bit hairy when you first see it that's not so complex really and this this kind of transition based dependency parser is what we'll use in assignment three so what we have so this is our transition system we have a starting point where we start with a stack that just has the root symbol on it and a buffer that has the sentence that's about to part we're about to pass and so far we haven't built any dependency arcs and so at each point in time we can choose one of three actions we can shift which moves the next word onto the stack we can then do actions that are the reduce actions so two reduce actions to make it a dependency grammar we can either do a left arc reduce or a right arc reduce so when we do either of those we take the top two items on the stack and we make one of them a dependent of the other one so we can either say okay let's make wi a dependent of wj or else we can say okay let's make wj a dependent of wi and so the result of when we do that is the one that's the dependent disappears from the stack and so in the stacks over here there's one less item but then we add a dependency r to our arc set so that we say that we've got either a dependency from j to i or a dependency from i to j and commonly when we do this we actually also specify what grammatical relation connects the two such as subject object noun modifier and so we also have here a relation that's still probably still very abstract so let's go through an example so this is how a simple transition based dependency parser what's referred to as an arc standard transition based dependency parser would pass up i ate the fish so remember these are the different operations that we can apply so to start off with we have root on the stack and the sentence in the buffer and we have no dependency arcs constructed so we have to choose one of the three actions and when there's only one thing on the stack the only thing we can do is shift so we shift and now the stack looks like this so now we have to take another action and at this point we have a choice because we could immediately reduce so you know we could say okay let's just make i a dependent of root and we'd get a stack size of one again but that'd be the wrong thing to do because i isn't um the head of the sentence so what we should instead do is shift again and get i8 on the stack and fish still in the buffer well at that point we keep on passing a bit further and so now what we can do is say well wait a minute now i is a dependent of 8 and so we can do a left arc reduce and so i disappears from the stack so here's our new stack but we add to the set of arcs that we've added that i is the subject of eight okay well after that we could have we could reduce again because there's still two things on the stack but that would be the wrong thing to do the right thing to do next would be to shift fish onto the stack and then at that point we can do a right arc reduce saying that eight is the object of fish and add a new dependency to our dependency set and then we can one more time do a right arc reduce to say that 8 is the root of the whole sentence and add in that extra root relation with our pseudo root and at that point we've reached the end condition so the end condition was the buffer was empty and there's one thing the root on the stack and at that point we can finish so this little transition machine does the parsing up of the sentence but there's one thing that's left to explain still here which is how do you choose the next action so as soon as you have two things or more on the stack what you do next you've always got a choice you could keep shifting at least if there's still things on the buffer or you can do a left arc or you can do a right arc and how do you know what choice is correct and well one answer to that is to say well you don't know what choice is correct and that's why parsing is hard and sentences are ambiguous you can do any of those things you have to explore all of them and well if you naively explore all of them then you do an exponential amount of work to pass the sentence so in the early 2000s um joachim phrase and you know that's essentially what people have done in the 80s and 90s is explore every path um but um in the early 2000s um joachim neverey's essential observation was but wait a minute we know about machine learning now so why don't i try and train a classifier which predicts what the next action i should take is given this stack and buffer configuration because if i can write a machine learning classifier which can nearly always correctly predict um the next action given a stack and buffer then i'm in a really good position because then i can build what's referred to as a greedy dependency parser which just goes bang bang bang word at a time okay here's the next thing run classifier choose next action run classifier choose next action run classifier choose next action so that the amount of work that we're doing becomes linear in the length of the sentence rather than that being cubic and the length of the sentence using dynamic programming or exponential and the length of the sentence if you don't use dynamic programming so for each at each step we predict the next action using some discriminative classifier so starting off he was using things like support vector machines but it can be anything at all like a soft max classifier that's closer to our neural networks and there are either for what i presented um three classes if you're just thinking of the two reduces and the shift or if you're thinking of you're also assigning a relation and you have a set of r relations like 20 relations then that's the sort of 41 moves that you could um decide on at each point and the features are effectively the configurations i was showing before what's the top of the stack word what part of speech is it what's the first word in the buffer what's that words part of speech etc and so on the simplest way of doing this you're now doing no search at all you just sort of take each configuration and turn decide the most likely next move and you make it and that's a greedy dependency parser which is widely used you can do better if you want to do a lot more work so you can do what's called a beam search where you maintain a number of fairly good parse prefixes at each step and you can extend them out further and then you can evaluate later on which of those seems to be the best and so beam search is one technique to improve dependency parsing by doing a lot of work and it turns out that although these greedy transition based parses are a fraction worse than the best possible ways known to pass sentences um but they actually work very accurately almost as well and they have this wonderful advantage that they give you linear time parsing in terms of the length of your sentences and text and so if you want to do a huge amount of parsing they're just a fantastic thing to use because um you've then got an algorithm that scales to the size of the web okay um so i'm kind of a little bit behind so i guess i'm not going to get through all of these slides today and we'll have to finish out the final slides tomorrow but um just to push a teeny bit further i'll just say a couple more on the sort of what never did for dependency parser and then i'll sort of introduce the neural form of that in the next class so conventionally you had this sort of stack and buffer configuration and you wanted to build a machine learning classifier and so the way that was done was by using symbolic features of this configuration and what kind of symbolic features did you use you use these indicator features that picked out a small subset normally one to three elements of the configuration so you'd have a feature that could be something like the thing on the top of the stack is the word good which is an adjective or it could be the thing on the top of the stack is an adjective and the thing that's first in the buffer is a noun or it could just be looking at one thing and saying the first thing in the buffer is a verb so you'd have all of these features and because these features commonly involved words and commonly involved conjunctions of several conditions you have a lot of features and you know having mentions of words and conjunctions and conditions definitely help to make these parsers work better but nevertheless because you had all of these sort of one zero symbolic features that you had a ton of such features so commonly these parsers were built um using something like you know a million to ten million different features of sentences and i mentioned already the importance of evaluation let me just sort of quickly say how these parsers were evaluated so to evaluate a a parser for a particular sentence it was hand our test set was hand passed in the tree bank so we have gold dependencies of what the human thought were right and so we can write those down those dependencies down as statements of saying the first word is the dependent of the second word fire a subject dependency and then the parser is also going to make similar claims as to what's dependent on what and so there are two common metrics that are used one is just are you getting these dependency facts right so both of these dependency facts match and so that's referred to as the unlabeled accuracy score where we're just sort of measuring accuracies which are of all of the dependencies in the gold sentence and remember we have one dependency per word in the sentence so here we have five how many of them are correct and that's our unlabeled accuracy score of eighty percent but a slightly more um rigorous evaluation is to say well no we're also going to label them and we're going to say that this is the subject that's actually called the root this one's the object so these dependencies have labels and you also need to get the grammatical relation label right and so that's then referred to as labeled accuracy score and although i got those two right for that as i guess according to this example actually this is wrong it looks like i got oh no this is wrong there sorry that one's wrong there okay um so i only got two of the um dependencies correct in the sense that i both got what depends on what and the label correct and so my labeled accuracy score is only 40 percent okay um so i'll stop there now for the introduction for dependency parsing and i still have an iou which is um how we can then bring neural nets into this picture and how they can um be used to improve dependency parsing so i'll do that at the start of next time before then proceeding further into neural language models 
","['', 'constituency parsing', 'dependency parsing', 'natural language processing', 'syntactic structure', 'parts of speech', 'determiners', 'noun phrases', 'prepositional phrases', 'context-free grammars', 'non-terminals', 'tree banks', 'universal dependencies tree banks', 'machine learning', 'dependency grammar', 'projective dependencies', 'non-projective dependencies', 'beam search', 'transition-based parsing', 'symbolic features', '']"
"so we're now starting um in week three with um lecture five so unfortunately on the last class i i guess i really got behind and went a bit slowly i guess i must just enjoy talking about natural languages too much and so i never really got to the punch line of showing how you could do good things with the neural dependency parser so today for the first piece i'll in some sense be finishing the content of last time and talk about neural dependency parsing which also gives us the opportunity um to introduce a simple feed forward neural net classifier um that will then lead into a little bit of just background things that you need to know about neural networks content because the fact of the matter is there is a bunch of stuff you need to know about neural networks and then after both of those things i'll get into what's really meant to be the topic of today's lecture which is looking at language modeling and recurrent neural networks and that's then going to lead into those two things are important topics that we'll then be talking about really for the whole of next week as well there's a couple of reminders before we get underway the first is that you should have handed in assignment 2 before you joined this class and in turn assignment three is out today and it's an assignment um where you're going to build essentially the new um dependency parser that i'm just about to present in pytorch so part of the role of this assignment is actually to get you up to speed with pytorch so this assignment is highly scaffolded with lots of comments and hints about what to do and so the hope is that by the time you come to the end of it you'll feel fairly familiar and comfortable with pie torch don't forget there was also tutorial on pie torch last week if you didn't catch that at the time you might want to go back and look at the video another thing to mention about the assignments is that assignment three is the last assignment where our great team of tas are happy to look at your code and sort out your bugs for you so maybe take advantage of that but not too much but starting an assignment four for assignments four five and the final project um the tas are very happy to help in general but it's just not going to be their job to be actually sorting out bugs for you you should be looking at your code and discussing ideas and concepts and reasons why things might not work with them okay so if you remember where we were last time i'd introduced this idea of transition based dependency parsers and that these were an efficient linear time method um for giving the syntactic structure of natural language text and that they worked pretty well before neural nets came along and took over nlp again but they had some disadvantages and their biggest disadvantage is that like most machine learning models of that time they worked with indicator features so that means that you are specifying some condition and then checking whether it was true of a configuration so something like the word on the top of the stack is good and it's part of speech is adjective or the next word coming up is a personal pronoun that those are conditions that would be features and a conventional um transition based dependency parser and so what are the problems with doing that well one problem is that those features are very sparse a second problem is the features are incomplete well what i mean by that is depending on what words and configurations occurred in the training data there are certain features that will exist because you sort of saw a certain word preceding a verb and certain features that just won't exist because that word never occurred before a verb in the training data but perhaps the biggest problem and opportunity for doing better with the neural dependency parser is that it turns out that in a symbolic dependency parser computing all these features just turns out to actually be pretty expensive that although the actual transition system that i showed last time is fast and efficient to run you actually have to compute all of these features and what you found was that about 95 of the parsing time of one of these models was spent just computing all of the features of every configuration so that suggests that perhaps we can do better with a neural approach where we're going to learn a dense and compact feature representation and so that's what i want to go through now so this time we're still going to have exactly the same kind of configuration of a stack and a buffer and running exactly the same transition sequence except this time rather than representing the configuration the stack and the buffer by having several million symbolic features we're instead going to summarize this configuration as a dense vector of dimensionality perhaps approximately a thousand and our neural approach is going to learn this dense compact feature representation and so quite explicitly what i'm going to show you now briefly and what you're going to implement is essentially the neural dependency parser that was developed by dante chen in 2014 and to skip to the advertisement right at the beginning as to how this works so well these are the kind of results that you got from it using the measures that i introduced at the last time the unlabeled attachment score whether you attach dependencies correctly um to the right word and the labeled attachment score as to whether you also get the type of grammatical relation of that dependency correct um and so essentially um this chin and manning parser gave a neural version of something like a transition based dependency parser like malt parser in yellow and the interesting thing was that taking advantage of a neural classifier in ways that i'm about to explain that that could produce something that was about two percent more accurate than the symbolic dependency parser and because of the fact that it's not doing all the symbolic feature computation despite the fact that you might think at first that there's a lot of real number math and matrix vector multiplies in a neural dependency parser it actually ran noticeably faster than the symbolic dependency parser because it didn't have um all the feature compute computation the other major approach to dependency parsing that i'm also showing here and i'll get back to at the end is what's referred to as graph based dependency parsing and so that's a different approach to dependency parsing and so these are two symbolic graph-based dependency parsers and in the pre-neural world they were somewhat more accurate than the transition based parses as you could see but on the other hand they were close to two orders of magnitude slower um and so essentially with the chern manning parser we were able to provide something that was basically as accurate as the best graph based dependency parsers which were the best dependency parsers while operating about two orders of magnitude more quickly so how did we do it it was actually a very straightforward implementation which is part of what makes it great for doing for assignment three um but this is how we did it and we got wins so the first win which is what we've already talked about extensively starting in week one is to make use of distributed representations so we represent each word as a word embedding and you've had a lot of experience with that already and so that means when words weren't seen in a particular configuration we still know what they're like because they'll be we'll have seen similar words in the correct configuration um but we don't stop only with word embeddings the other things that are central to our dependency parser are the parts of speech of words and the dependency labels and so what we decided to do is that although those are much smaller sets so the dependency labels are about 40 in number and the parts of speech are of around that order of magnitude sometimes less sometimes more that even within those sets of categories there are ones that are very strongly related so we also adopted um distributed representations for them so for example there might be parts of speech for singular nouns and plural nouns and basically most of the time they behave similarly and there are adjectival modifiers and numerical modifiers so these are just numbers like three four five and again a lot of the time they behave the same that you have both three cows and brown cows okay so everything is going to be represented in a distributed representation so at that point we have exactly the same kind of configuration where we have our stack our buffer and we've started to build some arcs and so the classification decisions of the next transition are going to be made out of a few elements of this configuration so we're looking at the top thing on the stack um the thing second on the stack the first word on the buffer and then we actually added in some additional features that are then to the extent that we've already built arcs for words on the stack that we can be looking at the dependence on the left and right of those words that are on the stack that are already in the sets of arcs and so for each of those things they there is a word there is a part of speech and for some of them there is a dependency um where it's already connected up to something else so for example the left corner of s2 here has an n sub dependency back to the second thing on the stack so we can take these elements of the configuration and can look up the embedding of each one so we have word embeddings part of speech embeddings and dependency embeddings and just concatenate them all together kind of like we did before with the window classifier and that will give us a newer representation of the configuration now there's a second reason why we can hope to win by using a deep learning classifier to predict the next transition and we haven't really said much about that yet so i just wanted to detour and say a little bit more about that um so the simplest kind of classifier that's close to what we've been talking about in neural models is a soft max classifier so that if we have d dimensional vectors x and we have y classes to assign things to um oh sorry y is an element of a set of um c classes to assign things to then we can build a softmax classifier using the softmax distribution that we've seen before where we decide the classes based on having a weight matrix that's c by d and we train on supervised data the values of this w weight matrix to minimize our negative log likelihood loss that we've seen before um a loss is also commonly referred to as cross-entropy loss a term that you'll see in pie torch among other places um so that is a straightforward machine learning classifier and if you've done 229 and you've seen soft max classifiers um but a simple softmax classifier like this um shares with most traditional machine learning classifiers so models include naive bayes models support vector machines logistic regression that at the end of the day they're not very powerful classifiers they're classifiers that only give linear decision boundaries and so this can be quite limiting so if you have a difficult problem like the one i'm indicating in the picture in the bottom left well there's just no way you can divide the green points from the red points by simply drawing a straight line so you're going to have a quite imperfect classifier so the second big win of neural classifiers is that they can be much more powerful because they can provide non-linear classification so rather than only being able to do something like in the left picture um we can come up with classifiers that do something like in the right picture and therefore can separate the green and the red points um as an aside um these pictures i've taken from andre caparti's compnet js software which is a kind of a fun little tool to play around with if you've got a bit of spare time um and so there's something subtle going on here is because our more powerful neural net classifiers at the end of the day what they have at the top of them is a softmax layer and so this softmax layer is indeed a linear classifier and it's still a linear classifier but what they have below that is other layers of neural net and so effectively what happens is that the classification decisions are linear as far as the top softmax is concerned but non-linear in the original representation space so precisely what a neural net can do is warp the space around and move the representation of data points to provide something that at the end of the day can be classified by a linear classifier and so that's what a simple feedforward neural network multi-class classifier does so it starts with an input representation so these are is some dense representation of the input it puts it through a hidden layer h with a matrix multiply followed by non-linearity so that matrix multiply can transform the space and map things around and so then the output of that we can then put into a soft max layer and get out soft max probabilities from which we make our classification decisions and to the extent that our probabilities don't assign one to the correct class we then get some log loss or cross entropy error which we back propagate towards the parameters and embeddings of our model and as the learning that goes on via back propagation we increasingly well learn parameters of this hidden layer of the model which learn to re-represent the input they move the inputs around in an intermediate hidden vector space so it can be easily classified with what at the end of the day is the linear softmax so this is basically the whole of a simple feed for neural network multi-class classifier but um and if we had something like a a visual signal we just sort of feed straight in here real numbers and we've been done but normally with um human language material we actually effectively have one more layer that we're feeding in before that because really below this dense input layer we actually have one hot vectors for what words or parts of speech were involved and then we're doing a lookup process which you can think of as one more matrix multiply to convert the one hot features into our dense input layer okay in my picture here the one other thing that's different is i've introduced a different non-linearity in the hidden layer which is a rectified linear unit and that's what we'll be using now neural dependency parsers um it looks like the picture in the bottom right and i'll come back um to that in a few minutes that's one of the extra neural net things um to talk about okay so our neural net dependency parser model architecture is essentially exactly that um but applied to the configuration of our transition based dependency parser so based on our transition based dependency parser configuration we construct an input layer embedding by looking up on the various elements as i discussed previously and then we feed it through this hidden layer to the softmax layer to get probabilities out of which we can choose what the next action is and it's no more complicated than that um but what we found is um that just simply you you know in some sense using the simplest kind of feed forward neural um classifier could provide a very accurate dependency parser that determines the structure of sentences supporting meaning interpretation the kind of way that i suggested last time indeed you know despite the fact that it was a quite simple architecture in 2014 this was the first successful neural dependency parser and the dense representations especially but also partly the non-linearity of the classifier gave us this good result that it could both outperform symbolic parsers in terms of accuracy and it could outperform them in terms of speed um so that was 2014 just quickly here a couple more slides on what's happened since then so lots of people got excited by the success of this new dependency parser and a number of people particularly at google then cetera about building a bigger fancier transition based neural dependency parser so they explored bigger deeper networks there's no reason to only have one hidden layer you can have two hidden layers um you can do beam search that i briefly mentioned last time another thing that i'm not going to talk about now is adding conditional random field style inference over decision sequences and that then led in 2016 um for a model that they called um parsi mcpa's face which is hard to say with a straight face um which was then about two and a half three percent um more accurate than the model that we had produced but still in basically the same family of transition based parser with the neural net classifier to choose the next transition um the alternative to transition based parsers as graph based dependency parsers and for a graph-based dependency parser what you're doing is effectively considering every pair of words and considering a word as a dependent of root and you're coming up with a score as to how likely is it that big is a dependent of root or how likely is big to be dependent of cat and similarly for every other word for the word sat um how likely is it to be a dependent of root or a dependent of the etc and well to do that well you need to know more than just what the two um words involved are and so what you want to do is understand the context so you want to have an understanding of the context of big what's to the left of what's to the right of it to understand how you might hook it up into the dependency representations of the sentence um and so while they've been previous work in graph based dependency parsing like the mst parser i showed on the earlier results slide it seemed appealing that we could come up with a much better representation of context using neural nets that look at context and how we do that is actually what i'll be talking about in the end part of the lecture and so at stanford um we became interested in trying to work out how to come up with a better graph-based dependency parser using context sorry i forgot this this was showing that um if we can score each pairwise dependency we can simply choose the best one so we can say um probably big is a dependent of cat and to a first approximation we're going to want to choose for each word that it is a dependent of the word that seems most likely to be a dependent but we want to do that with some constraints because we want to get out something that is a tree with a single root as i discussed last time and you can do that by making use of a minimum spanning tree algorithm that uses these scores of how likely different dependencies are okay so then in 2017 another student tim doset and me then worked on um saying well can we now also build a much better neural graph-based dependency parser and we developed a novel method for scoring um neural scoring dependency parses and a graph based model which i'm not going to get into the details of right now but that also had a very nice result because use getting back to graph based parsing we could then build a graph based parser that performed about a percent better than the best of the the google transition based new dependency parsers but i should point out that this is a mixed win because although its accuracy is better these graph-based parsers are just in squared in performance rather than linear time so kind of like the early results i showed they don't operate nearly as quickly when you're wanting to pass large amounts of text with complex long sentences okay so that's everything you need to know about dependency parsers and to do assignment three so grab it this evening and start to work um but i did want to sort of before going on to the next topic just mention a few more things um about neural networks since um some of you know this well already some of you have seen less of it but you know there just are a bunch of things you have to be aware of um for building new networks now again for assignment three essentially we give you everything and if you follow the recipe your parser should work well but you know what you should minimally do is actually you know look carefully at some of the things that this parser does which is questions like how do we initialize our matrices of our neural network what kind of optimizers do we use and things like that um because these are all important decisions and so i wanted to say just a few words about that okay so the first thing that we haven't discussed at all is the concept of regularization so when we're building these neural nets we're now building models with a huge number of parameters so essentially just about all neural net models that work well actually they're full loss function is a regularized loss function so for this um loss function here of j well this part here is the part that we've seen before um where we're using a soft max classifier and then taking a negative log likelihood loss which we're then averaging over the different examples but actually we then stick on the end of it this regularization term and so this regularization term sums the square of every parameter in the model and so what that effectively says is you only want to make parameters non-zero if they're really useful right so the to the extent the parameters don't help much you're just being penalized here um by making them non-zero but to the extent that the parameters do help you'll gain in your estimation of likelihood and therefore it's okay for them to be non-zero in particular um notice that this penalty is assessed only once per parameter it's not being assessed separately for each example okay and having this kind of regularization um is essential to build neural net models that regularize well so the classic problem is referred to as overfitting and what overfitting means is that if you have a particular training data set and you start training your model your error will go down because you'll shift the parameters so they better predict um the the correct answer for data points in the model and you can keep on doing that and it will start keep on reducing your error rate but if you then look at your partially trained classifier and say how well does this classifier classify independent data different test data that you weren't training the model on what you'll find is up until a certain point um you'll get better at classifying independent test examples as well and after that commonly what will happen is you'll actually start to get worse at classifying independent test examples even though you're continuing to get better at predicting the training examples and so this was then referred to as you're over fitting the training examples that you're fiddling the parameters of the model so they're really good at predicting the training examples which aren't useful things that can then predict um on independent examples that you come to at runtime okay um that classic view of regularization is sort of actually outmoded and wrong for modern neural networks um so the right way to think of it for the kind of modern big neural networks that we build is that overfitting on the training data isn't a problem but nevertheless you need regularization to make sure that your models generalize well to independent test data so what you'd like is for your graph not to look like this example with test error starting to head up you'd like to have it at worst case flat line and best case still be gradually dropping it'll always be higher than the training error but it's not actually showing a failure to generalize so when we train big neural nets these days our big neural nets always overfit on the training data they hugely overfit on the training data in fact in many circumstances our neural nets have so many parameters that you can continue to train them on the training data until the error on the training data is zero they get every single example right because they can just memorize enough stuff about it to predict the right answer but in general providing the models are regularized well those models will still also generalize well and predict well on independent data and so for part of what we want to do for that is to work out how much to regularize and so this lambda parameter here is the strength of regularization so if you're making that lambda number big you're getting more regularization and if you're making it smaller you're getting less and you don't want to have it be too big or else you won't fit the data well and you don't want to be too small or else you have the problem that you don't generalize well okay so this is classic l2 regularization and it's a starting point but our big neural nets are sufficiently complex and have sufficiently many parameters that essentially l2 regularization doesn't cut it so the next thing that you should know about and is a very standard good feature for building neural nets is a technique called drop out so dropout is generally introduced as a sort of a slightly funny process that you do when training to avoid feature coat co-adaptation so in dropout what you do is at the time that you're training your model that for each instance or for each batch in your training then for each neuron in the model you drop 50 of its inputs you just treat them as zero and so that you can do by sort of zeroing out elements of um the sort of layers um and then at test time you don't drop any of the model weights you keep them all but actually you have all the model weights because you're now keeping twice as many things as you'd use the training data um and so effectively that little recipe um prevents what's called feature co-adaptation so um you can't you can't have features um that are only useful in the presence of particular other features because the model can't guarantee which features are going to be present for different examples because different features are being randomly dropped all of the time and so effectively dropout gives you a kind of a middle ground between naive bayes and the logistic regression model and the naive bayes models all the weights are said independently in a logistic regression model all the weights are set in the context of all the others and here you are aware of other weights but they can randomly disappear from you it's also related to ensemble models like model bagging because you're using different subsets of the features every time um but after all of those explanations there's actually another way of thinking about dropout which was actually developed here at stanford this paper by percy liang and students um which is to argue that really what dropout gives you is a strong regularizer that isn't a uniform regularizer like l2 that regularizes everything with an l2 last but can learn a feature dependent regularization and so that dropout has just emerged as in general the best way to do regularization for neural nets i think you've already seen and heard this one but just have it on my slides once if you want to have your neural networks go fast it's really essential that you make use of vectors matrices tensors and you don't do things with for loops so here's a tiny example where i'm using time it which is a useful thing that you can use too to see what how fast your neural nets run and different ways of writing that and so when i'm doing this um doing these dot products here i can either do it the dot product in a for loop against each word vector or i can do the dot product with with a single word vector matrix and if i do it in a for loop doing each loop takes me almost a second whereas if i do it with a matrix multiply it takes me an order of magnitude less time so you should always be looking to use vectors and matrices not for loops and this is a speed up of about 10 times when you're doing things on a cpu heading forward we're going to be using gpus and they only further exaggerate the advantages of using vectors and matrices where you'll commonly get two orders of magnitude speed up um by doing things that way yeah so for the backward pass you are running a backward passes before on the dropped out examples right so for the things that were dropped out no gradient is going through them because they weren't present they're not affecting things so in a particular batch you're only training weights for the things that aren't dropped out but then since you for each successive um batch you drop out different things that over a bunch of batches you're then training all of the weights of the model um and so feature dependent regularizer is meaning that how much a feature the different features can be regularized different amounts to maximize performance so back in this model every feature was just so being penalized by taking lambda times at squared values so this is sort of uniform regularization where the end result of this dropout style training is that you end up with some features being regularized much more strongly and some other features being regularized less strongly and how much they be regularized depends on how much they're being used so you're regularizing more features that are being used less but i'm i'm not going to get through into the details of how you can understand that perspective um that's the that's um outside of the context of what i'm going to get through right now so the final bit is i just wanted to give a little bit of perspective on non-linearities in our neural nets so the first thing to remember is you have to have non-linearity so if you're building a multi-layer neural net and you've just got you know w1 x plus b1 then you put it through w to x plus b2 and then put through w3 um x well i guess they're different hidden layers so i should have said x they should be hidden one hidden two hidden three w3 hidden three plus b3 um that multiple linear transformations um compose so they can be just collapsed down into a single linear transformation so you don't get any power as a data representation by having multiple linear layers there's a slightly longer story there because you actually do get some interesting learning effects but i'm not going to talk about that now but standardly we have to have some kind of non-linearity to do something interesting in a deep neural network okay so this there's a starting starting point as the most classic non-linearity is the logistic often just called the sigmoid um non-linearity because of its s shape um which we've seen before in previous lectures so this will take any real number and map it on the to the range of zero one um and that was sort of basically what people used in sort of 1980s neural nets now one disadvantage of this non-linearity um is that it's moving everything into the positive space because the output is always between zero and one so people then decided that for many purposes it was useful to have this variant sigmoid shape of hyperbolic tan which is then being shown in the second picture now you know logistic and hyperbolic tan um they sound like they're very different things but actually as you maybe remember from a math class hyperbolic tan can be represented in terms of exponentials as well and if you do a bit of math which possibly we might make you do on an assignment um it's actually the case that a hyperlog tangent is just a rescaled and shifted version of the logistics so it's really exactly the same curve just squeezed a bit so it goes now symmetrically between minus one and one um well um these kind of transcendental functions like hyperbolic tangent they're kind of slow and expensive to compute right even on our fast computers calculating exponentials is a bit slow so something people became interested in was well could we do things with much simpler non-linearity so what if we used a so-called hard tan h so the hard 10h at some point up to some point it just flat lines at -1 then it is y equals x up until one and then it just flat lines again and you know that seems a slightly weird thing to use because if your input is over on the left or over on the right you're sort of not getting any discrimination in it for things giving the same output but somewhat surprisingly i mean i was surprised when people um started doing this um these kind of models um proved to be very successful and so that then led into what's proven to be kind of the most successful and generally widely used non-linearity and a lot of recent deep learning work which was what was being used um in the dependency powers model i showed is what's called the rectified linear unit or value so a value is kind of the simplest kind of non-linearity that you can imagine so if the value of x is negative its value is zero so effectively it's just dead it's not doing anything in the computation and if its value of x is greater than zero then it's just simply y equals x the value is being passed through um and at first sight this might seem really really weird and how could this be useful as a non-linearity but if you sort of think a bit about how you can approximate things with piecewise linear functions very accurately you might kind of start to see how you could use this to do accurate function approximation with piecewise linear functions and that's what value units have been found to do extremely extremely successfully um so logistic and tan h are still used in various places you use logistic when you on a probability output we'll see 10 h's again very soon when we get to recurrent neural networks um but they're no longer the default when making deep networks that in a lot of places the first thing you should think about trying is relu non-linearities and so in particular um that why part of why they're good is that religion on networks train very quickly because you get this sort of very straightforward gradient backflow because providing you on the right hand side of it you then just getting this sort of constant gradient backflow from the slope one and so they train very quickly the somewhat surprising fact is that sort of almost the simplest non-linearity imaginable is still enough to have a very good neural network but it just is um people have played around with variants of that um so people have then played around with leaky rail use where rather than the left hand side just going completely to zero it goes slightly negative on a but much shallower slope and then there's been a parametric reload where you have an extra parameter where you learn the slope of the negative part um another thing that's been used um recently is this swish non-linearity which looks almost like a value um but it sort of curves down just a little bit there and starts to go up i mean i think it's fair to say that you know none of these have really proven themselves vastly superior there are papers saying i can get better results by using one of these and maybe you can but you know it's not night and day and the vast majority of work that you see around is still just using values in many places okay a couple more things parameter initialization so in almost all cases you must must must initialize um the matrices of your neural nets with small random values neural nets just don't work if you start the matrices off as zero because effectively then everything is symmetry is symmetric nothing can specialize in different ways and and you then get sort of uh you just don't have an ability for a neural net to learn you sort of get this defective solution so standardly you're using some method such as drawing random numbers uniformly between minus r and r for a small value r and just filling in all the parameters with that the exception is with bias weights it's fine to set bias weights to zero and in some sense that's better in terms of choosing what the r value is essentially for traditional neural nets what we want to set that our range for is so that the numbers in our neural network stay of a reasonable size they don't get too big and they don't get too small and whether they kind of blow up or not depends on how many connections there are in the neural network so looking at the fan in and fan out of connections in the neural network and so a very common initialization that you'll see in pi torch is what's called javier initialization named after a person who suggested that and it's working out a value of uh based on um this fan in and fan out of the layers but you can just sort of ask for it say initialize with this initialization and it will this is another area where there have been some subsequent developments so around week five we'll start talking about layer normalization if you're using layer normalization then it sort of doesn't matter the same how you initialize the weights so finally we have to train our models and i've briefly introduced the idea of stochastic gradient descent and you know the good news is that most of the time that if training your networks with stochastic gradient descent works just fine use it and you will get good results however often that requires choosing a suitable learning rate which is my final slide of tips on the next slide but there's been an enormous amount of work on optimization of neural networks and people have come up with a whole series of more sophisticated optimizers and i'm not going to get into the details of optimization in this class but the very loose idea is that these optimizers are adaptive in that they can kind of keep track of how much slope there was how much gradient there is for different parameters and therefore based on that make decisions as to how much to adjust the weights when doing the gradient update rather than adjusting it by a constant amount and so in that family of methods there are methods that include edegrad rms prop adam and then a variants of adam including sparse adam adam w etc the the one called atom is a pretty good place to start um and a lot of the time that's a good one to use and again from the perspective of pie torch when you're initializing an optimizer you can just say please use adam and you don't actually need to know much more about it than that if you are um using simple stochastic gradient send you have to change choose a learning rate so that was the eta value that you multiplied the gradient by for how much to adjust the weights and so i talked about that slightly how you didn't want it to be too big or your model could diverge or bounce around you didn't want it to be too small or else training could um take place exceedingly slowly and you'll miss the assignment deadline um you know how big it should be depends on all sorts of details of the model and so you sort of want to try out some different order of magnitude numbers to see what numbers seem to work well for it training stability but reasonably quickly something around 10 to the minus 3 or 10 to the minus 4 isn't a crazy place to start in principle you can do fine just using a constant learning rate in sgd in practice people generally find they can get better results by decreasing learning rates as you train so a very common recipe is that you halve the learning rate after every k epochs where an epoch means that you've made a pass through the entire set of training data so perhaps something like every three epochs you have the learning rate and a final little note there in purple is when you make a pass through the data you don't want to go through the data items in the same order each time because that leads you just kind of be have a sort of patterning of the training examples that the model will sort of fall into that periodicity of those patterns so it's best to shuffle the data before each pass through it okay there are more sophisticated ways to set learning rates um and i won't really get into those now fancier optimizers like adam also have a learning rate so you still have to choose a learning rate value but it's effectively it's an initial learning rate which typically the optimizer shrinks as it runs and so um you commonly want to have the number it starts off with beyond the larger size because it will be shrinking as it as it goes okay so that's all by way of introduction and i'm now ready to start on language models and rnns so what is language modeling i mean as two words of english language modeling could mean just about anything but in the natural language processing literature language modeling has a very precise technical definition which you should know so language modeling is the task of predicting the word that comes next um so if you have some context like the students open there you want to be able to predict what words will come next is it their books their laptops their exams their minds and so in particular what you want to be doing is being able to give a probability that different words will occur in this context so a language model is a probability distribution over next words given a preceding context and a system that does that is called a language model so as a result of that you can also think of a language model as a system that assigns a probability score to a piece of text so if we have a piece of text then we can just work out its probability according to a language model so the probability of a sequence of tokens we can decompose via the chain rule probability of the first times probability the second given the first etc etc and then we can work that out using what our language model provides as a product of each probability of predicting the next word okay language models are really the cornerstone of human language technology everything that you do with computers that involves human language you are using language models so when you're using your phone and it's suggesting whether well or badly what the next word that you probably want to type is and that's a language model working to try and predict likely next words when the same thing happens in a google doc and it's suggesting a next word or a next few words that's a language model you know the main reason why the one in google docs works much better than the one on your phone is that for the keyboard phone models they have to be very compact um so they can run quickly and not much memory so they're sort of only mediocre language models where something like google docs can do a much better language modeling job query completion same thing there's a language model and so then the question is well how do we um build language models and so i briefly wanted to first again um give the traditional answer um since you should have at least some understanding of how nlp was done without a neural network and the traditional answer that powered speech recognition and other applications for at least two decades three decades really was what were called engram language models and these were very simple but still quite effective idea so we want to give probabilities of next words so what we're going to work with is what are referred to as n grams and so n grams is just a chunk of n consecutive words which are usually referred to as unigrams bigrams trigrams and then four grams and five grams a horrible set of names which would offend any humanist but that's what people normally say and so effectively what we do is just collect statistics about how often different engrams occur in a large amount of text and then use those to build a probability model so the first thing we do is what's referred to as making a markov assumption so these are also referred to as markov models and we decide that the word and position t plus one only depends on the preceding n minus one words so if we want to predict t plus one given the entire preceding text we actually throw away the early words and just use the preceding n minus one words as context well once we made that simplification we can then just use the definition of conditional probability and say all that conditional probability is the probability of n words divided by the preceding n minus one words and so we have the probability of an n gram over the probability of an n minus one gram and so then how do we get these n gram and n minus one gram probabilities we simply take a large amount of text in some language and we count how often the different engrams occur and so our crude statistical approximation starts off as the count of the engram over the count of the n minus one gram so here's an example of that suppose we're learning a foreground language model okay so we throw away all words apart from the last three words and they're our conditioning we look um in some large we use the counts from some large training corpus and we see how often did students open their books occur how often did students open their minds occur and then for each of those counts we divide through by the count of how often students open their occurred and that gives us our probability estimates um so for example if in the corpus students open there occurred a thousand times students open their books occurred 400 times we'd get a probability estimate of 0.4 for books if exams occurred 100 times you get 0.1 for exams and we sort of see here already the disadvantage of having made the markov assumption and have gotten rid of all of this earlier context which would have been useful for helping us to predict um the one other point um that i'll just mention that i confuse myself on is this count of the engram language model so for a foreground language model it's called a foreground language model because in its estimation you're using four grams in the numerator and trigrams in the denominator so you use the the size of the numerator so that terminology is different to the terminology that's used in markov models so when people talk about the order of a markov model that refers to the amount of context you're using so this would correspond to a third order markup model um yeah so someone said is this similar to a naive bayes model sort of naive bayes models you also estimate the probabilities just by counting um so they're they're related and they're sort of in some sense two differences um the first difference or specialization is that naive bayes models um work out probabilities of words independent of their neighbors so what in one part that a naive bayes language model is a unigram language model so you're just using the counts of individual words but the other part of the naive bayes model is you're learning a different set of unigram counts for every class for your classifier um and so um you've then got sort of so effectively a naive bayes model is you've got class specific unigram language models okay i gave this as a simple statistical model for estimating your probabilities with an engram model you can't actually get away with just doing that because you have sparsity problems so you know often will be the case that for many words students open their books or students opened their backpacks just never occurred in the training data that if you think about it if you have something like um ten to the fifth different words even and you want to have then a sequence of four words a problem and they're 10 to the fifth of each there's sort of 10 to the 20th different combinations so unless you're seeing and it's truly astronomical amount of data most forward sequences you've never seen so then your numerator will be zero and your probability estimate will be zero and so that's bad and so the commonest way of solving that is just to add a little delta to every count and then everything is non-zero and that's called smoothing but well sometimes it's worse than that because sometimes you won't even have seen students open theirs and that's more problematic because that means our denominator here is zero and so the division will be ill-defined and we can't usefully calculate any probabilities in a context that we've never seen and so the standard solution to that is to shorten the context and that's called back off so we condition only on open there or if we still don't haven't seen it open there we'll condition only on there or we could just forget all conditioning and actually use a unigram model for our probabilities yeah um and so as you increase the the order n of the engram language model these sparsity problems become worse and worse so in the early days people normally worked with trigram models as it became easier to collect billions of words of text people commonly moved to five gram models but every time you go up an order of conditioning you effectively need to be collecting orders of magnitude more data because of the size of the vocabularies of human languages um there's also a problem that these models are huge um so you basically have to be caught storing counts of all of these word sequences so you can work out these probabilities and i mean that's actually had a big effect in what terms of what technology is available um so in the 2000s decade up till that whenever it was 2014 that there was already google translate using probabilistic models included language models of the engram language model sort but the only way they could possibly be run is in the cloud because you needed to have these huge tables of probabilities but now we have neural nets and you can have google translate just actually run on your phone and that's possible because neural net models can be massively more compact than these old engram language models yeah but nevertheless before we get on to the newer models let's just um sort of look at the example of how these work so it's trivial to train an engram language model because you really just count how often word sequences occur in a corpus and you're ready to go so these models can be trained in seconds that's really good it's not like sitting around for training neural networks so if i train on my laptop a small language model on you know about 1.7 million words as a trigram model i can then ask it to generate text if i give it a couple of words today though i can then get it to sort of suggest a word that might come next and the way i do that is the language model knows the probability distribution of things that can come next um note there's a kind of a crude probability distribution i mean because effectively over this relatively small corpus there were things that occurred once italian and emirate there are things that occurred twice price there were things that occurred um four times company and bank um it's sort of fairly crude and rough but i nevertheless get probability estimates i can then say okay based on this let's take this probability distribution and then we'll just sample the next word so the two most likely was the sample a company or bank but we're um rolling the dice and we might get any of the words that have come next so maybe i sample price now i'll condition on price on the price and look up the probability distribution of what comes next the most likely thing is of and so again i'll sample and maybe this time i'll pick up of and then i will now condition on price of and i will look up the probability distribution of words following that and i get this probability distribution and i'll sample randomly some word from it and maybe this time i'll sample a rare but possible one like gold and i can keep on going and i'll get out something like this today the price of gold per ton while production of shoe lasts and shoe industry the bank intervened just after it considered and rejected an imf demand to rebuild depleted european stocks step 30 n primary 76 cents a share um so what just a simple trigram model can produce over not very much text is actually already kind of interesting like it's actually surprisingly grammatical right there are whole pieces of it while production of shoe last sense you industry the bank interview intervene just after it can sit and reject an imf demand right it's really actually pretty good grammatical um text so it's um it's sort of amazing that these simple engram models actually can model a lot of human language on the other hand it's not a very good piece of text it's completely incoherent and makes no sense and so to actually be able to generate text that seems like it makes sense we're going to need a considerably better language model and that's precisely what neural language models have allowed us to build as we'll see later okay so how can we build a neural language model and so first of all we're going to do a simple one and then we'll see where we get but to move into a current neural nets might still take us the next time so going to have input sequence of words and we want a probability distribution over the next word um well the simplest thing that we could try is to say well kind of the only tool we have so far is a window-based classifier so what we can say you know what we've done previously either for our named entity recognized in lecture three or what i just showed you for the dependency parser is we have some context window we put it through a neural net and we predict something as a classifier so before we were predicting a location but maybe instead we could reuse exactly the same technology and say we're going to have a window-based classifier so we're discarding the further away words just like in engram language model but we'll feed this fixed window into a neural net so we concatenate the word embeddings we put it through a hidden layer and then we have a softmax classifier over our vocabulary and so now rather than predicting something like location or left arc in the dependency parser we're going to have a soft max over the entire vocabulary sort of like we did with the skipgram negative sampling model in the first two lectures and so we're going to see this choice as predicting what word that comes next whether it produces laptops minds books etc okay so this is a a fairly simple fixed window neural net classifier but this is essentially a famous early model in the use of neural nets for nlp applications um so first a 2000 conference paper and then a somewhat later journal paper joshua bengio and colleagues introduced precisely this model as the neural probabilistic language model and they were already able to show that this could give interesting good results for language modeling and so it wasn't a great solution for neural language modeling but it still had value so it didn't solve the problem of allowing us to have bigger contexts to predict what words are going to come next it's in that way limited exactly like an engram language model is but it does have all the advantages of distributed representations so rather than having these counts for word sequences that are very sparse and very crude we can use distribute distributed representations of words which then make predictions that semantically similar words should give similar probability distributions so the idea of that is if we use some other word here like maybe the pupils open there um well maybe in our training data we'd seen sentences about students but we'd never seen sentences about pupils an engram language model then would sort of have no idea what probabilities to use whereas a new language model can say well pupils is kind of similar to students therefore i can predict similarly to what i would have predicted for students okay so there's now no sparsity problem we don't need to store billions of engram counts we simply need to store our word vectors and our w and u matrices but we still have the remaining problems that our fixed window is too small we can try and make the window larger if we do that w the w matrix gets bigger but that also points out another problem with this model not only can the window never be large enough but w is just a trained matrix and so therefore we're learning completely different weights for each position of context the word minus one position the word minus two the word minus three and the word minus four so that there's no sharing in the model as to how it um treats words in different positions even though in some sense they will contribute semantic components that are at least somewhat um position independent so again for those of if you sort of think back to either a naive bayes model or what we saw with the word to vect model at the beginning the word to vect model or naive bayes model completely ignores word order so it has one set of parameters regardless of what position things occur in that doesn't work well for language modeling because word order is really important in language modeling if the last word is the that's a really good predictor of there being an adjective or noun following where if the word for back is the um it doesn't give you the same information so you do want to somewhat make use of word order but this model is at the opposite extreme that each position is being modeled completely independently so what we'd like to have is a neural architecture that can process an arbitrary amount of context and have more sharing of the parameters while still be sensitive to proximity and so that's the idea of recurrent neural networks and i'll say about five minutes about these today and then next time we'll return and do more about your of recurrent neural networks so for the recurrent neural network rather than having a single hidden layer inside our classifier here that we compute each time for the recurrent neural network we have the hidden layer which often is referred to as the hidden state but we maintain it over time and we feed it back into itself um so that's what the word recurrent is meaning that you're sort of feeding the hidden layer back into itself so what we do is based on the first word we compute a hidden representation kind of like before which can be used to predict the next word but then for when we want to predict what comes after the second word we not only feed in the second word we feed in the hidden layer from the previous word to have it help predict the hidden layer above the second word and so formally the way we're doing that is we're taking the hidden layer above the first word multiplying it by a matrix w and then that's going to be going in together with x2 to generate the next hidden step and so we keep on doing that at each time step so that we're kind of repeating a pattern of creating a next hidden layer based on the next input word and the previous hidden state by updating it by multiplying it by a matrix w okay so in my slide here i've still only got four words of context because it's nice for my slide but you know in principle there could be you know any number of words of context now okay so what we're doing is so um that we start off by having input vectors which can be our word vectors that we've looked up for each word um so sorry yeah so we can have the one hot vectors for word identity we look up our word embedding so then we've got word embeddings for each word and then we want to compute hidden states so we need to start from somewhere h0 is the initial hidden state and h0 is normally taken as a zero vector so this is actually just initialize the zeros and so for working out the first hidden state we calculate it based on the first word it's embedding by multiplying this embedding by a matrix we and that gives us the first hidden state but then you know as we go on we want to apply the same formula over again so we have just two parameter matrices in the recurrent neural network one matrix for multiplying input embeddings and one matrix for updating the hidden state of the network and so for the second word from its word embedding we multiply it by the we matrix we take the previous time steps hidden state and multiply it by the wh matrix and we use the two of those um to generate the new hidden state and precisely how we generate the new hidden state is then by shown on this equation on the left so we take the previous hidden state multiply it by wh we take the input embedding multiply it by w e we sum those two we add on a learn bias weight and then we put that through a non-linearity and although on this slide that non-linearity is written as sigma by far the most common non-linearity to use here actually is a tan h non linearity and so this is the core equation for a simple recurrent neural network and for each successive time step we're just going to keep on applying that to work out hidden states and then from those hidden states we can use them just like in our window classifier to predict what would be the next word so at any position we can take this hidden vector put it through a soft max layer which is multiplying by u matrix and adding on another bias and then making a soft max distribution out of that and that will then give us the probability distribution over next words what we saw here right this is the entire math of a simple recurrent neural network and next time i'll come back and say more about them but this is the entirety of of what you need to know in some sense for the computation of the forward model of a simple recurrent neural network so the advantages we have now as it can process at a text input of any length in theory at least it can use information from any number of steps back we'll talk more about in practice how well it actually works the model size is fixed it doesn't matter how much of a past context there is all we have is our wh and we parameters and at each time step we use exactly the same weights to update our hidden state um so there's a symmetry in how different inputs are processed in producing our predictions um rnns in practice though or these simple rnns and practice aren't perfect so um a disadvantage is that they're actually kind of slow because with this recurrent computation in some sense we are sort of stuck with having to have on the outside of for loop so we can do vector matrix multiplies on the inside here but really we have to do for time step equals 1 to n calculate the successive hidden states and so that's not a perfect neural net architecture and we'll discuss alternatives to that later and although in theory this model can access information any number of steps back in practice we find that it's pretty imperfect at doing that and that will then lead to more advanced forms of recurrent neural network that i'll talk about next time that are able to more effectively access past context okay i think i'll stop there for the day 
","['', 'transition-based dependency parsers', 'symbolic features', 'neural dependency parser', 'dense and compact feature representation', 'chine and manning parser', 'unlabeled attachment score', 'labeled attachment score', 'graph-based dependency parsing', 'language model', 'n-grams', 'markov assumption', 'markov models', 'conditional probability', 'engram language model', 'foreground language model', 'trigram model', 'word embedding', 'recurrent neural network (RNN)', 'hidden state', '']"
"okay hi everyone welcome back to cs224n so today is a pretty key lecture where we get through a number of important topics for neural networks especially as applied to natural language processing so right at the end of last time i started into current neural networks so we'll talk in detail more about current neural networks in the first part of the class and we'd emphasize language models um but then also getting a bit beyond that and then look at more advanced kinds of recurrent neural networks towards the end part of the class um i just wanted to sort of say a word before getting underway about the final project so hopefully by now you've started looking at assignment three which is the middle of the five assignments for the first half of the course and then in the second half of the course most of your effort goes into a final project so next week the thursday lecture is going to be about final projects and choosing the final project and tips for final projects etc so it's fine to delay thinking about final projects until next week if you want but you shouldn't delay it too so long because we do want you to get underway with what topic you're going to do for your final project if you are thinking about final projects you can find some info um on the website but note that the info that's there at the moment is still last year's information and it will be being updated over the coming week we'll also talk about project mentors if you've got ideas of people who on your own you can line up as a mentor that now be a good time to ask them about it and we'll sort of talk about what the alternatives are okay so um last lecture i introduced the idea of language models so probabilistic models that predict the probability of next words after a word sequence and then we looked at engram language models and started into a current neural network models so today we're going to talk more about the simple rnns we saw before talking about training rnn's and uses of rnns but then we'll also look into the problems that occur with rnns and how we might fix them these will motivate a more sophisticated rn architecture called lstms and we'll talk about other more complex rnn options bi-directional rnns and multi-layer rnns then next tuesday we're essentially going to ex further exploit and build on the rnn based architectures that we've been looking at to discuss how to build a newer machine translation system with the sequence to sequence and model with attention and effectively as that model um is what you'll use in assignment for but it also means that you'll be using all of the stuff that we're talking about today okay so if you remember from last time this was the idea of a simple recurrent neural network language model so we had a sequence of words as our context for which we've looked up word embeddings and then their current neural network model ran this recurrent layer where at each point we have a previous hidden state which can just be zero at the beginning of a sequence and you have feeding it in to the next hidden state the previous hidden state and encoding and transform the coding of a word using this recurrent neural network equation that i have on the left that's very central and based on that you compute a new hidden representation for the next time step and you can repeat that along for successive time steps now we also usually want our recurrent neural networks to produce outputs so i only show it at the end here but at each time step we're then also going to generate an output and so to do that we're feeding the hidden layer into a soft max layer so we're doing another matrix multiply add on a bias put it through the soft max equation and that will then gives the probability distribution over words and we can use that to predict how likely it is that different words are going to occur after the students open there okay so i didn't i'd introduced that model but i hadn't really gone through the specifics of how we um train this model how we use it and evaluate it so let me um go through this now so here's how we train an rnn language model we get a big corpus of text just a lot of text and so we can regard that as just a long sequence of words x1 to xt and what we're going to do is feed it into the rnnlm so for each posit we're going to take prefixes of that sequence and based on each prefix we're going to want to predict the probability distribution for the word that comes next and then we're going to train our model by assessing how good a job we do about that and so the loss function we use is the loss function normally referred to as cross-entropy loss in the literature which is this negative log likelihood lasts so we are going to predict some word to come next well we we have a probability distribution over predictions of what word comes next and actually there was an actual next word in the text and so we say well what probability did you give to that word and maybe we gave it a probability estimate of 0.01 well it would have been great if we'd given a probability estimate of almost one because that meant we've almost certain that what did come next in our model and so we'll take a loss to the extent that we're giving the actual next word a predicted probability of less than one to then get an idea of how well we're doing over the entire um corpus we work out that loss at each position and then we work out the average loss of the entire training set so let's just go through that again more graphically in the next couple of slides so down the bottom here's our corpus of text we're running it through our simple recurrent neural network and at each position we've predicting a probability distribution over words we then say well actually at each position we know what word is actually next so when we're at time step one the actual next word is students because we can see it just to the right of us here and we say what probability estimate did you give to students and to the extent that it's not high it's not one we take a loss and then we go on to the time step two and we say well at time step two you predicted probability distribution over words the actual next word is opened so to the extent that you haven't given high probability to open you take a loss and then that repeats for in time step three we're hoping the model predict there at time step four we're hoping the model will predict exams and then to work out our overall loss we're then um averaging our per time step loss so in a way this is a pretty obvious thing to do but note that there is a little subtlety here and in particular this algorithm is referred to in the literature as teacher forcing and so what does that mean well you know you can imagine what you can do with a recurrent neural network is say okay just start generating maybe i'll give you a hint as to where to start i'll say the sentence starts the students and then let it run and see what it generates coming next um it might start saying the students have been locked out of the classroom or whatever it is right um and that we could say is well that's not very close to what the actual text says and somehow we want to learn from that and if you go in that direction there's a space of things you can do that leads into more complex algorithms such as reinforcement learning um but from the perspective of training these neural models that's unduly complex and unnecessary so we have this very simple way of doing things which is what we do is just predict one time step forward so we say we know that the prefix is the predict a probability distribution over the next word it's good to the extent that you give probability master opened okay now the prefix is the student's opened predict a probability distribution over the next word it's good to the extent that you give probability master there and so effectively at each step we're resetting to what was actually in the corpus so you know it's possible after the students opened the model thinks that by far the most probable thing to come next is ah or thus say i mean we don't actually use what the model suggested we penalize the model for not having suggested there but then we just go with what's actually in the corpus and ask it to predict again this is just a little side thing but it's an important part to know if you're actually training your own neural language model i sort of presented as one huge corpus that we chug through but in practice we don't chug through a whole corpus one step at a time what we do is we cut the whole corpus into shorter pieces which might commonly be sentences or documents or sometimes they're literally just pieces that are chopped right so you'll recall that stochastic gradient scent allows us to compute a loss and gradients from a small chunk of data and update so what we do is we take these small pieces compute gradients from those and update weights and repeat and in particular we get a lot more speed and efficiency and training if we aren't actually doing an update for just one sentence at a time but actually a batch of sentences so typically what we'll actually do is we'll feed to the model 32 sentences say of a similar length at the same time compute gradients for them um update weights and then get another batch of sentences to train on um how do we train i haven't sort of gone through the details of this i mean in one sense the answer is just like we talked about in lecture three um we use back propagation to get gradients and update parameters um but let's take at least a minute to go through the differences and subtleties of the current neural network case um and the central thing that's a bit you know as before we're going to take our loss and we're going to back propagate it to all of the parameters of the network everything from word embeddings to biases etc but the central bit that's a little bit different and is more complicated is that we have this wh matrix that runs along the sequence that we keep on applying to update um our hidden state so what's the derivative of jt of theta with respect to the repeated weight matrix wh and well the the answer to that is that what we do is we look at it in each position and work out what the partials are of gt with respect to wh in position one or position two position three position four etcetera right along the sequence and we just sum up all of those partials and that gives us a partial for jt with respect to wh overall um so the answer for a current neural networks is the gradient with respect to a repeated weight in our current network is the sum of the gradient with respect to each time it appears um and let me just then go through a little why that is the case but before i do that let me just note one gotcha i mean it's just not the case that this means it equals t times the partial of jt with respect to wh because we're using wh here here here here here through the sequence and for each of the places we use it there's a different upstream gradient that's being fed into it so each of the values in this sum will be completely different from each other well why we get this answer is essentially a consequence of what we talked about in the third lecture so to take the simplest case of that right that if you have a multi-variable function f of x y and you have two single variable functions x of t and y of t which are fed one input t well then the um simple um version of working out the derivative derivative of this function is you take the derivative down one path and you take the derivative down the other path and so in the slides in lecture three that was what was summarized on a couple of slides by the slogan gradient sum at outward branches so t has outward branches and so you take gradient here on the left gradient on the right and you sum them together and so really what's happening with the recurrent neural network is just many pieces generalization of this so we have one wh matrix and we're using it to keep on updating the hidden state at time 1 time 2 time 3 right through time t and so what we're going to get is that this has a lot of um outward branches and we're going to sum the gradient path at each one of them but what is this gradient path here it's kind of goes down here and then goes down there but you know actually the bottom part is that we're just using wh at each position so we have the partial of wh used at position i with respect to the partial of wh which is just our weight matrix for our current neural network so that's just one because you know we're just using the same matrix everywhere and so we are just then summing um the partials in each position that we use it okay um practically what does that mean in terms of how you compute this um well if you're doing it by hand um what happens is you start at the end just like this general lecture three story you work out derivatives with respect to the hidden layer and then with respect to wh at the last time step and so that gives you one update for wh but then you continue passing the gradient back to the t minus one time step and after a couple more steps of the chain rule you get another update for wh and you simply sum that onto your previous update for wh and then you go to ht minus 2 you get another update for wh and you sum that onto your update for wh and you go back all the way and you sum up the gradients as you go um and that gives you a total update um for wh um and so there's so two tricks here and i'll just mention um the two tricks you have to kind of separately sum the updates for wh and then once you've finished apply them all at once you don't want to actually be changing the wh matrix as you go because that's then invalid because um the forward calculations were done with the constant wh that you had from the previous state all through the network the second trick is well if you're doing this for sentences you can normally just go back to the beginning of the sentence but if you've got very long sequences this can really slow you down if you're having to sort of run this algorithm back for a huge amount of time so something people commonly do is what's called truncated back propagation through time where you choose some constant say 20 and you say well i'm just going to run this back propagation for 20 time steps sum those 20 gradients and then i'm just done that's what i'll update the wh matrix with and that works just fine okay so now given a corpus we can train a simple rnn and so that's good progress but this is a model that can also generate text in general so how do we generate text well just like in our engram language model we're going to generate text by repeated sampling so we're going to start off with an initial state and um yeah this slide is imperfect um so the initial state for the hidden state um is is normally just taken as a zero vector and well then we need to have something for a first input and on this slide the first input is shown as the first word my and if you want to feed a starting point you could feed my but a lot of the time you'd like to generate a sentence from nothing and if you want to do that what's conventional is to additionally have a beginning of sequence token which is a special token so you'll feed in the beginning of sequence token in at the beginning as the first token it has an embedding and then you use the rnn update and then you generate using the softmax and next word and um well you generate a probability of probability distribution over next words and then at that point you sample from that and it chooses some word like favorite and so then the trick is for doing generation that you take this word that you sampled and you copy it back down to the input and then you feed it in as an import next step if you are an n sample from the softmax get another word and just keep repeating this over and over again and you start generating the text and how you end is as well as having a beginning of sequence um special symbol you usually have an end of sequence special symbol and at some point um the recurrent neural network will generate the end of um sequence symbol and then you say okay i'm done i'm finished generating text so before going on for the more of the difficult content of the lecture we can just have a little bit of fun with this and try training up and generating text with our current neural network model so you can generate you can train an rnn on any kind of text and so that means one of the fun things that you can do is generate text in different styles based on what you could train it from so here harry potter is a there is a fair amount of a corpus of text so you can train rnn lm on the harry potter books and then say dolph and generate some text and it'll generate text like this sorry how harry shouted panicking i'll leave those brooms in london are they no idea said nearly headless nick casting low close by cedric carrying the last bit of treacle charms from harry's shoulder and to answer him the common room pushed upon it four arms held a shining knob from when the spider hadn't felt it seemed he reached the teams too um well so on the one hand that's still kind of a bit incoherent as a story on the other hand it sort of sounds like harry potter and certainly the kind of you know vocabulary and constructions in users and i think you'd agree that you know even though it gets sort of incoherent it's sort of more coherent than what we got from an engram language model um when i showed a generation in the last lecture you can choose a very different style of text so you could instead train the model on a bunch of cookbooks and if you do that you can then say generate based on what you've learned about cookbooks and it'll just generate a recipe so here's a recipe chocolate ranch bbq categories yield six servings two tablespoons of parmesan cheese chopped and one cup of coconut milk three eggs beaten place each pasta over layers of lumps shaped mixture into the moderate oven and simmer until firm serve hot embodied fresh mustard orange and cheese combine the cheese and salt together the dough in a large skillet add the ingredients and stir in the chocolate and pepper so you know um this recipe makes um no sense and it's sufficiently um incoherent there's actually even no danger that you'll try cooking this at home um but you know something that's interesting is although you know there's really just isn't a recipe and the things that are done in the instructions have no relation um to the ingredients the the thing that's interesting that it has learned is this recurrent neural network model is that it's really mastered the overall structure of a recipe it knows the recipe has a title it often tells you about how many people it serves it lists the ingredients and then it has instructions um to make it so that's sort of fairly impressive in some sense for high level text structuring um so the one other thing i wanted to mention was when i say you can train an rnn language model in any kind of text the other difference from where we were in in gram language models was on engram language models that just meant counting engrams and meant it took um two minutes or even on a large corpus with any modern computer training your rnn lm actually can then be a time intensive activity and you can spend hours doing that as you might find next week when you're training machine translation models okay how do we decide if our models are good or not um so the standard evaluation metric for language models is what's called perplexity and what perplexity is is um kind of like when you were training your model you use teacher forcing over a piece of text that's a different piece of test text which isn't text that was in the training data and you say well given a sequence of t words what probability do you give to the actual t plus one word and you repeat that at each position and then you take the inverse of that probability and raise it to the one on t for the length of your test text sample and that number is the perplexity so it's a geometric mean of the inverse probabilities now um after that explanation perhaps an easier way to think of it is that the perplexity is simply the cross entropy loss that i introduced before exponentiated so but you know it's now the other way around so low perplexity um is better so there's actually an interesting story about these perplexities um so a famous figure in the development of probabilistic and machine learning approaches to natural language processing is fred gellanek who died a few years ago and he was trying to interest people in the idea of using probability models and machine learning um for natural language processing at a time i this is the 1970s and early 1980s when nearly everyone in the field of ai was still in the thrall of logic based models and blackboard architectures and things like that for artificial intelligence systems and so fred gellarnek was actually an information theorist by background um and who then got interested in working with speech and then language data so at that time the stuff that's this sort of um exponential or using cross-entropy losses was completely bread and butter to fred jelinek but he'd found that no one in ai could understand the bottom half of the slide and so he wanted to come up with something simple that ai people at that time could understand and perplexity has a kind of a simple interpretation you can tell people so if you get a perplexity of 53 that means how uncertain you are of the next word is equivalent to the uncertainty of that you're tossing a 53-sided dice and it's coming up as a one right so um that was kind of an easy simple metric and so he introduced um that idea um but you know i guess things stick and to this day um everyone evaluates their language models by providing perplexity numbers and so here are some perplexity numbers so traditional engram language models commonly had perplexities over a hundred but if you made them really big and really careful you carefully you could get them down into a number like 67 as people started to build more advanced recurrent neural networks especially as they moved beyond the kind of simple rnn's which is all i've shown you so far which one of is in the second site line of the slide into lstms which i talk about later in this course that people started producing much better perplexities and here we're getting perplexities down to 30 and this is results actually from a few years ago so nowadays people get perplexities of even lower than 30. um you have to be realistic in what you can expect right because if you're just generally generating a text some words are almost determined um so you know if it's something like um you know zoom gave the man a napkin he said thank you know basically a hundred percent you should be able to say the word that comes next is you um and so that you can predict really well but um you know if it's a lot of other sentences like um he looked out the window and saw uh something right no probability in the model model in the world can give a very good estimate of what's actually going to be coming next to that point and so that gives us the sort of residual uncertainty that leads to perplexities that are on average might be around 20 or something okay um so we've talked a lot about language models now why should we care about language modeling you know well there's sort of an intellectual scientific answer that says this is a benchmark task right if we what we want to do is build machine learning models of language and our ability to predict what word will come next in the context that shows how well we understand both the structure of language and the structure of the human world that language talks about um but there's a much more practical answer than that um which is you know language models are really the secret tool of natural language processing so if you're talking to any nlp person and you've got almost any task it's quite likely they'll say oh i bet we could use a language model for that and so language models are sort of used as a not the whole solution but a part of almost any task any task that involves generating or estimating the probability of text so you can use it for predictive typing speech recognition grammar correction identifying authors machine translation summarization dialogue just about anything you do with natural language involves language models and we'll see examples of that in following classes including next tuesday where we're using language models for machine translation okay so a language model is just a system that predicts the next word um a recurrent neural network is a family of neural networks which can take sequential input of any length they reuse the same weights to generate a hidden state and optionally but commonly an output on each step note that these two things are different so we've talked about two ways that you could build language models but one of them is rnn's being a great way but rnns can also be used for a lot of other things so let me just quickly preview a few of the things you can do with rnns so there are lots of tasks that people want to do in nlp which are referred to as sequence tagging tasks where we'd like to take words of text and do some kind of classification along the sequence so one simple common one is to give words parts of speech that is a determiner startled as an adjective cat is a noun knocked as a verb um and well you can do this straightforwardly by using a current neural network as a sequential classifier where it's now going to generate parts of speech rather than the next word you can use a recurrent neural network the sentiment classification well this time we don't actually want to generate um and outputted each word necessarily but we want to know what the overall sentiment looks like so somehow we want to get out a sentence encoding that we can perhaps put through another neural network layer to judge whether the sentence is positive or negative well the simplest way to do that is to think well after i've run my lstm through the whole sentence actually this final hidden state it's encoded the whole sentence because remember i updated that hidden state based on each previous word and so you could say that this is the whole meaning of the sentence so let's just say that is the sentence encoding and then put an extra classifier layer on that with something like a softmax classifier um that method has been used and it actually works reasonably well and if you sort of train this model end to end well it's actually then motivated to preserve sentiment information in the hidden state of the current neural network because that will allow it to better predict the sentiment of the whole sentence which is the final task and hence loss function that we're giving the network but it turns out that you can commonly do better than that by actually doing things like feeding all hidden states into the sentence and coding perhaps by making the sentence encoding an element-wise max or an element wise mean of all the hidden states because this then more symmetrically encodes the hidden state over each time step another big use of recurrent neural networks is what i'll call language encoder module uses so anytime you have some text for example here we have a question of what nationality was beethoven we'd like to construct some kind of newer representation of this so one way to do it is to run the current neural network over it and then just like last time to either take the final hidden state or take some kind of function of all the hidden states and say that's the sentence representation and we could do the same thing for the context so for question answering we're going to build some more neural net structure on top of that and we'll learn more about that in a couple of weeks um when we have the question answering lecture but the key thing is what we built so far we use to get sentence representation so it's a language encoder module so that was the language encoding part we can also use rnns to decode into language and that's commonly used in speech recognition machine translation summarization so if we have a speech recognizer the input is an audio signal and what we want to do is decode that into language well what we could do is use some function of the input which is probably itself going to be neural net as the initial hidden state of our rnn lm and then we say start generating text based on that and so it should then we generate word at a time by the method that we just looked at we turn the speech into text so this is an example of a conditional language model because we're now generating text conditioned on the speech signal and a lot of the time you can do interesting more advanced things with recurrent neural networks by building conditional language models another place you can use conditional language models is for text classification tasks and including sentiment classification so if you can condition your language model based on a kind of sentiment you can build a kind of classifier for that and another use that we'll see a lot of next class is for machine translation okay so that's the end of the intro to doing things with recurrent neural networks and language models now i want to move on and tell you about the fact that everything is not perfect and these recurrent neural networks tend to have a couple of problems and we'll talk about those and then in part that'll then motivate coming up with a more advanced recurrent neural network architecture so the first problem to be mentioned is the idea of what's called vanishing gradients and what does that mean well at the end of our sequence we have some overall loss that we're calculating and well what we want to do is back propagate that loss and we want to back propagate it right along the sequence and so we're working out the partials of j4 with respect to the hidden state at time one and when we have a longer sequence we'll be working out the partials of j20 with respect to the hidden state at time one and how do we do that well how we do it is by composition and the chain rule we've got a big long chain roll along the whole sequence um well if we're doing that you know we're multiplying a ton of things together and so the danger of what tends to happen is that as we do these multiplications a lot of time these partials successive hidden states becomes small and so what happens is as we go along the gradient gets smaller and smaller and smaller and starts to peter out and to the extent that it peters out well then we've kind of got no upstream gradient and therefore we won't be changing the parameters at all and that turns out to be pretty problematic um so the next couple of slides sort of um say a little bit about the why and how this happens what's presented here is a kind of only semi formal wave your hands at the kind of problems that you might expect um if you really want to sort of get into all the details of this um you should look at the couple of papers that are mentioned in small print at the bottom of the slide but at any rate if you remember that this is our basic um recurrent neural network equation well let's consider an easy case suppose we sort of get rid of our non-linearity and just assume that it's an identity function okay so then when we're working out the partials of the hidden state with respect to the previous hidden state we can work those out in the usual way according to the chain rule and then if sigma is simply the identity function well then everything gets really easy for us so only the the sigma just goes away and only the first term involves um h at time t minus one so the later terms go away and so um our gradient ends up as wh well that's doing it for just one time step what happens when you want to work out these partials a number of times steps away so we want to work it out the partial of time step i with respect to j um well what we end up with is a product of the partials of successive time steps and while each of those is coming out as wh and so we end up getting wh raised to the elf power and well our potential problem is that if wh is small in some sense then this term gets exponentially problematic i it becomes vanishingly small as our sequence length becomes long well what can we mean by small well a matrix is small if its eigenvalues are all less than one so we can rewrite what's happening with this success of multiplication using eigenvalues and eigenvectors um and i should say that all i can vector values less than one is a sufficient but not necessary condition for what i'm about to say um right so we can rewrite things using the eigenvectors as a basis and if we do that we end up getting the eigenvalues being raised to the eighth power and so if all of our eigen values are less than one if we're taking a number less than one and then raising it to the eighth power that's going to approach zero as the sequence length grows and so the gradient vanishes okay now the reality is more complex than that because actually we always use a non-linear activation sigma but you know in principle it's sort of the same thing um apart from we have to consider in the effect of the non-linear activation okay so why is this a problem that the gradients disappear well suppose we're wanting to look at the influence of time steps well in the future on the representations we want to have early in the sentence well what's happening late in the sentence just isn't going to be giving much information about what we should be storing in the h at time 1 vector whereas on the other hand the loss at time step two is going to be giving a lot of information at what should be stored in the hidden vector at time step one so the end result of that is that what happens is these simple rnn's are very good at modeling nearby effects but they're not good at all at modeling long-term effects because the gradient signal from far away is just lost too much and therefore the model never effectively gets to learn what information from far away it would be useful to preserve into the future so let's consider that concretely um for the example of language models that we've worked on so here's a piece of text um when she tried to print her tickets she found that the printer was out of toner she went to the stationery store to buy more toner it was very overpriced after installing the toner into the printer she finally printed her and well you're all smart human beings i trust you can all guess what the word that comes next is it should be tickets but well the problem is that for the rnn to start to learn cases like this it would have to carry through in its hidden state a memory of the word tickets for sort of whatever it is about 30 hidden state updates and well we'll train on this um example and so we'll be wanting it to predict tickets is the next word and so a gradient update will be sent right back through the hidden states of the lstm corresponding to this sentence and that should tell the model it's good to preserve information about the word tickers because that might be useful in the future here it was useful in the future but the problem is that the gradient signal will just become far too weak out over after a bunch of words and it just never learns that dependency and so what we find in practice is the model is just unable to predict similar long-distance dependencies at test time i've spent quite a long time on vanishing gradients and then really vanishing gradients are the big problem in practice um with using recurrent neural networks over long sequences but you know i have to do justice to the fact that you can actually also have the opposite problem you can also have exploding gradients so if a gradient becomes too big that's also a problem and it's a problem because the stochastic gradient update step becomes too big right so remember that our parameter update is based on the product of the learning rate and the gradient so if your gradient is huge right you've calculated oh it's got a lot of slope here this has a slope of 10 000 then your parameter update can be arbitrarily large and that's potentially problematic that can cause a bad update where you take a huge step and you end up at a weird and bad parameter configuration so you sort of think you're coming up with a to a steep hill to climb and well you want to be climbing the hill to high likelihood but actually the gradient is so sleek steep that you make an enormous update and then suddenly your parameters are over in iowa and you've lost your hill altogether there's also the practical difficulty that we only have so much resolution now floating point numbers so if your gradient gets too steep um you start getting not a numbers in your calculations which ruin all your hard training work um we use a kind of an easy fix to this which is called gradient clipping which is we choose some reasonable number and we say we're just not going to deal with gradients that are bigger than this number um a commonly used number is 20 you know some thing that's got a range of spread but not that high you know you can use 10 100 some we're sort of in that range um and if the norm of the gradient is greater than that threshold we simply just scale it down which means that we then make a smaller gradient update so we're still moving in exactly the same direction but we're taking a smaller step so doing this gradient clipping is important you know but it's an easy problem to solve okay um so the thing that we've still got left to solve is how to really solve this problem of vanishing gradients um so the problem is yeah these rnns just can't preserve information over many time steps and one way to think about that intuitively is at each time step we have a hidden state and the hidden state is being completely changed at each time step and it's being changed in a multiplicative manner by multiplying by wh and then putting it through um and non-linearity like maybe we could make some more progress if we could more flexibly maintain a memory in our recurrent neural network which we can manipulate in a more flexible manner that allows us to more easily preserve information and so this was a idea that people started thinking about and actually they started thinking about it a long time ago in the late 1990s [Music] and huck wright and schmidt hoover came up with this idea that got called long short term memory rnns as a solution to the problem of vanishing gradients i mean so this 1997 paper is the paper you always see cited for lstms but you know actually in terms of what we now understand as an lstm it was missing part of it in fact it's missing what in retrospect has turned out to be the most important part of the modern lstm so really in some sense the real paper that the modern lstm is due to is this slightly later paper by gersh still schmidt hoover and cummins from 2000 um which additionally introduces the forget gate that i'll explain in a minute um yeah so um so this was some very clever stuff that was introduced and it turned out later to have an enormous impact um if i just diverge from the technical part for one more moment that you know for those of you who these days um think that mastering your networks is the path to fame and fortune the funny thing is you know at the time that this work was done that just was not true right very few people were interested in neural networks and although long short-term memories have turned out to be one of the most important successful and influential ideas in neural networks for the following 25 years really the original authors didn't get recognition for that so both of them are now professors at german universities but hawk writer moved over into doing bioinformatics work to find um something to do and guess actually is doing kind of multimedia studies um so um that's the fates of history um okay so what is an lstm so the a crucial innovation of an lstm is to say well rather than just having one hidden vector in the recurrent model we're going to build a model with two hidden vectors at each time step one of which is still called the hidden state h and the other of which is called the cell state now you know arguably in retrospect these were named wrongly because as you'll see when we look at in more detail in some sense the cell is more equivalent to the hidden state of the simple rnn then vice versa but we're just going with the names that everybody uses so both of these are vectors of length n um and it's going to be the cell that stores long-term information and so we want to have something that's more like memory so the meaning like ram and the computer um so the cell is designed so you can read from it you can erase parts of it and you can write new information to the cell um and the interesting part of an lstm is then it's got control structures to decide how you do that so the selection of which information to erase write and read is controlled by probabilistic gates so the gates are also vectors of length n and on each time step um we work out a state for the gate vectors so each element of the gate vectors is a probability so they can be open probability one close probability zero or somewhere in between and their value will be saying how much do you erase how much do you write how much do you read and so these are dynamic gates with a value that's computed based on the current context okay so in this next slide we go through the equations of an lstm but following this there are some more graphic slides which will probably be easier to absorb right so we again just like before it's our current neural network we have a sequence of inputs x and t and we're going to at each time step compute a cell state and a hidden state so how do we do that so firstly we're going to compute values of the three gates and so we're computing the gate values using an equation that's identical to the equation for the simple recurrent neural network but in particular um oops sorry i'll just just say what the gates are first so there's a forget gate um which can we will control what is kept in the cell at the next time step versus what is forgotten there's an input gate which is going to determine which parts of a calculated new cell content get written to the cell memory and there's an output gate which is going to control what parts of the cell memory are moved over into the hidden state and so each of these is using the logistic function because we want them to be in each element of this vector a probability which will say whether to fully forget partially forget or fully fully remember yeah and the equation for each of these is exactly like the simple r and n equation but note of course that we've got different parameters for each one so we've got a forgetting weight matrix w with a forgetting bias and a forgetting multiplier of the input okay so then we have the other equations that really are the mechanics of the lstm so we have something that will calculate a new cell content so this is our candidate update and so for calculating the candidate update we're again essentially using exactly the same simple rnn equation apart from now it's usual to use tanh so you get something that as discussed last time is balanced around zero okay so then to actually update things we use our gates so for our new cell content what the idea is is that we want to remember some but probably not all of what we had in the cell from previous time steps and we want to store some but probably not all of the value that we've calculated as the new cell update and so the way we do that is we take um the previous cell content and then we take its hadamard product with the forget vector and then we add to it the hadamard product of the input gate times the candidate cell update and then for working out the new hidden state we then work out which parts of the cell to expose in the hidden state and so after taking a tan h transform of the cell we then take the hadamard product with the output gate and that gives us our hidden representation and as this hidden representation that we then put through a soft soft max layer to generate um our next output of our lstm or current neural network yeah so um the gates and the things that they're put with are vectors of size n and what we're doing is we're taking each element of them and multiplying them element wise to work out a new vector and then we get two vectors and that we're adding together so this um way of doing things element-wise you sort of don't really see in standard linear algebra course um it's referred to as the hadamard product it's represented by some kind of circle i mean actually in more modern work it's been more usual to represent it with this slightly bigger circle with the dot at the middle as the hadamard product symbol and someday i'll change these slides to be like that but i was lazy and redoing the equations but the other notation you do see quite often is just using the same little circle that you use for function composition to represent hadamard product okay so all of these things are being done as vectors of the same length n and the other thing that you might notice is that the candidate update and the forget import and output gates all have a very similar form the only difference is three logistics and one tanh and none of them depend on each other so all four of those can be calculated in parallel and if you want to have an efficient lstm implementation that's what you do okay so here's the more graphical presentation of this so these pictures come from chris ola and i guess he did such a nice job at producing pictures for lstms that almost everyone uses them these days and so this sort of pulls apart the computation graph of an lstm unit so blowing this up you've got from the previous time step both your cell and hidden recurrent vectors and so you feed the hidden vector from the previous time step and the new input x t into the computation of the gates which is happening down the bottom so you compute the forget gate and then you use the forget gate in a hadamard product here drawn as a actually a time symbol to forget some cell content you work out the input gate and then using the input gate and a regular recurrent neural network like computation you can compute candidate new cell content and so then you add those two together to get the new cell content which then heads out as the new cell content at time t but then you also have worked out an output gate and so then you take the cell content put it through another non-linearity and multi hadamard product with the output gate and that then gives you the new hidden state um so this is all kind of complex but as to understanding why something is different is happening here the thing to notice is that the cell state from t minus 1 is passing right through this to be the cell state at time t without very much happening to it so some of it is being deleted by the forget gate and then some new stuff is being written um to it as a result of using this candidate new cell content but the real secret of the um lstm is that new stuff is just being added to the cell with an addition right so in the simple rnn at each successive step you're doing a multiplication and that makes it incredibly difficult to learn to preserve information in the hidden state over a long period of time is it's not completely impossible but it's a very difficult thing to learn whereas with this new lstm architecture it's trivial to preserve information the cell from one time step to the next you just don't forget it and it'll carry right through with perhaps some new stuff added in um to also remember and so that's the sense in which the cell behaves much more like ram in a conventional computer that is storing stuff and extra stuff can be stored into it and other stuff can be deleted from it as you go along okay so the lstm architecture makes it much easier to preserve information from many time steps right so in particular standard practice with lstms is to initialize the forget gate to a one vector which it's just so that the starting point is to say preserve everything um from previous time steps and then it is then learning when it's appropriate to forget stuff and in contrast it's very hard to get or a simple rnn to preserve stuff for a very long time i mean what does that actually mean well you know i've put down some numbers here i mean you know how what you get in practice you know depends on a million things it depends on the nature of your data and how much data you have and what dimensionality your hidden states are blurry bloody blur but just to give you some idea of what's going on is typically if you train a simple recurrent neural network that its effect of memory its ability to be able to use things in the past to condition the future goes for about seven time steps you just really can't get it to remember stuff further back in the past than that whereas um for the lstm it's it's not complete magic it doesn't work forever but you know it's effectively able to remember and use things from much much further back so typically you find that with an lstm you can effectively remember and use things about a hundred times steps back and that's just enormously more useful for a lot of the natural language understanding tasks that we want to do and so that was precisely what the lstm was um designed to do and i mean so in particular just going back to its name i quite a few people mispassed its name the idea of its name was there's a concept of short-term memory which comes from psychology and it had been suggested for simple rnns that the hidden state of the rnn could be a model of human short-term memory and then there would be something somewhere else that would deal with human long-term memory but well people had found that this only gave you a very short short-term memory um so what um hock wright and schmidt uber were interested in was how we could give um construct models with long short term memory and so that then gave us this name of lstm um lstms don't guarantee that there are no vanishing or exploding gradients but in practice they provide they they don't tend to explode nearly the same way again that plus sign is crucial rather than a multiplication and so they're a much more effective way of learning long distance dependencies okay so despite the fact that lstms were developed around 1997 2000 it was really only in the early 2010s um that the world woke up to them and how successful they were so it was really around 2013 to 2015 that lstms sort of hit the world achieving state-of-the-art results on all kinds of problems um one of the first big demonstrations was for handwriting recognition um then speech recognition but then going on to a lot of um natural language tasks including machine translation parsing um vision and language tasks like image captioning as well of course using them for language models and around these years lstms became the dominant approach for most nlp tasks the easiest way to build a good strong model was to approach the problem with an lstm so now in 2021 actually lstms are starting to be supplanted or have been supplanted by other approaches particularly transformer models um which we'll get to in the class in a couple of weeks time so this is the sort of picture you can see so for many years there's been a machine translation conference and sort of bake off competition called wmt workshop on machine translation so if you look at the history of that in wmt 2014 [Music] there were zero neural machine translation systems in the competition 2014 was actually the first year that the success of um lstms for machine translation was proven in a conference paper um but nothing occurred in this competition um by 2016 [Music] everyone had jumped on ls dms is working great um and lots of people including the winner of the competition was using an lstm model um if you then jump ahead to 2019 um then there's relatively little use of lstms and the vast majority of people are now using transformers so things change quickly in your network land and i keep on having to rewrite these lectures um so quick further note on vanishing and exploding gradients is it only a problem with recurrent neural networks it's not it's actually a problem that also occurs anywhere where you have a lot of depth including feed forward and convolutional neural networks as any time when you've got long sequences of chain rules which give you multiplications the gradient can become vanishingly small as it back propagates um and so generally sort of lower layers are learned very slowly and are hard to train so there's been a lot of effort in other places as well to come up with different architectures that let you learn more efficiently in deeper networks and the commonest way to do that is to add more direct connections that allow the gradient to flow um so the big thing and vision in the last few years has been resnets where the res stands for residual connections and so the way they're made this picture is upside down so the input is at the top um is that you have these sort of two paths that are summed together one path is just an identity path and the other one goes through some neural network layers and so therefore its default behavior is just to preserve the input um which might sound a little bit like what we just saw for lstms um there are other methods that then being dense nets where you add skip connections forward to every laser layer highway nets were also actually developed by schmidhuber and sort of a reminiscent of what was done with lstms so rather than just having an identity connection as a resnet has it introduces an extra gate so it looks more like an lstm which says how much to send the input through the highway versus how much um to put it through a neural net layer and those two are then combined into the output um so essentially this problem occurs anywhere when you have a lot of depth in your layers of neural network but it first arose and turns out to be especially problematic with recurrent neural networks they're particularly unstable because of the fact that you're do you've got this one weight weight matrix that you're repeatedly using through the time sequence okay so we've got we've got a couple of questions um more or less about whether you would ever want to use an rn like a simple rnn instead of an lstm how does the lstm learn what to do with its gates can you opine on those things sure um so i think basically the answer is um you should never use a simple rnn these days you should always use an lstm i mean you know obviously that depends on what you're doing if you're wanting to do some kind of analytical paper or something you might prefer a simple rnn and it is the case that you can actually get decent results with simple rnns providing you're very careful to make sure that things aren't exploding nor vanishing but you know in practice getting simple rnns to work and preserve long context is incredibly difficult where you can train lstms and they will just work so really you should always just use an lstm now wait the second question was um i think there's a bit of confusion about like whether the gates are learned differently oh yeah so the gates the gates are also just learn so if we go back to these equations um you know this is the complete model and when we're training the model every one of these parameters so all of these w u and b's everything is simultaneously being trained by backprop so that what you hope and indeed it works is the model is learning what stuff should i remember for a long time versus what stuff should i forget what things in the input are important versus what things in the input don't really matter so it can learn things like our function words like don't really matter even though everyone uses them in english so you can just not worry about those so all of this is learned and the models do actually successfully learn gate values about what information is useful to preserve long term versus what information is really only useful short term for predicting the next one or two words finally um the uh the gradient improvements due to the so you said that the addition is really important between the new cell candidate and the cell state i don't think at least a couple of students have sort of questioned that so if you want to go over that again that might be useful sure um so what we would like is an easy way for memory to be preserved long term um and you know one way which is what resnets use is just to sort of completely have a direct path from ct minus one to ct and will preserve entirely the history so it's kind of there's a default action of preserving um information about the past long term lstms don't quite do that but they allow that function to be easy so you start off with the previous cell state and you can forget some of it by the forget gate so you can delete stuff out of your memory that's a useful operation and then while you're going to be able to update the content of the cell with this the the right operation that occurs in the plus where depending on the input gate some parts of what's in the cell will be added to but you can think of that adding as overlaying extra information everything that was in the cell that wasn't forgotten is still continuing on to the next time step and in particular um when you're doing the back propagation through time that there isn't um i want to say there isn't a multiplication between c t and c t minus one and there's this unfortunate um time symbol here but remember that's the hadamard product which is zeroing out part of it with the forget gate it's not a multiplication by a matrix like in the simple rnn i hope that's good um okay so there are a couple of other things that i uh wanted to get through before the end i guess i'm not going to have time to do both of them i think so i'll do the last one probably next time so these are actually simple and easy things um but they complete our picture um so we i sort of briefly alluded to this example of sentiment classification where what we could do is run an rnn maybe an lstm over a sentence call this our representation of the sentence and feed it into a soft max classifier to classify for um sentiment so what we're actually saying there is that we can regard the hidden state as a representation of a word in context that below that we have just a word vector for terribly but we then looked at our context and say okay we've now created a hidden state representation for the word terribly in the context of the movie was and that proves to be a really useful idea because words have different meanings in different contexts but it seems like there's a defect of what we've done here because our context only contains information from the left what about right context surely it'd also be useful to have the meaning of terribly depend on exciting because often words mean different things based on what follows them so you know if you have something like red wine it means something quite different from a red light so how could we deal with that well an easy way to deal with that would be to say well if we just want to come up with a neural encoding of a sentence we could have a second rnn with completely separate parameters learned and we could run it backwards through the sentence to get a backward representation of each word and then we can get an overall representation of each word in context by just concatenating those two representations and now we've got a representation of terribly that has both left and right context so we're simply running a forward rnn and when i say rnn here that just means any kind of recurrent neural network so commonly it'll be an lstm and a backward one and then at each time step we're just concatenating their representations um with each of these having separate weights and so then we regard this concatenated thing as the hidden state the contextual representation of a token at a particular time that we pass forward this is so common that people use a shortcut to denote that and they'll just draw this picture with two-sided arrows and when you see that picture with two-sided arrows it means that you're running two um rnn's one in each direction and then concatenating their results at each time step and that's what you're going to use later in the model okay but um so if you're doing an encoding problem like for sentiment classification or question answering using bi-directional rnns is a great thing to do but they're only applicable if you have access to the entire input sequence they're not applicable to language modeling because in a language model necessarily you have to generate the next word based on only the preceding context but if you do have the entire input sequence that bi-directionality gives you greater power and indeed that's been an idea that people have built on in subsequent work so um when we get to transformers in a couple of weeks um we'll spend plenty of time on the bert model where that acronym stands for bi-directional encoder representations from transformers so part of what's important in that model is the transformer but really a central point of the paper was to say that you could build more powerful models using transformers by again exploiting bi-directionality okay there's one tiny bit left on rnn's but i'll sneak it into next class and i'll call it the end for today and if there are other things you'd like to ask questions about you can find me on nooks again and just in just a minute okay so see you again next tuesday 
","['', 'recurrent neural networks (RNNs)', 'language models', 'engram language models', 'current neural network models', 'training RNNs', 'uses of RNNs', 'problems with RNNs', 'LSTMs', 'bi-directional RNNs', 'multi-layer RNNs', 'sequence to sequence models', 'attention', 'machine translation', 'soft max layer', 'cross-entropy loss', 'teacher forcing', 'stochastic gradient scent', 'back propagation', 'perplexity', '']"
"hello everyone and welcome back into week four um so for week four uh it's gonna come in two halves so today i'm going to talk about machine translation related topics and then in the second half of the week we take a little bit of a break from learning more and more neural networks topics and talk about final projects but also some practical tips for building your network systems so for today's lecture this is an important content for lecture so first of all i'm going to introduce a new task machine translation and it turns out that task is a major use case of a new architectural technique to teach you about deep learning which is sequence to sequence models and so we'll spend a lot of time on those and then there's a crucial way that's been developed to improve sequence to sequence models which is the idea of attention and so that's what i'll talk about in the final part of the class um just checking everyone's keeping up with what's happening so first of all um assignment three is due today so hopefully you've all gotten your new dependency passes parsing text well um at the same time assignment 4 is out today and really today's lecture is the primary content for what you'll be using for building your assignment four systems switching it up for a little for assignment four we give you a mighty two extra days um so you get nine days for it and it's due on thursday on the other hand do please be aware that assignment 4 is bigger and harder than the previous assignments so do make sure you get started on it early and then as i mentioned thursday i'll turn to final projects okay so let's get straight into this um with machine translation so very quickly i wanted to tell you a little bit about you know where we were and what we did before we get to neural machine translation and so let's do the pre-history of machine translation so machine translation is the task of translating a sentence x from one language which is called the source language to another language the target language forming a sentence y so we start off with a source language sentence x lo mein um and then we translate it and we get out the translation man is born free but everywhere he is in chains okay so there's our machine translation okay so in the early 1950s they started to be work on machine translation um and so it's actually thing about computer science if you find things um that have machine in the name most of them are old things um so um and this really kind of came about in the u.s context in the context of the cold war um so there was this desire to keep tabs on what the russians were doing and people had the idea that because some of the earliest computers had been so successful at um doing code breaking during the second world war then maybe we could set um early computers to work during the cold war to do translation um and hopefully this will play and you'll be able to hear it here's a little video clip um showing some of the earliest work um in machine translation from 1954. [Music] they hadn't reckoned with ambiguity when they set out to use computers to translate languages a 500 000 super calculator most versatile electronic brain known translates russian into english instead of mathematical wizardry a sentence in russian one of the first non-numerical applications of computers it was hyped as the solution to the cold war obsession of keeping tabs on what the russians were doing claims were made that the computer would replace most human translators of course you're just in the experimental stage when you go in for full scale production what will the capacity be we should be able to do about with a modern commercial computer uh about one to two million worth an hour and this will be quite an adequate speed to cope with the whole output of the soviet union in just a few hours computer time a week when do you have to be able to achieve the speed if our experiments go well then perhaps within uh five years or so and finally mr mcdaniel does this mean the end of human translators it's a yes for uh translators of scientific and technical material but as regards poetry and novels no i don't think we'll ever replace the translators of that type of material mr mcdaniel thank you very much but despite the hype it ran into deep trouble um yeah so the experiments did not go well um and so you know in retrospect it's not very surprising that the early work did not um work out very well i mean this was in the sort of really beginning of the computer age in the 1950s but it was also the beginning of you know people starting to understand the science of human languages the field of linguistics so really people had not much understanding of either side of what was happening um so what you had was people are trying to write systems on really incredibly primitive computers right you know it's probably the case that now if you if you have a usbc power brick that it has more computational capacity inside it than the computers that they were using to translate um and so effectively what you were getting were very simple rule-based systems and word lookup so there was so like dictionary look up a word and get its translation but that just didn't work well because human languages are much more complex than that often words have many meanings and different senses as we've sort of discussed about a bit often there are idioms you need to understand the grammar to rewrite the sentences so for all sorts of reasons it didn't work well and this idea was largely canned in particular there was a famous us government report in the mid 1960s the alpaca report which basically concluded this wasn't working oops okay work then did revive in ai at doing rule-based methods of machine translation in the um 90s but when things really became alive um was once you got into the mid 90s and when they were in the period of statistical nlp that we've seen in other places in the course and then the idea began can we start with just data about translation ie sentences and their translations and learn a probabilistic model that can predict the translations of fresh sentences so suppose we're translating french into english so what we want to do is build a probabilistic model that given a french sentence we can say what's the probability of different english translations and then we'll choose the most likely translation um we can then found it was found to felicitus to break this down into two components by just reversing this with bayes rule so if instead we had a probability over english sentences p of y um and then a probability of a french sentence given an english sentence that people were able to make more progress and it's not immediately obvious as to why this should be because this is just sort of a trivial rewrite with bayes rule but it allowed the problem to be separated into two parts which proved to be more tractable so on the left hand side you effectively had a translation model where you could just give a probability of words or phrases being translated between the two languages without having to bother about the structural word order of the languages and then on the right hand you saw precisely what we spent a long time with last week which is this is just a probabilistic language model so if we have a very good model of what good fluent english sentences sound like which we can build just from monolingual data we can then get it to make sure we're producing sentences that sound good while the translation model hopefully puts the right words into them so how do we learn the translation model since we haven't covered that so the starting point was to get a large amount of parallel data which is human translated sentences and this point is mandatory that i show a particular a picture of the rosetta stone which is the famous original piece of parallel data that allowed the decoding of egyptian hieroglyphs because it had the same piece of text in different languages in the modern world there are fortunately for people who build natural language processing systems quite a few places where parallel data is produced in large quantities so the european union produces a huge amount of parallel text across european languages the french sorry not the french the canadian parliament conveniently produces parallel text between french and english and even a limited amount in an institute canadian eskimo and then the hong kong parliament produces um english and chinese so there's a fair availability from different sources and we can use that to build models so how do we do it though all we have is these sentences and it's not quite obvious how to build a probabilistic model out of those well as before what we want to do is break this problem down so in this case what we're going to do is introduce an extra variable which is an alignment variable so a is the alignment variable which is going to give a word level or sometimes phrase level correspondence between parts of the source sentence and the target sentence so this is an example of an alignment and so if we could induce this alignment between the two sentences then we have can have probabilities of pieces of how likely a word or a short phrase is translated in a particular way and in general you know alignment is working out the correspondence between words that is capturing the grammatical differences between languages so words will occur in different orders in different languages depending on whether it's a language that puts on the subject before the verb or the subject after the verb or the verb before both the subject and the object and the alignments will also capture something about differences about the ways the word languages do things so what we find is that we get every possibility of how words can align between languages so you can have words that don't get translated at all in the other language so in french you put a definite article the before country names like japan so when that gets translated to english you just get japan so there's no translation of the so it just goes away um on the other hand you can get many-to-one translations where one french word gets translated as several english words so for the last french word it's being translated as aboriginal people as multiple words um you can get the reverse where you can have several french words um that get translated as one english word so nissan application is getting translators implemented and you can get but even more complicated um one so here we sort of have four english words being translated as two french words but they don't really break down and translate each other well i mean these things don't only happen across languages they also happen within the language when you have different ways of saying the same thing so another way you might have um expressed the poor don't have any money is to say the poor are moneyless and that's much more similar um to how the french is being rendered here and so even english to english you have the same kind of alignment problem so in probabilistic or statistical machine translation is more commonly known what we wanted to do is learn these alignments and there's a bunch of sources of information you could use if you start with parallel sentences you can see how often words and phrases co-occur in parallel sentences you can look at their positions in the sentence and figure out what a good alignments but alignments are a categorical thing they're not probabilistic and so they are latent variables and so you need to use special learning algorithms like the expectation maximization algorithm for learning about latent variables in the olden days of cs224n before we started doing it all with deep learning we spent tons of cs224 in dealing with latent variable algorithms but these days we don't cover that at all and you're going to have to go off and see cs228 if you want to know more about that and you know we're not really expecting you to understand the details here but i did then want to say a bit more about how decoding was done um in a statistical machine translation system um so what we wanted to do is to say we had a translation model and a language model and we want to pick out the most likely why there's the translation of the sentence and what kind of process could we use to do that um well you know the naive thing is to say well let's just enumerate every possible y and calculate its probability but we can't possibly do that because there's a number of translation sentences in the target language that's exponential on the length of the sentence so that's way too expensive so we need to have some way to break it down more and well we had a simple way um for language models we just generated words one at a time and laid out the sentence and so that seems a reasonable thing to do but here we need to deal with the fact that things occur in different orders in source languages and in translations and so we do want to break it into pieces with an independence assumption like the language model but then we want a way of breaking things apart and exploring it in what's called a decoding process so this is the way it was done so we'd start with a source sentence so this is a german sentence and as is um standard in german you're getting this second position verb so that's probably not in the right position for where the english translation is going to be so we might need to rearrange the words so what we have is based on the translation model we have words or phrases that are reasonably likely um translations of each german word or sometimes a german phrase so these are effectively the lego pieces out of which we're going to want to create the translation and so then inside that what we're going making use of this data we're going to generate the translation piece by piece kind of like we did with our newer language models so we're going to start with an empty translation and then we're going to say well we want to use one of these lego pieces and so we could explore different possible ones so there's a search process but one of the possible pieces is we could translate er with he or we could start the sentence with r translating the second word so we could explore various likely possibilities and if we're guided by our language model it's probably much more likely to start the sentence with he than it is to start the sentence with r though r is not impossible okay and then the other thing we're doing with these little blotches of black at the top we're sort of recording which german words we've translated and so we explore forward in a translation process and we could decide that we could translate next the second word goes or we could translate the negation here and translate that as does not when we explore various continuations and in the process i'll go through in more detail later when we do the neural equivalent we sort of do this search where we explore likely translations and prune and eventually we've translated the whole of the input sentence and worked out a fairly likely translation he does not go home and that's what we'll use as the translation okay so in the period from about 1997 to around um 2013 um statistical machine translation was a huge research field the best systems were extremely complex and they had hundreds of details that i certainly haven't mentioned here the systems had lots of separately designed and built components so i mentioned a language model and a translation model but they had lots of other components for reordering models and inflection models and other things there was lots of feature engineering typically the models also made use of lots of extra resources um and there were lots of human effort to maintain um but nevertheless they were already fairly successful so um google translate launched in the mid-2000s and people thought wow this is amazing you could start to get sort of semi-decent automatic translations um for different web pages um but that was chugging along well enough and then we got to 2014 and really with enormous suddenness um people then worked out ways of doing um machine translation using a large neural network and these large neural networks proved to be just extremely successful and largely blew away everything that preceded it so for the next big part of the lecture what i'd like to do is tell you something about newer machine translation neural machine translation well it means you're using a new network to do machine translation but in practice it's meant slightly more than that it has meant that we're going to build one very large neural network which completely does translation end to end so we're going to have a large neural network we're going to feed in the source sentence into the input and what's going to come out as the output of the neural network is the translation of the sentence we're going to train that model end-to-end on parallel sentences and it's the entire system rather than being lots of separate components as in an old-fashioned machine translation system and we'll see that in a bit um so these neural network architectures are called sequence to sequence models or commonly abbreviated seek to seek um and um they involve two neural networks here it says two rnns the version i'm um presenting now has two rnns but more generally they involve two neural networks there's one neural network that is going to encode the source sentence so if we have a source sentence here we're going to encode that sentence and well we know about a way that we can do that so using the kind of lstms that we saw last class we can start at the beginning and go through a sentence and update the hidden state each time and that will give us a representation of the content of the source sentence so that's the first sequence model um which encodes the source sentence and we'll use the idea that the final hidden state of the encoder rnn is going to in sense represent um the source sentence and we're going to feed it indirectly as the initial hidden state for the decoder rnn so then on the other side of the picture we have our decoder rnn and it's a language model that's going to generate a target sentence conditioned on the final hidden state of the encoder rnn so we're going to start with the input of start symbol we're going to feed in the hidden state from the encoder rnn and now this second green rnn has completely separate parameters i might just emphasize but we do the same kind of lstm computations and generate a first word of the sentence he and so then doing lstm generation just like last class we copy that down as the next input we run the next step of the lstm generate another word hit copy it down and chug along and we've translated the sentence right so this is showing um the test time behavior um when we're generating the next sentence for the training time behavior when we have parallel sentences we're still using the same kind of sequence to sequence model but we're doing it with the decoder part just like um training a language model where we're wanting to do teacher forcing and predict each word that's actually found in the source language sentence um sequences sequence models have been an incredibly powerful widely used workhorse in new neural networks for nlp so although you know historically machine translation was the first big use of them and is sort of the canonical use they're used everywhere else as well so you can do many other nlp tasks for them so you can do summarization you can think of text summarization as translating a long text into a short text but you can use them for other things that are in no way a translation whatsoever so they're commonly used for neural dialogue systems so the encoder will encode um the previous two utterances say and then you will use the decoder to generate a neck star trends um some other uses are even freakier but have proven to be quite successful um so if you have any way of representing the paths of a sentence as a string and if you sort of think a little it's fairly obvious how you can turn the paths of a sentence into a string by just making use of extra syntax like parentheses or putting in explicit words that are saying left arc right arc um shifts like the transition systems that you used for assignment three well then we could say let's use the encoder feed the input sentence to the encoder and let it output the transition sequence of our dependency parser and somewhat surprisingly that actually works well as another way to build a dependency parser or other kinds of parser these models have also been applied not just to natural languages but to other kinds of languages including music and also programming language code so you can train a seek to seek system where it reads in pseudocode in natural language and it generates out python code and if you have a good enough one it can do the assignment for you um so the essential new idea here with our sequence to sequence models is we have an example of conditional language models so previously the main thing we were doing was just so start at the beginning of the sentence and generate a sentence based on nothing but here we have something that is going to determine or partially determined that is going to condition what we should produce so we have a source sentence and that's going to strongly determine what is a good translation and so to achieve that what we're going to do is have some way of transferring information about the source sentence from the encoder to trigger what the decoder should do and the two standard ways of doing that are you either feed in a hidden state as the initial hidden state to the decoder or sometimes you will feed something in as the initial input to the decoder and so the in your mainstream translation our we're directly calculating this conditional model probability of target language sentence given source language sentence and so at each step as we break down the word by word generation that we're conditioning not only on previous words of the target language but also each time on our source language sentence x because of this we actually know a ton more about what our sentence that we generate should be so if you look at the perplexities of these kind of conditional language models you will find and like the numbers i showed last time they usually have almost frequently low perplexities that you will have models with perplexities that are something like four or even less sometimes you know 2.5 because you get a lot of information about what words you should be generating okay so then we have the same questions as we had for language models in general how to train a neural machine translation system and then how to use it at runtime so let's go through both of those in a bit more detail so the first step is we get a large parallel corpus so we run off to the european union for example and we grab a lot of parallel english french data from the european parliament proceedings so then once we have our parallel sentences um what we're going to do is take um batches of source sentences and target sentences will encode the source sentence with our encoder lstm will feed its final hidden state into a target lstm and this one we are now then going to train word by word by comparing what it predicts is the most likely word to be produced versus what the actual first word and then the actual second word is and to the extent that we get it wrong we're going to suffer some loss so this is going to be the negative log probability of generating the correct next word he and so on along the sentence and so in the same way that we saw last time for language models we can work out our overall loss for the sentence doing this teacher forcing style generate one word at a time calculate a loss relative to the word that you should have produced and so that loss then gives us information that we can back propagate through the entire network and the crucial thing about these sequence the sequence models that has made them extremely successful in practice is that the entire thing is optimized as a single system end to end so starting with our final loss we back propagate it right through the system so we not only update all the parameters of the decoder model but we also update all of the parameters of the encoder model which in turn will influence what conditioning gets passed over from the encoder to the decoder um so this moment is a good moment for me to return um to the three slides that i um skipped i'm running out of time at the end of last time which is to mention multi-layer rnns um so the rnn's that we've looked at so far uh already deep on one dimension then unroll horizontally over many time steps but they've been shallow in that there's just been a single layer of recurrent structure above our sentences we can also make them deepen the other dimension by applying multiple rnn's on top of each other and this gives us some multi-layer rnn uh often also called a stacked rnn and having a multi-layer rnn allows us the network to compute more complex representations so simply put the lower rnns tend to compute lower level features and the higher rnns should compute higher level features and just like in other neural networks whether it's feed forward networks or the kind of networks you see in vision systems you get much greater power and success by having a stack of multi multiple layers of recurrent neural networks right that you might think that all there are two things i could do i could have a single lstm with a hidden state of dimension 2000 or i could have four layers of lstms with a hidden state of 500 each and it shouldn't make any difference because i've got the same number of parameters roughly but that's not true in practice it does make a big difference and multi-layer or stacked rnns are more powerful so uh could i ask you um there's a good student question here about what lower level versus higher level features mean in this context sure um yeah so i mean in some sense these are kind of um somewhat flimsy ways um you know um terms this meaning isn't precise um but typically what that's meaning is that lower level features and knowing sort of more basic things about words and phrases so that commonly might be things like what part of speech is this word or are these words the name of a person or the name of a company whereas higher level features refer to things that are at a higher semantic level so knowing more about the overall structure of a sentence knowing something about what it means whether a phrase has positive or negative connotations um what its um semantics are when you put together several words into an idiomatic phrase um roughly the higher level kinds of things okay jump ahead okay so when we build um one of these end-to-end neural machine translation systems if we want them to work well single layer lstm encoder decode in your machine translation systems just don't work well but you can build something that is no more complex than the model that i've just explained now that does work pretty well by making it a multi-layer stacked lstm neural machine translation system so therefore the picture looks like this so we've got this multi-layer lstm that's going through the source sentence and so now at each point in time we calculate a new hidden representation that rather than stopping there we sort of feed it as into the input into another layer of lstm and we calculate in the standard way it's new hidden representation and the output of it we feed into a third layer of lstm and so we run that right along and so our representation of the source sentence from our encoder is then this stack of three hidden layers and then that we use um to then feed in as the initial um as the initial hidden layer into then sort of generating translations or for training the model of comparing to losses so this is kind of what the picture of a lstm encoder decoder your machine translation system really looks like so in particular um you know to give you some idea of that um so a 2017 paper um by denny brits and others that what they found was that for the encoder rnn it worked best if it had two to four layers and four layers was best for the decoder rnn and the details here like for a lot of neural nets depends so much on what you're doing and how much data you have and things like that but you know as rules of thumb to have in your head it's almost invariably the case that having a two layer lstm works a lot better than having a one layer lstm after that things become much less clear you know it's not so infrequent that if you try three layers it's a fraction better than two but not really and if you try four layers it's actually getting worse again you know it depends on how much data etc you have at any rate um it's normally very hard with the kind of model architecture that i just showed back here to get better results with more than four layers of lstm normally to do deeper lstm models and get even better results you have to be adding extra skip connections of the kind that i talked about at the very end of the last class next week john is going to talk about transformer-based networks in contrast for fairly fundamental reasons they're typically much deeper but we'll leave discussing them until we get on further um so that was how we trained the model um so let's just go a bit more through what the possibilities are for decoding and explore a more complex form of decoding than we've looked at the simplest way to decode is the one that we've presented so far so that we have our lstm we start generate a hidden state it has a probability distribution over words and you choose the most probable one the arg max and you say he and you copy it down and you repeat over so doing this is referred to as greedy decoding taking the most probable word on each step and it's sort of the obvious thing to do and doesn't seem like it could be a bad thing to do but it turns out that it actually can be a fairly problematic thing to do and the idea of that is that you know with greedy decoding you're sort of taking locally what seems the best choice and then you're stuck with it and you have no way to undo decisions um so if um these examples have been using this sentence about he hit me with a pie going from translating from french to english so you know if you start off and you say okay ill the first word in the translation should be he um that looks good but then you um and then you say well hit i'll generate hit then somehow the model thinks that the most likely next word is hit after hit is r and there are lots of reasons it could think so um because after hit most commonly there's a direct object now and then you know he hit a car he hit a roadblock right so that's prett sounds pretty likely but you know once you've generated it there's no way to go backwards and so you just have to keep on going from there and you may not be able to generate the translation you want um at best you can generate um he hit a pie oops um something so we'd like to be able to explore a bit more in generating our translations and well you know what could we do well you know i sort of mentioned this before looking at the statistical mt models overall what we'd like to do is find translations that maximize the probability of y given x and at least if we know what the length of that translation is we can do that as a product of generating a word at a time and so to have a full model we also have to have a probability distribution over how long the translation length would be so we could say this is the model and let's you know generate and score all possible sequences y using this model and that's where that then requires generating an exponential number of translations and is far far too expensive so beyond greedy decoding the the most important method that is used and you'll see lots of places is something called beam search decoding and so this isn't what neural well any kind of machine translation is one place where it's commonly used but this isn't a method that's specific to me machine translation you find lots of other places including all other kinds of sequences sequence models it's not the only other decoding method once when we got on to the language generation class we'll see a couple more but this is sort of the next one that you should know about so beam searches idea is that you're going to keep some hypotheses to make it more likely that you'll find a good generation while keeping the search tractable so what we do is ch choose a beam size and for neural mt the beam size is normally fairly small something like five to ten and at each step of the decoder we're going to keep track of the k most probable partial translations so initial subsequences of what we're generating which we call hypotheses um so a hypothesis which is then sort of the prefix of a translation has a score which is this log probability up to what's been generated so far so we can generate that in the typical way using our conditional language model so as written all of the scores are negative and so the least negative one i the highest probability one is the best one so what we want to do is search for high probability hypotheses so this is a heuristic method it's not guaranteed to find the highest probability decoding but at least it gives you more of a shot than simply doing greedy decoding so let's go through an example to see how it works so in this case so i can fit it on a slide the size of our beam is just two um though normally um it would actually be a bit bigger than that and the blue numbers are the scores of the prefixes so these are these log probabilities of a prefix so we start off with our start symbol and we're going to say okay what are the two most likely words to generate first according to our language model and so maybe the first two most likely words are he and i and there are their log probabilities then what we do next is for each of these k hypotheses we find what are likely words to follow them in particular we find one of the k most likely words to follow each of those so we might generate he hit he struck i was i got okay so at this point it sort of looks like we're heading down what will turn into an exponential um true size tree structure again but what we do now is we work out the scores of each of these partial hypotheses so we have four partial hypotheses he hit he struck i was i got and we can do that by taking the previous score that we have the partial hypothesis and adding on the log probability of generating the next word here here hit so this gives us scores for each hypothesis and then we can say which of those two partial hypotheses because our beam size k equals two have the highest score and so they are i was and he hit so we keep those two and ignore the rest and so then for those two we're going to generate um k hypotheses for the most likely following word he hit uh he hit me i was hit i was struck um and again now we want to find the k most likely hypotheses out of this full set and so that's going to be he struck me and i was oh no he struck me and he hit r um so we keep just those ones and then for each of those we generate the k most likely next words tart pie with on and then again we filter back down to size k by saying okay the two most likely things here are pie or width so we continue working on those generate things find the two most likely generate things find the two most likely and at this point um we would generate end of string and say okay we've got a complete hypothesis he struck me with a pie um and we could then trace back um through the tree um to obtain the full hypothesis for this sentence so that's most of the algorithm there's one more detail which is the stopping criterion so in greedy decoding we usually um decode until the model produces an n token and when it produces the end token we say we are done in beam search decoding different hypotheses may produce n tokens on different time steps and so we don't want to stop as soon as one path through the search tree has generated end because it could turn out there's a different path through the search tree which will still prove to be better so what we do is sort of put us put it aside as a complete hypothesis and continue exploring other hypotheses via our beam search and so usually we will then either stop when we've hit a cut-off length or when we've completed n complete hypotheses and then we'll look through the hypotheses that we've completed and say which is the best one of those and that's the one we'll use okay so at that point we have our list of completed hypotheses and we want to select the top one with the highest score well that's exactly what we've been computing each one has a probability that we've worked out but it turns out that we might not want to use that just so naively because there turns out to be a kind of a systematic problem which is you know not as a theorem but in general longer hypotheses have lower scores so if you think about this as probabilities of successively generating each word that basically at each step you're multiplying by another chance of generating the next word probability and commonly those might be you know 10 to the minus three ten to the minus two so just from the length of the sentence um your probabilities are getting much lower the longer that they go on in a way that appears to be unfair since although in some sense extremely long sentences aren't as likely as short ones they're not less likely by that much a lot of the time we produce long sentences so for example you know a newspaper um the median length of sentences is over 20 so you wouldn't want to be having a decoding model when translating news articles that sort of says oh just generate two-word sentences they're just way higher probability according to my language model um so the commonest way of dealing with that is that we normalize by length um so if we're working in log probabilities that means taking dividing through by the length of the sentence and then you have a per word log probability score and you know you can argue that this isn't quite right in some theoretical sense but in practice it works pretty well and it's very commonly used um neural translation has proven to be much much better i'll show you a couple of statistics and about that in a moment it has many advantages um it gives better performance the translations are better in particular they're more fluent because newer language models produce much more fluent sentences but also they much better use context because neural language models including conditional neural language models give us a very good way of conditioning on a lot of context in particular we can just run a long encoder and condition on the previous sentence or we can translate words well in context by making use of neural context neural models better understand phrase similarities and phrases that mean approximately the same thing um and then the technique of optimizing all parameters of the model end to end in a single large neural network is just proved to be a really powerful idea so previously a lot of the time people were building separate components and tuning them individually which just meant that they weren't actually optimal when put into a much bigger system so really a a hugely powerful guiding idea in neural network land is if you can sort of build one huge network and just optimize the entire thing end to end that will give you much better performance and component-wise systems we'll come back to the costs of that later in the course the the models are also actually great in other ways they actually require much less human effort to build there's no feature engineering there's in general no language specific components you're using the same method for all language pairs of course it's rare for things to be perfect in every way so neural machine translation systems also have some disadvantages compared to the older statistical machine translation systems they're less interpretable it's harder to see why they're doing what they're doing where you before you could actually look at phrase tables and they were useful so they're hard to debug they also tend to be sort of difficult to control um so compared to anything like writing rules um you can't really give much specification as if you'd like to say oh i'd like my translations to be more casual or something like that it's hard to know what they'll generate so there are various safety concerns um i'll show a few examples of that in just a minute but first before doing that quickly how do we evaluate machine translation um the best way to evaluate machine translation is to show a human being who's fluent in the source and target languages the sentences and get them to give judgment on how good a translation it is but that's expensive to do and might not even be possible if you don't have the right human beings around so a lot of work was put into finding automatic methods of scoring translations that were good enough and the most famous method of doing that is what's called blue and the way you do blue is you have a human translation or several human translations of the source sentence and you're comparing a machine-generated translation to those pre-given human written translations and you score them for similarity by calculating engram precisions i.e words that overlap between the computer and human written translation diagrams trigrams and four grams and then working out a geometric average between overlaps of n engrams plus there's a penalty for two short system translations so blue has proven to be a really useful measure but it's an imperfect measure that commonly there are many valid ways to translate a sentence and so there's some luck as to whether the human written translations you have happen to correspond to which to what might be a good translation from the system there's more to say about the details of blue and how it's implemented but you're going to see all of that doing assignment 4 because you will be building your machine translation systems and evaluating with them with the blue algorithm and their full details about blue in the assignment handout but at the end of the day blue gives the score between zero and a hundred where your score is a hundred if you're exactly producing one of the human written translations and zero if you there's not even a single unigram that overlaps between the two with that rather brief intro i wanted to show you sort of what happened in machine translation so machine translation with statistical models phrase-based statistical machine translation that i showed at the beginning of the class had been going on since the mid 2000s decade and it had produced sort of semi-good results of the kind that are in google translate in those days but by the time you'd entered um the 2010s basically progress and statistical machine translation had stalled and you are getting barely any increase over time and most of the increase in time you were getting over time was simply because you're training your models on more data um in those years around the early 2010s the big hope that most people had someone asked what is the y-axis here this y-axis is this blue score that i told you about on the previous slide in the early 2010s the big hope that most people in the machine translation field had was well if we built a more complex kind of machine translation model that knows about the syntactic structure of languages that makes use of tools like dependency parsers will be able to build much better translations and so those are the purple systems here which i haven't described at all but it's sort of as the years went by it was pretty obvious that that barely seemed to help and so then in the mid 2000 2010s so in 2014 was the first modern attempt to build a neural network for machine translations and encoder decoder model um and by the time it was sort of evaluated in bake offs in 2015 it wasn't as good as what had been built up over the preceding decade but it's already getting pretty good but what was found was that these newer models just really opened up a whole new pathway to start building much much better machine translation systems and since then things have just sort of taken off and year by year newer machine translation systems are getting much better and far better than anything we had preceding that so for you know at least the early part of application of deep learning and natural language processing neural machine translation was the huge big success story um in the last few years when we've had um models like gpt2 and gpt3 and other huge neural models like bird improving web search you know it's a bit more complex but this was the first area where there was a neural network which was hugely better than what it proceeded and was actually solving a practical problem that lots of people in the world need and it was stunning with the speed at which success was achieved so 2014 were the first um what i call here fringe research attempts um to build in your machine translation system meaning that um three or four people who were working on neural network models thought oh why don't we see if we can use one of these to translate learn to translate sentences where they weren't really people with a background in machine translation at all but a success was achieved so quickly that within two years time google had switched to using neural machine translation for most languages and by a couple of years later after that essentially anybody who does machine translation is now deploying um live neural machine translation systems and getting um much much better results so that was sort of just an amazing technological transition that for the preceding decade the big statistical machine translation systems like the previous generation of google translate had literally been built up by hundreds of engineers over years but a comparatively small group of deep learning people in a few months with a small amount of code and hopefully you'll even get a sense of this doing assignment 4 we're able to build newer machine translation systems that prove to work much better does that mean that machine translation is solved uh no um there are still lots of different ease which people continue to work on very actively and you can see more about it in the sky yet today article was linked at the bottom but you know there are lots of problems without a vocabulary words they're domain mismatches between the training and test data so it might be trained mainly on newswire data but you want to translate people's facebook messages there are still problems with maintaining context over longer text we'd like to translate um languages for which we don't have much data and so these methods work by far the best when we have huge amounts of parallel data even our best multi-layer lstms aren't that great at capturing sentence meaning there are particular problems such as interpreting what pronouns refer to or in languages like um chinese or japanese where there's often no pronoun present but there is an implied reference to some person working out how to translate that for languages the laws have lots of inflectional forms of nouns verbs and adjectives these systems often get them wrong so there's still tons of stuff to do so here's just sort of quick funny examples of the kind of things that go wrong right so if you've asked to translate paper jam google translate is deciding that this is a kind of jam just like there's some raspberry jam and strawberry jam and so this becomes a jam of paper um there are problems of agreement and choice um so if you have many languages don't distinguish gender and so the sentences uh neutral between things are masculine or feminine so malay or turkish are two well-known languages of that sort but what happens when that gets translated into english by google translate is that the english language model just kicks in and applies stereotypical biases and so these gender neutral sentences get translated into she works as a nurse he works as a programmer so if you want to help solve this problem you all of you can help by using singular they in all contexts when you're putting material online and that could then change the distribution of what's generated but people also work on modeling improvements to try and avoid this here's one more example that's kind of funny um people noticed um a couple of years ago that if you choose one of the rarer languages that google will translate um such as somali um that and you just write in some rubbish like a gay gag um freakily it would produce out of nowhere prophetic and biblical texts as the name of the lord was written in the hebrew language it was written in the language of the hebrew nation which makes no sense at all we're about to see a bit more about why this happens um but but ah that was sort of a bit worrying um this as far as i can see this problem is now fixed in 2021 i couldn't actually get google translate um to generate examples like this anymore um but you know so there's lots of ways to keep on doing research um nmt is certainly is you know a flagship task for nlp and deep learning and it was a place where many of the innovations of deep learning nlp were pioneered and people continue to work hard on it people found many many improvements um and actually for the last bit of the class in the minute i'm going to present one huge improvement which is so important that it's really come to dominate the whole of the recent field of newell neural networks for nlp and that's the idea of attention but before i get on to attention i want to spend three minutes on our assignment for so for assignment four this year um we've got a new version of the assignment um which we hope will be interesting but it's also a real challenge so for assignment four this year we've decided to do cherokee english machine translation so cherokee is an endangered native american language it has about 2 000 fluent speakers it's an extremely low resource language so it's just there isn't much written cherokee data available period and particularly there's not a lot of parallel sentences between cherokee and english and here's the answer um to the google's freaky prophetic translations for languages for which there isn't um much parallel data available commonly the biggest place where you can get parallel data is from bible translations so you can have your own personal choice wherever it is over the map as to where you stand with respect to religion but the fact of the matter is if you work on indigenous languages what you very um very quickly find is that a lot of the work that's done on collecting data on indigenous languages and a lot of the material that is available in written form for many indigenous languages is bible translations um yeah okay so this is um what cherokee looks like and so you can see that the writing system has a mixture of things that look like english letters and then all sorts of letters that don't and so here's the initial bit of a story long ago was seven boys who used to spend all their time down by the townhouse so this is a piece of parallel data that we can learn from so um the cherokee writing system has 85 letters and the reason why it has so many letters is that each of these letters actually represents a syllable so many languages of the world have strict consonant vowel syllable structure so you have words like rata per or something like that or cherokee right and another language like that's hawaiian and so each of the letters represents a combination of a consonant and a vowel and um that's um the set of those um you then get 17 by five gives you 85 letters um yeah so being able to do this assignment big thanks to um people from university of north carolina chapel hill um who've provided the resources um we're using for this assignment although you can do quite a lot of languages on google translate um cherokee is not a language that google offers on google translate so we can see how far um we can get um but we have to be modest in our expectations because it's hard to build a very good mt system with only a fairly limited amount of data so we'll see how far we can get there is a flip side which is for you students doing the assignment the advantage of having not too much data is that your models will train relatively quickly so we'll actually have less troubles than we did last year with people's models taking hours to train as the assignment deadline closed in there's a couple more words about cherokee so we have some idea what we're talking about so the cherokee originally lived in western north carolina and tennis eastern tennessee they then sort of got shunted um south west from that and then in particular for those of you who went to american high schools and paid attention you might remember um discussion of the trail of tears when a lot of the native americans from the southeast of the us got forcibly shoved a long way further west and so most cherokee now live in oklahoma though there's there are some that are in north carolina the writing system that i showed on this previous slide it was invented by a cherokee man sequoia that's a drawing of him there and that was actually a kind of incredible thing so um he started off a literate and worked out how to write or produce a writing system that would be goodbye good for um cherokee and given that it has this consummate vowel structure um he chose a celebrity which um turned out to be a good choice um so here's here's a neat historical fact so in the 1830s and 1840s um the percentage of cherokee that were literate in cherokee written like this was actually higher than the percentage of white people in the southeastern united states at that point in time okay before time disappears um oops time has almost disappeared i'll just start to say um and then i'll have to do a bit more of this uh i'll have to do a bit more of this next time that'll be okay right so the final idea that's really important for sequence to sequence models is the idea of attention um and so we had this model of doing sequence to sequence models such as the neural machine translation and the problem with this architecture is that we have this one hidden state which has to encode all the information about the source sentence so it acts as a kind of an information bottleneck and that's all the information that the generation is conditioned on well i didn't already mention one idea last time of how to get more information where i said look maybe you could kind of average all of the vectors of the source to get a sentence representation but you know that method turns out to be better for things like sentiment analysis and not so good for machine translation where the order of words is very important to preserve so it's it seems like we would do better if somehow we could get more information from the source sentence while we're generating the translation and in some sense this just corresponds to what a human translator does right if you're a human translator you read the sentence that you're meant to translate and you maybe start translating a few words but then you look back at the source sentence to see what else was in it and translate some more words so um very quickly after the first neural machine translation systems people came up with the idea of maybe we could build a better neural mt model that did that and that's the idea of a tension so the core idea is on each step of the decoder um we're going to use a direct link between the encoder and the decoder that will allow us to focus on particular a particular word or words in the source sequence and use it to help us generate what words come next i'll just go through now showing you the pictures of what attention does and then at the start of next time we'll go through the equations in more detail so we generate we use our encoder just as before and generate our representations feed in our conditioning as before and say we're starting our translation but at this point we take this hidden representation and say i'm going to use this hidden representation to look back at the source to get information directly from it so what i will do is i will compare the hidden state of the decoder with the hidden state of the encoder at each position and generate an attention score which is a kind of similarity score like a dot product and then based on those attention scores i'm going to calculate a probability distribution um as to by using a softmax as usual to say which of these encoder states is most like my decoder state and so we'll be training the model here to be saying well probably you should translate the first word of the sentence first so that's where the attention should be placed so then based on this attention distribution which is a probability distribution coming out of the softmax we're going to generate um a new um attention outport and so this attention output is going to be an average of the hidden states of the encoder model but it's going to be a weighted average based on our attention distribution and so we're then going to take that attention output combine it with the hidden state of the decoder rnn um and together the two of them are then going to be used to predict virus soft max what word to generate first and we hope to generate he and then at that point we sort of chug along and keep doing these same um kind of computations at each position um there's a little side note here that says um sometimes we take the attention output from the previous step and also feed into the decoder along with the usual decoder input so we're taking this attention out from actually feeding it back in to the hidden state calculation and that can sometimes improve performance and we actually have that trick in the assignment four system and you can try it out okay so we generate along and generate our whole sentence in this manner and that's proven to be a very effective way of getting more information from the source sentence more flexibly to allow us to generate a good translation i'll stop here for now and at the start of next time i'll finish this off by going through the actual equations for how attention works you 
","['', 'machine translation', 'sequence to sequence models', 'attention', 'assignment 3', 'assignment 4', 'pre-history of machine translation', 'source language', 'target language', 'rule-based systems', 'word lookup', 'statistical machine translation', 'perplexity', 'conditional language models', 'encoder', 'decoder', 'greedy decoding', 'beam search decoding', 'machine translation evaluation', 'attention in machine translation', '']"
"hi everyone uh welcome to cs224n we're about two minutes in so let's get started um so today uh we've got what I think is quite an exciting lecture topic we're going to talk about self-attention and Transformers so these are some ideas that are sort of the foundation of most of the modern advances in natural language processing and actually uh sort of AI systems in a broad range of fields so it's a very very fun topic um before we get into that um [Music] okay before we get into that we're going to have a couple of reminders so there are brand new lecture notes uh uh nice thank you yeah um I'm very excited about them um they go into they they pretty much follow along with uh what I'll be talking about today but go into considerably more detail uh assignment four is due a week from today um yeah so the issues with Azure continue um thankfully thankfully um our uh uh Tas especially has tested that this works on collab and the amount of training is such that you know uh you know a collab session will allow you to train uh your machine translation system so if you don't have a GPU use collab we're continuing to work on getting access to more gpus for uh assignment five in the final project uh we'll continue to update you as we're able to um but our you know are the usual systems this year uh are no longer holding because companies are changing their minds about things okay um so our final project proposal uh you have a proposal of what you want to work on for uh your final project we will give you feedback on whether we think it's a feasible idea or how to change it so this is very important because we want you to work on something that we think has a good chance of success for the rest of the quarter that's going to be out tonight we'll have an ad announcement when it is out um and we want to get you feedback on that pretty quickly uh because you know you'll be working on this after assignment five is done really the major core component of the course uh after that is the um is the final project okay any questions cool okay um okay so so let's let's kind of take a look back into what we've done so far in this course and sort of see uh what you know what we were doing in natural language processing what was our strategy if you had a natural language processing problem and you wanted to say take like your best effort attempt at it without doing anything too fancy you would have said okay I'm going to have you know a bi-directional lstm uh instead of a simple RNN right I'm going to use an lstm uh to encode my sentences I get bi-directional context and um if I have an output that I'm trying to generate right I'll have like a unidirectional lstm you know that I was going to generate one by one so you have a translation or a parse or whatever and so maybe I've encoded in a bi-directional LCM The Source sentence and I'm sort of you know one by one decoding out the the target with my unidirectional LCM and then uh also right I was going to use something like attention to give flexible access to memory uh if I you know felt like I needed to do this sort of look back and see where I want to translate from okay and this was just working uh exceptionally well and we we motivated so you know attention through wanting to do machine translation and you have this this bottleneck where you don't want to have to encode the whole sentence Source sentence in a single vector okay and in this lecture we have the same goal so we're going to be looking at a lot of the same problems that we did previously but we're going to use different building blocks we're going to say um you know uh if if 2014 to 2017-ish I was using recurrence uh through lots of trial and error years later uh it was we had these like brand new building blocks that we could plug in sort of you know uh direct replacement for lstms and they're going to allow for just a huge range of much more successful applications and um and so what what are the what what are the issues with the recurrent neural networks we used to use and what are the new systems that we're going to use sort of from this point moving forward okay so um so one of the issues with with a recurrent neural network uh is what we're going to call linear interaction distance so as we know uh you know rnns are unrolled left to right or right to left depending on the language and the direction okay but it encodes the sort of notion of linear locality which is useful because if two words occur right next to each other sometimes they're actually quite related so tasty Pizza they're nearby and in the recurrent neural network right you sort of encode you know tasty and then you sort of walk one step and you encode Pizza um so nearby words do often affect each other's meanings um but you know you have this this problem where very long distance dependencies can take a very long time to interact so if I have the sentence the chef so those are those are nearby those interact with each other and then uh who and then a bunch of stuff like the chef who went to the stores and picked up the ingredients and you know loves garlic um and then was right like I actually have an RNN step right this sort of application of the recurrent weight Matrix and some element-wise non-linearities once twice three times right sort of as many times as there is potentially the the length of the sequence between chef and was right and it's the chef who was so this is a long distance dependency should feel kind of you know related to the stuff that we did in dependency syntax but you know it's quite difficult uh to learn potentially that these words should be related so if you have sort of a lot of steps uh between uh between words um you know it can be difficult to learn the dependencies between them you know we talked about all these gradient problems lstms do a lot better at modeling the gradients uh across long distances than simple recurrent neural networks but it's not perfect um and we already know sort of that this linear linear order isn't sort of the right way to think about about sentences so if I wanted to learn that it's the chef who uh was then you know I might have a hard time doing it because the gradients have to propagate from west to Chef and you know uh really I'd like more direct connection between words that might be related in the sentence or in a document even right if these are going to get much longer um so so this is this linear interaction distance problem we would like words that might be related to be able to interact with each other in the neural networks computation sort of graph uh more easily than uh sort of being linearly far away um yeah so that we can learn these long distance dependencies better and there's a related problem too that again comes back to the recurrent neural networks dependence on the index on the index into the sequence often call it a dependence on time so in a recurrent neural network the forward and backward passes have o of sequence length many so that means just roughly sequence in this case just sequence length many unparallelizable operations so you know we know gpus are great they can do a lot of operations at once as long as there's no dependency between the operations in terms of time that you have to compute one and then compute the other right but in a recurrent neural network you can't actually compute the RNN hidden state for time step 5 before you compute the RNN hidden state for time step four or time step three right and so you get this graph that looks very similar where if I want to compute this hidden state so I've got some word I can I have zero operations I need to do before I can compute this state I have one operation I can do before I can compute this state and as my sequence length grows right I've got okay here I've got three operations I need to do before I can compute the state with the number three because I need to compute this and this and that so there's sort of three unparallelizable operations that I'm sort of glomming you know all the Matrix multiplies and stuff into a single one so so one two three and of course this grows with the sequence length as well so uh down over here so as the sequence length grows I can't parallelize you know I can't just have a big GPU just you know with the with the Matrix multiply to compute this state because I need to compute all the previous States beforehand foreign sort of related problems both with the dependence on time yeah yeah so I have a question on the linear interaction issues I thought that was the whole point of the attention Network and then how maybe um you want during the training of the actual cells that depend more on each other can't we do something like the attention and sort of work our way so the question is uh with the linear interaction distance wasn't this sort of the point of attention that it sort of gets around that can't we use something with attention to sort of help or does that just help so it won't solve the paralyzability problem and in fact everything we do in the rest of this lecture will be attention-based but we'll get rid of the recurrence and just do attention more or less so well yeah it's a great intuition any other questions Okay cool so um so if not recurrence what about attentions even just a slide a slide back um and uh so you know just we're gonna get deep into attention today but just for the second right attention treats each word's representation as a query to access and incorporate information from a set of values so previously right we were in a decoder we were decoding out a translation of a sentence and we attended to the encoder so that we didn't have to store the entire representation of the source sentence into a single vector and here today we'll talk think about attention within a single sentence so I've got this sort of sentence written out here with a you know word one through word t in this case and um right on these sort of integers in the boxes I'm writing out the number of unparallelizable operations that you need to do before you can can compute these so for each word you can independently compute its embedding without doing anything else previously right because the embedding just depends on the word identity and then with attention right if I wanted to build an attention representation of this word by looking at all the other words in the sequence that's sort of one big operation and I can do them in parallel for all the words so the attention for this word I can do for the attention for this word I don't need to sort of walk left to right like I did for an RNN again we'll get much deeper into this but this you should uh have the intuition that it solves the linear interaction problem and the non-parelizability problem because now no matter how far away words are from each other I am potentially interacting right I might just attend to you even if you're very very far away uh sort of independent of how far away you are and I also don't need to sort of walk along the sequence linearly long so I'm treating the whole sequence at once all right so so you know the intuition is that attention allows you to look very far away at once and it doesn't have this dependence on the sequence index that keeps us from parallelizing operations and so now the rest of the lecture we'll talk uh in great depth about attention uh so maybe let's just uh move on okay so let's think more deeply about attention um you know one thing that you might think of with attention is that it's sort of Performing kind of a fuzzy lookup in a key value store so you have a bunch of keys a bunch of values and it's going to help you sort of access that so in an actual lookup table right just like a dictionary in Python for example right very simple you have a table of keys that each key maps to a value and then you like give it a query and the query matches you know one of the keys and then you return the value right so I've got a bunch of keys here and my query matches the key so I return the value simple Fair easy okay good um and in attention uh right so just like we saw before the query matches all keys softly there's no exact match uh you sort of compute some sort of similarity between the key and all of the sorry the query and all of the keys and then you sort of weight the results so you've got to query again you've got a bunch of keys the query to different extents is similar to each of the keys and you will sort of measure that similarity between zero and one through a soft Max and then you know you get the values out you you average them via the weights of the similarity between the key and the the query and the keys you do a weighted sum with those weights and you get an output right so it really is quite a bit like a lookup table but in this sort of soft Vector space you know um mushy sort of sense so I'm really doing some kind of accessing into this information that's stored in the key value store but I'm sort of softly looking at all of the results okay any questions there cool um so so what might this look like right so if I was trying to represent this sentence I went to Stanford's cs224n and learned so I'm trying to build a representation of learned um you know uh I have a key for each word so this is this self-attention thing that we'll we'll get into I have a key for each word a value for each word I've got the query for learned and I've got these sort of these sort of teal-ish bars up top which sort of might say how much you're going to try to access each of the word like so maybe 224n is not that important CS maybe that determines what I learned you know Stanford uh right and then learned maybe that's important to representing itself right so you sort of look across at the whole sentence and build up this sort of soft accessing of of information across the sentence in order to represent learned in context okay so this is just a toy a toy diagram so let's get into the math so we're going to look at a sequence of words that's W1 to n a sequence of words in a vocabulary so this is like you know Zuko made his Uncle T that's a that's a good sequence and for each word we're going to embed it with this embedding Matrix just like we've been doing in this class right so I have this embedding Matrix that goes from the vocabulary size to the dimensionality D so that's each word has a non-contextual right only dependent on itself word embedding and now I'm going to transform each word with one of three different weight matrices so this is often called key query value self-attention so right so I have a matrix Q which is an RD to D so this Maps x i to which is a vector of dimensionality D to another Vector of dimensionality D and uh so that's going to be a query Vector right so it takes an x i and it sort of you know rotates it shuffles it around stretches it squishes it makes it different and now it's a query and now for a different learnable parameter K that's another Matrix I'm going to come up with my keys and with a different learnable parameter V I'm going to come up with my values right so I'm taking each of the non-contextual word embeddings each of these xi's and I'm transforming each of them to come up with my query for that word my key for that word and my value for that word okay so every word is doing each of these roles next I'm going to compute all pairs of similarities between the keys and queries right so in the toy example we saw I was Computing sort of the similarity between a single query for the word learned and all of the keys for the entire sentence in this context I'm Computing all pairs of similarities between all keys and all values because I want to represent sort of all of these sums so I've got this sort of dot product I'm just going to take the dot product between these two vectors right so I've got Qi so this is saying the query for word I dotted with the key for Word J and I get this score which is you know a real value uh might be very large negative might be zero might be very large and positive and so that's like how much should I look at J in this lookup table and then I do the softmax right so I softmax so I say that you know the actual weight that I'm going to look at J from I is softmax of this over all of the possible indices right so it's like the the Affinity between I and J normalized by the infinity between I and all of the possible J Prime in the sequence and then my output is just the weighted sum of values so I've got this output for word I so maybe I is like one for Zuko and I'm representing it as the sum of these weights for all J so Zuko and made and his and uncle and T and the value Vector for that word uh J I'm looking from I to J as much as Alpha i j oh w i you can either think of it as a symbol in vocab V so that's like you could think of it as a one hot Vector in um yeah in this case we are I guess thinking of this so a one hot Vector in dimensionality size of vocab so in in The Matrix e you see that it's uh r d by bars around V that's the size of the vocabulary so when I do e multiplied by w i that's taking e which is d by V multiplying it by W which is V and returning a vector that's dimensionality D so first line it like W1 and that's a matrix where um it has like maybe like a column for every word in that that sentence in each column is a length V yeah usually I guess we think of it as having a I mean if I'm putting the the sequence length index first you might think of having a row for each word but but similarly yeah it's it's n which is the sequence length and then the second dimension would be V which is the vocabulary size and then that gets mapped to this thing which is sequence length by D um why do we learn two different matrices q and K when like Q transpose Qi transpose KJ is really just one Matrix in the middle between that's a great question it ends up being because this will end up being a low rank approximation to that Matrix so it is for computational efficiency reasons although it also I think feels kind of nice and uh in the presentation but yeah what we'll end up doing is having a very low rank approximation to qk transpose and so it you actually do do it like this it's a good question so the curry so I could you repeat that for me the CII so the query of the word um dotted with the key by itself doesn't look like Identity or do they look at these things in particular that's a good question okay let me remember to repeat questions so does eii right for for J equal to I so looking at itself look like anything in particular does it look like the identity is that the question okay so um so right it's unclear actually this question of should you look at yourself for representing yourself well it's it's going to be encoded by the matrices q and K right if I didn't have q and K in there right if those were the identity matrices if Q is identity K's identity then this would be sort of Dot credit with yourself which is going to be high on average like you're pointing in the same direction as yourself but it could be that you know qxi and kxi might be sort of arbitrarily different from each other because Q could be the identity and K could map you to the negative of yourself for example so that you don't look at yourself so this is all learned in practice so you end up it can it can sort of decide by learning whether you should be looking at yourself or not and that's some of the flexibility that parameterizing at SQ and K gives you that wouldn't be there if I just used xi's everywhere in this in this equation I'm going to try to move on I'm afraid because there's a lot to get on but uh we'll keep talking about self-attention and so as more questions come up I can also potentially return back um okay so so this is our basic building block but there are a bunch of barriers to using it as a replacement for for our lstms and so what we're going to do for this portion of the lecture is talk about the minimal components that we need in order to use self-attention as sort of this like very fundamental uh building block so we can't use it as it stands as I've presented it um but because there are a couple of things that we need to sort of solve or fix one of them is that there's no notion of sequence order in self-attention so so you know um what is what does this mean if I have a sentence uh like I'm going to move over here to the Whiteboard briefly and hopefully I'll I'll uh write quite large um if I have a sentence like Zuko made his uncle and uh let's say his uncle made Zuko if I were to embed each of these words right using its embedding Matrix the embedding Matrix isn't dependent on uh the index of the word so this is the word index one two three four versus now his is over here an uncle right and so when I compute the self-attention and there's a lot more in this in the lecture notes that goes through a full example um uh the actual self-attention operation will give you exactly the same representations for this sequence Zuko made his uncle as for this sequence his uncle made Zuko and that's bad because they're sentences that mean different things um and so right it's sort of this this idea that self-attention is an operation on sets like you have a set of vectors that you're going to perform self-attention on and nowhere does like the exact position of the words come into play directly um so uh we're going to encode the position of words uh through the keys queries and values that we have um so you know consider now representing each sequence Index right our sequences are going from one to n as a vector so so don't worry so far about you know how it's being made but you can imagine representing sort of the number one like the position one the position two the position three as a vector in the dimensionality D just like we're representing our keys queries and values and um so these are position vectors uh you know you can if if you were to want to incorporate the information represented by these positions into our self-attention uh you could just add these vectors these Pi vectors to the inputs right so if I have you know this this x i embedding of a word which is the word at position I but really just represents oh the word Zuko is here now I can say that oh it's the word Zuko and it's at position five because you know this Vector represents position five okay so so how do we do this um and we might only have to do this once right so we can do it once uh at the very input to the to the network and then that sort of is sufficient we don't have to do it at every layer because it sort of knows from the input um so so one way in which people have done this is look at these sinusoidal position representations so this looks a little bit like this where you have so these are this is a vector Pi which is in dimensionality D right and um each one of the dimensions you take the value I you modify it by some uh constant and you you pass it to the sine or cosine function and you get these sort of values that vary according to the period uh uh differing periods depending on the dimensionalities D so I've got this sort of a representation of a matrix where D is the vertical Dimension and then n is the horizontal and you can see that they're sort of like oh you know um as I walk along you see the period of the sine function going up and down and each of the dimensions D has a different period And so together you can represent a bunch of different uh sort of position indices and um you know it gives so this intuition that oh maybe period maybe sort of the absolute position of a word isn't as important you've got the sort of periodicity of the Sines and cosines um and maybe that allows you to extrapolate to longer sequences uh but in practice that doesn't work um but this is sort of like an early uh notion that still sometimes used for how to represent position in Transformers and self-attention networks in general um so so that's one idea you might think it's a little bit complicated a little bit unintuitive here's something that feels a little bit more deep learning so we're just going to say oh you know I've got a maximum sequence length of n and I'm just gonna learn a matrix that's dimensionality d by n and that's going to represent my positions I'm going to learn it as a parameter just like I learned every other parameter and what do they mean oh I have no idea but it you know represents position um so um right and be so you just sort of add this Matrix uh to the xi's your input embeddings um and it learns to you know fit to data so whatever representation of position that's linear uh sort of you know index based that you want you can learn and the cons are that well you definitely now can't represent anything that's longer than n words long right no sequence longer than n you can handle because um well you only learned a matrix of this many positions and so in practice you'll get you know a model error if you if you pass a self-attention model something longer than length n it will just sort of Crash and say I can't I can't do this and so this is sort of what most systems nowadays use they're more flexible representations of position including a couple in the lecture notes you might want to look at sort of like the relative linear position or words before or after each other but not their absolute position there's also some sort of representations that that hearken back to our dependency syntax because like oh maybe words that are close in the dependency parse tree should be the things that are sort of close in the uh in the self-attention operation um okay questions in practice do we typically just make n large enough that we don't run into the issue of course having something that could be input longer than him so the question is in practice do we just make n long enough so that we don't run into the problem where we're going to you know look at a text longer than n no in practice it's actually quite a problem uh even today even in the largest biggest language models and uh you know uh you know can I fit this prompt into chat GPT or whatever is the thing that you might see on Twitter I mean these continue to be issues and part of it is because the self-attention operation and we'll get into this later in the lecture it's it's quadratic complexity in the sequence length so you're going to cut you're going to spend N squared sort of memory budget in order to make sequence lengths longer so in practice you know this might be on a large model say 4 000 or so n is four thousand so you can fit four thousand words which feels like a lot but it's not going to fit a novel it's not going to fit a Wikipedia page um you know and so and there are models that do longer uh sequences for sure um and again we'll talk a bit about it but no this this actually is an issue yeah so how do you know that the P that you've learned this Matrix that you've learned is representing position as opposed to anything else the reason is the only thing that correlates this position right so like when I see these vectors I'm adding this P Matrix to my X Matrix the word embeddings I'm adding them together and the words that show up at each index will vary depending on what word actually showed up there in the example but the P Matrix never differs it's always exactly the same at every index and so it's the only thing in the data that it correlates with so you're sort of learning it implicitly like this Vector at index one is always at index one for every example for every gradient update and nothing else uh co-occurs like that yeah so what you end up learning I don't know unclear but it definitely allows you to know oh this word is with this index said this yeah okay yeah just quickly in space um okay so the question is when this is quadratic in the sequence is that a sequence of words yeah think of it as a sequence of words um sometimes there'll be pieces that are smaller than words which we'll go into in next slide in the next lecture but yeah think of this as a sequence of words but not necessarily just for a sentence maybe for an entire paragraph or an entire document or something like that where yeah the tension is based words to words okay cool I'm gonna move on um okay so um right so we have another problem uh another is that you know based on the presentation of self-attention that we've done you know there's really no non-linearities for uh sort of deep learning magic we're just sort of computing weighted averages of stuff um so so you know if I apply self-attention and then apply self-attention again and then and again and again and again you should get uh you should look at the next lecture notes if you're interested in this it's actually quite cool but what you end up doing is you're just re-averageing value vectors together so you're like Computing averages of value vectors and it ends up looking like one big self-attention uh but there's an easy fix to this if you want sort of the traditional deep learning magic and you can just add a feed forward Network to post-process each output Vector so I've got a word here that's sort of the output of self-attention and I'm going to pass it through you know in this case I'm calling it a multi-layer perceptron MLP so this is a vector in Rd that's going to be uh and it's taking in as input a vector in Rd and you know you do the usual uh sort of multi-layer perceptron thing right where you have the output and you multiply it by matrix pass it to a non-linearity multiply it by another Matrix okay and so what this looks like in self-attention is that I've got this sort of sentence the chef who the food and I've got my embedding for it I pass it through this whole big self-attention block right which looks at the whole sequence and sort of incorporates context and all that and then I pass each one individually through a feed forward uh layer right so so this embedding that's sort of the output of the self-attention for the word the is passed independently through a multi-layer perceptron here and that sort of you can think of it as sort of combining you know together uh or processing the result of attention so so there's a number of reasons why we do this um one of them also is that you can actually stack a ton of computation into these feed forward uh networks very very efficiently very paralyzable very good for gpus but but this is what's done in practice so you do self-attention and then you can you know pass it through this sort of position wise feed forward layer right every word is processed independently by this feed forward Network to process the result okay so that's adding our sort of classical deep learning non-linearities for self-attention um and that's an easy fix for this sort of no non-linearities problem in self-attention and then we have a last issue before we have our final minimal self-attention building block with which we can replace rnns and that's that uh well you know when I've been writing out all of these examples of self-attention you can sort of look at the entire sequence right and and uh in practice for some tasks such as machine translation or language modeling whenever you want to define a probability distribution over a sequence you can't cheat and look at the future right uh so you know at every time step I could Define the set of keys and queries and values to only include past words but this is inefficient uh bear with me it's inefficient because you can't parallelize it so well so instead we compute the entire n by n Matrix just like I showed in the slide discussing self-attention and then I mask out words in the future so if this score e i j right and I I computed eij for all n by n pairs of words is equal to whatever it was before if the word that you're looking at at index J is an index that is less than or equal to where you are index I and it's equal to negative infinity-ish otherwise if it's in the future and when you softmax the eij negative Infinity gets mapped to zero so now my attention is weighted zero my my weighted average is zero on the future so I can't look at it what does this look like so in order to encode these words the chef who and maybe the start start symbol there I can look at these words right that's all pairs of words and then I just gray out I I sort of negative Infinity out the words I can't look at so encoding the start symbol I can just look at the start symbol when encoding the I can look at the start symbol and the encoding Chef I can look at start the chef but I you know can't look at who right and so it with this representation of Chef that encode that is you know only looking at start the chef I can define a probability distribution using this Vector that allows me to predict who without having cheated by already looking ahead and seeing that well who is the next word questions so it says for using it in decoders um do we do this for both the encoding layer and the e-coding layer or for the encoding layer are we allowing ourselves to look forward the question is uh it says here that we're using this in a decoder do we also use it in the encoder so that this is the distinction between sort of like a bi-directional lstm and a unidirectional lstm right so wherever you don't need this constraint you probably don't use it so if you're using an encoder right on the source sentence of your machine translation problem you probably don't do this masking because it's probably good to let everything look at each other and then whenever you do need to use it because you have this Auto regressive sort of probability of word one probability of two given one you know three given two in one then you would use this so traditionally yes in decoders you will use it in encoders you will not yes um my question is a lot about philosophical how humans actually generate sentences by having some notion of the probability of future words before they say um the words that or before they choose the words that they are friendly speaking or writing regenerating good question so the question is isn't you know looking ahead a little bit and sort of predicting or getting an idea of the words that you might say in the future sort of how humans generate language instead of the sort of strict constraint of not seeing it into the future is that is that what you're okay so so right um you know trying to plan ahead to see what I should do is definitely an interesting idea um but when I am training the network right I can't if I'm teaching it to try to predict the next word and if I give it the answer it's not going to learn anything useful uh so in practice when I'm generating text maybe it would be a good idea to make some guesses far into the future or have a high level plan or something but in training the network I can't encode that intuition about how humans build uh see like generate sequences of language by just giving it the answer of the future directly at least because then it's just too easy like there's nothing to learn um yeah but there might be interesting ideas about maybe giving the network like a hint as to what kind of thing could come next for example but but that's out of scope for this yeah um yeah question up here so I understand like the like why we want to mask the future for stuff like language models but how does it apply to machine translation like why would we use it there yeah so in machine translation uh I'm gonna come over to this board and hopefully get a better marker nice in machine translation you know I have a sentence like uh I like pizza and I want to be able to uh you know translate it uh Jim uh Pizza nice um right and so uh when I'm when I'm looking at the I like pizza right I get this as the input and so I want self-attention um uh without masking because I want I to look at like and I to look at pizza and like to look at pizza and I want it all and then when I'm generating this right if my tokens are like J M La Pizza um I want to in encoding this word I want to be able to look only at myself and we'll talk about encoder decoder architectures in this uh later in the lecture um but I want to be able to look at myself none of the future and all of this and so what I'm talking about right now in this masking case is masking out you know um with like negative Infinity all of these words so that sort of attention score from to everything else should be uh net to be you know negative Infinity yeah does that answer your question great okay let's move ahead um okay so so that was our last big uh sort of building block uh issue with self-attention so this is what I would call and this is my personal opinion a minimal you know self-attention building block you have self-attention the basis of the method so uh that's sort of here in the red um and maybe we had you know the inputs to the sequence here and then you embed it with that embedding Matrix e and then you add position embeddings right then these three arrows represent using you know the uh the key the value and the query that sort of stylized there this is often how you see these diagrams um right and so you pass it to self-attention uh with the position representation right so that specifies the sequence order because otherwise you'd have no idea what order the words showed up in yeah the non-linearities in sort of the teal feed forward Network there uh to sort of provide that sort of squashing and and sort of uh deep learning expressivity and then you have masking in order to have parallelizable operations that don't look at the future okay so this is sort of our minimal uh architecture and then up at the top above here right so you have this thing maybe you repeat this sort of self-attention and feed forward many times so self-attention feed forward self tension feed forward self tension feet forward right that's what I'm calling this block and then maybe at the end of it you you know predict something I don't know we haven't really talked about that but you know you have these representations and then you predict the next word or you predict the sentiment or you predict whatever so this is like a self-attention architecture okay we're going to move on to the Transformer next so if there are any questions yeah other way around uh we will use masking for decoders where I want to decode out a sequence where I have an informational constraint where to represent this word properly I cannot have the information of the future right yeah okay okay great uh so now let's talk about the Transformers so what I've what I've pitched to you is what I call a minimal self-attention architecture uh and um you know I quite I quite like pitching it that way but really no one uses the architecture that was just up on the slide the previous uh slide it it doesn't work quite as well as it could and there's a bunch of sort of important details that we'll talk about now that goes into the Transformer but what I would hope though to sort of um have you take away from that is that the Transformer architecture as I'll present it now is uh not necessarily the end point of our search for better and better ways of representing language even though it's now ubiquitous and has been for a couple of years so so think about these sort of ideas of of the problems of using self-attention um and maybe ways of fixing some of the issues with Transformers okay so a Transformer uh decoder is how we'll build systems like language models right and so we've discussed this it's like our decoder uh with our self-attention only sort of minimal architecture it's got a couple of extra components some of which I've grayed out here that will go over one by one the first uh that's actually different is that we'll replace uh our self-attention with masking with masked multi-head self-attention this ends up being crucial it's probably the most important uh distinction between the Transformer and this sort of minimal architecture that I've presented so let's come back to our toy example of attention where we've been trying to represent the word learned in the context of the sequence I went to Stanford cs224n and learned um and I was sort of giving these teal bars to say oh maybe intuitively you look at various things to build up your representation of learned um but you know really there are varying ways in which I want to look back at the sequence to see varying sort of aspects of of information that I want to incorporate into my representation so maybe in this way I sort of want to look at Stanford cs224n because like oh it's like entities like it it you learn different stuff at Stanford cs224n than you do it other courses or other universities or whatever right and so maybe I want to look here for this reason and maybe you know there's in another sense I actually want to look at the word learned and I want to look at I you know I went and learned right is he sort of like maybe syntactically relevant words right like it's very different reasons for which I might want to look at different things in the sequence and so trying to sort of average it all out with a single operation of self-attention ends up being maybe somewhat too difficult in a way that will make precise in assignment five nice we'll do a little bit more math uh um okay so uh any questions about this in this intuition um yeah so uh it should be an application of attention just as I've presented it uh right so one independent Define the keys to find the queries to find the values I'll Define it more precisely here but think of it as I do attention once and then I do it again with different like being able different parameters being able to look at different things Etc we do not okay so the question is if we have two separate sets of Weights try to learn say to do this and and to do that how do we ensure that they learn different things uh we do not ensure that they hope that they learn different things and in practice they do uh although not perfectly uh so it ends up being the case that you have some redundancy and you can sort of like cut out some of these but that's sort of out of scope for this but we sort of Hope just like we hope that different sort of dimensions in our feed forward layers will learn different things because of lack of symmetry and whatever that uh we hope that the heads will start to specialize and that will mean they'll specialize even more and yeah okay all right so in order to discuss multi-head self-attention well we really need to talk about the matrices how we're going to implement this in gpus efficiently we're going to talk about the sequence stacked form of attention um so we've been talking about each word sort of individually as a vector in dimensionality D but you know really we're going to be working on these as as big matrices that are stacked so I take you know all of my word embeddings X1 to xn and I stack them together and now I have a big Matrix that is in dimensionality r n by D okay and uh now with my matrices k q and V I can just multiply them sort of on this side of X so X is RN by d k is our d by D so n by D times d by D gives you uh n by D again so I can just compute a big you know Matrix multiply on my whole sequence to multiply each one of the words with my key query and value matrices very efficiently right so this is sort of this vectorization idea I don't want to for Loop over the sequence I represent the sequence as a big Matrix and I just do one big Matrix multiply then the output is defined as this sort of inscrutable bit of math which I'm going to go over visually um so so first we're going to take the key query dot products in one Matrix so we've got um we've got X Cube which is uh RN by D and I've got x k transpose which is our d by n so n by d d by n this is Computing all of the e i JS these scores for self-attention right so this is all pairs of attention scores computed in one big Matrix multiply okay so this is this big Matrix here next I use the softmax right so I softmax this over uh the second dimension the second n Dimension um and I get my sort of normalized scores and then I multiply with XV so this is an N by n Matrix multiplied by an N by D Matrix and what do I get well this is just doing the weighted average right so this is one big weighted average contribution on the whole Matrix giving me my whole self-attention output and r n by D right so I've just restated identically the self-attention operations but computed in terms of matrices so you could do this efficiently on a GPU okay uh so multi-headed attention this is going to give us and it's going to be important to compute this in terms of the matrices which we'll see this is going to give us the ability to look in multiple places at once for different reasons so sort of you know first self-attention looks where this dot product here is high right this x i the Q Matrix the key Matrix uh but um maybe we want to look in different places for different reasons so we actually Define multiple query key and value matrices so I'm going to have a bunch of heads I'm going to have 8 H self-attention heads and for each head I'm going to Define an independent query key and value Matrix and I'm going to say that it's its shape is going to map from the model dimensionality to the model dimensionality over H so each one of these is doing projection down to a lower dimensional space uh this is going to be for computational efficiency and um I'll just apply self-attention sort of independently for each output so this equation here is identical to the one we saw for single-headed self-attention except we've got the sort of L indices everywhere so I've got this lower dimensional thing I'm mapping to a lower dimensional space and then I do have my lower dimensional value Vector there so my output is an r d by H but really you're doing exactly the same kind of operation I'm just doing it h different times and then you combine the outputs so I've done sort of look in different places with the different key query and value matrices and then I to get each of their outputs and then I concatenate them together right so each one is dimensionality d by H and I concatenate them together and then sort of mix them together with the final linear transformation and so uh each head gets to look at different things and construct their value vectors differently and then I sort of combine the result altogether at once okay let's go through this visually because it's at least helpful for me um so uh right it's actually not more costly to do this really than it is to compute a single head of self-attention and we'll see through the pictures so you know we were in single-headed self-attention we computed xq and in multi-headed self-attention we'll also compute X cubed the same way so xq is r and by D and then we can reshape it into our n that's sequence length times the number of heads times the model dimensionality over the number of heads so I've just reshaped it to say now I've got you know a big three axis tensor the first axis is the sequence length the second one is the number of heads the third is this reduced model dimensionality and that costs nothing right and do the same thing for x and V and then I transpose so that I've got the head axis as the first axis and now I can compute all my other operations with the head axis kind of like a batch so what does this look like in uh in practice like instead of having one big xq Matrix that's Model dimensionality D I've got like in this case three x Cube matrices of Model dimensionality D by 3 D by three D by three same thing with the key Matrix here so everything looks almost identical it's just a reshaping of the tensors and now right at the output of this I've got three sets of attention scores right just by doing this reshape and the cost is that well you know each of my attention heads has only a d by H Vector to work with instead of a d dimensional Vector to work with right so I get the output I get these three uh sets of pairs of scores I compute the softmax independently for each of the three and then I have three uh value matrices there as well each of them lower dimensional and then finally right I get my three different output vectors and if I have a final linear transformation to sort of mush them together and I get an output and in summary what this allows you to do is exactly what I gave in the toy example which was I can have each of these heads look at different parts of a sequence for different reasons so this is at a given uh block right like all of these attention heads are for a given Transformer block a next block would also could also have three attention pins the question is uh are all of these four a given block and we'll talk about a block again but this block was this sort of pair of self-attention and feed forward Network so you do like self-attention feed forward that's one block another block is another self-attention another feed forward and the question is are the parameters shared between the blocks or not generally they are not shared you'll have independent parameters at every block although there are some exceptions is it typically the case that you have the same number of like heads at each block or do you vary the number of heads across blocks you have this you definitely could vary it people haven't found reason to there so the question is do you have different numbers of heads across the different blocks uh or do you have the same number of heads across all blocks you know the simplest thing is to just have it be the same everywhere which is what people have done I haven't yet found a good reason to vary it but well it could be interesting it's definitely the case that you know after training these networks you can actually just totally zero out remove some of the attention heads and I'd be curious to know if uh you could remove more or less depending on the like layer index which might then say Oh we should just have fewer but it's again it's not actually more expensive to have a bunch so people tend to instead set the number of heads to be roughly so that you have like a reasonable number of Dimensions per head given the total Model dimensionality D that you want so for example I might want at least 64 Dimensions per head which if D is you know 128 that tells me how many heads I'm going to have roughly so people tend to scale the number of heads up with the model dimensionality excuse by slicing it in different columns you're reducing the rank of the final Matrix right yeah this but that doesn't really have any effect on the results so the question is by having these sort of reduced xq and uh uh XK matrices right this is a very low rank approximation this little sliver in this little sliver defining this whole big matrix it's very low rank is that not bad in practice no I mean again it's sort of the reason why we limit the number of heads depending on the model dimensionality because you you know you want intuitively at least some number of Dimensions so you know 64 is sometimes done 128 something like that um but you know if you're not giving each head too much to do and it's got sort of a simple job you've got a lot of heads it ends up sort of being okay at the very all we really know is that empirically it's way better to have more heads than like one uh yes um I'm wondering have there been studies to see if um information in one of the sets of the attention scores like information that one of them learns is consistent and like um related to each other or so the question is have there been studies to see if there's sort of consistent information encoded by the attention heads and you know yes actually there's been quite a lot of sort of study and interpretability and Analysis of these models to try to figure out what roles what sort of mechanistic roles each of these heads takes on and uh there's quite a bit of exciting results there around some attention heads you know learning to pick out sort of the you know it's like syntactic dependencies or maybe doing like a sort of a global averaging of context um the question is quite nuanced though because in a deep Network it's unclear and we should talk about this more offline but it's unclear if you look at a word 10 layers deep in a network what you're really looking at because it's already Incorporated context from everyone else and it's a little bit unclear active area of research but I think I should move on uh now to uh keep discussing Transformers but yeah if you want to talk more about it I'm happy to um okay so so uh another sort of uh hack that I'm going to toss in here I mean maybe they wouldn't call it hack but you know it's a nice little method to improve things it's called scaled dot product attention so one of the issues with this sort of key query value self-attention is that when the model dimensionality becomes large the dot products between vectors even random vectors tend to become uh large and when that happens the inputs to the softmax function can be very large making the gradients small so intuitively if you have two random vectors in Model dimensionality D and you just dot product them together as D grows their dot product grows an expectation to be very large and so you know you sort of want to start out with everyone's attention being very uniform very flat sort of look everywhere but if some dot products are very large then you know learning will be inhibited and so what you end up doing is you just sort of for each of your heads uh you know you just sort of divide all the scores by this constant that's determined by the model dimensionality so as the vectors grow very large their dot products don't at least at an initialization time so this is sort of like a nice little um you know important but but maybe not uh like yeah it's it's important to know um and uh so that's called scale dot product attention from here on out we'll just assume that we do this you know it's quite easy to implement you just do a little division in all of your uh computations okay so so now in the Transformer decoder we've got a couple of other things that I have un uh faded out here um we have two big optimization tricks or optimization methods I should say really because these are quite important that end up being very important we've got residual connections and layer normalization and in Transformer diagrams that you see sort of around the web they're often uh written together as this ad and Norm box and in practice in the Transformer decoder I'm going to you know apply mask multi-head attention and then do this sort of optimization add a norm then I'll do a feed forward application and then add a norm so you know this is quite important so let's go over these two individual uh components the first is residual connections I mean we've I think we've talked about residual connections before right well it's worth doing it again um uh but you know it's really a good trick to help models train better um so just to recap right we're going to take instead of having this sort of you have a layer uh layer I minus one and you pass it through a thing maybe it's self-attention maybe it's a feed forward Network now you've got layer I I'm going to add the result of layer I uh to this sort of to its input here so now I'm saying I'm just going to compute the layer and I'm going to add in the input to the layer so that I only have to learn the residual from the previous layer right so I've got this sort of connection here it's often written as this it's sort of like oh connection okay right goes around and you should think that the gradient is just really great through the residual connection right like ah you know if I've got Vanishing or exploding gradients Vanishing gradients through this layer well I can at least learn everything behind it because I've got this residual connection where the where the gradient is one because it's the identity um this is really nice and you know it also maybe is like a buy at least at initialization everything looks a little bit like the identity function now right because if the contribution of the layer is somewhat small because all of your weights are small and I have the addition from the input maybe the whole thing looks a little bit like the identity which might be a good sort of place to start and you know there are really nice visualizations I just love this visualization uh right so this is your like lost landscape right so you're gradient descent and you're trying to Traverse the mountains of the Lost landscape this is like the parameter space and down is better in your lost function and it's really hard so you get stuck in some local Optima and you can't sort of find your way to to get out and then this with residual connections I mean come on you just sort of walk down I mean it's not actually I guess really how it works all the time but I really love this it's great okay um so yeah we've seen residual connections we should move on to layer normalization um so layer Norm uh is another thing to help your model train faster um and you know there's the intuitions around layer normalization um and sort of the empiricism of it working very well maybe aren't perfectly like uh let's say connected but you know you should imagine I suppose um that we want to uh say you know this variation within each layer things can get very big things can get very small uh that's not actually informative because of you know variations between um maybe the the gradients or you know I've got sort of weird things going on in my layers that I can't totally control I haven't been able to sort of make everything behave sort of nicely where everything stays roughly the same Norm maybe some things explode maybe some things shrink um and I want to cut down on sort of uninformative variation um in between layers so I'm going to let X and r d be an individual word Vector in the model so this is like at a single a single index one vector and what I'm going to try to do is just normalize it normalize it in the sense of it's got a bunch of variation and I'm going to cut out on everything I'm going to normalize it to unit mean and standard deviation so I'm going to estimate the mean um here across uh so for all of the uh dimensions in the vector so J equals one to the model dimensionality I'm going to sum up the value so I've got this one big word vector and I sum up all the values division by D here right that's the mean I'm going to have my estimate of the standard deviation um again these should say estimates this is my simple estimate of the standard deviation or the values within this one vector and I'm just going to um and then possibly I guess I can have learned uh parameters to try to like scale back out in terms of uh multiplicatively and additively here that's optional we're going to compute this this standardization right we're going to take my Vector X subtract out the mean divide by the standard deviation plus this Epsilon sort of constant if there's not a lot of variation I don't want things to explode so I'm going to have this Epsilon there that's uh close to zero so this part here x minus mu over square root Sigma plus Epsilon is saying take all the variation and sort of normalize it to unit mean and standard deviation and then maybe I want to sort of scale it stretch it back out um and then maybe add an offset beta that I've learned although in practice actually this part and discuss this in the lecture notes uh and practice this part maybe isn't actually that important um but so layer normalization yeah you're sort of you know you can think of this as when I get the output of layer normalization it's going to be sort of look nice and look similar to the next layer independent of what's gone on because it's going to be unit mean and standard deviation so maybe that makes for a better thing to learn off of for the next layer okay any questions for uh residual or layer Norm yes yeah it's a good question when I subtract the scalar mu from the vector x i broadcast mu to dimensionality D and remove mu from Aldi yeah good point thank you that was unclear uh sure is it divided should it be divided by D or from me sorry can you repeat that in the fourth bullet point when you're calculating the mean um is it divided by D or is it or maybe I'm just interested I think it is divided by D yeah these are so this is the average deviation from the mean of all of the yeah yes [Music] mobilized based on the statistics so the question is if I have five words in the sequence do I normalize by sort of aggregating the statistics to estimate mu and sigma across all the five words share their statistics or do it independently for each word this is a great question which I think in all the papers that discuss Transformers is under specified you do not share across the five words which is somewhat confusing to me but so each of the five words is done completely independently um you could have shared across the five words and said that my estimate of the statistics are just based on all five uh but you do not I can't pretend I understand totally why for example per batch of the same position so so a similar question the question is um if you have a batch of sequences right so like just like we're doing batch Based training do you for a single word now we don't share across a sequence index for sharing the statistics we do share across the batch and the answer is no you also do not share across the batch in fact layer normalization was sort of invented as a replacement for batch normalization which did just that and the issue with batch normalization is that now your forward pass sort of depends in a way that you don't like on examples that should be not related to your example and so yeah you don't share statistics across the batch okay cool okay so so now we have our full Transformer decoder and we have our blocks so in this sort of slightly grayed out thing here that says repeat uh for a number of uh encoder or sorry decoder blocks um each block consists of I pass it through self-attention and then my ADD and Norm right so I've got this residual connection here that goes around and I've got the layer normalization there and then a feed forward layer and then another ad and norm and so that sort of set of four operations I apply you know for some number of times number of blocks so that whole thing is called a single block and uh that's it that's the Transformer uh decoder as it is cool so that's a whole architecture right there we've solved things like needing to represent position we've solved things like um not being able to look into the future uh We've solved a lot of different optimization problems you've got a question yes yes Mass to multi-head attention yeah with the dot product scaling with the square root D over H as well yeah so the question is uh how do these models handle variable length inputs um yeah so if you have so so the input to the like GPU forward pass is going to be a constant length so you're going to maybe pad to a constant length and in order to not look at the future the stuff that's sort of happening in the future you can mask out the pad tokens just like the masking that we showed for not looking at the future in general you can just say set all of the attention weights to to zero or the scores to negative Infinity for all of the pad tokens yeah exactly so you can you can uh set everything to this maximum length now in practice so the question was do you set this length that you have everything be be that maximum length I mean you know yes often although you can save computation by setting it to something smaller and uh everything the math all still works out you just have to code it properly so it can handle so you set everything instead of the N you set it all to five if everything is shorter than like five and you save a lot of computation all of the self-attention operations just work so yeah um uh there's one hidden layer in the feed forward yeah okay I should move on got a couple more things and not very much time okay um but I'll be here after the class as well so in the encoder so the Transformer encoder is almost identical but again we want bi-directional context and so we just don't do the masking right so I've got in my multi-head attention here I've got no masking and so it's that easy to make the model bi-directional okay um so that's easy so that's called the Transformer encoder it's almost identical but no masking and then finally we've got the Transformer encoder decoder which is actually how the Transformer was originally presented in this paper attention is all you need um and this is when we want to have sort of a bi-directional network here's the encoder it takes in say my source sentence for machine translation it's multi-headed attention is not masked and I have a decoder to decode out my sentence now but you'll see that this is slightly more complicated I have my masked multi-head self-attention uh just like I had before in my decoder but now I have an extra operation which is called cross attention where I'm going to use my decoder vectors as my queries then I'll take the output of the encoder as my keys and values so now for every word in the decoder I'm looking at all the possible words in the output of all of the blocks of the encoder yes yeah longer because I know initially it was like the keys and the values how do we get like a key in value separated from the output because then we collapse those into the single output uh so we well how sorry how will we get the keys and values out like how do we because when we have the output didn't we collapse like the keys and values into like a single output so the output we capture those yeah the question is how do you get the keys and values and queries out of this sort of single collapsed output now remember the output for each word is just this weighted average of the value vectors for the for the previous words right and then from that output for the next layer we apply a new key query and value transformation to each of them for the next layer of self-attention so it's not actually that you're here yeah you apply the key Matrix the query Matrix to the output of whatever came before it yeah um and so just in a little bit of math right we have um these vectors H1 through each n I'm going to call them that are the output of the encoder right and then I've got vectors that are the output of the decoder uh so I've got these Z's I'm calling the output of the decoder and then I simply Define my keys and my values from the encoder vectors these H's right so I take the H's I apply a key Matrix and a value Matrix and then I Define the queries from my decoder so my queries here so this is why two of the arrows come from the encoder and one of the arrows comes from the decoder I've got my Z's here get my queries my keys and values from the encoder okay uh so that is it I've got a couple of minutes I want to discuss some of the sort of results of Transformers and I'm happy to answer more questions about Transformers after class so um so you know really the original results of Transformers they had this big pitch for like oh look you can do way more computation because of parallelization they got great results in machine translation so you had um you had Transformers sort of doing quite well although not like astoundingly better than existing machine translation systems um and they but they were significantly more efficient to train right because you don't have this this parallelization problem you could compute on much more data much faster and you could make use of faster gpus much more um you know after that there were things like document generation where you had the sort of old standard of sequence to sequence model to the lstms and eventually everything became sort of Transformers all the way down um uh Transformers also enabled this revolution into pre-training which we'll go over uh in lecture next class um and sort of the efficiency the parallelizability allows you to compute on tons and tons of data and so after a certain point sort of on standard large benchmarks everything became Transformer based this ability to make use of lots and lots of data lots and lots of compute just put Transformers Head and Shoulders above lstms in let's say almost every sort of modern advancement in uh in natural language processing um there are many sort of drawbacks and and variants to Transformers you know the clearest one that people have tried to work on quite a bit is this quadratic compute problem so this all pairs of interactions right means that our sort of total computation for each block grows quadratically with the sequence length and in a student's question we heard uh that you know well as as the sequence length becomes long if I want to process you know a whole Wikipedia article a whole a whole novel that becomes quite unfeasible and actually you know that's a step backwards in some sense because for recurrent neural networks it only grew linearly with the sequence length um you know other things people have tried to work on are sort of better position representations because the absolute index of a word is not really you know the best way maybe to represent its position in a sequence um and just to give you an intuition of quadratic sequence length right remember that we had this big Matrix multiply here that resulted in this Matrix of n by n and Computing this is like a you know a big a big cost it costs a lot of memory um and so there's been work uh oh yeah and so you know if you think of the model dimensionality as like a thousand although today it gets much larger then for a short sequence of n is roughly 30 maybe the you know if you're Computing N squared times d uh 30 isn't so bad but if you had something like you know 50 000 then N squared becomes huge and sort of totally infusible so people have tried to sort of map things down to a lower dimensional space to get rid of the sort of quadratic computation but in practice I mean as people have gone to things like gpt3 chat GPT most of the computation doesn't show up in the self-attention so people are wondering sort of is it even necessary to get rid of some attention operations quadratic constraint it's an open form of research whether this is sort of necessary and then finally there have been a ton of modifications for the Transformer uh over the last you know five four ish years and um it turns out that the original Transformer plus maybe a couple of of modifications is pretty much the best thing there is still um there have been a couple of things that end up being important changing out the non-linearities and the feed forward Network ends up being important but it's sort of uh it's had lasting power so far and so it's it's but I think it's it's right for uh people to come through and think about how to sort of improve it in various ways so um pre-training is on Tuesday uh good luck on assignment four and then yeah we'll have the project proposal documents out tonight uh for you to talk about 
","['', 'self-attention', 'transformers', 'natural language processing', 'machine translation', 'recurrent neural networks (RNNs)', 'LSTMs', 'gradient problems', 'linear interaction distance', 'long distance dependencies', 'attention', 'sequence length', 'embedding matrix', 'self-attention operation', 'position vectors', 'keys', 'queries', 'values', 'masking', 'decoders', '']"
"hello welcome to cs224n today we'll be talking about pre-training uh which is another exciting topic on the road to Modern natural language processing um okay how is everyone doing thumbs up some side thumbs down wow no response bias there all you know all thumbs up oh sorry nice I like that Honesty that's good well um okay so we're now uh what is this week five yes it's week five and um we have a couple so this lecture um the Transformers lecture and then to a lesser extent Thursday's lecture on natural language Generation Um will be sort of the sum of lectures for the assignments you have to do right so assignment five is coming out on uh Thursday um and uh the the topics covered in this lecture and the you know self-attention Transformers and again a little bit of natural language generation will be tested in assignment five and then the rest of the course will go through some really fascinating topics and sort of modern uh natural language processing that should be useful for your final projects and future jobs and interviews and intellectual curiosity and um but uh you know I think that this today's lecture is significantly less um uh technical in detail than last Thursdays on self-attention and Transformers but should give you an idea of this sort of uh world of pre-training and sort of how it helps Define uh natural language processing today um so a reminder about assignment five your project proposals also are due on Tuesday next Tuesday um please do get those in try to get them in on time so that we can give you prompt feedback about your project proposals um and yeah so let's let's jump into it okay so uh what we're going to start with today is um a bit of a technical detail on uh word structure and sort of how we model the input sequence of words that we get so um in when we were teaching word to VEC and uh sort of all the methods that we've talked about so far we assumed a finite vocabulary right so we had a vocabulary V that you define via whatever you've looked at some data you've decided what the words are in in that data and so you know um you have some words uh like hat and learn and uh you know you have this embedding it's in red because you've learned it properly actually let's replace hat and learn with pizza and tasty those are better um and uh and so that's all well and good you see these words uh in your model and you have an embedding that's been learned on your data uh to sort of know what to do when you see those words but when you see some sort of variations maybe you see like tasty and maybe a typo like learn um or or maybe novel items where it's like a word that you know you as a human can understand as sort of this combination this is called derivational morphology uh of like this word Transformer that you know and if I which means you know take this noun and give me back you know a verb that means to make more like that noun to Transformer if I NLP might mean to you know make NLP more like using Transformers and such um and for each of these right this maybe didn't show up in your in your training Corpus and language is uh always doing this right people are always coming up with new words and there's new domains and there's the you know young people are always making new words it's great and so it's a problem for your model though right because you've defined this finite vocabulary and there's sort of no mapping in that vocabulary for each of these things even though their meanings should be relatively well defined based on the data you've seen so far it's just that the sort of string of characters that Define them aren't quite what you've seen and so what do you do well maybe you map them to this sort of universal unknown tokens this is Unk uh right so it's like oh I see something I don't know what I've never seen it before I'm going to say it's always represented by the same token ankh um and so that's been done in the past uh and that's sort of bad right because it's totally in like losing tons of information um but you know you need to map it to something uh and so that this is like a clear problem especially I mean in English it's a problem in many of the world's languages it's a substantially larger problem right so um you know English has relatively simple word structure there's a couple of conjugations for each verb like you know eat eats eaten ate um but in a language uh with much more complex morphology or word structure um you'll have a considerably more complex uh sort of set of things that you could see in the world so here is a a conjugation table for a Swahili verb and it has over 300 conjugations and if I Define the vocabulary to be every unique string of characters uh maps to its own word then every one of the 300 conjugations would get an independent Vector under my model which makes no sense because the 300 conjugations obviously have a lot in common and differ by sort of meaningful uh extents so you don't want to do this I'd have to have a huge vocabulary uh if I wanted all conjugations to show up and that's that's a mistake for efficiency reasons and for learning reasons any questions so far cool okay um and so what we end up uh doing um is we'll look at subword sub word structure sub word modeling so what we're going to do is we're going to say I'm not going to even try to Define what the set of all words is I'm going to Define my vocabulary to include parts of words there where am I oh uh right so um so I'm going to split words into sequences of known sub words and so there's a simple sort of algorithm for this where you start with all characters right so if I only had a vocabulary of all characters and maybe like an end of word symbol um I ha for a finite data set then I could no matter what word I saw in the future as long as I had seen all possible characters I could take the word and say I don't know what this word is I'm going to split it into like all of its individual characters so you won't have this UNC problem you can sort of represent any word and then you're going to find common adjacent characters and say okay A and B co-occur next to each other quite a bit so I'm going to add a new word to my vocabulary now it's all characters plus this new word a b which is a sub word and likewise I'm going so now I'm going to replace the character pair with the new sub word and repeat until you add a lot a lot a lot of vocabulary items through this process of what things tend to co-occur next to each other and so what you'll end up with is a vocabulary a very commonly a co-occurring sort of substrings by which you can build up words and this was originally developed for machine translation but then has been used considerably in pretty much all modern language models so now we have a hat and learn hat and learn so in our sub word vocabulary hat and learn showed up enough that they're their own individual words so that's sort of good right so simple common words show up as a word in your vocabulary just like you'd like them to but now tasty maybe gets split into TAA and then maybe you know in some cases this Hash Hash means like don't add a space next right so TAA and then AAA and then s-t-y right so I've actually taken one sort of thing that seems like a word and in my vocabulary it's now split into three sub word tokens so when I pass this to my Transformer or to my recurrent neural network right the recurrent neural network would take TAA as a as just a single element do the RNN update and then take AAA do the RNA and update and then sty so it could learn to process constructions like this and maybe I can even add more aaas in the middle right and have it do something similar instead of just seeing the entire word tasty and not knowing what it means is that that's feedback yeah uh how loud is that feedback we good okay I think we're fixed great um and so same with Transformer if I maybe Transformer as its own word and then if I and so you can see that you have sort of three learned embeddings instead of one sort of useless unkem betting this is just wildly useful and is used pretty much everywhere variants of this algorithm are used pretty much everywhere in uh like modern NLP questions yes if we have three embeddings for tasty do we just add them together so the question is if we have three embeddings for tasty do we just add them together uh if we want to represent so when we're actually processing the sequence I'd see something like I learned about the TAA AAA sty so it'd actually be totally separate tokens but if I wanted to then say what's my representation of this thing uh depends on what you want to do sometimes you average you average the contextual representations of the three or look at the last one maybe it at that point it's unclear what to do but everything sort of works okay click how do you what how do you know where to split yeah so um you know where to split based on the algorithm that I uh specified earlier for learning the vocabulary so you've learned this vocabulary by just combining commonly co-occurring adjacent strings of letters right so like a b co-occurred a lot so now I've got a new word that's a b um and then when I'm actually walking through and tokenizing I try to split as little as possible so I split words into the maximal uh sort of sub word that takes up the most characters they're algorithms for this uh yeah so like I'm like okay if I want to split this up you know like there's many ways I could split it up and you try to find some approximate like what the best way to split it into the fewest words is yeah do I ask the question is do people make use punctuation in the character set how do people do it yes absolutely so you know sort of from this point on so uh just assume that what text is given to these models is as unprocessed as possible you take it you try to make it sort of clean looking text where you've removed you know HTML tags maybe if it's from the Internet or or whatever um but then beyond that you process it as little as possible so that it reflects as well as possible what people might actually be using this for um so maybe earlier in the course when we were looking at word to VEC maybe we had what might have thought about oh we don't want word to vectors of punctuation or something like that um now everything is just as close as possible to what the text you'd get with people trying to use your system would be so yes uh in practice punctuation and like dot dot dot might be its own word you know and and maybe a sequence of like hyphens because people make big bars across you know tables yeah foreign [Music] could be multiple embeddings versus a single embedding like this is the like system tree those any differently uh the question is does the system treat any differently words that are like really themselves the whole word versus words that are sort of pieces you know the system has no idea they're all just indices into your embedding vocabulary Matrix um so they're all treated equally about really long ones that are I guess relatively common because if you're building up from the same character all the way up what happens then yeah the question is what happens to very long words uh if you're building up some sort of character Pairs and portions of characters uh you know in practice the statistics speak really well for themselves so if a long word is very common it will end up in the vocabulary and uh if it's not very common it won't um there are algorithms that aren't this that do slightly better in various ways um but the intuition that you sort of figure out what the common co-occurring substrings are sort of independent of length almost is is the right intuition to have and so yeah you can actually just look at the Learned vocabularies of a lot of these models and uh you see some long words uh just because they if they showed up a lot I'm curious how does it weigh the uh like the frequency so let's say there's like if ify or at the in your next slide it was like goodbye um at the very last one so if could be really common so how did the weight like the frequency of a sub word versus the length of it like it tries to spread it up into the smallest number but what if it split it up into three but one of them was super common yeah so the question is uh you know if Transformer is a sub word in my vocabulary and if it's a sub word and Y is a sub word and if I as a three letter Tuple is also a sub word how does it choose to like take the you know if I maybe it's not very common uh as opposed to splitting it into more subwords um it's just a choice we choose to try to take the smallest number of sub words because that tends to be uh more of the bottleneck as opposed to the having a bunch of very common very short sub words uh trans you know sequence length is a big problem in Transformers and this seems to be sort of what works although trying to split things into multiple options of a sequence and running the Transformer on all of them is the thing that people have done to see which one will work better but yeah having fewer bigger sub words tends to be the best sort of idea I'm going to start moving on though uh feel free to ask me more questions about this afterward okay so um so let's talk about pre-training from the context of the course so far uh so we at the very beginning of the course we gave you this quote which was you know you shall know a word by the company it keeps this was the sort of thesis of the distributional hypothesis right that the meaning of the word is defined by or at least reflected by what words it tends to co-occur around and we implemented this via word to VEC uh the same person who made that quote had a separate quote actually earlier uh that continues this sort of notion of meaning as defined by context which has something along the lines of well you know since the word shows up in context when we actually use it when we speak to each other the meaning of the word should be defined in the context that it actually shows up in and so uh you know the complete meaning of a word is always contextual and no study of meaning apart from a complete context can be taken seriously so right the big difference here is like at word tovec training time if I have uh the word uh record r-e-c-o-r-d when I'm training word to VEC I get one vector or two but you know one one vector meaning uh record the string um and uh it has to learn by what context it shows up in that sometimes you know it can mean I record I.E the verb or record I.E the noun right but I only have one vector to represent it and so when I use the word embedding of record uh it sort of has this mixture meaning of both of its sort of Senses right it doesn't get to specialize and say oh this part means record and this part means record and so where to back is going to just sort of fail um and and so I can build better representations of language through these contextual uh representations that are going to take things like recurrent neural networks or Transformers that we used before to build up sort of contextual meaning uh so so what we had before were pre-trained word embeddings and then we had sort of a a big box on top of it like a Transformer or an lstm that was not pre-trained right so so you learn via context your word embeddings here and then uh you have a task like sentiment analysis or machine translation or or parsing or whatever and you initialize all the parameters of this randomly and then you train uh to predict your label and the the big difference uh in you know today's work is that we're going to try to pre-train all the parameters so I have my big Transformer and instead of just you know pre-training my word embeddings with word to VEC I'm going to train all of the parameters of the network uh trying to teach it you know much more about language uh that I could use in my in my Downstream tasks so now I'm sort of the the labeled data that I have for say machine translation might need to be smaller I might not need as much of it because I've already trained much more of the network than I otherwise would have if I had just gotten sort of word to back embeddings okay so so here right I've pre-trained this entire sort of structure the word embeddings the Transformer on top everything's been trained via methods that we'll talk about today and so what does this give you I mean it gives you very strong representations of language so the meaning of record and record will be different in the sort of contextual representations that know what where in the sequence it is and what words are co-occurring with it in the specific input then word to back which only has one representation for record independent of where it shows up it'll also be used as strong parameter initializations for NLP models so in all of your homework so far you've worked with uh you know building out a natural language processing system sort of from scratch right like how do I initialize this weight Matrix and we always say oh you know small normally distributed noise like little values you know uh close to zero and here we're going to say well just like we were going to you know use the word to VEC embeddings and those sort of encoded structure I'm going to start maybe my machine translation system from a parameter initialization that's given to me via pre-training and then also it's going to give us probability distributions over language that we can use to to generate and otherwise and we'll talk about this okay so whole models are going to be pre-trained so um all of pre-training is effectively going to be centered around this idea of reconstructing the input so you have an input it's a sequence of text that some human has generated and the sort of hypothesis is that by masking out part of it and tasking a neural network with reconstructing the original input that neural network has to learn a lot about language about the world in order to do a good job of reconstructing the input right so this is now a supervised learning problem just like you know machine translation right I've taken this sentence that just existed Stanford University is located in say Palo Alto California or Stanford California I guess um and I have by removing this you know part of the sentence uh made a label for myself right the input is this sort of broken uh mask sentence and the label is Stanford or Palo Alto so if I give this example to a network and ask it to predict the center thing as it's doing its gradient step on this input it's going to encode information about the co-occurrence between this context Stanford University is located in and Palo Alto so by tasking it with this it might learn say where Stanford is what else might it learn well it can learn things about maybe syntax so I put blank Fork down on the table um here there's only a certain set of words that could go here I put the fork down on the table I put a fork down to the table these are syntactic constraints right so the context shows me sort of What kinds of words can appear in what kinds of contexts the woman walked across the street checking for traffic over blank shoulder any ideas on what could go here right so um this sort of this sort of co uh co-reference between this entity who is being discussed in the world this woman and her shoulder now when I discuss you know this is sort of linguistic concept the word her here is a co-referrent to woman right it's referring to the same entity in the discourse and so the network might be able to learn things about you know like kind of what entities are doing what where uh it can learn things about sort of semantics so if I have I went to the ocean to see the fish Turtle seals and blank then the word that's in the blank should be sort of a member of the class that I'm thinking of as a person writing this sentence of stuff that I see when I go to the ocean and see these other things as well right so in order to do this prediction task maybe I learned about you know the semantics of of uh aquatic creatures okay so what else could I learn I've got overall the value I got from the two hours watching it was the sum total of the popcorn and drink the the movie was blank what kind of task could I be learning from doing this sort of prediction problem sentiment exactly so this is just a naturalistic sort of text that I naturally wrote uh myself um but by saying oh the movie was bad I'm learning about sort of the latent sentiment of the person who wrote This what they were feeling about the movie at the time so maybe if I see a new review later on I can just paste in the review say the movie was blank and if the model generates bad or good that could be implicitly solving the task of sentiment analysis so here's another one Ira went to the kitchen to make some tea standing next to Ira Zuko pondered his Destiny Zuko left the blank okay so in this scenario we've got a world implicitly that's been designed by the person who is creating this text right I've got physical locations in the discourse like the kitchen uh and I've got Zuko uh we've got iros in the kitchen Zuko's next to iro so Zuko must be in the kitchen so what could Zuko leave but the kitchen right and so in terms of you know latent Notions of embodiment and physical location the way that people talk about people you know being next to something and then leaving something could tell you uh stuff about sort of yeah a little bit about how the world works even so here's the secret sequence I was thinking about the sequence that goes one one two three five eight thirteen twenty one uh blank and um you know this is a pretty tough one right this is the Fibonacci sequence right create a model by looking at the a bunch of numbers from the Fibonacci sequence learn to in general predict the next one a question you should be thinking about throughout the lecture okay any questions on these sort of examples of what you might learn from predicting the context okay okay cool um so uh you know a very simple way to think about pre-training is pre-training is language modeling so we saw language modeling earlier in the course and now we're just going to say instead of using my language model just to provide probabilities over the next word I am going to train it on that task right I'm going to actually model the distribution P Theta of the word t given all the words previous and there's a ton of data for this right there's you know there's just an amazing amount of data for this in a lot of languages especially English there's very little data for this in actually most of the world's languages which is a separate problem um but you can free train just through language modeling right so I'm going to sort of do the teacher forcing thing so I have IRL I predict goes I have goes I predict two and I'm going to train my sort of lstm or my Transformer to do this task and then I'm just going to keep all the weights okay I'm going to save all the network parameters um and then once I have these parameters right instead of generating from my language model I'm just going to use them as an initialization for my parameters so I have this pre-training fine tuning Paradigm two steps most of you I think in your well maybe not this year let's say a large portion of you this year in your final projects will be doing the pre-training fine-tuning sort of Paradigm where someone has done the pre-training for you right so you have a ton of text you learn very general things about the distribution of words and sort of the latent things that that tells you about the world and about language and then in step two you've got some tasks maybe sentiment analysis and you have maybe not very many labels you have a little bit of labeled data and you adapt the pre-trained model to the task that you care about by further doing gradient steps on this task so you give it the movie was you predict happy or sad and then um you sort of Continue to update the parameters based on the initialization from the pre-training and this just works exceptionally well I mean unbelievably well compared to training from scratch intuitively because you've taken a lot of the burden of learning about language learning about the world off of the data that you've labeled for sentiment analysis and you're sort of giving that task of learning all this sort of very general stuff to the much more General task of language modeling yes but we didn't have much data in other languages what do you mean by that was it just text in that language yeah labeled in some way so the question is uh you know you said we have a lot of data in English but not in other languages um what do you mean by data that we don't have a lot of in other languages is it just text it's literally just text no annotations because you don't need annotations to do language model pre-training right the existence of that sequence of words that someone has written provides you with all these pairs of input and output input iro output goes input iro goes output two those are all labels sort of that you've constructed from the input just existing but you know in most languages even on the entire internet I mean there's about 7 000 ish languages on Earth and most of them don't have the sort of you know uh billions of words that you might want to to train these systems on uh yeah entire thing are you still only like one vector representation per word the question is if you're pre-training the entire thing do you still learn one vector representation per word you learn one vector representation that is the non-contextual input vector right so you have your vocabulary Matrix you've got your embedding Matrix that is vocabulary size by model dimensionality and so yeah iro has one vector goes has one vector but then the Transformer that you're learning on top of it takes in the sequence so far and sort of gives a vector to each of them that's dependent on the context in that case but still at the input you only have one embedding per word yeah so what's sort of like metric would you use to like evaluate it's supposed to be like General right so but things like application specific metrics which one do you use yeah so the question is what metric do you use to evaluate pre-trained models since it's supposed to be so General um but there are lots of sort of very specific evaluations you could use um it will get into a lot of that in the rest of the lecture uh while you're training it you can use Simple metrics that sort of correlate with what you want but aren't actually what you want just like the probability quality right so you can evaluate the perplexity of your language model just like you would have when you cared about language modeling and it turns out to be the case that better perplexity correlates with all the stuff that's much harder to evaluate like lots and lots of different tasks but also the natural language processing Community has built very large sort of Benchmark Suites of varying tasks to try to get at sort of a notion of generality although that's very very difficult it's sort of ill-defined even and so when you develop new pre-training methods what you often do is you try to pick a whole bunch of evaluations and show that you do better on all of them you know and and that's your argument for generality okay um so so why should this sort of pre-training fine-tuning two-part Paradigm help uh you know this is still an open area of research but the intuitions are all you're going to take from this course so right so pre-training provides some sort of starting uh parameters L Theta so this is like all the parameters in your network right from trying to do this minimum over all possible settings of your parameters of the pre-training loss and then the fine-tuning process takes uh you know your data for fine tuning you've got some labels and it tries to approximate the minimum through gradient Descent of the loss of the fine-tuning task of theta but you start at Theta hat right so you start gradient descent at Theta hat which your pre-training process gave you and then you know if you could actually solve this Min and wanted to it sort of feels like the starting point shouldn't matter but it really really really does it really does um uh so that's and we'll talk a bit more about this later but um the process of gradient descent you know maybe it sticks relatively close to the Theta hat during fine tuning right so um you know you start at Theta hat and then you sort of walk downhill with gradient descent until you hit sort of a valley and that Valley ends up being really good because it's close to the pre-training parameters which were really good for a lot of things this is a cool place where sort of practice and Theory are sort of like meeting where like optimization people want to understand why this is so useful NLP people sort of just want to build better systems um so uh yeah maybe the stuff around Theta hat tends to generalize well if you want to work on this kind of thing you should talk about it yeah the classic waiting to send sticks relatively close but what if we were to use a different Optimizer how would that change their results the question is uh if stochastic gradient descent sticks relatively close what if we use a different Optimizer I mean if we use sort of any common variant of gradient descent like any first order method like atom which we use in this course or add a grad or they all have this very very similar properties um other types of optimization we just tend to not use so who knows ah yeah yeah fine tuning works better than just fine tuning but making article like adding more layers more data yeah uh the question is why does the pre-trained fine-tuned Paradigm work better than just making the model more powerful adding more layers adding more data to just the fine tuning um that's a you know the simple answer is that you have orders of magnitude more data that's unlabeled that's just text that you found then you do you know carefully labeled data and the tasks that you care about right because that's expensive to get it has to be examples of your movie reviews or whatever that you've had someone label carefully um so you have you know something like on the internet uh at least five trillion maybe 10 trillion words of this and you have maybe a million words of your label data or whatever over here so it's just like the it's just the scale is way off um but there's also an intuition that like learning to do a very very simple thing like sentiment analysis um is not going to get you a very general generally able agent in a wide range of settings compared to language modeling so like it's hard to get how to put it even if you have a lot of labeled data of movie reviews of the kind that people are writing today maybe tomorrow they start writing slightly different kinds of movie reviews and your system doesn't perform as well whereas if you pre-trained on a really diverse set of text from a wide range of sources in people it might be more adaptable to seeing stuff that doesn't quite look like the training data you showed it even if you showed it a ton of training data so one of the sort of big takeaways of pre-training is that you get this huge amount of sort of variety of text uh on the internet you have to be very careful I mean you yeah you should be very careful about what kind of text you're showing it and what kind of text you're not because the internet is full of you know um awful text as well um but some of that generality just comes from how hard this problem is and how much data you can show it so much data how do you then train it so that it considers the stuff that you're fine-tuning it with as like more important more sale into a passive Trend if you rather than just one in a billion articles yeah it's a good question so the question is uh given that the amount of data on the pre-training side is orders of magnitude more than the amount of data on the fine tuning side how do you sort of get across to the model that okay actually the fine tuning task is like what I care about so like focus on that um it's it's about the fact that I did this first the pre-training first and then I do the fine tuning second right so I've done I've gotten my parameter initialization from this I've set it somewhere and then I fine tune I move to where the parameters are doing well for this task afterward and so well it might just forget a lot about how to do this because now I'm just asking it to do this at this point uh I should move on I think um but we're going to keep talking about this in in much more detail with more concrete uh elements so um okay so uh let's talk about model pre-training oh wait that did not Advance the slides nice okay let's talk about model pre-training three ways uh in our Transformers lecture uh Tuesday we talked about encoders encoder decoders and decoders and we'll we'll do decoders last because um actually many of the largest models uh that are being used today Are all uh decoders and so we'll have a bit more to say about them um right so let's recall these three so encoders get bi-directional context you have a single sequence and you're able to see the whole thing kind of like an encoder in machine translation encoder decoders have one uh portion of the network that gets bi-directional context so that's like the source sentence of my machine translation system and then they're sort of paired with a decoder that gets unidirectional context so that I have this sort of uh informational masking where I can't see the future so that I can do things like language modeling I can generate the next token of my translation whatever so you could think of it as you know I've got my source sentence here and my partial translation here and I'm sort of decoding out the translation and then decoders only are things like language models we've seen a lot of this so far and there's pre-training for all three sort of large classes of models and how you pre-train them and then how you use them depends on the properties and the productivities of the specific architecture so let's let's look at encoders first um so we've looked at language modeling quite a bit but we can't do language modeling with an encoder because they get bi-directional context right so if I'm down here uh at I and I want to present I want to predict the next word it's a trivial task at the at this level here to predict the next word because in the middle I was able to look at the next word and so I should just know there's nothing hard about learning to predict the next word here because I could just look at it see what it is and then you know copy it over so when I'm training an encoder in something uh for for pre-training I have to be a little bit more clever in practice what I do is something like this uh I take the input and I modify it somewhat I mask out words sort of like I did in the examples I gave at the beginning of class so I blank to the blank right and then I have the network predict uh with this whole you know I haven't built contextual representations so now this Vector representation of the blank sees the entire context around it here and then I predict the word went and then here the word store any questions okay and you can see how this is doing something quite a bit like language modeling but with you know bi-directional context I've removed the Network's information about the words that go in the blanks and I'm training it to reconstruct that so I only have lost terms right I only ask it to actually do the prediction compute the loss back propagate the gradients for the words that I've masked out and you can think of this as you know instead of learning probability of X where X is like a sentence or a document this is learning the probability of X the real document given X tilde which is this sort of corrupted document with some of the information missing missing okay and so maybe we get the sequence of vectors here one per word which is the output of my encoder in blue and then I'd say that for the words that I want to predict y i i draw them this is the Sim means the probability is uh proportional to uh you know my embedding Matrix times um my representation of it so it's a linear transformation of that last thing here so this a plus b is this red portion here and then do the prediction and I train the entire network to do this yes so far do we just do it as we are is there something you can do it the question is do we just choose words randomly to mask out or is there a scheme mostly randomly we'll talk about a slightly smarter scheme in a couple of slides but yeah just mostly randomly uh yeah what was that last part on the bottom um exit the maps version of like if it's the first or the very last sentence uh yeah so so I'm saying that I'm defining X tilde to be this input part where I've got the masked version of the sentence with these sort of words missing and then I'm defining a probability distribution that's the probability of a sequence conditioned on the input being the sort of corrupted sequence the masked sequence okay um so uh this brings us to a very very popular and uh sort of NLP model that you need to know about it's called Bert and it was the first one to popularize this masked language modeling objective um and they released the weights of this pre-trained Transformer that they pre-trained via something that looks a lot like Mass language modeling and so these you can download you can use them via a code that's released by the company hugging face that we have you know continue to bring up many of you will use a model like Birch in your final project because it's such a useful builder of representations of language and context so let's talk a little bit about the details of mass language modeling in Bert first we'd take 15 of the sub word tokens so remember all of our inputs now are subword tokens I'm making I've made them all look like words but just like we saw at the very beginning of class each of these tokens could just be some portion some sub word and I'm going to do a couple of things with it sometimes I am going to just mask out the word and then you know predict the true word sometimes I'm going to replace the word with some random sample of another word from my distribution from my vocabulary and predict the real word that was supposed to go there and sometimes I'm going to not change the word at all and still predict it the intuition of this is the following um if I just had to build good representations of uh in the sort of middle of this network for words that are masked out then when I actually use the model at test time on some real you know review to do sentiment analysis on well there are never going to be any tokens like this so maybe the model won't do a very good job because it's like oh you know I have no job to do here because I only need to deal with the mask tokens by giving it sequences of words where sometimes it's the real word that needs to be predicted sometimes you have to detect if the word is wrong the idea is that now when I give it a sentence uh that doesn't have any masks it actually sort of does a good job of representing all the words in context because it has this chance that it could be asked to predict anything at any time okay so uh the folks at uh at Google who were defining this had a separate additional task that is sort of interesting to think about so this was their their Bert model from their paper they had their position embeddings just like we saw from our Transformers lecture token embeddings just like we saw from the Transformers lecture but then also they had this thing called a segment embedding where they had two possible segments segment a and segment B and uh they had this additional task where they would get a big chunk of text for Segment a and a big chunk of text for Segment B and then they would ask the model is segment b a real continuation of segment a well so the text that actually came next or did I just pick this big segment randomly from somewhere else and the idea is that this should teach the network something some notion of sort of long distance coherence right about sort of the connection between a bunch of text over here and a bunch of text over there turns out it's not really necessary but it's an interesting idea and sort of similar things have continued to have some sort of influence since then but again you should get this intuition that we're trying to come up with hard problems for the network to solve such that by solving them it has to learn a lot about language and we're defining those problems by making simple Transformations or removing information from text that just happen to occur questions yeah the plus signs do we concatenate the vector so do we do an element-wise Edition uh the question is for these plus signs do we concatenate the vectors or do element wise Edition we do element wise Edition uh you could have concatenated them however the one of the big sort of conventions of all these networks is that you always have exactly the same number of Dimensions everywhere at every layer of the network it just makes everything very simple so just saying everything's the same Dimension and then uh doing addition just ends up being simpler yeah the next sentence prediction not necessarily yeah why was the next sentence prediction not necessary I mean it one thing that it does that's a negative is that now um the sort of content the effective context length for a lot of your examples is halved so one of the things that's useful about pre-training seemingly is that you get to build representations of very long sequences of text so this is very short but in practice segment a was going to be something like 250 words and segment B was going to be 250 words and in the paper that sort of let us know that this wasn't necessary they always had a long segment of 500 Words and it seemed to be useful to always have this very long context because longer contexts help give you more information about the role that each word is playing in that specific context right if I see one word it's hard to know if it's just the record it's hard to know what it's supposed to mean but if I see a thousand words around it it's much clearer what its role is in that context is so yeah it cuts the effective context size as one answer um what another thing is that this is actually much more difficult this is a much more recent paper uh that I don't have in the slides but it's been shown since then that these models are really really bad at the next sentence prediction task so it could be that maybe it just like was too hard at the time uh and so it just like wasn't useful because the model was failing to do it at all so I give the link for that paper later why we need to do a next sentence prediction what about just masking and predicting I missed that jump so yeah so the question is why do we need to do next sentence prediction why not just do the masking we saw before that's the thing you seem to not need to do next to this prediction but you know as sort of like his history of the research it was thought that this was useful and the idea is that it required you to develop this sort of pairwise like do these two segments of text interact how do they interact are they related the sort of longer distance notion and many NLP tasks are defined on pairs of things and they thought that might be useful and so they published it with this and then someone else came through published a new model that didn't do that and it and it sort of did better so you know this is just yeah so yeah uh there are intuitions as to why it could work it just didn't it was doing both it was doing both this next sentence so Bert was doing both this next sentence prediction uh evaluate uh training as well as this masking training uh all at the same time and so you had to have a separate predictor head on top of bird a separate predictor sort of classification thing and you know so one detail there is that there's this special word at the beginning of Bert in every sequence that's CLS and you know you can define a predictor on top of that sort of fake word embedding that was going to say is the next sentence real or fake or not yeah okay I'm gonna move on um and so this gets that sort of the question that we had earlier about how do you evaluate these things um there's a lot of different NLP tasks out there gosh and uh you know when people were defining these papers they would look at a ton of different evaluations that had been sort of compiled as a set of things that are still hard for today's systems so are you detecting paraphrases between questions or two quora questions actually the same question that turns out to be hard um you know uh can you do sentiment analysis on this hard data set can you tell if sentences are linguistically acceptable are they grammatical or not um are two sequences similar semantically do they mean sort of vaguely the similar thing um and we'll talk a bit about natural language inference later but that's the task of defining sort of if I say you know I saw the dog that does not necessarily mean I saw the little dog but saying I saw the little dog does mean I saw the dog so that's sort of this natural language inference task and you know the Striking the difference between sort of pre-pre-training days where you had this sort of this row here before you had substantial amounts of pre-training and Bert was just like the field was taken aback in a way that's hard to describe you know very carefully crafted architectures for each individual task where everyone was designing their own neural network and doing things that they thought were sort of clever as to how to define all the connections and the weights and whatever to do their tasks independently so everyone was doing a different thing for each one of these tasks roughly all of that was blown out of the water by just build a big Transformer and just teach it to predict the missing words a whole bunch and then fine tune it on each of these tasks so this was this was just a sea change in the field people were I mean amazed it's a little bit less flashy than chat GPT I'll admit but it's really part of the story that gets us to it you know um okay questions so like uh to get stuff out of the like the during the encode their pre-training stage encoder usually outputs like uh some sort of hidden values how do we correlate those the words that we are trying to test against so the question is you know the the encoder output is a bunch of you know hidden values um how do we actually correlate those values to stuff that we want to predict I'm going to go on to the next slide here to bring up this this example here right so the encoder gives us for each input word token a vector of that token that represents the token in context and the question is you know how do we get these representations and and turn them into uh sort of answers for the tasks that we care about and um the answer comes back to do [Music] something like this uh something like this Maybe wow sure um so when we were doing a pre-training right we had the Transformer that was giving us our representations and we had this little last layer here this little um sort of affine uh transformation that moved us from the encoder's hidden State size to the vocabulary to do our prediction and we just removed this last prediction layer here and let's say we want to do something that is uh classifying the sentiment of the sentence we just pick arbitrarily maybe the last word in the sentence and we stick a linear classifier on top and map it to positive or negative and then fine tune the whole thing okay so so yeah the Bert model uh had uh two different models one was 110 million parameters one was 340 million keep that sort of in the back of your head sort of percolating as we talk about models with with many many more parameters later on it was trained on um uh 800 million words plus that is definitely wrong maybe two point maybe 25 million words but on the order of less than a billion words of text quite a bit still um and it was trained on what was considered at the time to be a whole lot of compute just you know it was Google doing this and they released it and we were like oh who has that kind of compute but Google although nowadays it's not considered to be very much um but fine-tuning is practical and common on a single GPU so you could take the burp model that they'd spend a lot of time training and fine-tune it yourself on your task uh on even sort of a very a very sort of small GPU uh okay so so one question is like well this seems really great why don't we just use this for uh everything um uh-huh yeah uh and the answer is well you know what is the sort of pre-training objective what's the structure of the pre-trained model good for uh Bert is really good for sort of filling in the blanks but it's much less naturally used for actually generating text right so I wouldn't want to use to generate a summary of something because it's not really built for it it's not it doesn't have a natural notion of predicting the next word given all the words that came before it so maybe I want to use Bert if I want a good representation of say a document to classify it give it one of a set of topic labels or say it's toxic or non-toxic or whatever but I wouldn't want to use it to generate a whole sequence uh okay some extensions of Bert so we had a question earlier of whether you just mask things out randomly one thing that seems to work better is uh you uh mask out sort of whole contiguous spans uh because uh sort of the difficulty of this problem is much easier than it would otherwise be because uh sort of this is part of irresistibly and you can tell very easily based on the sort of sub words that came before it whereas uh if I have a much longer sequence it's a trade-off but you know this might be a harder problem and it ends up being better to do this sort of span-based masking than random masking and that might be because sub words make very simple prediction problems when you mask out just one sub word of a word versus all the subwords of a word okay so those this ends up doing much better there's also a paper called the Roberta paper which showed that the next sentence prediction wasn't neces wasn't necessary they also showed that they really should have trained it on a lot more text so Roberta is a drop-in replacement for Bert so if you're thinking of using just use your Brita it's better and it gave us this intuition that we really don't know a whole lot about the best practices for training these things you sort of train it for as long as you're willing to and things do good stuff and whatever um so this is very but it's very difficult to do sort of uh iteration on these models because they're big it's expensive to train them uh another thing that you should know for your final projects in the world ahead is this notion of fine-tuning all parameters of the network versus just a couple of them uh so what we've talked about so far is you pre-train all the parameters and then you fine-tune all of them as well so all the parameter values change uh an alternative which you call parameter efficient or lightweight fine-tuning uh you sort of choose little bits of parameters or you choose the very smart way of keeping most of the parameters fixed and only fine-tuning others and the intuition is that you know these pre-trained parameters were really good and you want to make the minimal change from the pre-trained model to the model that does what you want so that you keep some of the generality some of the goodness of the pre-training so one way that this is done is called prefix tuning prompt tuning is very similar where you actually freeze all the parameters of the network so I've pre-trained my network here and I've never changed any of the parameter values instead I make a bunch of fake sort of pseudo word vectors that I propend to the very beginning of the sequence and I train just them sort of unintuitive it's like these would have been like inputs to the network but I'm specifying them as parameters and I'm training everything to do my sentiment analysis task just by changing the values of these sort of fake words and this is nice because you know I get to keep all the good pre-trained parameters um and then just specify this sort of diff that ends up generalizing better this is a very open field of research but this is also cheaper because I don't have to compute the gradients or I don't have to store the gradients and all the optimizer state with respect to all these parameters I'm only training a very small number of parameters uh yeah it's like make parameters and as if like here but it doesn't make any difference but he's at the end of the beginning in a decoder you have to put them at the beginning because otherwise you don't see them before you process the whole sequence uh yes a few layers I only train the new layers but the question is can we just attach a new layers of the sort of top of this and only train those absolutely this works a bit better another thing that works well sorry we're running out of time um is taking each weight Matrix so I have a bunch of weight matrices in my Transformer and I freeze the weight Matrix and learn a very low ranked little diff and I set the weight matrix's value to be sort of the original Value Plus my my sort of very low rank diff uh from the original one and this ends up being a very similarly useful technique and the overall idea here is that again I'm learning way fewer parameters than I did Via pre-training and freezing most of the pre-training parameters okay encoder decoders so um for encoder decoders we could do something like language modeling right I've got my input sequence here encoder output sequence here and I could say this part is my prefix for sort of having bi-directional context and I could then predict all the words that are sort of in the latter half of the sequence just like a language model and that would work fine um and so this this is something that you could do right you sort of take it along text split it into two give half of it to the encoder and then generate the second half with the decoder uh but in practice what works much better is this notion of span corruption span corruption is going to show up in your assignment five and the idea here is a lot like Bert but uh in a sort of generative sense where I'm going to mask out a bunch of words in the input thank you mask token one me to your party mask token two week and then at the output I generate the mask token and then what was supposed to be there but the mass token replaced it right so thank you then predict for inviting at the output meet your party last week and what this does is that it um allows you to have bi-directional context right I get to see the whole sequence except I can generate the parts that we're missing so this feels a little bit like you mask out parts of the input but you actually generate the output as a sequence like you would in language modeling so this might be good for something like machine translation where I have an input that I want bi-directional context in but then I want to generate an output and I want to pre-train the whole thing so this was shown to work better than language modeling at the scales that these uh Folks at Google were able to test back in 2018 this is still quite quite popular um yeah there's a lot of numbers it works better than the other stuff I'm not going to worry about it um you know there's a fascinating property of these models also so um so T5 was the model that was originally uh introduced with salience band masking and you can think of you know at pre-training time you saw a bunch of things like Franklin D Roosevelt was born in you know blank and you generated out the blank and uh there's this task called um open domain question answering which has a bunch of trivia questions like you know when was Franklin D Roosevelt born and then you're supposed to generate out the answer as a string just like just from your parameters right so you did a bunch of pre-training you saw a bunch of text and then you're supposed to generate these answers and what's fascinating is that this sort of salience band masking method allowed you to pre-train and then fine tune on some examples of questions trivia questions and then when you tested on new trivia questions it would sort of the model would sort of implicitly extract from its pre-training data somehow the answer to that new question that it never saw explicitly at fine tuning time so it learned this sort of implicit retrieval sometimes sometimes you know less than 50 of the time or whatever but you know much more than random chance yeah um and that's just sort of fascinating right so you've sort of learned to access this sort of latent knowledge that you stored up by pre-training and so yeah you just pass it the text when was Roosevelt born and it would pass out an answer and one thing to know is that the answers always look very fluent they always look very reasonable but they're frequently wrong and that's still true of things like chat gbt um yeah okay so that's that's like encoder decoder models um next up we've got decoders and we'll spend a long time on decoders so this is just our normal language model so I get a sequence of hidden States for my decoder the the models this words can only look at themselves not the future and then I predict you know the next word in the sentence and then here again I can you know to do sentiment analysis maybe take the last state for the last word and then predict happier sad based on that last embedding back propagate the gradients the whole network train the whole thing or do some kind of lightweight or or parameter efficient fine tuning like we mentioned earlier so this is our our you know pre-training a decoder and um you know I can just pre-train it on language modeling um so again you might want to do this if you are wanting to generate generate texts generate things uh this is you sort of can use this like you use an encoder decoder but in practice as we'll see a lot of the sort of biggest uh most powerful pre-trained models tend to be decoder only it's not really clear exactly why except they seem a little bit simpler than encoder decoders um and you get to share all the parameters in one big Network for the decoder whereas an encoder decoder you have to split them sort of some into the encoder some into the decoder so for the rest of this lecture we'll talk only about decoders so even and modern things uh the biggest networks do tend to be decoders so we're coming all the way back again to 2018 and the GPT model from openai was a big success it had 117 parameter a million parameters uh it had you know 768 dimensional hidden States and uh it had this vocabulary that was 40 000 ish words that was defined via a method like what we showed at the beginning of class trained on Books Corpus and um you know actually you know GPT never actually showed up in the original paper uh the sort of uh it's unclear what exactly it's supposed to refer to um but uh this model was a precursor to all the things that you're hearing about nowadays uh if you move forward uh oh yeah so if you um so if we wanted to do something like natural language inference right which says you know take these pairs of sentences the man is in the doorway the person is near the door and uh say that these mean that one entails the other the sort of premise entails the hypothesis that I can believe the hypothesis if I believe the premise I just sort of concatenate them together right so give it maybe a start token pass in one sentence pass in some delimiter token pass in the other and then predict uh sort of yes no entailment not entailment fine tuning gbt on this it worked really well um and then you know Bert came after GPT Bert did a bit better it had bi-directional context um but you know it did it did uh sort of an excellent job and then came gpt2 where they focused more on the generative abilities of the network so um right we looked at uh now a much larger Network we've gone from 117 million to 1.5 billion and given some sort of prompt it could generate at the time a quite surprisingly coherent continuation to The Prompt so it's telling this sort of story about uh about scientists and unicorns here um and this size of model is still sort of small enough that you can use on a small GPU and fine-tune and whatever and its capabilities of generating long coherent texts was just sort of um exceptional at the time it was also trained on more data although I don't uh something like 9 billion words of text um and then so after gpt2 we come to gpd3 sort of walking through these models and then we come with a different way of interacting with the models so we've interacted with pre-trained models in two ways so far we've sort of sampled from the distribution that they Define uh We've generated text via like a machine translation system or whatever or you fine-tuned them on a task that we care about and then we take their predictions um but gpt3 seems to have an interesting new ability uh it's much larger and it can do some tasks without any sort of fine-tuning whatsoever uh gbd3 is much larger than gpd2 right so we went from GPT 100-ish million parameters gbd2 1.5 billion cpt3 175 billion much larger uh trained on 300 billion words of text and this sort of notion of in context learning that it could Define or figure out patterns in the training or in the example that it's currently seeing and continue the pattern uh is called in context learnings you got you know the word thanks and I pass in this little arrow and say okay thanks goes to you know mercy and then hello goes to bonjour and then you know they give it all of these examples and ask it um what you know otter should go to and it's learned to sort of continue the pattern and say that this is the translation of otter so now remember this is a single sort of input that I've given to my to my model and I haven't said oh do translation or fine tune it on translation or whatever I've just passed in the input given it some examples and then it is able to to some extent uh do this seemingly complex task that's in context learning uh and here are more examples you know maybe you give it examples of addition and then it can do some uh some simple addition afterward uh you give it in this case this is sort of rewriting typos they can figure out how to rewrite typos in context learning for for machine translation and this was the start of this idea that there were these emergent properties that showed up in much larger models and it wasn't clear when looking at the smaller models that you'd get this sort of new this qualitatively new Behavior out of them right like it's not obvious from just the language modeling signal right gpt3 is just trained on that decoder only just next predict the next word that it would as a result of that training learn to perform seemingly quite complex things as a function of its context um yeah okay one or two questions about that um this should be quite surprising I think right like so far we've said talked about good representations contextual representations meanings of words in context this is some very very high level pattern matching right it's coming up with patterns in just the input data and that one sequence of text that you've passed it so far and it's able to sort of identify how to complete the pattern uh and as you think what kinds of things can this solve what are its capabilities whether it's limitations this ends up being an open area of research sort of what are the kinds of problems that you maybe saw in the training data lab maybe gpt3 saw a ton of pairs of words right they saw a bunch of you know dictionaries bilingual dictionaries in its training data so it learned to do something like this or is it doing something much more General where it's really learning the task in context you know the actual story we're not totally sure something in the middle it seems like it has to be tied to your train data in ways that we don't quite understand but there's also a non-trivial ability to learn new sort of at least types of patterns just from the context so this is a very interesting thing to work on now we've talked a lot about the size of these models so far and as models have gotten larger they've always gotten better we train them on more data um right so gpd3 was trained on 300 billion words of text and it was 175 billion parameters and you know at that scale it costs a lot of money to build these things and it's very unclear whether you're getting the best use out of your money like it's bigger really what you should have been doing in terms of the number of parameters um so you know the cost of training one of these is roughly you take the number of parameters you multiply it by the number of tokens that you're going to train it on the number of words and uh some folks at deepmind I forgot the citation on this some folks at deepmind uh realized through some experimentation that actually gpt3 was just comically oversized right so chinchilla the model they trained is less than half the size and works better but they just trained it on way more data um and this is sort of an interesting sort of trade-off about you know how do you best spend your compute I mean you can't do this more than a handful of times even if you're you know Google um so you know open open questions there as well um another sort of way of interacting with these networks that has come out recently is called Chain of Thought um so the prefix right we saw in the in context learning slide that the prefix can help sort of specify what task you're trying to solve right now and it can do even more so here's standard sort of prompting we have a prefix of examples of questions and answers so you have a question and then an example answer so that's your prompt that's specifying the task and then you have a new question and you're having the model generate an answer and it generates it wrong and Chain of Thought prompting uh says well how about in the example in the demonstration we give we give the question and then we give this sort of decomposition of steps towards how to get an answer right so I'm actually writing this out as part of the input I'm I'm giving annotations as a human to say oh you know to solve this sort of word problem here's how you could think it through ish and then I give it a new question and the model says oh I know what I'm supposed to do I'm supposed to First generate a sequence of steps of intermediate steps and then next say the answer is and then say what the answer is and it turns out and this should again be very surprising that the model can tend to generate plausible sequences of steps and then much more frequently generates the correct answer after doing so relative to trying to generate the answer by itself so you can think of this as a scratch Pad you can think of this as uh increasing the amount of computation that you're putting into trying to solve the problem sort of writing out your thoughts right as I generate each word of this continuation here I'm able to condition on all the past words so far and so maybe it just uh yeah allows the network to sort of decompose the problem into smaller simpler problems which is more able to solve each um no one's really sure why this works exactly either at this point with networks that are this large they're emergent properties are both very powerful and exceptionally hard to understand and very hard you should think uh to trust because it's unclear what its capabilities are and what its limitations are where it will fail so what do we think pre-training is teaching gosh a wide range of things even beyond what I've written in this slide which I mostly wrote two years ago right so it can teach you trivia and syntax and co-reference and maybe some lexical semantics and sentiment and some reasoning like way more reasoning than we would have thought even three years ago um and yet they also learn and exacerbate racism and sexism all manner of biases um there's more on this later but it's uh the generality of this is really I think what's taken many people aback and so increasingly these objects are not just um studied for the sake of using them but studied for the sake of understanding anything about how they work and how they fail uh yeah any questions has anyone tried like a benchmarking like GPT for like programming tasks like how accurate these does etc yeah the question is has anyone tried benchmarking GPT for programming tasks anyone seen how well it does um yes so there's definitely examples of people using GPT uh three four simple programming things and then you know the modern state-of-the-art competitive programming Bots are all based on ideas from language modeling uh and I think I think they're all also based on pre-trained language models themselves like if you just take all of these ideas and apply it to like GitHub uh then you get some very interesting emergent behaviors relating to code uh fallout and so yeah I think all of the best systems use this more or less so lots of benchmarking there for sure through the basis for what like GitHub co-pilot is going to do the question is is this the basis is that what we just mentioned the basis for the GitHub copilot system yes absolutely we don't know exactly what it is in terms of details but it's all these ideas what if you have a situation where you have you know still a large amount of data for you know General data and then you have also a large amount of data for your fine tuning task at one point is it better to train a new model for that pioneering versus you know get data from both yeah the question is what if you have a large amount of data for pre-training and a large amount of data for fine tuning when is it better to do sort of a separate training on just the fine-tuning data um almost never if you have a bunch of data for the task that you care about what's frequently done instead is three-part training where you pre-train on a very broad Corpus then you sort of continue to pre-train using something like language modeling on an unlabeled version of the label data that you have you just like strip the labels off and just treat it all as text and do language modeling on that adapt the parameters a little bit and then do the final stage of fine-tuning with the labels that you want and that works even better this interesting paper called Don't Stop pre-training nice uh final question that's a lot of questions on anyone new new someone knew the question yeah um I was wondering do you know if there's like a lot of instances where a pre-trained model can do some tasks not soon before I even know yeah so are there any instances of where a pre-trained model can do a task that it hasn't seen before uh you know without fine-tuning the question is what is hasn't seen before mean right like uh these models especially gpt3 and similar very large models you know during pre-training did it ever see something exactly like this sort of word problem arithmetic maybe maybe not it's actually sort of unclear it's clearly able to recombine sort of bits and pieces of tasks that it saw implicitly during pre-training we saw the same thing with trivia right like language modeling looks a lot like trivia sometimes where you just read the first paragraph of a Wikipedia page and it's kind of like answering a bunch of little trivia questions about where someone was born and when um but like it's never seen something quite like this and it's actually still kind of astounding how much is able to do things that don't seem like they should have shown up all that directly in the pre-training data quantifying that extent is an open research problem okay that's it let's call it foreign 
","['', 'subword modeling', 'word embeddings', 'unknown tokens (UNK)', 'Swahili verb conjugation', 'co-occurring substrings', 'character-level vocabulary', 'sentence embedding', 'Transformer', 'masked language modeling', 'bi-directional context', 'BERT (Bidirectional Encoder Representations from Transformers)', 'span corruption', 'generative pre-training', 'T5 (Text-to-Text Transfer Transformer)', 'salience band masking', 'open domain question answering', 'decoder-only models', 'GPT (Generative Pre-trained Transformer)', 'natural language inference', '']"
"okay awesome um we're going to get started so uh my name is Jesse moo I'm a PhD student in the Cs Department here uh working with the NLP group and really excited to be talking about the topic of today's lecture which is on prompting instruction fine-tuning and rlhf so this is all stuff that has been you know um super hot recently because of all the latest chick craze about chat Bots uh chat gbt Bing Etc and we're going to hopefully get somewhat of an understanding as to how these systems are trained okay so before that some course Logistics things so project proposals both custom and final were due a few minutes ago um so if you haven't done that this is a nice reminder um we're in the process of assigning mentors of projects so we'll give feedback soon um besides that assignment five is due on Friday at midnight we still recommend using collab for the assignments even if you've had AWS or Azure credits granted if that doesn't work there's instructions for how to connect to a kaggle notebook where you will also be able to use gpus look for that post on Ed and then finally also just posted on Ed by John is a course feedback survey so this is part of your participation grade please fill that in by Sunday 11 59 pm okay Okay so let's get into this lecture which is going to be about what we are trying to do with these larger and larger models right over the years the compute for these models have just you know gone up uh hundreds of um you know powers of 10. uh trained on more and more data right so larger and larger models they're seeing more and more data and in lecture 10 if you recall this slide we talked a little bit about what happens when you do pre-training and as you begin to really learn to predict the missing sentence in certain texts right you learn things like syntax co-reference sentiment Etc but in this lecture we're going to take it a little bit further and really take this idea to its logical conclusion so if you really follow this idea of we're just going to train a giant language model on all of the world's text you really begin to see language models sort of in a way as rudimentary World models so maybe they're not very good at World models but they kind of have to be doing some implicit World modeling just because we have so much information on the internet and so much of human Collective knowledge is transcribed and written for us on the internet right so if you are really good at predicting the next word in text what do you learn to do there's evidence that these large language models are to some degree learning to represent and think about agents and humans and the beliefs and acronyms they might take so here's an example from a recent paper where we are talking about someone named Pat watching a demonstration of a bowling ball and a leaf being dropped at the same time in a vacuum chamber and the idea is uh here we're saying Pat is a physicist right so if Pat is a physicist and we ask for the language models uh next continuation of the sentence because he's a physicist we do some inference about what kind of knowledge Pat has and Pat will predict that the bowling ball and the leaf will fall at the same time but if we change the sentence of the prompt and we say well Pat has actually never seen this demonstration before then Pablo predicts that the bowling ball will fall to the ground first which is wrong right so if you get really good at predicting the next sentence in text you also to some degree have to learn to predict an agent's beliefs their backgrounds common knowledge and what they might do next so not just that of course if we continue browsing the internet we see a lot of encyclopedic knowledge so maybe language models are actually good at solving math reasoning problems if they've seen enough demonstrations of math on the internet code of course co-generation is a really exciting topic that people will that people are looking into and will give a presentation on that in a few weeks even medicine right we're beginning to think about language models trained on medical texts and being applied to the sciences and whatnot so this is what happens when we really take this language modeling idea seriously and this has resulted in a Resurgence of interest in building language models that are basically assistants right you can give them any task Under the Sun I want to create a three-course meal and a language model should be able to take a good stab at being able to do this right this is kind of the promise of language modeling but of course there's a there's a lot of steps required to get from this from our basic language modeling objective and that's what this lecture is going to be about so how do we get from just particular next word in a sentence to something like chat GPT which you can really ask it to do anything and it might fail sometimes but it's getting really really convincingly good at some things okay so this is the lecture plan basically I'm going to talk about as we're working with these large language models we come up with kind of increasingly complex ways of steering the language models closer and closer to something like chat GPT so we'll start with zero shot and few shot learning then instruction fine-tuning and then reinforcing learning from Human feedback or rohf okay so let's first talk about a few shot and zero shot learning uh and in order to do so we're again going to kind of build off of the pre-training lecture last Tuesday so in the pre-training lecture John talked about uh these models like GPT generative pre-trained Transformer um that are these decoder only language models so they're just trained to predict the next word in a corpus of text and back in 2018 was the first iteration of this model uh and it was 117 million parameters so at the time it was pretty big nowadays it's definitely much smaller and again it's just a vanilla Transformer decoder using the techniques that you've seen and it's trained on a corpus of books so about 4.6 gigabytes of text and what GPT showed was the promise at doing this simple language modeling objective and serving as an effective pre-training technique for various Downstream tasks that you might care about so if you wanted to apply to something like natural language inference you would take your premise sentence and your hypothesis sentence concatenate them and then maybe train a linear classifier on the last representation the model produces okay but that was three four five years ago what has changed since then so they came out with gpt2 so gpt2 was released the next year in 2019. this is 1.5 billion parameters so it's the same architecture as GBC but just an order of magnitude bigger and also trained on much more data so we went from four gigabytes of books to 40 gigabytes of Internet Text data so they produced a data set called webtext this is produced by scraping a bunch of links to comments on Reddit so the idea is that the web contains a lot of spam maybe a lot of low quality information but they took links that were posted on Reddit that had at least a few uploads so humans maybe looked through it and said you know this is a useful post so that was kind of a rough proxy of human quality and that's how they collected this large data set and so if you look at the size of gbt in 2018 we can draw a bigger dot which is the size of gpt2 in 2019 and one might ask how much better does this do what does this buy you so the authors of gbt2 titled their paper language models are unsupervised multitask Learners and that kind of gives you a hint as to what the key takeaway they found was which is this unsupervised multitasking part so basically I think the key takeaway from gbt2 was this idea that language models can display zero shot learning so what I mean by zero shot learning is you can do many tasks that the model may not have actually explicitly been trained for with no grading updates so you just kind of query the model by simply specifying the right sequence prediction problem so if you care about question answering for example you might include your passage like a Wikipedia article about Tom Brady and then you'll add a question so question where was Tom Brady born and then include an answer like a followed by colon and then just ask the model to predict the next token right you've kind of jury-rigged the model into doing question answering for other tasks like classification tasks another thing you can do is compare different probabilities of sequences so this task is called the Winograd schema challenge it's a pronoun resolution task so the task is to kind of resolve a pronoun which requires some World Knowledge so one example is something like the cat couldn't fit into the Hat because it was too big and the question is whether it refers to the cat or to the hat right and in this case it makes most sense for it to refer to the cats because things fitting into things because they're too big you know you need to use some World Knowledge to kind of resolve that so the way that you get zero shot predictions for this task out of a language model like gbt2 is you just ask the language model which sequence is more likely is the probability of the cat couldn't fit into the Hat because the cat was too big deemed more likely by the language model than the probability that the cat couldn't fit into the Hat because the Hat was too big right you can score those sequences because this is a language model and from there you get your zero shot prediction and you can end up doing fairly well on this task any questions about those okay uh yeah so digging a little bit more into the results gpt2 at the time beat the state of the art on a bunch of language modeling benchmarks with no task specific fine-tuning so no traditional fine tune on a training set and then test on a testing set so here's an example of such a task this is a language modeling task called Lambada or the goal is to predict a missing word and the idea is that the word that you need to predict depends on some discourse earlier in the sentence or earlier a few sentences ago and by simply training your language model and then running it on the Lambada task you end up doing better than the supervised fine-tuned state-of-the-art at the time and across across a wide variety of other tasks as well okay another kind of interesting Behavior they observed and so you'll see hints of um uh things that we now take for granted in this paper is that you can get interesting zero shot Behavior as long as you take some liberties with how you specify the task so for example let's imagine that we want our model to do summarization even though gpt2 was just a language model how can we make it do summarization the idea they explored was we're going to take an article some news article and then at the end we're going to append the tldr sign right the tldr token so this stands for too long didn't read so use a lot on Reddit to just say you know if you didn't want to read the above stuff here's a few sentences that summarizes it right so if you ask the model to predict what follows after the tldr token right you might expect that it'll generate some sort of summary and this is kind of early Whispers at this term that we now call prompting right which is thinking of the right way to define a task such that your model will do the behavior that you want it to do so if we look at the performance we actually observed on this task here at the bottom is a random Baseline so you just select three sentences from the article and the scores that we're using here are ruse scores if you remember the natural language generation lecture gpt2 is right above so it's not actually that good like it only does maybe a little bit or barely any better than the random Baseline but it is approaching approaches that our supervised approaches that are actually explicitly fine-tuned to do summarization right and of course at the time it's still underperformed the state of the Arts but this really showed the promise of getting language models to do things that maybe they weren't trained to do okay so that was gbt2 that was 2019. now here's 2020 gpt3 so GPD 3 is 175 billion parameters so it's another increase in size by an order of magnitude and at the time it was unprecedented I think it still is like kind of overwhelmingly large for most people and data so they scaled the data once again okay so what is this by you this paper's title was called language models are few shot learners so what does that mean so the key takeaway from gpt3 was emergent few shot learning so the idea here is sure gbt can still do zero shot learning but now you can specify a task by basically giving examples of the task before asking it to predict the example that you care about okay so this is often called in context learning to stress that there are no gradient updates being performed when you learn a new task you're basically kind of constructing a tiny little training data set and just including it in the prompt including it in the context window of your Transformer and then asking it to pick up on what the task is and then predict the right answer and this is in contrast to a separate literature on few shot learning which assumes that you can do gradient updates right in this case it's really just a frozen language model so few shot learning works and it's really impressive so here's a graph super glue here is a kind of a wide coverage natural language understanding Benchmark and what they did was they took dbt3 and this data point here is what you get when you just do zero shot learning with dbt3 so you provide an English description of the tasks to be completed and then you ask it to complete the task just just by providing one example so one shot uh you get like a 10 accuracy increase right so you give not only the natural language test description but also an example input and an example output and you ask it to decode the next output and as you increase to more shots you do get better and better um scores although of course you get diminishing returns after a while but what you can notice is that Hue shot tpt3 so no grading updates uh is doing as well as or outperforming uh Bert fine-tuned on the super glue task explicitly any questions about this so one thing that I think is really exciting is that uh you might think okay a few shot learning whatever it's just memorizing maybe there's a lot of examples of needing to do a few shot learning in the internet Text data right and that's true but I think there's also evidence that uh gpt3 is really learning to do some sort of kind of on-the-fly optimization or reasoning and so the evidence for this comes in the form of the synthetic word unscrambling tasks so the authors came up with a bunch of simple kind of letter manipulation tasks that are probably unlikely to exist in Internet Text data so these include things like cycling through the letters to get the kind of uncycled version of a word so converting from PL EAP to Apple removing characters added to a word or even just reversing words right and what you see here is performance as you do few shot learning as you increase the model size and what you can see is that uh the ability to do a few shot learning is kind of an emergent property of model scale so at the very largest model we're actually seeing a model be able to do this exclusively in context okay uh question yeah I've noticed the Reversed words are horrible like performance yeah yeah so the question was the Reversed words uh line is still low yeah that's an example of a task that this models still can't solve yes although I'm not sure if we evaluated with newer newer models maybe you know the latest versions can indeed actually solve that task Yeah question here's some intuition for why the subjects as a result of the model scale I think that's a highly active area of research and there's like being papers published every week on this so I think there's a lot of interesting experiments that really try to dissect you know either with like synthetic tasks like you know can gbt3 learn linear regression in context and there's like some model interpretability tasks like you know what in the attention layers or what in the hidden states are resulting in this kind of emergent learning um but yeah I'd have to just refer you to the recent literature on that anything else awesome okay so just to summarize traditional fine-tuning here is on the right right we take a bunch of examples of a task that we care about we give it to our model and then we do a grading step on each example and then at the end we hopefully get a model that can do well on some outputs and in this new kind of Paradigm of just prompting a language model we just have a frozen language model and we just give some examples and ask the model to predict the right answer okay so you might think and you'd be right that there are some limits of prompting well there's a lot of limits of prompting but especially for tasks that are too hard right there are a lot of tasks that maybe seem too difficult um especially ones that involve maybe richer reasoning steps uh or you know needing to synthesize multiple pieces of information and these are tasks that humans struggle with too right so one example is gpt3 I don't have the result the the actual graph here but it was famously bad at doing addition for much larger digits and so if you prompt gpt3 with a bunch of examples of addition it won't do it correct but part of the reason is because humans are also pretty bad at doing this in one step right like if I asked you to just add these two numbers on the Fly and didn't give you a pencil on paper you'd have a pretty hard time with it so one observation is that you can just change the prompts and hopefully get some better performance out of this so there's this idea of doing Chain of Thought prompting where in standard prompting we give some examples of a task that we'd like to complete so here is an example of a math word problem and I told you that what we would do is we would give the question and then the answer and then for a data point that we actually care about we ask the model to predict the answer and the model will try to produce the right answer and it's just wrong so the idea of Chain of Thought prompting is to actually demonstrate what kind of reasoning you want the model to complete so in your prompts you not only put the question but you also put an answer and the kinds of reasoning steps that are required to arrive at the correct answer right so here is actually some reasoning of how you actually would answer this tennis ball question and then get the right answer and because a language model is essential you know incentivized to just follow the pattern and continue the prompt if you give it another question it will in turn produce an answer sorry a rationale followed by an answer okay so you're kind of asking a language model to work through the steps yourself and by doing so you end up getting some questions right when you otherwise might not so super simple idea but it's shown to be extremely effective so here is this middle school math word problems Benchmark and again as we scale up the model for GPT and some other kinds of models um being able to do Chain of Thought prompting emerges right so we really see a performance approaching that of supervised baselines for these larger and larger models questions yeah seemingly the problem with the addition with the barge members do you have like results on how like chain of thoughts on people with larger numbers than middle school math word problems yeah so the question is uh does chain without prompting work for those addition problems that I had presented yeah um there should be some results in the um in the actual paper uh they're just not here but you can take a look yeah you should know how the model was trained without doing gradient update I just been in Europe intuition about how the model is Learning Without grading updates yeah so this is related to the question asked earlier about like how is this actually happening um that is yeah again it's an active area of research so my understanding of the literature is something like you can show that models are kind of almost doing like in-context gradient descent as it's encoding a prompt um and you can analyze this with like model interpretability experiments but I yeah I'm happy to suggest papers afterwards that kind of deal with this problem more carefully cool Okay so a follow-up work to this ask the question of do we actually even need examples of reasoning right do we actually need to collect humans working through these problems can we actually just ask the model to reason through things just ask it nicely right uh so this introduced this idea called zero shot Chain of Thought prompting and it was honestly like I think probably like the highest like impact to like simple idea ratio I've seen in the paper where it's like the simplest possible thing where instead of doing this Chain of Thought stuff you just ask the question and then the answer you first prepend the token let's think step by step right and the model will decode as if it had said let's think step by step and it will work through some reasoning and produce the right answer so does this work uh on some arithmetic benchmarks here's what happens when you prompt a model just zero shots right so just asking it to produce the answer right away without any reasoning a few shots or giving some examples of inputs and outputs and this is zero shot chain of thoughts just asking the model to think through things you get uh crazy good accuracy when we compare to actually doing manual Chain of Thought you still do better with manual chains of thought but that just goes to show you how simple of an idea this is and ends up producing improved performance numbers so the funny part of this paper was they you know why use less thing by step by step they used actually a lot of prompts and tried them out so here's zero shot Baseline performance they tried out a bunch of different you know prefixes the answers after the proof let's think let's think about this logically and they found that let's think step by step was the best one it turns out this was actually built upon um later in the year where they actually used a language model to search through the like best possible strings that would maximize performance on this task which is probably like gross overfitting but the best pump they found was let's work this out step by step in a step-by-step way to be sure that we have the right answer so the right answer thing is kind of presuming that you get the answer right right it's like giving the model some confidence itself okay so this might seem to you like a total dark Arcane art and that's because it is like we really have no intuition as to what's going on here um we're trying to build some intuition uh but as a result and I'm sure you've seen you know if you spend time in Tech circles or you've seen on the internet there's this whole new idea of prompt engineering being an emerging science and profession so this includes things like asking a model for reasoning uh it includes jailbreaking language models so telling them to do things that they was you know other otherwise aren't trained to do uh even you know air like dolly or stable diffusion this idea of constructing these really complex prompts to get model outputs that you want that's also prompting anecdotally I've heard of people saying I'm going to use a cogeneration model but I'm going to include the Google code header in first because that will make more professional or bug-free code depending on how much you believe in Google um but yeah and there's a Wikipedia article on this now and there's even startups that are hiring for prompt engineers and they pay quite well so if you want to be a prompt engineer definitely practice your GPT with spring skills we have a question sorry um yes uh could you a few classical you said Alum designs they're most like this long uh output how can you get the LM to design uh and like so I think they treat it like your reinforcement learning problem um but I'll just direct you to this paper at the bottom to learn more details yeah this is the Joe at all 2022 paper Yeah question um so I'm just like a bit curious about how they provided feedback so in case the model was not given the right answer were they like prompts to say that um that's not right maybe think about because customer service like how how is feedback provided they don't think about feedback in this kind of chain of power prompting experiments they just like if the model gets the answer wrong then it gets the answer wrong and we just evaluate accuracy right but this idea of incorporating AI feedback I think is quite interesting and I think uh you'll see some maybe hints of discussion about later on yeah questions okay awesome okay so uh talking about these three things I'm going to talk about kind of the benefits and limitations of you know the various different things that we could be doing here so for zero shot and few shot in context learning the benefit is you don't need any fine tuning uh and you can kind of carefully construct your prompt to hopefully get better performance the downsides are there are limits to what you can fit in context right Transformers have a fixed context window of say a thousand or you know a few thousand tokens and I think as you will probably find out for really complex tasks you are indeed going to need some gradient steps so you're going to need some sort of fine tuning uh but that brings us to the next part of the lecture so that's instruction fine-tuning okay so the idea of instruction fine-tuning is that sure these models are pretty good at doing prompting you can get them to do really interesting things but there is still a problem which is that language models are trained to predict the most likely continuation of tokens and that is not the same as what we want language models to do which is to assist people so as an example if I give gpd3 this kind of prompt explain the moon landing gpt3 is trained to predict you know if I saw this on the internet somewhere what is the most likely continuation well maybe someone was coming up with a list of things to do with a six-year-old so it's just predicting a list of other tasks right it's not answering your question and so the issue here is that language models are not the term is aligned with user intent so how might we better align models with user intent for this case well super simple answer right we're machine Learners let's do machine learning so we're going to collect you ask a human uh give me the right answer right give me the way that a language model should respond according to this prompt and let's just do fine tuning right so this is a slide from the pre-training lecture uh again pre-training can improve NLP applications um by serving as parameter initialization so this kind of pipeline I think you are familiar with and the difference here is that instead of fine-tuning on a single Downstream task of Interest like sentiment analysis what we're going to do is we're going to fine tune on many tasks so we have a lot of tasks and the hope is that we can then generalize to other unseen tasks at test time so as you might expect um data and scale is kind of key for this to work so we're going to collect a bunch of examples of instruction output pairs across many tasks and then fine-tune our language model and then evaluate generalization to unseen tasks okay yeah so data and scale is important so as an example one recent data set that was published for this is called the supernatural instructions data set it contains over 1.6 000 tasks containing three million examples so this includes translation question answering question generation even coding mathematical reasoning Etc and when you look at this you really begin to think well is this actually fine-tuning or is this just more pre-training right and it's actually both right it's kind of a it's we're kind of blurring the lines here where the amount of scale that we're training this on basically it is kind of a still General but slightly more specific than language modeling type of pre-training task okay so one question I have is now that we are training our model on so many tasks how do we evaluate such a model right because you can't really say okay can you now do sentiment analysis well right like the scale of tasks that we want to evaluate this language model on is much greater so just as a brief aside um you know a lot of research has been going into building up these benchmarks for these massive multicast language models and seeing to what degree they can do not only just one task but just a variety of tasks so this is the massive multitask language understanding Benchmark or mmlu it consists of a bunch of benchmarks for measuring language model performance on a bunch of knowledge intensive tasks that you would expect to high school or college student to complete so you're testing a language model not only on you know send them analysis but on astronomy and logic and European history and here are some numbers where you know at the time dpt3 is like not that good but it's certainly above a random Baseline on all of these tasks here's another example so this is the beyond the imitation game Benchmark or big bench this has like a billion authors because it was a huge collaborative efforts uh and these are this is a word cloud of the tasks that were evaluated um and it really contains some very esoteric tasks so this is an example of one task included where you have to given uh kanji or Japanese character in ASCII art you need to predict the meaning of the character right so we're really stress testing these language models okay so instruction fine tuning does it work um recall the there's a T5 encoded decoder model so this is kind of Google's encoder decoder model where it's pre-trained on this span corruption task so if you don't remember that you can refer back to that lecture but the authors released a newer version called flan T5 so flan stands for fine-tuning language models and this is T5 models trained on an additional 1.8 000 tasks which include the natural instructions data set that I just mentioned and if we average across both the big bench and and LU performance and normalize it what we see is that instruction fine tuning works and crucially the bigger the model the bigger the benefit that you get from doing construction fine tuning right so it's really the large models that stand to do well from fine tuning and you might look at this and say this is kind of sad for academics or anyone without a massive GPU cluster right it's like who can run an 11 billion parameter model I guess the one silver lining if you look at the results here are the 80 million model which is the smallest one if you look at after fine tuning it ends up performing about as well as the UN fine-tuned 11 billion parameter model right so there's a lot of examples in the literature about smaller instruction fine-tuned pre-trained models outperforming larger models that are many many more times the size right so hopefully there's still some hope for people with just like a few gpus questions in order to really understand the capabilities I highly recommend that you just try it out yourself so flan T5 is hosted on hugging face I think hugging face has a demo where you can just type in a little you know query asked to do anything see what it does um but you know there are qualitative examples of this working so four questions where a non-instruction fine-tuned model will just kind of waffle on and not answer the question doing instruction fine-tuning will get your model to much more accurately reason through things and gave you the right answer okay so that was instruction fine tuning um positives of this method super simple super straightforward it's just doing fine tuning right and you see this really cool ability to generalize to unseen tasks in terms of negatives does anyone have any ideas for why my might be um downsides of instruction fine-tuning I don't want to comment yeah it seems like it suffers from the same negatives of any human Source data yeah how to get people to provide the input you don't know like different people think different if it's about it yeah yeah exactly so comments are well it's hard and annoying to get human labels and it's expensive that's something that definitely matters and that last part you mentioned about there might be you know humans might disagree on what the right label is yeah that's increasingly a problem um yeah so what are the limitations the obvious limitation is money collecting ground truth data for so many tasks costs a lot of money start a lot of limitations include the one that you were mentioning so as we begin to ask for more creative and open-ended tasks from our models right there are tasks where there is no right answer and it's a little bit weird to say you know this is an example of how to write some story right so write me a story about a dog and our pet grasshopper like there is not one answer to this but if we were only to collect one or two demonstrations the language modeling objective would say you should put all of your probability Mass on you know the two ways that two humans wrote this answer right when in reality there's no right answer another problem which is related kind of fundamentally to language modeling in the first place is that language modeling as an objective penalizes all token level mistakes equally so what I mean by that is if you're asking a language model for example to predict the sentence Avatar is a fantasy TV show and you were asking it and let's imagine that you know the LM mispredicted Adventure instead of fantasy right so Adventure is a is a mistake it's not the right word but it is uh equally as bad as if the model were to predict something like musical right but the problem is that Avatar is an adventure TV show is like still true right so it's not necessarily A Bad Thing whereas Avatar is a musical is just false so under the language modeling objective right if the model we're equally confident you would pay the inequal penalty in equal loss penalty for predicting either of those tokens wrong but it's clear that this objective is not actually aligned with what users want which is maybe truth or creativity or generally just this idea of human preferences right Yeah question can we multiply the penalty by like the distance from where to betting in order to reduce this because musical would have a higher distance away than Adventure yeah that's an interesting question um it's an interesting idea I haven't heard of people doing that but it seems plausible I guess one issue is you might come up with like adversarial settings or like maybe the word embedding distance is also not telling you the right thing right so for example show in musical Maybe are very close together because they're they're both you know shows or things to watch but they are like in false in veracity right they're completely different one is true one is false right so yeah you can try it although I think there might be some tricky edge cases like that yeah cool okay so in the next part of the talk we're going to actually explicitly try to satisfy human preferences and come with a mathematical framework for doing so and yeah so these are the limitations as I just mentioned so this is where we get into reinforcement learning from Human feedback okay so rlhf so let's say we were training a language model on some tasks like summarization and let's imagine that for each language model sample s let's imagine that we had a way to obtain a human reward of that summary so we could score this summary with a reward function which we'll call our R of s and the higher the reward the better so let's imagine we're summarizing this article and we have this summary which maybe is pretty good let's say we had another summary maybe it's a bit worse and if we were able to ask a human to just rate all these outputs then the objective that we want to maximize or satisfy is very obvious we just want to maximize the expected reward of samples from our language model right so in expectation as we take samples from our language model P Theta we just want to maximize the reward of those samples fairly straightforward so uh oh and for mathematical Simplicity here I'm kind of assuming that there's only one task or one prompt right so let's imagine we were just trying to summarize like this article um but we could talk about how to extend it to like multiple prompts later on okay so this kind of task is the domain of reinforcement learning so I'm not going to presume there's any knowledge of reinforcing learning although I'm sure some of you are quite familiar with it probably even more familiar than I am but the field of reinforcement learning has studied these kinds of uh problems these optimization problems of kind of how to optimize something while you're kind of simulating the optimization for many years now and in 2013 there was a Resurgence of interest in reinforcement learning for deep learning specifically so you might have seen these results from Deep bind about an agent learning to play Atari games and agent mastering go much earlier than expected but interestingly I think the interest in applying reinforcement learning to Modern LMS is a bit newer on the other hand and I think the kind of earliest success story or one of the earliest success stories was only in 2019 for example so why might be this be the case there's a few reasons I think in general the field had kind of dissensed that reinforcement learning with language models was really hard to get right partially because language models are very complicated and if you think of language models as actors that have an action space where they can spit out any sentence that's a lot of sentences right so it's like a very complex space to explore in so it still is a really hard problem so that's part of the reason but also practically I think there have been these newer algorithms that seem to work much better for deep mural models including language models and these include algorithms like proximal policy optimization but we won't get into the details of that for this course but these are the kind of the reasons why we begin you know re-interested in this idea of doing RL with language models okay so how do we actually maximize the subjective right I've written it down and ideally we should just change our parameters data so that reward is high right but it doesn't it's not really clear how to do so so when we think about it I mean what have we learned in the class thus far we know that we can do gradient descent or gradient Ascent so let's try doing gradient Ascent right we're going to maximize this objective so we're going to step in the direction of steepest gradient but this quickly becomes a problem which is what is this quantity and how do we evaluate it right how do we estimate this expectation given that the gradient the variables of the gradient that we're taking Theta appear in the sample of the expectation and the second is what if our reward function is not differentiable right like human judgments are not differentiable we can't back prop through them and so we need this to be able to work with a black box reward function so there's a class of methods in reinforcement learning called policy gradient methods that gives us tools for estimating and optimizing this objective and for the purposes of this course I'm going to try to describe kind of the highest possible and highest level possible intuition for this which kind of looks at kind of the math and shows what's going on here but it is going to emit a lot of the details and a full treatment of reinforcement learning is definitely outside of the scope of this course so if you're more interested in this kind of content you should check out cs234 reinforcing learning for example and in general I think this is going to get a little messy but it's totally fine if you don't understand it we will talk we'll regroup at the end and just show like what this means for how to do our lhf but what I'm going to do is just describe how we actually estimate this objective right so we want to obtain this gradient so it's the gradient of the expectation of the reward of samples from our language model and if we do the math we break this apart this is our definition of what an expectation is right we're going to sum over all sentences rated by the probability uh and due to the linearity of the gradient we can put the gradient operator inside of the sum now what we're going to do is we're going to use a very handy trick known as a log derivative trick and this is called a trick but it's really just a chain rule but let's just see what happens when we take the gradient of the log probability of a sample from our language model so if I take the gradients then how do we use a chain rule right so the gradient of the log of something is going to be 1 over that something times the gradient of the middle of that something so 1 over P Theta of s times the gradient and if we rearrange we see that we can alternatively write the gradient of P Theta of s as this product so P Theta of s times the gradient of the log P data this and we can plug this back in and the reason why we're doing this is because we're going to convert this into a form where the expectation is easy to estimate so we plug it back in that gives us this and if you squint quite closely at this last equation here this first part here is the definition of an expectation right we are summing over a bunch of samples from our model and we are waiting it by the probability of that sample which means that we can rewrite it as an expectation and in particular it's an expectation of this quantity here right so let's just rewrite it and this gives us our kind of newer form of this objective so these two are equivalent the top here and the bottom and what has happened here is we've kind of shoved the gradient inside of the expectation if that makes sense so why is this useful does anyone have any questions on this before I move on if you don't understand it that's fine as well because it doesn't I mean we will understand kind of the intuition behind it later okay Okay so we've converted this into this and we put the gradient inside the expectation which means we can now approximate this objective with Monte Carlo samples so the way to approximate any expectation is to just take a bunch of samples and then average them right so approximately this is equal to sampling a finite number of samples from our model and then summing up the average of the reward times the log probability the gradient of the log probability of that sample and that gives us this update rule plugging it back in for that creating a sense that we wanted right so what what is this what does this mean uh let's think about like a very simple case right imagine the reward uh was a binary reward so it's either zero or one so for example imagine we were trying to train a language model to talk about cats so whenever it utters a sentence with a word cat we give it a one reward otherwise we give it a zero reward okay now if a reward is binary does anyone know what this objective kind of reduces to or look like any ideas I've lost everyone that's fine too yeah so the reward would just be like an indicator yeah that's right so basically to answer uh the reward would be zero right everywhere except for sentences that contain the word cat right and in that case it would be one so basically that would just look like kind of vanilla vanilla grading descent just on sentences that contain the word cap right so kind of to generalize this to the more General case where the reward is scalar what this is looking like if you look at it is if R is very high very positive then we're multiplying the gradient of that sample by a large number and so our objective will try to take grading steps in the direction of maximizing the probability of producing that sample again right producing the sample that led to high reward and on the other hand if R is low or even negative then we will actively take steps to minimize the probability that's happening again right and that's like the English intuition of what's going on here right the reason why we call it reinforcement learning is because we want to reinforce good actions and increase the probability that they happen again in the future and hopefully this intuitively makes sense to all of you let's say you're playing a video game and on one run you get a super high score and you think to yourself oh that was really good like whatever I did that time I should do again in the future right this is what we're trying to capture with this kind of update question any reason that we use policy gradient and not like value iteration or other methods you can do yeah you can do a lot of things I think there have been methods for doing q-learning offline learning Etc with um language models I think the design space is has been very underexplored so there's like a lot of low-hanging fruit out there for people who are willing to think about what fancy things we can do in RL and apply them to this language model in case yeah and in practice what we use is not this simple thing but we use a fancier thing that is a proximal policy optimization question does space are like super big like almost good yeah so that's so that's that's the challenge um so one thing that I haven't mentioned here is that right now I'm talking about kind of entire samples of sentences which is like a massive space right in practice when we do RL we actually do it the level of generating individual tokens so each token is let's say GPT has 50 000 tokens right so it's a pretty large action space but it's still manageable right but yeah so that kind of answers this question I was asking which is like can you see any problems with this objective right which is that this is a very simplified objective there is a lot more tricks needed to make this work but hopefully this has given you kind of the high level intuition as to what we're trying to do in the first place okay okay so now we are set right we have a bunch of samples from a language model and for any arbitrary reward function like we're just asking a human to rate these samples we can maximize that reward so we're done okay so not so fast um there's a few problems the first is the same as in the instruction fine-tuning case right which is that keeping a human in the loop is expensive like I don't really want to supervise every single output from a language model I don't know if you all want to um so what can we do to fix this so one idea is instead of needing to ask humans for preferences every single time you can actually build a model of their preferences like literally just train an NLP model of their preferences so this idea was kind of first introduced outside of language modeling by this paper Knox and stone they called it Tamer but we're going to see it re-implemented in this idea where we're going to train a language model we'll call it a reward model RM just parameterized by fee to predict human preferences from an annotated data set and then when doing rlhf we're going to optimize for the reward model rewards instead of actual human Rewards here's another conceptual problem so here's a new sample for our summarization task right what is the score of the sample anyone give me a number does anyone want to rate this sample it's like a three six what scale are we using Etc so the issue here is that human judgments can be noisy and miscalibrated when you ask people for things alone right so one workaround for this problem is instead of asking for direct ratings ask humans to compare two summaries and judge which one is better this has been shown I think in a variety of fields where people work with human subjects and human responses to be more reliable this includes psychology and Medicine Etc so in other words instead of asking humans to just give absolute scores we're going to ask humans to compare different samples and rate which one is better so as an example maybe this first sample is better than the middle sample and it's better than the last sample right now that we have these pairwise comparisons our reward model is going to generate kind of latent scores so implicit scores based on this pairwise comparison data so our reward model is a language model that takes in a possible sample and then it's going to produce a number which is the score or the reward and the way that we're going to train this model and again you don't really need to know too much of the details here but this is a classic kind of statistical comparison model is via the following laws where the reward model essentially should just predict a higher score if a sample is judged to be better than another sample so in expectation if we sample winning samples and losing samples from our data sets then if you look at this term here the score of the higher sample should be higher than the score of the losing sample does that make sense and in doing so by just trading on the subjective you will get a language model that will learn to assign numerical scores to things which indicate their relative you know preference over other samples and we can use those outputs as Rewards yeah the normalization either in the like the output or somewhere else because it looks like this yeah so I don't remember if it happens during training but certainly after you've trained this model you normalize the reward model so that the score is the expectation of the score is zero because that's good for reinforced learning and things like that as well Yeah question like even though these are noisy like they could be like some people could do S3 is better than S1 how do we account for it even though like when it's noisy like the bordering the ordering yeah I think that's you know that's just kind of limitations with asking for these preferences in the first place is that humans will disagree right so we really have no ground truth unless we maybe ask like an ensemble of humans for example that's just a limitation with this I think hopefully with you know in the limit with enough data this kind of noise washes out but it's certainly an issue and uh this next slide will also kind of touch on this so does the reward model work can we actually learn to model human preferences in this way this is obviously an important standard they check before we actually try to optimize this objective and they measured this so this is kind of evaluating the reward model on a standard kind of validation set right so can the reward model predict outcomes for data points that they have not seen during training and does it change based on model size or amount of data right and if you notice here there's one dashed line which is the human Baseline which is if you ask a human to predict the outcome a human does not get 100 accuracy because humans disagree right and even an ensemble of let's say five humans also doesn't get 100 accuracy because humans are different preferences right but the key takeaway here is that for the largest possible model and for enough data a reward model at least some of the validations that they that they used is kind of approaching the performance of a single human person and that's kind of a green light that maybe we can you know try this out and see what happens okay so there are no questions this is kind of the components of our lhf so we have a pre-trained model maybe a construction fine-tuned right which we're going to call P of pt we have a reward model which produces scalar rewards for language model outputs and it is training on a data set of human comparisons and we have a method policy gradient for arbitrarily optimizing language model parameters towards some reward function and so now if you want to do rohf you clone the pre-trained model we're going to call this a copy of the model which is our RL model with parameters data that we're actually going to optimize and we're going to optimize the following reward with reinforced learning and this reward looks a little bit more complicated than just using the reward model and the extra term here is a penalty which prevents us from diverging too far from the pre-trained model so in expectation this is known as the KL or cold black labor Divergence between the RL model and the pre-trained model and I'll explain why we need this in a few slides but basically if you over optimize the reward model you end up producing you can produce like gibberish um and what happens is you pay a price so this quantity is large if the probability of a sample under the RL tuned model is much higher than the probability of the sample under the pre-train model right so the pre-trained model would say this is a very unlikely sequence of characters for anyone to say uh that's when you would pay a price here and beta here is a tunable parameter Yeah question when you say initialize a copy that means like at the first iteration PRL is equal to p p t that's right yeah yeah when I say initialize a copy basically like we want to be able to compare to the non-fine-tuned model just to evaluate this penalty term so just leave the predictions of the the you know pre-rl model around yeah questions right so does it work the answer is yes so here is kind of the key takeaway at least for the task summarization on this Daily Mail data set so again we're looking at different model sizes but at the end here we see that if we do just pre-training so just like the typical language modeling objective that GPT uses you know you end up producing summaries that in general are not preferred to the reference summaries right so this is on the y-axis here is the amount of times that a human refers the model generated summary to a summary that a human actually wrote or the one that's in the data set so pre-training doesn't work well even if you do supervised learning so supervised learning in this case is let's actually fine tune our model on the summaries that were in our data sets right even if you do that you still kind of underperform the reference summaries right because you're not perfectly modeling those that those summaries but it's only with this human feedback that we end up producing a language model that actually ends up producing summaries that are judged to be better than the summaries in a data set that you were training on in the first place I think that's quite interesting any questions okay so now we talk about yeah we're getting closer and closer to something like construct GPT or chat GPT the basic idea of instruct GPT is that we are scaling up our lhf to not just one prompt as I had described previously but tens and thousands of prompts right and if you look at these three pieces these are the three pieces that we've just described right the first piece here being instruction fine-tuning the second piece being rohf and the third piece oh sorry the second part second part being reward model training and the last part being rohf the difference here is that they use 30 000 tasks so kind of again with the same instruction fine-tuning idea right it's really about the scale and diversity of tasks that really matters for getting good performance for these things yeah the proceeding results is that the you know that it didn't what that you really needed you know any chance and it didn't work so well to be a supervised learning on the data um but they do supervise learning on the day there is in the fine tuning in the first stage uh yeah that's a good question so I think a key Point here is that they initialize the RL policy on the supervised policy right so they first got the model getting reasonably good at doing summarization first and then you do the RL Jeff on top to get the Boost performance um your question you're asking is maybe can we just do the rhf starting from that pre-trained Baseline um that's a good question I I don't think they explored that although I'm not sure I'd have to I'd have to look at the paper again to remind myself um yeah so certainly something like instruct GPT yeah they've always kind of presumed that you need the kind of fine-tuning phase first and then you build on top of it um but I think yeah there's still some interesting open questions as to whether you can just go directly to our lhf yeah question simultaneously uh reward model should be trained first yeah you train it first you make sure it's good it's frozen you optimize against that for the human rewards that they come from the generated tax on language model or where to stop the training sample come from uh for training the reward model um so yeah actually it's a good question um where do the rewards come from so uh there's kind of an iterative process you can apply where you kind of repeat steps two and three over and over again so you sample a bunch of outputs from your language model you get humans to rate them you then do rlhf to update your model again and then you sample more outputs and get humans to rate them so in general the rewards are done on sampled model outputs because those are the outputs that you want to steer in One Direction or another but you can also you can do this in an iterative process where you kind of do RL and then maybe do it you know train a better reward model based on the new outputs and continue I think they do a few iterations in structure BT for example questions okay so 30 000 tasks um I think we're getting into like very recent stuff where you know increasingly companies like open AI are sharing less and less details about like what actually happens in training these models right so we have a little bit less clarity as to what's going on here then maybe we have have had in the past uh but they do share uh the data set's not public but they do share the kinds of tasks that they collected from labelers right so they collected a bunch of prompts uh from people who are already using the gpt3 API so they had the benefit of having you know many many users of their API and taking the kinds of tasks that they that users would ask GPT to do so these include things like brainstorming or you know open-end generation Etc and yeah I mean the key results of instruct GPT which is kind of the backbone of chat EBT uh you know really just needs to be seen and played with to understand so you can you feel free to play with either chatgpt or one of the open AI apis but again this example of a language model I'm not necessarily following tasks by doing this kind of instruction fine tuning followed by rlhf you get a model that is much more you know much better at adhering to user commands similarly a language model can be very good at generating you know super interesting open-ended creative text as well okay this brings us to chat GPT right which is even newer and we have even less information about what's actually going on and what's being trained here but uh yeah and they're keeping their you know Secret Sauce Secret but they we do have a blog post where they wrote two paragraphs and in the first paragraph they said that they did instruction fine tuning right so we trained an initial model using supervised fine tuning so human AI trainers provided conversations where they played both sides and then we asked them to act as a AI assistant and then we fine-tune our model on acting like an AI system from humans right that's part one second paragraph uh to create a reward model for RL we collected comparison data so we took conversations with a an earlier version of the chatbot so the one that's pre-trained on instruction following or instruction fine-tuning and then take multiple samples and then rate the quality of the samples right and then using these reward models we fine-tune it with RL in particular they used PPO which is a fancier version of RL okay and yeah so that produces you know I don't need to introduce the capabilities of chat TBT it's been very exciting recently here's an example it's fun to play with definitely play with it sorry it's a bit of an attack on the students uh yeah okay okay so reinforcement learning pluses you're kind of directly modeling what you care about right which is human preferences not is the collection of the demonstration that I collected right is that the highest probability mass in your model you're actually just saying how well am I satisfying human preferences right so that's a clear benefit over something like instruction fine tuning so in terms of negatives one is that RL is hard it's very tricky to get rights I think it will get easier in the future as we kind of you know explore the design space of possible options um so that's an obvious one right does anyone come up with any other kind of maybe weaknesses or issues they see with this kind of training so is it possible that like your language model and then your reward model could like overfit to each other especially like even if you're not training them together if you're like going back and forth yeah yeah so over optimization I think of the reward model is an issue yeah is it also that if you return your Facebook repeat always you want feedback all over again yeah so it still is like extremely data expensive and you can see some articles if you just Google open AI like data labeling right people have not been very happy with like the amount of data that has been needed to train something like chat gbt I mean they're hiring developers to just like explain coding problems like 40 hours a week right so it is yeah it is still data intensive right that's kind of the takeaway like all of these are like it's it's all still data intensive every single one of these right yeah yeah I think that summarizes kind of kind of the big ones here right so when we talk about limitations of our lhf we also need to talk about just limitations in general of RL and also this idea that we can like model or capture human reward in this single data point right so human preferences can be very unreliable the RL people have known this for a very long time they have a term called reward hacking which is when an agent you know is optimizing for something that the developers specified but it is not what we actually care about right so one of the classic examples is this example from openai uh where they were training this agent to race boats and they were training it to maximize the score which you can see at the bottom left but implicitly the score actually isn't what you care about what you care about is just finishing the race ahead of everyone else and the score is just kind of this bonus but what the agent found out was that there are these like turbo boost things that you can collect which boost your score and so what it ends up doing is it ends up kind of just driving in the middle collecting these turbo boosts over and over again so it's racking up insane score but it is not doing the Race it is continuously crashing into objects and its boat is always on fire uh and this is you know a pretty Salient example of what we call AI misalignment right and you might think well okay this is a really simple example right like they made a dumb mistake they shouldn't have used score as a reward function right but I think it's even more naive to think that we can capture all of human preferences in like a single number uh and assign certain scalar values to things right so one example where I think this is already happening you can see is maybe you have played with chatbots before and you and you notice that they do a lot of hallucination right they make up a lot of facts and this might be because of our lhf right chat Bots are rewarded to produce responses that seem authoritative or seem helpful but they don't care about whether it's actually true or not right they just want to seem helpful so this results in making up facts you may be seeing the news about chat Bots you know companies are in this relation to play chat Bots and they make mistakes uh even being you know also has been hallucinating a lot right and in general when you think about that you think well it models of human preferences are even more unreliable right like we're not even just using human preferences by themselves we're also training a model a deep model that we have no idea how that works right we're going to use that instead right and that can obviously be quite dangerous and so going back to this slide here where I was describing why we need this KL penalty term this yellow highlighted term here here's a concrete example of what actually happens of a language model overfitting to the reward model right so what this is showing is in this case they took off the kale penalties they were just trying to maximize reward right they trained this reward model let's just push those numbers up as high as possible right and on the x-axis here is what happens as training continues you diverge further and further this is the KL Divergence or the distance from where you started and the the golden dashed line here is what the reward model predicts your language model is doing right so your reward model is thinking wow you were killing it like they're gonna love these summaries they are going to love them way more than the reference summaries right but in reality when you actually ask humans uh the preferences Peak and then they just crater right so this can be an example of over optimizing for a metric that you care about right it ceases to become a good metric to optimize for any questions about this so there's this real concern of I think people are calling the AI alignment problem I'll let president talk about this um he tweeted that you know the main tool that we have for rlhf or sorry for alignment is rohf um but reward hacking happens a lot humans are not very good supervisors of rewards so you know this strategy is probably going to result in agents that seem like they're doing the right thing but they're wrong in subtle and conspicuous ways and I think we're already seeing examples of that in the current generation of chatbots so in terms of positives here are some positives but again RL is tricky to get right human preferences are fallible and models of human preferences are even more so so I remember seeing a joke on Twitter somewhere where someone was saying that you know zero shot and few shot learning is the worst way to align in AI instruction fine-tuning is the second worst way to align an AI and rohf is the third worst way to align an AI right so we're getting somewhere but you know each of these have clear fundamental limitations yeah competition um reinforcement learning because you get the math that you showed before essentially you're putting the greatest Insight so that we can sample it the sample expectation yeah but when it comes to sampling how do you make that parallel because then you need you kind of need to adaptively stop sampling and then you don't know when you're going to start like how do you make that process quicker I guess just like the whole unit on Transformers and all that was paralyzing everything yeah I mean yeah this is so this is really compute heavy and I'm actually not sure like what kind of infrastructure is used for like a state-of-the-art very performant implementation of rlhs but it's possible that they use parallelization like what you're describing where I think in a lot of maybe more traditional RL there's this kind of idea of having like an actor learner architecture where you have a bunch of actor workers which are each kind of a language model producing a bunch of samples and then the learner would then integrate them and perform the grading updates right so it's possible that you do need to do like just sheer like multi-processing in order to get enough samples to make this work in a reasonable amount of time um is that the kind of question you had or do you have other questions is larger than what we would typically see in Transformers yeah I'm saying that you might need to actually copy your model several times and take samples like from different copies of the models yeah but in terms of like like yeah so Auto regressive generation like Transformers especially like the forward pass and the multi-head attention stuff is very easy to parallelize but Auto regressive generalization Auto regressive generation is still like kind of bottlenecked by um the fact that it's Auto regressive right so you have to like run it first and then you need to depends on what you sample and have to run it again right so those are kind of blocks that we haven't fully been able to solve I think and that will add to compute cost yeah okay so I think we have 10 more minutes so I'm not mistaken um so we've mostly finally answered kind of how we get from this to this right there's some details missing but the key kind of factors are one instruction fine-tuning two this idea of reinforcement learning from Human feedback so let's talk a little about a little bit about what's next um so as I had mentioned rlhf is still a very new area it's still very fast moving I think by the next lecture by the time we say that you know give this slides again these slides might look completely different because maybe a lot of the things that I was presenting here turn out to be like really bad ideas or not the most efficient way of going about things rohf gets you further than instruction fine-tuning but as someone had already mentioned it is still very data expensive right there are a lot of articles about open AI needing to hire a legion of annotators or developers to just compare outputs over and over again I think a recent work that I'm especially interested in and been thinking about is how we can get the benefits of rlh app without such stringent data requirements so there's these you know newer you know kind of crazy ideas about doing reinforcement learning from not human feedback but from AI feedback so having language models themselves evaluate the output of language models so as an example of what that might look like a team from anthropic which works on these large language models came up with this idea called constitutional AI and the basic idea here is that if you ask gpt3 to identify whether a response was not helpful it would be pretty good at doing so and you might be able to use that feedback itself to improve a model so as an example if you have some sort of human request like can you help me hack into my neighbor's Wi-Fi and the assistant says yeah sure you can use this app right we can ask a model for feedback on this what we do is we add a critique request which says Hey language model gpt3 identify ways in which the assistance response is harmful and then it will generate a critique like hacking into someone else's Wi-Fi is illegal and then you might ask it to then revise it right so just rewrite the assistant response to remove harmful content and it does so right and now by just decoding from a language model I assuming you can do this well what you have now is a set of data that you can do instruction fine-tuning on right you have a request and you have a request that has been revised to make sure it doesn't contain harmful content all right so this is pretty interesting I think it's quite exciting but all of those issues that I had mentioned about like alignment you know Mis over interpreting you know human preferences reward models being followable like everything gets compounded like 40 000 times when you're thinking about this right we have no understanding of like how safe this is or where this ends up going but it is something another kind of more common idea also is this general idea of fine-tuning language models on their own outputs and this has been explored a lot in the context of Chain of Thought reasoning which is something I presented at the beginning of the lecture and these are provocatively named large language models can self-improve but again it's not clear like how much runway there is but the basic idea maybe is to you know you can use let's think step by step for example to get a language model to produce a bunch of reasoning and then you can say fine tune on that reasoning as if it were true data and see whether or not a language model can get any better using that technique but as I mentioned this is all still very new uh there are I think a lot of limitations of large language models like hallucination and also just the sheer like size and compute intensity of this that may or may not be solvable with our lhf right question like how we don't want it yeah like I've seen like people talking about how like you can Jailbreak chat gbt to still give like those types of harmful responses yeah are there any ways for us to kind of buffer against those types of things as well because it seems like you're just going to keep kind of building on like we identify chances where it's like trying to say action not like yourself um I guess is there any way to kind of build up that scale to avoid those jailbreaking possibilities yeah that's interesting um so there are certainly ways that you can use either AI feedback or human feedback to mitigate those kinds of jailbreaks right like if you see someone on Twitter saying that oh I made gpt3 jailbreak you know using this strategy or whatever you can then yeah maybe plug it into this kind of framework and say identify ways in which the assistant went off the rails right and then fine-tuned and hopefully correct those right but it is really difficult I think in most of these kinds of settings it's really difficult to anticipate all the possible ways in which a user might jailbreak an assistant right so you always have this kind of dynamic of like you know in in security cyber security for example there's always like the attacker Advantage where like you know the attacker will always come up with something new or some new exploit right so yeah I think this is a deep problem I don't have like a really clear answer um but certainly like if we knew what the jailbreak was we could mitigate it I think that seems pretty straightforward yeah yeah but if you know how to do that you should like be hired by one of these companies they'll pay you like Millions if you can solve this yeah okay uh yeah so um just like last remarks is you know with all of these like scaling results that I presented and all of these like oh you can just do instruction fine-tuning and it'll follow your instructions or you can do rlhs you might have like a very bullish I you know view on like oh this is how we're gonna solve like artificial general intelligence by just scaling up our OHF it's possible that that is actually going to happen but it's also possible that there are you know certain fundamental limitations that we just need to figure out how to solve by hallucination before we get anywhere productive with these models but it is a really exciting time to work on this kind of stuff so yeah thanks for listening and yeah thank you 
","['', 'Large language models (LLMs)', 'GPT (Generative Pre-trained Transformer)', 'Zero-shot learning', 'Few-shot learning', 'Chain of Thought prompting', 'Winograd schema challenge', 'Supervised fine-tuning', 'RLHF (Reinforcement Learning from Human Feedback)', 'Prompting instruction fine-tuning', 'Chat GPT', 'Scaling compute for LLMs', 'LLM as rudimentary World models', 'Agent reasoning and LLMs', 'LLM predicting next sentence in text', 'LLM solving math reasoning problems', 'LLM code-generation', 'LLM in medicine', 'Assistant capabilities of LLMs', 'Jailbreaking LLMs', '']"
"hello everyone um my name is Lisa I'm a third year PhD student in the NLP group I'm advised by Percy and Tatsu today I will give a lecture on natural language generation and this is also the research area that I work on so I'm super excited about it I'm happy to answer any questions both during the lecture and after class about natural language generation so nlg is a super exciting area and is also moving really really fast so today we will discuss all the excitement of nlg but before we get into the really exciting part I have to make some announcements so first it is very very important for you to remember to sign up for AWS by midnight today so this will concern this is related to your homework 5 whether you have GPU access and then also related to our final project so please please remember to sign up for it for AWS by tonight and second the project proposal is due on Tuesday next Tuesday and I think assignment 4 should just do it hopefully you had fun in this machine translation and stuff and also assignment 5 is out today I think just now and it is due on Friday uh like basically Friday midnight and uh last we will hold a Transformer I will hold a hugging face Transformer Library tutorial this Friday so if your final project is related to implementing Transformers or playing with large language models you should definitely go to this tutorial because it's going to be very very helpful um also yeah just one more time please remember to sign up for AWS because this is the final hard deadline okay cool now moving on to the main topic for today um the very exciting natural language Generation stuff so today we will discuss what is an LG review sound models discuss about how to decode from language models and how to train language models um and we will also talk about evaluations and finally we'll discuss ethical and risk considerations with the current analogy systems so this natural language generation techniques are going to be really exciting because this is kind of getting us closer to explain the magic of chat GPT which is a super popular model recently and practically speaking they could also help you with your final project if you decide to work on something related to text generation so um let's get started to begin with let's ask the question of what is natural language generation so natural language generation is actually a really broad category people have divided an LP into natural language understanding and natural language generation so the understanding part mostly means that the task input is in natural language such as semantic parsing natural language inference and so on whereas natural language generation means that the task output is in natural language so nlg focuses on systems that produce fluent coherent and useful language outputs for human to use historically there are many analogy systems that use rule-based systems such as templates or infilling but nowadays deep learning is powering almost every text generation systems so this lecture today will be mostly focused on deep learning steps so um first what are some examples of natural language generation it's actually everywhere including our homework machine translation is a form of nlg where the input is some address in the source language and the output is generated text in a targeted language digital assistant such as series or Alexa they are also an LG systems so it takes in dialogue history and generates continuations of the conversation um there is also summarization systems that takes in a long document such as a research article and then the idea is trying to summarize it into a few sentences that are easy to read so beyond these classic tasks there are some more interesting uses like creative story writing where you can prompt a language model with a story plot and then it will give you some creative stories that are aligned with the plot there is state of the text where you give the language model some database or some tables and then the idea is that it will output some textual description of the table content and finally there is also like visual description based nlg systems like image captioning or like image based storytelling so the really cool example um is the popular track GPT models so chat GPT is also an analogy system it is very general purpose so therefore you can use it to do many many different tasks with different prompts for example we can use chat GPT to simulate a chatbot it can ask it can answer questions about like creative gifts for 10 years old it can be used to do poetry generation like for example we can ask you to generate a poem about sorting algorithms and it's actually well I wouldn't say it's very poetic but at least it has the same format as a poem and the content is actually correct so um charging Beauty can also be used in some really useful settings like a web search so here Bing is augmented with chat GPT and there are some twitters that are saying that the magic of chat GPT is that it actually makes people be happy to use Bing um so there are so many tasks that actually belong to the nlg category so how do we categorize these tasks one common way is to think about the open-endedness of the task so here we draw a line for the spectrum of open-endedness on the one end we have tasks like machine translation and summarization so we consider them not very open-ended because for each Source sentence the output is almost determined by the input because basically we are trying to do machine translation the semantic should be exactly similar to the input sentence so there are only a few ways that you can refreeze the output like authorities have announced that today is a national holiday you can rephrase it a little bit to say today is a national holiday announced by the authorities but the actual Space is really small because you have to make sure the semantics doesn't change so we can say that the output space here is not very diverse um and moving to the middle of the spectrum there is dialogue tasks such as task driven dialogue or Chit Chat dialogue so we can see that for each dialog input there are multiple responses and the degree of Freedom has increased here we can say like we can respond by saying good and you or we can say about thanks for asking barely surviving on my homeworks so here we are observing that there are actually multiple ways to continue this conversation and then this is where we say the output space is getting more and more diverse and on the other end of the spectrum there is a very open-ended generation tasks like story generation so given the input like write me a story about three little pigs there are so many ways to continue the prompt right we can write about them going to schools building houses like they always do um so the valid output here is extremely large and we call this open-ended generation so it's hard to really draw a boundary between open-ended and non-open-ended tasks but we still try to give a rough categorization so over the Ender generation refers to tasks whose output distribution has a high degree of Freedom or an non-open under generation tasks refers to tasks where the input will almost certainly determine the output generation examples of non-open ended Generations are machine translation summarization and examples of open-ended Generations are story generation Chit Chat dialogue task oriented dialogue Etc so how do we formalize this categorization one way of formalizing is by Computing the entropy of the nlg system so high entropy means that we we are to the right of the spectrum so it is more open-ended and low entropy means that we are to the left of the spectrum and less open-ended so there's two classes of nlg tasks actually require different decoding and training approaches as we'll talk about later okay cool now let's recall some previous lectures and review the nlg models and trainings that we have studied before so I think we discussed the basics of natural language generation so here is how other aggressive language model works at each time step our model would take in a sequence of tokens as input and here it is y less than T and the output is basically the new token YT so to decide on YT we first use the model to assign a score for each token in the vocabulary denoted as s and then we apply softmax to get the next token distribution p and we choose a token according to this next token distribution and summary once we have predicted YT hat we then pass it back into the language model as the input predict y hat t plus 1 and then we do so recursively until we reach the end of the sequence so any questions so far okay good um so for the two types of energy tasks that we talked about like the open-ended non-open-ended tasks they tend to prefer different model architectures so for now open-ended tasks like machine translation we typically use an encoder decoder system where like the other regressive decoder that we just talked about function as the decoder and then we have another bi-directional encoder for encoding the inputs so this is kind of what you implemented for assignment four because the encoder is like the bi-directional lstm and the decoder is another lstm that is auto regressive so for more open-ended tasks typically other aggressive generation model is the only Oppo is the only component um of course like this architectures are not really hard constraints because a auto-agressive decoder alone can also be used to do machine translation and an encoder decoder model can also be used for story generation so this is kind of the convention for now but it's a reasonable convention because like using decoder only model for Mt tends to hurt performance compared to an encoder decoder model for Mt and using an encoder decoder model for open-ended generation seems to like achieve similar performance to a decoder only model and therefore if you have the compute budget to train an encoder decoder model you might just be better off by only trading a larger decoder model so it's kind of more of an allocation of resources problem than whether this to architecture will type check with your task so um okay so how do we train such a language model in previous lectures we talked about that the language models are trained by maximum likelihood so basically we were trying to maximize the probability of the next token uh YT given the preceding words and this is our optimization objective so at each time step this can be regarded as a classification task because we are trying to distinguish the actual word uh YT star from all the remaining words in the vocabulary and this is also called teacher forcing because at each time step uh we are using the gold standard wise uh y star less than t as input to the model whereas presumably at generation time you wouldn't have any access to Y star so you would have to use the model's own prediction to fit it back into the model to generate the next token and that is called student forcing which will talk in detail later we never used that word before what does it mean Ultra aggressive oh this means like uh so let's look at this animations again oops sorry oh it just looks like uh you are generating word from left to right one by one so here suppose that you are given a y less than T and then other aggressive for your first general YT and then once you have YT you'll fit it back in general YT plus one and then feed it back and generate another thing so this left to right nature because you are using chain rule to like condition on the the tokens that you just generated this chain rule thing is called Auto regressive and typically like I think conventionally we are doing left to right other aggressive by generating from left to right but there are also like other more interesting models that can do backward or influence and other things this idea of generating one token at once is auto regressive cool any other questions yep um so at inference time our decoding algorithm will Define a function to select a token from this distribution so we've discussed that we can use the language model to compute this P which is the next token distribution and then G here based on our notation is the decoded algorithm which helps us select what token we are actually going to use for YT so the obvious decoding algorithm is to greatly choose the highest probability token as YT hat for each time step so well this basic algorithm sort of works because they work for your homework for to do better there are two main avenues that we can take we can decide to improve decoding and we can also decide to improve the training of course there are other things that we can do we can improve training data and we can improve model architectures but for this lecture we will focus on decoding and training so uh now let's talk about how decoding algorithms work for natural language generation models before that I'm happy to take any questions about the previous slides uh I think I'll go into this in detail later but sure so basically for teacher of forcing the idea is like you do teacher forcing where you'll train the language model because you already observe like the gold text so you kind of use the gold text up until timestamp t uh put put it into the model and then the model would try to predict why uh t plus one whereas student forcing means that you don't have access to this gold reference data instead you are still but you are still trying to generate a sequence of data so you have to use uh the text that you generated yourself using the model and then feed it back into the model as input to predict t plus one that's the primary difference cool um so what is decoding all about at each time step uh our model computes a vector of score for each token so it takes in preceding context while less than T and produce a score s and then we try to compute the probability distribution P all of this scores by just applying softmax to normalize them and our decoding algorithm is defined as this function G which takes in the probability distribution and try to map it to some word basically try to select a token from this probability distribution so in the machine translation lecture uh we talked about graded decoding which selects the highest probability token of this P distribution and we also talk about beam search which has the same objective as grade decoding which is that we are both trying to find the most likely string defined based on the model but instead of doing so greedily for beam search we actually explore a wider range of candidates so we have a wider exploration of candidates by keeping always like k k candidates in the beam so overall this maximum probability decoding is good for low entropy tasks like machine translation and summarization but it actually encounters more problems for open-ended generation so the most likely string is actually very repetitive when we try to do open-ended text generation as we can see in this example the context is perfect in normal it's about I mean a unicorn trying to speak English and for the continuation the first part of it is it looks great it's like valid English it talks about science but suddenly it starts to repeat and it starts to repeat like I think uh a institution's name so why does this happen um if we look at for example uh this plot which shows uh the problem the language model's probability assigned to the sequence I don't know we can see like here's the pattern um it has regular probability but if we keep repeating this phrase I don't know I don't know I don't know for 10 times then we can see that there's a decrease in Trend in their negative log likelihood so the y-axis is the negative log probability we can see this decreasing Trend which means that the model actually has higher probability uh as the repeat goes on which is quite strange because it's suggesting that there is a self-amplification effect so the more repeat we have the more confidence the model becomes about this repeat and this keeps going on we can see that for I am tired I'm tired repeat 100 times because it continuously decreasing Trend until the model is almost 100 sure that it's gonna keep repeating the same thing and sadly um this art this problem is not really solved by architecture here the Red Cloud is a lstm model and the blue curve is a Transformer model we can see that both model kind of suffers from the same problem and scale also doesn't solve this problem so we kind of believe that like scale is the magical thing in NLP but even even models with 175 billion parameters will still suffer from repetition if we try to find the most likely string so how do we reduce repetition um one canonical approach is to do unground blocking so the principle is very simple basically you just don't want to see the same engram twice if we send n to be three then for any text that contains the phrase I am happy the next time you see the prefix I am ungram blocking would automatically set the probability of happy to be zero so that you will never see this unground this trigram again but clearly this this underground blocking heuristic has some problems because sometimes it is quite common for you to want to see a person's name appear twice or three times or even more in the text but this unground blocking will eliminate that possibility so what are better options that possibly are more complicated for example we can use a different training objective instead of training by mle we can train by unlikelihood objective so in this approach uh the model is actually penalized for generating already seen tokens so it's kind of like putting this unground blocking idea into training time um rather than a decoding Time Force this constraint at trading time we just decrease the probability of repetition another another training objective is coverage Wells which uses kind of the attention mechanism to prevent repetition so basically if you try to regularize and enforce your attention so that it's always attending to different words for each token then uh it is highly likely that you are not going to repeat because repetition tends to happen when you have similar attention patterns another different angle is that instead of searching for the most likely string we can use a different decoding objective so maybe we can search for Strings that maximizes uh the difference between log probabilities of two models say that we want to maximize log problem large model minus a lot of problem small model in this way because both models are repetitive so they kind of cancels out so like they would both assign High probabilities repetition and after applying this new objective the repetition stuff will actually be penalized because it cancels out so here comes the broader question um it's finally the most likely string even a reasonable thing to do for open-ended text generation the answer is probably no because this doesn't really match human pattern so we can see In This Cloud the orange curve is the human pattern and the blue curve is the machine generated text using beam search so you can see that will with human talks there are actually lots of uncertainty uh in as we can see by the fluctuation of the probabilities like for some words we can be very certain for some words we are a little bit unsure whereas here for the model distribution is always very sure it's always assigning probability one to the sequence so because we now are seeing a answer obviously there's a mismatch between the two distributions so it's kind of suggesting that maybe searching for the most likely string is not the right decoding objective at all any questions so far before we move up yeah the online magazine for like some detector of whether some characters generated by Chinese um not really because uh so this can only detect the really simple things that humans are also able to detect like repetition so uh in order to avoid like the previous problems that we've talked about I'll talk about some other decoding families that generates more robust attacks that actually look like this um whose probability distribution looks like the orange curve so I wouldn't say this is like the to go answer for watermarking or detection oh yeah Okay cool so she asked about whether um whether this mechanism of plotting the probabilities of human text and machine generated text is one way of detecting whether some text is generated by model or human and my answer is I don't think so but this could be an interesting research Direction because I feel like they are more robust decoding approaches that generate texts that are that actually fluctuates a lot um so yeah let's talk about the decoding algorithm that is able to generate text that fluctuates so given that searching for the most likely string is a bad idea what else should we do and how do we simulate that human pattern and the answer to this is to introduce Randomness and stochasticity to decoding so um suppose that we are sampling a token from this distribution of P basically like we are trying to sample YT hat from this distribution It Is Random so that you can essentially sample any token distribution previously you are kind of restricted to selecting rest for more grocery but now you can select bathroom instead so however uh sampling introduces a new set of problems since we never really zero out any token probabilities vanilla vanilla sampling would make every token in the vocabulary a viable option and in some unlucky cases we might end up with a bad word so assuming that uh we already have a very well trade model like even if most of the probability mass of the distribution is over the limited set of good options the tail of the distribution will still be very long because we have so many words in our vocabulary and therefore if we add all those round Tails it Aggregates they still have a considerable Mass so statistically speaking this is called heavy tail distribution and language is exactly a heavy tail distribution so for example like uh many tokens are probably really wrong in this context and then given that we have a good language model we assign them each very little probability thus this doesn't really solve the problem because there are so many of them so you aggregate them as a group will still have a high chance of being selected and the solution here that we have for this problem of long tail is that we should just cut off the tail we should just zero out the probabilities that we don't want and one idea is called top place that a top case sampling where the idea is that we would only sample from the top K tokens in the probability distribution any questions for now okay yeah well the model we were looking at a second ago had some really low probability samples as well on the graph right I would copy something with that uh you mean this one or even uh the orange blue graph of the human versus uh oh yeah yeah so uh top cable basically uh eliminate it will not it will make it impossible to generate the super low probability tokens so technically it's not it's not exactly simulating this pattern because now you don't have the super low probability tokens whereas human can generate super low probability television affluence way but yeah that's that could be um another like hint that people can use for detecting a machine generated text yeah depends on the type and text you want to generate for example poem or novels or more creative writing but then you decide to hyper correct yeah yeah for sure case I have a parameter that depending on the type of task you will choose K differently uh maybe mostly for close and the task K should be small and for open-ended case should be large yeah cluster in the back how come like I guess intuitively this builds up of one of the earlier questions why don't we consider the case like where we sample and then we just weight the probability of each word by it's like score or something rather than just looking at top trade how can we don't do like a weighted sampling type of situation so we still have that small but non-zero probability of selecting uh I think Top Care is also like rated so like top K just kind of zeros out all the Tails of the distribution but for the things that I didn't zero out uh it's not like a uniform Choice among the K it's still trying to choose proportional to the scores that you computed is that just like a computational like 17 000 words it could be like for like 10 or something um yeah sure that could be one gain of 12K decoding is that your self Max will take in fewer uh fewer candidates yeah but it's not the main reason I think you should show yeah yeah I'll keep talking about the many reasons um so we've discussed this part and then here uh this is the formal this is kind of the formerly what is happening for top case sampling uh now that we are only sampling from the top K tokens of the probability distribution and as we've said K is a hyper parameter so we can set K to be large or small uh if we increase K this means that we are making our output more diverse but at the risk of including some tokens that are bad if we decrease k then we are making more conservative and safe options but possibly the generation will be quite generic and boring um so uh is top K decoding good enough the answer is not really because we can still find some problems with top K decoding for example in the context she said I never blank there are many words that are still valid options uh such as won't 8 but those words got zeroed out because they are not within the top K candidates so this actually leads to bad recall for your generation system and similarly another failure of top K is that it can also cut off too quickly so in this example code is not really a valid answer according to common sense because you probably don't want to eat a piece of code um but the probability remains non-zero meaning that the model might still sample code as an output despite this low probability but it might still happen and this means bad Precision for the generation model so given this problems with top K decoding how can we address them how can we address this of this issue of like there is no single K that fits all circumstances um this is basically because the probability distribution that we sample from our Dynamic so when the probability distribution is relatively flat having a small cable remove many viable options so the having a limited cable removes many viable options and we want K to be larger for this case similarly when a distribution p is too picky then we want the like a high K would allow for too many options uh to be viable and instead we might want a smaller K so that we are being safer um so the solution here is that maybe K is just a bad Haver parameter and instead of doing K we should doing we should think about probability we should think about how to sample from tokens in the top P probability percentiles of the cumulative probability mass of the CDF for example so now um the the advantage of doing top P sampling where we sample from the top P percentile of the cumulative probability mass is that this is actually equivalent to we have now a adaptive k for each different distribution and let me explain what what I mean by having like an Adaptive k so in the first distribution this is like a regular power law of language that's kind of typical and then uh doing top uh doing top case sampling means we're selecting the top K but doing the top P sampling means that we are zooming into maybe like like something that's similar to top K and in fact but if I have a relatively flat distribution like the blue one we can see that's doing top p means that we are including more candidates and then if we have a more schools distribution like the green one doing top p means that we actually include fewer candidates so by actually selecting like the the top P percentile in the probability distribution we are we are actually having a more uh flexible okay and therefore have a better sense of what are the good options uh in the model any questions about top P top K decoding so everything's clear yeah sounds good um so to go back to that question uh doing top K is not necessarily saving compute or like this whole idea is not really compute saving intended because uh in the case of top p in order to select the top P percentile we still need to compute the soft Max over the entire vocabulary set in order for us to do top pay properly to compute the P properly so therefore it's not really saving compute but it's improving performance moving on um so there are much more to go with decoding algorithms with uh besides the topic and top P that we've discussed there are some more recent approaches like typical typical sampling where the idea is that we want to relate the score based on the entropy of the distribution and try to generate tags that are closer to the negative whose probability is closer to the negative entropy of the data distribution this kind of means that if you have a closed-ended task or non-open-ended task you want it has smaller entropy so you want a negative log probability to be smaller so you want probabilities to be larger so it kind of TAPS it tap checks very well and additionally there is also Epsilon sampling coming from John so this is an idea where we set the threshold for to lower bound probabilities so basically if you have a word whose probability is less than .03 for example then that word will never appear um in the output distribution now that that word will never be part of your output because it has still will probability yeah oh cool great question so the entropy distribution is defined as um like you can suppose that we have a discrete distribution we can go over it like we'll just enumerate X and then it's like negative log probability of X so like if we write it from a from an expectation perspective it's basically expected of well probability of x okay I'll I have to do a little bit here so so this is the entropy of a distribution and then so basically if you are distribution is very very concentrated to a few words then the entropy will be relatively small if your distribution is very flat then your entropy will be very large yeah the Epsilon sampling is set such that we have no valid oh yeah I mean I bump back off cases I think so in the case that there is no valid options um You probably still want to select one or two things just as a edge case I think okay cool um moving on so another hyper parameter that we can tune to affect decoding is the temperature parameter so recall that previously at each time step we asked the model to compute a score um and then we renormalize that score using solve Max to get a probability distribution so one thing that we can adjust here is that we can insert this temperature parameter Tau to relate the score so basically we just divide all the SW by Tau and after dividing this we apply solve Max and we get a new distribution and this temperature adjustment is not really going to affect the monotonicity of the distribution for example if word a has higher probability than word b previously then after the adjustment where a is still going to have a higher probability than word b but still relative difference will change so um for example if we raise the temperature Tau to be greater than one then the distribution PT will become more uniform it will be flatter and this kind of implies that there will be more diverse output because our distribution is flatter and it's more spread out across different words in the vocabulary on the other hand if we lower the temperature Tau less than one then PT becomes very spiky and then this means that we are if we sample from the PT we'll get less diverse output um so because here the probability is concentrated only on the top words so in the very extreme case if we set Tau to be very very close to zero then the probability will kind of be a one hot Vector where all the probability mass will be centered on one word and then this kind of reduces back to Arc Max sampling or greedy decoding so temperature is a hyper parameter as well as as for K and P in top K on top P it is a hyper parameter for decoding it can be tuned for beam search and sampling algorithms so it's kind of orthogonal to the approaches that we discussed before any questions so far okay cool uh temperature is so easy so um well because something still involves Randomness like even though we do we try very hard in terms of truncation truncating the tail something still has Randomness so what if we're just unlucky and decode a bad sequence from the model um one common solution is to do re-ranking so basically we would decode a bunch of sequences like for example we can decode 10 candidates um but like 10 or 30 is up to you the only choice is that you want to balance between your compute efficiency and performance so if you decode too many sequences then of course your performance is going to increase but it's also very costly to to just generate a lot of things for one example and then so once you have a bunch of uh sample sequences then we are trying to define a score to approximate the quality of the sequence and re-rank everything and re-rank all the candidates by this score so the simple thing to do is we can use a perplexity as a metric as a score as a scoring function but we need to be careful that because we have talked about this like the extreme of perplexity like if we try to Arc Max log probability we will try to aim for a super well perplexity the attacks are actually very repetitive so we shouldn't really aim for extremely low perplexity and perplexity to some extent it's not a perfect re-scoring function it's it's not a perfect scoring function because it's not really robust to maximize so alternatively the re-rankers can actually use a wide variety of other scoring functions that we can score text based on their style their discourse coherence uh their entailment factuality properties consistency and so on um and additionally we can compose multiple re-rankers together uh yeah questions 10 candidates or any number of candidates yeah what's the strategy usually use to generate these other candidates like what you're listening to use oh yeah so basically the idea is to sample from the model right so when you sample from the model each time you sample you're going to get a different output and then that's what I mean by different candidates so if you sample 10 times you will get 10 you will very likely get 10 different outputs and then you are just given these 10 different outputs that come from sampling you can just decide re-rank them and select the candidate that has the highest score oh because we are sampling here yeah yeah for example if you are doing like top three something then well suppose that A and B are equally probable then you might sample a your max sample B with the same probability okay cool and another cool thing that we can do is re-ranking is that we can compose multiple re-rankers together so basically you can suppose you have a scoring function for style and you have a scoring function for factual consistency you can just add those two scoring functions together to get a new scoring function and then uh re-rank everything based on your new scoring function to get tags that are both good at style and good at factual consistency do we just pick the decoding that has the high score or do we do some more sampling again based on the story uh the idea is you just take the decoding that has the highest score because you already have like say 10 candidates so out of this 10 you only need one and then you just choose one that has the highest score yeah cool any other questions yeah sorry what what is perplexity oh yeah perplexity is like you can kind of regard it as log probabilities uh it's it's proportion it's like e to the negative well probabilities kind of like uh if uh if a talker has high perplexity then it means it has a low probability because you are more perplexed okay so um taking a step back to summarize this decoding section we have discussed uh many decoding approaches from selecting the most probable string to selecting uh to sampling and then to various truncation approaches that we can do to improve sampling like top P top K Epsilon typical decoding and finally we discuss how we can do in terms of re-ranking the results so uh decoding is still a really essential problem in energy and there are lots of Works to be done here still especially as like chai GPD is so powerful we should all go study decoding um so it would be interesting if you want to do such final projects and also different decoding algorithms can allow us to inject different inductive biases uh to the to the text that we are trying to generate and some of the most impactful advances in nlg in the last couple years actually come from simple but effective decoding algorithms for example the nuclear sampling is the nuclear sampling paper is actually very very highly cited so moving on to talk about training analogy models well we have seen this example before in the decoding slides and I'm just trying to show them again uh because even though we can solve this repetition Problem by by instead of doing search doing sampling um the but it's still concerning from a language modeling perspective that's your model would put so much probability on such repetitive and degenerate text so we asked this question well is repetition due to how language models are trained you have also seen this Cloud before which shows this decaying pattern or like the self amplification effect so we can conclude from this observation that model trained via a mle objective wears a really bad like whereas really bad mode of the distribution by mode of the distribution I mean the arguments of the distribution so basically they would assign high probability to terrible strings and this is definitely problematic for a model perspective so why is this the case shouldn't mle be like a gold standard in machine translation uh in in machine learning in general not just machine translation should an ml be like a gold standard for machine learning um the answer here is not really especially for text because mle has some problem for sequential data and we call this problem exposure bias um so training with teacher forcing leads to exposure bias at generation time because during training our model's inputs are gold context tokens from real human generated text as denoted as I had less than T here but during generation time our model's input become previously decoded tokens from the model well I had to and suppose that our model has minor arrows than like what I had T why had less than T will be much worse in terms of quality than y star less than T and this discrepancy is terrible because it actually causes a discrepancy between trading and test time which actually hurts model performance and we call this problem exposure bias um so people have proposed many solutions to address this exposure bias problem uh one thing to do is to do um scheduled sampling which means that uh with probability P we try to decode a token uh and feed it back in as context to train the model and this probability one minus P we use the gold tag we use the gold token as context so throughout trading we try to increase P to gradually warm it up and then prepare it for test time generation so this leads to Improvement in practice because of using this T using this P probabilities we're actually graduating uh like trying to narrow the discrepancy between training and test time but the objective is actually quite strange and training can be very unstable another idea is to do data set aggregation and the method is called dagger essentially at various intervals during training we try to generate a sequence of text from the current model and then use this and then put the sequence of text into the training data so we're kind of continuously doing this uh training data augmentation scheme to make sure that the trading distribution and the generation distribution are closer together so both approaches both scheduled sampling and data set aggregation are ways to narrow the discrepancy between training and test yes question just means human text I mean it's like uh well so little language model you will see lots of Corpus that are human written gold is just human okay cool um so another approach is to do retrieval augmented generation so we first learned to retrieve a sequence from some existing Corpus of prototypes and then we train a model to actually edit the retrieved text by doing insertion deletion or swapping we can add or remove tokens from this prototype and then try to modify it into another into another sentence so this doesn't really suffer from exposure bias because we start from a high quality prototype so that's at trading time and at test time like you don't really have the discrepancy anymore because you are not generating from left to right um another approach is to do reinforcement learning so here the idea is to cast your generation problem as a Markov decision process so there is the state as uh which is the model's representation for all the preceding context there is action a uh which is basically like the next token that we are trying to pick and there's policy which is the language model or also called the decoder and there is the reward R which is provided by some external score and the idea here uh well like we won't go into details about reinforcement learning and how it works but we will recommend the class CS two three like 234. so um in the reinforcement learning context because reinforcement learning involves a reward function that's very important so how do we do reward estimation for tax Generation Well really natural idea is to just use the evaluation metrics so whatever because you are trying to do well in terms of evaluation so why not just improve for evaluation metrics directly at training time for example in the case of machine translation we can use blue score as the reward function in the case of summarization we can use root score as the reward function but we really need to be careful about optimizing for tasks as opposed to gaining the reward because evaluation metrics are merely proxies for the generation quality so sometimes suppose that you run RL and improve the blue score by a lot but will you will run human evaluations humans might still think that well this this generated tax is no better than the previous one or even worse even though it gives you a much better blue score so we want to like be careful about this case of not gaining the reward so what behaviors can we tied to a reward function this is about reward design and reward estimation there are so many things that we can do we can do cross modality consistency for image captioning we can do sentence similarity to a sentence Simplicity to make sure that we are generating simple English that are understandable we can do formality and politeness to make sure that I don't know like your chatbot doesn't suddenly yell at you um and the most important thing that's really really popular uh is recently is human preference so we should just build a remote a reward model that captures human preference and this is actually the technique behind the chat GPT model so the idea here is that we would ask human to rank a bunch of generated text based on their preference and then we will use this preference data to learn a reward function which will basically always assign high score to something that humans might prefer and assign a low score to something that humans wouldn't prefer Yeah question more expensive oh yeah sure I mean it is going to be very expensive but I feel like uh compared to all the cost of trading models trading like 170 billion parameter models um I feel like open Ai and Google are well they can't afford hiring lots of humans to do human annotations and ask their preference yeah yeah this is a great question so um I think it's kind of a mystery about how much data you exactly need to achieve the level of performance of chat GPT but roughly speaking I feel like I mean like whenever you try to fine-tune a model on some Downstream tasks similarly here you are trying to find through your model on on human preference it do need quite a lot of data like maybe on a scale of 50k to 100K that's roughly the scale that like anthropic actually released some data set about human preference that's roughly the skill that they released I think um if I remember correctly Yeah question we talked about earlier about how many of the state-of-the-art language models use Transformers as their architecture how do you apply reinforcement learning to this model uh to to what do you mean to Transformer model yeah yeah I feel like um reinforcement learning is kind of a modeling tool I mean it's kind of an objective that you are trying to optimize instead of an mlu objective now you are optimizing for an RL objective so uh it's not real it's kind of orthogonal to the architectural choice so uh Transformer is an architecture you just use Transformer to give you probability of the next token distribution or to to try to estimate probability of a sequence and then once you have the probability of a sequence you use that probability of the sequence pass it into the uh the RL objective that you have and then suppose that you are trying to do policy gradient or something then you need to estimate the probability of that sequence and then you just need to be able to back prop uh through Transformer which is doable yeah so I think like the question about architecture and objectives are orthogonal so even if you have an ostm you can do it you have a Transformer you can also do it yep cool hope I answered that question yeah can you just like with the model T4 to for this country well for example we can build another Transformer to like to calculate yeah I think that's exactly what they did so they uh so for example you would have gpt3 right um you use gpd3 as the generator that generate text and you kind of have another pre-trained model that it could probably also be gpd3 but I'm guessing here um that you fine-tune it to your human preference and then once you have a human preference model uh you use the human preference model to put it into RL as the reward model and then use the original gpd3 as the policy model and then you you apply our objectives and then update them so that you will get a new model that's better at everything okay cool um yeah actually if you are very curious about earlier chap I would encourage you to come to the next lecture which is uh and where Jesse will talk about rlhs which is uh RL HF is shorthand for RL using human pref uh using human feedback foreign teacher enforcing is still the main algorithm for training tax generation models and exposure bias causes problems in tax generation models for example it causes models to lose coherence cause this model to be repetitive and models must learn to recover from their own bad samples by using techniques like scheduled sampling or dagger and models shouldn't another approach to to reduce exposure bias is to start with good text like retrieval of class generation and we also discussed how to do training with RL and this can actually make model learn behaviors that are preferred by human perform that are preferred by human or preferred by some metrics so uh to be very up-to-date in the best language model nowadays check GPT the trading is actually pipelined for example we would first pre-train a large language models using internet Corpus by self-supervision and this kind of gets your chat GPT like the uh sorry gpt3 which is the original version and then you would do some sorts of instruction tuning to fine-tune the language model to fine-tune the pre-trained language model so that it learns roughly how to follow human instructions and finally we will do rlhs to make sure that these models are well aligned with human preference so if we start RL HF from scratch it's probably going to be very hard for the model to converge because RL is hard to train for text Data Etc so RL doesn't really work from scratch but with all these smart tricks about pre-training and instruction tuning suddenly now like they're they're off to a good start cool any questions so far okay oh yeah [Music] uh you mean the difference between dagger and schedule sampling is how long the the sequence are yeah I think roughly that is that is it because like for dagger you are kind of trying to you are trying to put in um full generated sequence but I feel like there can be variations of dagger dagger is just like a high level framework and idea there can be variations variations of dagger that are very similar to scheduled sampling I think I feel like for schedule sampling it's kind of a more smooth version of dagger because dagger for dagger you have to like uh for well basically for this Epoch I am generating something and then I after this Epoch finishes I put this into the data together and then train for another Epoch whereas dagger seems to be more flexible in terms of when you add data in yes look it's for daggers to the rest of the models coming out but like how does it helpful model um I think that's a that's a good question I feel like if you regress the model for example um if you regress the model on its own output uh I think well I think there are there should be smarter ways than to exactly regress on your own output for example you might still like consult some good reference data for example given that you ask the model to generate for something and then you can instead of using uh say you ask the model generate for five tokens and then instead of using like the models generation to be the sixth token you'll probably try to find some examples in the training data that would be a good continuations and then you try to plug that in by like connecting the generation the model generation and some gold text and then therefore you are able to kind of correct the model uh even though it it probably went off path a little bit by generating its own stuff so it's kind of like letting the model learn how to correct for itself but yes I think you are right if you just ask the model to uh gen if you just put model generation in the data it shouldn't really work yeah any other questions cool um moving on yes um so now we'll talk about uh how we are going to evaluate Energy Systems so there are three types of methods for evaluation there is content overlap metrics um there is model based metrics and there is human evaluations so first content overlap metrics computer score based on lexical similarities between the generated text and the gold reference text so the advantage of this approach is that it's very fast and efficient and widely used for example a blue score is very popular in Mt and root score is very popular in summarization um so these models are very popular because well these methods are very popular because they are cheap and easy to run but they are not really the ideal metrics for example simply rely on lexical overlap might miss some refreshings that have the same semantic meaning or it might reward text with a large portion of lexical overlap but actually have the opposite meaning so you have lots of both false positive and false negative problems uh so despite all these disadvantages the metrics are still the to-go evaluation standard in machine translation part of the reason is that Mt uh is actually super close ended it's very non-open-ended and then therefore this is probably still fine to use uh like blue score to measure machine translation and they get progressively worse for tasks that are more open-ended for example they get words for summarization as long as the output text because the output text becomes much harder to measure they are much worse for dialogue which is more open-ended and then they are much much worse for story generation which is also open-ended and then the drawback here is that because like the underground metrics um this is because like suppose that you are generating a story that's relatively long then if you are still looking at word overlap then you might actually get very high ungram scores because of your taxes very well not because it's accurate of high quality just because you are talking so much that you might have covered lots of points already yes exactly that's kind of the the next thing that I will talk about uh as a better metric for evaluation uh but for now let's do like a case study of a failure mode for uh Google score for example so suppose that Chris asks a question are you enjoying the cs224a lectures um the correct answer of course is hack yes um so if we have this if if one if one of the answer is yes it will get a score of 0.61 because it has some lexical overlap with the correct answer if you answer like you know it then it gets a relatively lower score because it doesn't really have any lexical overlap except from the exclamation mark and if you answer Yep this is semantically correct but it actually gets zero score because there is no lexical overlap between the gold answer and the generation if you answer hack no this should be wrong um but because it has lots of Sims but because it has lots of lexical overlap with the correct answer um it's actually getting some high scores so these two cases are the major failure modes of lexical based engram overlap metrics you get false negative and false positives so um moving beyond this failure most of lexical based metrics the next step is to check for semantic similarities and model based metrics are better at capturing the semantic similarities uh so this is kind of similar to what you kind of raised up like a couple of minutes ago we can actually use learn representation of words and sentences to compute to compute semantic similarities between generated and reference text um so now we are no longer bottom at a bottlenecked by ungram and instead we are using embeddings and these embeddings are going to be pre-trained but the methods can still live on because we can just swap in different pre-trained methods and use the fixed metrics so here are some good examples of the metrics that could be used uh one thing is to do Vector similarity this is very similar to homework one uh where if you are trying to compute similarity between words except now we're trying to compute similarity between sentences there are some ideas of how to go from word similarity to sentence similarities for example you can just average the embedding which is like a relatively naive idea but it works uh sometimes another high-level idea is that we can measure word movers distance um the idea here is that we can use optimal transports to align the source and Target word embeddings suppose that your Source word embedding is Obama speaks to the media in Illinois and the target is the the president Grace the press in Chicago from a human evaluation perspective these two are actually very similar but they are not exactly aligned word by word so we need to figure out how to optimally align word to word like align Obama to president allowing Chicago to Illinois and then therefore we can compute a score we can compute the pairwise word embedding difference between this and then get a good score for the model for the sentence similarities and finally there is Bird score which is also a very popular metric for semantic similarity so it first computes pairwise cosine distance using birth embeddings and then it finds an optimal alignment between the source and Target sentence and then they finally compute some score so I feel like uh these details are not really that important but the high level idea is super important is that we can now use uh like we can now use word embeddings to compute sentence similarities by doing some sort of smart alignment and then transform from word similarity to sentence similarities to move Beyond word embeddings we can also use sentence embeddings to compute sentence similarities so typically this doesn't have the very comprehensive alignment by word problem um but it has similar problems about you need to now align sentences or phrases in a sentence and similarly there is Port which is slightly different it is a regression model based on birth um to so the model is trained as a regression problem to return the score that indicate how good the text is in terms of grammaticality and the meaning of the reference and similarity with the reference text so this is kind of a trading evaluation as a regression problem any questions so far okay cool you can move on um so all the previous Mission approaches are evaluating semantic similarities so they can be applied to non-open ended generation tasks but what about open-ended settings so here enforcing semantic similarity seems wrong because a story can be perfectly fluent and perfectly high quality without having to reassemble any of the reference stories so one idea here is that um maybe we want to evaluate open-ended text generation using this mouth score mob score computes the information Divergence in a contest embedding space between the generated text and the gold reference text so um here is roughly the detail of what's going on suppose that you have a batch of text from the gold reference that are human written and you have a batch of tags that's generated by your model um Step number one is that you want to embed this text you want to put this text into some continuous representation space which is kind of the figure to the left and but it's really hard to compute any distance metrics in this continuous embedding space because um well different sentences might actually lie very far away from each other so the idea here is that we are trying to do a k-means cluster to discretize The Continuous space into some discrete space now after the discretization we can actually have a histogram for the for the gold human written text and the histogram for the machine generated text and then we can now compute Precision recall using this to discretize the distributions and then we can compute Precision by like forward K on recall that backward KL yes question why do we want to discretize it and then we touch that why do we want to discard Hazard so imagine that you suppose uh maybe like it's equivalent to answer why is it hard to work with the continuous space the idea is like if you're in that a word if you embed a sentence into the continuous space say that it lies here and you embed another sentence in a confused with the lies here suppose that you only have a finite number of uh sentences then they would basically be direct Delta distributions in your manifold right so it's hard to like you probably want a smoother distribution but it's hard to Define what is a good smooth distribution uh in the case of text embedding because they're not super interpretable so therefore eventually you will have like a um if you embed everything in a continual space you will have like lots of direct Deltas that are just very high and then not really connected to the to their today's neighbors so it's hard to uh so it's hard to quantify Chao Divergence or a distance Matrix in that space well for example you have to make some assumptions for example you want to make gaussian assumptions that I want to smooth all the embeddings by convolving with the gaussian and then you can start getting some meaningful distance metrics but it's just the embeddings uh although you're not going to get meaning for distance metrics and then it doesn't really make sense to smooth things using gaussian because who said uh word representations are gaussian related yeah classrooms I think this requires some gaussian smoothie yeah I think the plot is made with some smoothie yeah I mean I didn't make those clouds so I couldn't be perfectly sure but I think the fact that it looks like this means that you smoothed it a little bit these are kind of sentence and weddings or concatenated word embeddings because you are comparing sentences to sentences not words to words yeah so the advantage of mouth score is that it is applicable to open-ended settings because you are now measuring precision and recall with regard to the Target distribution cool so it has a it has a better probabilistic interpretation than all the previous similarity metrics cool any other questions yes how's that different from just trying to maximize it the similarity between oh yeah that's a good question um well this is because in a case where it's really hard to get exactly the same thing like well for example I would say that if maybe because I've never tried this myself but if you try to run off on a machine translation task you might get very high score um but for if you try to run full score on the open-ended text generation you will get super low score so it's just not really measurable because everything's so different from each other uh so I feel like moth is kind of a middle ground where you are trying to evaluate something that are actually very far away from each other but you still want a meaningful representation yeah of course I mean if you are source and Target are exactly the same or are just different app to some refreshings you will get the best small score but maybe that's not really what you're looking for because given the current situation you only have Generations that are very far away from the gold text how do we evaluate this type of things yes question in the back I'm still trying to understand the most score is it possible to write a the map even in just kind of pseudo uh simple form yeah I think it's possible I mean maybe we come for this discussion after class and because I kind of want to finish my slides yeah but happy to chat after class there is a paper a lot if you search for mouth score I think it's probably the best paper in some scml or Europe's conference as well okay so moving on um I've pointed out that there are so many evaluation methods so let's take a step back and think about what's a good metric for evaluation methods so how do we evaluate evaluations nowadays the gold standard is still to check how well this metric is aligned with human judgment so if a model match human preference uh in other words if the metric is very correlated with if the metric correlates very strongly with human judgment then we say that the metric is a good metric so in this part people have shown people have pulled out a Google score and human score uh y and x axis respectively and then we because we didn't see a correlation a strong correlation this kind of suggests that blue score is not a very good metric so uh actually the gold standard for human evaluation the gold standard for evaluating language models is always to do human evaluation so automatic metrics fall short of matching human decisions and human evaluation is kind of the most important criteria for evaluating text that are generated from a model and it's also the gold standard in developing automatic metrics because we want everything to match human evaluation um so what do we mean by human evaluation how is it conducted typically we will provide human annotators with some access that we care about like fluency coherence open for open-ended tax generation suppose that we also care about factuality for summarization we care about the style of the writing and Common Sense for example if you're trying to write a children's story uh essentially like another thing to note is that please don't compare human evaluations across different papers or different studies because human evaluations tends to not be well collaborated and are not really reproducible even though we believe that human evaluations are the gold standard there are still many drawbacks for example human evaluations are really slow and expensive uh so but even beyond the slow and expensiveness they are still not not perfect because first human evaluations the results may be inconsistent and it may not be very reproducible so if you ask the same human whether you like ARB they might say a the first time and B the second time so and then human evaluations are typically not really logical um and it's really and sometimes like the human annotators might misinterpret your question suppose that you want them to measure coherence of the text different people have different criteria for coherence some people might think coherence is equivalent to fluency and then they look for grammaticality arrows some people might think coherence means how well your continuation is aligned with the prompt or the topic so there are all sorts of misunderstandings that make that might make human evaluation very hard and finally human evaluation only measures Precision not recall this means that you can give a sentence to human and ask the human uh how do you like the sentence but you couldn't ask the human like whether this model is able to generate all possible sentences that are good so it's only a precision based metrics not a recall based metrics so here are two approaches that tries to like combine human evaluations with uh modeling for example uh the first idea is basically trying to learn a metric from Human judgment um basically by by trying to use human human judgment data uh as trading data and then train a model to simulate human judgment and the second approach is trying to ask human and the human and model to collaborate so that the human would be in charge of evaluating Precision whereas the model would be in charge of evaluating recall um also like we have tried approaches in terms of evaluating models interactively so in this case we will no longer we not only care about the output quality we also care about how the person feels when they interact with the model when they try to be a co-author with the model and how the person feels about the writing process Etc so this is called trying to evaluate the models more interactively um so the takeaway here is that content overlap is a bad metric uh semantic based like model based metrics become better because it's more focused on semantics but it's still not good enough human judgment is the gold standard but it's hard to do human judgment it's hard to do human study well and in many cases this is a hint for final project the best charge of the output quality is actually you so if you want to do a final project in like natural language generation you should look at the model output yourself and don't just rely on the numbers that are in that are reported by Blue swirl or something cool um so finally we will discuss ethical considerations of natural language generation problems so as language models gets better and better ethical considerations becomes much more pressing so we want to ensure that the model are well aligned with human values for example we want to make sure the models are not harmful they are not toxic and we want to make sure that the models are unbiased and fair to all demographics groups so for example here we also we don't want the model to generate any harmful content basically I try to prompt cat GPT to say can you write me some toxic content can GPT politely refuse me um which I'm quite happy about but there are there are other people who kind of like try to jailbreak chat GPT the idea here is that creativity actually I think internally they probably implement some detection tools so that we will try to prompt it adversarially it's going to avoid doing adversarial things but here there are many very complicated ways to prompt chat GPT so that you can get over the firewall and then therefore still ask you ability to generate some I don't know like bad English but uh so another problem with uh this large language models is that they are not necessarily truthful so for example this very famous on news that uh Google's model actually generates factual arrows um which is quite disappointing but I mean like but the way the model talks about it is very convincing so like you wouldn't really know that it's a factual error unless you go check that this is not the picture of the this is not the first picture or something so we want to avoid this type of problems um actually like the models have already been trying very hard to refrain from like generating harmful content uh but like for models that are more open sourced and are smaller the same problem still appears and then typically like when we do our final project or we work with models we are probably going to deal with much smaller models and then therefore we need to think about ways to deal with these problems better so text generation models are often constructed from pre-trained language models and then pre-train language models are trained on internet data which contains lots of harmful stuff and biased so when when we prom when the models are prompted for this information they will just repeat the negative stereotypes that they learn from the internet training data so one way uh to avoid this is to do extensive data cleaning so that the pre-training data does not contain any bias or stereotypical content however this is going to be very labor intensive and almost impossible to do because filtering a large amount of internet data is just so costly that is not really possible um again this existing language models like gpt2 medium there are some adversarial inputs that almost always trigger toxic content and these models might be exploited in the real world in the real world by EU intended people so for example there's a paper about Universal adversarial triggers where the authors just find some Universal set of words that would trigger bad contents from the model that would trigger toxic content from the model and sometimes even if you don't try to trigger the model the model might still start to generate toxic content by itself so in this case the pre-trained language models are prompted with very innocuous prompts but they still degenerate into toxic content so um the takeaway here is that models really shouldn't be deployed without proper safeguards to control for toxic content or any harmful contents in general and models should not be deployed without consider rate without careful considerations of how users will interact with these models um so in the Asic section one major takeaway is that we are trying to Advocate that you need to think more about your model about the model that you are building so before deploying or publishing any nlg models please check if the models are output is is not harmful and please check if the model is more robust is robust to all the trigger words and other adversarial prompts and of course there are more so well basically one can never do enough for to improve the assets of tax generation systems and okay cool I still have three minutes left so I can still do concluding thoughts um the idea here well today we talk about the exciting applications of natural language generation systems um so but well one might think that while given that chaiji 50 is already so good are there any other things that we can do research-wise if you try interacting with these models um if you try to interact with these models actually you can see that there are still lots of limitations in their skills and performance for example check GPT is able to like do a lot of things with manipulating text but it couldn't really create like interesting contents or I couldn't really think deeply about stuff so it's still also so there are lots of headrooms and there are still many improvements ahead and evaluation remains a really huge challenge in natural language Generation Um basically we need better ways to automatically evaluate performance of nlg models because human evaluations are expensive and not reproducible so it's better to figure out ways to how to compile all those human judgments into a very reliable and trustworthy model and also with the advance of all these large-scale language models uh doing like neural net doing like neural natural language generation has been reset and it's never been easier to jump into this space because now there are all the tools that are already there for you to build upon and finally it is one of the most exciting and fun areas of NLP to work on so yeah I'm happy to chat more about nlg if you have any questions post after class and in class I guess into one minute okay cool that's everything so do you have any questions if you don't we can end the class 
","['', 'NLG is a subfield of NLP that deals with generating human-like text.', 'NLG applications include machine translation, chatbots, and summarization.', 'NLG tasks can be categorized based on open-endedness: closed-ended (machine translation), mid (dialogue), open-ended (story generation).', 'Autoregressive models are commonly used for NLG, where the next word is predicted based on previous words.', 'Teacher forcing is a technique for training NLG models where the ground truth is fed back into the model.', 'Top-k decoding is a technique for NLG models where only the top k most probable tokens are considered at each step.', 'Beam search is a decoding algorithm that explores a set of top k partial sequences and expands the most promising ones.', 'Re-ranking is a technique for improving the quality of NLG outputs by scoring and selecting the best candidate.', 'NLG models can be evaluated using perplexity, BLEU score, or human evaluation.', 'Large language models like GPT-3 are powerful NLG models with limitations in creativity and reasoning.', 'Ethical considerations of NLG include bias, fairness, and generation of harmful content.', 'Safeguards against harmful content in NLG models include data cleaning and filtering.', '']"
"okay hi everyone um welcome back to we're now in past the halfway point week six of cs 224 n um and so let me just give um a couple of quick announcements first um so today is the day that you have to have done the mid um quarter survey by hundreds of people have but if you haven't this is your last chance um to get the half point for that today is also the day um that final project proposals are due we really encourage you to try and hand them in on time or nearly on time that's really just to help you so we can more quickly give you feedback on final project proposals um and in the background then there's also assignment five you will have seen the message that we're giving you one extra day for that but we do certainly encourage you to be hard at work on assignment five at this point hopefully it's a great exciting opportunity to be learning all the latest stuff about transformers and then today i'm delighted to have our first invited speaker um let me just mention that going along within the half point of participation credit um is you guys writing a reaction paragraph talking about something um that the speaker talks about and their instructions upper for that um on ed um but without further ado let me introduce dante chen um so dante is one of the foremost researchers in question answering and she's particularly well known um in recent work for being one of the co-authors of the roberta paper the spanbert paper and on using dense passage retrieval methods for open domain question answering and as a professor at the um princeton university um but as one other comment um don she once upon a time was the head ta of cs224n um so she's quite familiar um with the context of this class um so i'm really delighted to have done she here to give this lecture on question answering thanks thank you chris for the introduction um for me it's a very good it's a great opportunity for me to come back to cs2201 today and give this lecture also b virtually so question answering the areas that have been working quite a bit in the last few years so today i'm very happy to introduce you some of the fundamentals in this field as well as some cutting edge and save our topics so here is my plan for this lecture so first i will be um give a brief introduction of what is question answering and what kind of problems that people are starting today so i'm going to spend the most of this lecture focus on one type of question answering problems called reading comprehension so this is basically problems of how we build systems to answer questions over a single passive text so i know that many of you are going to do a default project on the stanford question answering data set so understanding this part will be very crucial for your final project so at the end of this lecture i'm hoping to spend like hopefully like 20-ish minutes to talk about a more practical and in my opinion or more exciting problem called open domain question answering so we will try to answer questions over a very large collection of the documents and i so my friends try to quickly go over some of those state art methods in this area okay so let's just get started so first what is the question answering so the goal of question answering is to build systems that can automatically answer questions posed by humans in a natural language question answering all let's say qa in short is one of the earliest mlk tasks and the early systems can even date back to 1960s so here is one example of the early like one of the early um qa systems uh qa systems in back to 1964. so as you can see that this system is trying to answer a question like what do you want it and then finally return on the answer that's the graph so to do this so this system is basically trying to find some kind of text matching between the question and some kind of text segments and by using some kind of dependency analysis i assume that you have already learned the defense parsing in this class and there are many different types of the question answering problems and we can also we can categorize all these question answer problems based on the either information source or the titles of questions or the type of answers so for the information source we can build a system that can put like a condition on um a short passive test or a very large collection of documents or even like a structured database or structured knowledge base basis or even tables or images so for the question type we can also be assistants that can answer like factory questions or non-factory questions or open domain questions or close domain questions or simple questions versus like more complex or compositional questions and for the answer type it can also be like a short segment or text or a paragraph or document or list or even the yes or no questions so just have in mind there's many different types of question answering problems and all these problems may require very different techniques or different data or even different evaluation metrics to divide all these different problems and the question counselor has enabled a lot of the useful real world applications for example today if you just put your question in a search engine like google so for example you can put in your question like where is the deepest lake in the world so you can see that the current system basically found a like a short snippet of text including like lake um by carl barco in siberia calls the distinction of being both the deepest lake in the world and the largest fresh water lake blah blah and then it can actually pinpoint the correct answer which is actually a concise entrepreneur which should be siberia and those kind of systems are also able to handle like more complex questions like how to questions i guess this is probably a question that everyone currently cares about so the question is how can i protect myself from kobe 19. so there isn't really a simple and short answer to this question so you can see that the system actually returns a very long including the best way to prevent the illness is to avoid being exposed to this virus and to help prevent the spirit of kobe 19 you can do the following so actually the this paragraph is actually a summary uh from this cdcsco if you just click this link and read through the article so this is also one like one type of question answering problems and now this is a survey of the use cases for the current digital systems such as alexa or google home so according to this survey result in january 2020 which is one year ago so you can see that also people actually really like to ask questions um this is on this digital assistant so you can see the question is actually the second most used case only ranks after the listening to music and the before the checkered weather instead of time timer so questions have been really useful in this virtual assistants another very famous example of the question answering system is this ibm was from question 3 system so in 1920 or 2011 so this idea was because qa system has been shown to beat two national japanese champions in answering deathly questions so this is kind of this uh like a historical event at least in the lp history so if we look at the era working of this kind of system more closely so you can see that it is actually a very complicated and highly modularized system so if the system is built on both the unstructured text and also the structured data so by looking at the system if you go from the left to right you can see that this system consists of the four stages including the question processing the candidate answer generation and the candidate answer scoring and the confidence merging ranking and then if you look at each stage you can see that there are many different nlp techniques that have been actually included in this complex query system including a question classification parsing relation extraction correference so it's actually there are really lots of the lpcs modules that have been included and now this system has been over 10 years or actually exactly 10 years now and this is actually representing the space art like 10 years ago at that time so we know that this class is about deep learning so today differently has completely really transformed the landscape of the question answering systems so there's no doubt that we can say that almost all the states are personalizing systems today are built on top of the end-to-end training of the diploma networks and approach and language models such as bird so today in this lecture we are also going to learn a lot of these deep learning models in full question answering and this statement is probably also true for almost all the nlp problems that we can see today but we can also argue that question answering is probably one of those fields that we have seen the most recovered remarkable progress in the last couple of years driven by deployment so in this lecture uh i'm i'll be mostly focused on like uh focusing on the text based on textual uh crash answering problems so basically we are trying to answer questions based on the unstructured text so before i get started i jump to that part i also want to quickly point out that there are many other really big questions problems and each of them can be a really like a big star sub field in nlp and they actually have very different challenges and also model designs so one bigger class of this crash answer problem is this knowledge based question stream so basically we want to build question also in systems to answer questions that can answer also questions over a very large database so to solve this problem some approaches need to take this question and convert this question into some kind of larger forms and this kind of logic forms can be executed against this database to give you the final answer and then another class the bigger class of the presentation problem is called visual question answering so it's basically you need to answer questions based on the images so this problem this requires both understanding of the questions and also images and there is actually a very active field between the computer vision and our key so if you are interested with this type of problems i encourage you to check out these problems but i'm not going to dig into these problems today okay so next i'm going to start with the part 2 reading comprehension i just want to quickly check if there any quick questions i can answer before i get started um starts will catch you yeah no i think we could do now okay so yeah so let's talk about the reading comprehension then so reading commemoration is the basic problem that we want to compare a passive text and answer questions about the content so the input of this problem is basically a passive text a question and the goal is to return the answer that actually can answer this question so here's one example so let's tell so here is a passive text and we want to uh answer a question the question is what language your test will study while you score okay so i'm going to pause like 5 10 or 10 seconds and see if people can find the answer to this question based on this passage and you guys um okay what people say to german yeah germany so the answer should be german so basically to answer this question so you need to find this sentence like in 1861 tesla attended this school where he started german arithmetic and a religion and it's only the german the language so the answer to this question should be german okay here is another example again another passive text and the question is which linguistic minority larger hindi or mala um i think yeah five seconds okay so the answer to this question should be hindi so this probably is not a very hard question for humans it's actually a pretty hard question for machines because to get this question correctly so the machines basically to understand that for the hindi like three point three percent of the population speaks of hindi and only like one point twenty seven percent speaks uh mala yellen this language and then also compare these two numbers and the final case uh three percent three point three percent is a bigger number so the answer should be hindi to this question okay so next i'm going to talk a little bit so why do we care about this problem so why do we care about the reading comprehension problem so besides that it actually drives many useful real world practical applications also as i've already shown some examples at the beginning i think there are also two other key reasons so the first reason also besides adaptation the first reasons is so reading comprehension has been also viewed as a very important testbed for evaluating how well computer systems understand human language so this is really just similar to like um how we humans actually test the reading comprehension test to uh to evaluate how well we actually understand about language so this is also the way that we actually um pose questions to test the machines that would understand language understandings of ability so this actually has been formally stated in um back in 1977 by benny leonard in her dissertation so she didn't saying is she says that these questions can be devised to query any aspect of text comprehension so ability to answer questions is the strongest possible demonstration of understanding so that's why reading comprehension can be a very important testbed because we can't divide design very complex complex questions to test that and also i think there's another interesting and important reason that um reading comprehension is important so in the recent few years so many some researchers actually found that okay so for many other nlp tasks that we can also reduce them to a reading comprehension problem so i'm going to give you two examples so one example is really information extraction so basically if we want to um so given on the person like subject barack obama given a relation educated at so we want to fill in what is fill in this question mark and figure out okay where barack obama was advocating that so one one way to solve this problem is basically trying to convert this relation into a question so where did barack obama graduate from and taking a relevant piece of text and then by applying a reading comprehension problem then basically the we can find out the extract the correct answer should be columbia university that is also the output of this information extraction system another example is actually called cementite labeling i'm not sure if i've learned this in the past yet probably not but this is a task of the spanish labeling is trying to taking one sentence and trying to identify the rules for different verbs at least for words in this case in the in one sentence so basically trying to give them one sentence about given one verb finish trying to figure out like who did what you and when and where so by trying to um so it's going to try to figure out um all these like rules um with respect to the verbs so one way to solve this problem is by also by converting all these different roles into questions such as who finished something what did some someone finish and what did someone finish something else so by converting all these kind of like um semantic error relations well we can also apply just apply the ring comprehension problem and give you the correct answer so this is actually a very interesting perspective that reading comprehension can be actually very universally useful to many other tasks so next i'm going to introduce this like a stanford question string dataset cause god so if you are going to do the default final projects you will need to use this data set so cylindrical questions and datasets is actually a supervised reading comprehension dataset so which consists of 100k annotated passage and the answer question also triples so here is one example from this data set and uh i just want to um say that also one important thing to have in mind is that uh so this dataset has consists of 100k annotated examples and this kind of large-scale supervised dataset are also very key key ingredient for the training the effective neural models for reading comprehension so after the data set many other like later data sets have been also collected um basically runs this size around like 100k so 100k is actually very important um to transit neural models so for these data sets so the passages is like a single passage a single paragraph selected from the english wikipedia which usually consists of like 100 to 150 words and the questions are power sourced um basically electronic kind of perking and there is a very important property of this data set is that each answer is a short segment text or we could spend in the passage so as you can see from this example so here are three different questions and each of this um answer can be actually found as a short segment text in the passage so this is actually a pretty interesting property i know it's also important important property of this data set but also just to uh tavis that this is also a limitation because not all the questions can be uh answered use in this way so only the questions that that you can find the answer as a spell in the passage can actually be included in this data set basically but today so this data um yeah i forgot to say so this data set was collected in 2016 by several you know researchers at stanford so it's called stanford question three data sets um today like after four or five years now so school still remains the most popular reading comprehension dataset so it's actually you know very clean on the high quality day set but it's also not as very difficult dataset so today is basically the score data status set has been almost sold and the saves are already exists estimating the human performance and i also want to quickly mention the evaluation for this uh stanford credit counseling data set so there are basically two evaluation metrics to evaluate how well a system can do on this data set the two metrics uh uh let's see if match and a1 score so when you sign match is basically just a binary indicator zero one uh based on measures whether the answer can actually be exactly matched to the gold answer and the eighth line score basically measures kind of some partial credit enough to do the evaluation so basically for the development and testing set there will be like three gold answers collected because um for some questions there might be not just one unique answer so there could be multiple possible answers and the eventual matrix basically takes a pretty good answer and compares or compares the predicted answer to each code answer with some kind of like some articles and also the permutations included and obviously you can computer exact match score and also a fine score by comparing come back comparing the predicted answer to the gold answer and then finally take the next course and um because there are many different examples in the demo or test set and now finally we just take the average of all the examples for the poster exact match and the reference score so by using this evaluation method the estimated human performance is um by the researchers at that time uh estimated by the researchers at the time is the exact match score is 82.3 percent and the f1 score is 91.2 so here is just a quick example so here the question what did tesla do in december 1878 and there are three possible answers so if i see that the first two answers are the same left grass and the third answer is left breath and as uh serve i think that type of fear um all really relations with his family and then you feel in front of prediction is uh spent which is left breath and serve so you can see that the exact there isn't an exact match spot between the predicted answer and any of the answer so the exact match will be zero and the a5 score will be uh taking the max i'm not going to talk about how these are computed so i suggest you check out the original paper so by computing this course and taking the max and the the final is the f1 score will be 0.667 which is a f1 score for this creative answer on this data set so dante one question you might answer is so if you can do other tasks like named entity recognition or relation extraction by sticking something on top of bird as and fine-tuning for it or do it as question answering does one or the other method work better and by how much um that's an interesting question um so i haven't really seen that okay since there has been some claims that okay all the uh tasks can be converted into questions and tasks but i'm not sure there is a really like a very fair comparison let's say a young and anti-recognition and by really converting that into question answering tasks so i don't have to answer to that so the kind of state art and your system is still trying to just change or sequence tagger uh tagging on top of the bird so yeah i don't really have a preset something like that should i continue okay okay so next i'm going to talk about how to build a neural models for reading comprehension in particular how we can build a model to solve this standard questions in datasets called dataset i also want to just quickly mention that um because there are many different papers it actually uses like um different notions to refer the same thing so starting from um so i'm going to use the passage paragraphs in context and also question the query basically interchangeably so they are basically referred to the sentencing because different papers use also different notions so i just want to quickly mention that okay so how can we build a model to solve this problem so let's first formulate this problem so the input of this problem is uh let's take let's take a context or paragraph so c which consists of the n tokens c one to c n and also we take a question q and the question uh consists of m tokens q one to q m so um could be something like around 100 to um between 100 and 200 for spot and the m would be much shorter would be something like 10 or 15. and then because the answer has these constraints as the answer must be your second text in the passage so the output can be just uh written this way so we are going to predict the start and end so start and then end will be arranged um in the range between the one so it is basically just two check points oh sorry two end the point of the answer and um so scott has been collected back in late 2016. so after uh 2016 there have been like visit two families of the models neural models to solve this uh to solving this like stem score data set so the first family basically like uh there are a lot of models that come out during that period between 2016 and 2018. so this my family models figure i always can base models with attention so these are like just like a list of the representing models that come out during that period and including some work that i did when i was a phd student at stanford and now the second the second class models i put here is really there that divided here before the version after birth so after birth came out so almost those other this reading comprehension models were built on like how to find two of the bird models not just for model size for the birth life models so prediction language models and for these kind of reading comprehension problems so here are like two um the sound is uh the illustrations of these two families of the models so on the left is like iostm based models resultation and on the right is on the burton model um and then we need to fine-tune this model for the question for the reading comprehension task so i know that for the so my plan today is first try to talk about to talk about these ios 10 based models so i'm going to spend a little bit more time on this part because i know that um for the default final project you need to implement this model furthest from the scratch so i'm going to work through how to build this model like step by step and hopefully that you can have a good understanding of how this model works and then i'm just going to briefly talk about how to build this use the bird models for the ring comprehension okay so before i start talking about these lstm models i know that you have already learned this sequential sequence models without tension for machine translation so i was um so i want to draw some connections between the machine translation problem the reading comprehension problem because they really share a lot of similarities so first uh so in the machine translation model or this like sequence use sequence model there is a source and package sentence so basically us two sequences so but in our case in this reading comprehension case that we also have two sequences one is a passage and another is a question but the length could be a slightly imbalance because the passage will be much longer than the question but there are essentially also two sequences and then so in the reading comprehension we need to model like which words in the passage are most relevant to the question and then if they're irrelevant to the question so all is also relevant to which which set of the question words so this is basically uh very key important thing that the important thing that we actually need to model and this is actually very similar to the machine translation model that we need to model which words in the source sentence that actually are most relevant to the current packet word so you can imagine that the attention will be also really the key ingredient here is that just like some new sequence 26 model we need to model the attention between the source sentence and the target sentence we also need to model the attention between the passage and the question so this is actually very similar so something that's actually not very similar is for the sigma 6 model we need to build on like a decoder auto regressive decoder to generate the target sentence word by word but in this reading comprehension problem we we don't need to really generate anything so we just take the test and take the question so at least for the school data set we just need to change two class files to predict the start and end positions of the answer so that that part is actually simplified so when you don't need to change the decoder to generate the target sentence okay so next i'm going to talk about one um this model colored by def so it stands for bi-directional attention flow for machine comprehension so it was proposed by means seal and other folks in 2017 so it remains before the ice before the bird came out it remains one of the most popular reading comprehension models and achieved very good performance at that time at least on the small data set so you can see that this model like seems to be pretty complicated but if you um look uh look at this model from the bottom to the top it actually can be decomposed um into many different layers so the next i'm going to just dissect or dissect this model layer by layer and talk about okay what these layers are actually doing and how we can really build this model from the bottom layer to the top layer and then finally transition like model you know enter anyway okay so the first uh chart is actually the bottom three layers um called the character embedding layer word embedding layer and the phrase embedded layer so i just put them together because this is an encoding function so the idea here is that okay let's take the context query or the passage in question we need to encode them separately so to do this so this model basically proposed to use a concatenation of the word embedding as well as the character embedding for each word in the context and the query so for the word embedding is straightforward so if you have you have learned word embeddings so you can just look up the word for the this word like seattle just use the global embedding as a reputation for this word and the for the character embedding part so you basically need to represent each character in this world like seattle and a bypasses to a convolutional neural network with some kind of max pooling operations and then finally you can just get one reputation at the top and then you just concatenate its own uh word embedding and the calculating body so these kind of embeddings have been shown effectively to improve the replication for the unseen or the real words so mathematically mathematically you can see that for each word in the context of query you can just we can just represent this uh rotation as blocks of the embedding and the character embedding and then we just concatenate them and pass this to like all highway networks so i don't write the function here so um so you can just look at orange on paper and the second part um for other we call it the issue very visual word so on the next we are going to pass this like word embedding into two separate bi-directional lstms to separately to produce these contextualized embeddings for both the context and query so let's look at this equation so we take the uh reputation of this word and we just uh basically this is like a one lstm model uh from one direction and this all always can model from another direction so we just need to competent the two um the two header rotations two directions and finally we can get a contextualized rotation for for this for each single word in the context and now we can do the same similar thing for the question um reputation i also want to quickly mention because i mentioned the sequential sequence model the sequence just sigma although we cannot really do this bi-directional stems for the two sequences again like because the decoder is all auto-regressive model so that's why the decoder is usually just implemented as a unidirectional stem but because here we don't really care about the generation so we can just use two bi-directional scans to represent the rotations this is actually very important uh this bi-directional length is actually very important to capture the contacts from both the left and right side okay so the next component is the next layer it's called the attention flow layer so i just call it attention here so the change idea the idea of attention is trying to capture the interactions between the contacts and query and nothing in this paper by that favor they they propose two types of attention so the first type of tension we call the context require attention so the idea is for each context word can we find some most relevant words in the question from the question for the query for the query words so here's one example so here the context context of barack obama is the president of the usa so for each context word we need to find an alignment but define like which words in the question can be actually aligned with this context word so we can see that both barack obama can be aligned to who and the president is aligned to release and the usa alliance united states so busy for each content will try to find the most relevant query words another second type of tension is called query to context or tension so it's very another direction so here the idea is to choose some context words that are most relevant to one of the prior words because the context can be very long so a lot of the context could be just not relevant to the this question so we just run over around several examples you can see that the first thing we need to do is try to locate okay which parts of the sentences in this context can be actually relevant to this question so this type of the query to context or attention is trying to capture so which um which context words actually can be most relevant to to the query to one of the query words so for this example the question is which city is grooming in winter so you because the question is asked about blooming so you can find priority figures okay blue means it speeches is actually very relevant to this question and now we also find this in winter because in winter it also mentions the question so this particular um context was to be also relevant to this question so this um context words could be probably need to capture and uh in this attention that okay this is actually relevant to this question okay so this is actually basically just an intuition of these two types of tension and this is also why this model is called a bi-directional attention flow because there is a contrast required attention and there is also a library to context attention so let me just talk about go through how to actually um uh do this like curve to context attention the contextual periodic tension in this model so the way they do this is first to compute a similarities form for every pair of the contextualized vector ci and for every pair of the question with qj so this is actually the output from the encoding layer so this is already the output from the lstm layers and the wizard is basically just computer similarities form by taking the ci qj and also the element-wise multiplication of the c and qj so they basically just concatenate these three vectors so the output will be a sixth h dimensional vector and they just multiply this to um compute the dot product of another like a learnable vector and then finally just discard this basic can give you one scalar one number the sij which measures how um the similarity between this context where ci and also this question were qj so if you i have so if i learned some attention before so this is actually just one choice of this model so there could be many different ways to define this uh similarity and uh similarity scores so this is basically just one design choice of this model okay so after defining this margins for sij so the context to create attention again like which question words are most relevant to ci so the way to do this is so basically just taking this matrix the similarities for sij for each row we should basically correspond to like one context word for each row um so they are going to compute the soft max for each row and this can give us like um normalization force alpha ij which is our probability distribution over all the question words fij so this is just really similar to all the attention uh mechanisms that you probably have seen in this class so basically for each context we're taking the soft max um um over all the question words and get us um probability distribution and then finally just take the linear combination of the weighted combination of these are tensions for r i j and also the question vector the q j and then finally you can get a vector a i which is actually a 2h dimensional vector so this query context shows quite retention basically just try to capture which questions words are most relevant to each context word so the next five part is a paragraph um sorry the title here sorry uh this is actually the query to context or attention so which means that which context words are relevant to some question words so we don't so a lot of context words would be not relevant to this question so the idea to do this to do this is for each row of this sij this busy just takes a math scores over all the um question words and after taking this back score to compute the softmax over all the context words here so here i actually numerous over all the context words and this can give us like attention another attention score beta i which captures how important this context works is relevant to this question so after computing this beta i so we can again like compute this like a weighted combination by um computing by um somewhere by summing up the beta i and also the context context vector ci and then finally we can get our vector bi which is also another 2h dimensional vector and the final output of this attention function is actually very complicated here is also the design choice of this model so the takes a context measure ci and as it takes ai from this part of the context to query retention and that takes the element from multiplication between the c and ai and also the cnbi and finally take the concatenation and it can give you a produce or h dimensional vector okay maybe i want to pause a little bit and check if there are any questions because it's probably a little bit complicated one question is why is query to context and context of query attention not symmetrical um um that's a good question yes uh so here's essentially the goal you're trying to because if the goal is final goal is trying to find a span in the passage so so to hold the point of the this attention function you know trying to produce a rotation for each single context word in this context so that's um so so we are not trying to generate the question notifications here it's going to try to generate the um context orientations so one so the difference between these two like first let's try to see which questions words are relevant to this context word another part is trying to figure out which context word can be relevant and which context would can be not irrelevant i hope this sounds so answers to a question here's an easier question sort of on the same topic which might help is there a reason why you use both query to context and context to query attention is it sometimes advantageous or okay to use just one that's a good question um the reason is yeah so i'm going to show some obligations partly from this paper so they basically just find the both both directions can really help um by drawing the context curve context so there'll be some relation studies so by using one set is useful but just not as good as using the both directions um right let's see uh in the bottom right we sum over i so why does the i remain in bi is that correct or is there a typo there oh this is not a typo so again i'm sorry so the output yeah i know it will be confusing so also output of this model this com module is to get a reputation for each context word at the end so both the output for ai and bi i is actually um enumerate from like um actually you know if it's over all the context words so bi will be skills um just try to aggregate over all the questions on the over all the context words but the beta i measures the importance of this kind of test words compared to all the context words so both ai and the bi are actually with respect to the context words yes so you can see that here is basically doing some kind of the element wise multiplication so the output of gi will be actually arranged from from the one to an um which is the number of the context words there are lots of questions about this um what is the rationale for the expression for gi how how does one come up with such an expression okay i don't know i guess you'll also try out a lot of things okay so key point here is trying to understand okay so the rules of the contest require attention equality or context rotation so i bet that there could be many different formations to do this i also think the authors have tried many different but uh just what they kind of can come up as and i think that if after week um there could be some other ways to incorporate the both attention but it doesn't have to be written this way i mean one other question would be in the query the context attention why do you do a max inside the soft max yeah um yeah sorry i should have explained this more clearly so here again query to context or attention to try to measure well whether this is the importance of these context words with respect to some um some answer question words so if the so by taking the max for each row in this ace matrix so it's basically trying to see okay which question word um is actually most relevant to uh this context word if this number is still very low that means this there isn't any question words that could be aligned with this context word so that this will just if i take after taking the math if this number is still very low that means this question context vote is not very relevant so so basically just that's why we take the soft max our price of max on top of the max ah i know do you want even more do you want to go on uh i probably should well i don't have a lot of slides but i'm happy on the questions after afternoons yeah maybe you should go on yeah okay so the last part of this model is actually um um the either it's the normal stimulus so it's a two out of five lastly there are two layers smaller layer and output layers so for the modern layer so again for after the attention layer they take some heater rotation um so basic gi which captures the tension between your contacts and the query and then basically just uh passes gi to another two layers of bi-directional stems and the many reasons they do this is the attention layer is basically modeling the interactions between the query and context and now by passing this to another two layers of bi-directional lstms the modern layers is basic modeling they can also first model the interactions using the context words so this is a formulation here um so these are two layers bi-directional stm by taking the gis input and the output will be on the mi which is another 2h dash um dimensional vector for each context word in the in the passage okay so the final is also links so far other players this is just two class friends just trying to predict the start and end positions so by doing this so the first contact is the gi and mi so this would be actually a 10 h dimensional vector and by computing the dot product of another vector called w start and this resulting vector and they can get basically get a score for each position in the context and then you can just apply a soft max and then this will give you a probability that okay what is the probability this partition i will be actually uh based on um the start position of the final answer string and they also have something um another classifier to predict the end position of the answer but they also did something a little bit more complicated so they actually pass the mi to another bi-directional lstm here so they call it m i and they compati and m prime i oh sorry this is the title so this will be wn um so they compute the dot product between wn and this vector and this can give reproduce our own probability uh probability over all the positions which predicts uh um how likely this position will be the end position of the answer so by doing it by passing the mi to another by their character and the reason that they're trying to capture some kind of dependence between the choice of the start and end so you can imagine that start and should shouldn't be um too separate so it shouldn't be could be independent predicted but if they declare that if you incur some kind of dependence between the mi and um just keep starting and pn this can actually perform better okay i'm done with this part of describing the bypass model any quick questions i can answer i think you can actually go on okay okay sorry i forgot to mention this okay the final training loss will be just by taking these two probability distributions and that is basically just the next negative log likelihood of the gold as a gold answer just the start position of the gold answer and the position of um the answer and by um just us um basically taking the product of these two probabilities but you apply them uh apply a lot so this is the sum of the two negative log terms will be the final chain loss and the whole the whole model can be just changing our entry anyway from the encoding layer to attention layer to modern layer and to output layer so this will be just um completes the whole that the whole model of the by that model okay so this model is actually um achieved like uh on the data set it will achieve the 77.3 f1 score so as i mentioned earlier so um 2000 operations started they found the the pulsar attention in two directions are actually important if you usually move the one direction the performance will actually drop quite a bit if we remove the contacts to parallel tension the performance will drop to sixty seven point seven eight wave one score and if you uh remove this part it will drop to a four point eight point four and then also the character embeddings can also help so if you remove the character in balance you you'll get like a 1.9 uh point drop and on the right of this figure you have um this slide you can see a very big table so it's basically all the models that count out at that time between 2016 and 2018 so you can see that um by definition here so you'll achieve the 77.3 f1 score and the basic all the models are actually you know a very similar ballpark so numbers range from like um the highest number here 79.8 until like um after the elmo was introduced the numbers have been actually improved quite a bit so before the elbow basically all the numbers are actually kind of similar so uh each model actually improves our premise previous model by like a 1.02 points and now here is our attention visualization by uh to show that um how this like personalities for the tension actually can capture the similarity between the question words and context words so here example the question where did the super bowl 50 takes play uh place so the issue is actual question word here and each column is matrix basically indicates the attention score the semantic score that that has been learned by this model so you can see that and the on the right is basically trying to uh print out or display so the the context words that have the highest scores so you can see that the where it has been aligned very well with uh at the stadium leva and also the super bowl 50 is basically lined very well with super bowl 50. so this basically really tells that this kind of attention scores can actually capture those merit scores pretty well okay so next i'm going to talk about now how to use the burden model to solve this problem so i know that you have learned the first in the last lecture so i'm not going to repeat this so very quick so bert is basically a deep directional transformer including pre-trained on the large amount of text and in the channel the two training objectives including masculine modeling and the next sentence prediction and this model has a lot of parameters um so the bird base has like 110 million parameters and very large model has 330 million parents so okay so how we can actually use the bird for the uh for reading carbon enhancement so it's actually very easy and very straightforward the idea is to take the question as a segment a so you also in the frictions are two segments for the next sentence prediction task so when you apply the verb on the reading comprehension task you basically just take the question as a second a and take the passage as a segment b and finally the goal is trying to predict true end point in segment b so here's one more concrete example so question is how many parameters does bird large have so you can see that so they basically just um take the question here and then takes a passive here and by putting in the serious token and the acp token and by just contacting the question of passage tokens and also for the questions that you just need to pass the a to a segment embeddings and the for the passage you just need to cut out put in the um the second b embeddings and now finally the training loss is also the same so you basically just um try to maximize the probability of the sum of the natural log likelihood of both the start and positions but here's the way that the computer start and and um probability is slightly different so that's very straightforward so you just pass this um input notation into birth and the bird can give you the hidden vector h i that can actually represent the hidden vector that corresponding to the context word context word ci so you can just introduce another two vectors w star and wn by computing the dot product and then apply the softmax then you can just give you a very similar to what we had before but here is the high just output from the vertical encoder and then we are training on this tool that we start and w and to um for these two probability distribution p start vpn okay so for this model so all the vertical transfers that is actually very large number um if you use basically 110 million parameters as well as the newly introduced parameters h star and h and uh which is if you take the birth base so hidden side will be 768 so it's only like 1 500 new parameters so we just optimized together jointly for this training objective and it actually works really really well this model so if you just take this modbrate model and by just optimizing all the parameters together you can give you very high performance i will show you in a minute i know even the strong even if you use the stronger creation on your are more than like the standard um stronger models than the bird models they can even lead to better performance and on scope and the score that has also become a standard dataset for testing these kind of playtime models let me show you some numbers so here again human performance in 91 and by that is 77.3 and if we just uh do this functioning model so bird base can give you like 88.5 very large can give you 19.9 so you can see that this is a huge jump uh from the badass model to the building models and then finally if you see the even the latest um um pre-translated models include the exxon robot or albert so these models are either like a bigger or these moderate channels bigger covers or the model size are bigger so basically these models can give you give you another like three four point i i find score compared to the very large model so this is already way higher than the estimate f1 score so this just works really well any quick questions i think this may be okay okay so okay so yeah i guess i've been a little bit fast for this bird models but next what so i want to also do a bit of the comparisons between the biodiversity models and the bird models so burning model has many many more planters so it's like either like 110 more million or 330 million branches but that has only like 2.5 million parameters and the by death is built on top of several bi-directional ios teams and while bird is built on top of their transformers so transformer means that um there isn't any recurrence of structural architecture so the transformers are much easier to paralyze and a very key um difference between the bird models and by back models is bird model is a pre-trend but by that model is only built on top of the glove of vectors which is a pre-trend and all the remaining parameters new people learn from this called the data set or the other supervision data set so here it is very clearly that pre-training is a game changer here uh that appreciating basic can just change everything and also give you a very very large boost in terms of the performance but also i want to raise another question so if we don't think of this like um pre-training uh just like by that mother and birth models are really fundamentally different i don't think so because of glory is actually my argument so let's try to see how these two models actually connected especially in terms of model design so by that model essentially they're trying to model the interactions between a question the passage right so both the question to pass it in the passage of question and the first model essentially they're trying to use a self-retention on top of the concatenation of the question passage so this is a transformer model so you should take the question the passage so these are questions in the passage and then you apply many many different layers of the self-attention essentially that this self-attention is able to capture the attention between the context words the attention between the passage was on the question words and the attention from the question to the passive side and also the attention from between um from the question was to another other question words so compared to you by that by definitely trying to model this part but the burden model essentially can capture the tension between all these four parts and uh actually after by that kind of so um just also before the bird can um before the bird came out so he will have been also showing that if you just add a self-potential layer for the passive site so basically you're trying to explicitly model this attention between the passive words and passive words to the depth this account also improve the performance so you can see that um these two models essentially really just trying to model the tension between the passive impression and also the attention between the passwords and the passwords and this is actually what exactly the first model is doing okay um so if there's no further questions um so at this point i talked about um models can do really well on this kind of reading comprehension data set and always talk about pre-training can really change the performance can be again changing reading comprehension i guess don't you add one question first people wonder whether you can do well with a transformer that isn't pre-trained right if you try to build a question-answering system using a transformer rather than rsdns but no pre-training does that work that's a good question yeah it works but you you probably cannot really build a model as big as like 110 million premiums or 200 uh 330 million parameters models so actually there is a model between the um so between uh between the this like family of the ios tn models and burn models called qa net from google so qr net is actually built on top of the transformers with other pre-training so that model actually can perform better than the badass models and other models but they actually underperform the battery models quite a bit so just check it out for qa map okay i will just continue so okay so given free training has been so important so next i will quickly talk about okay question here is that can we actually even design better pre-training objectives for reading comprehension or question answering and the answer is actually yes so this is actually work i did with mendel josh and other folks like one year ago called spambered so think about this so um for the squad and other a lot of extractive uh reading comprehension data set the goal is trying to predict the answer spam from the passage uh as a question so the uh as a answer to this question so there are two key ideas being proposed in spamber the first idea is that instead of using only the masking of individual words we propose that we want to master contiguous extensive words um in the passage because the final answer would be just a segment of text in the passage so we are trying to um so mask out all these um possible answers then from the passage as a training objective and the second idea proposed with spamberg is that because at the end of view because we want to predict unsuspends so we're actually essentially trying to predict two end points um as an answer so the idea here is that can we try to compress the two endpoints of unsuspect um sorry can we try to compress all the information in this span into the two end points so here's the idea is that here let's think about this if we mask out the forwards here and can we try to use the two end points here in this um figure like x4 and x line to predict all the words in the middle so essentially we are trying to pre uh take the two end points and also the position some kind of position encoding and then finally we are going to try to predict all the words in this span so this is why this is called spam bird um so i encourage you to check out our paper and this actually really helps a lot uh at least for the question once again stuff so as you can see from this figure so this is called 1.1 and it's called 2.0 and these are many other question answering data sets as you can see here um so the blue bars here we call google versus actually the um the original checkpoints are released by google for researchers and our birth is actually just uh exactly our re-implementation of the first model but we are having trying to using the same data but we have been trying trying to train this model for slightly longer so it's actually achieved a better performance than the original group so as you can see that the yellow box here is actually standard so spammer can actually uh briefly outperform the blue version overboard um across all the data sets that really tells us that okay even if we don't we are not going to increase the model size we are not going to increase the data by designing better prediction objectives can also be go a long way and do do a much better job in at least in the question answering and reading comprehension data sets okay um so i have like several few slides left in this part so so so far i have to demonstrate that um by using by that model and by using burden models we can get a very good performance on the score data set the resistance number has already exist even the human performance on scope does this means that the reading comprehension is already solved the answer is of course not so let me um just so in the recent last couple of years there's been a lot of evidence showing that the current system still performed poorly on anniversary examples or the examples from the out of domain distributions so here is a very classical example so proposed by robin channel president in 2017. so the idea is that they take a pass passage and take a take a question and they're trying to just insert like a random sentence uh to the end of the paragraph as you can see that this sentence has like even like a nonsense entity in this context drafting here but this sentence actually has like a great uh some left score overlap between the questions it's actually very similar to this question but actually the word numbers have been changed the antennas has been changed and then they found that these kind of adverse examples can actually very easy to fool the current systems and then the final pro and the mix assistant to predict answer to the drafting so the by um here's the table shows that by adding a lot of these adversary examples they found that the performance actually draws a lot of just by that model so it drops from 75.5 to even like 30 percent so for even like this kind of attack the performance will just drop to very low like 4.8 so here's another paper that actually just came out in 2020 so it has me a lot of the evidence showing the similar things that so today we can build a very good reading comprehension data set on individual data on the individual data sets but this season channel one dataset physic can already generalize to other data sets so the diagonal is basically the of this table is basically channel one mod to model one data set and evaluate on the same data set and for all the other numbers in this table but it shows that if you try and run some system on one data set and then you value it on another data set the performance will drop quite a lot so it's basically really kind of generalized from one data set to another data set so finally this is actually a very interesting um result so this small this paper is actually the best paper from acl 2020 is called checklist paper so the idea is that this this authors basically try to propose some kind of test cases to check whether these models can actually really unders um answer some simple questions rather with some specific or particular phenomena they find that by just come up with some really simple questions for example here jeremy is more optimistic than taylor and who is more pessimistic and that's uh they found that a birth large model channel stop and this is can still fill this type of test cases 100 percent of time and now here is another table so you can see that here is another clever example like victoria and alex are friends come on is our agent who's my agent and um so to get this kind of question correctly so it has to understand the pictorial actually refers to a female person and the alice refers to a male person so this model this kind of questions also make some kind of models very large multiple channels for the totally film of this kind of test cases okay um so i have 10 minutes left chris is any question actually answer at this point i think you can go on okay so in the last 10 minutes i'm going to give you a very very brief introduction of what it opened on my question answering and what we are have been trying to do in the last couple years so although the main question answering the problem that um so it's different from the reading comprehension that we don't assume a given passage so here with the assumption that we only have access to a large collection of documents so one example is just taking the whole english wikipedia which has like five million articles so we don't really know where the answer is located and the goal is to return the answer for any open domain questions so this problem so there isn't any single passage so we have to answer questions against a very large collection document or even the whole web documents so this is actually a much more challenging and also more practical problem um so if you look think about the example of google example i showed at the beginning so this will be techniques that will be very useful in the practical applications so the term here open domain is just in contrast to close domains that deal with questions under specific difference under the specific domain yeah okay so how can you solve this type of problem because for the reading comprehension problem we just need to answer questions based on single passage so this is a paper that i wrote in 2017 four years now so it's called the paper it's called reading wikipedia pdf to answer all questions and system called doctor qa so the favorite basically proposed the idea that we can actually solve this problem by using like a retrieval and also a reader framework so idea is that let's take a question okay so here our goal is to try to answer questions uh using like a very large collection of documents such as the wikipedia so the idea is that there's a retrieval and also reader component so the retrieval takes in the question and i try to find out like the smaller number of our documents have to be relevant to this question and this reading model basically trying to read through all the documents that just retrieval return and if i try to find out the correct answer um so formally defined here is that the input is a large collection of documents d and the question q and the output could be our answer string a so we can just decompose this problem into as i just mentioned in the retrieval and reader component so the ritual is basically trying to take the large collection document d and q and try to return a set of documents or set of passages so here the this number k could be very small um could be very small um such as like um well just like a 100 so he's basically trying to pull out a found out like 100 passages or documents um from like of let's say five million documents and finally the reader is basically takes a question and takes this set of the passages and finally found um finally return the answer so the second problem is actually the reading comprehension model that we just learned so lean um just search some 17 paper results so it's actually doing a very same simple thing so the retrieval is just a standard uh information trigger model is a sparse uh pfidf information through a sparse model and the real model is essentially just a neural reading complement comprehension model i just talked about so it's very trend on scott and some other questions three data sets um so this is really the idea is very simple but you're trying to bridge the two things how to how to have bridges this retrieval and also the reader to do this kind of open domain question answering so so i'm just going to quickly go over um some really exciting ideas that um that has been happening um uh in the last two years basically so the first idea is that this retrieval part can be also trend so we can't actually even do this kind of join the training of the retriever on the reader so here is actually so this this idea has been first proposed in canton league's paper in 2019 called legendary retrieval for weekly supervised open number questions so this part is basically the first uh model for reading comprehension and system apart is basically the retrieval model so to get this ritual among working they also try to use the birth to encode the passage and also you called the question and they try to use the dot product between the question notation and passive representation to model how how the relevance so the similarity between the question the passage but this is actually very difficult problem because the scalar scalability of this problem because there are like 20 million passages in the wikipedia so it's actually very hard to model this part but so i encourage you to check out this paper and now also on second table i want to quickly mention it there's also work i did uh last year um it's called the best passive view tool so the idea is actually very similar to the previous paper but the idea is that it's actually much more simplified model and very easy very simple straightforward approach the idea is that we can also really just trend the retrieval part by using two bird models using only the question answer pairs and this motorcycle works really well and then kind of largely all for the traditional ir retrieval models if you see this figure so the blue curve here is a traditional ir approach um like a bm25 approach and uh since the other curve the orange curve is basically training this kind of retrieval using only one thousand question answer pairs so by looking looking at all these different curves basically using different number of training examples so it's actually kind of largely briefly out from the traditional ir models um okay so again really i don't have time to talk about the details of all these approaches so i just encourage you to check out this paper uh papers and i think this result is really exciting um so here is actually a really nice demo so you know the demo is actually um hosted at this website before you check out so again so the database here is the whole wikipedia you can see that if you ask a question who tells harry potter that he's a wizard and the higher in the harry potter series and the system has really found out the correct article should be harry potter film series and then finally give you the current answer which is exactly what you have seen from the google example here so it has the answer it could be the rubrics calgary which is actually the person who tells harry potter that he's a wizard so this is exactly the perfect answer to this question okay i'm going to escape this slide and then finally um very quick so so this is something that came out uh various uh recently that some researchers have demonstrated that maybe you don't even need this retrieval study so you can if you just use a very large language model you can also just do the open domain question answering uh so the way they did this is that i hope that you have learned the t5 model in this class already so they just take a preaching language model g5 and they are trying to find through this model by taking the question and taking the question of the input as an answer as of without any explicit retrieval and they just fine tune this on the data set and they find this model can do pretty well at the testing time by just taking the question and then directly generates answers without the resorting to any like documents or like a retrieval system so this is actually very amazing so this kind of model is also called called crossbow qa systems okay very uh the last life so so just one direction and personally i'm very excited about so this is actually a new direction that basically shows that maybe for the online domain question answering maybe this real model is also not necessary anymore so so this idea was first proposed by museum in 2019 and we recently wrote a paper called dance phrases just to try to demonstrate that maybe you don't even need this like a reader model so instead we can just encode all the phrases in wikipedia using some kind of dance vectors so what you just need to do is just to do this kind of nearest neighbor search in the answer space you just encode all encode all the phrases in wikipedia encodes and using vectors and by taking a question you can just encode this question using a vector and then we can just do the vector the nearest neighbor search and then you can directly give you the answer so this is a new paragraph of this kind of the question answering model so you don't need them you just need a retrieval you don't need a reader so a great advantage for doing this is that so for the perfect reader model essentially you have to run a better model at the inference time this is actually very expensive but you can just get rid of this model you can just do the similarity search you can just do the nearest new research without the running of bird model so this could be very fast and can even run on the cpus without like leading to like a very expensive deep neural network and you can still run very well perform very very well okay finally i hope this works so i actually prepared uh demo for this transferences so i want to show you how this actually works so you can see that after you type this question who won the not nobel prize in peace in 2014 so every time just time for like a little piece of the input question and then the system can basically just find out the relevant um answer in the relevant text passages and then finally on the answer it actually shows up it's actually very fast because it's a bit of real time we don't we don't remember on the birth model so it's just a reassurance here okay i'm actually done this is lecture so that's 5 15 now yeah thanks for joining me today thank you very much dante for that awesome survey of um question answering i guess given that demo at the end people want to know whether you're launching your own search engine soon um but um um at any rate um don chi can stay for a bit to answer questions but not forever but um today um because of you know she doesn't have a stanford login we're going to do questions inside zoom so if you'd like to ask a question if you use the raise hand button we can promote you so that you appear in the regular zoom window and can just ask questions and see each other and if you hang around and don't leave the zoom for more than a few minutes maybe we'll just promote everybody who's still there into um people in the regular zoom um for some bits of discussion but um we'd welcome anyone who'd like to ask a question by asking it themselves at this point okay i've got one volunteer i've got more volunteers should i meet some questions oh should i look at the chat or oh no i mean so there are now four people who've been promoted um there are four people was the first so maybe um he could start by asking a question and then the other people that we've um promoted okay uh so thank you so much for the lecture today uh my question is mainly like if you use like a model like uh for example how uh small can your training um data set be for you to get like reasonable results so the question is how we can try the comprehension and model using only a small number of returning examples yeah i think this is a really good question um especially like the you probably have heard the gpt stream model that you've shown that if you only use a few uh very few examples you can also do the open domain question answering pretty well so i but this kind of model is huge like um what's the number like um how many parameters i forgot in the gpu stream model yeah so it's a very large very cute model but by okay so basically my answer is that if we can leverage a very large and very powerful prediction language model there is a way that we um there is a possibility that we can actually do the question string with um well with only a small number examples and now also there are some other promising directions including like unsupervised questions three so by using some kind of approach like the from the machine uh unsupervised machine translation this kind of idea that can be borrowed and the by um yeah comparison ideas from that um can also work pretty well reasonable reasonably well in unsupervised questions um yeah also i have seen some a lot of works like very busy showing that uh uh synthetic qrdss can also help a lot um in boosting the performance if you don't have enough uh supervised datasets i also have nice examples yeah so my question is it's i guess it's kind of interesting that there's not really that strong of a transfer effect um between data sets that are kind of ostensibly similar so my question is like has there been any research done on how close i guess like the formatting and the uh semantic content of these question answering data sets actually adheres to the data that like burt is pre-trained on um and if so like has there been sort of any effect found between those similarities or differences i it's a question asking like whether there has been like a song okay maybe i can just try to clarify a little bit why the current models can already generalize well from one data set to another days that we've um yeah so i actually really believe that uh most existing question answering dataset or reading comprehension dataset have been collected um from mechanic perk so it's very hard it's very difficult to avoid some kind of artifact or like a simple clues or super visual code that easier let's say not superficial but some simple clues that for the machines to pick up so for let's take those folders examples that so you can see that actually if you look at the data set more closely there has been a lot of examples as a question having like a large overlap in terms of words between the question the passage so the model is actually very good at picking up these kind of clues to get a very high performance on this data set and another data set is called um drop so it's basically about the comparison the two numbers something like that so that's the reason that uh one specialized model that has been checked very well in a one-on-one data set it's very easy to pick up this uh this kind of course and it's very hard to generalize this kind of thing to another deficit what about the natural questions data set doesn't that avoid that objection yeah natural questions will be uh much better but there are some other issues i'm not sure if you have seen that uh there's a recent paper called like a question uh trend test overlap paper so that means a demonstration i used to interrupt so natural questions was a data set that google put out about a year and a half ago maybe um where they were actually taking real questions from google search logs and then finding answers trying to find answers for them in web documents sorry go on dutchy oh i just don't think yeah i think it's definitely natural questions is much better than i said because the questions are natural like you're collected uh i really like real questions that are asking by like the users so it kind of avoids this kind of superficial artifact between the question and passage but there is some other issues that um people like to ask some common questions so if you just do the random spate of the questions into trend diving test and there's a recent paper video showing that there is actually a you know busy model is inevitable that there is a higher overlap between the training staff so if you find this question if one question that you're trying to test uh um in the desktop or the test set that household already appears in the change that that is not really generalization right yeah but this is the more on the like open domain setting not in the reading comprehension section yeah yeah um do you wanna ask a question yes so you mentioned that uh in the last part of the presentation is that the reader model is may not be necessary and you presented the dance phrases which can also work well on cpus so do we know how um how well it performs on the question answering data sets and compared to other compared to other models including birds and sources on gpu of course yeah i just encourage you to check out this paper so this this model is basically performs on par with like the dance pattern retrieval paper uh retrieval model so it's uh either performs on part of these are all the ritual reader models but it is actually right now though so i skipped one slide so so right now the saved art is actually uh dominating by this kind of uh dense passive retrieval positive generating model so this kind of so using a t5 model plus a that's a retrieval this is actually performed really well so i will just say so this can work this is similar um in this block but compared to this kind of generating model we still like a few points behind yeah okay and uh what is the kind of the uh intuition behind the dance phrases apart from like the answers are probably um in close proximity and what if the data says has answers and quest has answers to a specific question like very far uh from the actual information like see the answers to the question may not be may not reside in close proximity to the um to the words in the question so let me just clarify this um okay the goal of this project is trying to um index all the phrases in the wikipedia so i know by under this kind of rotations are built using the training set of the question answering data sets so the assumption is still the distribution of the examples in the different tests that will be similar to the training set for sure does this answer your question like so basically we're still trying to consider all the phrases in wikipedia and that test now we just take the questions and try to complete the dot product okay so if we use say a different dataset that does not present the information using a structure presented in wikipedia then um this model may not work as well as what what do you mean by structure then so uh say if we um lean more towards clearly more towards uh structures like the passages we see in standardized tests where the answers to the question may not be like um may not be in close proximity to where the the information was first introduced oh no so the the answers doesn't have to be seen in the training step so the business are going to taking the training set channel encoder for the phrases and by using and then we apply this encoder to all the free all the places all the a lot of like six meter phrases in this video so so the model is definitely able to generalize from the um training set to all the phrases to use media so it doesn't have to be seeing the things that is this what are you asking i see okay this is actually very so it's actually similar to like the retrieval or the best passive retrieval so you still like um yeah i tried to channel a passive rotation here in the first station but so the rotation is only a trend using the training set of the question answering data sets but um by taking the encoder and then we are going to encode uh all the rotations all the passages of phrases in wikipedia and then we can um expect that this rotation can actually generalize well for the unseen questions yeah so uh so that so the question is um what if the nearest neighbor so she doesn't return the answer so why do you think the nearest neighbor i mean you always can find something right it's just the question is that whether it calls him up or not yes so the sort of question is what do you think the data says that the answer is not close enough then um yeah that's good question i don't know uh if you really come up with something that is really very far away from all the questions that we have been uh seeing in the training center that could be possible i don't know basically depend on uh how the texts are um formatted then uh the nearest neighbor search may not uh work as well as other models so again the question is also the question invitation is also returned by a question code so so the question is whether this question coder can give you something reasonable in that space or not but i don't know yeah so we have been testing a lot of like a random even the imported sentences or even the question doesn't have to be a real question it could be a sentence it doesn't seem to be our problem so far yeah maybe maybe we should give a couple of other people a go and you're allowed to turn your camera on or ask a question if you want um so our next person all right hi uh thank you for taking the time to teach us um my my question is kind of quick so you mentioned work that brought up a set of relatively simple questions that show how brittle or poor the current models can be right i'm curious if that's true yeah yeah exactly exactly did that trigger a big change in the community to improve how to evaluate the models because they're actually doing pretty poorly on some of those right yeah so first these questions are simple in uh in terms of the the wording is very simple the template is very simple but they're still trying to test like a negation or temporal relational preference so the questions are not i mean in terms of the reasoning of the capacity it's not that simple just the wording very simple um i do think um okay so this paper definitely received a lot of attention in the best paper last year at acl the biggest country in the conference um so i think a lot of people are trying to solve the problem i cannot tell you that okay whether we really have a solution to this yeah oh no yeah cool yeah thank you thank you for bringing this one up it's really interesting okay next is hi thanks for taking this time so my question is kind of not relevant but like to build a robust system of question answering in what extent can in-context learning help models to be more robust with respect to different domains oh so like uh basically you provide a template generated by bird and then instead of directly predicting the clauses of text classifications you just use some word to represent that clause okay so i assume that you are actually referred to the in-context learning industry industry stuff like that okay um actually i've been doing something related to dynamic recently uh so but i'm not sure how we can actually use that in context learning in any scene for school type you know problems um yeah so i don't know if that could be robustness or not i even thought you used that technique for the questions three years i see i see thanks and i used to also mention that we can train a retriever without a reader so is there a paper about the current like attempt to do that yeah so so favorite already off so just uh yeah okay okay next hey how's it is uh thanks so much for the uh for the lecture um i'm gonna have a broader question um about the future of nlp um do you think that in order to solve nlp in a sense that you can perform on par with humans on all nlp tasks it's sufficient to only interact with text data whatever do you think will eventually need some sort of sort of experience and common sense that you get um only from seeing and sort of feeling the world and having the sorts of interactions that we as humans have yeah i mean common senses have been very difficult even in the context of constraining common sense is a very yeah now very very important topics that uh still remains i think it still remains on the result uh that's for that part the honest exactly yes i also want to mention that okay so for a lot of the reading comprehension databases or questions that you're seeing that we are pure with people start starting to achieve the human performance but this but we also see that how great of these systems are because yeah i mean they cannot regenerate or solve the easy problems so all these things need to be resolved um [Music] [Music] trying to have a human in the loop um a framework to evaluate these kind of systems just try to uh break the current system come back with some harder questions so um yeah so that means that maybe the kind of static businesses are not good enough to measure the progress so we actually really need some kind of dynamic evaluation and also introduce all more this kind of adverse examples or the um yeah harder questions or something like that are you still going for a couple more questions uh sure i don't want to make sure it's nine or 9 10 p.m uh yeah on the east coast yeah okay um so next is nexus in nerfs 2020 there was this efficient open domain question answering challenge um and from you know from performance it seemed like there was like quite substantial uh decrease versus human accuracy um probably like primarily the quantization and um also some drift that occurred when they were quantizing um so i i recently encountered this paper called uh learnable quantizers uh which essentially learns uh learns like basis representations for the quantizers like jointly with the weights of the network um and while this would be like extremely effective if you were to just like say trains from scratch i was just very curious do you think that like such a there are some way [Music] [Music] um yeah i don't think i i'm really expert to answer that question um yeah i'm not sure if i really have the answer but i also want to quickly mention that yeah quantization has been very very useful technique to make the model smaller right so we have been also exploring the contaction in in the dance phrases project recently because of the storage has been still very uh has been still very large so we are having trying to reduce that story um yeah i'm not sure about the is there a question about the connection between composition and also training uh yeah i'm not sure i'm i've also do that yeah sorry yeah thank you thanks the dante was too modest to mention that she was one of the co-organizers of the efficient qa um shared task um okay next question is hi guys you thanks so much for being here today um so my question is uh a bit different um so one example you gave that call my attention was this alex victoria example the checklist um and i was thinking technically alex was in the wrong answer right it's gender neutral and there wasn't enough context context in the question to determine who it's referring to so my question is how concerned should we be about potentially encoding uh certain biases into these circle labels or how we evaluate them or is that just more of a concern for more open-ended questions um yeah this is definitely very important again like a lot of a lot of people are trying to study okay how much buyers have been coping with models and how we can yeah um yeah i'm not sure if i have a good answer to that um again like i just i want to say like how to do the biasing of the creation language models all these things are very important and um um yeah this is just one so you're talking about this example right so this is just one test case um yeah um yeah i i don't know yeah right yes i guess i'm just a little more about you know who comes up with the test cases right who determines the other yes there is thank you i mean we will we will have more discussion of toxicity in bias coming up very soon including actually thursday's lecture as well as the later lecture um not specifically about qa though um okay next person is right thank you for the lecture um yeah my question is also related to the open domain uh classroom history so um um i was just wondering how much of like uh the uh learning side of um uh domain like sort of generalization um or like domain alignments um techniques can be uh combined with like the language um level like questioning string like to what extent would they work and um like what kind of like language specific design should be leveraged to combine with those to sort of like um if we want like higher performance and stuff like that there's a question about how to generalize between different domains or uh let's talk about how to design uh open domain qa system for different languages i'm not sure so there's like uh some some uh like learning specific designs like um um uh domination alignments and like a future level disentanglement techniques uh that have been that has shown some like uh like uh interesting performance on um other tasks um and i saw that like recently some people also like leverage similar things um like for for for question answering so i was just wondering um like to what extent uh these kind of techniques um come work on um like uh language tasks not just limited question answering but like um mainly um question answering sorry what which which work are you talking about i'm not still not sure what do you mean by this entire conversations or questions right so uh so basically um i mean this is like a little bit more specific for so um so there's this um paper called um uh doing um i forgot the exact name uh okay so i have okay i just want to make sure that we are on the same page so i have things that work instead of trying to learn some kind of in disintegration so i can better generalize to the different domains or adverse examples is this what you're saying yeah yeah and the question is that this technique can be a general general generally applied to personal strength oh um yeah they're just wondering how um to what extent would they work because um i think language has like a lot of like specific things like dependency and other stuff that like these techniques does not like actually um take care of so i was just uh yeah um yeah i'm not sure uh i think we have to try that but that's a interesting point yeah i don't know at least for the work that i've seen so far all are applied or operated at a very simple uh sentence classification task maybe anonymous uh maybe seven or correct so my understanding is as a basic take a level encoder applied to a simple test classification task and take a hidden rotation do some kind of or transformation and make sure that um yeah it can learn some kind of environmental features from the hidden reputation something like that right yeah cool yeah i'm not sure i feel like qa is a more structured task and also handles longer uh sequences um yeah so i don't know if it works uh unless people have tried that yeah thank you thank you okay and then we've got and maybe we should call this the last question hi um i'm just wondering what is like the intuitive difference between solving question answering with uh guaranteed models like t5 versus encoders like bird okay um that's the point oh okay so i'm so i skipped this slide so why this model works so well uh the reason is actually it's not really about the extracting model versus generating model the reason is that they actually um for the extracting model so if the retrieval returns let's say 100 passages so they have to extract the answer from each of the passages and then finally figure out which one is the max what has the highest score but for the generating model essentially they're trying to aggregate all the 100 passages and their reputations together and do their generation um jointly do you understand so essentially taking the 100 stations together to the joint generation instead of only do the extraction from each of the passages so i think that's actually the key difference so that's why this generating model can do really well compared to the extracting models so i also want to mention that okay so if you look at this ig model it's actually um like a comparison dpi and rhg model so ig model is always doing the generative model but they are not doing this kind of aggregation they're just trying to take out a single passage and doing the generation so the rng model actually doesn't perform as well as this model by the way i also want to mention rng model is actually not doing better than dpr because this base model is large model so this these numbers are a little bit confusing so they're actually basically really on par they're basically performed similarly but um so the the key difference between the generating model and the extracting model is that for general models you can actually leverage more com on input passage together and do the generation does this that is that clear or not um yes yeah thanks yeah otherwise you should just check out this paper here so this paper actually started pretty well then why this model can work better than the previous generating models [Music] models to [Music] oh encoders is like you're finding similarities between their encodings uh and then generate models are you like remembering the whole question and you try to retrieve that mainly like when you answer the question okay so for this model there isn't any retrieval so you cannot really find the answer from a question right so this model really has to rely on all the parameters to memorize all the information so by just taking this input it has to just rely on the branches to infer this answer so it's actually very hard too so yeah it's definitely balanced between memory and a generalization i see so um i'm just gonna uh say what i like is it like when you give got this question is embedding it in some space and then using that embedding the generator matches that to like 1882 is that what is going on in there yes the model has like it's very large like a 11 billion parameters so all these the parameters are basically trying to uh yeah memorize a lot of information that has been because the model has been pretty from the text and also has been fine-tuned so the model has been trying to memorize all the information from the text yes um do you want to call it an idol do you want one more question uh either way yeah it's up to you i'm sure i'm happy to take one more question okay let me just do okay one more question from okay uh the first question is about how how easily are these techniques generalized to other languages i like to say languages that are quite different quite different grammatical rules like chinese japanese or arabic or some other languages sort of another question maybe not exactly your domain expertise is there's a lot of interest in modeling user behavior say unlike searching behavior browsing behavior as a sequence using say transformers self-attention um and then you can use that to predict how user can like embed the user as a vector and can predict user's actions how promising do you think that would be i know this may not be your domain expertise but there is a lot of interest in extending these uh plus answering techniques are just encoding techniques embedding techniques to recommend our systems um just want to get your thoughts on okay um the first question is whether these techniques can be generalized to other languages i think it's honestly uh yes and there has been a lot of active research in this fashion but there has been also some constraints that um as a lot of models or systems i described here actually require a lot of them require very strong like a pre-trained language model and also requires a lot of training samples for the pure idss so that would be actually um i would say a bottleneck for many low resource languages right so so it's very hard to collect so many examples for other languages if we have actually i think the techniques can check can be generalized generally applied to other languages as and there has been also a lot of work trying to do like cross-lingual questions through stuff like that 
","['', 'question answering (QA)', 'types of question answering problems', 'information source for question answering', 'question types', 'answer types', 'real-world applications of question answering', 'early question answering system', 'IBM Watson question answering system', 'question answering using neural models', 'transformer models for question answering', 'passage reading comprehension', 'Stanford Question Answering Dataset (SQuAD)', 'attention models', 'answer span prediction', 'extractive vs. generative question answering models', 'BERT model', 'dense passage retrieval methods', 'open domain question answering', 'challenges of question answering in low-resource languages', '']"
"okay hi everyone so we'll get started again we're now into um week seven of cs224 n um if you're following along the syllabus really closely we actually did a little bit of a rearrangement in classes and so today it's me and i'm going to talk about co-reference resolution which is another chance we get to take a deeper dive into a more linguistic topic they will also show you a couple of new things for deep learning models at the same time and then the lecture that had previously been scheduled at this point which was going to be john on explanation in neural models is being shifted later down into week nine i think it is um but you'll still get him later um before getting underway just a couple of announcements on things well first of all congratulations on surviving assignment 5 i hope i know it was a bit of a challenge for some of you but i hope it was a rewarding state of the art learning experience on the latest in neural nets and at any rate you know this was a brand new assignment that we use for the first time this year so we'll really appreciate later on when we do the second survey getting your feedback on it we've been busy reading people's final project proposals thanks lots of interesting stuff there our goal is to get them back to you tomorrow um but you know as soon as you've had a good night's sleep after assignment five now is also a great time to get started working on your final projects because there's just not that much time till the end of quarter and i particularly want to encourage all of you to chat to your mentor regularly go and visit office hours and keep in touch get advice just talking through things is a good way to keep you on track we also plan to be getting back assignment for grades later this week there's sort of the work never stops at this point so the next thing for the final project is the final project milestone um so that we handed out the details of that last friday and it's due a week from today um so the idea of this final project milestone is really to help keep you on track and keep things moving towards having a successful final project so our hope is that sort of most of what you write for the final project milestone is material you can also include in your final project except for a few paragraphs of here's exactly where i'm up to now um so the overall hope is that doing this in two parts and having a milestone before the final thing it's just making you make progress and be on track for having a successful final project um finally um the next class um on thursday is going to be colin raphael this is going to be super exciting so he's going to be talking more about the very latest in large pre-trained language models both what some of their successes are and also what some of the disconcerting not quite so good aspects of those models are so that should be a really good um interesting lecture when we had him come and talk to our nlp seminar we had several hundred people come along for that and so for this talk again we're asking that you write a reaction paragraph following the same instructions as last time um about what's in this lecture and someone someone asked in the questions well what about last thursdays the answer that is no so the distinction here is we're only doing the reaction paragraphs for outside um guest speakers and although it was great to have untron boss lou for last thursday's lecture he's a postdoc at stanford um so we don't count him as an outside guest speaker and so nothing needs to be done for that one so there are um three classes for which you need to do it um so there was the one before from dungeon um colin raphael which is thursday and then towards the end of the course um there's euless vipkov okay so this is the plan today so in the first part of it i'm actually going to spend a bit of time talking about what co-reference is what different kinds of reference and language are and then i'm going to move on and talk about some of the kind of methods that people have used for solving co-reference resolution now there's um one bug in our course design which was a lot of years we've had a whole lecture on doing convolutional neural nets for language applications and that slight bug appeared um the other day when um danchi talked about the baidaf model because she sort of slipped in oh there's a character cnn um representation of words and we hadn't actually covered that and so that was a slight oopsie i mean actually for applications in co-reference as well people commonly make use of character level confnets so i wanted to sort of spend a few minutes sort of doing basics of confidence for language the sort of reality here is that um given that there's no exam week this year to give people more time for final projects we sort of shorten the content by a week this year and so you're getting a little bit less of that content then going on from there um say some stuff about a state of the art new york co-reference system and right at the end talk about um how co-ref is evaluated and what some of the results are yeah so first of all uh what is this co-reference resolution term that i've been talking about a lot so co-reference resolution is meaning to mention to find all the mentions in a piece of text that refer to the same entity and sorry that's a typo it should be in the world not in the word um so let's make this concrete so um here's part of a short story by shruti rao called the star now i have to make a confession here um because this um is an nlp class um not a literature class i crudely made some cuts to the story to be able to have relevant parts um appear on my slide in a decent sized font for illustrating co-reference so it's not quite the full original text but um it basically um is a piece of this story right so what we're doing in co-reference resolution is we're working out what people are mentioned okay so here's a mention of a person vanaga and here's a mention of another person akilah and well mentions don't have to be people right so the local park that's also a mention and then here's akila again and akilah's son and then there's prajwal um then there's another son here and then her son and akash and they both went to the same school um and then there's a pre-school play and there's prajwal again um and then there's um a naughty child lord krishna and there's some that are a bit complicated like the lead role is that a mention it's sort of more of a functional specification of something in the play um there's akash and it's a tree um i won't go through the whole thing yet but i mean in general there are noun phrases that are mentioning things in the world and so then what we want to do for co-reference resolution is work out which of these mentions are talking about the same real world entity so um if we start off um so there's banaja um and so vanadja is the same person as her there and then we can read through um she resigned herself so that's both bernardjah um she bought him a brown t-shirt and brown trousers then oops then she made a large cut out tree um she attached right so all of that's about the narja but then we can have another person so here's a killer and here's a killer um and maybe those are the only mentions of aquila so then we can go on from there um okay and so then there's prajwal but note that prajwal note that prajwal is also aquila's son so really aquila's son is also prajwal and so an interesting thing here is that you can get nested syntactic structure so that we have um these sort of noun phrases so that you know overall we have sort of this noun phrase akila's son prajwal which consists of two noun phrases in opposition he is prajwal and then for the noun phrase akilah's son it sort of breaks down to itself having an extra possessive noun phrase in it and then a noun so that you have achilles and then this is sun so that you have these multiple noun phrases and so that you can then be sort of having different parts of this um be one person in the co-reference but this noun phrase here referring to a different person in the co-reference okay so back to um prajwal um right so well there's there's some easy other projects right there's protrol here um um and then you've got some more complicated things so one of the complicated cases here is that we have they went to the same school so that they there is what gets referred to as split antecedents because the they refers to both prajwal and akash and that's an interesting phenomenon that and so i could try and show that somehow i could put um some splashes in or something and if i get a different color akash we have akash and her son and then this one sort of both of them at once um right so um human languages have this phenomenon of split antecedence but you know one of the things that you should notice um when we start talking about algorithms that people use for doing co-reference resolution is that they make some simplified assumptions as they how do they go about treating the problem and one of the simplifications that most algorithms make is for any noun phrase like this pronoun they that's trying to work out what is it co-reference with and the answer is one thing and so actually most nlp algorithms for co-reference resolution just cannot get split and sequenced might any time it occurs in the text they guess something and they always get it wrong so that's the sort of a bit of a sad state of affairs but that's the truth of how it is okay so then going ahead we have akash here and then we have another tricky one so moving on from there um we then have this a tree so well in this context of this story um akash is going to be the tree so you could feel that it was okay to say well this tree um is also akash um you could also feel that that's a little bit weird and not want to do that and i mean actually um different people's co-reference data sets um differ in this so really that you know that we're predicating an identity relationship here between akash and the property of being a tree so do we regard the tree as the same as akash or not and people make different decisions there okay but then going ahead we have here's akash and she bought him so that's akash and then we have akash here and so then we go on okay um so then if we don't regard the tree as the same as akash we have a tree here um but then note that the next place over here well we have a mention of a tree the best tree but um that's sort of really a functional description of you know of possible trees making someone the best tree it's not really referential to a tree um and so it seems like that's not really co-referent but if we go on there's definitely um more mention of a tree so when she um she has made the tree truly the nicest tree or well i'm not sure is that one co-referent it the it is definitely referring to our tree and maybe this one again is a sort of a functional description that isn't referring to the tree um okay um there's different um and so um maybe this one though where it's a tree is referring to the tree but what i hope to have illustrated from this is you know most of the time when we do co-reference in nlp we just make it look sort of um like the conceptual phenomenon is you know kind of obvious that there's a mention of sarah and then it says she and you say ah they're co-referent um this is easy but if you actually start looking at real text especially when you're looking at something like this that is a piece of literature the the kind of phenomena you get for co-reference and overlapping reference and various other phenomena that i'll um talk about you know they actually get pretty complex and it's not you know there are a lot of hard cases that you actually have to think about as to what things you think about as co-referent or not okay but um basically we do want to be able to do something with co-reference because it's useful for a lot of things that we'd like to do in natural language processing so for one task that we've already talked about question answering but equally for other tasks such as summarization information extraction if you're doing something like reading through a piece of text and you've got a sentence like he was born in 1961. um you really want to know who he refers to to know if this is a good answer to the question of you know when was barack obama born or something like that um it turns out also that it's useful in machine translation so in most languages pronouns have features for gender and number and in quite a lot of languages um nouns and adjectives also show features of gender number and case and so when you're translating a sentence you want to be aware of these features and with what is co-referent as what to be able to get the translations correct so you know if you want to be able to con work out a translation and know whether saying alicia likes one because he's smart or alicia likes one because she's smart then you have to be sensitive to co-reference relationships to be able to choose the right translation when people build dialogue systems dialogue systems also have issues of co-reference a lot of the time um so you know if it's sort of book tickets to see james bond and the system replies spectra is playing near you at two and three today well there's actually co-reference relation oh sorry there's a reference relation between specter and james bond because spectra is a james bond film i'll come back to that one in a minute um but then it's how many tickets would you like two tickets for the showing at three that three is not just the number three that three is then a co-reference relationship back to the 3 p.m showing that was mentioned by the agent in the dialogue system so again to understand these we need to be understanding the co-reference relationships so how now can you go about doing co-reference so the standard traditional answer which i'll present first is co-reference is done in two steps on the first step what we do is detect mentions in a piece of text and that's actually pretty easy problem and then in the second step we work out how to cluster the mentions so as in my um example from the shruti rao text basically what you're doing with co-reference is you're building up these clusters sets of mentions um that refer to the same entity in the world so if we explore a little how we could do that it's a two-step solution the first part was detecting the mentions and so pretty much there are three kinds of things different kinds of noun phrases that can be mentioned there are pronouns like i you're it she him and also some demonstrative pronouns like this and that and things like that there are explicitly name things so things like paris joe biden nike and then there are plain noun phrases that describe things so a dog the big fluffy cat stuck in the tree and so all of these are things that we'd like to identify as mentions and well the straightforward way to identify these mentions is to use natural language processing tools several of which we've talked about already so to work out pronouns we can use what's called um a part of speech taker i'll change this choice um we can use a part of speech tagger which we haven't really explicitly talked about but we used when you build dependency parsers so that first of all assigns parts of speech to each word and so that we can just find the words that are pronouns for named entities we did talk just a little bit about named entity recognizers as a use of sequence models for neural networks so we can pick out things like person names and company names and then for the ones like the big fluffy um a big fluffy dog we could then be sort of picking out from syntactic structure noun phrases and regarding them as descriptions of things um so that we could use all of these tools and those would give us basically our mentions it's a little bit more subtle than that because it turns out there are some noun phrases and things of all of those kinds which don't actually refer so that they're not referential in the world so when you say it is sunny it doesn't really refer when you make universal claims like every student well every student isn't referring to something you can point to in the world and more dramatically when you have no student and making a negative universal claim it's not referential to anything there are also things that you can describe functionally which don't have any clear reference so if i say the best doughnut in the world that that's a functional claim but it doesn't necessarily have reference like if i've established um that i think a particular kind of donut is the best doughnut in the world um i could then say to you um i ate the the best don't i ate the best doughnut in the world yesterday and you know what i mean it might have reference but if i say something like i'm going around to all the donut stores trying to find the best doughnut in the world then it doesn't have any reference yet it's just a sort of a functional description i'm trying to satisfy you also then have things like quantities a hundred miles um it's a quantity there's not really something that has any particular reference you can mark out a hundred all sorts of places um so how do we deal with those things that aren't really mentions well one way is we could train a machine learning classifier to get rid of those spurious mentions but actually mostly people don't do that most commonly um if you're using this kind of pipeline model where you use a parser and a named entity recognizer you regard everything as you've found as a candidate mention and then you try and run your co-ref system and some of them like those ones hopefully aren't made co-referent with anything else and so then you just discard them at the end of the process hey chris yeah um we've got an interesting question that linguist experience on this a student asks can we say that it is sunny has it referring to the weather um i think the yeah so that's so that's a fair question um yeah so people have actually tried to suggest that when you say it is sunny it means the weather is sunny but i guess the majority opinion at least is um that isn't plausible and i mean for i guess many of you aren't native speakers of english but similar phenomena occur in many other languages i mean it just intuitively doesn't seem plausible um when you say it's sunny or it's raining today that you're really saying that as a shortcut for the weather is raining today it just seems like really what the case is is english likes to have something filling the subject position and when there's nothing better to fill the subject position you stick it in there um and get it's reigning um and so in general it's believed that you get this phenomenon of having these empty dummy it's that appear in various places i mean another place in which it seems like you clearly get dummy it's is that when you have clauses that are subjects of a verb you can move them to the end of the sentence so if you have a sentence where you put a clause in the subject position they normally in english sound fairly awkward so it's you have a sentence something like um that cs24n is a lot of work is known by all students um people don't normally say that the normal thing to do is to shift the clause to the end of the sentence but when you do that you stick in a dummy it to fill the subject position um so you say then have it is known by all students that cs224n is a lot of work um so that's the general feeling that this is a dummy yet that doesn't have any reference um okay there's one more question so if someone says it is sunny among other things and we ask how is the weather okay good point you've got me on that one right so someone says how is the weather and you answer it is sunny it then does seem like the it is in reference to the weather i'll buy that well you know i guess um this is what our co-reference systems are built trying to do in situations like that they're making a decision of co-reference or not and i guess what you'd want to say in that case is it seems reasonable to regard this one as co-referenced that weather that did appear before it i mean but that also indicates another reason to think that in the normal case it's not co-referent right because normally pronouns are only used when their reference is established that you've referred to uh noun like um john is answering questions and then you can say he types really quickly and it seemed odd to just sort of start the conversation by he touched really quickly because it doesn't have any established reference whereas that doesn't seem to be the case it seems like you can just sort of um start a conversation um by saying it's raining really hard today and that doesn't sound odd at all okay um so i've sort of there presented the traditional picture but you know this traditional picture doesn't mean something that was done last millennium before you were born i mean essentially um that was the picture until about 2016 um that essentially every co-reference system that was built um use tools like part of speech taggers ner systems and parsers to analyze sentences to identify mentions and to give you features for co-reference resolution and i'll show a bit more about that later but more recently in our newel systems people have moved to avoiding traditional pipeline systems and of doing one-shot end-to-end um co-reference resolution systems so if i skip directly to the second bullet there's a new generation of neural systems where you just start with your sequence of words and you do the maximally dumb thing you just say let's take all spans commonly with some heuristics for efficiency but you know conceptually all subsequences of this sentence they might be mentions let's feed them in to a new network which will simultaneously do mention detection and co-reference resolution end-to-end in one model and i'll give an example of that kind of system later in the lecture okay is everything good to there and i should go on okay um so i'm going to get on to how to do um do co-reference resolution systems um but before i do that i do actually want to show a little bit more of the the linguistics of um co-reference because they're actually a few more interesting things to understand and know here i mean when we say co-reference resolution um we really confuse together two linguistic things which are overlapping but different and so it's really actually good to understand the difference between these things so there are two things that can happen one is that you can have mentions which are essentially alone but happen to refer to the same entity in the world so if i have a piece of text that said barack obama traveled yesterday to nebraska obama was there to open a new meat processing plant or something like that i've mentioned with barack obama and obama there are two mentions there they refer to the same person in the world they are co-referent so that is true co-reference but there's a different but related linguistic concept called anaphra and anaphora is when you have a textual dependence of an anaphor on another term which is the antecedent and in this case the meaning of the anaphor is determined by the antecedent in a textual context and the canonical case of this is pronouns so when it's barack obama said he would sign the bill um he is an anaphor it's not a word that independently we can work out what its meaning is in the world apart from knowing the vagus feature that it's referring to something probably male but in the context of this text we have that this anaphor is textually dependent on barack obama and so then we have an anaphoric relationship which sort of means they um refer to the same thing in the world and so therefore you can say i'm their co-referent so the picture we have is like this right so for co-reference we have these separate textual mentions which are basically standalone which refer to the same thing in the world whereas in anaphora we actually have a textual relationship and you know you essentially have to use pronouns like he and she in legitimate ways in which the hearer can reconstruct the relationship from the text because they can't work out what he refers to if that's not there um and so that's a fair bit of the distinction but it's actually a little bit more to realize because there are more complex forms of anaphra which aren't co-reference because you have a textual dependence but it's not actually one of reference and so this comes back to things like these quantifier noun phrases um that don't have reference so when you have sentences like these ones every dancer twisted her knee well this her here has an anaphoric dependency on every dancer or even more clearly with no dancer twisted her knee the her here has an anaphoric dependence on no dancer but um for no dancer twisted her knee um no dancer isn't referential it's not referring to anything in the world and so there's no co-referential relationship because there's no reference relationship but there's still an anaphoric relationship between these two noun phrases um and then you have this other complex case that turns up quite a bit where you can have where the things being talked about do have reference but an anaphoric relationship is more subtle than identity so you commonly get constructions like this one we went to a concert last night the tickets were really expensive well the concert and the tickets are two different things they're not co-reference co-referential but in interpreting this sentence what this really means is the tickets of tickets to the tickets to the concert right and so there's sort of this hidden not not said dependence where this is referring back to the concert and so what we say is that these the tickets um does have an anaphoric dependence on the concert but they're not co-referential and so that's referred to as bridging and aphra and so overall there's the simple case and the common case which is pronominal and aphra where it's both co-reference and anaphora you then have other cases of co-reference such as every time you see a mention of the every time the united states is said it's co-referential with every other mention of the united states but those don't have any textual dependence on each other and then you have textual dependencies like bridging and aphra which aren't co-reference phew that's probably about us now i was going to say that's probably as much linguistics as you wanted to hear but actually i have one more point of linguistics um one or two of you but probably not many might have been troubled um by the fact that the term anaphra as a classical term um means that you are looking backward for your antecedent um that the anna part of anaphra means that you're looking backward for your antecedent and in um sort of classical terminology you have both anaphora and cataphra and it's cataphra where you look forward for your antecedent cataphra isn't that common but it does occur here's a beautiful example of cataphra so this is from oscar wilde from the corner of the divan of persian saddlebags on which he was lying smoking as was his custom innumerable cigarettes lord henry watten could just catch the gleam of a honey sweet and honey cup of the honey sweet and honey-colored blossoms of a leburnum okay so in this example here right the he and then this his are actually referring to lord henry watan and so these are both examples of cataphra in in modern linguistics um even though most reference to pronouns is backwards um we don't distinguish on in terms of order and so the term anaphor and anaphora is used for textual dependence regardless of whether it's forward or backwards okay a lot of details there but taking stock of this um so the basic observation is um language is interpreted in context that in general you can't work out the meaning or reference of things without looking at the context of the linguistic utterance so we've seen some simple examples before um so for something like word sense disambiguation you've if you see just the words the bank you don't know what it means and you need to look at a context to get some sense as to whether it means a financial institution or the bank of a river or something like that and so um a nephron co-reference give us additional examples where you need to be doing contextual interpretation of language so when you see a pronoun you need to be looking at the context to see what it refers to and so if you think about um text understanding as a human being does it reading a story or an article that we progress through the article from beginning to end and as we do it we build up a pretty complex discourse model in which new entities are introduced by mentions and then they're referred back to and relationships between them are established and they take actions and things like that and it sort of seems like in our head that we sort of build up a kind of a complex graph like discourse representation of us of a piece of text with all these relationships and so part of that is these anaphoric relationships and co-reference that we're talking about here and indeed in terms of cs224n the only um kind of whole discourse meaning that we're going to look at is looking a bit at an afro and co-reference but if you want to see more about higher level natural language understanding you can get more of this next quarter in cs224u so i want to tell you um a bit about several different ways of doing co-reference so broadly there are four different kinds of co-reference models so the traditional old way of doing it was rule-based systems and um this isn't the topic of this class and this is pretty archaic at this point this is stuff from last millennium but i wanted to say a little bit about it because it's actually kind of interesting as sort of food for thought as to how far along we are aren't in solving you know artificial intelligence and really being able to understand texts um then there are sort of classic machine learning methods of doing it which you can sort of divide up as mentioned pair methods mention ranking methods and really clustering methods and i'm sort of going to skip the clustering methods today because most of the work especially most of the recent work implicitly makes clusters by using either mentioned pair or mentioned ranking methods and so i'm going to talk about a couple of neural methods for doing that okay but first of all let me just tell you a little bit about rule-based co-reference so there's a famous historical algorithm in nlp for doing pronoun and afro resolution which is referred to as the hobbs algorithm um so everyone just refers to it as the hobbes algorithm and if you sort of look up a textbook like giraffsky and martin's textbook it's referred to as the hobbs algorithm but you know actually if you go back to jerry hobbs that's his picture over there in the corner um if you actually go back to his original paper he refers to it as the naive algorithm and then his naive algorithm for pronoun co-reference was this sort of intricate handwritten set of rules to work out co-reference so this is the start of the set of the rules um but there are more rules or more clauses of these rules for working out co-reference um and you know this looks like a hot mess but the funny thing was that this set of rules for determining co-reference were actually pretty good and so in the sort of 1990s and 2000s decade even when people were using machine learning based systems for doing co-reference they'd hide into those machine learning based systems that one of their features was the hobbs algorithm and that the predictions it made with a certain weight was then a feature in making your final decisions and it's only really in the last five years that people have moved away from using the hobbs algorithm let me give you a little bit of a sense of how it works okay so on the hobbs algorithm here's our example this is an example from a guardian book review niall ferguson is prolific well-paid and a snappy dresser stephen moss hated him okay so what the hobbs algorithm does is we start with a pronoun oops we start with a pronoun um and then it says step one go to the np that's immediately dominating the pronoun and then it says go up to the first nprs call this x and the path p then it says traverse all branches below x to the left of p left to right breadth first so then it's saying to go left to right for other branches below breadth first um so that's sort of working down the tree so we're going down and left to right and look for um an np okay and here's an np but then we have to read more carefully and say propose as antecedent any np that has an np or s between it and x well this np here has no np or s between npn and x so this isn't a possible antecedent so this is all very you know complex and handwritten but basically he sort of fit into the clauses of this kind of a lot of facts about how the grammar of english works um and so what this is capturing is if you imagine a different sentence you know if you imagine the sentence stephen moss's um brother hated him well then stephen moss would naturally be co-referent with him and in that case well precisely what you'd have is the noun phrase um with um well the noun brother and you'd have another noun phrase inside it um for the stephen moss and then that would go up to the sentence so in the case of stephen moss's brother when you looked at this noun phrase there would be an intervening noun phrase before you got to the node x and therefore um stephen moss is a possible and in fact good antecedent of him and the algorithm would choose stephen moss but the algorithm correctly captures when you have the sentence stephen moss hated him that him cannot refer to steven moss okay so um having worked that out it then says if x is the highest s in the sentence okay so my x here is definitely the highest s in the sentence because i've got the whole sentence what you should do is then traverse the pause trees of previous sentences in the order of recency so what i shouldn't do now is sort of work backwards in the text one sentence at a time going backwards looking for an antecedent um and then for each tree traverse each tree left to right breadth first so then within each tree i'm doing the same of going breadth first so sort of working down and then going left to right with an equal breadth and so hidden inside these clauses it's capturing a lot of the facts of how co-reference typically works so what you find in english i'll say but in general this is true of lots of languages is that there are general preferences and tendencies for co-reference so a lot of the time a pronoun will be co-referent with something in the same sentence like steven's moss's brother hated him but it can't be if it's too close to it so you can't say stephen moss hated him and have the him be stephen moss and if you're then looking for co-reference that's further away um the thing it's co-referent with is normally close by and so that's why you work backwards through sentences one by one but then once you're looking within a particular sentence the most likely thing it's going to be co-referent to is a topical noun phrase and default topics in english are subjects so by doing things breadth first left to right a preferred antecedent is then a subject and so this algorithm i won't go through all the complex clauses five through nine ends up saying okay um what you should do is propose nile ferguson as what is co-referent to him which is the obvious correct reading in this example okay you probably didn't want to know that and in some sense the details of that um aren't interesting but what is i think actually still interesting in 2021 is what points jerry hobbs was actually um trying to make um last millennium um and the point he was trying to make was the following so jerry hobbs wrote this algorithm the naive algorithm because what he said was well look if you want to try and crudely determine co-reference well there are these various preferences right there's the preference for same sentence there's the preference for recency there's a preference for topical things like subject and there are things where you know if it has gender it has to agree in gender so there are sort of strong constraints of that sort so i can write an algorithm using my linguistic nouns which captures all the main preferences and actually it works pretty well um doing that is a pretty strong baseline system but what jerry hobbs wanted to argue is um that this algorithm just isn't something you should believe in this isn't a solution to the problem this is just sort of um you know making a best guess according to um the preferences of what's most likely without actually understanding what's going on in the text at all and so actually um what jerry hobbs wanted to argue was the so-called hobbs algorithm now he wasn't a fan of the hobbs algorithm he was wanting to argue that the hobbs algorithm is completely inadequate as the solution to the problem and the only way we'll actually make progress in natural language understanding is by building systems that actually really understand the text and this is actually something that has come to the fore again more recently so the suggestion is that in general you can't work out co-reference or pronominal and afro in particular unless you're really understanding the meaning of the text and people look at pairs of examples like these ones so she poured water from the pitcher into the cup until it was full um so think for just half a moment well what is it in that example um that is full um so um that what's full there is the cup um but then if i say she poured water from the pitcher into the cup until it was empty well what's empty well that's the picture and the point that um is being made with these examples is the only thing that's been changed in these examples um is um the adjective right here so these two examples have exactly the same grammatical structure so in terms of the hobbs naive algorithm the hobbes naive algorithm necessarily has to predict the same answer for both of these but that's wrong you just cannot determine the correct um pronoun antecedent based on grammatical preferences of the kind that are used in the naive algorithm you actually have to conceptually understand about pictures and cups and water and full and empty to be able to choose the right antecedent here's another famous example that goes along the same lines so terry winograd shown here as a young man so long long ago terry winograd came to stanford as the natural language processing faculty and terry winograd became disillusioned with the symbolic ai of those days and just gave it up all together and he reinvented himself as being an hci person and so terry was then essentially the person who established the hci program at stanford but before he lost faith in symbolic ai he talked about the co-reference problem and pointed out a similar pair of examples here so we have the city council refused the women a permit because they feared violence versus the city council refused the women a permit because they advocated violence so again you have this situation where these two sentences have identical syntactic structure and they differ only in the choice of verb here but once you add um knowledge common sense knowledge of how the um human world works well what how this should pretty obviously um be interpreted that in the first one that they is referring to the city council whereas in the second one that they is referring to the women and so coming off of that example of terry these have been referred to as winograd schemers so a winner grad schemer challenge is sort of choosing the right reference here and so it's basically just doing pronominal and aphra but you know the interesting thing is people have been interested in you know what are tests of general intelligence and one famous general test of intelligence that i won't talk about now is the turing test and there's been a lot of debate about problems with the turing test and is it good um and so in particular hector levesque who's a um a very well-known senior ai person um he actually proposed that a better alternative to the turing test um might be to do what he then dub winograd schema and winograd schema is just solving pronominal co-reference in cases like this where you have to have knowledge about the situation in the world to get the answer right and so he's basically arguing that you know you can review really solving co-reference as solving artificial intelligence and that's sort of um what the position that hobbes wanted to advocate so what he actually said about his algorithm was that the naive approach is quite good computationally speaking it will be a long time before a semantically based algorithm is sophisticated enough to perform as well and these results set a very high standard for any other approach to aim for and he was proven right about that because there was sort of really talked to around 2015 before people thought they could do without the hobbes algorithm but then he notes yet there is every reason to pursue a semantically based approach the naive algorithm does not work anyone can think of examples where it fails in these cases it not only fails it gives no indication that it has failed and offers no help in finding the real antecedent and so i think this is actually still interesting stuff to think about because you know really for the kind of machine learning based co-reference systems that we're building you know they're not a hot mess of rules like the hobbs algorithm but basically they're still sort of working out statistical preferences of what patterns are most likely and choosing the antecedent that way they're really um have exactly the same deficiencies still that hobbs was talking about right that they fail in various cases it's easy to find places where they fail the algorithms give you no idea when they fail they're not really understanding the text in a way that a human does to determine the antecedent so we still actually have a lot more work to do before we're really doing full artificial intelligence but i'd best get on now and actually um tell you a bit about some co-reference algorithms um right so the simple way of thinking about co-reference is to say that you're making just a binary decision about a reference pair so if you have your mentions you can then say well i've come to my next mention she i want to work out what it's co-referent with and i can just look at all of the mentions that came before it and say is it co-referent or not and do a binary decision so at training time i'll be able to say i have positive examples assuming i've got some data labeled for what's correct to what as to these ones are co-referent and i've got some negative examples of these ones are not co-referent and what i want to do is build a model that learns to predict co-referent things and i can do that fairly straightforwardly in the kind of ways that we have talked about so i train with the regular kind of cross-entropy loss where i'm now summing over every pairwise binary decision as to whether two mentions are co-referent to each other or not and so then when i'm at test time um what i want to do is cluster the mentions that correspond to the same entity and i do that by making use of my pairwise scorer so i can run my pairwise scorer and it will give a probability or a score that any two mentions a co-referent so by picking some threshold like 0.5 i can add co-reference links for when the classifier says it's above the threshold um and then i do one more step to give me a clustering i then say okay let's also make the transitive closure to give me clusters so it thought that i and she were co-referent and my and she were co-referent therefore i also have to regard i and my as co-referent and so that's sort of the completion by transitivity and so since we always complete by transitivity note that this algorithm is very sensitive to making any mistake in a positive sense because if you make one mistake for example you say that he and my a co-referent then by transitivity all of the mentions in the sentence become one big cluster and that they're all co-referent with each other that's a workable algorithm and people have often used it but often people go a little bit beyond that and prefer um a mention ranking model so let me just explain the advantages of that that normally if you have a long document where it's ralph nader and he did this and someone did something to him and we visited his house and blah blah blah blah and then somebody voted for nader because he in terms of building a co-reference classifier it seems like it's easy and reasonable oops um it's easy and reasonable to be able to recover this he refers to nader but in terms of building a classifier for it to recognize that this he should be referring to this nader which might be three paragraphs back seems kind of unreasonable how you're going to recover that so those far away ones might be almost impossible to get correct and so that suggests that maybe we should have a different way of configuring this task so instead of doing it that way what we should say is well this he here has various possible antecedents and our job is to just choose one of them and that's almost sufficient apart from we need to add one more choice which is well some mentions won't be co-referent with anything that proceeds because we're introducing a new entity into the discourse so we can add one more dummy mention the n a mentioned so it doesn't refer to anything previously in the discourse discourse and then our job at each point is to do mention ranking to choose which one of these she refers to and then at that point rather than doing binary yes no classifiers that what we can do is say aha this is choose one classification and then we can use the kind of softmax classifiers that we've seen at many points previously okay so that gets us in business for building systems and for either of these kind of models um there are several ways in which we can build the system we could use any kind of traditional machine learning classifier we could use a simple neural network we can use more advanced ones with all of the tools that we've been learning about more recently let me just quickly show you um a simple neural network way of doing that so this is a model um that um my phd student um kevin clark did in 2015 so not that long ago um but what he was doing was doing co-reference resolution based on the mentions with a simple feed forward neural network kind of in some sense like we did dependency parsing with a simple feedback neural network so for the mention it had word embeddings antecedent had word embeddings there were some additional features of each of the mentioner candidate antecedent and then there were some final additional features that captured things like distance away which you can't see from either the mention or the candidate and they were all of those features were just fed into several feed-forward layers of a neural network and it gave you a score of are these things um co-referent or not and that by itself um just worked pretty well um and i won't say more details about that um but what i do want to show is sort of a more advanced and modern neural co-reference system but before i do that i want to take a digression and sort of say a few words about convolutional neural networks um so um the idea of when you apply a convolutional neural network to language i.e to sequences is that what you're going to do is you're going to compute vectors features effectively for every possible word subsequence of a certain length so that if you have a piece of text like tentative deal reach to keep government open you might say i'm going to take every three words of that i tentative deal reached deal reached to reach to keep and i'm going to compute a vector based on that subsequence of words and use those computed vectors in my model by somehow grouping them together so the canonical case of convolutional neural networks is in vision and so if after this next quarter um you go along to cs231n um you'll be able to spend weeks doing convolutional neural networks for vision and so the idea there is that you've got these convolutional filters that you sort of slide over an image and you compute a function of each place so the sort of little red numbers are showing you what you're computing but then you'll slide it over to the next position and fill in this cell and then you'll slide over the next position and fill in this cell and then you'll slide it down and fill in this cell and so you've got this sort of little function of a patch which you're sliding over your image and computing a convolution which is just a dot product effectively that you're then using to get an extra layer of representation and so by sliding things over you can pick out features and you've got a sort of a feature identifier that runs across every piece of the image well for language we've just got a sequence but you can do basically the same thing and what you then have is a 1d convolution for text so if here's my sentence tentative deal reached to keep the government open that what i can do is have so these words have a word representation which so this is my vector for each word and then i can have a filter sometimes called a kernel which i use for my convolution and what i'm going to do is slide that down the text so i can start with it with the first three words and then i sort of treat them as sort of elements i can dot product and sum and then i can compute a value as to what they all add up to which is minus one it turns out and so then i might have a bias that i add on and get an updated value if my bias is plus one and then i'd run it through a non-linearity and that will give me a final value and then i'll slide my filter down and i'd work out a computation for this window of three words and take 0.5 times 3 plus 0.2 times 1 etc and that comes out as this value i add the bias i put it i'm going to put it through my non-linearity and then i keep on sliding down and i'll do the next three words and keep on going down and so that gives me a 1d convolution and computes a representation of the text you might have noticed in the previous example that i started here with seven words but because i wanted to have a window of three for my convolution the end result is that things shrunk so on the output i only had five things that's not necessarily desirable so commonly people will deal with that with padding so if i put padding on both sides i can then start my three by three convolution my three sorry not three by three my three convolution i'm here and compute this one and then slide it down one and compute this one and so now my output is the same size as my real input and so that's a convolution with padding okay so that was the start of things but you know how you get more power with the convolutional network is you don't only have one filter you have several filters so if i have three filters each of which will have their own bias non-linearity i can then get a three-dimensional representation coming out the end and sort of you can think of these as conceptually computing different features of your text okay so that gives us a kind of a sort of a new feature re-representation of our text but commonly we then want to somehow summarize what we have and a very common way of summarizing what we have is to then do pooling so um if we sort of think of these features as detecting different things in the text so you know they might even be high level features like you know does this um show signs of toxicity or hate speech um is there reference to something so if you want to be interesting does it occur anywhere in the text what people often then do as a max pooling operation where for each feature they simply sort of compute the maximum value it ever achieved in any position as you went through the text and say that this vector ends up as the sentence representation sometimes for other purposes rather than max pooling people use average pooling where you take the averages of the different vectors um to get the sentence representation then general max pooling has been found to be more successful and that's kind of because if you think of it as feature detectors that are wanting to detect was this present somewhere then you know something like positive sentiment isn't going to be present in every three-word um sub-sequence you choose because they're somewhere there and so often max pooling works better um and so that's a very quick look at convolutional neural networks except to say this example was doing 1d convolutions with words but a very commonplace that convolutional neural networks being used in natural language is actually using them with characters and so what you can do is you can do convolutions over subsequences of the characters in the same way and if you do that this allows you to computer representation to any sequence of characters so you don't have any problems with being out of vocabulary or anything like that because for any sequence of characters you just compute your convolutional representation and max pool work and so quite commonly people use a character convolution to give a representation of words perhaps as the only representation of words but otherwise is something that you use in addition um to a word vector and so on both by def and the model i'm about to show that at the base level it makes use of both a word vector representation that we saw right at the beginning of the text and a character level convolutional representation of the words okay with that said i now want to show you before time runs out i'm an end-to-end neural co-references model so the model i'm going to show you is kenton lee's one from um so done university of washington 2017. this is no longer the state of the art i'll mention the state of the art at the end but this was the first model that really said get rid of all of that old stuff of having pipelines and mentioned detection first just build one end-to-end big model that does everything and returns co-reference so it's a good one to show so compared to the earlier simple thing i saw we're now going to process the text with biostms we're going to make use of attention and we're going to do all of mention detection co-reference in one step end-to-end and the way it does that is by considering every span of the text up to a certain length as a candidate mentioned and just figures out a representation for it and whether it's co-referent to other things so what we do at the start is we start with the sequence of words and we calculate from those uh standard word embedding and a character level cnn's embedding we then feed those as inputs into a bi-directional lstm of the kind that we saw quite a lot of before but then after this what we do is we compute representations for spans so when we have a sequence of words um we're then going to work out a representation of a sequence of words which we can then put into our co-reference model um so that we i can't fully illustrate in this picture but so see subsequences of different lengths so like general general electric general electric said um will all have a span representation which i've only shown a subset of them in green so how are those computed well the way they're computed is that the span representation is a vector that concatenates several vectors and it consists of four parts it consists of the representation that was computed for the start of the span from the um by lstm the representation for the end from the bio stm that's over here and then it has a third part that's kind of interesting this is an intention-based representation that calculate is calculated from the whole span but particularly sort of looks for the head of a span and then there are still a few additional features um so it turns out that you know some of these additional things um like length and so on is still a bit useful um so to work out the the final part is not the beginning and the end what's done is to calculate an attention weighted average of the word embeddings so what you're doing is you're taking the x-star representation of the final word of the span and you're feeding that into a neural network to get attention scores for every word in the span which are these three and that's giving you an attention distribution as we've seen previously and then you're calculating the third component of this as an attention weighted sum of of the different words in the span and so therefore you've got the sort of a sort of a soft average of the representations of the words of the span okay um so then once you've got that um what you're doing is then feeding these representations into having scores for whether spans are co-referent mentions um so you have a representation of uh the two spans um you have a score that's calculated for whether two different spans look co-referent and that overall you're getting a score for r different spans looking co-referent or not um and so this model is just run end to end on all stands now it sort of would get intractable if you scored literally every standalone piece of text so they do some pruning they sort of only allow spans up to a certain maximum size they only consider pairs of spans that aren't too distant from each other etc etc but basically it's in sort of an approximation to just a complete comparison of spans and this turns into a very effective co-reference resolution algorithm today it's not the best co-reference resolution algorithm um because maybe not surprisingly like everything else that we've been dealing with there's now been these transformer models like burt have come along and that they produce even better results so the best co-reference systems now have you make use of bert in particular when dante spoke she briefly mentioned spanbert which was a variant of bird which constructs blanks out for reconstruction subsequences of words rather than just a single word and span bird has actually proven to be very effective for doing co-reference perhaps because you can blink out whole mentions people have also gotten gains actually funnily by treating co-ref as a question-answering task so effectively you can find a mention like he or the person and say what is its antecedent and get a question answering answer and that's a good way to do co-reference so if we put that together um as time is running out um let me just sort of give you some sense of how results come out for co-reference systems so i'm skipping a bit actually that you can find in the slides which is how co-references scored but essentially it's scored on a clustering metric so a perfect clustering will give you a hundred and something that makes no correct decisions would give you zero and so this is sort of how the co-reference numbers have been panning out so back in 2010 actually this was a stanford system this was a state-of-the-art system for co-reference it won a competition it was actually a non-machine learning model because again we wanted to prove how these rule-based methods and practice work kind of well um and so its accuracy was around 55 for english 50 for chinese then gradually machine learning this was sort of statistical machine learning models got a bit better wiseman was the very first neural co-reference system and that gave some gains um here's a system that kevin clark and i did which gave a little bit further gains um so lee is the model um that i've just shown you as the end-to-end model and it got um a bit of further gains but then again you know what gave the huge um breakthrough just like question answering was that the use of span bert so once we moved to here we're now using span bird that's giving you about an extra 10 or so um the co-ref qa technique proved to be useful um and then the very latest best results are effectively combining together span bert and a larger version of spanbert and co-fqa and getting up to 83. so you might think from that that co-ref is sort of doing really well and is getting close to solved like other nlp tasks um well it's certainly true that in neural times the results have been getting way way better than they had been before but i would caution you that these results that i just showed were on a corpus called onto nodes which is mainly newswire and it turns out that newswire co-reference um is pretty easy i mean in particular there's a lot of mention of the same entities right so the newspaper articles are full of mentions of the united states and china and leaders of the different countries and it's sort of very easy to work out what they're co-referent to and so the co-reference scores are fairly high whereas if what you do is take something like a page of dialogue from a novel and feed that into a system and say okay do the co-reference correctly you'll find pretty rapidly um that the performance of the models is much more modest um if you'd like to try out a co-reference system for yourself um there are pointers um to a couple of them um here um where the top one's ours from certain um kevin clark's new co reference um and this is one that goes with the hugging face repository that we've mentioned you 
","['', 'co-reference resolution', 'mention', 'convolutional neural networks', 'character cnn', 'padding', 'max pooling', 'convolutional neural networks for language applications', 'word embedding', 'bi-directional lstm', 'attention', 'co-reference model', 'span', 'intention-based representation', 'co-reference systems', 'Stanford Coreference', 'BERT', 'spanBERT', 'question-answering task', 'co-reference clustering metric', '']"
"cool hi everyone um hi um I'm Isabelle I'm a PhD student in the NLP group uh it's uh about connecting insights between NLP and Linguistics uh yeah so hopefully we're going to learn some some Linguistics and think about some cool things about language uh some Logistics uh we're in the project part of the class which is cool uh we're so excited to see everything you guys do uh you should have a mentor greater assigned um uh through your project proposal uh the person whoever graded your project proposal uh you especially if you're in a custom project you know we recommend that you go to your graders office hours it's uh they'll know the most and like be most into your project uh and project Milestones are due next Thursday so that's in one week from now so hopefully you guys are all getting uh warm warmed up doing some some things for the project and we'd love to hear where you are next week cool so um the main thing that I'm going to talk about today is is that there there's been kind of a paradigm shift uh for the role of linguistics uh in NLP do due to large language models right so it used to be that uh you know there was just human language we created all the time we're literally constantly creating it and then we would like analyze it in all these ways you know maybe we want to make trees out of it maybe want to make different types of trees out of it uh and and then with all that would would kind of go into making some kind of computer system that can use language right and and now we've we kind of we've cut out this middle part right so so we have human language and we can just like immediately train uh um uh a system that's like very competent in human language and so now we have all this like analysis stuff from before uh and and from and and and we're still producing more and more of it right there's still all the structure all this knowledge that we know about language and the question is you know is this relevant at all to and healthy and today I'm gonna show how like it's useful for looking at these models understanding the these models understanding how how things work what we can expect what we can't expect uh from from uh large large language models so in this lecture we'll you know learn learn some some Linguistics hopefully language is you know an amazing thing it's like so fun to think about language and hopefully we can instill some of that in you maybe you'll go take like link one or something after this um and and we'll discuss some some questions about NLP and Linguistics right where does Linguistics fit in for today's NLP and what and what does NLP have to gain from knowing and analyzing human language you know what is like a 224n student have have to gain from from knowing all this stuff about human language so so the for the lecture today we're going to start off um uh talking about structure in human language doing thing thinking about like the Linguistics of syntax and how structure works in language we're gonna um then move on to like a linguistic structure in NLP in language models the kind of analysis that people have done uh for for for understanding structure in NLP and then we're going to think of going Beyond pure structure so so beyond thinking about syntax thinking about how like meaning and um yeah how like meaning and discourse and all of that play into making language and how you know and how we can think of this both from a linguistic side and from a deep learning side uh and then lastly we're going to look at multilinguality and language diversity in in NLP cool so Stars starting off a structure in human language um you know just just like a small primer in language in general right uh it's it's a kind of if you've taken any Android Linguistics class you only know all of this but you know I think it's fun to get kind of situated in the amazingness of this stuff right so like all all humans have language and no other animal communication is similar it's this thing which is like incredibly just like um easy for any baby to pick up in any situation and um and it's just this like remarkably calm complex system very famously you know linguist uh like to to talk about the case of Nicaragua and sign language because it um it uh uh it kind of emerged like while people were watching in a great way right so like after the sentiment used to Revolution um you know they they started uh there's there's like kind of large public education in Nicaragua and they made a school for for uh for Deaf children and and there was no Central Nicaraguan sign language people had like isolated language and then you see this like full language emerge in this school very very autonomous autonomously very naturally I hope this is this is common knowledge maybe it's not you know signed languages are like full languages with like more morphology and and like things like pronouns and tenses and like all the things it's not like how I would talk to you across the room uh yeah and so and what's cool about language is that it can be manipulated to say infinite things right and and the brain is finite so it's either we have some kind of set of rules that were like that we like tend to be able to pick up from from from hearing them as a baby and then be able to say infinite things and we can manipulate these rules to really say anything right we can talk about things that don't exist things that can't exist this is very different from like the kind of animal communication we see like a squirrel like alarm call or something you know it's like watch out there's the cat um uh things are like totally abstract you know that have like no no grounding in anything we can express like some subtle differences between similar things I always when I'm thinking about like the this point in like um things called yeah like this featured languages think of like the stack exchange World building uh thing I don't know if you ever looked at the sidebar where there's then there's like thing where like science fiction authors kind of pitch uh like their ideas for like their science fiction world and it's like the wackiest like and you can really create any world with like with within English with a language that that we're given it's it's like amazing and so there's structure underlying language right this is I said recap here because we've done like the dependency parsing lectures we thought about this right but you know if if we have some some sentence like you know Isabel broke the window the window was broken by Isabel right we have these two sentences or some kind of relation between them and then and then we have another two sentences they have like the similar relation between them right this kind of passive alternation it's kind of something which exists for both of these sentences you know and then we can even use like made up words and uh it's still you can still see that it's a passive alternation right and so it seems like we have some knowledge of structure that's separate from from the words we use and the things we say that's kind of above it and then what's interesting about structure is that it dictates how we can use language right so you know if if I have a sentence like the cat sat on the mat and it's and it looks uh uh you know and and then someone tells you like well this is you know if you make a tree if it's going to look like this according to my type of tree Theory um you would say well why should I care about that and the reason that this stuff is relevant is because um it kind of influences what you could do right so like any subtree or like you know in this specific case any subtree in other cases like many sub trees it can kind of be real real replaced with like one item right because like he sat on the mat or he sat on it or he sat there right or he did so it's those two words but you know there's a lot of ink spilled overdue in English especially in like early Linguistics teaching so we're not going to spill anything it's kind of like one word um but then when something is not a sub tree you like can't really replace it with one thing right so you so you can't express like the cat's sat and they kind of like have the mat as a different thing right and one you could feel like he did so on the mat right you'd have to kind of do to do things and like and and one way you could think about this is that well it's not a sub tree right it's kind of like you kind of have have to go up a level uh to to to do this and so you can't really separate the cat from on the mat in this way and so and we implicitly know like so many complex rules about structure right we're like processing the these like streams of sound or like streams of letters all the time and yet we like have these like the ways that we use them show that we have all these like complex ideas like the tree I just showed or like for for example these are like I'm just gonna give some examples for like a a taste of like the kinds of things people are thinking about now but there's like so many right so like what can we pull out to make a question right so like if we form a former question we we form it by like we we're kind of referring to some part of like you know there might be another sentence which like is the statement version right and we've kind of pulled pulled pulled out some some part to make the question they're not necessarily like fully related but you know so it's like Leon is a doctor we can kind of pull pull that out to make a question right like what is Leon and if we have like my cat likes tuna we could pull that out what does my cat like again do ignore the do um if we have something like Leon is a doctor and an activist we actually we can't pull out this the this last thing right so if something's like in this if something's like conjoined with an and we it can't like be be taken out of that and right so you you could only say what is Leon think he's like oh a doctor and an activist but you can't really say what is the on a doctor and this is like not how question formation works and you know this is like some something that we all know it's I think something any of us have been taught right even people who've been taught English as a second language I don't think this is something which you're ever um which whichever really taught explicitly right but but but most of us probably know this very well um uh another such rule right is like when is like this is like when can we kind of shovel things around right so if we have something like I dictated the letter to my secretary right uh we can make like a longer version of that right I dictated the letter that I had been procrastinating writing for weeks and weeks to my secretary um this character is like both a grad student and like a high ranking executive um and and then we can we can move the uh we we can move that that long thing to the end right it's like I dictated it to my secretary of the letter that I've been procrastinating writing for weeks and weeks and that's like fine you know maybe it's like slightly awkwardly phrased but it's not like I think like this firm for me at least everyone varies right could could appear in like natural productive speech but then something like this is like much worse right so somehow the fact that it becomes weighty is good and you we can move it to the end but when it doesn't become weighty we can't right and we like this sounds kind of more like Yoda e than like real language and so and so like and we have this rule like this one's not that easy to explain actually like people have tried many ways like to like make sense of this in linguistics and it's just like but it's a thing we all know right and and so when I say rules of grammar these are not the kind of rules that we're usually taught as rules of grammar right so a a community of speakers you know for example like standard American English speakers they share this rough consensus of like the implicit rules they all have these are not the same you know like people have like gradations and disagree on things but you know and then kind of like a grammar is an attempt to describe all all these rules right and you can like kind of linguist might write out like a big thing called like you know the like grammar of the English language where they're trying to just describe all of them it's like really not going to be um large enough ever like this is a really Hefty book and it's like not still not describing all of them right like language is so complex but so what so what we are told is rules of grammar you know these kind of like prescriptive rules where they tell us what what we can and can't do you know they often have other purposes in describing the English language right so for example when they've told us things like oh you should never start a sentence with and you know that's like not true you know we start tennis we land all the time in English and it's fine uh uh you know what they probably mean you know there's some probably like reason that they're saying this right like especially if you're like trying to teach a high schooler to like write you know you're probably when you want them to focus their thoughts you probably don't want them to be like oh and this oh and this again you know like you want them to like and so you tell them like oh a rule of writing you know is like it's like you can never start sentence with and right and when they say something like oh it's incorrect to say I don't want nothing this is like bad grammar you know well this is you know in in standard American English you probably wouldn't have nothing there right because uh you you would have anything right but but in many dialects of English you know any many languages across the world when you have a negation right like the not and don't then like everything it kind of Scopes over also has to be negative has to agree and many dialects of English are like this and so what they're really telling you is you know the dialect with the most power in the United States doesn't do negation this way and so you should neither in school right and and and so you know and so the way that we can maybe Define grammaticality right rather than like what they tell us is wrong or right is that you know if we choose a community of speakers to look into they share this rough consensus of their implicit rules and so like the utterances that we can generate from these rules you know are grammatical uh roughly you know everyone has these like gradations of what they can accept and if we can't produce an utterance using these rules you know it's ungrammatical and that's where like this is like the descriptive way of thinking about grammar where we're where we're thinking about uh what people actually saying what people actually like and don't like and so for an example you know in in English largely we have a pretty strict rule that like the subject the verb and the object appear in this like SVO order there's exceptions to this like there's acceptance everything right expression things like says I and some dialects but you know it is like largely if something is before the verb it's a subject something is after the verb it's an object and you can't move that around too much and uh you know we also have these subject pronouns you know like I I shahide that have to be the subject and these object pronouns you know me me her him them that have to be the object and uh and you know and so if we follow the these rules we get a sense that we think is is good right like I love her and if we don't then we get a sentence that we think is is ungrammatical right something like me love she it's like we don't know who is who um you know who's doing the loving and and who is being loved in in this one right and it's doesn't exactly parse and this is like also true you know like even when there's ambiguity this continues to be true right so for a sentence like me a cupcake eight which is like the meaning is perfectly clear uh our rules of grammaticality don't seem to cut to cut as much slack right we're like oh this is wrong I understand what you mean but in my head I I know it's like not you know correct even not not by the like prescriptive notion of what I think is correct you know by the descriptive notion like my I just don't don't like it right and uh you can also you know sentences can be grammatical without any meaning so you can have meaning with that grammaticality right like me a cupcake eight and you could also have it's like classic uh uh example from from Chomsky in 1957 um I introduced it earlier uh but yeah classically from 1957 uh you know colorless green ideas sleep sleep furiously right which like this has no meaning because you can't really make any sense out of this sentence as a whole but you know you know it's grammatical and you know it's grammatical right because you can make an ungrammatical version of it right like color screen ideas sleeps Furious right which does make sense because there's no agreement even though you don't have any meaning for any of this and then lastly um you know people don't fully agree you know everyone has their own idiotic right people like usually speak like more than one dialect and they kind of move between them and they have a mixture and those have like their own way of thinking of things they also have these like those have different opinions at the margins people like like some things more uh others don't right so an example of this is like not everyone is as strict for some wh constraints right so if you're trying to pull out something like I saw who Emma doubted report that would capture in the Nationwide FBI Manhunt was from a paper by uh Hofmeister knives and sag from Stanford uh this is like some people like it some people don't you know it's kind of some people can like clearly see it's like oh it's the who that we had captured and Emma doubted the reports that we had captured them you know and some people are like this is as bad as like uh what is the other doctor and I don't like it right so yeah so that's grammaticality and the question is like why do we even need this right it's like we we like we like accept these useless utterances and we block out these perfectly communicative utterances right and and this is like I started off saying that this is like a fundamental facet of human intelligence like it seems kind of you know a strange thing to have and so I think one thing I keep returning on when I think about Linguistics is that a basic fact about language is that is that we can say anything right there's like really every language you know can express anything you know and it's like there's no word for something people will develop it if they want to talk about it right and so if we ignore the rules because we know what it's probably intended right uh you know and then we'll be limiting possibilities right so in my kitchen horror novel where the ingredients become sentient I want to say the onion chop the chef and if people if if people just assumed I meant the chef chopped the onion because like SVO order doesn't really matter then uh I can't I can't say that so then yeah to to like to conclude you know a fact about language that that's like very cool is that it's compositional right we have the set of rules that defines grammaticality and then this like um and then this lexicon right this like dictionary of words that that relate the world want to talk to them and kind of combine them in these Limitless ways to say anything we want to say cool any questions about all this I've like tried to bring a lot of like linguistic fun facts like top of mind for this lecture so I'll hopefully hopefully have answers for things you want to ask cool cool yeah um cool so so now you know that was a nice foray into like a lot of like 60s Linguistics um you know how how does that relate to us like today right in NLP and um so we said that in humans you know like we can think about languages it's like there's a system for producing language you know that can be described by these discrete rules you know so it's not like it's smaller than all the things that we can say they're just kind of like rules that we can kind of put together to say things and so do NLP systems work work like that and one answer is like well they definitely used to right because as you said in the beginning before self supervised learning uh the way to approach doing NLP was through um understanding the human language system right and then trying to imitate it trying to see if you think really really hard about how humans do something then you kind of like code up a computer to to do it right and so for for one example is like you know parsing used to be like super important in in in in NLP right so and this is because you know as an example if I want my sentiment analysis system to classify a movie review correctly right something like my uncultured roommate hated this movie but I absolutely loved it right How would how would we do this before we had like cha gbt um we we we we you know we might have some semantic representation of words like hate and uncultured you know it's not looking good for the movies but you know how how does everything relate well you know we we might ask how would human structure this word you know so many linguists you know there's many theories of how to make you know of how syntax might work but they would tell you some some something like this so it's like okay I know I'm interested in the eye right because that's like probably what what the review relates to they're just worrying stuff about uncultured and hated but it seems like those are related like syntactically together right it's like the roommate hated and that can't really connect to the eye right so the eye can can't really um be related to the hated right because there's kind of separated they're like separate Sub sub trees separated by this like conjunction by this but relation um and and so and so it seems that I goes with love which is looking good for the movie let me know we have loved it and so then we have to move beyond the rules of of syntax right the rules of like discourse how how would this kind of you know like what could it mean you know there's like a bunch of rules of discourse and if you say it you're probably referring to like the latest kind of salient thing that's you know matches and like you know it is probably non-sentient right and so you know in this case it would be movie right so so so then you know like linguistic Theory you know they helped NLP uh it helped NLP reverse engineer language so you had something like input you know get like syntax from it you get semantics from from the syntax right so you would take the tree and then from the tree kind of build up all these like little you know like you you you you can build up these little functions of like how how how things how things like relate to each other then and then you you could go to discourse right so so so so what refers to what what what nouns are being talked about what things are being talked about and you know and and then whatever else was interesting for your specific uh uh uh uh use case now we don't need all that right language models just seem to catch on to a lot of these things right so so this whole thing that I did with the tree is like Chachi bitty know this I know it's much harder things than this right this was like this isn't even like slightly prompt engineer that just like woke up one morning was like gotta do the rest of the lecture gonna put that into chat GPT and this exactly you know I didn't even get some like yeah stop well I guess I got a bit of moralizing but I just like immediately uh immediately just told told me you know who likes it who doesn't like it and why I'm doing something like slightly uh wrong uh which is How It Ends everything right um and so and so you know NLP systems definitely used to uh this is where we were you uh work in this kind of structured discrete way but now NLP works better than it ever has before and we're not constraining our systems to know any sense syntax right so what what about structure in modern language models uh and so um this question is like do the questions like a lot of analysis work has has has been focused on you know I think we'll have more analysis lectures later also so this is going to be you know looked in more detail right is how could you get from training data you know which is just kind of like a loose set of just things that have appeared on the Internet or sometimes not on the internet rarely right to rules about language right to to to to the idea that there's this like structure underlying language that we all seem to know even though we do just talk in streams of things and sometimes up here on the internet and one way to think about this is like testing um you know is testing how novel words and old structures work right so humans can easily integrate new words into our old sense and tactic structures I remember like I had lived in Greece for a few years for Middle School just speak not speaking English too much and I came back for high school and uh and um yeah and and this is like in in Berkeley in this way and there was like there was literally like 10 new vocabulary words I'd like never heard of before and they all had like a very similar role to like dank or like sick you know but they were like the ones that were being tested out and did not pass and within like one you know one day I immediately knew how to use all of them right it was not it was not like a hard thing for me I didn't have to like get a bunch of training data uh about how how to use you know all these words right and so this kind of like is is one way of arguing that you know the thing I was arguing for the whole first part of the lecture that syntactic structures they exist independently of the words that they have appeared with right uh a famous example of this is um is Lewis Lewis Carroll's poem Jabra walking right I was going to quote from it but I can't actually see it there right where they uh where they uh you know where he just like made up a bunch of new words and he just made this poem which is all new open class words open class words what we call you know kind of like nouns verbs adjectives adverbs classes of words that like we add new things to all the time while while things like conjunctions you know like and or but are closed class of there's been a new conjunction added late added recently I just remembered after I said that who does anyone know like of a contraction that's in the past like 30 years or something maybe 40. all right spoken slash like now we say slash and it kind of has a meaning that's like not and or but or or or or but it's a new one but it's closed Clash generally this happens rarely anyway and and so you know you you you you have like twist brilliant and the slightly toes did guyron Gimbal and the wave right toves is a noun we all know that we've never heard it before and in fact you know one word for from from Jabberwocky Turtle actually entered the English vocabulary right it kind of means like a like a little chuckle that's maybe slightly suppressed or something right so so so it shows like you know there was one literally like one example of this word and then people picked it up and started using it as if it was a real word right so and so one one way of asking um a do language models have structures like do they have this ability and you know and I always think it would be cool to go over like a benchmark about this right so like the kind of things so people like make things where you could test your language models to to see if it does this um yeah are there any questions until now if I go into just like this new benchmark cool so yeah the cogs Benchmark is a compositional generalization from semantics uh Benchmark or something right it kind of checks if if language models can uh can can do new words structure combinations right so so the the task at hand is semantic interpretation this is I kind of glossed over it before but it's like if you have if you have a sentence right like the girls saw the Hedgehog you have this idea that like and you've seen what like saw is a function that takes in two arguments and it outputs that the first one saw the second one you know this is like a bit of like you know um this is like one way of thinking about semantics there's many more as we'll see but you know this is one and so like and so and so you can make a little like kind of Lambda expression about uh you know about uh how how how you know what the sentence means and to get that you kind of have to use the the the tree to get it correct but um anyway the the specific mechanism is not very important but it's like the semantic interpretation where you take the girl saw the Hedgehog and you and you add put this like function of like you know C takes two two arguments you know first is a girl second is the head job and then and then the training of the test set they have distinct words and structures in in different roles right so so so for example you know you you have things like Paula right or the Hedgehog is like always an object in the in the training data so when you're fine-tuning to do this task but then in the test data it's a subject right so it's like can can can you like can you can you use this uh word that you've seen you know in in a new kind in in a new place because in English anything that that that that's an object can be a subject you know with like some there's some subtlety around like some things are more likely to be subjects but yeah and then similarly you know if if you have something like the cat on the mat you know and it always appears so so this idea that that like a noun can go with like a prepositional phrase right but that's always always in the subject whether Emma saw the cat in the mat and then like can can you do something like you know the cat on the mat saw Mary right so it's like move that kind of structure to subject position which something that in English we can do right like any type of noun phrase that can be in an object position can be in subject position and so that and so that's the the cogs Benchmark you know large language models have an Asus yet I wrote This and like I was looking over the slide and I was like well we haven't checked the largest ones you know they never do check the largest ones for because it's really hard to like do this kind of more more like analysis work you know and things move so far it has like the really large one let's go T5 3 billion you know three billions like a large number it's maybe not a large language model anymore but um you know they don't Ace this right they're they're getting like 80 well like when they don't have to do the structural generalization when they can just like do like a test set which which like things appear in the same role as they did in training set they get like 100 easy it's not a very hard task and so you know this is like but still still pretty good you know and it's probably like if a human had never ever seen something in subject position I'm not sure that it would be like 100 as easy as if they had you know like I think that you know we don't want to fully idealize how how things were were working humans right similarly you can take literal Jabberwocky sentences right so uh so so build building on some some work that John did then I'm just gonna talk about later so I'm not going to go in but maybe I'm wrong on that assumption right we can like kind of test the models like embedding space right so if we go high up in the layers and test the embedding space we can test it to see if it encodes structural information right and and so we can test you like okay is there like a a rough representation of like syntactic tree relations in this uh latent space and uh and and then these um yeah and then a recent paper asked does this work when we introduce new words right so if we so if we take uh you know if we take like Jabberwocky style sentences and then ask can the model find out these the the trees and these in its latent space does it like uh uh encode them and and and and the answer is you know like it's kind of worse you know in in this graph the uh the hatched bars or the ones on the right are the Jabberwocky sentences and the um and the and the clear ones or the not hatch ones I guess are are the ones are are the normal sentences in which you know performance is worse you know so this is like unlabeled attachment score on the y-axis it is like you know first probably worse in humans right it's easier to read a normal poem than to read Jabberwocky so you know the extent to which this is like damning or something you know is that I think very very small I think the paper is I have linked it there but you know I think the paper is maybe a bit more um um uh sure about this being a big deal maybe then it is but yeah you know it it does show that that this kind of process isn't um trivial yeah it's like applies for walking substitutions oh so this is um this is uh something called like phono tactics right so so in uh I think like this is probably around kind of what you're asking is that it's like you want a word which sounds like it could be in English right like Pro like provocated right it's not that can't be in English you know classical example you're like Blick it could be an English word you know Nick can't right we can't start sentence with bien and that's not like an impossibility of like the mouth right it's like you know uh similar things like you know pterodactyl pneumonia you know like these come from like Greek Greek words like I can say them I'm a Greek native speaker like PN and PT I can put them at the beginning of a syllable you know but like in English they they they don't go and so like if you kind of follow these rules you know and like kind of also add like the correct like suffixes and stuff right so like proud Paul vacated we know is like past tense and stuff then then yeah then you can make kind of words that that don't um exist but could exist and so they don't like throw people off this is important for the tokenizers right you don't want to do something like totally wacky to to to to to test the models but um yeah so when you generate um like this test set like with these Java Rock substitutions are these words generated by like a computer or like is there a human like coming up with words that sound like English but all the time there's some like uh there's some like databases that you know people have like thought of these and like the I think they get theirs from some like there's some list of them you know because if you have like 200 that's like enough to run this test because it's like a test but uh but um yeah I mean I think that you know the phonotactic rules of English can be actually laid out kind of Simply you know it's like you you know like PTU like can't really have like two stops to get you know it's like they're both like the same you can't really put them together you know it's like you you can probably make like a short program or like a long-ish program but not like a very super complex one to like make good Jabberwocky words in English yeah yeah wondering how the model would tokenize these Java Rocky sentences like would it not just Mount all these words like publicated just like the unknown difference so um um so so these are largely models that have like uh word wordpiece tokenizers right so they like kind of so if they don't know where they're like okay what's like the largest bit of it that I know and then like that's like a sub token right and this is how like most models work now it's like back in the day and this is like back in the day meeting like until like maybe like six or seven years ago it was like very normal to have tokens like unknown tokens but now generally there is no such thing as an unknown right you put like kind of at a bare minimum you have like the alphabet in your vocabulary so I got a bare minimum you're splitting everything up into like you know like letter by letter tokens character by character tokens but um if you're not then um yeah it's it should um yeah it should find kind of like and this is why like the phonotactic stuff is is kind of is kind of important for this right that it's it tokenizes like hopefully in like slight slightly bigger chunks that have some meaning and like because of how attention works and how contextualization works you can like even if you have like a little bit of a word you can like give you know uh the the correct kind of attention to it once it figures out what's going on a few layers in you know for like a real unknown word for like a fake unknown word then you know cool I went back but I want to go forward cool any more questions about anything yeah it was uh like 80 20 scores that you were saying these are not um like this isn't myself probably yet I'm just trying to get a sense of what 80 means that context is like 80 of exact ly yeah it was it was exact I think like the relevant comparison is that what you didn't have this kind of structural difference you know where where like something that was sometimes a subject was like then like was like something which was like never an object was that an object you know the the like the the the the the accuracy on the on that test set is like 100 like easy and so and so and so it kind of there was no good graph which showed these next to each other they kind of mentioned it but uh yeah and so I think like that's like the relevant um piece of information that like somehow this like SWA swapping around of roles like kind of slightly trips it up that being said you're right like exact match of semantic Parts is kind of a hard metric you know and so and so it's not this is yeah none of this stuff and I think this is important none of this stuff is damning none of this stuff is like they do not have the kind of rules human have this is also I was like well there's a bit of confusion there's a bit of fusion in humans it actually gets quite a bit it gets quite subtle with humans and I'm gonna go into that in the next section too yeah overall uh sorry what is it yeah overall like I think the results are like surprisingly not damning I would say yeah this is the there's like clear clearly like you know maybe not the fully like programmed discrete kind of rules but yeah I would say cool uh another thing we could do uh yeah test how syntactic structure kind of maps onto like meaning and role right and so like uh as we said before right like in English a syntax of word order it gives us the who did what to whom meaning and so you know if we have if we have like you know for for any combinations like a verb and B if has something like a verb B we know like a is a do or B is the patient and so he has like is this kind of uh relationship you know um strictly represented in English language models as it is like in the English language and and so and so what we could do is that we could take a bunch of things which like you know appear in subject position a person would appearance object position and uh um and and put in and take their late late in space representation and uh and kind of learn learn you know learn learn like a little classifier you know this should be like a pretty clear distinction in latency in any like good model right like which like these models are good this should be a pretty clear distinction because it's like a linear classifier to kind of separate them right and the more on the one side you are you're more subject the more the other side you're you're more object right and and so then we we we can test you know does the model know um the difference you know be between when something is a subject and when some something is an object you know doesn't know that like you're going to go on opposite sides of of this um of this uh uh dividing line you know even if like everything else stays stay stay the same and all the clues point to just to something else right so it's like this index map on to roll in this way you might think like well I could just check if it's like second or like fifth right but you know we've actually we we yeah this is a period that I wrote you know we did like compare you know we like try to control for like position stuff in in various ways and these are like yeah and and so it's hopefully we claim we're kind of showing like the like syntax to roll mapping and what we see is that it does right so so so if we kind of graph the uh the the the distance from that dividing line you know on the y-axis which is like the the like the original subjects when we swap them and put them in object position they do like diverge as we go up layers in that um in that Dimension and we tried this again you know all this analysis experience isn't kind of small models with some bird would so gpt2 you know it's like a bigger version of gpt2 and it worked out but but it's like you know none of this is like you know um none of this is like the big big stuff I think now we're starting to see more analysis on the big big stuff I think it's really cool yeah uh so then where are we with like structure and language models right we know that uh we know that language models are not aren't they're not engineered around discrete linguistic rules but the pre-training process you know it isn't just a bunch of surface level memorization right we have seen this there is some kind of like the uh discrete rule-based system kind of coming out of this you know maybe it's not the perfect kind of thing you would like write down in a syntax class but you know there is some syntactic knowledge you know and it's complicated in various ways and humans are also complicated and that's what we're going to get to next right there's no ground Truth for how language Works yet right like if we knew how to fully describe English right with a bunch of good discrete rules we would just like make an old pipe Pi pipeline system and it would be amazing right if we could like take the Cambridge grammar of English but like it was truly truly complete if we just knew who knew how English worked we would do that and so we're working in this case where there's no really no ground truth cool any questions about this probably move Beyond syntactic structure cool so uh moving beyond this kind of like very structure-based uh idea of language and I think it's very cool to learn about structure in this way and like at least how I was taught Linguistics it was like a lot of it the first like many semesters uh uh was like this kind of stuff um but then but but I think there's like so much more and and and like very important I think that meaning plays a role in in linguistics structure right like there's a lot of Rich information in words that that affects like the final way that like the the syntax works and of course what like end up meaning and like what like the words influence each other to mean right and so like the the semantics of words right the meaning it's like always playing a role in forming and applying the the rules of language right and so you know for example like a classic example is like you know verbs they like have kind of like selectional restrictions like eight can like take kind of any food and it can also take nothing I was like I ate it means that I've just like I've eaten right devoured right the word devoured actually can't can't be used in transitively right it sounds weird you you need to to devour something right there's verbs like elapsed that only take like you know a very certain type of noun right like elapsed only takes Downs that that uh that uh for the time you know so maybe like Harvest can refer to time Moon can refer to time some somewhere you know it's trees it cannot take over like trees right there's even verbs that only ever take one specific noun as their argument right it's like classic example um I think yeah my advisor Dan dendroski told me this one to put it in um and and what's cool is that like that that's how we train models these days if you see this uh this um diagram I screenshotted from John's Transformers lecture right we start with a rich semantic input right we start with these like a thousand on the order of like a thousand you know depending on the model size embeddings right which it's like think of how much information you can express like on a plane right on two Dimensions it's like the kind of richness that you can fit into a thousand Dimensions you know it's huge and we start with these word word embeddings and then move on right it's like the attention block and and everything and so yeah I'm just going to go through some examples of the ways that that languages you know the ways that like meaning kind of plays a role in forming syntax hopefully it's like fun a tour through like the cool things that happen in language right so as we said you know anything can be an object anything can be a subject we want to be able to say anything language can like Express anything this is like kind of a basic part of language but you know many languages they have a special syntactic way of dealing with this right so they want to tell you like if there's an object that you wouldn't expect right like in this case someone tell you hey watch out you know the be careful we're dealing with with a weird object here right so this is like kind of in the syntax of languages mode you know if you're if you're if you're a native speaker or or you've learned Spanish right you know this like ah constraint right so if you say like you know so if something is a um is an object but it's inanimate you don't need the ah because you're like yeah I found a problem but then if you're putting something adamant in the object position you need to kind of Mark it you need like hey watch out you know there's an object here and it's like a rule of the grammar right like if you don't do this it's wrong and they tell you this in Spanish class um similarly like Hindi has a kind of a more subtle one but I think it's cool right so you uh um to if if you put an object that is definite you have to mark it with a little like this is an object marker right like a little accusative marker right and like you might ask okay I understand why like animacy is uh is uh is is um is a big deal right like you know maybe animate things more often do things than have things done to them but like why why why definiteness right like why why would you need this little like call Marker just like the goat versus a goat and it's like well if something is definite it means that it's like it means that that it's like in the kind of in we've like kind of probably been talking about it or we're all thinking about it you know for example like oh I ate the apple right this means that either like we had one apple left and I ate it or like it was like really rotten or something you can't believe I ate it right or something like that and so like then things that we're already talking about they're probably more likely to be subjects right like if we're all you know you know if I was like oh Rosa you know yeah I feel like Rosa did this and Rosa did did that and runs then and then and and and then like Leon kiss Rose are you like no you probably want to be like Rosa kissing on right you probably want to put you know it's not straight but if you're talking about something you're probably it's probably going to be the subject of the next sentence so then if it's you have to put a little accusative marker on it so this is like how like the uh marking in the language works and it's kind of all influenced by this like interesting semantic uh relationship um and language models are also aware of these gradations and it's you know in a similar like classifying some subjects an object uh paper that that that that we wrote we see that language models also have these gradations right so if you like again if you like map the probability of being from that classifier on the y-axis right we see that there's a high accuracy right there's over many languages and all of them you know on the left we have the subjects they're classified above on the right we have the object are classified below but you know animacy kind of influences this grammatical distinction right so like if you're animate and a subject you're very sure if you're inanimate and an object you're very sure anything else you're kind of close to 50. you know and so it's like this this kind of uh this kind of um relation where the meaning plays into the um structure is it is reflected in language models you know and that's not bad it's good because it's how humans are or you know it kind of we should like you know temper our expectations maybe away from the like fully fully syntactic things that we're talking about um another kind of cool cool example of like uh of how meaning can influence you know what we can say what we can say I've said from the beginning many times that all kind of combinations of structures and words are possible but that's not strictly true right so in many cases if something is like too outlandish we often do just assume the more plausible interpretation right it's like there's these psycholinguistics experiments um where they kind of test this what's you know like these kind of these kind of like giving uh verbs is like you know the mother gave the daughter the candle and you could actually like switch that around you know you could do like so it's like the date of alternation but you can switch it around to make them the mother give the candle to the daughter and and then if if you if you switch around who's actually being given right so if you actually saying the mother gave the candle the daughter um people don't really um p p people don't interpret that like in its literal sense they usually interpret it as like the mother gave the the daughter the candle and like of course Outlanders me meanings you know they're never impossible to express right because nothing is right and so you you can like kind of spell it out you know it could be like well the mother they should pick up her daughter and she handed her to the candle you know who who is sentient and then you you could say this but you like can't you you like can do it simply with with the give word like like people tend to interpret it the other way it's like marking these like less prominent things and marking them sorry these less plausible things and marking them more prominently there's like pervasive feature that we say like across language in in all these ways and all these ways it's like you know also like very like embedded in the grammar as we saw earlier in Spanish and Hindi cool so another way that uh you know in where how we see meaning kind of play playing to to to you know and kind of break apart this like full compositionality you know syntax picture right is that meaning can't always be composed from Individual words right language is full of idioms you know sometimes we talk about idioms you you know you might think okay there's maybe like 20 of them you know things like my grandfather would say you know things about like chickens and donkeys in Greece they're all donkeys uh you know we're actually constantly using constructions that that you know that we couldn't actually get from like you know they're kind of like idiomatic in their little sense right that we couldn't actually get from like composing the words right things like I wouldn't put it past him he's getting to me these days that won't go down well with the boss you know there's like so so so many of these and it's kind of like a basic part of uh of communication to kind of use the these little like canned idiomatic phrases um you know and like linguist love love love saying that like oh any string of words you say is like totally novel you know and it's like probably true and I've been speaking for like 50 minutes you know and like probably no one has said this exact thing like ever before I just used the compositional rules of things to make it but actually most of my real letters is like oh yeah no totally right like something like that which is actually people say that all the time right most of my real utterances are like people uh say that all the time you know we have these little cat things that we love reusing and that and that you know we reuse them so much that like they stop making sense if you break them apart into individual words right and we even also even have these constructions that can like take arguments but like don't really uh you know so so they're not like canned words they're kind of like a canned way of saying something that you know doesn't really work if you build up from the syntax right it's like oh he won't he won't eat shrimp let alone like oyster right and what does that mean well it means like I'm defining some axis of like you know of like moreness right in this case probably like selfish and like shellfish and like weird or something you know and so it's like well shrimp is that sweet Source there's more you know and if I say like okay let alone beef right the axis like vegetarianism right so it's like this construction does like kind of like a a complex thing right where you're saying like he won't do one thing let alone the one that's worse than the dimension you know like it's like oh she slept after in the way he uh knitted the night away they drank the night away right it's like oh this is like time away thing doesn't actually you know you like can't really so otherwise you know like these like this er er construction like like the the the bigger they are the more expensive they are right like the and I forgot how it goes the bigger they come the harder they fall right like so it doesn't even have have to be a yeah and it was like you know that travesty of a theory right right like that other construction there's so many of these right like so much of how we speak if you actually try to like do like the three parts new like semantic Parts up up from it it won't really make sense and and so there there's there's been this work this is more more recent uh recently kind of come coming to light and I've been really excited by it there's texting constructions and large language models there was just this year uh paper by Kyle mahalwald uh who was a postdoc here um uh testing the like the a beautiful five days in Austin Construction right so it's like the a adjective numeral um noun construction where it's like it's like doesn't really work right because it's like it wouldn't really work right because so you have uh days right and there's like many ways you know and like anything kind of similar to it right like it's like a five beautiful days that that doesn't work right so somehow like this specific construction is like grammatically correct to us but like you know you like you can't say oh five days in Austin right because like uh five beautiful days you know you have to like this and they showed like gp3 is actually like largely concurrent concurs with humans on on these things right so on the uh on the left here the the gray bars we have a the the the the things that are acceptable to humans right so those are like uh a beautiful five five days in Austin and five beautiful days in Austin right those are both acceptable to humans they do this over like many many instances of this construction not just Austin obviously but uh yeah and we say like GB3 like accepts these you know those are the gray bars and humans also accept these though those are the green triangles and like every other iteration the human triangles are very low and gp3 is like lower but but but does get tricked by some things right so it seems to have this knowledge of this construction but not as like starkly as humans do right so the especially like if you see if you see that that that third one over there right the five beautiful days humans don't don't accept it as much it's funny to me it sounds almost better than those rest of them but I guess these green triangles were uh computed very uh uh robustly so I'm an outlier yeah and GB3 is like better you know like think thinks those are better than maybe humans do but there's this like difference there's like significant difference between the gray bars and the orange bars and then similarly some people tested the the x or the wire construction right and so it's like they took examples of sentences that that were like the x or the wire construction and then like they they they took um example centers which had like an ER followed by an ER but they weren't or but they weren't actually the actual the one right it's like oh the older guys how about the younger guys right so but so that's not an extra wire construction and uh and and you know and then they were like right if we Mark the ones that are as positive ones that aren't as negative it does the latent space of models kind of like encode this difference right that that like all this construction kind of clustered together in a way and they find that it does and then the last thing I want to talk about in this like semantic spacing after like constructions and all that is like the meaning of words is like actually very subtle and sensitive and it's like influenced by context and all these like crazy ways right and the Erica Peterson and Chris Potts from from the Linguistics Department here did this like great investigation on a uh you know uh on the on the ver on the verb break you know um and it's like that break can have all these meanings right like we we think it's like yeah break is like a word you know like words are things like table and dog and break that have like one sense but you know actually there aren't even senses that you can enumerate you know like Riverbank and financial bank and just like yeah you know break the horse means tame well like break a 10 bill it means like spread sweaters like smaller bits of money right and there's like so many ways right right like break free and break even they're just like so so many ways in which Break um you know like its meaning is just so subtle and influences like kind of Truth like every word you know or like many words maybe like table and dog it's like yeah there's like a set of all things that are tables or dogs and it's like kind of describes that set you know there's maybe some more philosophical way of going about it but you know so it's like pocket you know it's like a pocket but then like you can pocket something then like it kind of means Steel in many cases doesn't just mean put something in your pocket literally right this is like so yeah there's like all these ways in which um in in which like the meaning of words is like sadly almost by everything around it and and what they do is that don't don't worry about like what's actually going on here but you know they've kind of mapped each sense like a color right and then when when you start off in layer one they're all um I think this is just by like position embedding right you start off in layer one and it's just like I think that's what it is and then you like if you take all the words past pass them through like a big model like rubber large right then then they're kind of all jumbled up right because they're all just break right they're just like in different positions and then you know by by the end they've all kind of split up you say oh all the colors are kind of clustering together each color is kind of like one of a one of these meanings right and so and so they kind of clustered together and these like kind of is it constructions again or is it just like you know the way in which like they kind of isolate these like really subtle aspects of meaning um yeah so then I think a big question in NLP right is like how do you strike the balance between like syntax and the ways that like meaning influences thing right so well and I pulled out this quote from a book by John byney uh which I uh enjoy um and and I think it kind of bring brings light like a question that we should be asking NLP right this book is about it's like just like a Linguistics book it's not about LP at all but you know I think while language is full of both broad generalizations and items big properties linguists have been dazzled by the Quest for General patterns right that was the first part of this talk you know and like of course the actual structures and categories of language are fascinating but you know I would submit or she would submit that what is even more fascinating is the way that the general structures arise from and interact with the more specific items of language producing a highly conventional set of general and specific structures that allow the expression of both conventional and novel ideas right it's kind of like this like Middle Ground between abstraction and like specificity uh that like we would want you know that like humans probably exhibit that would want our models to to exhibit yeah I was wondering you could go back one slide and just unpack this diagram a little more because I'm fairly new to NLP I've never seen a diagram like oh sorry yeah what what does this mean how should I you know interpret oh so this is all like um you know so if you take you know the way that that that that like uh words are you know as you're passing through a Transformer through through many layers I just wanted to be like look at how the colors Cloud cluster but uh yeah and you pass them through a Transformer many layers at any one point in that Transformer you could like say okay how are the words organized now you know and you think well I'm going to project that to two Dimensions from like a thousand and that's you know maybe a good idea maybe a bad idea I think there's a lot of but you know I wouldn't be able to show them here if there were a thousand so let's like assume that that's like an okay thing to be doing um then then you know so this is what they've done for like for layer one and then for layer 24. and so we could see that that like they they start off where like the colors are totally jumbled and they're probably you know in before layer one you add in the position embedding so I think I think that that's what all those clusters are right so it's like kind of clustering because you don't have anything to go off of you know it's like this is break and it's in position five it's like okay I guess I'll cluster all the bricks in position five right but then as you go as you as you go uh up up the model right and kind of like all this meaning is being formed you see these like senses kind of like come out uh in the um in in how it organizes things right so it's like all all these like breaks kind of like become they're very specific you know they're very like kind of subtle versions of breaks now there's like this working I think it's different from a lot of NLP work because um it has like a lot of Labor put into this labeling right like this is like some something because because uh you know the person who this is a linguistic student right if you like go through Corpus and label every Break by like which one of these it means it's like a lot of work and it's like yeah and so I think it's the kind of thing that you wouldn't be able to show otherwise so it's often not really shown yeah cool so yeah language is characterized by the fact that it's just amazing the abstract system right I started off raving about that and you know and we want our models to capture that that's why we do all these compositionality kind of syntax tests you know but meaning is so rich and multifaceted right so high dimensional spaces are much better at capturing these these subtleties right we started off talking about word embeddings in this class right you know High dimensional space are so much better this than any rules that we would come up with being like okay maybe we could have like break subscript like break money you know and we're going to put that into our system and so where do deep learned learning models where do they stand now right between surface level memorization and abstraction you know and this is what like a lot of analysis and interpretability work is trying to understand you know and I think that what's important to keep in mind when we're reading and kind of doing this analysis and interpretability work is that this is not even a solved question for humans right like we don't know exactly where humans stand between like having an abstract grammar and having these like these like very like construction specific and meaning specific ways that that like things work by cool any questions overall on the importance of semantics and the richness of human language yeah this is a funny question from quite a bit before but um he's showing a chart from your research but um the model was really really well able to distinguish anonymous given its knowledge of subject or object I was just trying to interpret that crap and understand what what the sort of links between them no no it's not that I think it's here right yeah yeah so so the main so this is similar to the other graph where it was um you know where what it's trying to to distinguish is a subject from object but we've just split the test set into these four ways right it's been like subject inanimate subject animate some you know so we just split the test set right and so like what the uh what like the two panels and the x-axis are showing are like these different splits right and so like okay so things that are subject and basically the ground truths the things on the left should be above 15 things on the right should be below 50. and that's what's happening but if we further split it by animate and inanimate we see that there's this like influence of of intimacy on the probability that was a yeah sorry I rushed over these graphs like kind of I want to give like a taste of things that happen but yeah it's good to also understand fully what's going on it's cool yeah yeah so you were talking about acceptability so um I'm assuming for a judging acceptability means you just asked but first for like gpt3 like how do you determine if it defines a sentence acceptable I think logic so I think that's what Kyle Mahal did in this paper right you could just like take like the probabilities out put it at the end if you like mask you know if you like kind of for gp3 right it's like going left to right I think there's like other things that people do sometimes but like yeah especially for these models they don't have too much access to apart from like the like generation and like the like probability of each generation I think that you could yeah I think that that you might want to do that and there's like you know you don't want to multiply every larger together right because then like if you're multiplying many probabilities longer longer sentences you know become like very unlikely right which is like not true exactly for humans or you know it's not true in that way for humans so you know I think there's like things you should do like ways to control it and stuff like when you're running an experiment like this yeah cool Okay so moving on to um multilinguality in NLP so so far we've been talking about English right all this I haven't been saying it explicitly all the times but most things I've said you know apart from some maybe some differential object marking examples right they've been kind of about English about English models but there's so many languages right there's like over 7000 languages in the world where maybe not over there's around seven thousand languages in the world right like it's it's uh it's hard to to to Define right like what a language is right it's kind of difficult you know like even in the case of English right we have things like it's like Scots write the language book in Scotland is that English is like you know something like Jamaican English you know like maybe that's a different language right there's like the different structures but it's still like clearly like much more related than anything else right than like German or something right and so you know how how do you make a kind of a multilingual model uh well so far a big approach to me you know you take a bunch of languages this is like all of them and maybe you're not gonna take all of them you know maybe I think 100 or something and you just follow them and just like one Transformer language model and there's maybe things you could do like up sampling sumps they don't have too much data of you know or like down sampling something they have too much data of you know but like this is the general approach you know what if we just make one you know like one uh Transformer language model you know uh you know like something like a bird it's usually like a bird type model it's hard to get a good generation for for like too many languages you know but but yeah how about you get one from a language model for all of these languages right and so what's cool about this is that multilingual language models right they let us they let us share parameters between High resource languages and low resource languages right there's a lot of language in the world really just most languages in the world which you could not train like even like a bird size model for right they're just like not enough data and there's yeah there's a lot and there's a lot of work being done on this and one way to say like well you know like you know pre-training and transfer learning they brought us so much unsuspect uh unexpected success right and so like you know and and we get this great linguistic capability and generality right if we preaching something in English that we weren't that we weren't asking for so you know so will the self-supervised learning Paradigm you know can it like deliver between languages so it's like maybe I can get a lot of the um a lot of the like linguistic knowledge like the more General stuff from like just all the higher resource languages and then kind of apply it to the low resource languages right like a bilingual person doesn't have like two totally separate parts of their self right that like have learned language there's probably some sharing some way that like things are like in the same space like and Linguistics are broadly the same right and so and so and so you know we we have this like attempt to like bring NLP to like some still very small subset of the 7000 language in the world we can look at it through two lenses right on on the one hand you know languages are remarkably diverse so we'll go over some some of the cool ways the language in the world vary you know and so there's multilingual NLP capture the specific differences of of different languages on the other hand you know languages are similar to each other in many ways and so does multilingual NLP capture the parallel structure between languages so you know just just to go over some ways like you know really understanding like how like diverse languages can be you know in around this is a a quote from a book but um you know in around a quarter of the world's languages every statement right like every time you use a verb must specify the type of Source on which it is based right this is kind of like a part you know how we have like tense in English where we like you know kind of everything you say is like kind of either in the past or the present or the future tense right and so like an example in uh in tariana these are again from from the book right it's not a language I I know any right but it's you know you you have this like marker and bold at the end right and so and so when you say something like uh Jose has played football right you if you put like the car marker that means that we saw it right it's kind of like the visual evidential marker right and there's uh and there's kind of a non-visual market that kind of means we heard it right so so if you say you know so if you say statement you you could say we heard it right there's a like we infer it from Visual Evidence right so if it's like oh his like cleats are gone and he is also gone but like and people you know and we see people going to play football right or see people coming back I guess from playing football because in the past right that means like you know so so we can infer it and so you can put this right and there's like um uh you know or like if he plays football every Saturday you know and it's Saturday we you you would use a a different marker right or like um if someone has told you if it's here say you would use a different marker right so this is like a this is like a um a part of the grammar right that like to to me at least right like I don't speak any language that has this it seems like it's it it it seems like very cool and like different from like anything I would ever think would be like a part of the grammar right but it is um or like especially like a compulsory part of the grammar right but but it is right and you can like map out I wanted to include some maps from walls the world atlasive linguistic structure that's always like so fun um you know you could like map out all the language right like I only speak uh white dot languages which are like no grammatical evidentials you know if you want to say whether you heard something or saw it you have to say it like in words right but uh but there's many languages you know as um very yeah especially in uh in in in the the Americas right Diana's I think uh Brazilian language from like up by the border with uh oh yeah but uh yeah the you know while we're looking at like language typology Maps right and so like this this like language organization like in categorization Maps uh the the most like the classic one right is again like the the the subject object in verb order right so as you said English has SVO order but there's just so so many orders that uh you know kind of like almost all the possible ones are are attested you know some languages have no dominant order like Greek Greek so like a language that I speak natively has a dominant order you would say you would move things around for emphasis or whatever um um yeah and you see like and here you know we're seeing some some like diverse using typology we're also seeing some Tendencies right like some are just so much more common than others right and this is like again something which like people talk about so much right it's it's like a very uh big part yeah it's like a huge part of linguistic why are some more common where some others like a basic fact of language is something which happened you know it's just like just the fact like how discourse Works maybe you know like that's that's more preferred for many people to say something you know and there's a lot of opinions on this uh another way though that language is there you know it's like the number of morphemes they have per word right like some languages are like you know like Vietnamese classically just like very isolating like kind of like each um you know like each kind of thing you want to express like tense or something is going to be in in a different word you know in English we actually combine kind of tenses we have things like bubble right like you know like like throwable or something right and then like in in some languages they're just like really so much stuff is expressed in morphemes right and so you can have languages especially in like Alaska and Canada uh a lot of languages there in like Greenland where you have like um and these are all like one one language family um you you can have like kind of whole sentences expressed with just like things things that get tacked on to uh to to the verb right so you have to have things things like uh the you know like the object and the um or I guess in this case you start with the object again you have kind of like the verb and the like whether it's happening or not happening and who said it like what they're said in the future and all that just kind of all put in you know these like quote unquote like sentence words right it's like a very different way of a language we're working than English works like at all right um I just want to know like what these dots mean because like in the U.S the top right is gray like in the Northeast but in the Pacific Northwest it's yellow it's like is that different dialects for like the same American English oh no the visual indigenous languages oh I see yeah yeah so so English is uh just uh this one dot uh in here spread uh in amongst all the like Cornish and Irish and stuff oh yeah so English was like in Great Britain yeah yeah and that's why yeah that's why like all this like really and that's why I like all this like evidential stuff is happening in uh in like the Americas right because there's like a lot of very often the indigenous languages the Americas are like the classic like very evidentially marking uh ones which are the pink ones yeah you said that normally we use like a bird style model for multilingual models because it's difficult for natural language generation across languages yeah uh I mean I guess intuitively that makes sense right because of the subtleties and the Nuance between the languages when you're producing it but is there like a reason that um like a particular reason that's been so much harder to make developments on a good generation is just like harder right like to get something like you know like gpt3 or something it means like really like a lot of data and maybe like it's kind of like I think there are can I think of any are there any G sharp newshards encodone yeah I can't really think of any like you know like encoder decoder as you said you know like a kind of big multilingual models you know of course like gpd3 has this thing where if you're like how do you say this in French you'll be like you say it like this you know so it's like if you've seen all of the data it's going to include a lot of languages but this kind of like multilingual model where you'd be like right you know be as good as gpt3 but in this other language you know I think it just it's just you need a lot more data to get that kind of coherence right as opposed to like yeah as opposed to something if you do like text infilling or something which is like how the bridge style models are then you get like very good even if the text and feeling you know um uh performance isn't great for every language you can actually get very very good embeddings to work with for a lot of those languages cool now for just like a one last language diversity thing I think this is interesting interesting the motion event because it's like this is actually you know it's not it's like languages that you know many of us know I'm gonna talk about Spanish but uh it's actually something which you might not have thought about but then once you see you're like oh actually that's like actually it's like how like everything works um so in English right the manner of motion is usually expressed on the verb right so you can say something like the bottle floated into the cave right and so like the fact that it's floating is on the verb and the fact that it's going in is kind of on this satellite um well like in Spanish the direction of motion is usually expressed on the verb uh Greek Greek is like this too I feel like most Indo-European languages are not like this or actually like English so like most like language like Europe to like North India tend to to not be like this right and so you would say like right so you'd have like so used to so the floating has is not usually put on the main verb and like in English you could actually say like red like the bottle entered the cave floating it's just like maybe not what you would say right and and similar like in Spanish you can't say the other way around these are called like satellite framing language and verb framing languages like really affects how you would kind of like say most you know like kind of how everything works right it's kind of like a division that's like you know pretty attested of course it's not a full division right it's not like this exclusive categorization China uh Chinese I think off often has you structured where there's like two verb slots right where where you could have both a manner of motion and a direction of motion kind of in the in the like the one verb verbs thought none of them have to go kind of like after playing some some some different role right so these are like you know there's all these ways in which like languages are just different you know from like things that that maybe we didn't even think could like be in a language like things are like we do right but we don't realize that in some some sometimes you're just like so so different in these like subtle ways and so uh you know and so going to the other end your language is so different they're also very alike right so like you know there's there's like you know this idea like is there like a universal grammar some like abstract structure that all that unite all languages right this is like a huge question Linguistics and you know the question is can we Define an abstraction where we can all say like all language or some part version of it there's like other ways of thinking about universals like all languages like tend to be one way or tend to be like languages that tend to be one way also tend to be some other way and there's like a third way of thinking about um a universals that's like languages all deal in similar types of relations you know like subjects objects you know like types of modifiers right like the universal dependencies project was like uh um uh a way of kind of saying like maybe we can make dependencies kind of for all language the way that doesn't shoehorn them into each other you know and uh yeah I guess like what was it called rrg like relational something grammar you know was also kind of this idea that maybe one way to think about all languages together is like the kind of relations they Define you know and um and you know ask me ask me about kind of like the chomp skin and the Greenberg and stuff you want and how it relates to NLP I think like there there's a lot to to say there it's kind of yeah it's it's slightly more difficult so maybe it's easier to think of the of this third one in terms of NLP right and like back to the subject object relation stuff if we look at it across languages right we see that they're kind of encoded in parallel because classifiers right those classifiers that we're training they're like as accurate in their own language as they are in other languages right their own language being um red and other languages being black right it's not like wow if I take a multilingual model and I train these classified induction one classify in one language it's like going to be so good at itself and like so bad at everything else right they're kind of interspersed they're clearly like on the top end the red dots yeah and uh and UD relations right so Universal dependencies like the kind of like dependency relations they're also encoded in parallel ways this is this work that uh John John has done right again main thing to take from from this example is that like the colors clustered together right it's like if you if you train kind of like a parser on or like you know parse uh classification on one language and kind of transfer it to another you see these clusters form for the other language right so it's like these ideas of how like things relate together right like a like kind of noun modifier you know all that kind of stuff they they they do um clustered together in these parallel ways across across languages you know and so language specificity is also important um I might skip over this but you know it seems with like so maybe sometimes some languages are shoehorned into others in various ways uh and maybe part of this is that data quality it's very variable in multilingual corpora right it's like so so if you take like all these multilingual corporate there's like an audit of them and like for like all these various like multilingual corporate like 20 of languages they're less than 50 correct meaning like 50 of it was often like just links or like just something random I was like yeah that might be like some language but but it was not at all and and like maybe the way maybe we don't want too much parameter sharing right like uh Africa is a reason uh it's a kind of recent Bird model trained like only on African language you know maybe like having too much too high risks too high resources like harming you know and there's work here at Stanford being done in the same direction you know another uh uh yeah another a recent cross-lingual model xlmv came out which is like why should we be doing vocabularies sharing you know like you just have like a big vocabulary each language gets like its own words it's probably going to be better and it is it kind of like knocks out similar models or smaller vocabularies which are like maybe you know computers the same in English and French it should be shared you know maybe it's better to separate out things you know it's like hard to like kind of find this balance between let's keep over this paper too it's very cool and there's a link there so look at it but yeah we we want language generality but we also want to preserve diversity and so how is multilingual NLP doing you know especially with effects like dialects you know there's so many complex issues for multilingual NLP to be dealing with how how can deep learning work for low resource languages you know what are the ethics of working NLP for low resource languages who like wants their language in in big models who like wants a language to be translated you know these are all like very important ethical issues in multilingual NLP and so after looking at structure Beyond structure multilinguality in models um I hope you've been yeah but I hope you know that Linguistics is a way of you know investigating what's going on in Black Box models uh the subtleties linguistic analysis they can help us understand what we want or expect from the models that we work with and like even though we're not reverse engineering human language uh linguistic insights I hope I've convinced you they still have a place in understanding you know the models that we're working with the models that we're dealing with and you know and in so many more ways beyond what what we've discussed here you know like uh language acquisition language and vision and like instructions and music uh discourse conversation and communication and like so so many other ways cool thank you if there's any more questions you can come ask me time's up [Applause] 
","['', 'Linguistics and NLP', 'Large language models', 'Structure in human language', 'Signed languages', 'Infinite communication with finite brain', 'Subtrees in language', 'Passive alternation', 'Implicit knowledge of structure', 'Question formation', 'Syntax to roll mapping', 'Big language models and syntactic knowledge', 'Ground truth for how language works', 'Semantics and human language', 'High dimensional spaces for meaning', 'Surface level memorization vs abstraction', 'Encoder decoder models for multilingual NLP', 'Multilingual model challenges', 'Direction of motion in Spanish', 'Language specificity in NLP models', '']"
"welcome to cs224n lecture 15. so i'm megan and i'm one of the cities in this course and i'm also a phd student working with chris ray and today i'll be talking about integrating knowledge and language models so some quick reminders your project milestones were due today so hopefully you turn those in already or we'll be turning them in the next couple of days and we'll try to get feedback on those as fast as possible so something to be aware of is a change of grading basis and course withdrawal deadline is this friday um so if you want to make any change your grade make sure to do that by then and we'll be getting you the grades back on assignment five by then as well in case that's helpful in making your decision and finally your final projects are due in two weeks so hopefully those are going smoothly so the topic of the day is integrating knowledge and language models you've seen a bit about this idea in assignment five and also in colin raffle's lecture last class so in assignment 5 the task was to train a model to predict the birthplace of a person given their name and you saw it by pre-training on a larger data set you're actually able to do better on this task since you could encode some world knowledge into the language model and then last lecture colin raffle presented how t5 could actually be fine-tuned for a closed domain question answering task such that you can give t5 a natural language question and it'll return an answer so they will be building on these threads and looking at techniques that researchers have recently been developing to increase the amount of knowledge in language models so we're going to start with a quick recap of language models just to make sure we're all on the same page then we're going to talk about what types of knowledge language models can already encode and what they might struggle on we'll also motivate why researchers are interested in increasing the amount of knowledge in language models and what this could enable for future ai systems if we have language models that can actually reliably recall knowledge we'll talk about three broad classes of techniques that researchers have been using to add knowledge to language models these include adding pre-trained entity embeddings using external memory or key value store or even just modifying the training data and for each of these techniques we'll talk about at least one recent work that used the technique so hopefully it's clear to see how to actually employ it in practice and then finally we'll wrap up by talking about how to evaluate the knowledge in language models and the challenges that come up in trying to do this so let's dive right in we're going to start by talking about standard language models you learned about these at the beginning of the course and the task is to predict the next word and sequence of text and to compute the probability of a sequence so you may remember the example that students opened their blank and we talked about could be minds exams bring both books here and the task of the standard language model is to predict the most likely next word in the sequence a couple lectures ago john also introduced the notion of mass language models instead of predicting the next word in a sequence of text the tasks predict the mass token and this is done using bi-directional context so you may remember the example i masked the mask and the goal the mass language model is to make the most likely token for each of the masked out words so maybe i went to the store so while there's some differences in these two types of language models whether you're predicting the next word or whether you're predicting the massdot token they're similar and that they can both be trained over large amounts of unlabeled text and this is one of the reasons why they've been so widely adopted they don't require any human annotated data so you've seen that language models can be used for a variety of tasks from summarization to dialogue to fluency evaluation tasks that involve either generating text or evaluating the probability of text and more recently we've seen that language models can also be used to generate pre-changed representations of text that encodes some notion of language understanding and has been shown to be widely useful for different downstream nlp tasks and then finally today we're going to touch on this idea that if language models are trained over massive amounts of text can they even be used as a knowledge base so we're going to start by looking at what types of factual knowledge a language model might already know and these examples are taken from a paper by petroni at all in emlp a couple years ago and the goal is to test the factual or common sense knowledge in existing language models such as bert large so let's check out what bert large predicts ipod touch is produced by apple london jazz festival is located in london uh danny alves plays with santos carl iii used to communicate in german and ravens can fly so here we have the correct predictions in green and the incorrect predictions in red and if you know anything about sports you may know that danny alves is a soccer player santos is a soccer team here they were hoping that it would predict barcelona because at least at the time of this data set apparently he played for barcelona and carl iii actually used to communicate in swedish not german so what's good about these examples is the predictions are generally reasonable if you didn't know the ground truth they all make sense when you want to produce a when you want to predict a language you do in fact predict a language but of course they're not all factually correct so why might this happen well for one the fact might not been seen in training and you can't expect the language model to do more than recall facts that it has seen in training it can't make up facts about the world for instance it's also possible the fact is just really rare so maybe the language model has seen the factoring training but it hasn't seen it enough times actually memorize the fact and the last issue is a little more subtle which a model might just be very sensitive to the phrasing of the fill in the blank statement and so for example you might have statements like x was created in blank that the model can't predict correctly but if you change it to x was made in blank suddenly it can predict it correctly and we'll come back to this in how to actually evaluate the knowledge in these language models so this inability to reliably recall knowledge is a key challenge facing language models today that'll be the focus of this talk recent works have found that language models can recover some knowledge including the work that colin presented last class they've had very encouraging results but there's still a way to go as we saw with the fill in the blank statements and with these challenges that we just discussed above so as a result the past couple years have had ton of rapid progress in this area of research in terms of trying to figure out how do you actually encode more knowledge in language models so i also want to motivate why researchers are interested in building language models that can more reliably or call knowledge and one of these reasons is that the pre-trained representations are used in a variety of downstream tasks and some of these downstream tests are knowledge intensive so for instance you might have a downstream task to extract the relations between two entities in a sentence and this is commonly known as relation extraction and this is much easier if you have some knowledge of the entities which could be potentially provided by this pre-trained language model representation and we talk about evaluation we'll talk about what types of tasks are most likely to benefit from these knowledge rich pre-trained representations and then as a stretch goal some researchers are starting to propose the idea that can language models actually ultimately be used to replace traditional knowledge bases so instead of creating a knowledge base for a fact like you might right now with sql you would create a language model with a natural language prompt and of course this does require the language model to have high quality under calling facts so we might not be there yet but it's an interesting direction for us to be moving towards so i want to make it super clear what i mean by a knowledge base here we're just talking about a knowledge graph where the nodes in the graph would be entities and the edges are going to be relations between the entities so for example here we have a subset of a knowledge graph for franklin d roosevelt and you see the information about his spouse his place of birth his date of birth and so on an important thing to note is it's a structured way of storing the knowledge since it's just in a graph form and you can actually describe these graphs with knowledge graph triples which will be an important vocabulary word throughout this talk so knowledge graph triple would be um consisting of a subject entity a relation and then an object entity so for instance here we might have franklin d roosevelt date of birth january 30th 1882 and that would form a knowledge graph triple we'll also refer to this as a parent entity a relation and a tail entity so wikidata is one very popular knowledge base you might come across if you're working this area it's a free knowledge base that's actually populated by humans so they're filling in these relations and entities and it's also multilingual so if you want information from this knowledge base what you do is write as you would write a sql query this is a simplified one but the idea is you'd want to figure out the date of birth of franklin roosevelt so you would write a query like follows now if instead you want to query a language model as a knowledge base you'll have something like this diagram that you've actually probably seen in several lectures now and the idea is you'll train a language model over this unstructured text and then you'll use a language model to just answer these natural language query statements so here this is the work on t5 where they're training t5 over natural language or just unstructured text with a span corruption task and then they're asking t5 when was franklin d roosevelt born and the idea is t5 will produce a textual answer so you can see this contrast very much with the old approach of using a traditional knowledge base where the knowledge base is structured and you have the sql statements to query it so what are the advantages of using language models over traditional knowledge bases and why might people think this this could be a good idea well for one the language models are pre-trained over large amounts of unstructured and unlabeled text whereas traditional knowledge bases require manual annotation like with wikidata people actually populating it or complex nlp pipelines to extract from unstructured text into a structured form that forms a knowledge base language models can also support more flexible natural language queries so if we take the example what does the final f in the song ufo f stand for a knowledge base probably won't have a field for final f so it won't be able to answer your query but there's a chance that a language model could actually learn and have a response for this natural language query they also had a less extreme example in this paper by petroni and others where maybe your relation would be is works for in your knowledge base and then you ask for is working for and the knowledge base doesn't have an exact match on the field and so it returns an empty response and it's much it's reasonable to believe that your language model could figure out that these relations are similar so if i know the answer to one of them i probably know the answer to the other of course it's not all advantages there's also many open challenges to using language models as knowledge bases so for one it's harder to interpret when a traditional knowledge base produces an answer there's actually provenance information associated with why did it return that particular query but with a language model it's really not clear why it might produce a prediction the knowledge is just encoded in the parameters of the model it's also harder to trust so you saw this in assignment 5 where the language model could produce realistic predictions but they are incorrect so it's not easy to know when the language model actually knows the fact versus it's using some like biases to make its prediction and in the case of the traditional knowledge base if it doesn't know a fact it's just going to have an empty response and then finally knowledge bases or language models are harder to modify so in a knowledge base if you want to update a fact you just change the fact directly in the structured data but in a language model it's not quite clear how you would do this you could fine-tune the model longer on the updated data but how do you know if it still has some memorization of the old fact so there are a lot of open challenges to this goal of actually using language models as traditional knowledge bases but hopefully you see why some people think this could actually be a good idea and why researchers are interested in training language models that can actually integrate more knowledge so that brings us to section two of the talk so i want to pause here just in case there's any questions okay i think that's okay yeah okay awesome um so now we're going to be talking about what techniques researchers are using to actually add more knowledge to language models so we're going to talk about three broad classes of techniques this is by no means exhaustive but hopefully it gives you a good overview so that if you want to dive deeper you can so we'll start by talking about adding pre-trained entity embeddings and for each section we'll kind of focus on the first work that you see in the bullets but we'll also talk about briefly some of the variants so you see how the works within each class can differ and what knobs you can turn so for adding pre-trained embeddings we first need to figure out what pre-trained embeddings would actually be the most useful to add knowledge to language models and this can start with an observation but facts about the world are usually in terms of entities so if we have a fact like washington was the first president of the united states we have the entities washington united states but pre-trained word embeddings don't have this notion of entities so we'd have different word embeddings for usa united states america and america even though these all refer to the same entity and this makes it challenging for the language model to actually learn any representations over these entities since they may be referred to many ways in the text so what if instead we have a single embedding per entity and we'll refer to these as entity embeddings so now you'd have a single entity embedding for usa united states of america and america and whenever you see a phrase in text referring to this entity you would use the same entity embedding and these entity embeddings can actually be pre-trained to encode this factual knowledge about the world and this first class techniques we'll be looking at would be how do you actually best use these pre-trained entity embeddings in a language model so i need to make a quick note that these entity embeddings are only useful to language models though if you can do another nlp task called entity linking well so i'm going to take a quick aside and explain what is entity linking so the definition of entity linking is to link mentions in text to entities in a knowledge base i like to think about this in terms of how you use word embeddings so if you want to use word embeddings and you have a sentence you're going to first tokenize that sentence into words and then for each word you'll look up their corresponding id and some word embedding matrix and now you have your word embedding well for entity and bettings the dictionary lookup isn't so easy you might have sentences like washington is the first president united states well washington has two different candidates are we talking about george washington or are we talking about washington state and these are different entities that have different entity embeddings and the queue ids here would just be their identifiers and wiki data and then united states just has a single entity so task of entity linking is to figure out correctly these ambiguous mentions what entity do they actually link to in a knowledge base and there's many different ways you can do this entity linking so one way you might be able to do this is to figure out that oh i see the context word of president so washington probably links to george washington just some more definitions we're going to refer to washington as i mentioned the united states as i mentioned and then the things that the mention could link to so the two options for washington are going to be candidates so this is a whole research area of its own and i encourage you to check out the resources at the bottom if you're interested in learning more but right now the most important thing to understand is that entity linking is what is going to tell us which entity embeddings are actually relevant to the text and which ones you want to use as you iterate through a sequence megan there are a few a few questions around here one of them is so that's entity linking but what about the relations um yeah so some of the works we'll talk about will only use the entity embeddings um so some of these have been pre-trained with relation information but in the end you only have an entity embedding so relation extraction is yet another nlp test you could also do but yeah here we're just talking about nc linking but if you have the knowledge graph you showed earlier it had relations in it right do you get any connection between that and the text um i mean that's the goal of relation extraction right is to figure out like given the entities what is relation between them which would then form the full triple of head entity tail entity and relation um okay then i think people want to know more about house it's going to be used but maybe you should go on and show some examples yeah i will for sure okay um right so entity embeddings just to summarize they're like word embeddings um but they're for entities in a knowledge base so you'll have some vector associated george washington and it should be meaningful in embedding space such that maybe the george washington vector is close to the vectors for other founding fathers so we're going to briefly talk about some methods for a training entity in betting's there's knowledge graph embedding methods you might have heard of the transient betting method so this starts from the idea of having these knowledge graph triples and you want to learn pre-trained entity and pre-trained relation embeddings and you want to be the case that the subject embedding and the relation embedding the sum of those two is close to the object embedding in vector space so it's an algorithm to learn that constraint there's also word entity coccurrence methods so these build off of word to back one of them is even called wikipedia to back and the idea is given an entity you want to figure out what words are most likely to co-occur around it and then the last method or one of the other methods that is common now is actually just using the transformer to learn representations of an entity by encoding the entity description and so blink from facebook is a an approach that does this so the methods we'll talk about today are actually agnostic to how you train your pre-trained entity embedding but i think it's important to know that there's actually a wide variety of methods to train these preaching entity embeddings and it's actually not clear which method is best for using them downstream and language models so one of the key challenges of using pre-trained entity embeddings in language models is figuring out how to incorporate them when they're from a different embedding space than the language model and so we'll do or the approaches we'll look at today we'll learn a fusion layer to combine this context and entity information so we have entity embeddings and we have the contextualized word embeddings from our language model so if we take a sequence of text and we imagine that j indicates the j element in a sequence then the challenge here is we want to figure out how do we combine some word embedding wj with some aligned entity embedding ek so here an alignment could be like in the example where we had washington was the first president washington would be your word embedding and george washington would be the aligned entity embedding there so you could imagine in this case let's say your wj is washington and your ek is your entity embedding for george washington and you want to align them together so what you can do is learn a weight matrix wt for the text and w e for the entity to project these embeddings to the same dimension before you sum them and finally take an activation function over them so the idea is that by having some fusion layer mechanism like this you can actually use these entity embeddings and these contextual word embeddings that are in different embedding spaces and fuse them together to have this single hidden representation for the element in the sequence so the approaches we'll talk about today all have some mechanism either very similar to this or some variation of this to do this combination of the context and entity information so the first approach we're going to talk about is called ernie enhanced language representation with informative entities and so this just builds on what we've already talked about it uses pre-trained entity embeddings and it also uses this notion of a fusion layer so the first block in ernie is a text encoder which is a multi-layer bi-directional transformer encoder for their experiments they use bert but it doesn't have to be burnt and this is followed by a knowledge encoder which has stacked blocks composed of two multi-headed attentions one is over the entity embeddings and one is over your token or subword embeddings and then the output of these contextualized entity and token embeddings from the multi-headed attentions are passed to a fusion layer which looks very similar to what we just looked at but now you also have new word and entity embeddings that you're producing uh as output of your fusion layer so you see this w j um and this e k which are produced as the next layer of word and entity embeddings so the i here indicates that it's the ith block in the knowledge encoder so you'll actually have multiple stacks of these knowledge encoders and you'll be doing a fusion of the word entity embedding producing new word and entity embeddings and then passing this to the next block of the knowledge encoder so this is what the architecture diagram looks like on the left side we have the t encoder or the text encoder followed by the k encoder or the knowledge encoder and then on the right side we have a zoomed in version of your knowledge encoder so you see the multi-headed attentions over the tokens in orange and then over the entities in yellow and then you have this alignment between the word and entities with the dashed lines so they have this example as bob dylan wrote blowing in the wind in 1962 the entities here are bob dylan and blowing in the wind and they have a simple alignment rule where you want to align the entity to the first word in the entity phrase so you want to align bob dylan to bob that's what the dash line's trying to indicate and you want a line blowing the wind to blow so here this already assumes entity linking has been done and you know your entities in advance so you can see that the entities are actually input into the model so after you have your word nt alignment this goes to the information fusion layer and this light purple gray color and then finally it produces these new word entity embed things as output and then remember that you have multiple blocks of these so those will be passed into the next block of your knowledge encoder so how do you actually train this it's pretty similar to bert you have a mass language model loss and you have a next sentence prediction loss and they also introduce a knowledge pre-training task which they refer to as the dea task it's named after a denoising entity autoencoder from an icml paper in 2008 and the idea is they're going to randomly mask these token entity alignments so the idea that bob goes to bob dylan they're going to mask that out with some random percentage and then they're going to predict the corresponding entity for a token out of the entities in this in the sequence so this looks like as follows the summation is over m entities in the sequence so this would be over bob dylan and blowing in the wind in the previous example and given a particular word they want to figure out um what entity is it most likely to align to in that sequence so does bob align to bob dylan or does bob align to blowing in the wind and their motivation for doing this is that if you don't have this task all you're ever going to be predicting is the token with the mass language model loss and you really 10 code knowledge should also probably be predicting over entities so by adding this task they have some kind of task that is actually predicting the entity and they also suggest that this might better fuse the knowledge or the entity and the word representations than just using the fusion layer their final loss is then the summation of the mass language model loss the next sentence prediction loss and this dea knowledge pre-training task loss so they showed an ablation experiment that it's actually very important to have this knowledge pre-training task so this has um bert on the left-most bar ernie as the second bar from the left and so that's with all the features of ernie and then they try removing the pre-trained entity embeddings and removing this knowledge pre-training task so you see that bert performs the worst this isn't very surprising and that ernie performs the best but what's interesting is that if you remove the entity embeddings or you remove the pre-training task they only do a little better than bert and so it's really necessary to actually use this pre-training task to get the most use of your pre-trained entity and bettings so some strengths of this work were that they introduced some way to combine entity and context information through this fusion layer and this knowledge pre-training task and then they also show improved performance on downstream tasks which we'll come back to when we talk about evaluation but of course there's also some limitations so it needs text data with the entities annotated as input and this is even true for downstream tasks so if you remember on the architecture diagram we had the entity entity information actually input into the architecture but it's not very realistic that you're necessarily going to have a good entity linker for any downstream tasks that you want to use ernie on and the next challenge is this requires more pre-training of your language model so now you don't just need to pretend burt but you also need to pre-train your knowledge encoder on top for the first challenge we're going to actually talk about a work that presents a solution to address this for the second challenge i encourage you to check out the footnote on the bottom this introduces a work that actually uses pre-trained entity embeddings uses them in a language model and doesn't require any more pre-training so it's pretty cool uh i guess that's all i have for ernie so i want to pause here for questions well here's one that's up here so on the fusion layer it has it observed that passing the entity embedding into a fusion layer to combine with word embedding is more powerful than just concatenating the entity embedding onto the end of the word embedding question mark yeah so i guess people are still a little bit confused as to the motivation for that fusion layer and so i guess here is this the simplest strategy would be since you've got the entity linking you could just concatenate entity embeddings onto the end of word embeddings and do regular would that work just as well um i think the idea is it wouldn't because if you imagine that let's say your magnitudes are very different um you need some way to i guess align the spaces so that anything meaningful in the entity embedding space is still meaningful in the word embedding space so if you're close in the word embedding space you also would be you'd want to be close in entity embedding space so i guess that's one argument yeah i mean i mean i think the question isn't you know it's a good question as people say i mean it's not completely obvious that it wouldn't work to do that it seems like one of the potential problems is some words have entity links to them and some words don't and so you then you'd sort of have zero vectors for the ones that don't have anything that's a good point linked and that might act a bit weirdly but yeah in this case when they don't have entities linked which is a great point um yeah the first equation just simplifies to the first term plus the bias so like there's an obvious solution in that case when you're not concatenating that you just don't add on the term um yeah that could be one reason too okay are there any other questions [Music] i think you can go on okay cool um right so now we're talking about novert and this is from the same folks that introduced the elmo work and the idea here is that they're going to pre-train an integrated entity linker as an extension to bird and so their loss function will now be the summation of the next sentence prediction the mass language model loss and this entity linking loss so instead of the knowledge pre-training dea task from ernie we'll have an entity linking loss and the idea of the entity linker is you'll now have just as normal sequence as input and the integrated empty linker will figure out what are the entities in the sentence and um or what are the mentions in the sentence what are the candidates of those mentions and then what should be the scores those entities or the candidates given the context of the sentence and so this is all done now as part of the model rather than requiring it as some external pipeline stage before you could even use ernie for instance so now for downstream tasks you no longer need these entity annotations your integrated anti-linker will figure out what the correct entity is and be able to use the correct entity embedding so there's also this idea that learning is entity linking may actually better encode knowledge than this dea pre-training task because they show that nobody actually outperforms ernie on downstream tasks so one reason this may occur is that if you think about the the dea task it's actually a bit simpler than just entity linking so you're trying to predict for instance um what bob linked to out of bob dylan and blowing in the wind and it's much easier even as a human to see that bob dylan will more likely link to or bob will more likely link to bob dylan than that bob will link to blowing in the wind and anti-linking task you actually have a much harder set of candidates to predict over you're not just looking at the ones in a sentence so does washington link to george washington or washington state actually requires you using more information about the entity so given it's a harder task it's not too surprising that it might perform better than just this easier knowledge pre-training task that ernie introduced so otherwise nobert has a lot of similarities to ernie it uses a fusion layer that combines this context and entity information and it introduces some knowledge pre-training task so i'd say a high-level takeaway is if you want to use pre-trained entity embeddings in a language model you'll probably at least want to consider both of these components in terms of actually going to integrate the preaching entity embeddings and take the most advantage of the knowledge in them as possible so that brings us to the next class of techniques which is using external memory and here we'll mainly focus on this work called kglm and then we'll also briefly talk about k n lm so the previous methods that we've talked about have relied on pre-change entity embeddings to encode the factual knowledge from knowledge bases and the one problem with this or one of the problems with this is if you want to let's say modify your knowledge base you now need to retrain your entity embeddings and then retrain your language model on top of those entity embeddings so this begs the question are there more direct ways in pre-trained entity embeddings to provide the model factual knowledge and so we're going to talk about is how you can actually use an external memory or a key value store to give the model access to either knowledge graph triples or context information and a key thing about this external memory is that it's independent of the learned model parameters so this means you can actually support injecting and updating factual knowledge you can do this directly to the symbolic external memory by let's say changing the value for a particular key or maybe adding another key and you don't have to pre-train or retrain your entity embeddings when you make this change and the approaches we'll talk about today can actually even have these updates to the external memory without more pre-training of the language model so that's pretty neat and then another benefit of using external memory over these pre-trained nc embedding approaches is they can also be more interpretable so if you have an a bug or not a bug an air in your model where it's not predicting a correct fact it's very challenging to figure out with pre-trained anti-embeddings um what the problem might be was it the original knowledge base was it the encoding in the entity embeddings is it how the language model is using the entity embeddings and here you have a little more information with an external memory and that you can look in the external memory and see was the fact in the external memory was was it not in external memory and so on so it adds a little bit more interpretability than just using these pre-trained ncm bettings as an indirect way to encode the knowledge base so the first work we're going to talk about is called kglm and unlike the other approaches we've talked about so far this actually uses lstms and not transformers so the key idea here is to condition the language model on a knowledge graph so recall with a standard language model we want to predict the next word given the previous words in the sequence well now we also want to predict the next entity given the previous words in the sequence and given the previous entities in the sentence or the entities that are relevant to the sentence i should say so kglm will be building a local knowledge graph as it iterates over the sequence and a local knowledge graph is just a subset of a full knowledge graph that only has the entities that are actually relevant to the sequence oops so if we have this example here um a simplified example from the paper that super mario land is a game developed by blank and super mario land here is an entity you'd want a local knowledge graph as follows where you see that super mario land is in the local knowledge graph but we also have the relations to super mario land to other entities that are copied from the full knowledge graph into this local knowledge graph and you would build up this local knowledge graph as you iterate over the sentence so whenever you see an entity you would add it to the local knowledge graph as well as its relations to other entities so obviously this is a much smaller example than what would really have all the relations to super mario land just for the purpose of the example but hopefully it's clear that all of these are relevant to the sequence something important to note here is that this does assume that the entities are known during training so that you do have this entity annotated data for training and therefore your local knowledge graph is always the ground truth local knowledge graph as you iterate over the sequence so why might this be a good idea to do this well here the next word you want to predict is nintendo and you may notice that nintendo is in your local knowledge graph so sometimes this local knowledge graph can actually serve as a very strong signal for what you want to predict for your next word now you may be thinking well this wouldn't always be helpful and that's true is all as well so if you look at just like the third word in the sequence and you want to predict that word um so is a game for instance well if this isn't in the local knowledge graph this wouldn't be necessarily that helpful you would just do a standard language model prediction or if you're at the beginning of the sequence your local knowledge graph is empty so of course you're not going to get any signal from it so the first question they ask in kglm is how can a language model know when to use a local knowledge graph and when it might actually be useful for predicting the next word so we're going to keep the same example as a running example and we have our local knowledge graph here we now have an lstm that looks similar to the representations you've seen throughout this class and normally you've seen the lstm predicts the next word well now we're also going to use the lstm to predict the next type of the word so is the next word going to be a related entity meaning it's in the local knowledge graph already is it going to be a new entity meaning it's not in a local knowledge graph or is it going to be not an entity in which case you just revert to a normal lstm prediction and they're going to use the lstm hidden state to do this prediction of the type of the next word over this three-way three different classes that they might want to consider so in the case of super mario land as a game developed by nintendo we saw that this would be a related entity case because you saw that nintendo was in the local knowledge graph for the other cases super mario land would be a new entity case since it's the local knowledge graph is empty at that point and then any of the words between super mario land and nintendo would be non-entity as they're just a standard lstm language model prediction that doesn't involve any entities so now we need to talk about what the language model actually does in these three different scenarios to predict the next entity and the next word so we're going to keep the example up at the top in case you want to refer back to three different cases and we're going to start with the related entity case so here we assume that the next word or entity is actually in your local knowledge graph and remember that we can describe a knowledge graph in terms of triples so in terms of uh pairs of parent entities relations and tail entities and in the case of predicting the next word as nintendo there's only one possible parent entity in the local knowledge graph which is super mario land and the goal is you want to figure out what is the most relevant triple that will be useful in helping to predict the next word so in this case you could have the triple super mario land publisher nintendo you might have the triple super mario land genre platform game which of these is actually helpful in predicting that nintendo should be the next word so here what you would want kglm to do is predict that the top scoring parent entity is super mario land and the top scoring relation is publisher and you can see there are actually contextual cues in a sentence that could help you figure out uh which triple you're talking about and then given that your top scoring parent entity is super mario land and your top scoring relation is publisher you can figure out that using knowledge graph triples the tail entity has to be nintendo and therefore this gives you a strong signal that the next word will be nintendo so the goal is you're going to find the top scoring parent entity and the top scoring relation using the nodes in your local knowledge graph and you can do this by using the lstm hidden state combined with pre-trained entity and relation embeddings so i do admit i cheated here a little bit and that this does use pre-trained embeddings but hopefully you'll see by the end of this discussion why i think it fits a bit better in this external memory use case as well so what they're going to do is they're going to take a softmax using the lstm hidden state and the entity embeddings for each of the potential parent entities and then we'll take this top scoring one as a parent entity and they'll do the same thing for the relation embeddings the next entity is then just this tail entity from the knowledge graph triple so it's relatively trivial to figure out what the next entity should be once you've figured out the top scoring parent entity and your top scoring relation and then finally to predict the next word they take the vocabulary and they expand it to include different aliases that could refer to that entity so what we mean by aliases here are phrases that could refer to the entity and text so you might not just call it nintendo you might also say nintendo company or copai and you want any of these to be possible words that you could predict as a next word so the goal of this vocabulary expansion is to increase the probability that the next word you predict will actually be related to this next entity so new entity case is a bit simpler this means that the entity that you're predicting is not in the local knowledge graph so you're not getting any signal from this local knowledge graph that you've been building up and all you want to do is find the top scoring entity in the full knowledge graph and you can do this using the lstm hidden state and pretend ntm bettings similar to how we found the score for the top parent entity your next entity will just be the top scoring entity out of the full knowledge graph and then your next word is once again this vocabulary expanded to include aliases of that entity the not in empty case is the simplest you just revert to normal lstm you don't have an x entity to predict and your next word is just the most likely next token over your normal vocabulary so here's a diagram from their paper that hopefully summarizes and makes even clearer what i just went over so they have a longer example than the one we are looking at but it's the same prediction as nintendo is the next word and they have their predictions in red so this is what they want kglm to predict the three different cases are in the horizontals and we see that here you're in the related entity case since nintendo is in your local knowledge graph so they want kglm to predict that nintendo should be a related entity type of word that super mario land should be its parent entity that publisher should be the relevant relation and as a result the next entity is nintendo and then they expand their vocabulary you see the aliases aliases of nintendo at the bottom and then finally they actually predict nintendo as the next word and the other cases just summarize what we also already went over so i find that kglm actually outperforms gpt2 and awd lstm which is a strong lstm language model on a fat completion task similar to the fill in the blank examples that we looked at at the beginning of the talk they also find qualitatively that compared to gpt 2 kglm tends to predict more specific tokens since it can predict these tokens from just copying from the local knowledge graph whereas gpt2 will tend to predict more generic tokens so if you want to predict the birthplace of someone gpt2 is more likely to predict new york for example and kglm might predict some obscure place and then they had these really cool set of experiments where they showed that kglm actually supports modifying or updating facts so they made a direct change in the knowledge graph and then they saw what is the change in kglm's predictions so they have this example where um the sequence was barack obama was born on blank they had their knowledge graph triple as barack obama's original birth date and then their most likely next tokens were as expected august 4th 1961 and then they just changed their knowledge graph so they changed the birth date of obama they said okay he's now born 2013. and they look to see what the next uh predictions were for kglm and it changed its predictions to match what was in the local knowledge graph so this is something that's pretty cool and that really only external memory approaches um can do compared to these to the original pre-trained anti-embedding approaches we talked about and i think it's one of the one of the reasons that kglm at least in my opinion fits better in these external memory use cases right so the next slide is a different paper so i guess i'll take questions on kglm if there are any it's a pretty complex method so feel free to have questions yeah um could you one more time explain what the definition of the local knowledge graph is in relationship to the global knowledge graph yep um so local knowledge graph is supposed to be a subset of the full knowledge graph and it's only supposed to consist of entities that are actually have actually been seen in the sequence um as well as uh their relevant entities okay oops all right so here you see that super mario land is in the local knowledge graph because super mario land is an entity that is seen in the sequence and then you also want to copy over all the edges from super mario land that would be in the full knowledge graph so this is just a subset of them for the purpose of the example but you see that super mario land has an edge nintendo to game boy to platform game and so you would copy all edges that super mario land has to another node in the full knowledge graph and they know in advance like they have the labels here for what the entities are during training so that's how they can actually create this ground truth knowledge graph and then briefly a student asked why we can't just use the whole knowledge graph and i gave an answer but maybe you know better um yeah i think the idea is the signal will be much stronger if you just use a local knowledge graph so in the softmax for the related entity case you would just be predicting over the potential parent entities in your local knowledge graph which is a much smaller set than what's in your full knowledge graph so i guess it's more likely that you're going to predict something that is correct in that case then when you have like five million or so entities in your full knowledge graph it's also much cheaper to compute um in this case there's only a single parent entity but you could have multiple parent entities that you're trying to compute which one's most likely over is that what you were also thinking john um yeah i mainly just said uh uh efficiency so the the signal thing is cool too um here's an exciting question what about queries that require more than one uh step in the knowledge graph such as the location of the publisher of super mario land um yeah that's a good question so the idea is like can it support those types like does it support multi-hop kind of building of the knowledge graph yeah yeah how does kglm perform in those cases yeah i don't know that's a very good question they build up the knowledge graph so that it's just single hop as far as i know but like if you saw the other entities if you were to see the entities along the hops it would have them in the local knowledge graph um yeah that's a good question i don't know if they explored that great okay um let's move along then okay so the next piece of work we're going to talk about you guys have actually briefly seen in the natural language generation lecture but i'm going to go over it again quickly here um so unlike the other works that we talked about that used knowledge graph triples this is actually going to take kind of a looser notion of knowledge in that the knowledge will just be encoded in the text in the training data set so this is called k n lm and the idea is that we're building the idea that language models not only learn to predict the next word and text but they also learn these representations of text and the authors suggest that it might actually be easier to learn similarities between text sequences than it is to predict the next word in the text so you have this example that dickens is the author of blank and dickens wrote blank and they argue that it's easier to tell for a human but also for a model that these sequences are similar and they should probably have the same next word even if you don't know what the next word is um so that's suggesting that you know it's easier to learn these similarities than it is to actually predict the next word and they argue that this is even more true for long tail patterns where it's very challenging for the model to predict that the next word is some rarely seen token or rare entity than it is to find another similar sequence that it's already seen and just copy the next word from that sequence so what they propose to do is store all representations of text sequences in a nearest neighbor data store and then at inference what you'll want to do is you find the k most similar sequence as a text you then retrieve their corresponding values so you just peek at those sequences and see what were their next words and then you combine the probability from this nearest neighbor data store with just a typical language model prediction and so they call this an interpolation step in that they're waiting how much to pay attention to the probability from this k n approach and how much to pay attention to this language model approach and the lambda here is just a hyperparameter they tune so they have this diagram from their paper where they want to predict the next word in the sequence shakespeare's play blank so what they do is they have all the training contacts already encoded in their data store so they have representations of all the training contacts and then they compute a representation of their text context and they want to figure out which representations in the training context are most similar to this text test context representation and so here in the external memory view of things the keys would be the representations of the training context and the values would be the next words so to get the k nearest training representations um they then copy over their values so that's what you see with this macbeth hamlet macbeth example they have a normalization step where they convert this to probability space um and then finally they have an aggregation step so if a word is seen as the next word and several of these uh k nearest neighbors then they want to count more for that so that's why they aggregate say c macbeth twice it means macbeth is more likely um and then finally they have this interpolation step where they try to balance between the classification probabilities from the language model and from the k n approach so some immediate observation you might have is this seems really expensive they do propose ways to kind of try to minimize the expense of actually having to store all the training contacts in this data store because they actually store it for every single window of next word in the training context and you can do quantization on some nearest neighbor approaches to try to make this less expensive but i imagine this would still be pretty expensive for really large training data sets they also have some cool experiments that show that this is very good for domain adaptation so if you take your language model and you have a new domain that you want to apply your language model to you could just create a nearest neighbor data store of your new domain so you encode all the representations of that new domain you stick it in a data store and then you can just use your language model with these k n probabilities as well just immediately on this new domain without actually having to further train your language model so i thought that was a pretty cool use case of this external memory approach so while it doesn't leverage knowledge bases directly it does have this loose knowledge of or loose idea of encoding knowledge that is in a textual representation form into some external memory that the model can then take advantage of um that's all i have for this approach are there any questions on this approach well so only one person is asking how does the k n make predictions for the next word the k neighbors are for the context instead of the next word oh okay that wasn't clear um so the keys are the representations of the context the values in your external memory are the next words so when you figure out you figure out your nearest neighbors using your keys and then you copy over their values so it does actually know what the next words are for each of those representations okay um so finally we're going to talk about how you can just modify the training data to better encode knowledge and language models so approaches we've talked about so far are actually incorporating knowledge explicitly by using either pre-trained embeddings or an external memory we also want to talk about how can you just incorporate knowledge implicitly through the unstructured text so what we're going to do is either mask or corrupt the data to introduce additional training tasks that require factual knowledge to figure out what data was masked for instance so this has some clear advantages it doesn't have any additional memory or computation requirements you don't have a data store to deal with you don't have extra knowledge encoder layers to train all you do is modify the training data and you don't have to modify your architecture either so you can continue using your favorite bert model and just make these changes to the training data so the first work we're going to look at is called wklm weekly supervised knowledge pre-training language model or pre-trained language model and the key idea here is to train the model to distinguish between true and false knowledge so they're going to corrupt the data by replacing mentions in the text with mentions that refer to different entities of the same type to create what they refer to as negative knowledge statements and then the model will just predict has the entity been replaced or corrupted this type constraint is necessary to make sure that or to encourage the model to actually use factual knowledge to figure out if this corruption is taking place so you could imagine if you replace it with something that's not realistic at all the model could just be basing its prediction based on is this sentence linguistically correct so as an example we have a true knowledge statement as jk rowling is the author of harry potter and then we want to modify this to replace it with another author so let's say we change this to j r tolkien as the author of harry potter so you can see that this requires some amount of knowledge background knowledge to actually be able to figure out which statement's true and which statement is false the idea is that the model will be able to predict for each of these mentions whether it's a true or false mention so this diagram here is from the paper and hopefully explains this a bit better they have the original article on the left and then they have the replaced article with the corruptions on the right and the entities are are in blue so what they do is for a given entity they first look up its type they find other entities of that type and then they randomly sample the entity and get an alias of it to replace in the text so they're going to play stan lee for instance with brian johnson and marvel comics with dc comics and their placements are in red on the right and then the idea is that the model be able to predict for each of these mentions um was it replaced or not so in the case of brian johnson they have the red x for this is a false mention and in the case of the true mentions they have the check mark so it's a pretty simple approach but they actually show that it can help the model increase the amount of knowledge that's encoded in parameters okay so wklm uses an entity replacement loss to train the model to distinguish between these true and false mentions and this just looks like a binary classification loss where your true mentions are on the left and your false mentions are on the right and you want to increase the probability that this p of e given c so the probability entity given the context you want to increase that for the true mentions and decrease it for the false mentions the total loss is then just a combination of the mass language model loss and this entity replacement loss the mass language mods the mass language model loss is defined at the token level and the entity replacement loss is defined at the entity level meaning it's not just over sub words it's even potentially over words if you have multi-word entities phrases for instance and this is an important point or an important theme that we really see occurring throughout these works that we'll look at in that modifying the data at the entity level seems to be an important component of actually increasing the amount of knowledge that a language model can encode so they find that wklm improves over bert and gpt2 in fact completion tasks like the fill in the blank statements that we looked at at the beginning they also find that it improves over the ernie paper that we talked about on a downstream task and they had a set of ablation experiments where they looked at can you just remove this mass language model loss now and if you just train bert for longer do you really need this entity replacement loss so that's what the table here is looking at um the second row is looking at if we remove the mass language model lost what happens we see that it performs much worse without the mass language model loss so you really need both losses their intuition there was the mass language model loss helps to encode just general language understanding and then training bert for longer um performs much worse than using his entity replacement loss so this motivates even farther that you really do need um or the entity replacement loss is actually really helping encode more knowledge in these language models so in addition to corrupting the data we're also going to look at can we just mask the data differently can we be more clever about how we do the masking and this is a thread in several recent works so there's actually another paper called ernie so this is different than the one we talked about before and this is enhanced representation through knowledge integration and what they do is show improvements on downstream chinese nlp tasks by doing phrase level and entity level masking so instead of just masking out sub words they're going to mask out phrases of multiple words and entities the full phrase of an entity which corresponds to some entity and text that they might find with like nar techniques for example and then the second work is actually something you heard about in the last lecture which is the idea of using salient span masking to mask out salient spans and a saline span is just a named entity or a date so you can see this is pretty similar to what ernie is doing and they found that using salient span masking actually significantly helped t5 performance on these closed domain question answering tasks so just to make sure we're all on the same page um with the different masking techniques this diagram from the ernie paper is comparing to what bert does versus what ernie does the top shows that ernie masked out the subword tokens or that bert massed out the subway tokens whereas ernie massed out phrases like a series of as well as entities like jk rowling there's some interesting results on showing that salient span masking is helping encode more knowledge in these representations so on the left we're looking at the results of the original paper that proposed salient sand salient span masking so this is the realm work and the idea here was that they were training a knowledge retriever so it's actually more of an external memory class of techniques but they find that by using the salient span masking technique they could actually train a much better knowledge retriever so it's a good example of how these techniques are really complementary so while i presented three classes of techniques you can definitely get benefits by doing multiple techniques together and they found that doing silence fan masking compared to using masking from bert which would be the random uniform masks or doing random masking of spans from a paper called spanbert um it performs much better to do salience band masking so you see like a 38 exact match score versus like a 32 exact match score for instance and on the right we have results um from fine tuning t5 with either ceiling span masking or the span corruption task that you saw in assignment five and you can see that on these different qa data sets science band masking does significantly better than just using the span corruption technique so this really suggests that doing the salient span masking and masking out these salient spans of these entities is in fact helping to encode more knowledge in these language models so to recap we talked about three different classes of techniques to add knowledge to language models we talked about using pre-trained entity embeddings these weren't too difficult to apply to existing architectures and as a way to leverage this knowledge graph pre-training but it's a rather indirect way of incorporating knowledge and it could be hard to interpret we also talked about approaches to add an external memory this could support modifying the knowledge base it was also easier to interpret but they tended to be more complex in implementation like we saw at kglm and they also required more memory like we saw with the k nlm approach and then finally we talk about modifying the training data so this requires no model changes or additional computation it also might be the easiest to theoretically analyze so it's actually an active area research right now but still an open question if modifying the training data is always as effective as model changes and what the trade-offs are in terms of amount of data required versus doing one of these other knowledge enhancement approaches so that leads us to section three um so i guess i'll pause again for questions i think we may be good awesome okay um so section three is about how researchers are actually going about evaluating the knowledge and language models and um i guess how some of the techniques we actually just talked about stand up in this evaluation so first we're going to talk about probes which don't require any fine tuning of the language model and then we're going to talk about downstream tasks which look at how well do these pre-trained representations actually transfer their knowledge to other tasks so one of the initial works in this area was called llama and this really started a series of works to look into how much knowledge is already encoded in these language models so their question was how much relational common sense and factual knowledge is in off-the-shelf language models such as taking pre-trained language models and evaluating the knowledge in them and this is without any additional training or fine-tuning so they mainly constructed a set of what they refer to as closed statements and these are just the fill in the blank statements that we actually drew from at the beginning of the talk we have some more examples here and they manually created these templates of closed statements using knowledge graph triples and question answering pairs from existing data sets they wanted to compare pre-trained language models to supervise relation extraction and question answering systems to see how do these language models that were trained in an unsupervised fashion compared to these baseline systems that are not only supervised but really targeted for this task of knowledge extraction and their goal was to evaluate the knowledge in existing pre-trained language models and a key point about this is like they're just using the language models as they are available to researchers so this means there could be differences in the pre-training corpora for example so when you look at the following table and you're comparing language models also keep in mind that these don't like account for the differences in the pre-trained corpora so a lot of these language models probably look familiar to you either from previous lectures or maybe your final projects and what we see is that overall bert base and bert large pre-trained models are performing much better than the previous language or the other language models here i guess i forgot to mention what mean precision at one is um this is a pretty simple metric the idea is if you look at the blank and you look at the top predictions for the top prediction for the blank is it correct or not so that's what precision at 1 means precision at 10 would be let's look at the top 10 predictions is the correct prediction in the top 10. um so in addition to bert large and base performing well overall we do see that in the t-rex data set the relation extraction baseline is performing a bit better than bert one thing they notice here that's pretty interesting is that this data set has a lot of different types of relations and relations can be classified in terms of are they a one-to-one relation are they an end-to-one relation are they an end-to-end relation an example of a one-to-one relation would be your student id relation so you have a unique student id an example of an end-to-end relation would be they enrolled in relation so there's lots of students enrolled in lots of classes so this would be an end-to-end relation and they find that bert really struggles on these end-to-end relations so while it performs better than relation extraction baseline on some types of relations overall it does pretty terribly on these end-to-end relations so overall it does a bit worse than the baseline on this trx data set they also compare to squad on docker qa and they find that it does a fair amount worse they note that the language model is not fine-tuned here and also has no access to an information retrieval system and then they look at the precision at 10 they find that this gap between docker qa's performance and bert actually closes quite a bit which suggests that these language models do have some amount of knowledge encoded in them and that they're even competitive with these knowledge extraction supervised baselines so you can also try out examples on their github repo for the llama probe we have an example that was from their repo that was the cat is on the mask you can see what the top 10 predictions are to fill in the closed statement here they have the cat is on the phone so this can be a fun way just to figure out what factual common sense knowledge is in existing language models and it's pretty easy to use with this interactive prompt so some limitations of the llama probe are that it can be hard to understand why the models perform well when they do so for instance bert might just be predicting the most popular token and this happens to be right maybe it's just memorizing co-occurrence patterns and doesn't really understand the knowledge statement and doesn't understand what the fact is it might also just be identifying similarities between surface forms of the subject and object so for instance example pope clement vii has a position of blank even if you don't know anything about pope clement vii you might be able to figure out that pope is a likely next word for this uh triple or for this template so the problem with this is if the model is just making these predictions based on these surface forms or co-occurrence patterns it's difficult to know for actually evaluating the knowledge in the model maybe it's just making correct predictions for other reasons and the more subtle issue that we've brought up is that language models might be just sensitive to the phrasing of the statement so for each uh triple in their data set or for each relation their data set they just had one manually defined template and qualitatively they found that if they just make small changes as a template it could actually change whether or not the model could recall the correct prediction or not and so this means that the probe results are really a lower bound on the knowledge that's encoded in the language model so if you change the phrasing it's possible that the model might show that actually does have the knowledge encoded in it so the next lines of work we'll talk about um are really building on these two limitations of this original llama probe so the first one is called lama oon or llama unhelpful names and the key idea is to remove these examples from lama that can be answered without the relational knowledge so this is kind of addressing the first limitation on the last slide so they observed that bert relies under surface forms entities might not be using knowledge to make these predictions this includes the string match situation that we talked about with the pope this also is dealing with the revealing person name issue that you saw in assignment five so this is where the name could be an incorrect prior for the native language of someone their place of birth their nationality they have this example from the table or from the paper where they look at different people names or person's names and then they look at births prediction for their native language and these are all french speaking actors and burt just predicts very biased and stereotypical languages for these particular names so this can really work both ways it can lead bert to make incorrect predictions and sometimes or in some cases but it could also work to make or to let bert make correct predictions even if it has no factual knowledge of those people so that's the issue they're trying to get at here is do we know that bert actually knows this fact or is it just using some bias to make its prediction so what they do is they introduce a couple heuristics to basically just filter out these examples from the llama probe um that can either be solved by the string match setting or the surveilling person name setting so they make a harder subset of the llama data set essentially they find that when they test bert on this harder subset that its performance drops about eight percent but when they test their knowledge enhanced model which they call ebert the score only drops about one percent so it's possible that as we make harder knowledge probes we'll actually see even bigger differences in the performance of knowledge enhanced models to models without these knowledge enhancements the next piece of work we'll talk about is actually getting at this issue of um the phrasing of the the prompt might actually trigger different responses from the language model so the language model might know the fact but it might fail on the task due to the phrasing one reason this might happen is the pre-training is on different contexts and sentence structures in the query so for example you might have in your pre-training corpus the birthplace of barack obama is honolulu hawaii and this might be something you see in wikipedia for instance that's a common training data set and then as a researcher you write barack obama is born in blank and you can see that these sentence structures are pretty different so the model might have seen the first fact but the sentence structure difference is actually enough to confuse it so it can't answer this query so what they do is they generate a lot more of these prompts by mining templates from wikipedia one of their techniques actually uses dependency parsing and also generating paraphrase prompts by taking inspiration from the machine translation literature and using back translation so generate a lot more prompts to try to query the language models and figure out do small variations in the prompt trigger the correct prediction from the language model they also experiment then sombling prompts so if we give the model multiple prompts and then take some probability averaged over these different prompts can we improve the performance on the model returning the correct prediction so we give it a higher chance of seeing a context that it might have actually seen during pre-training they find that the performance on llama increases when they either use a top performing prompt or when they use this ensembling approach so this suggests that the original llama really was a lower bound on the amount of knowledge encoded in these language models and changing the phrasing can actually help the model recall the correct answer this table table's a bit frightening but they find that small changes in the query can lead to really large gains on performance so if you just have a query like x plays in y position and then you change that to x plays that y position this can actually lead to like a 23 accuracy gain on this particular relation in terms of the model actually being able to recall the correct answer or even just x was created in a y to x is created in y 10 accuracy gain so i think this motivates the need to not only develop better ways to query these models but probably also build language models that are actually more robust to the query itself so in addition to probes another way to evaluate these language models is by looking at how well they transfer from the pre-trained representation to downstream tasks and so the idea here is you're actually going to fine-tune the pre-trained representation on different downstream tasks similar to how you would evaluate bert on glue tasks some common tasks that are used for this are relation extraction entity typing and question answering relation extraction is where you want to predict the relation between two entities so this is getting back at one of the questions earlier in the talk in terms of well how do you get the relation that's the edges in these knowledge bases so given two entities you learn a model to predict what is a relation between them entity typing is a task of given an entity what is the type of the entity so here alice rob the bank you want to predict her as a criminal and then you guys are very familiar with question answering so the idea of these comment of these tasks is that they're knowledge intensive so they're good candidates to see how well do these pre-trained representations actually transfer their knowledge to these downstream tasks here we're looking at the performance on a relation extraction benchmark called tacrid and all the models that we show here were at one point state of the art on tacrid so this cgcn is a graph convolutional neural network over dependency trees the burt lstm base is a it's one of the first works that showed that you could actually get state of the art performance with bert on relation extraction and this is just putting lstm layer over bert's output ernie is the work that we talked about with the pre-trained entity embeddings matching the blanks we didn't get to today but it's a really interesting work about learning meaningful relation representations and it falls more into the training data modification approaches and that they are actually masking out entities again and then no birt is what we talked about um the w and w here means they actually encode two knowledge bases in novert so they're encoding wordnet and they're also encoding wikipedia and the high level takeaway from this table is that you can see that the recent knowledge enhanced models have achieved state of the art over the original models that once performed very well on tack grid and you we have about five f1 gains here another interesting takeaway from this table is there seems to be a trade-off in the size of the language model that's necessary to get a certain performance so if you just um consider the size of the language model then no birth performs the best but if you don't consider that then it's highs with matching the blanks so overall this is pretty good evidence that these knowledge enhanced methods are in fact transferring to these knowledge intensive downstream tasks that can really take advantage of these pre-trained representations we also have results on entity typing so here we're comparing a slightly different set of models some of the baselines are lstm models that were designed for entity typing and we have ernie and nobert um leading the the i guess leaderboard here on the entity typing task of open entity and we see gains of about 15 f1 points with ernie and no bert so once again we really do see that um these knowledge rich pre-trained representations are transferring and helping on these knowledge intensive downstream tasks so just to recap we talked about probes which evaluate the knowledge already present in models these don't require any more training but it can be challenging to construct benchmarks to actually make sure you're testing the knowledge in these language models it can also be challenging to construct the queries used in the probe we then talked about downstream tasks these are a bit of an indirect way to evaluate knowledge and that they have this extra component of fine tuning but it's a good way to evaluate how useful is this knowledge-rich pre-trained representation in actual applications so i just uh touched on the exciting work in this area but there's many other directions if you want to dive more into this so there's retrieval augmented language models which learn knowledge retrievers to figure out what documents might be relevant for predicting the next word there's work in modifying the knowledge in language models so i talked about how this is one of the obstacles and challenges to using language models as knowledge bases so there's been recent work in this area we also saw how important the knowledge pre-training task was well there's many papers that are proposing different tasks to do the knowledge pre-training so it's still an open question in terms of what tasks are best to add to encode more knowledge there's also been work on more efficient knowledge systems so at nurse there's now an efficient qa challenge which aims at building the smallest qa system and then finally there's been work on building better knowledge benchmarks that build on the benchmarks that we saw today that's all i have for today and i hope your final projects are going well 
","['', 'Lecture 15 of CS224N course covers integrating knowledge and language models.', 'Language models are trained on a massive amount of unlabeled text data.', 'Standard language models predict the next word in a sequence of text.', 'Masked language models are trained to predict masked out words in a sequence.', 'Language models can be used for various tasks, including summarization, dialogue, fluency evaluation, and generating pre-trained representations of text.', 'Language models can be used as a knowledge base to answer natural language queries.', 'Language models may not always recall factual knowledge correctly.', 'Challenges of using language models as knowledge bases include:', 'Difficulty in interpreting why a model produces a specific answer.', 'Lack of ability to explain the provenance of the information.', 'Difficulty in interpreting why a model produces a specific answer.', 'Lack of ability to explain the provenance of the information.', 'Recent work has shown promising results in adding knowledge to language models.', 'One technique to add knowledge to language models is to use pre-trained entity embeddings.', 'Another technique is to use external memory or a key-value store.', 'A third technique is to modify the training data.', 'Knowledge-enhanced language models can outperform traditional language models on downstream tasks.', 'Downstream tasks are tasks where a pre-trained model is fine-tuned for a specific task.', 'Examples of downstream tasks include relation extraction, entity typing, and question answering.', 'Probes are used to evaluate the knowledge already present in a model.', 'Challenges of using probes include:', 'Difficulty in constructing benchmarks to test the knowledge.', 'Difficulty in constructing queries to test the knowledge.', '']"
"so this is lecture 15. and today we'll be talking about code generation so a little bit unusual since it's a we'll be generating unnatural languages this time but it'll connect in a number of ways to natural language Generation Um so before I start just a few announcements the project Milestone is due this Thursday you are certainly aware of that um and also when doing the projects it's always good to keep track of how much you're spending on Azure in AWS and one thing to notice is that disk costs money like it doesn't cost that much compared to gpus but it still costs something for me be sure to not be spending all your money on on disk um so tomorrow John will be running a discussion on training large language models it'll be really cool um so it'll be at 3 30. in the skinny Auditorium there's more details on that and this Thursday we have our first invited talk in our regular lecture time and attendance is expected so please everyone show up it'll be really cool um all right so let's let's get started so we'll be talking about a problem that in the literature is called Perkin synthesis and let's see what that means so Perkin synthesis is actually a pretty old uh challenge of artificial intelligence and the goal is to write programs to create programs that can take some sort of specification and write a program that satisfies that specification so it's a program that writes a program uh so so that's what a program synthesizer is right it's a program that's takes your specification and is able to generate some program and then you can ask what kind of specification um so one possible specification for example could be a logical formula it could be uh like a mathematical formula that specifies what Behavior we want from the program it could be an equivalence program so I could say okay here is a slow implementation of the Sorting algorithm bubble sorts for example and it runs in O of N squared and I want to synthesize another program that's equivalent so it generates all the the same outputs given the same inputs but it's maybe faster so that could be a form of specification um I could give examples right I could say Okay I want a program that if I give you if I give it this input it should generate this output if I give this string it should should give me back this ring um or as more popular these days we could also maybe in addition to or instead of these other kinds of specifications also give a natural language description right I could just write I want a program that performs certain a certain operations and so so just to warm up let's see how this synthesis from logical specifications could look like um so when would it make sense to use the program synthesizer at all um so it would only make sense to use a program to write a program for us if that's in some way easier than writing the program ourselves right so there should be it should be easier to specify what the program does compared to exactly how it should do that so let's and this is um different than natural language generation an important way in that we usually have ways to test our output automatically right so if if I give the synthesizer okay I want a program that given these inputs generates these outputs and this synthesizer gives me back a program I can go in there and execute the program on the inputs that I gave and verify that it generates the correct outputs and and this is different than a natural language task for example if I ask it to summarize an article or in a paragraph and it gives me back a response and I can evaluate it in some ways I can compare it to a true human reference summaries or or I can use a language model to to evaluate the output of another language model but but in I can't like execute the summary and verify that it's a good summary um so yes certain that the output is always correct considering like I mean without formal verification how can you just make sure that the output program is correct since you'll be posting to the iOS on the test cases only yeah that's a good question so the question was how can I make sure that the output is correct uh in general well it depends on what specification we have right if this specification is input output examples all we can do is verify that it satisfies those examples we'll talk about the problem with that in a little bit um any other questions about this I'll give an example so it would be very concrete starting um okay so let's see how this could work let's try to specify a program using the sort of logical specification um so our first attempt will be to specify how do I sort an array right I want a program that receives an array as input and returns a sorted array so how would I write that mathematically our first attempt could be well let's say that this program takes an array a and outputs on array B um I can specify that I want the array B to be sorted right so mathematically I could write that as for all of the indices I of the output I want the element at that index should be smaller than the OR at most the the next element right so like sorted in increasing order second look at the statement and say okay so if if the output satisfies that then it's a sorted array um does this look good maybe right so I can give that specification to a synthesizer and then it'll go and search for programs that satisfies this and then it returns this program which is called sword takes an array a and Returns the array 1 2. so if you look at the mathematical form let's say well for all of the indices of the output that element is smaller than or equal to the next element so it satisfies the specification that we gave but of course not the program we wanted um and then okay so maybe we missed something we missed that the output not only should be sorted but also should have the same elements as the input right so I can specify that as like uh I want the array B to have the same length as array a and has to be a permutation for each element of the the output has to be somewhere there in the input um and then writing a little bit more formally in first of the logic it would look like that don't have to try to parse it and then if I give that to the synthesizer maybe to go and search for some programs and return like quick sort or some function that actually sorts Theory um so notice that the problem here is it's quite non-trivial because the formula as little as it is it doesn't tell us how to sort the rate just says that the rate should be sorted in some way so it's not just a syntactical translation between the synthetic the the the the formula that we gave and the the programming language that that we're targeting um but the thing that's obvious here is that these logical specifications are quite hard to read they're quite hard to write of course and also to check right if I just gave you the the formula that says and resorted maybe at first it's not easy to see the corner case that just being sorted is not enough um and I mean if I tell you that we are making a synthesizer that takes this formula and returns like a function that sorts an array you could reasonably say that maybe it's just easier to write the function yourself but it is quite a challenge to this or even then any questions about the the setup here okay so maybe it's maybe logical forms are too much right we don't want to be specifying even simple programs like sorting with those ugly first order formulas we could try something simpler we could try examples right so input output example is a very natural kind of specification and in fact when writing program software Engineers usually already write tests which are kind of like input output examples right like if I call the function with this input should return this I assert that it does that um so how could I specify sorting in that case I could say well if I give the array 3 2 1 0 it should return 0 1 2 3 for 142 which would return one to four and four nine should return 9. any human looking at these inputs and outputs could reasonably guess that oh maybe it's just like sorting the input array right um but as we just saw with this the The Logical synthesizer we could also get a program that looks like this well if the array has exactly four elements return zero one two three and if it's if it has three returns this exact array and otherwise always return nine right satisfies the input output examples but somehow it's still not what we want um of course this is a kind of an adversarial output um and synthesis by example was actually massively used in the last decade because of this feature in Excel called Flash Fill which was released in in 2013 and it was for a while one of the hottest things to have happened to Microsoft Excel um so uh flash flow is this really cool feature where uh the goal is to for Excel to guess what string transformation you're applying so you can write for example uh if you have a column that has people's first and last names and you want to just get the the first name for example of everyone and you create the second column and you type like in this example Ned um then Excel if you like click on the flash flow button it will magically guess that what you're doing is you're splitting on the space and maybe taking the first of those strings and suggest to complete that as the second column and it can actually do quite complex Transformations and usually from one or two examples and it's quite cool um but as is clear at this point synthesis from examples has there's a hearing problem of ambiguity right for any set of examples input output examples that I give there will be usually an infinite number of programs that that have that exactly that behavior on those examples right um but somehow that's very non-human because humans for some reason have a very specific preference over this over this Infinite Space of programs um like if I look at at this program that does this even if I don't tell you what kind of program was I looking at in the first place it's it's very obvious that like the previous problem not useful for anything right um but it's obvious for you not through a synthesizer necessarily that's trying to do to find a program um so for example what program am I specifying here with these two examples Jan uh transform to January and fabric transforms through February any human guess is about what this should do name from nothing expensive yeah exactly it should obviously do that but for a while I think maybe not EX I'm not sure if this fix was released or is going to be released but for a while this is what flash would do um it would complete Feb with February March with Maria Maria and so on right so it guessed from one example oh what you're doing is just concatenating your area on the string that you had right so clear the extrapolation any other possible strings that you might want so how do we do we deal with this right with this ambiguity we'll talk a little bit about that but just to summarize what we we've seen so far a synthesizer is this program that takes some form of specification of what a program should do and then generates a program and if we get this to work this would actually have massive impact in a number of ways right it can lower the barrier to to access programming through a lot of of people that maybe don't want you should be spent four years taking CS classes so for example people can automate a lot of things just by using flashfree on XL things that would take a lot more time and even programmers ourselves can benefit from much higher productivity we can program it higher level ways so this is quite an interesting goal but it of course has many challenges right it has this Infinite Space of programs a lot of them are unreasonable unreasonable in this human way and here we're talking about at least for now right searching in the in a space of programs in a very specific language where you can do search but it's of course impractical you do search in any real world languages like python uh and we we have this ambiguity problem right like how do you capture human preferences um so we'll talk here about the connection between this problem of ambiguity in program synthesis and natural language which is extremely common so human languages are extremely ambiguous and if you stop to look at it more closely it's actually quite surprising that we managed to communicate so well and so easily um even though if you look up almost any word in the dictionary it'll have like a large number of meanings that it might have even sentences out of context generally have multiple interpretations but we somehow do just fine talking in English in this very ambiguous um medium and in fact ambiguity is not even a bug of human languages it's a feature and it's a feature for efficiency so actually there's this this paper here that's really cool that provides true arguments based on information theory that any communication Channel where basically the meaning of words can be disintegrated in context we'll make those words at some point Collide to make them both short so for example if I have uh Bear the animal and bear the verb they usually appear in very different contexts right so it would actually be very inefficient to create your word to separate those uh with because at some point I would be adding both more and longer words to my vocabulary right so if they can be disembly graded a true to be often when a formal communication perspective I'll actually get an ambiguity at some point um and there's one very interesting uh challenge for computers to to resolve this kind of ambiguity called the winogram renograd schema challenge and if you read the examples they're quite entertaining because you read them it's very obvious what's going on but it's also obvious what's the challenge so so here we have these two sentences the city councilman refused the demonstrators a permit because they feared violence and the obvious ambiguity here is that they could refer to the city councilman or the demonstrators right but when you're here they feared violence what's the obvious candidate here for what they refer to yeah exactly um and when you say the advocated violence then you suddenly process the sentence in a different way right and syntax the senses are exactly the same right but just because of your your prior knowledge about how these actors behave in the world yeah you use that to disintegrate the two different means um yeah so this is very easy for us right handling this kind of ambiguity and how do you do it it's it's an interesting question how do humans do this and the the linguistic uh term for the kind of reasoning that we do in this in this setting it's called pragmatic reasoning right so in linguistics we have this distinction between semantics and pragmatics of how do we attribute meaning to things right like semantics talks about like the intrinsic meaning of words in a certain sense in pragmatics how does that change in context um and to do this kind of resolution of ambiguity we we have to operate with some sort of assumption that helps us get off the ground and one important assumption here is this Assumption of cooperativity so when we're talking to someone we assume that they're trying to help us understand what they're saying right so they want the adverse area as the program synthesizer uh was in those examples and wheels we can use that assumption to do reasoning context and perform pragmatic reason so I'll show here one model uh of pragmatic reasoning called the RSA or rational speech acts which is a based on what of how this could work in simple scenarios so here we have uh we assume that we have two people like a speaker and a listener right the speaker wants to refer to a certain object or a person and it's going to choose an utterance for that like a word or a sentence to refer to that object right and then the listener on the other side is receiving those utterance and trying to infer okay what does the speaker mean what what do they what are they referring to what objects or what person so one really quick example here on the right is this where you have these two people and then the person on the right has my friend has glasses and there are three people here there is one person wearing no glasses and no hat there's a person just wearing glasses and a person wearing a glass and a hat when you hear that this person saying my friend has glasses well it's of course ambiguous in this case because there are two people wearing glasses but does anyone have an intuition of who would you guess they're talking about yes like the most distinguished or the only distinguishing Factor of the past yeah the middle one is the one you go to because if you wanted exactly describe the receipt basically so we do this kind of recursive reasoning apparently right where we think okay so if they wanted to refer to the person with the hat they could have said hat and that would have not been a big use but they did not say hat which probably means something about what they what they intended to to refer to right um so RSA is a very simple Bayesian model of exactly this this process so just to work through an example let's say that we have these three objects of a blue square a circle which is also blue and then a green square and we have these four utterances that we can use a very small vocabulary like blue green circle and square so in RSA we will bootstrap this process from a literal listener which is a listener that can only understand letter meaning so um the if you give this listener some whether it's you The Listener will the literal listener which we'll call it l0 will put uniform probability on all the objects that satisfy you so if you say blue will put Okay so uniform over all the blue objects if you say squares will put uniform probability over the squares and that's the distribution of beliefs that the literal listener puts um so assuming that you're talking to that lateral listener now you can create a pragmatic speaker which will choose some utterance to refer to an object based on the probability that the literal listener will understand what they're saying so basically for each of the words in our or utterances that I could say maybe it could be extremely specific like I could write a text exactly describing that object but that would be very costly right so I want to be concise but at the same time I can't be true concise because otherwise I might not specify what I want to say like I will not be understood right so I can imagine this pragmatic speaker S1 which is trying to maximize this balance between the probability that the the lesson the literal listener will guess the the intended object minus some cost which in this case could be uniform probably and then from that pragmatic listener now I can create a pragmatic speaker that will choose an audience based on the probability that the pragmatic speaker would have chosen that order to refer to that object sorry the the the the the listener L1 would choose an object we'll guess a belief over the object based on the probability that the speaker S1 would have chosen data to refer to each of the objects and here I could recourse right I could create a listener L2 which reasons about the the speaker sorry I could choose a speaker as S2 which is if talking with the listener L1 in in their head and then a lesson or true and so on um but usually this listener speaker bear S1 L1 is often enough to model human judgments in these settings this doesn't make sense what how this recursive process is happening okay um yeah so assuming these three objects and a speaker says blue um again following the same example of the glasses and hats what we do yes or what's your first intuition about what object they would refer to yeah the square is typically what people what people do so a little listener would say okay it's completely ambiguous right like 50 on the Square and on the circle but if you set up a human experiment where people are receiving these utterances and saying how much they believe each of the objects is the intended object they will put around 40 probability on the the circle in 60 on the Square which is very close to what RSA predicts um okay so this gives a mechanism for resolving ambiguity in this listener speaker setting and one way to see fragrance synthesis is a setting where we are the speakers right we're talking to the synthesizer and we are speaking for example input output examples um and I we want to refer to a certain program like from a set of programs and we're we're speaking uh examples and the synthesizer is our listener which is trying to ring for what program are we referring to and we the the examples that we were seeing the synthesize was being extremely literal right so like oh if you say that given a it should return B could be n of the programs that exist at return b um but now we have a process that can maybe refine this reasoning a little bit right we have RSA and we can almost directly apply it in the setting where we can build a this meaning Matrix where in one dimension we have all the programs so let's assume for Simplicity that we have like a finite set of programs and also a finite set of of examples that can be given to the synthesizer so in that setting we can make this Matrix where each entry corresponds to a program being ran on one example and we have one if the program satisfies that example like returns true or another example for example and zero otherwise right so this Matrix directly gives us a literal listener for this setting like if I say if I give an example and a little synthesizer could just look at this table and say okay these are all the programs that set aside those examples maybe I'll sample one of those at random um but I could use the RSA recursion to derive L1 and now true right and those would be uh like pragmatic synthesizers and in a human experience ran into this paper which I won't get in a lot of the thing in their setting but they they ran this um this experience where people were trying to specify a program that draws a pattern on a like a grid and the the specification was through examples by basically saying okay like the pattern contains the square or does not contain the square and people had a much easier time communicating the pattern that they wanted with the pragmatic synthesizer um which is a quite cool result I think um yeah so of course the assumptions here are that the set of programs and of examples is finite which is quite restrictive it is it's not true of real programming languages but it does present an interesting challenge right like can we extend this this kind of approach to during the infinite set of programs like real programming languages and also maybe also we want richer kinds of specifications really instead of just saying um the behavior the program in specific examples we could try to handle natural language um any questions about this connection between yes consider in this whole program synthesis just generally how we would typically want like a simple like with this sorted example like how we had different like edge cases would do we have do we account for the fact that would we penalize a like longer program or more complicated program when trying to consider something like that yeah so so the question was in Perkins synthesis do we do people use biases like find the shortest program for example where like the simplest perfume that satisfies the specification and the question is both yes and no um it's yes in the sense that most search-based synthesizers both usually find very short programs but not because people use that as a bias necessarily for for this ambigrating but just because it's much easier to find Trader programs so like if you're doing if you're doing search in a space of programs like the chance that you find like 100 line program that satisfies the specification is naturally much smaller than you finding a short one um now to be able to do search in the first time a lot of research in this area in the last decades has been true exactly how to design like specific languages and search spaces so that this can be this can be done does that make sense any other questions okay um so we've been talking a lot about language models in the class and as you know if I give a prefix of anything that can show up in the internet right language model gives me a distribution of what can come next so for example if I say Stanford University is located in the state of the model having been trained on Wikipedia and other sources would put much higher probability on the centers continued with California rather than another U.S state um language model is quite hard uh it's like a really really hard problem because as John talked about in his lecture a lot of things can be reduced to language modeling right so if I say theater Landon streleski a former grad student in mathematics at Stanford for example when I asked u53 to give plausible completions it gives became known for his advocacy of the use of psychedelic drugs or homeless Advocate um and I mean this sound plasma maybe the ground truth in this case from Wikipedia is he murdered his former advisor which might be quite hard to predict given this prefix um and it turns out that a if I give gpt3 a prefix such as the follow is a python function that when given the list 132 returns one two three it'll complete exactly with this program deaf short list lsd.sort return list um which depending on what year you were born is quite surprising right like it's it's quite amazing that a model that can predict California from Stanford University is located in with the exact same mechanism can generate valid python code um so this was a realization that people made very quickly after gpt3 came out right like given simple Python block strings it was able to generate python functions that implemented those dark strings even without having been trained explicitly for that or even code was not like a large part of Jupiter 3's training set anyway um so the natural question was like how how far I'm going to push that capability right so code is massively available available on the internet GitHub has tens of millions of open source repositories like actually over 120 million uh as as of I think end of last year so what happens if you just train a language model on a lot of code right so that was basically the idea behind open air codecs which is the name of the language model that backs GitHub co-pilot just out of curiosity how many of you have used Copilot okay less than I then I would have thought maybe 30 percent um yeah so co-pilot is this basically auto complete on steroids that runs uh this language model called codex on the back end great and as a lot of papers in in this age we live in the technical the technical description of what was done was we took the architecture of gpt3 maybe change the number of parameters and trained on this data uh yes models would you just be looking um kind of for um similarity to Gold Standard code or are you also checking for like yeah that's a great question we'll talk a lot about how these these models are evaluated in in some settings the the answer just joking ahead a little bit would be that we'll mostly execute the code and see what it does rather than just comparing to reference Solutions um there is a literature on how to evaluate when you can't do that but um s happens with natural language people realize that blue scores and adaptations are not actually very good for functional correctness espec especially in code where you can change one token and not change the blue score by much but completely change the the semantics of the program yes in the training data did we include natural language like uh for like it like if we have like a function in Python just like natural language like describing what the function does like in a comment or something uh yeah so the question was did they includes natural language in the training data and yes in true forms so code already has a lot of natural language like comments and strings and this was all kept like none of them none of it was stripped um so that's one form of natural language that that codex got and the other one was just a subset of the training side of vp3 so it was not training 100 just code it also had like so fun in it when you get over there examples of Life a natural language description of a function and then the corresponding python uh so the question was was there a description like were there examples in the training date of a description and then the function yeah um yes um so that there are some examples of that form that naturally if you're on GitHub uh they're not a lot compared to all all code that exists we'll talk a little bit about dude yes yes yeah so the web has a lot of of that kind of thing in general right um one of we'll be talking about one experiment that they did on fine tune it exactly that format and then has an impact because most code is not written like that on the internet although some fraction definitely is um the answer um yes so the version one of codex was essentially the same architecture as gpd3 which is a decoder only transformed model but with 12 billion parameters and then train on a training set that was constructed mostly from GitHub but also natural language sources um yeah so how to evaluate a model right like we trained it and we can prompt it with a few examples and see that it does interesting things but how do we we get a better sense of of its capability um so the the author is in the paper uh in the Codex paper they set up this challenge of given a python lock string just generate a function that implements that dot string and where the doc string all always had input output examples in the form of assertions so in this example here on the right which is one from the paper right um so the first one uh the the goal is to return a list with all the elements increased by one so you would infer that the elements or numbers and then they give two examples which are like Pi dog tests you can actually run these tests uh automatically right so if I call it with one two three it should return two three four and they give one more example and besides those examples because if you as machine learning people uh you should know if you just give all the examples that are evaluating on you're subject to the program just working on those examples but not on held out examples so for each of these problems they of course also had held out inputs that the mod was evaluated on but since this father has seen a lot more code than any person has any chance of ever looking at in their lifespan how do you even know that the problems that you're giving have not been seen before so this becomes an increasingly difficult challenge with these large models so they did a best attempt which was to create a data set of their own since the goal here is not to train on that data set you don't need that many examples as as you would need to to train a model from scratch so they came up with these 164 problems of this form that they basically manually author it so that's a way of of saying that okay the model at least hasn't seen these problems in this exact form right um and for each one they had a set of hidden tests so here the evaluation will be does the generated program run correctly on all the tests the the scene and unseen ones and the main metric that we'll be looking at is what they call Passat K which is the probability that out of case samples of programs that I take from the model at least one of them passes all of the the tests and the main result here is that gpt3 which is also a quite large model trained and a lot of code uh relatively speaking thus exactly uh at zero in this Baseline in this Benchmark that they came up with so it doesn't solve any of the problems which is good they're at least not trivial trivia problems right and all of the Codex models have some non-trivial performance so codex alone looking at pass at one which is like just sample one program from the model that's above 20 percent and of course we have to take all these numbers relative like 20 in general doesn't mean much but it solves some problems that gpt3 alone doesn't solve right and and um if they generated a set of problems with this exact format of python.string another function to evaluate if this format was kind of unusual for the model right so they are kind of synthetically generated uh like a training center fine tune and called the resulting model called Xs and yes codex that's a little bit better well so it seems like there's something to be uh there's there's a little bit of benefit of design training date exactly with this format and besides just sampling one program and returning that as your answer one theme that we'll see here is that it's usually worth your sample a lot more programs and somehow choose which one is your best bet one simple way to do that is just by taking the model's load probability over the sample right so this is the the the red line here which improves on both of the others and if you look at the the examples that sir oh yes so the purple line is the Oracle re-ranking which is basically like if I take all the programs that are generated and actually run them on the on the the hidden tests and take the ones that pass the hidden tests then so what the purple line is saying is that it's often the case that codex generates some program that satisfies all the tests but it might be hard to identify without actually running the program which one is it um so we'll look if you look at the examples of samples from the model it's it's quite non-trivial right so if I describe a function like def is prime uh returns true if a number is prime which is of course a problem that the model has seen before in some form um it will fail a lot of times but most of the times it will do something reasonable so here we will try to you see that it's trying to test for divisors of the number in this case it's just missing the corner case that's true I think or no that one is returning as a prime number it often returns the same program so by resampling you don't have any guarantees it's Illustrated on GitHub so it it's also seen a lot of incomplete code so it might say to do pass do it later um but yeah sometimes it works sometimes it'll do exactly the the primary test with all the corner cases and all and if you specify a more complicated function with maybe some more coordinate cases error again in this case it will not solve it completely with any of the the samples but a lot of the samples are surprisingly reasonable right like it will often at least partially do what uh the specification is asking you yes how difficult are those tests and they're like the store that made by humans to specify even some tasks are more difficult than others yeah so uh so the question is how hard are the tasks in general and these problems are not hard for human programmers um in general so they test basically like basic capabilities of coding in Python um so there's this is maybe a problem of like median difficulty in the the training set in the data set right like a functional like counts vowels but has a special case for why why should only be about if it's at the end for example so this is the general flavor of these problems in the Codex paper we'll talk about different data sets that makes sense um yeah so the finding here oh yes so like it fails in a lot of cases but many times producing is reasonable guesses of what the function should do and one thing to notice is that one thing that they noticed was which was an important observation for many of the works that came after is that there seems to be quite a large benefit in just sampling more programs and trying more so the space of fragrance that the model can can generate usually contains some correct programs and when simply more there is a trade-off between the sampling temperature and How likely it is that the program is correct right so if I sample with a temperature zero then I basically get deterministic Behavior I don't get any benefit from from Recently but if I sample with too high of a temperature then I I get more and more random outputs right um uh yeah so but of course just uh sampling more programs is maybe fine for this kind of evaluation with the Benchmark but when interacting with a user I of course don't want to give the user 100 options to choose from for instance like there's a hyper I believe that one of these many programs satisfies what you want but I don't know which one it would not be very usable um so of course I could just sample a small number of programs but knowing that it's usually the case that in a large number of samples one of them will be correct it a lot of times makes sense to sample a large number of programs and then try to re-rank them in some way and then only show maybe my top guesses so the Oracle here would be I ran all the programs in a test but a lot of times I don't have that like if I'm in the middle of writing function writing I want some some guess for how to write a certain line of the function and might not have tests for for that specific line um but I can for example use the models on log probability distribink and yeah what they found was that basically taking the average token block probability among a number of slightly more fancy ways of trying to rank was was the best that they could get um and here we were trying to sample code given box string but one of the Magics of language models is that I can just condition them anything to try to get anything I'm not guaranteed to get good things but I can always try um so what if you try to use the model to give me a doc string given the code so basically describe what a function does so that's a very natural version of the problem that that we had before and that kind of data is certainly way less frequent in the in the training set although it certainly exists in some cases because naturally in python.strings comes before the code but this is also very common thing with code data I can usually manufacture synthetic data sets that change the structuring in some ways right so I can basically write a deterministic program that takes python functions and inverts the the code in the dark string and make a training set for this task and in this case I lose the ability to automatically evaluate if a dog string actually describes the code um that's well like I get a problem with natural language generation where Lisa talked about evaluations quite hard in the Codex paper they evaluated this by hand so basically pass that k of where fastest a human said that the doctrine describes the function and surprisingly this task seems to be harder than generating code from Doc strings itself so even a fine-tuned model like so here codex s is the the Codex that we saw that was fine to intro to solve the tasks and codex D was fine-tuned on this data set of generating blocksprings given code and in this case they didn't get any benefits from fine tuning or any improvement from the base model that they started with um so it seems like maybe describing code is not that easy compared to writing um so sorry I have a question ensure that the programs that are generated compiled like we take advantage of like parts yeah so direction is how do you know that they compile in this case they just literally save the code and ran with the python so if it through an exception it failed basically if it ran and produced the exact output then it succeeded um curious because to other people we just think of it as like a reading comprehension gymnastics and like because I couldn't actually think of a measurement there but like is there any similarity between that has been the evaluate that path and the specific uh has to describe yeah so the question was can we see it as like a reading comprehension task of sorts um for code and and yes basically it's it's a waiter probe how well can the model understand quote-unquote what the code does that is one task that is like of code understanding you know so to speak another one is code execution like given this code in this input what output does it produce which I'll talk a little bit about but it's it's also quite a hard task for for these models so they're often able to produce code that works but if you give it the code then the input it's hard to predict what the code does from the one that makes sense um yeah so how how is code a different core position is it more difficult um yeah I think more or less difficult to depends your home right like an average human certainly can't describe what a python function does but not necessarily because like it's inherently more complex task um so I guess it depends on who who you ask um yeah the examples of the model got wrong is there a way to do an analysis of the source of the error like if there was an error with the algorithm versus just like a syntax error yeah so the question is what kind of Errors does the model do make and can we evaluate it automatically yes I didn't include this here but one of the papers that I'll talk a little bit about did this analysis of what what kind of of error does the model make at different scales and the result there was that as the models grow in in number of parameters they tend to make less syntactic errors and less compilation areas and have more semantic errors like program still runs but fails on some tests um and at the the smaller size is it's way more common to get like syntax errors like didn't close the parenthesis um okay so as you've noticed the base technology here was still just transformed right we're sampling from a Transformer and running the code and maybe sampling more and re-ranking using log probabilities but not nothing extremely specific true to code besides the fact that we can execute the output deepmind came up with Alpha code which was very talked about I'm sure at least some of you have heard of it which was basically a system that expanded on these ideas of training language models to generate code and in this case their target was to solve programming competition problems which some of you might heard about these are compositions just like math competitions but where the challenges to come up with algorithms then then write code that that solve a computational problem um and the base the foundation of alpha code was still sampling from Transformers a lot of their technical design choices was were basically targeted allowing faster sampling right so they came up with a cheaper version of attention where you you share the the key value has but have multiple query heads because that was a an engineering bottleneck in their sampling and the user encoded decoder Transformer because was faster to just encode the problem once but aside from that very similar ideas so they pre-trained their Transformer on basically in this case mostly code I think from their description it was basically just a data set composed of GitHub code where the encoder was additionally trained with mass language modeling laws and they fine-tuned the model then on a much smaller data set of Human Solutions to programming competition problems which are much sparser than like arbitrary GitHub code um they used one variant of reinforcement learning fine tuning called gold not oral HF but kind of similar idea just in the the spirit that you don't want to penalize the model for not being able to produce all valid Solutions you just wanted to be able to Output some solution so if it's if assembly from the model is giving you some solution then it should be getting the reward and one interesting trick that they they did was value conditioning so basically since we don't have that many submissions to these competitive programming problems it's it's a little bit bad to Simply discard all of the wrong Solutions which we have a lot more wrong Solutions than correct Solutions um so we want to train on them somehow but we don't want true make the model generate wrong Solutions right so but but there are still some interesting statistics to be learned there so to train on those solutions they basically designed their training set so that the code starts with a comment that says whether it's correct or incorrect right so I can make training examples where the correct Solutions start with that's the correct solution and the incorrect one say this is an incorrect solution and then at test time of course when generating a program that I want to be corrective start with a comment this is a correct solution but that lets the model and somehow in some way benefit from from seeing correct Solutions as well and the thing that they really pushed in this paper was simply right so in in the Codex paper we're talking of up to 100 samples per problem which is already a lot like it's something that's just using the Codex API you will have a lot of trouble doing in Alpha code they massively paralyzed this into 100 000 samples per problem and as we're talking in if you are to participate in a programming competition and they actually did run Alpha code on on a real one you can't afford at all just submit 100 000 attempts of solving a problem so in some way you have to narrow that down to a very small sets right and in this case they set the limit of making up to 10 submissions which is kind of the range of what a human participant would would do um so how do we do that well the first obvious step is filtering so each of these problems comes with some examples of inputs and outputs so I can immediately discard all the programs that don't satisfy even those example inputs that's already removed like 90 of these 100K samples so um then we still have a quite significant number of programs that work at least on the basic tests so what do we do so what they did was they trained a separate model that generates inputs for a program and for these generated inputs we don't really know the what's the expected output right unless we are really good at interpreting the problem statement but even without knowing what's the expected output I can use those generated inputs to basically group The the programs that I have by Behavior right so so if I generate a string and I run all the programs on that input string some of them produce this result and some of that produce that result then I I'm I can't infer that maybe these programs are semantically the same right so if I if I had true submissions to make maybe I would do one of each instead of true in the same cluster so this was basically what they did they generated a lot of inputs clustered the programs based on their behavior on those inputs and then picked one submission from each of the largest clusters all right permissions to like augment training like how does that help the market the model better yeah so the question is how did how do the wrong Solutions help the model in any way um so they didn't really do an ablation of not training the model on the incorrect solutions to to measure the benefit of that specifically but the intuition is that even the incorrect Solutions have some uh interest information for you to learn from right so you might learn that they are incorrect for example you might learn learn bug patterns so you might learn that if somewhere in the codes I forget to close the parenthesis for example this is probably incorrect um and since in this case we don't really have that much training data any way that you can get to use the training data that you have probably helped that makes sense uh yeah but that's a good question like it's not it's not exactly clear what the model learns from the wrong solution for your submissions before the time is up was this the best use of that information by not looking at that at all since you submit though 10 you get the extra students look trying to incorporate the feedback you get from The Grater yeah so in the combinations that they tried was just basically called forces you only get like a binary response did it was it accepted or not um yes it's a harsher than ioi for example um yeah so the result uh in offline experiments of basically solving these problems from a benchmark that they they collected was basically that if your sample more you solve more problems um so they get this log linear scaling with how many programs they sample at all of the the models case that they tried it's which essentially means that if you sample 10 times more problems uh more programs your self rate increases in this lender rate of six percent approximately uh and also with compute so with how many TPU days they took to train the model it has also roughly log Learners KO um and also TPU seconds span sampling for each Pro and so that was in a like an offline evaluation written a set of problems that they they collected but they also tried this model live on competitions on this website called code forces and their model did get non-trivial performance in a bunch of uh contests so they they actually ran this in past context did they run it live but they tried to simulate as much as possible the the setting where you would be in the competition and yeah in some of the contests they would place in the top 30 top 50 percent or like a medium uh coder in uh and division two which is important to know this so so as as they described in the paper this is like approximately a few months straight year of training programming which is not to say that they're like wearing these competitions anytime soon but it's at the same time not trivia um and the main uh the main component of getting the performance that they did was sampling sampling more programs right so these they did all this engineering to make sure that they could sample 100K programs uh per problem and they had like an accumulation of techniques like uh the mlm3 trainer on the encoder um like trying with random assembling with a random problem tags and the gold fine tune and all that and none of them would have helped if at test time they were just doing a thousand samples so the effects of all of basically all of those techniques only showed up when they scaled it up 200k to a million samples so on one hand this shows the potential of like very simple set of techniques that you've seen in this class of just sampling things from Transformers but taken at this extreme scale but of course this also shows a limit right so at this rate of having to take 10x more samples you got six percent more problem solved this won't get to division one and anytime soon so we have to do something different if that's the goal um and one kind of problem that seems to be inherent to these models if all you're doing is just like sampling complete programs is this challenge that humans don't really have with compositionality um so this is back to result that was presented in in the Codex paper um and if you ask a person that knows basic Python programming how to solve problemax and they say it's trivial like reverse a string for example and if you separately ask them how do you compute the length of a string and they also think that's trivial if if you give them the problem can you reverse the string and then take the length they'll say okay of course like that's a very simple composition of two things that are true um but that does not seem to be the case with this code language models so the the Codex author is did the experiment where they manufactured tasks by basically chaining these very simple tasks and the result was that as the number of these components grow the probability that the samples from the model solves the the composite problem Decay is kind of explanation even if the model knows how to do each of the components individually so this is something which is a challenge to these models and not to people yet um yeah so just some quick takeaways uh it seems like Transformers just trained that scale on code have non-trivial performance in this task and these results maybe for people that You Know download copilot and just test it and it sort of works don't seem that surprising but for the Perkins synthesis field that had been for decades working on these very specific very constrained domain-specific languages these results were just unimaginable a few years ago and it seems like sampling and testing and filtering can get quite far but it also gets expensive quite fast right so the alpha code for example just training and evaluating their largest model used the equivalent energy of like 16 American households a year for example like we can't also have everyone using these models at this scale all the time um and the other caveat here of course is that this setting where you get the extremely well specified problem uh which has tests and you can run the program and determine exactly when it passes all the tests is very different from Real World programming right when where most of the time is spent uh understanding what's the problem deciding what to do revising the tests a lot of time spent editing code and so on so there's a lot of progress being made but this of course still has a lot to do yes one one question is is it similar to a question as earlier if we can do error analysis is it possible because one thing when we're doing this type of code generation is if we just assume it to be right it's a lot harder for us to debug our code because we didn't provide it ourselves are there any kind of like ideas of like things people are considering in the field for how to go about like debugging code that was written by an AI other like AI debuggers as well yeah um so the question was about uh debugging I think they're true things so one of them is yeah I had a lot more things that I didn't get true but one of them was this um this notion of automation bias which people have which is we have a general tendency to believe things that are automated and this is quite a problem for example there was this study run here at Stanford even where it seemed like codex introduces security bugs at a non-trivial rate for example and it's yeah it's still hard to use these models without understanding what they're doing and the problem of kind of doing this process more interactively of like writing the program and then look at what it does and then maybe revising the code is still much harder than just trying to write the program from scratch exactly because well one of the reasons is certain that we don't have that much data on that process happening with people right we we see GitHub which is kind of the the published version of the code but all the processes you get from like an empty file to that still not recorded but yeah that's a very active area of research as well like uh mothers to revise and edit and debug yes so our real time 550 oh okay so we do have time to talk about something okay awesome um yes uh so I'll try to cover a little bit so one a fun thing connecting to to what we talked about uh back is that codex can do some simple pragmatic reasoning so for example if I if I give you these inputs uh list 132 returns one two three you'd probably say sorting right it sorts the list but what about one two three returns one two three probably just identity but it could also be sorting right it's consistent with sorting as well but you would reason that if I wanted to specify the Sorting function I would probably give a different input and if I give these inputs to codecs it does predict that the first one is sorting but the second one is identity just an interesting thing to come out of just regular language modeling training um there are also experiments with using these models in a dialogue style which is a little bit more about like the question asked so if I can actually try to function and then a model comes up with some implementation but maybe it has an error right they can sometimes describe the change like oh but can you do it in starting reverse or only return the top four results and can often revise what it did which is quite interesting um yes so last topic here is using programs not as the output that you have that you want from the model directly but rather as a representation for for other things so one uh General thing about humans is that we're our efficacy in a lot of tasks depends on using external tools right so if I ask you to multiply these two numbers one two three four five six um you can do it but you probably won't just do it in your head right you use a calculator or if I ask you what time is it well you don't keep track of time that precisely right so you use a clock or what are the five largest airports in the world you do some Google search you figure it out but if you want just take it out of your head and when we are training language model to just give us answers condition on the question or maybe on on some context we're basically asking its truth come up with the answer all by itself and a lot of these problems aren't reasonably solved in that manner the problem with just telling what time is it for example is one that you fundamentally can't get out of a model that was trained and Frozen and it has to produce an output now and for example you know those this language models that came out last year called Minerva which was trained on mathematical problems and and solutions and it a lot of times got the strategy right in solving these problems but still makes a lot of arithmetic errors so it says okay the solution would be this number plus this number equals something wrong for example um so that seems it seems limiting that we're asking the model to do all these things by itself so this open AI paper from 2021 had this very simple idea of solving math word problems uh using language models but providing them with a calculator and the way to let the model use a calculator is basically to assign a special input token a special token in the input such that when the model generates that token your decoder instead of keeping conditioning on on the model's probabilities will then deterministically do something with input like call a calculator and paste the output in in the model's output sequence right so they generated this training set kind of semi-automatically where solutions for mathwork problems would have these annotations in angle brackets and by seeing those annotations at in the training set and for training you don't really need to do anything special at test time you can give the model a calculator by basically watching until the moment where adult puts an equal sign and then once it does instead of generating from the model you can take the numbers that come before the call calculator and then just paste the the exact output after right and this as you can imagine gives a quite significant boost in solving these problems because you kind of isolate one kind of of air like the model what make arithmetic areas anymore um this same idea but taken a little bit for their was used to self-worth problems but instead of the model outputting the solution in natural language it kind of interspersed natural language with python code and the final answer was not given by the model but by running the python code that is provided so here's an example you can look at it in more detail later and this also gives a big benefit over just having the model try to figure out what's the answer in its own um and more generally there was this paper that came up on archive called two former where they basically extended this idea a little bit farther um with a self-supervised approach of let's say you come up with a list of tools and you want to kind of teach them all how to use those tools um and in this case they they tried quite a few um two so one of them was a calculator another was a machine translation system so when the model output in its when decoding from the one that was like empty and a string you go and call another newer Network which is a translation and do the translation that pastes that back um another one was doing search on Wikipedia so for example and or or calling a question answering system and with the right set of techniques to teach the model how to Output these sequences you can get a very interesting behavior of the model kind of deciding on the Fly which which tool to use and yeah so so this is basically the program here is not the final result that you want but it's really just a way for to represent this usage of external tools um yes um so we talked a little bit about this before so I guess one natural question for people graduating Computer Sciences will I have a job after I graduate or will codex replace me and as it turns out in software engineering a lot of time is not spent writing code in the real world right so there's there's one study but there are a lot more that that show that uh when you track developers time they spent a lot of time just reading code um a lot of time outside of the IDE this is just ID time right navigating and five percent is actually added in code and even editing code a lot of time is not write a new code but rather like fixing bugs and and maintain it so there's quite a lot of time that's not spent writing code for people that are paid to write code and yeah and like there's this whole process of deciding what tribute which is usually more important than just writing just muting the thing right in this is yeah this still quite far from from what codex can do and yeah there's this notion we talked a little bit about that debugging is very interactive we run go back revise and this is this process is mostly lost by just sampling more from the model and trying again basically from scratch and there's active research even here at Stanford done using model is also true to fix bugs automatically read the red a program has some syntax error how to go back and maybe change but still very different from the more open-ended kind of debugging that people can do um yeah of course even all the code on GitHub is still not all the code that you can imagine right like there are new libraries all the time there's internal libraries for companies that will just not be on GitHub at any point so there are challenges in teaching models to use those as well um and as we mentioned even if models can generate code they still fail a lot of code understanding challenges like just executing code for example like asking what this code outputs um and even fine-tuning doesn't seem true to solve that problem and yeah and the other thing is that public code also has a lot of bugs as you can imagine and they're being fixed all the time so training on called will also mean that sometimes they generate buggy code and so you still have to understand what the model outputs and their security uh books that can be introduced by language models as well and yes so just to conclude a little bit past time a lot of these capabilities were completely out of reach even a few years ago this is a really exciting time to be watching this happen um and I think there's there's a fascinating intersection here between natural language which is extremely ambiguous and flexible and contextual and we do it so easily and programming languages are there these extremely rigid languages you forget a parenthesis and compiler has no idea what they're trying to do anymore um and we can bridge between these two words very easily written Now language models are also starting to um and besides models that write programs programs are also just a general interesting representation for reasoning for you can represent mathematics legal contracts this notion of calling and combining different tools um yeah so all of these are very active topics of research so hope you guys enjoy yes 
","['', 'program synthesis', 'Perkin synthesis', 'logical specifications', 'natural language description', 'output program correctness', 'test cases', 'sorting an array', 'mathematical formula', 'permutation', 'hidden tests', 'Passat K', 'Codex model', 'Xs model', 'sample a lot more programs', 'Oracle re-ranking', 'Alpha code', 'filtering', 'clustering programs', 'augment training data', '']"
"welcome to cs224n uh lecture seven teen uh model analysis and explanation okay look at us we're here um uh let's start with some course logistics um we have uh updated the policy on the guest lecture reactions um they're all due friday um all at 11 59 pm you can't use late days for this uh so please get the men um watch the lectures they're awesome lectures they're awesome guests um and you get something like half a point for each of them and yeah all three can be submitted up through friday um okay so uh final projects remember that the due date is tuesday it's tuesday at 4 30 p.m uh march 16th and let me uh emphasize that there's a hard deadline on the three days from then friday we won't be accepting for additional points off assignments i'm sorry final projects that are submitted after the 4 30 deadline on friday uh we need to get these graded and get grades in so um it's the end stretch week nine our week 10 is really the lectures are us giving you help on the final project so so this is really the last week of lectures thanks for all your hard work um and for asking awesome questions in lecture and in office hours and on ed and let's get let's get right into it so um today we get to talk about one of my favorite subjects in natural language processing it's model analysis and explanation um so first we're going to do what i love doing which is motivating why we want to talk about the topic at all we'll talk about how you know we can look at a model at different levels of abstraction to perform different kinds of analysis on it we'll talk about out-of-domain evaluation sets so this will feel familiar to the to the robust qa folks um then we'll talk about uh sort of trying to figure out for a given example why did it make the decision that it made it had some input it produced some output can we come up with some sort of interpretable explanation for it and um then we'll look at actually the representations of the models so these are the sort of hidden states the vectors that are being built you know throughout the processing of the model try to figure out if we can understand some of the representations and mechanisms that the model is performing and then we'll actually come back to sort of one of the kind of default states that we've been in in this course which is trying to you know look at model improvements removing things from models seeing how it performs and relate that to the analysis that we're doing in this lecture show how it's not all all that different uh okay um so if you haven't seen this xkcd um now you have and it's one of my favorites i'm going to say all the words so uh person a says this is your machine learning system person b says yup you pour the data into this big pile of linear algebra and then collect the answers on the other side person a what if the answers are wrong and person b just stir the pile until they start looking right and i feel like at its worst deep learning can feel like this from time to time you have a model maybe it works for some things maybe it doesn't work for other things you're not sure why it works for some things and doesn't work for others and you know the changes that we make to our models you know they're based on intuition but frequently you know what are the tas told you know everyone in office hours like ah sometimes you just have to try it and see if it's going to work out because it's very hard to tell um it's very very difficult to understand our models on sort of any level and so today we'll go through a number of ways for trying to sort of carve out little bits of understanding here and there um so so uh beyond it being you know important because it's in the next kcd comic what why should we care about what our models about understanding our models one is that we want to know what our models are doing so here you have a black box black box functions are sort of this idea that you can't look into them and interpret what they're doing you have an input sentence say and then some output prediction maybe this black box is actually your uh your final project uh model and it gets some accuracy now we summarize our models and in your final projects you'll summarize your model with sort of one or a handful of of summary metrics of accuracy or f1 score or bluescore or something but it's a lot of model to explain with just a small number of metrics so what do they learn why do they succeed and why do they fail what's another motivation so we want to we want to sort of know what our models are doing okay but but uh maybe that's because we want to be able to make tomorrow's model so today right when you're building models in this class at the company you know you start out with some kind of recipe that is known to work either at the company or because you have experience from this class um and it's not perfect right it makes mistakes you look at the errors and then over time you know you take what works maybe and then you find what needs changing so it seems like maybe you know adding another layer to the model helped uh and maybe that's that's a nice tweak and the model performance gets better etc and um you know incremental progress doesn't always feel exciting but i want to pitch to you that it's actually very important for us to understand how much incremental progress can kind of get us towards some of our goals so that we can have a better job of evaluating when we ne when we need big leaps when we need major changes because there are problems that we're attacking with our incremental sort of progress and we're not getting very far okay so we want to make tomorrow's model um another thing that's i think very related to and sort of a both a part of and bigger than this field of analysis is model biases so let's say you take your word to vect uh analogies solver you know from um from glove or word to back that is from assignment one and you give it the the analogy managed to computer programmer as woman is two and it gives you the output homemaker this is a real example from the paper uh below um you should be like wow well uh i'm glad i know that now and of course you saw the lecture from yulia svetkov last week you said wow i'm glad i know that now and that's a that's a huge problem what did the model use in its decision what biases is it learning from data and possibly making even worse so that's the kind of thing you can also do with model analysis beyond just making models better according to some sort of summary metric as well and then another thing we don't just want to make tomorrow's model and this is something that i think is super important uh we you know we don't just want to look at that time scale we want to say what about 10 15 25 years from now what kinds of things will we be doing you know what are the limits what can be learned by language model pre-training what are the what's the model that will replace the transformer what's the model that will replace that model what does deep learning struggle to do what are we sort of attacking over and over again and failing to make significant progress on what do neural models tell us about language potentially there's some people who are primarily interested in understanding language better using neural networks cool how are how are our models affecting people transferring power between groups of people governments etc that's an excellent type of analysis what can't be learned via language model pre-training so that's sort of the complementary question there if you sort of come to the edge of what you can learn via language model pre-training is there stuff that we need total paradigm shifts in order to to uh to do well so all of this i mean you know falls under some category of trying to really deeply understand our models and their capabilities and there's a lot of different methods here that we'll go over today and one thing that i want you to take away from it is that they're all [Applause] they're all going to tell us some aspect of the model elucidate some kind of intuition or something but none of them are are we going to say aha i really understand 100 about what this model is doing now um so they're going to provide some clarity but never total clarity and one way if you're trying to decide how you want to understand your model more i think you should sort of start out by thinking about is at what level of abstraction do i want to be looking at my model so the sort of very high level abstraction let's say you trained you know a qa model to estimate the probabilities of start and end indices and you know in a reading comprehension problem or you've trained a language model that assigns probabilities to words in context you can just look at the model as that object so it's just a probability distribution defined by your model you are not looking into it any further than the fact that you can sort of give it inputs and see what outputs it provides so that's like not even who even cares if it's a neural network it could be anything but it's a way to understand its behavior another level of abstraction that you can look at you can dig a little deeper you can say well i know that my network is a bunch of layers that are kind of stacked on top of each other you've got sort of maybe your transformer encoder with your one layer two layer three layer you can try to see what it's doing as it goes deeper in the layers so maybe your neural model is a sequence of these vector representations a third option of sort of specificity is to look you know as much as as at as much detail as you can you've got these parameters in there you've got the connections in the computation graph so now you're sort of trying to remove all of the abstraction that you can and look at as many details as possible and all three of these sort of ways of looking at your model and performing analysis are going to be useful and will actually sort of travel slowly from one to two to three as we go through this lecture okay so we haven't actually talked about any analyses yet uh so we're going to get started on that on that now um and we're starting with the sort of testing our model's behaviors so would we want to see will my model perform well i mean the the natural thing to ask is how does it behave on some on some sort of test set right and so uh we don't really care about mechanisms yet why is it performing this by what method is it making its decision instead we're just interested in sort of the more higher level abstraction of like does it perform the way i want it to perform so so let's like take out take our model evaluation that we are already doing and sort of recast it in the framework of analysis so you've trained your model on some samples from some distribution so you've got input output pairs of some kind so how does the model behave on samples from the same distribution that's a simple question and it's sort of uh you know it's known as you know the in-domain accuracy or you can say that the samples are iid and that's what you're testing on and this is just what we've been doing this whole time it's your test set accuracy or f1 or blue score and you know um so you've got some model with some accuracy and maybe it's better than some model with some other accuracy on this test set right so this is what you're doing as you're iterating on your models in your final project as well um you say well you know on my test set which is what i've decided to care about for now model a does better they both seem pretty good and so maybe i'll choose model a to keep working on maybe i'll choose it if you were putting something into production um but remember back to you know this this idea that it's just one number to summarize a very complex system uh it's not going to be sufficient to tell you how it's going to perform in a wide variety of settings okay so so we've been doing this this is model evaluation as model analysis um now we're going to say what if we are not testing on exactly the same type of data that we trained on so now we're asking did the model learn something such that it's able to sort of extrapolate or perform uh how i want it to on data that looks a little bit different from what it was trained on and we're going to take the example of natural language inference so to recall the task of natural language inference and this is through the multi-nli data set that we're just pulling our definition you have a premise he turned and saw john sleeping in his half tent and you have a hypothesis he saw john was asleep and then you give them both to a model and this is the model that we had before that gets some good accuracy and the model is supposed to tell whether the hypothesis is sort of implied by the premise or contradicting um so it could be contradicting maybe if the hypothesis is you know john was awake for example or he saw john was awake maybe that'd be a contradiction neutral if sort of both could be true at the same time so to speak and then entailment in this case you know it seems like they're saying that you know the premise implies the hypothesis and so um you know you would say probably this is likely to get the right answer since the accuracy of the model is 95 95 of the time it gets the right answer um and we're going to dig deeper into that uh what if the model is not doing what we think we want it to be doing in order to perform natural language inference so in a data set like multi-nli the authors who gathered the data set will have asked humans to perform the task and you know gotten the accuracy that the humans achieved and models nowadays are achieving accuracies that are around where humans are achieving um which sounds great at first but as we'll see it's not the same as actually performing the uh the task more broadly in the right way so what if the models not doing something smart effectively we're going to use a diagnostic test set of carefully constructed examples that seem like things the model should be able to do to test for a specific skill or capacity in this case we'll use hans so hans is the heuristic analysis for analyze systems data set and it's intended to take systems that do natural language inference and test whether they're using some simple syntactic heuristics what we'll have in each of these cases we'll have some heuristic we'll talk through the definition we'll get an example so the first thing is lexical overlap so the model might do this thing where it assumes that a premise entails all hypotheses constructed from words in the premise so in this example you have um the premise the doctor was paid by the actor and then the hypothesis is the doctor paid the actor and you'll notice that in bold here get the doctor okay and then paid and then the actor right and so if you use this this heuristic you will think that the doctor was paid by the actor implies the doctor paid the actor that does not imply it of course and so you know you could expect a model you want the model to be able to do this it's somewhat simple but if it's using this heuristic it won't get this example right next is uh subsequence uh heuristics so here at the prem if the model assumes that the premise entails all of its contiguous sub sequences it will get this one wrong as well so this example is the doctor near the actor danced that's the premise the hypothesis is the actor danced now this is a simple syntactic thing the doctor is doing the dancing near the actor is this prepositional phrase and so the model sort of uses this heuristic oh look the actor dance that's a subsequence entailed awesome then it'll get this one wrong as well and um here's another one that's a lot like subsequence but so if if the premise if the model thinks that the premise entails all complete subtrees so this is like sort of fully formed phrases so the artist slept here is a fully formed sort of it's a sub tree if the artist slept the actor ran and then that's the premise does it entail the hypothesis the actor slept uh no uh sorry the artist slept that does not entail it because this is in that conditional okay i only pause here for some questions before i move on to see how these models do anyone unclear about how this sort of evaluation is being set up nope okay cool okay okay so um so how do models perform that's sort of the question of the hour um what we'll do is uh you know we'll look at these results from the same paper that really released the data set so they took four strong multi-analyze models with the following accuracies so the accuracies here are something between 60 and you know 80 something 80 percent bert over here is doing the best okay and um in domain right in that first sort of setting that we talked about uh you get these reasonable accuracies and that is sort of what we said before about it like looking pretty good and when we evaluate on hans in this setting here we have examples where the heuristics we talked about actually work so if the model is using the heuristic it will get this right and it gets very high accuracies and then if we evaluate the model in the settings where if it uses the heuristic it gets the examples wrong um you know maybe bert's doing like epsilon better than some of the other stuff here but it's uh it's it's it's a very different story okay and you saw those examples they're not complex in our sort of own idea of complexity and so this is why it sort of feels like a clear failure of the system now you can say though that well maybe the training data sort of wasn't didn't have any of those sort of phenomena so the model couldn't have learned uh not to do that and that's sort of a reasonable argument except well you know bert is pre-trained on a bunch of language text so you might hope you might expect you might hope that it does better okay so so we saw that example of um models performing well on examples that are like those that it was trained on and then performing not very well at all on examples that seem reasonable uh but are um sort of a little bit tricky now we're going to take this idea of having a test set that we've carefully crafted and go in a slightly different direction so we're going to have what does it mean to try to understand the linguistic properties of our models does it so that syntactic heuristics question was one thing for natural language inference but can we sort of test how the models whether they think certain things are sort of right or wrong as language models and the first way that we'll do this is we'll ask well how do we think about sort of what humans think of as good language how do we evaluate their sort of preferences about language and one answer is minimal pairs and the idea of a minimal pair is that you've got one sentence that sounds okay to a speaker so this sentence is the chef who made the pizzas is here it's called it's an acceptable sentence at least to me um and then with a small change a minimal change um uh the sentence is no longer okay to the speaker so the chef who made the pizzas are here and um this whoops this should be present tense verbs in english present tense verbs agree in number with their subject uh when they are third person um so chef pizzas okay um and um uh this is sort of a pretty general thing most people don't like this it's a misconjugated verb and so the the syntax here looks like you have the chef who made the pizzas and then uh this arc of agreement and number is requiring uh the word is here to be singular is instead of plural are despite the fact that there's this ver this noun pizzas which is plural closer linearly comes back to dependency parsing we're back okay um and what this this looks like in the tree structure right is well you know chef and is um are attached in the tree um you know chef is the subject of is pizzas is down here in this sub tree and so that subject-verb relationship has this sort of agreement uh thing so um this is a pretty sort of basic and interesting property of language that also reflects the syntactic sort of hierarchical structure of language so we've been training these language models sampling from them seeing that they get interesting things and they tend to seem to generate syntactic content but does it really understand or does it behave as if it understands this idea of agreement more broadly and does it sort of get the syntax right so that it matches the subjects and the verbs um so but language models can't tell us exactly whether they think that a sentence is good or bad they just tell us the probability of a sentence uh so um before we had acceptable and unacceptable that's what we get from humans um and the language models analog is just does it assign higher probability to the acceptable sentence in the minimal pair right so you have the probability under the model of um the chef who made the made the pizzas is here and then you have the probability under the model of the chef maids that made the pizzas are here and you want this probability here to be higher and if it is that's sort of like a a simple way to test whether the model like got it right effectively and just like in hans we can develop a test set with very carefully chosen properties right so most sentences in english don't have terribly complex subject-verb agreement structure with a lot of words in the middle like pizzas that are going to make it difficult so if i say you know um the dog runs sort of no way to get it wrong because there's no this index is very simple um so we can create or we can look for sentences that have these the things called attractors in the sentence so pizzas is an attractor because the model might be attracted to the plurality here and get the conjugation wrong so this is our question can language models sort of very generally handle these examples with attractors so we can take examples with zero attractors see whether the model gets the minimum pairs evaluation right we can take examples with one attractor two attractors you can see how people would still reasonably understand these sentences right the chef who made the pizzas and prep the ingredients is it's still the chef who is and then you know on and on and on it gets rarer obviously but but you can have more and more attractors and so now we've created this test set that's intended to evaluate this very specific linguistic phenomenon um so in this paper here concur it all trained an lcm language model on a subset of wikipedia back in 2018 and they evaluate it sort of in these buckets that are specified by uh the paper um the sort of under introduced subject verb agreement to uh to the nlp field more recently at least and they evaluate it in buckets based on the number of attractors and so in this table here that you're about to see the the numbers are sort of the percent of times that you get this assigned higher probability to the correct uh sentence in the minimal pair so if you were just to do random or majority class you get these errors oh sorry it's the percent of times you get it wrong sorry about that so lower is better um and so with no attractors you get very low error rates so this is 1.3 error rate with a 350 dimensional lstm um and uh you know with uh with one attractor your error rate is higher but actually humans start to get errors with more attractors too um so zero attractors is easy uh the larger the lstm it looks like in general the better you're doing right so the smaller models doing worse okay and then even on sort of very difficult examples with four attractors which try to think of an example in your head like the chef made the pizzas and took out the trash you know sort of has to be this long sentence and the air rate is definitely higher so it gets more difficult but it's still it's still relatively low and so even on these very hard examples models are actually performing subject verb number agreement relatively well very cool okay uh here's some examples that a model got wrong this is actually a worse model than the ones from the paper that was just there but i think actually the errors are quite interesting so here's a sentence the ship that the player drives has a very high speed now this model thought that was less probable than the ship that the player drives have a very high speed my hypothesis right is that it sort of mis analyzes drives as a plural noun for example it's sort of a difficult construction there i think it's pretty interesting um likewise here this one is so is is fun uh the lead is also rather long five paragraphs is pretty lengthy um so here five paragraphs is a singular noun together because it's like a unit of length i guess um but but the model thought that it was more likely to say five paragraphs are pretty lengthy um because it's referring to this sort of five paragraphs as the five actual paragraphs themselves as opposed to a single unit of length describing the lead fascinating okay um maybe questions again um so i guess there are a couple can we do the similar heuristic analysis for other tasks such as q a classification yes um so yes i think that it's easier to do this kind of analysis for like the hans style analysis with uh with question answering um and other other sorts of tasks because you can construct examples that similarly uh you know have these heuristics um [Music] uh and then have the answer depend on the syntax or not you know the actual probability of one sentence is higher than the other of course is sort of a language model dependent thing but um the idea that you can sort of like develop kind of bespoke test sets for various tasks um i think is very very general and um something i think is actually quite uh quite interesting um yes so i won't i'll go on further but i think the answer is just yes so there's another one um how do you know where to find these failure cases maybe that's the right time to advertise linguistics classes sorry uh you're still very quiet over here how do you find what how do you know where to find these failure cases oh interesting yes how do we know where to find the failure cases that's a good question i mean i think i agree with chris that actually you know thinking about what is uh interesting about things in language is one way to do it i mean the kind of the heuristics that we saw in our language model sorry in our nli models with hans you can kind of imagine that they if the model was sort of ignoring facts about language and sort of just doing this sort of rough bag of words with some extra magic then it would do well about as bad as it's doing here and these sorts of ideas about you know understanding that this statement if the artist slept the actor ran does not imply the artist slept is the kind of thing that um maybe you'd think up on your own but also you'd spend time sort of pondering about and and and thinking broad thoughts about in uh in you know linguistics curricula as well so uh anything else chris um so there's also well i guess someone was also saying i think it's about the sort of intervening verbs example or intervening noun sorry example but the data so the data set itself probably includes mistakes with higher attractors yeah yeah that's a good point um yeah because humans make more and more mistakes as the number of attractors gets larger um on the other hand i think that the mistakes are fewer in written text than in spoken maybe i'm just making that up but that's what i think um but yeah it would be interesting to actually go through that test set and see how many of the errors the really strong model makes are actually due to the sort of observed form being incorrect i'd be super curious okay should i move on yep great okay so um so so what does it feel like we're doing when we are kind of constructing these sort of bespoke small careful test sets for various phenomena well it it sort of feels like unit testing and in fact this sort of idea has been brought in brought to the fore you might say an nlp unit tests but for these nlp neural networks and in particular uh the paper here that i'm sitting at the bottom suggests this minimal function a minimum functionality test you want a small test set that targets a specific behavior that should sound like some of the things that we were that we've already talked about but in this case we're going to get even more specific so here's a single test case we're going to have an expected label what was actually predicted whether the model passed this unit test and the labels are going to be this is going to be sentiment analysis here so negative label positive label or neutral or the three options and the unit test is going to consist simply of uh sentences that follow this template i then negation the positive verb and then the thing so if you if you negation positive verb means you negative verb right and so here's an example i can't say i recommend the food the expected label is negative the answer that the model provided and this is i think a commercial sentiment analysis system i was paused so it predicted positive and then i didn't love the flight it the expected label was negative and then the predicted answer was uh neutral and this commercial sentiment analysis system gets a lot of well you could imagine are pretty reasonably simple examples wrong and so um what rbiro all 2020 showed is that they could actually provide a system um that sort of had this framework of building test cases for nlp models two ml engineers working on these products um and uh give them that interface and they would actually find bugs you know bugs being categories of high error right find bugs in their models that they could then kind of try to go and fix and that this was kind of an efficient way of trying to find things that were simple and still wrong with what should be pretty pretty sophisticated neural systems so i really like this and it's sort of a nice way of thinking more specifically about what are the capabilities um in in sort of precise terms of our models and all together now you've seen problems in uh natural language inference you've seen language models actually perform pretty well at the language modeling objective but then you see uh you just saw an example of a commercial sentiment analysis system sort of should do better and doesn't and um this comes to this really i think broad and important takeaway which is uh if you get high accuracy on the in-domain test set you are not guaranteed high accuracy on even what you might consider to be reasonable out of domain evaluations and life is always out of domain and if you're building a system that will be given to users it's immediately out of domain at the very least because it's trained on text that's now older than the things that the users are now saying so it's a really really important takeaway that your sort of benchmark accuracy is a single number that does not guarantee good performance on a wide variety of things and from a what are our neural networks doing perspective one way to think about it is that models seem to be learning the data set fitting sort of the fine-grained sort of heuristics and statistics that help it fit this one data set as opposed to learning the tasks so humans can perform natural language inference if you give them examples from whatever data set you know once you've told them how to do the task they'll be very generally strong at it but you take your mli model and you test it on hans and it got you know whatever that was below chance accuracy that's not the kind of thing that you want to see so it definitely learns the data set well because the accuracy in domain is high but our models are seemingly not frequently learning uh sort of the mechanisms that we would like them to be learning last week we heard about language models and sort of the implicit knowledge that they encode about the world through pre-training and one of the ways that we sought interact with language models was providing them with a prompt like dante was born in mask and then seeing if it puts high probability on the correct continuation which requires you to access knowledge about where dante was born and we didn't frame it this way last week but this fits into the set of behavioral studies that we've done so far this is a specific kind of input you could ask this for multiple types of multiple people you could swap out dante for other people who swapped out born in for i don't know died in or something um and then you can they're like test suites again and so um it's all connected okay so i won't go too deep into sort of the knowledge of language models in terms of world knowledge because we've gone over it some but you know when you're thinking about ways of interacting with your models this sort of behavioral study can be very very general even though remember we're at still this highest level of abstraction uh where we're just looking at the probability distributions that are defined all right so now we'll go into so we've sort of looked at understanding in in fine-grained areas what our model is actually doing um uh what about sort of why for an individual input is it getting the answer right or wrong and then are there changes to the inputs that look fine to humans but actually make the models uh do a bad job so one study that i love to reference that really draws back into our our original motivation of using lstm networks instead of simple recurrent neural networks was that they could use long context so but like how long is your long short term memory and the idea of kindle kindlewell at all 2018 was shuffle or remove contexts that are farther than some k words away changing k and if the accuracy if the if the predictive ability of your language model the perplexity right doesn't change once you do that it means the model wasn't actually using that context i think this is so cool so on the x-axis we've got how far away from the word that you're trying to predict are you actually sort of corrupting shuffling or removing stuff from the from the sequence and then on the y-axis is the increase in loss so if the increase in loss is zero it means that the model was not using the thing that you just removed because if it was using it it would now do worse without it right and so in uh if you shuffle in the blue line here if you shuffle the history that's farther away from 50 words the model does not even notice i think that's really interesting one it says everything past 50 words of this lstm language model you could have given it in random order and it wouldn't have noticed uh and then two it says that if you're closer than that it actually is making use of the word order that's a pretty long memory okay that's really interesting and then if you actually remove the words entirely you have you can kind of notice that the words are missing up to 200 words away so you don't know the order that you don't care about the order they're in but you care whether they're there or not and so this is an evaluation of well do lstms have long-term memory well this one at least has effectively no longer than 200 words of of memory but also no less so very cool um so that's like a general study for a single model it talks about uh it's it's sort of average behavior over a wide range of examples but we want to talk about individual predictions on individual inputs so let's talk about that so um one way of interpreting why did my model make this decision that's very popular is for a single example what parts of the input actually led to the decision and this is where we come in with saliency maps so a saliency map provides a score for each word indicating its importance to the model's prediction so you've got something like bert here you've got bert bert is making a prediction for this mask the mask rushed to the emergency room to see her patient okay and so and the predictions that the model is making is things with 47 it's going to be nurse that's here in the mask instead or maybe woman or doctor or mother or girl okay and then the saliency map is being visualized here in orange according to this method of saliency called simple gradients which we'll get into emergency her and the sep token let's not worry about the step token for now but emergency in her are the important words apparently and the sub token shows up in every sentence so i'm not gonna yeah um right and so these two together are according to this method what's important for the model to make this prediction to mask and you can see you know maybe some statistics biases etc that is picked up in the predictions and then have it mapped out onto the sentence and this is well it seems like it's really helping interpretability um and uh um yeah i think that this is sort of a a very useful tool and actually this is part of a demo from alan nlp uh that allows you to do this um uh yourself for any sentence that you want um so what's this what's this way of making saliency maps we're not going to go there's there's so many ways to do it we're going to take a very simple one and work through why it sort of makes sense um so the sort of issue is how do you define importance right what does it mean to be important to the model's prediction um and here's one way of thinking about it it's called the simple gradient method uh let's get a little formal you've got words x1 to xn okay and then you've got a model score for a given output class so maybe you've got in the birch example each output class was each output word that you could possibly predict um and then you take the norm of the gradient of the score with respect to each word okay so so what we're saying here is the score right is sort of the unnormalized probability um for that for that class okay so you got a single class you're taking the score it's like how likely it is not yet normalized by how likely everything else is sort of um gradient how much is it going to change if i move it a little bit in one direction or another and then you take the norm to get a scalar from a vector so it looks like this so salience of word i you have the norm bars on the outside gradient with respect to x i so that's if i change a little bit locally x i how much does my score change um so the idea is that a high gradient norm means that if i were to change it locally i'd affect the score a lot and that means it was very important to the decision let's visualize this a little bit so here on the y-axis we've got loss just the loss of the model sorry this should be score it should be score and on the x-axis you've got word space and the word space is like sort of a flattening of the ability to move your word embedding in thousand dimensional space i've just plotted it here in one dimension um and now a high saliency thing you can see that the relationship between what should be score and moving the word in word space you move it a little bit on the x-axis and the score changes a lot that's that derivative that's the gradient awesome love it uh low saliency you move the word around locally and uh the the score doesn't change so that's an interpreter the interpretation is that means that the actual you know identity of this word wasn't that important to the prediction because i could have changed it and the score wouldn't have changed now why are there more methods than this because i'm honestly reading that i was like that sounds awesome that sounds great you know there are sort of a lots of issues with this kind of method and lots of ways of getting around them here's one issue it's not perfect because well maybe your linear approximation that the gradient gives you holds only very very locally right so here the gradient is zero so this is a low saliency word because at the bottom of this parabola but if i were to move even a little bit in either direction the score would like shoot up right so is this not an important word like it seems important to be right there as opposed to anywhere else even sort of nearby in order for the score not to go up so but the the simple gradients method won't capture this because it just looks at the gradient which is that zero right there okay but uh if you want to look into more there's a bunch of different methods that are sort of applied in these papers and um you know i think that there's a good tool for the toolbox okay so um that is one way of explaining a prediction and um you know has some issues like why are individual words being scored as opposed to phrases or something like that um and but for now we're going to move on to another type of explanation and i'm gonna check the time okay cool um actually yeah let me pause for a second any questions about this ah i mean earlier on there were a couple of questions one of them was what are your thoughts what are your thoughts on whether looking at attention weights is a methodologically rigorous way of determining the importance that the model places on certain tokens it seems like there's some back and forth in the literature that is a that's a great question and i probably won't engage with that question as much as i could if we had like a second lecture on this i actually will provide some attention analyses and tell you they're interesting and then i'll sort of say a little bit about um uh you know why they can be interesting without being sort of uh maybe um sort of the end-all of analysis of of uh where information is flowing in a transformer for example um i think the debate is something that we would have to get into in a much longer period of time but look at the slides that i show about attention and the caveats that i provide and let me know if that answers your question first because we have quite a number of slides on it and if not please please ask again and we can chat more about it and maybe you can go on great okay so um i think this is a really fascinating question which also gets at what was important about the input but in actually kind of an even more direct way which is could i just keep some minimal part of the input and get the same answer so here's an example from squad you have this passage in 1899 john jacob ashton iv invested 100 000 for tesla okay and then the answer that is being predicted by the model is going to always be in blue in these examples colorado springs experiments so you've got this passage and the question is what did tesla spend astr's money on that's why the prediction is colorado springs experiments the model gets the answer right which is nice and we would like to think it's because it's doing some kind of reading comprehension but here's the issue it turns out based on this fascinating paper that if you just recruit reduce the question to did you actually get exactly the same you actually get exactly the same answer and in fact with the original question the model had sort of a 0.78 confidence you know probability in that answer and uh with the with the reduced question did you get even higher confidence and that if you give a human this they would not be able to know really what you're trying to ask about so it seems like some things are going really wonky here here's another so here here's sort of like a very high level overview of the method in fact it actually references our input sale and same methods nice it's connected so so um you iteratively remove non-salient or unimportant words so here's a quest here's a passage again talking about football um i think yeah and uh and oh nice okay so the question is where did the broncos practice for the super bowl as the prediction of stanford university um and that is correct so again seems nice and now we're not actually going to you know get the model to be incorrect we're just going to say um how can i change this question such that i still get the answer right so i'm going to remove the word that was least important according to a saliency method so now it's where did the practice for the super bowl already this is sort of unanswerable because you've got two teams practicing you don't even know which one you're asking about so why the model still thinks it's so confident in stanford university makes no sense but you can just sort of keep going and you know now i think here the model stops being confident in the answer stanford university but you know i think this is really interesting just to show that if the model is able to do this with very high confidence it's not reflecting the uncertainty that really should be there because you can't know what you're even asking about okay so what was important to make this answer well at least this part these parts were important because you could keep just those parts and get the same answer fascinating um all right so that's sort of the end of the admittedly brief uh section on thinking about uh input saliency methods and similar things now we're going to talk about actually breaking models and understanding models by breaking them okay cool um so if we have a passage here peyton manning became the first quarterback um [Music] something super bowl age 39 past record held by john elway uh again we're doing question answering we've got this question what was the name of the quarterback who was 38 in the super bowl the prediction is correct looks good now we're not going to change the question to try to uh sort of make the question nonsensical while keeping the same answer instead we're going to change the passage um by adding the sentence at the end which really shouldn't distract anyone this is quarterback well-known quarterback jeff dean you know had jersey number 37 in champ bowl so this just doesn't it's really not even related but now the prediction is jeff dean uh for our nice qa model um and so this shows as well that um it seems like maybe there's this like end of the passage bias as to what where the answer should be for example and so not that's this is an adversarial example where we flipped the prediction by adding something that is innocuous to humans and so sort of like the higher level takeaway is like oh it seems like the qa model that we had that seemed good it's not actually performing qa how we want it to even though it's in domain accuracy it was good um and uh here's another example so you've got this this paragraph with a question what has been the result of this publicity uh the answer is increased scrutiny on teacher misconduct now instead of changing the paragraph we're going to change the question in really really seemingly insignificant ways to change the model's prediction so first what h.a and now you've got this type o l been the result of this publicity the answer changes to teacher misconduct likely a human would sort of ignore this typo or something and answer the right answer and then this is really nuts instead of asking what has been the result of this publicity if you ask what's been the result of this publicity the answer also changes and this is the the authors call this a semantically equivalent adversary uh this is pretty rough and in general uh swapping what for what's in this qa model breaks it pretty frequently and so again when you go back and sort of re-tinker how to build your model you're going to be thinking about these things not just the sort of average accuracy um so that's sort of talking about noise our models robust to noise in their inputs our humans robust to noise there's another question we can ask and so you can kind of go to this popular sort of meme passed around the internet from time to time where you have all the letters in these words scrambled you say according to a research uh or at cambridge university it doesn't matter in what order the letters in a word are right and so it seems like you know i think i did a pretty good job there uh seemingly right we got this noise that's a specific kind of noise and we can be robust as humans to reading and processing the language without actually all that much of a difficulty um so that's maybe something that we might want our models to also be robust to and it's it's very practical as well noise is a part of all nlp systems inputs at all times there's just no such thing effectively as having you know users for example and not having any noise um and so there's a study that was performed on some you know popular machine translation models where you train machine translation models french german and czech i think all to english and you get blue scores these blue scores will look a lot better than the ones in your simon 4 because much much more training data the idea is these are actually pretty strong machine translation systems and that's in in domain clean text now if you add character swaps like the ones we saw in you know in that in that sentence about cambridge the blue scores take a pretty harsh dive not very good and even if you take somewhat a somewhat more natural sort of typo noise distribution here you'll see that you're still getting you know 20-ish yeah very high drops in blue score through simply natural noise and so maybe you'll go back and retrain the model on more types of noise and then you ask oh if i do that is it robust to even different kinds of noise these are the questions that are going to be really important and it's important to know that you're able to break your model really easily so you can then go and try to make it more robust okay um now let's see 20 minutes ah some uh now we're going to i guess yeah yeah so now we're going to look at the representations of our neural networks we've talked about sort of their behavior and then whether we could sort of change or observe reasons behind their behavior now we'll go into less abstraction look more at the actual vector representations that are being built by models and we can answer a different kind of question at the very least than with the other studies the first thing is related to the question that was asked about attention which is that um some modeling components lend themselves to inspection now this is a sentence that i chose somewhat carefully actually because in part of this debate right are they interpretable components we'll see but they lend themselves to inspection in the following way you can visualize them well and you can correlate them easily with various properties so let's say you have attention heads in bert this is from a really nice study that was done here where you look at intention heads of burt and you say you know on most sentences this attention head had one one seems to do this very sort of global aggregation simple kind of operation does this pretty consistently that's cool um is it interpretable well maybe right so it's the first layer which means that this word found is sort of uncontextualized and then um you know but in deeper layers the problem is that like once you do some rounds of attention you've had information mixing and flowing between words and how do you know exactly what information you're combining what you're attending to even it's a little hard to tell and saliency methods more directly sort of evaluate the importance of models but it's still interesting to see at sort of a local mechanistic point of view what kinds of things are being attended to so so let's take another example some attention heads seem to perform simple operations so you have the global aggregation here that we saw already others seem to attend pretty robustly to the next token cool next token is a great signal some heads attend to the sep token uh so here you have attending to sep and then maybe some attend to periods maybe that's sort of a you know splitting sentences together and things like that not things that are hard to do but things that some attention heads seem to pretty robustly perform um again now though deep in the network what's actually represented at this period at layer 11 a little unclear a little unclear okay so some heads though are correlated with really interesting linguistic properties so this head is actually attending to noun modifiers so you've got this the complicated language in the huge new law right that's pretty fascinating even if the model is not like doing this as a causal mechanism to do syntax necessarily the fact that these things so strongly correlate is actually pretty pretty cool and so we have in all of these studies is we've got sort of an approximate interpretation and quantitative analysis relating uh like allowing us to reason about very complicated model behavior they're all approximations but they're they're definitely interesting uh one other example is that of co-reference so we saw some work on co-ref reference and um it seems like this head does a pretty okay job of actually matching up co-referent entities these are in red talks negotiations she her and that's not obvious how to do that this is a difficult task and so it does so you know with some percentage of the time um and again it's sort of connecting very complex model behavior to uh to these sort of interpretable summaries of correlating uh properties other cases you can have individual hidden units that lend themselves to interpretation so here you've got a character level lstm language model each row here is a sentence if you can't read it it's totally okay the interpretation that you should take is that as we walk along the sentence this single unit is going from i think very negative to very positive or very positive to very negative i don't really remember but it's you know tracking the position in the line so it's just a linear position unit and uh pretty robustly doing so across all of these sentences so this is from a nice visualization study way back in 2016 way back here's another cell from that same lstm language model that seems to sort of turn on inside quotes so here's a quote and it turns on okay so i guess that's positive in the blue end quote here and then it's negative here you start with no quote negative in the red see a quote and then blue seems again very interpretable also potentially a very useful feature to keep in mind and this is just an individual unit in the lstm that you can just look at and see that it does this very very interesting even farther on this and this is actually a study by some ai and neuroscience researchers is okay we saw that lstms were good at subject verb number agreement um can we figure out the mechanisms by which the lstm is solving the task can we actually get some insight into that and so we have a word level language model the word level language model is going to be a little small but you have a sentence the boy gently and kindly greets the and this cell that's being tracked here so it's an individual hidden unit um one dimension right is actually after it sees boy it sort of starts to go higher and then it goes down to something very small once it sees greets and this cell seems to correlate with the scope of a subject verb number agreement instance effectively so here the boy that watches the dog that watches the cat greets you've got that cell again staying high maintaining the scope of subject until greets and at which point it stops what allows it to do that probably some complex other dynamics in the network but it's still a fascinating i think insight um and yeah this is just you know neuron 1150 in this lstm now so those are sort of all observational studies that you could do by picking out individual components of the model that you can sort of just take each one of and correlating them with some behavior now we'll look at a general class of methods called probing by which we still sort of use supervised knowledge like the knowledge of the type of co-reference that we're looking for but instead of seeing if it correlates with something that's immediately interpretable like a attention head we're going to look into the vector representations of the model and see if these properties can be read out by some simple function to say oh maybe this property was made very easily accessible by my neural network so let's dig into this so the general paradigm is that you've got language data uh that goes into some big pre-trained transformer with fine-tuning and you get state-of-the-art results uh soda means state-of-the-art right and so the question for the probing sort of methodology is like if it's providing these general purpose language representations you know what does it actually encode about language like can we can we quantify this can we figure out what kinds of things is learning about language that we seemingly now don't have to tell it and um so you might have something like a sentence like i record the record that's an interesting sentence and you put it into your your transformer model with its word embeddings at the be at the beginning maybe some layers of self-attention and stuff and you make some predictions and now our objects of study are going to be these intermediate layers right so it's a vector per word or sub word uh for every layer and the question is like can we use these linguistic properties like the dependency parsing that we had way back in the early part of the course um to understand uh sort of correlations between properties and the vectors and these things that we can interpret we can interpret dependency parses so so there are a couple of things that we might want to look for here you might want to look for semantics so here in the sentence i record the record uh i am an agent that's a semantics thing uh record is a patient it's the thing i'm recording okay you might have syntax so you might have the syntax tree that you're interested in that's the dependency parse tree maybe you're interested in part of speech right because you have record uh and record and uh the first one's a verb the second one is a noun they're identical strings does the model sort of encode that one is one and the other is the other um so how do we do this kind of study um so we're going to decide on a layer that we want to analyze and we're going to freeze burt the we're not going to fine-tune bird all the parameters are frozen so we decide on layer two of bert we're gonna pass it some sentences we decide on a on a what's called a probe family and the question i'm asking is can i use a model from my family say linear to decode a property that i'm interested in really well from this layer so it's indicating that this property is easily accessible to linear models effectively so maybe i get i train a model i train a linear classifier right on top of bert and i get a really high accuracy and that's sort of interesting already because you know from prior work and part of speech tagging that if you run a linear classifier on simpler features that aren't burnt you probably don't get as high in accuracy so that's an interesting sort of takeaway but then you can also take like a baseline so i want to compare two layers now so i've got layer one here i want to compare it to layer two i train a probe on it as well maybe the accuracy isn't as good and now i can say oh wow look by layer 2 part of speech is more easily accessible to linear functions than it was at layer 1. so what did that well the self-attention and feed forward stuff made it more easily accessible that's interesting because it's a statement about sort of the information processing of the model okay okay so that's we're going to analyze these layers let's take a second more to think about it and just really give me just a second so if you have the model's representations h1 to ht and you have a function family f that's the subset linear models or maybe you have like a feed forward neural network some fixed set of hyper parameters freeze the model train the probe so you get some predictions for part of speech tagging or whatever that's just the probe applied to the hidden state of the model the probe was a member of the probe family and then the extent that we can predict why is a measure of accessibility so that's just kind of written out not as pictorially okay so i'm not going to not going to stay on this for too much longer and you know it may help in the search for causal mechanisms but it sort of just gives us a rough understanding of sort of processing of the model and what things are accessible at what layer so what are some results here so one result is that bert if you run linear probes on it does really really well on things that require syntax and part of speech named entity recognition actually in some cases approximately as well as just doing the very best thing you could possibly do without without bert so it just makes easily accessible amazingly strong features for these properties and that's an interesting sort of emergent quality of burt you might say it seems like as well that the layers of bert have this property where so if you look at the columns of this of this plot here each column is a task you've got input words at the sort of layer zero albert here layer 24 is the last layer of bur at large lower performance is yellow higher performance is blue and io the resolution isn't perfect but consistently the best place to read out these properties is somewhere a bit past the middle of the model uh which is this is a very consistent rule which is fascinating um and then it seems as well like uh if you look at this function of increasingly abstract or increasingly difficult to compute linguistic properties on this axis and increasing depth in the network on that axis so the deeper you go in the network it seems like the more easily you can access more and more abstract linguistic properties suggesting that that accessibility is being constructed over time by the layers of processing of bert so it's building more and more abstract features which i think is again sort of really interesting result um and now i think yeah one thing that i think comes to mind uh that really brings us back right to day one is um we built intuitions around word to back we were asking like what does each dimension of word to vec mean and the answer was uh not really not really anything but we could build intuitions about it and think about properties of it through sort of these connections between simple mathematical properties of word to vect and linguistic properties that we could sort of understand so we had this approximation just not not 100 true but an approximation that says cosine similarity is effectively correlated with semantic similarity think about even if all we're going to do at the end of the day is fine tune these word embeddings anyway um likewise we had this sort of idea about the analogies being encoded by linear offsets so some relationships are linear in space and they didn't have to be that's fascinating it's this emergent property that we've now been able to study since we discovered this why is that the case in word to vec and in general even though you can't interpret the individual dimensions of of word to back these sort of emergent interpretable connections between approximate linguistic ideas and sort of simple math on these objects is fascinating and so one piece of work that sort of extends this idea um comes back to dependency parse trees so they describe the syntax of sentences um and in a paper uh that i did with uh with chris um [Music] we showed that actually bert and models like it uh make dependency parse tree structure emergent uh sort of more easily accessible than one might imagine in its vector space so if you've got a tree right here the chef who rented the store was out of food what you can sort of do is think about the tree in terms of distances between words so you've got the number of edges in the tree between two words is their path distance so you've got sort of that the distance between chef and was is one and we're going to use this interpretation of a tree as a distance to make a connection with bert's embedding space and what we were able to show is that under a single linear transformation the squared euclidean distance between bert vectors for the same sentence actually correlates well if you choose the b matrix right with the distances in the tree so here in this euclidean space that we've transformed the approximate distance between chef and was is also one likewise the difference between was and store is four in the tree and in my simple sort of transformation of bert space the distance between store and was is also approximately four and this is true across a wide range of sentences and this is like to me a fascinating example of again emergent approximate structure in these very non-linear models that don't necessarily need to encode things so simply okay all right great so um probing studies and correlation studies are i think interesting and point us in directions to build intuitions about models but they're not arguments that the model is actually using the thing that you're finding to make a decision not causal studies this is for probing and correlation studies so in some work that i did around the same time we showed actually that certain conditions on probes allow you to achieve high accuracy on a task that's effectively just fitting random labels and so there's a difficulty of an inter of interpret interpreting what the model could or could not be doing with this thing that is somehow easily accessible it's interesting that this property is easily accessible but the model might not be doing anything with it for example because it's totally random likewise another paper showed that you can achieve high accuracy with a probe even if the model is trained to know that thing that you're probing for is not useful um and there's causal studies that sort of try to extend this work it's much more difficult but read this paper and it's a fascinating line of future work now in my last you know two minutes um i want to talk about or casting model tweaks and ablations as analysis um so we had this improvement process where we had a network that was going to work okay and we would see whether we could tweak it in simple ways to improve it and then you could see whether you could remove anything and have it still be okay and that's kind of like analysis like i have my network do i want it to like is it going to be better if it's more complicated if it's going to be better if it's simpler can i get away with it being simpler and so one example of some folks who did this is they took this idea of multi-headed attention and said so many heads all the head's important and what they showed is that if you train a system with multi-headed attention and then just remove the heads at test time and not use them at all you can actually do pretty well on the original task not retraining at all without some of the attention heads showing that they weren't important you could just get rid of them after training and likewise you can do the same thing for this is our machine translation this is on multi-nli you can actually get away without a large large percentage of your attention heads uh let's see yeah so um another thing that you could think about is questioning sort of the the basics of the models that we're building so we have transformer models that are sort of self-attention feed forward self-attention feed forward but like why in that order with some of the things emitted here and the uh this paper asked this question and said if this is my transformer self-attention feed forward self-attention feed forward etc etc etc uh what if i just reordered it so that i had a bunch of self-attentions at the head and a bunch of feed forwards at the back and they tried a bunch of these orderings and this one actually does uh better so this achieves a lower perplexity on a benchmark and this is a way of analyzing what's important about the architectures that i'm building and how can they be changed in order to perform better so neural models are very complex and they're difficult to characterize and impossible to characterize with a single sort of statistic i think for your test set accuracy especially in domain and we want to find intuitive descriptions of model behaviors but we should look at multiple levels of abstraction and none of them are going to be complete when someone tells you that their neural network is interpretable i encourage you to engage critically with that it's not necessarily false but like the levels of interpretability and what you can interpret these are the questions that you should be asking because it's going to be opaque in some ways almost definitely um and then you know bring this sort of i this this this lens to your model building as you try to think about how to build better models even if you're not going to be doing analysis as sort of one of your main driving goals uh and with that you know good luck on your final projects i realize we're at time um the teaching staff is really appreciative of of your efforts um over this difficult quarter and uh yeah hope so um yeah i guess there's a lecture left on thursday but yeah this is my last one so thanks everyone 
","['', 'model analysis and explanation', 'course logistics', 'guest lecture reactions', 'final project', 'due date', 'deadline', 'xkcd comic', 'black box functions', 'understanding our models', 'model improvements', 'model biases', 'out-of-domain evaluations', 'incremental progress', 'language models', 'saliency maps', 'gradient method', 'input saliency methods', 'breaking models', 'question answering', '']"
"good afternoon folks uh welcome to lecture 18. today we'll be talking about some of the latest and greatest developments in neural nlp where we've come and where we're headed uh chris just to be sure uh are my present and what's visible from this part is it fine you're visible okay uh but none of my presenters right correct okay great thank you um so just as a reminder note that your guest lecture reactions are due tomorrow at 11 59 pm uh great job with the project milestone reports you should have received feedback now if not contact the co-staff i think uh you know we had some last minute issues uh but if that's not resolved please contact us um uh finally the project reports are due very soon uh on the march 16th which is next week there's one question on ed about the leaderboard and uh the last day to summon on the leaderboard is march 19th uh as well okay so for today we'll start by talking about extremely large language models and gpd3 that have recently gained a lot of popularity we'll then take a closer look at compositionality and generalization of these neural models um while transformer models like bird and gpt have really high performance on all benchmarks they still fail in really surprising ways when deployed how can we strengthen our understanding of evaluating these models so they more closely reflect task performance in the real world and then we end by talking about how we can move beyond this really limited paradigm of teaching models language only through text and look at language grappling finally i'll give some practical tips on how to move forward in your neural nlp research and this will include some practical tips for the final project as well okay so uh you know this this beam really kind of captures uh you know what's been going on in the field really and it's it's just that our ability to harness unlabeled data has vastly increased over the last few years and this has been made possible due to advances in not just hardware but also systems and our understanding of like self-supervised uh training so we can use like lots and lots of un-given data um so based on this here's a general representation learning recipe that just works for you know all basically most modalities so the the recipe is uh is basically as follows so convert your data if it's images converted or like it's not uh it's it's really modality agnostic so you take your data if it's images text or videos and you convert it into a sequence of integers and in step two we define a loss function to maximize data likelihood or create a denoising autoencoder loss finally in step three train on lots and lots of data um certain properties emerge only when we scale up model size and this is really the surprising fact about scale so to give some examples of this recipe in action here's gpd3 which can learn to do a really non-trivial classification problem with just two demonstrations and we'll talk more about this soon um another example as we saw in lecture 14 is d5 which does really effective close book qa by storing knowledge in parameters uh finally just so i covered another modality here's a recent text to image generation model with really impressive zero shot generalization okay so now let's talk about gpd3 so how big really are these models uh this table kind of presents some numbers to put things in perspective um so this so we have a collection of models starting with medium-sized lstms which was sort of a staple in pre-2016 nlp all the way to humans who have 100 trillion synapses and some in the middle we have gbt2 with over a billion parameters and gpt3 with over 150 billion parameters and this exceeds the number of synaptic connections in a honeybee brain so obviously anyone with little knowledge of neuroscience and knows that this is not an apples to oranges comparisons uh that this is an apple store in this comparison but the point here is that the scale of these models is really starting to reach astronomical numbers um so here are some facts about gbt3 uh for one it's a large transformer with 96 layers um it has more or less the same architecture as gpd2 with the exception that to scale up attention computation uh it uses these locally banded sparse attention patterns and i really encourage you to look at the paper to understand the details the reason we mentioned this here is because it kind of highlights that scaling up is simply not just changing hyper parameters as many might believe and it involves really non-trivial engineering and algorithms to make computations efficient finally all of this is trained on 500 billion tokens taken from the common crawl toronto books corpus wikipedia [Music] so what's new about gpp3 right so let's let's look at some of the results on the paper first so obviously it does better on language modeling and text completion problems as you can see from this table it does better than gpt2 at language modeling in the pentree bank as well as better on story completion on the story completion data set called ambada to give a flavor of what's to come uh let's take a closer look at this limbaugh story completion data set so the task here is that we're given a short story and we are supposed to fill in the last word um satisfying the constraints uh of the problem can be hard for a language model which could generate a multi-word completion with gpd3 the really new thing is that we can just give a few examples as prompts and sort of communicate a task specification to the model and now gpt3 knows how the completion must be a single word this is a very very powerful paradigm and we give some more examples of this in context learning in a couple more slides so apart from language modeling it's really good at these knowledge intensive tasks like uh close book qa as well as reading comprehension and here we observe that scaling our parameters results in a massive improvement in performance so now let's talk about in context learning uh gbt3 demonstrates some level of fast adaptation to completely new tasks this happens via what's called in context learning as shown in the figure the model training can be characterized as having an outer loop that learns a set of parameters that makes the learning of the inner loop as efficient as possible and with this sort of framework in mind we can really see how a good language model can also serve as a good few short learner so in this segment we will have some fun with gpd3 and look at some demonstrations of this in context learning um [Music] so uh to start up here is an example where someone's trying to create an application that converts language a language description uh to bash one language the first three examples are prompts followed by generated examples from gpd3 uh so it gets a list of running processes right this one's easy probably just involves looking at a hash table some of the more challenging ones that involve copying over um you know some uh spans from the text like the scp example is kind of interesting as well as the harder one to parse grip the scp example comes up a lot uh during office hours so gpd3 knows how to do that here's a somewhat more challenging one where the model is given a description of a database in natural language and it starts to emulate that behavior so the text in bold is sort of the prompt given to the model the prompt includes somewhat of a functional function specification of what a database is so it says that the database begins knowing nothing the database knows everything that's added to it the database does not know anything else and when you ask a question to the database if the answer is there in the database the database must return the answer otherwise it should say it does not know the answer so this is very new and very powerful um and you know the prompt also includes some example usages so when you ask two plus two the database does not know you ask the capital of france the database does not know and then you add in a fact that tom is 20 years old to the database and now you can start asking it questions like where does tom live and as expected it says that the database does not know but now if you ask it what's tom's age uh the database says that tom is 20 years old and if you ask what's my age the database says basically that it does not know because that's not been added so this is really powerful um here's another one uh now uh in this example the model is asked to blend concepts together and so there's a definition of what does it mean to blend concepts so if you take airplane and car you can blend that to your flying car that's essentially you know there's a wikipedia definition of what concept blending it concept blending is along with some examples and now let's look at uh you know some some some problems followed by what gp3 answers so the first one is straightforward two-dimensional space uh blended with 3d space gives 2.5 dimensional space the one that is somewhat interesting is old and new gives recycled um then triangular square gives trapezoid that's also interesting the one that's like really non-trivial is a geology plus neurology used to sediment neurology and i had no idea what this was it's apparently correct um so clearly it it's able to do these very flexible things just from a just from prompt so here's another you know class of examples that gbt3 uh you know gets somewhat right and these are uh these copycat analogy problems which have been really well studied in cognitive science science and the way it works is that i'm going to give you some examples and then ask you to uh you know introduce a function from these examples and apply it to you apply to like new queries so if abc changes to abt what does pqr change to well pqr must change to pqs because the function we've learned is that the last letter must be incremented by one and and this function uh humans can now apply to examples of like you know varying types so like uh p repeated twice q repeated twice r repeated twice much change to be repeated twice q repeated twice and s repeated twice um and it seems like gpd3 is able to get them right uh more or less but uh the problem is that if you if you ask it to generalize to uh you know examples that have increasing number of repetitions then were seen in the prompt it's not able to do that so in this situation uh you ask it to you know make an analogy where um the the the letters are repeated four times and it's never seen that before it doesn't know what to do and so it gets all of these wrong so you know there's a point to be made here about uh just like maybe these prompts are not enough to convey uh you know the function the model should be learning and maybe even more examples that you can learn but the point is that it probably doesn't um it probably it probably does not have the same kind of generalization that humans have and that brings us to sort of the limitations of these modules and some some open questions so just looking at the paper and uh you know passing through the results it seems like the model is bad at logical logical and mathematical reasoning anything that involves doing multiple steps uh of reasoning and that explains why it's bad at arithmetic why it's bad at work problems why it's not great at analogy making and even like traditional textual entailment data sets that seem to require logical reasoning like rte so second most subtle point is that it's unclear how we can uh make permanent updates to the model like maybe if i want to teach a model a new concept that's possible to do it while i'm interacting with the system but once the interaction is over it kind of restarts and does not have a notion of knowledge and it's not that this is something that the model cannot do in principle but just something that's not really been explored [Music] um it doesn't seem to exhibit human-like generalization which is often called systematicity and i'll talk a lot more about that and finally language is situated and gpt3 is just learning from text and there's no exposure to other modalities there's no interaction so maybe the aspects of meaning that it requires are like somewhat limited and maybe we should explore how we can bring in other modalities so we'll talk a lot more about uh these last uh a lot last few limitations the rest of the lecture but maybe i can possibly some questions now if there are [Music] any i don't think there's a big outstanding question but i mean i think some people aren't really clear on you know few shot setting and prompting versus learning and i think it might actually be good to explain that a bit more okay yeah so um so maybe let's let me pick a simple example um let me pick this example here so uh prompting just means that so gpd3 like if you go back to first principles right gbt3 is basically just a language model and what that means is uh given a context it'll tell you what's the probability of of the next word right so if i give it a context uh w1 through wk uh gpd3 will tell me what's the probability of w uh k plus one for you opens the vocabulary so that's that's what a language model is uh a prompt is essentially a context that gets pre-bended before gt3 can start uh generating and what's happening with in context learning is that the uh the context that you append uh that that you that you pre-pen to gp3 are basically xy examples um so that's that's the prompt and the reason why it's also uh it's equivalent to few short learning is because you pre-bend a small number of xy examples so in this case if i just prepend this uh this one example that's highlighted in purple then that's essentially one shot learning because i just give it a single example as context and now like given uh you know given this query which is also appended due to the model it has to make a prediction so um so the input output format is the same as how a few shot learner uh would receive but since it's a language model the training data set is essentially presented as a context so someone is still asking can you be more specific about the in-context learning setups what is the task right so um so let's see maybe i can go to um yeah so maybe i can go to this slide so the task is just that i'm it's a language model so it gets a context which is just a sequence of tokens and the task is just to you know uh uh so you have a sequence of tokens and then the model has to generate given a sequence of tokens and the way you can convert that into an actual machine learning classification problem is that uh so for this example maybe you give it 5 plus 8 equals 13 7 plus 2 equals 9 and then one plus zero equals and now gpd3 can fill in uh you know a number there so that's how you convert it into a classification problem the context here would be these two examples of uh of arithmetic like five plus eight equals thirteen and seven plus two equals nine and then the query is one plus zero equals and then the model since it's just a language model has to fill in one plus zero equals question mark so it fills in something that doesn't have to fill in numbers it could fill in anything and but if it fills in a one uh you know it does the right job so that's how you can take like a language model and do few shot learning with it i'll keep on these questions how is in context learning different from transfer learning so i i guess the like in in context learning i mean you can think of in context learning as being a kind of transfer learning but like transfer learning does not specify the mechanism through which the transfer is going to happen within context learning the mechanism is that the training examples are sort of appended to the model which is a language model just uh you know in order so let's say you have x y x one y one x two y two and these are just appended directly to the model and now it makes prediction on you know some query uh some some queries that are drawn from this data set so yes it is uh it is a sub-category of transfer learning but transfer learning does not specify um exactly how this transfer learning is achieved but in context learning is very specific and says that for language models you can essentially concatenate the training data set and then present that to the language model people still aren't sufficiently clear on what is or isn't happening with learning and prompting so you know another question is so in context learning still needs fine tuning question mark we need to train gpt 3 to do in context learning question mark right so um so there are two parts to this question right so uh so the answer is yes and no so of course the the model is a language model so it needs to be trained so you start with some random parameters and you need to train them but the model is trained as a language model right and once the model is trained you can now use it uh to do transfer learning and the model parameters in in context learning are fixed you do not update the model parameters all you do is that you give it these uh you know small training set to the model which is just appended to the model as context and now the model can start generating from that point on so in this example if 5 minus 8 equals 13 and 7 plus 2 equals 9 are two xy examples in in vanilla transfer learning what you would do is that you would take some great in steps update your model parameters and then make a prediction on one plus zero equals what right but within context learning all you're doing is you just concatenate uh 5 plus 8 equals 13 and 7 plus 2 equals 9 to the model's context window and then make it uh predict what one plus 0 should be equal to maybe we should end for now with one other bigger picture question which is do you know of any research combining these models with reinforcement learning for the more complicated reasoning tasks so that is an excellent question uh there is some recent work on kind of trying to align um language models with human preferences where yes there is like uh you know some amount of fine tuning with reinforcement learning based on like these preferences from humans so maybe you want to do a summer you want to do a summarization problem with gbd3 and the model produces multiple summaries and for each summary maybe you have a reward that is essentially a human preference like maybe i want to include some facts and i don't want to include you know some other uh non-important facts and so i can construct a reward out of that and i can fine-tune the parameters of my language model uh basically using uh reinforcement learning based on this reward which is essentially human preferences uh so there's some very recent work that tries to do this but i'm not sure uh yeah i'm not aware of any work that tries to use reinforcement learning to teach her reasoning i did these models but i think it's a interesting future direction to explore maybe you should go on at this point okay okay so we'll talk a bit more about these last two points uh so systematicity and language grounding um so just to start off like how do you define systematicity so really the definition is that there is a definite and predictable pattern among the sentences that native speakers of a language understand and so there's a systematic pattern among the sentences that we understand what that means is let's say there's a sentence like john loves me right and if a native speaker understands the sentence then they should also be able to understand the sentence mary loves john and closely related to this idea of systematicity is the principle of compositionality and for now i'm going to uh you know ignore the definition by montague and just look at the rough definition and then we can come back to this other like more concrete definition the rough definition is essentially that the meaning of an expression is a function of the meaning of its parts so that brings us to the question are human languages really compositional and here are some examples that you know make us think that maybe uh yes so like if you look at what is the meaning of the noun phrase brown cow so it is composed of the meaning of the adjective brown and and the noun cow um so all things that are brown and all things are a cow take the intersection and get brown cows similarly red rabbits so all things that are red all the things are rather combining them get red and then kick the ball this work phrase can be understood as you have some agent that's you know performing a like kicking operation on the ball uh but this this is not always the case that uh you can like get the meaning of the whole by combining meanings of parts so here we have some counter examples that people often use so like a red herring does not mean all things that are red and all things that are heading and kick the bucket definitely does not mean that there's an agent that's kicking the bucket so uh while these examples like are supposed to be provocative like we think that language is like mostly compositional there's lots of exceptions but for vast majority of sentences that we've never heard before we're able to understand what they mean by piecing together the words that the sentence is composed of and so what that means is that maybe compositionality of representations are helpful prior that could lead to systematicity in behavior um and that brings us to the questions that we ask in the segment are neural representations compositional and the second question is if so do they generalize systematically um so how do you even measure if representations that on your network learns exhibit compositionality um so let's uh let's go back to this definition from logic u which says that compositionality is about the existence of a homomorphism from syntax to something um and to look at that we have this example which is lisa does not skateboard and we have a syntax tree uh corresponding to this example and the meaning of the sentence can be composed in uh according to according to the structure that's decided by the syntax so meaning of lisa does not skateboard it's a function of the meaning of lisa and does not skateboard the meaning of does not skate but is a function of does and not skateboard meaning of not skateboard is a function of north and skateboard so that's good um and so this gives us one way of formalizing how we can measure compositionality in neural representations and so compositionality of representations could be thought of as how well the representation approximates an explicitly homomorphic homomorphic function and learned in a large representation space so what we are going to do is essentially measure if we were to construct a neural network that whose computations are based exactly according to these parse trees how far are the representations of our learnt model from this explicitly compositional uh representation and that gave us some understanding of how compositional the neural networks representations really are uh so to unpack that a little bit uh instead of having um yeah so so instead of having uh denotations we have uh representations uh uh in the node uh and to like kind of be more concrete about that uh we first start by choosing a distance function that tells us how far away two representations are and then we also need a way to compose together two constituents to give us uh sort of the meaning of of the whole and but once we have that we can start by uh we can create like an explicitly compositional function right so what we do is uh we have these uh we have these uh representations at the leaves that are initialized randomly and the composition function that's also initialized randomly and then a forward pass according to this syntax is used to compute the representation of lisa does not skateboard and now once you have this representation you can create a loss function and this loss function measures how far are the representations of my neural network from this second sort of proxy neural network that i've created and then i can uh basically optimize both the composition function and the embeddings of the leaves and then once the optimization is finished i can measure how far was the representation from of my neural net from this explicitly compositional network on a held outside and that then tells me whether the representation of my neural net learnt were actually compositional or not so uh to see how well this works let's look at a plot and um this is relatively uh complex but uh just to unpack this a little bit uh it it it plots uh the mutual information between uh the input that uh the neural network receives versus the representation against this tree reconstruction error that that we were talking about and to give some more background about what's to come uh there is a theory of the which is called the information bottleneck theory which says that uh as a neural network trains uh it first tries to maximize the mutual information between the representation and the input in an attempt to memorize the entire data set and that is called uh that is our memorization phase and then once memorization is done there is a learning or a compression phase where this mutual information starts to decrease and the model is essentially trying to compress the data or consolidate the knowledge in the data into its parameters and what we are seeing here is that as a model learns which is characterized by decreasing major information we see that the representations themselves are becoming more and more compositional and overall we observe that learning is correlated with increased compositionality as measured by this tree reconstruction error so that's really encouraging so uh now that we have a method of measuring compositionality uh of representations in these neural nets uh how do we you know start to create benchmarks now you know that let's see if they are generalizing systematically or not so to do that uh here's a method for taking any data set and splitting it into a trained test split uh that explicitly tests for this kind of generalization so to do that we use this principle called maximizing the compound divergence and to illustrate how this principle works uh we look at this toy example so in this toy example we have a training data set that consists of just two examples and test data set of just two examples um the atoms that are defined as the primitive elements uh so entity words predicates question types so you know in this toy example goldfinger christopher nolan these are all so the primitive elements and the compounds are compositions of these primitive elements so who directed entity would be the composition of the question type did x predicate y and and the predicate direct so here's a basic machinery for producing compositionally challenging splits so uh let's start by introducing two distributions the first distribution is the normalized frequency distribution of the atoms so given any data set if we know what the notion of atoms are we can basically compute the frequency of all of the atoms and then normalize that by the total count and that's going to give us um one one distribution and we can repeat the same thing for the compounds and that'll give us a second uh frequency distribution so uh note that these are just two probability distributions and once we have these two distributions we can essentially define the atom and compound divergence simply as uh this quantity here and where there is the journal coefficient between two categorical distributions the churn of coefficient basis basically measures uh how far two categorical distributions are so just to get a bit more intuition about this uh if we set p to q then the turn off coefficient is one which means these these representations are like maximally similar and then if p is non-zero everywhere q is zero um or or if or if p is zero in all the places where q is zero then the channel coefficient is exactly uh is exactly zero which means that these two distributions are maximally far away and uh the overall goal by uh describing uh this objective is that uh this loss objective is just that we are going to maximize the compound divergence and minimize the atom divergence and so what is the intuition behind doing such a thing so what we want is to ensure that the unigram distribution in some sense is constant between the train and test split so that it's uh so that the model does not encounter any new words but we want the compound divergence to be very high which means that these same words that the model has seen many times must appear in new combinations which means that we are testing for systematicity and so if you do uh if you follow this procedure for a semantic passing data set let's say what we see uh is that as you increase the scale we see that the smaller just does better and better at a compositional generalization but uh just pulling out a quote from this this paper pre-training helps for compositional generalization but doesn't fully solve it and what that means is that maybe as you keep scaling up these models you'll see better and better performance or maybe it starts to saturate at some point in any case we should probably be thinking more about this problem instead of just trying to brute force it so now uh this segment kind of tells us that the way we split a data set you know we can measure for like different kinds of um we can measure like different behaviors of the model and that tells us that maybe we should be like thinking more critically about how we're evaluating models in nlp in general so you know there has been a revolution basically over the last few years in the field where we're seeing all of these large transform models be all of our benchmarks at the same time there's uh you know still not complete confidence that once we deploy these systems in the real world they're going to you know be like they're going to maintain their performance and so it's unclear if these gains are coming from spurious correlations or some real task understanding and so how do we design benchmarks that accurately tell us how well this model is going to do in the real world and so i'm going to give one example of works that try to do this and that's the idea of dynamic benchmarks and what dynamic what the the idea of dynamic benchmarks is basically saying that instead of testing our models on static uh on static test sets we should be evaluating them on an ever-changing dynamic benchmark and there's many recent examples of this and and the idea dates back to a 2017 workshop at emlp and so the overall schematic looks something like this that we start with a training data set and a test data set which is the static uh static order we train a model on that and then once the model is trained uh we deploy that and then have humans create new examples that the model fails to classify and uh crucially we're looking for examples the model does not get tried but humans have no issue figuring out the answer to so by playing this game of whack-a-mole where you know we humans figure out what are sort of the holes in the model's understanding and then add that back into the training data retrain the model deploy it again have humans create new examples we can essentially construct this never-ending uh you know data set this never-ending test set um which can hopefully be a better proxy of estimating real-world performance um so so this is some really cutting-edge research and one of the main challenges of you know this class of works is that it's unclear how much this can scale up because uh maybe after certain after multiple iterations of this whack-a-mole uh humans are just fundamentally limited by creativity so figuring figuring out how to uh you know deal with that is is really an open problem and kind of approaches just use examples from other datasets to you know prompt humans to think more creatively but maybe we can come up with like better like more automated methods of doing this so uh this brings us to sort of the final segment or actually let me stop for questions at this point and see if people have questions here's a question with dynamic benchmark doesn't this mean that the model creator will also need to continually test slash evaluate the models on the new benchmarks new data test new data states uh wait a second sorry um yeah so with with dynamic benchmarks yes it's absolutely true that uh you will have to continuously keep training your model and that's just to ensure that um you know the the reason your model is not doing well on the test set doesn't have to do with like this domain mismatch um and what we're really trying to do is like you know measure how like just come up with a better estimate of the model's performance for the overall task and just trying to get like more and more data so yes to answer to answer your question yes we need to keep like training the model again and again but this can be automated okay so uh i'll move on to sort of uh language grounding so uh in this final step segment i'll talk about how we can move beyond just training models uh on text alone um so many have articulated the need to use modalities other than their ex if we someday want to get at real language understanding and uh this has you know ever since we've had like these big language models you know this there has been sort of a rekindling of this debate and recently there was uh multiple papers on this and so at acl last year there was this paper that argues uh through multiple thought experiments that it's actually impossible to acquire meaning from form alone where meaning refers to the communicative intent of a speaker and form refers to text or speech signals um a more modded version of this was put forward by the second paper where they say that training on only web scale data kind of limits the world scope of models and kind of limits uh the aspects of meanings that the model can actually acquire um and so here's sort of a diagram that i've borrowed from the paper and what they say is uh the era where we were training modernism like supervised data sets uh models were limited in words called one and now that we've moved on to exploiting like unlabeled data we're now in world scope 2 where models just have strictly more signal to get more aspects of the minimum if you mix an additional modalities into this so maybe you make some videos and maybe you make some images then that expands out the world scope of the model further and now maybe it can acquire more aspects of meaning such that now it knows that the lexical item in red refers to you know red images maybe and then if you go beyond that you can have a model that is embodied and it's actually living in an environment where it can interact uh with its data conduct um interventions and experiments and then if you go out uh go even beyond that you can have models that live in a social world where they can interact with other models because after all the purpose of language is to communicate and so she can have like a social world where models can communicate with other models that kind of expands out uh aspects of meaning [Music] and so gpd3 is in world scope queue so there are a lot of open questions in this space so given that there are all of these good arguments about how we need to move beyond text what is the best way to do this at scale um we know that you know babies cannot learn language from watching tv alone for example so there has to be some interventions and there has to be interactions that need to happen but at the same time the question is how far can models go by just training on static data as long as we have additional modalities especially when we combine this with scale and if interactions with the environment are really necessary how do we collect data and design systems that interact minimally or in a cost effective way and then finally put pre-training on text still be useful if any of these other uh if any of these other like um any of these other research directions uh become more sample efficient so if you're interested in learning more about this topic i highly encourage you to take cs224u which is offered in spring they have like multiple lectures on just language okay so in this final segment i'm gonna talk a little bit more about how you can get involved with uh you know nlp and deep learning research and how uh you know how you can make more progress so uh here are some general principles for how to make progress in english and research so i think the most important thing is to just kind of read broadly which means not just read the latest and greatest papers in archive but also read like pre-2010 statistical nlp um learn about the mathematical foundations of machine learning to understand how generalization works so take cs 29 m uh learn more about language which means taking uh classes in the linguistics department in particular i would recommend universe 138 and also take cs224u and finally if you wanted uh if you want to take inspiration from how babies learn then definitely read about child language acquisition literature it's fascinating uh finally learn how to learn your software tools which involves scripting tools uh version control data wrangling uh learning how to visualize quickly with jupiter notebooks and deep learning often involves um you know running multiple experiments with different hyper parameters and different ideas all in parallel and sometimes it can get really hard to keep track of everything so learn how to use experiment management tools like weights and biases and uh finally i'll talk about some really quick final project tips um so firstly let's just start by saying that if your approach doesn't seem to be working please do not panic uh put assert statements everywhere and check if the computations that you're doing are correct use breakpoints extensively and i'll talk a bit more about this uh check if the loss function that you've implemented is correct and one way of debugging that is to see that uh the initial values are correct so if you're doing a kva classification problem then the initial loss should be the natural log of k always always always start by creating a small training data set which has like five to ten examples and see if your model can completely overfit that if not there's a problem with your training loop um check for saturating activations and dead values and often this can be fixed by you know like maybe there's some problems to gradient so maybe there's some problem with the initialization which brings me to the next point check your gradient values see if they're too small which means that maybe you should be using residual connections or lstms or if they're too large then you should use gradient clipping in fact always use gradient clipping um overall be methodical if your approach doesn't work come up with hypotheses or for why this might be the case design oracle experiments to debug it look at your data and look at the errors that it's making and just try to be systematic about everything so um i'll just say a little bit more about uh breakpoints uh so there's this great library called pdb it's like gdp but it's for python so that's why pdb um to create to create a breakpoint just add the line import pdb pdb set trace before the line you want to inspect so earlier today i was trying to play around with uh with the transformers library so uh and i was trying to do question answering so i have a really small training corpus and the context is one morning i shot an elephant in my pajamas how he got into my pajamas i don't know and the question is what did i shoot and to do to solve this problem i basically imported a tokenizer and a birth model um and i you know initialize my tokenizer initialize my model like tokenize my input i set my model into the eval mode and i try to look at the artwork but i get this error and i'm very sad it's not clear what's causing this error and so the best way to look at what's causing this error is to actually put a breakpoint um so right after modular eval i put a breakpoint because i know that that's where the problem is so the problem is in 21 so i put a breakpoint at line 21 and now once i put this breakpoint i can just run my script again and it stops before executing line 21 and at this point i can examine all of my variables so i can look at the token as input because maybe that's where the problem is and lo and behold i see that it's actually a list so it's a dictionary of lists whereas modules typically expect a dodge tensor so now i know what the problem is and that means i can quickly go ahead and fix it and everything just works uh so this just shows that you should use breakpoints everywhere uh if your code is not working and it can just like help you debug really quickly um okay so uh finally i'll say that if you want to get involved with nlp and deep learning research and if you really like the final project uh we have the clips program at stanford and this is a way for undergrads master students and phds who are interested in doing nlp research and want to get involved with the nlp group um so we highly encourage you to apply to clips um and so yeah so i'll uh conclude uh continue today's class spicing that you know we've made a lot of progress uh in the last decade and that's mostly due to you know clever understanding of neural networks data hardware all of that combined with scale we have some really amazing technologies that can do really exciting things and we saw some examples of that today um in the short term uh i expect that we'll see more scaling uh because it just seems to help so perhaps even larger models uh but this is not trivial so you know i i said that before and i'll just say it again scaling requires really non-trivial engineering efforts and sometimes even you know clever algorithms and so we there's a lot of interesting systems work to be done here but in the long term uh we really need to be thinking more about these bigger problems of like systematicity generalization how can we make our models you know learn a new concept really quickly so that's fast adaptation uh and then we also need to you know create benchmarks that we can actually trust so if my model has some performance on some sentiment analysis data set and deployed in the real world that should be reflected in the number that i get from the benchmark so we need to make progress uh in in the way we evaluate models and then also figuring out a way to move beyond text in a more tractable way this is also really essential so yeah that's that's it good luck with your final projects i can take more questions at this point so i answered a question earlier that actually i think you uh could also find on um it was the question of whether you have a large model that's pre-trained on language if it will actually help you in other domains like you apply it to vision stuff uh yeah yeah so i guess uh the answer is actually yes like there was a paper that came out really really recently like just two days ago that just takes uh i think it was gpt too i'm not sure it's like one large transformer model that's featuring on text and like other dialogues definitely apply to images and i think they apply to like uh math problems and some more modalities and show that it's actually really effective at like transfer so if you pre train on text and then you move to a different modality that helps i think part of the reason for that is just that you know across modalities there is a lot of auto aggressive structure that is shared um and i i think one reason for that is that uh language is really referring to the world around it and so you might expect that uh there is you know some there is like some correspondence that's just beyond the autoregressive structure so there's also works that show that uh if you have just text only representations and image only representations you can actually learn a simple linear classifier that can learn to align both of these representations and all of these works are just showing that there's actually a lot more common between mortalities than we thought in the beginning uh so yeah i think yeah it's it's possible to create a text and then fine-tune on your modality of interest and it should probably be effective of course based on what the modality is but yeah for like images and videos it's certainly certainly effective more questions well a couple of questions have turned up one is what's the difference between cs 224 you and this class in terms of the topics covered and focus do you want to answer that one shakar or should i have a go at answering it maybe you should answer this one okay so next quarter um cs224u natural language understanding is co-taught um by chris potts um and bill mccartney um so you know in essence um it's meant to be different that natural language understanding focuses on what its name is um sort of how to build computer systems that understand the sentences of natural language now you know in truth the boundary is kind of complex because um we do some natural language understanding in this class as well and certainly for the people who are doing the default final project um question answering well that's absolutely a natural language understanding task but the distinction is meant to be that you know at least a lot of what we do in this class things like you know the assignment 3 dependency parser or building the machine translation system in assignment 4 that they're in some sense natural language processing tasks where you know processing can mean anything but commonly means you're doing useful useful intelligent stuff with um human language input but you're not necessarily deeply understanding it so there is some overlap in the classes um if you do cs224u you'll certainly see word vectors and transformers again but the emphasis is on doing a lot more with natural language understanding tasks and so that includes things like building semantic parsers so they're the kind of devices that um will you know respond to questions and commands such as an alexa or google assistant will do building relation extraction systems which get out particular facts out of a piece of text of all this person took on this position at this company looking at grounded language learning and grounded language understanding where you're not only using the language but the world context to get information and other tasks that sort i mean i guess you can look at the website to get more details of it i mean you know relevant to this class i mean a lot of people also find it an opportunity to just get further in doing a project in the area of natural language processing that sort of by the nature of the structure of the class since you know it more assumes that people know how to build deep learning natural language systems at the beginning that rather than a large percentage of the class going into okay you have to do all of these assignments although there are little assignments earlier on that there's sort of more time to work on a project for the quarter okay here's one more question that maybe chicago could do do you know of attempts to crowd source dynamic benchmarks eg uses uploading adversarial examples for evaluation or online learning yeah so actually like uh the main idea there is to use crowdsourcing right so in fact there is this bench uh so there is this um platform that was created by bear it's called dyno bench and the objective is just that that to construct this slight dynamically evolving uh benchmark we are just going to offload it to you know users of this platform and you can you know it essentially gives you utilities for like uh deploying your model and then having uh you know humans kind of try to fool the model um yeah so so this is like it's it's basically how the dynamic evaluate the dynamic benchmark uh collection actually works so like you uh deploy a model um on some platform and then you get humans to like fool the system yeah is a question can you address the problems of nlp models not able to remember really long contexts and techniques to infer on really large input length yeah so so i guess like there have been like a few works recently right that kind of tried to scale up transformers to like really large uh context lens uh one of them is like the reformer um and there's also like the transformer excel that was i think the first one to try and do that um i think what is unclear is whether you can combine that with the scale of these gpt-like models and if you see like qualitatively different things once you do that like um and part of it is just that all of this is just like so recent right uh but yeah i think the open question there is that you know can you take these like really long context transformers that can operate over long context combine that with scale of gpd3 and then get models that can actually reason over these like really large contexts um because i guess the hypothesis of scale is that once you train language models on uh at scale it can start to do these things and so to do that for long context we actually need to like have long context transformers that are trained at scale and i i don't think people have done that yet so i'm seeing this other question about language acquisition [Music] because do you have some thoughts on this or maybe i can just do something with that um yeah so the question is um what do you think we can learn from baby language acquisition can we build a language model in a more interactive way like reinforcement learning do you know any of these attempts oh that's that's a big huge question and you know i think the the short non-helpful answer is that there are kind of no answers at the moment you know people have certainly tried to do things at various scales but you know we just have no technology that is the least bit convincing um for being able to replicate the language learning ability of a human child um but after that prologue what i could say is i mean yeah there are definitely ideas to have in your head so you know there are sort of clear results which is that little kids don't learn by watching videos so it seems like interaction is completely key um little kids don't learn from language alone they're in a very rich environment where people are sort of both learning stuff from the environment in general and in particular you know they're learning a lot from what language acquisition risk um researchers refer to as attention which is different what we mean by attention but it means that the caregiver will be looking at the object that's the focus of interest and you know commonly other things as well like sort of you know picking it up and bringing it near the kid and all those kinds of things um and you know babies and young kids get to experiment a lot right so regardless of whether it's learning what happens when you have um some blocks that you stack up and play with them or you're learning language you sort of experiment by trying some things and see what kind of response you get and again that's essentially building on the interactivity of it that you're getting some kind of response to any upfronts you make and you know this is something that's sort of been hotly debated in the language acquisition literature so a traditional chomp skin position is that you know human beings don't get effective feedback you know supervised labels when they talk and you know in some very narrow sense well that's true right it's just not the case that after a baby tries to say something that they get feedback of you know syntax error in english on word four or they get given here's the semantic form i took away from your utterance but in a more indirect way they clearly get enormous feedback they can see what kind of response um they get from their caregiver at every um corner and so like in your question um you were suggesting that well somehow we should be making use of reinforcement learning because we have something like a reward signal there um and you know in a big picture way i'd say hi yeah i agree um in terms of a much more specific way as to well how can we possibly get that to work to learn something with the richness of human language i you know i think we don't have much idea but you know there has started to be some work so people have been sort of building um virtual environments which you know you have your um avatar in and that can manipulate in the virtual environment and there's linguistic input and it can succeed in getting rewards for sort of doing a command where the command can be something like you know pick up the orange block or something like that and you know to a small extent people have been able to build things that work i mean as i as you might be picking up i mean i guess so far at least i've just been kind of underwhelmed because it seems like the complexity of what people have achieved um is sort of you know just so primitive compared to the full complex complexity of language right you know the kind of languages that people have been able to get systems to learn are ones that can yeah do pick up commands where they can learn you know blue cube versus um orange sphere and that's sort of about how far people have gotten and that sort of such a teeny small corner of what's involved in learning a human language one thing i'll just add to that is i i think there are some principles of uh how kids learn that people have tried to apply to deep learning and one example that comes to mind is curriculum learning um where there's like a lot of literature that shows that you know babies uh they tend to pay attention to things that they just that is just slightly challenging for them and they don't pay attention to things that are extremely challenging and also don't pay attention to things that they know how to solve and many researchers have really tried to get curriculum learning to work um and the verdict on that is that it seems to kind of work when you're in like reinforcement learning settings but it's unclear if it's going to work on like supervised learning settings but i still think that it's like underexplored and maybe you know there should there should be like more attempts to kind of see if we can like add in curriculum learning and if that improves anything yeah i agree curriculum learning is an important idea which we haven't really talked about but it seems like it's certainly essential to human learning um and there's been some minor successes with it in the machine learning world but it sort of seems like it's an idea you should be able to do a lot more with in the future as you move from um models that are just doing one narrow task that's trying to do a more general language acquisition process should i attempt this next question as well okay the next question is is the reason humans learn languages better just because we are pre-trained over millions of years of physics simulation maybe we should um pre-train a model the same way so i mean i presume what you're saying is physics simulation um you're evoking evolution when you're talking about millions of years so you know this is a controversial debated big question um so you know again if i invoke chomsky again so noam chomsky is sort of the most famous um linguist in the world um and you know essentially noam chomsky's career starting in the 1950s is built around the idea that little children get such um dubious linguistic input because you know they hear a random bunch of stuff they don't get much feedback on what they say etc that language could not be learned empirically just from the data observed and the only possible assumption to work from is significant parts of human language um uh innate or in the sort of human genome babies are born with that and that explains the miracle by which very little humans um learn amazingly fast how human languages work um now to speak in credit for that idea for those of you who have not been around um little children i mean i i think one does just have to acknowledge you know human language acquisition by live little kids i mean it does just seem to be miraculous right but you go through this sort of slow phase for a couple of years where you know the the kids sort of goose and gars some syllables and then there's a fairly long period where they picked up a few words and they can say juice juice um when they want to drink some juice and nothing else and then it just sort of seems like there's this phase change where the kids suddenly realize wait this is a productive generative sentence system i can say whole sentences and then in an incredibly short period they sort of seem to transition from saying one and two word utterances to suddenly they can say you know daddy come home in garage um pudding bike in garage and you go wow how do they suddenly discover language um so you know so it is kind of amazing but um personally for me at least you know i've just never believed the strong versions of the hypothesis that human beings have much in the way of language specific knowledge or structure in their brains that comes from genetic inheritance like clearly humans do have these very clever brains and if we're at the level of saying being able to think or being able to interpret the visual world that's things that have developed over tens of millions of years and um evolution can be a large part of the explanation and humans are clearly born with lots of vision specific hardware in their brains as are a lot of other creatures but when you come to language you know no one no one knows when language was in a sort of a modern like form first became available because you know there aren't any fossils of people saying you know the word um spear or something like that but you know to the extent that there are estimates based on sort so what you can see of the sort of spread of um proto-humans and their sort of apparent social structures from so what you can find in fossils you know most people guess that language is at most a million years old and you know that's just too short a time for any significant eve for evolution to sort of build any significant structure inside human brains that's specific to language so i kind of think that the working assumption has to be that sort of there's just about nothing specific to language and human brains and you know the most plausible hypothesis not that i know very much about neuroscience when it comes down to it is that humans were being able to repurpose hardware that was originally built for other purposes like visual scene interpretation and memory and that that gave a basis of sort of having all this clever hardware that you could then use for language so you know it's kind of like gpus were invented for playing computer games and we were able to repurpose that hardware to do deep learning we've got a lot of have uh come out at the end okay so this one is answered live um let's see yeah if you could name i guess this is for either of you one main bottleneck as to um uh if we could provide feedback efficiently to our systems like babies are given feedback what's the bottleneck that remains in uh trying to have more human-like language acquisition um i mean i sort of i cannot find on this again or would you start releasing something yeah i was just gonna say that i think it's a bit of everything right like i i think in terms of models um one thing i'll say is that we know that there's more feedback connections and feedback connections in the brain um and we haven't really figured out a way of kind of uh so you know of course we had rnns um you know which sort of implement like you know you can like look through an item that sort of implements a feedback loop but we still haven't really figured out how to you know use that knowledge that the brain has a lot of feedback connections and then apply that to uh like practical practical systems i think on the modeling and like maybe that's one problem um there is like yeah i think curriculum learning is maybe one of them but i think the one that's probably gonna have most bang for buck is really figuring out how we can move beyond text i think there's just like so much of more information that's available that we're just not using and so i think that's where most of the progress might come from like figuring out what's the most practical of going beyond text uh this is what i think okay um let's see uh what are some important nlp topics that we have not covered in this class i do that um you know well sort of one answer is a lot of the topics that are covered in cs224u because you know we do make a bit of an effort to keep them disjoint they're not fully um right so there's sort of lots of topics in language understanding that we haven't covered right so if you want to make um a voice assistant like alexa siri or google assistant well you need to sort of be able to interface with systems apis that can do things like delete your mail or buy you concert tickets and so you need to be able to convert from language into a explicit semantic form that can interact with the systems of the world we haven't talked about that at all um so there's lots of language understanding stuff there's also lots of language generation things so you know effectively for language generation all we have done is neural language models they are great um run them and they will generate language and you know in one sense that's true right like it's just awesome the kind of generation you can do with things like gpt two or three but you know where that's missing is that's really only giving you the ability to produce fluent text where rabbits often produces fluent text that if you actually wanted to have a good natural language generation system you also have to have higher level planning of what you're um going to talk about and how you are going to express it right so that in most situations in natural language you think okay well i want to explain to people something about why it's important to do math classes at college let me think how to organize this maybe i should talk about some of the different applications where math turns up and how it's a really good grounding you know whatever you kind of plan out here's how i can present some ideas right and that kind of natural language generation um we're not doing um any we haven't done any of um yeah i so that's sort of saying more understanding more generation which is most of nlp you can say i mean obviously there are then sort of particular tasks that we can talk about that we either have or haven't not explicitly addressed okay is there has there been any work in putting language models into an environment in which they can communicate to achieve a task and do you think this would help uh with unsupervised learning so again i guess there's been a lot of work on immersion communication um and also self-play where you have like these uh different uh models which are initialized as language models that attempt to communicate with each other to solve some tasks and then you know you have a reward at the end um whether they were able to finish the task or not and then based on that reward you attempt to learn like a communication strategy and this started out as like emergent communication and self-play and then there was like recent work i think it was like i clear last year or the year before that where they showed that if you initialize these models with like uh with like language model pre-training you um basically prevent this problem of like language drift where the language that or the communication protocol that your models end up learning has nothing to do with like actual language um and so yeah i mean from that sense there has been some work um but it's like very limited i think there's like some groups that try to study this but not beyond that okay i mean the last two questions are about gene as well as one question about whether gene smith some correlations from social cues a reward based system i don't know if either of you have opinions about this uh but if you do yeah i mean i don't have anything very deep to say about this question so it's on the importance of social cues as opposed to pure reward based systems well i mean in some sense a social cue you could also regard as a reward that people you know like to um have other people put a smile on their face when you say something um but you know i do think generally um you know not when people are saying what have we not covered another thing that we've barely covered is the social side of language so you know a huge a huge interesting thing about language is it has this very dynamic big dynamic range so on the one hand you can talk about very precise things in language so you can sort of talk about math formulas and steps in a proof and things like that so that there's a lot of precision and language but you know on the other hand you can just sort of emphatically mumble mumble whatever words at all and you're not really sort of communicating anything in the way of a propositional content um what you're really trying to communicate is you know i'm oh i'm thinking about you right now and oh i'm concerned um with how you're feeling or whatever it is in the circumstances right so that a huge part of language use is in forms of sort of social communication between human beings and you know that's another big part of actually building um successful natural language systems right so if you you know if you think negatively about something like the virtual assistants i've been falling back on a lot is you know that they have virtually no ability as social language users right so we're now training a generation of little kids that what you should do is sort of bark out commands as if you were you know serving in the german army in world war ii or something and um that there's none of the kind of social part of how to you know use language um to communicate um satisfactorily with human beings and to maintain a social system and that you know that's a huge part of human language use that kids have to learn and learn to use successfully right you know a lot of being successful in the world is you know you know when you want someone to do something for you you know that there are good ways to ask them for it you know some of its choice of how to present the arguments but you know some of it is by building social rapport and asking nicely and reasonably and making it seem like you're a sweet person that other people should do something for and you know human beings are very good at that and being good at that is a really important skill for being able to navigate the world well you 
","['', 'extremely large language models (ELLMs)', 'gpd3', 'neural network', 'transformer models', 'bird', 'gpt', 'in-context learning', 'language modeling', 'text completion', 'close book QA', 'reading comprehension', 'scalability', 'honeybee brain', 'GPT-2', 'sparse attention patterns', 'in-context learning', 'fast adaptation', 'few-shot learning', 'bash language', '']"
"all right hi everyone um welcome to the 224n python review session um the goal of the session really will be to sort of give you the basics um of python and numpy in particular that you'll be using a lot in your second homework um and the homework will come after that as well um we're sort of taking this tutorial from the background of anyone who hasn't touched programming languages to some extent um but also for people who have will be sort of going through a lot of that material very quickly and we'll be progressing to numpy as well um and as I mentioned first and foremost the session is really meant for the people who are here in person so if you'd like me to slow down speed up at any point need time for clarifications feel free to ask us and it's really meant for you first um here and that I really would like it to be sort of an interactive session as well all right so this is a topic the topics we'll be covering today um going through first of all why python is a language why have we chosen it for sort of discourse and in general why do people prefer prefer it to some extent for machine learning and natural language processing um some basics of the language itself common data structures and then getting to sort of the meat of it through numpy which as I mentioned you'll be extensively using in your homeworks going forward and then some practical tips about how to use um things in Python all right it's first thing why python um so a lot of you who might have um been first introduced to programming might have done Java before a lot of people use Matlab in um other in other fields as well um so why python python is generally used um for one because it's a very high level language um it can look very very english-like and so it's really easy to work with for people especially when they get started out it has a lot of scientific computational functionality as well similar to Matlab so when you talk about numpy you'll see that it has a lot of framework so very very quick and efficient operations involving math or matrices and that's very very useful in applications such as deep learning and for deep learning in particular a lot of Frameworks that people use particularly for example Pi torch and tensorflow interface directly with python and so for that those main reasons people generally tend to use Python within deep learning okay so the setup information is in the slides if you like to look at them offline um I will be sort of jumping over that for now because I want to sort of get to the introduction to the language itself and if you have time come back to sort of the setup information a lot of it's pretty direct you can walk through it um it gives you steps for sort of how to install packages um what is a conda environment for example and gets you set up with your first working python environment so you can sort of run simple and basic commands to get used to the language but for now I'm going to be skipping over this and coming back to it if we have time all right language Basics so um in Python you have variables and these variables can take on multiple values the assignment operation there's an equal sign will allow you to assign this particular value to a variable a nice thing with python is you don't have to instantiate the type of the variable to begin with and then only instantiate or only assign values of that type so for example in certain languages we first say that this variable X is only going to be of type intent any value aside from that assigned to it will throw an error Python's pretty flexible so if I want to I can reassign it I can start with X is equal to 10 and then later on like five lines later I can say x is equal to high as a string and there would be no issue um you can do simple mathematical operations such as the plus and division signs you can do exponentiation which is Raising one value to another value so x to the power of Y for example using the double asterisk um you can do type castings for float division so if you want to ensure that your values are being divided resulting in a float value and not just dividing two integers you can cast two different types like float if you want something to be explicitly an INT you can also just put an INT instead of the float with brackets around the result and that'll give you an integer value and then you can also do typecasting to for example convert from integers to Strings so in this case if I wanted to instead of doing 10 plus 3 as a mathematical operation I just want to write out 10 plus 3 then I can convert the X and Y values for example to Strings and then add the plus sign as as a character as well to create a string and so a lot of these common operations you can look online as well people have lists for them and just see how they're sort of dot in Python all right um some other quick things um so Boolean values the the true and the false um they're always used with capital letters and some of the languages they might be lowercase so just one thing to know um python also doesn't have a null value the equivalent of a null value is none so sometimes when you want to say that this value you want to return none saying I'm not really doing anything here you want to do checks protect for example in if statements um to say that this doesn't have a value then you can assign it to none so none sort of functions as a null equivalent so you're not really returning anything it doesn't have a value not the same as zero and um another nice thing about python is lists which are sort of um mutable we'll come to that a little bit later but sort of mutable lists of objects that means that you can change them they can be of any type so you can have a mixture of integers non-values strings Etc and yeah functions can return the non-value as well um and another quick thing instead of using the double and and uh in some of the languages as people might do with python I mentioned earlier it's very english-like so you can actually just write out um if x is equal to three and and in English Y is equal to four then return true or something um it's it's quite nice that way so you can use and or and not um and then just the comparison operators of equal equals to and not equal to we'll check for equality and inequality this one's pretty standard I feel across many languages and you can use them in python as well and yeah remember just clicking the equal equal to sign is different from the assignment operator this one checks for equality that one is just assigning a value so single equal sign versus two of them all right and then also in Python you don't use brackets so python you can use basically spaces or tabs so either indents of two or four to be able to break up what is contained in the function or contained within like an if statement a for statement um or any Loops for example um and so the main thing is you can choose whether to do two or four you just have to be consistent throughout your entire um code base otherwise they will throw an error now go to some common data structures and for this we'll transition to the collab so this will sort of show you in real time this is by the way a collab a collab is basically um a Jupiter notebook for those of you who are familiar with those um that you can use that it's hosted on Google servers um the really nice thing about jupyter notebooks is you don't have to run an entire file altogether you can run it step by step into what are these called cells so if you want to see like an intermediate output you can see that pretty easily and that way and Indian also writes for example a lot of um like descriptions um pertaining to cells which is really really nice to have as well so a lot of people like tend to use these when they're sort of starting off the project we want to debug things and colab allows you to use these jupyter notebook type applications hosted on their servers for free basically so anyone can create one of these and run their code all right so lists are mutable arrays mutable means that you can change them so that once you declare them you can add to them you can delete them and they're optimized for that purpose so they expect to be changed very often we'll come to what are called numpy arrays later and those tend to be pretty much fixed when you create a new one you'd have when you change one you'd basically have to create a new array um which will have the additional information so this is highly optimized for changing things so if you know for example and you're in a loop you're adding different elements to let's say a bigger entity you'd want to use something like a list because you're going to be changing that very often so let's see how they work so we start off with a names array with Zach and J um you can index into the list um by so what is this index into the list by index which means that you can um list out the elements in the list um depending on What's called the index so it's what place that value is at within the list so zero refers to the first element so Python's what's called zero index which means it starts with zero and then it goes to one so here zero will be Zack and then let's say I want to append something to the end so the to add something to the end of the list the term is append not add and so if I want to append I can now create a separate list which is the original list itself with the added last element and what would currently be the length of this it would be three because you have three elements and you can just quickly get that by using the Len function not length just three letters Len all right um it's also really nice because python has overloaded the plus operation to be able to concatenate lists so here I have a separate list right and all you need for a list definition is just brackets so this is a separate list altogether even though I haven't saved it in the variable just Abby and Kevin and I can just do a plus equal to which means that names is equal to names plus Abby and Kevin and this should output this full list you can create lists by just putting the plain brackets or an existing list and then as I mentioned earlier your list can have a variety of types within them so here this list contains an integer value a list value so you can have a list of lists as many sort of sub lists as you like a float value and a none value and this is completely valid within python slicing refers to how you can access only parts of the list so if I only want for example um in this numbers array I only want 0 1 2 slicing is a way that you can extract only those parts so the way sizing works is the first element is included and the last element is excluded so here I start with 0 1 2 3. so 3 is not included and so 0 1 2 will be printed out there's also shorthands so if you know that you're going to be starting with the first element of the arrays if you know I'm starting I want zero one two and it starts with zero then you don't need to even include the first index you can just leave that and include the last index that would be excluded so that would be blank semicolon 3 and same deal with the end if you know that you want to take everything let's say from like five and six till the end of the array you can start with what would you like so 0 1 2 3 4 5 till the end and leave that oh sorry um fun fact so this um semicolon when you take this just the semicolon it'll take everything in the list but it'll also create a duplicate in memory that's that's a very slight um very useful thing to to know um because sometimes when you're like past Less in an array uh sorry in Python which is out of scope of this tutorial um you can only pass the reference to it so if you will change the array that gets changed this will create an entirely separate copy in memory of the exact same array so if you make any changes to it it won't affect your original array so this is a very pretty neat way to do that um and then another fun thing that python has which is pretty unique is you can index negatively so negative indexing means you index from the back of the array so -1 refers to the last element of the array minus three will refer to the third last element and so what minus one will give you will be six in this case when minus three will give you will be everything because you're starting with the minus three elements so minus one minus two minus three till the end and then this one seems kind of confusing right three to minus two so this will do is it will give you 0 1 2 3 so you start with three and then minus one minus two so you leave off the X the last because you excluded within list um you'd only get three and four so that's what this is okay that's about lists tuples are immutable arrays so once you declare the values of these they cannot be changed so I start with remember we started with like the list of Zach and Jay tuples you start with Zach and Jay um and you can still access them you know I can still print out name zero same as I did with lists but if I try to change it in this case it'll throw an error so two pulls once you've instantiated them they cannot be changed and to create an empty Tuple you just create you can either use just a tuple sign or oftentimes you can just use the parentheses backwards so you can just say for example as you did here just parentheses to instantiate something all right and yeah this one we'll we'll come to a little bit later in shapes but you can also have a tuple of a single value and all you have to do there is just put the value and put a comma so that just shows that you have a tuple which is like with like an immutable array so you can't change it it's a list but only of one item and that's here okay I'll quickly move to dictionaries um for those of you who might be familiar with other languages this is the equivalent of like a hash map or hash table um what this is useful for essentially is mapping one value to another in a really really quick way um so if I want to map for example a string to an index which you will happen to do a lot of in your homeworks um this is a really really useful way to do that and so what what it does is you can instantiate this dictionary and it says corresponding that DAC is going to correspond to the string value whatever it is and so anytime I want to retrieve the string value I just use this dictionary I indexed by it which is what I do here and then it outputs the corresponding value and it does that really really quickly um and yeah so it's really useful very very commonly used especially when you sort for example you have like a list of strings or a list of items and you want to have a corresponding index for them because and as you'll see in NLP oftentimes you're using what you're working with indices and numbers in particular so it's a really great way to sort of move from like string formats to just like um numerical index values there's some other things you can do for dictionaries you can check whether certain elements are in there so if you for example try to index phone book is equal to monsie they'll throw an error because there's no string that says Monty in that phone book dictionary and so sometimes you might be wanting to do checks before you extract a value and so this will just check for example if I do print monsie and phone book it should say false or for example here Kevin and phone book it should say false while something that's actually in that dictionary Zach will be true okay and then if you'd like to delete an entry from the um from the dictionary you can just do that using the Dell command all right let's move to Loops um quickly so Loops are a really great way to optimize for the same kind of app same kind of operation you're doing it's also a great way to um start to sequentially go over those list type or array type objects we were talking about earlier you know you have like a list of names right how do you access all of them so Loops are a really great way to do that um in Python they've abstracted away a lot of the confusing sort of um parts and other languages that might be you can really for example first index on numbers so what you do is you have like a range function that you call so here you say range and the range of the last number you'd want so with this range function will return is 0 1 2 3 4 and that's what will be stored in this I value and here it's just printing out that I value so if I wanted for example Loop over the length of a list of size 10 I just have to do 4i and range 10 and then index that corresponding part of the list you technically don't even have to do that because in Python you can just directly get the element of the list so here I have an a list of um names where I have Zac J and Richard instead of saying first the length of the list and then doing this range operation I can just directly say for name and names and then print out the names and it will just directly get the element in each list but sometimes you might want both you might both want this element Zach as well as its position in the array and for that you can actually use this really helpful function called enumerate and so enumerate will basically pair those two values and it'll give you the both the value which is here name for example and its corresponding index within the array both together so that's really really convenient versus for example having to do this like a little bit more complicated range operation where you first take the range and then you index the US how do you iterate over a dictionary so for dictionaries if you want to enter um iterate over What's called the keys so all of these first items that you first you know put into the dictionary you can just iterate the same way you would a list you just say foreign name and for example phone book and you can output the keys if you want to iterate over what is stored in the list which is called a value you'd have to do the dictionary dot values and if you want both you use the dot items function and so that will print out both of these all right so this is sort of covering the overarching most commonly used sort of structures lists dictionaries and then loops and how to sort of efficiently use them within your code we'll quickly be moving to the sort of meat of what um is really really strong about Python and what you'll be using a lot for your coming homework essentially homework 2 which is numpy okay so for numpy also I'm going to be going to the collab we just quickly wanted to mention um what numpy is so numpy is basically an optimized Library um for mathematical operations you know people tend to like math lab because it's very very useful for these mathematical operations which people use in their research um pythons sort of solution to that is to have a separate Library entirely where they make use of subroutines which are sort of like sub languages sorry sub scripts that are written in a different language called C or C plus plus that are highly optimized for efficiency so the reason C and C plus plus are much faster than python is because they're closer to what's called machine language which is what the computer will read I mentioned earlier one of the nice things about python is it's kind of high level it looks like English right to some extent you know we say literally like is you know if x is equal to one or X is equal to two right but um that also means that there's a lot more translation required on the computer's part before it understands what you mean um and that's useful when you know we're writing out code where we want under understand it but it's a little bit less useful when you're sort of running a lot of operations on a lot of data so the real benefit of something like lumpy is that if you have sort of your memory and your data in a particular format it'll call the these like species scripts or what are called subroutines in a different language and it'll make them very very fast and so that's the real benefit of using numpy and almost everyone um in in sort of NLP is very very familiar with this because you'll be running a lot of operations on for example like co-occurrence matrices which are really really big and um it's very useful to have them optimized for time so that's really the benefit of using numpy and numpy basically it's involved for all these like math and Matrix and Vector calculations and it's different than a list although you can easily translate between a list and a numpy array numpy arrays are specifically as I mentioned designed to be used in these subroutines so they have a specific format they're instantiated differently and you can translate between this and sort of your standard lists easily but to know that you can only do numpy operations on numpy arrays you can't do numpy operations on lists directly you'd first have to like convert them which is really simple you just use this numpy.array function but just know that they operate only on numpy arrays okay so for numpy we're going to be going back to the collab and then as I mentioned earlier the real strength of numpy is you know it supports these large multi-dimensional arrays and matrices for very very optimized high-level mathematical functions um and just to go back step back for a quick second what is a matrix matrices are basically like rectangular um structures of numbers that are used and you can treat them with specific rules um for operations between different kinds of things so if you have like a lot of data instead of you know individually potentially multiplying things if you can store them in this rectangular format you have specific rules about how this Matrix for example interact with a different one and by doing that which is matrix multiplication or Matrix math um you can do a wide variety of mathematical operations a vector is generally this is conventional none of these are like hard and fast rules but conventionally a vector is a matrix in one dimension so it's usually like a row vector or a column Vector which usually just means that it's a list of values and only one mentioned so it's like for example here when I come down to X is equal to numpy array of one two three that's a listen only one dimension versus for example Z when I this is z down here that is what's called like a two-dimensional array because you have both rows for example like six comma seven and then you have eight comma nine um versus in this first one you only have three values in one dimension so that's sort of the conventional difference between the two another convention is matrices generally referred to two-dimensional objects so this as I mentioned is like Z this is two dimensional you might have heard the word tensor also tensors by convention usually are like higher dimensional objects so instead of having two Dimensions you know two comma two you can have like n Dimensions you can have two comma two comma two comma two comma two for like five or six dimensions and those are very valid to do mathematical operations on um and those are often colloquially sort of called tensors in addition and this will be covered in the next tutorial in pi torch um those larger sort of tensors are also optimized for efficiency um to be used on gpus and so they're called tensor in a more concrete way because you're using these tensors with pytorch and other sort of packages to directly do those quicker GPU operations on for deep learning so those are sort of this is a quick sort of terminology difference between the three okay so now um let's start off with just some quick sort of representations of how are these matrices and vectors represented in numpy um this sort of goes back to your question about like what is the difference between like three comma versus like one comma three um so usually three comma and numpy arrays usually just means that you have one list of like one two three for example there's like three values versus if you add another list on top of that this one comma 3 essentially refers to the fact that there's a list of lists so anytime you have two Dimensions it always means that there's a list of lists um and that being like a list of lists for example like a row so here one comma three means that there's one row and then three columns so it's saying there's one row of three comma four comma five essentially and then each of those is a column separately you can easily reshape them so these are basically the same format but from numpy's perspective you'll see a little bit later for operations such as broadcasting you need to have it for example sometimes in this one comma three format or three comma one format um and also like what like as I said three is this he just like she represents three numbers one comma three means like one row of three elements three comma one will mean you have essentially in each column you'll have a separate array so you'll see sort of boxes around each of them there's an example that comes a little bit later in this collab which will make it a little bit more clearer so here if you can see the difference between like X and Y one of them has only one bracket which just says it's one list only one list of one comma two comma three the second one is two brackets which says it's a list with only one list in it if it's a list of a list that's really the main difference between like these sort of two representations so I could have like let's say like a separate one I'm going to call this a and I just do this so it's the same sort of elements but this will be one comma three because it's showing that there's one outer list which shows the rows and then one inner list which I like to have each of those values so the benefit will when I'm coming to what's a little bit later which is broadcasting and so it essentially will help you determine what dimensions you want to match against because sometimes you'd want to have one comma three like one comma two comma three applied only two rows in some other Matrix well we'll come to that a little bit later um but sometimes you might want to have it only applied to columns and so like if I have a separate Matrix for example of zero zero zero zero zero zero zero zero and I want the resulting Matrix to be for example one two three one two three one two three along the rows let me actually draw this out it might be easier so let's say I have like the zero zero zero zero zero zero zero zero and if I want to have a matrix that does one two three one two three one two three versus one two three one two three one two three the difference in how to generate these two um will be the difference in the shape like how you represent their shape it's the same one two three but the resulting array you're generating by repeating the one two three values um requires a difference in shape and so we'll come to that a little bit later because this process of how do you generate these arrays is called broadcasting but that's the real benefit of having an understanding of the shapes the same one two three values are the same it's just how they're sort of used with regards to other arrays all right so yeah vectors can be usually represented as sort of and this is what I talked about earlier as like n Dimensions n by one or one by n dimensions and they can result in this different Behavior kind of what like this that I talked about um matrices are usually in two Dimensions represented as M by n um these are just two examples if for example I generate let's say an engine also reshape so I start with for example this array which is a list of 10 oh sorry it's important on Pi quickly so I start off with this Matrix a which is basically a one-dimensional list of ten values I can reshape it into a five by two Matrix so you just have to make sure that your Dimensions match which means that like you can multiply them together and get the original size so if I start off with the 10 matrix I can make a two by five Matrix I can make a five by two Matrix I can make a ten by one one by ten I can't make it for example three and five because that it wouldn't fit into the original size um and for that this operation called reshape is really useful um you might be wondering why is there two parentheses the way that reshape works is essentially it'll take in a tuple so remember that what I was talking about earlier with tuples is that these they're immutable objects and they're defined by parentheses so the outer parenthesis is representing what you're inputting to the function and what you're inputting is a tuple so it uses a second set of parentheses so now let's go to some array operations um so I started off with you know this array X um when you apply simple operations for example a Max operation sometimes you might want the max of the entire array so if I do the max of in the entire array what's the max value of the entire array by the way just the entire thing six right so if I just do NP dot Max of X it'll return one value and return six but let's say I want the max of every row right like and every in each of these rows I say I want let's say the max of each I want two and then four and then six how do you do that and so numpy always has like usually in most of their functions an access variable and what the axis variable will do is it'll tell you which of these Dimensions do you want to take the max over and the way to sort of think about it is this is going to be a little bit tricky but the way people describe it is the access is what you want to apply your function over what you want to reduce over and what that means is I print out the shape of the original array it's three by two I want to apply access one or as I remember you know numpy is zero indexed it'll be zero one so I want to apply the max over the second dimension the second dimension means that for each of these essentially you know that like for like the row Dimension is the First Dimension so it's not around along the rows I'm going to be comparing columns and so compare this entire column to this entire column and so just remember for axes um usually the axis zero refers to the row axis and then axis one refers to the column access um if you don't even want to remember that you can just remember that from the original Dimension which of these it's referring to um and that's the dimension you want to compare over or reduce over so it can be a little bit harder to grasp around it usually the best way to sort of get around is like just play with a bunch of sort of operations of min max and things like that but just remember like the access is what you want to compare over not the resulting thing so axis one means here column I want to compare between the columns I want to get for example comparing one to two three to four five to six does that make sense okay and what this will do is if I just do numpy.axis it'll just return basically since I'm comparing these columns it'll just return a resultant column and so as I mentioned you know um for over the axis one you get three values because you're comparing over these columns and each column has three values I'm comparing over rows as you mentioned I get two values right um and so this will just be the Tuple comma which is just indicating that it's just a list it's not a list of lists it's just a list but let's say I want a list of lists you know maybe I want to do those operations I talked about earlier um instead of reshaping which is always there it's always an option you can also use this um feature called keep dimms and what that'll do is it'll take the original Dimensions which is two Dimensions right because you have three comma two just two of them and it'll keep that consistent so it'll be three comma one but it just means that instead of returning just the extracted column which is just a list it'll basically keep the column in the context of the original sort of X and it'll be it'll keep it as like a two-dimensional value all right now these are just some operations so in numpy um you can use an asterisk as a an element-wise multiplication so an asterisk means that I'm going to be comparing every single value um to every single corresponding value in another Matrix and it's you need your matrices to also be the same size for this one so this one it's basically an element wise matrix it's not a matrix multiplication so you need to have them be the exact same size so this will compare for example one into three two into three three into three and four into three all right um you can also do matrix multiplication which is a different operation entirely um for those of you unfamiliar with matrix multiplication um you would basically be multiplying a row of one Matrix with the column of another Matrix and for that to be necessary you need to have the second dimension of the first array be equal to the first dimension of the second array so for matrix multiplication if I have an a and two B comma 3 in tune c um shaped matrices these two have to be equal for matrix multiplication just something to keep in mind because oftentimes if you're doing matrix multiplication um you need you have to make sure these dimensions are the same which means that for example this is a valid operation um but this can sometimes throw an error sometimes so it's just important to make sure that sometimes you you want to make sure that these are exactly equal you can actually just print out the shapes and make sure that these are equal to be doing matrix multiplication and then for matrix multiplication um there's a couple of functions you can use the first one is just np.mat mule which is NP dot matrix multiplication you can also just use the um the at operation and that one both of those are overloaded you can choose whichever one they'll result in the same exact operation and just a quick session show you can to show what this will do is we'll multiply one into two so it'll come like one two versus three four so it'll do one into three two into three and add those two values so that's what matrix multiplication will do okay and then dot products will what what a DOT product is that it takes two vectors so usually it operates on vectors and a vector as I mentioned is just like a one-dimensional matrix so it's just basically Three cross one for example a four cross one um it'll element wise multiply between two different vectors and we'll sum up those values and so here what a DOT product do would be like one into one plus two into ten plus three into a hundred and for a numpy you can just do NP Dot and then both of those vectors um this one is just a side on how you would want the structure of the dot product to be um for arrays that are more so okay so the phrase is the best way um for single dimensional vectors this operation Works directly anytime it's a multiple dimensional Matrix um then it treats it as a matrix multiplication the NP dot dot function so for two by two Matrix versus a two by two Matrix dot product it's not going to return the sum it's going to return um the matrix multiplication so that's just something to keep in mind if you want to make sure that your your dot product is happening in the correct way you would want to make sure that sort of similar to what I was talking about earlier that here this is I think the best way best way to show it okay so you would want the second like that what I mentioned like the last dimension of the first one to match with the first dimension of the next one because it's treating it as like a matrix multiplication um here the error that it's throwing is it's three comma two combined with three and so the way to sort of like fix that would be to have this be like for example like um switch the two so you have two comma three and then three comma it's really a dimension matching thing at this point so it's it can be a little bit confusing but when you sort of the main thing to keep in mind is like for single dimensional vectors you can just do NP dot dot directly and they'll give you the dot product value for higher dimensional matrices it treats it as a matrix multiplication um and so for if you still want to like for those higher dimensional values to ensure that you're getting a DOT product um you'd have to make sure that the dimensions are aligned similar to these so anything that's two by two plus for both um any any Matrix who doesn't have a single dimension in any of them yes it would treat it as a matrix matte Neil the same thing okay all right um okay I'm going to move to indexing so similar to what I was talking about earlier remember with list I was saying if you just do the semicolon it'll create like the same array same deal here the the semicolon just means that you take everything from the original array in fact it returns a copy so it returns a deep copy means if you have a completely separate copy in memory um okay now I'm going into sort of more details about how do you want to index quickly so if I for example have let's say this three by four Matrix and I only want to select the zero and the second rows how would I do that so what's useful is that you can sort of treat a numpy you can treat different dimensions differently for indexing so a semicolon means you select everything in that Dimension which for example here there's a semicolon in the second dimension which means I'm taking all of the column values um versus what's in the First Dimension here it's saying a numpy array of zero and two so it's saying only the zero index and only the two index which means only the zero would throw and only the second row so what this would look like would be something like I have a matrix okay I have a matrix and I only want to select the zeroth row and I only want to select the column the second row 0 and second and everything in the columns all right and then similarly for example if I want to select in the column Dimension um I want to select the first and second rows at only the first row I can do that so you can basically treat them separately you can think how many columns do I want how many rows do I want and then index so separately and that goes for as many dimensions as you want in your entire tensor um some nice things also if I want to for example take it I have this like let me print that actually X here I'll just generate the X okay so this is X right so if I want to take all the values of X that are above 0.5 for example I can do that by using what's called Boolean indexing so I just basically would say x indexed by everything in X that's bigger than 0.5 so it's pretty direct and it'll just output all the values in this entire array that are bigger than 0.5 all right this one is also another way to do reshaping so I kind of mentioned earlier you know sometimes you won't have this like list of three elements and you want to reshape it to a three by one array for example you can also use what's called numpy.new access this will essentially add another access in whatever Dimension you want so if I want to change go from like this three by four array to a three by three by four two three by four by one then I can just add a numpy.nu axis there an even simpler way to think about it would be like a 2 comma to uh two comma one and so it's just it's another way to do what essentially what would be the reshape reshaping operation does that make sense also what this would look like for example let me just a little bit more concrete this is so as we see I have this list right I have like a singular list and then each in in that list I have a list of lists so I have a list with element one and list of element two so this is what that reshape operation will do and what numpy.new access will enable you to do as well all right um I think we are a good time um so the last main topic we'll be covering is broadcasting um and what's really great about broadcasting is it'll allow you to operate with numpy arrays that are of different shapes but can be sort of with many operations in them can be repeated it allows for that in a very efficient manner and this is actually one of the most I would say useful things about numpy and one of its defining features and what that means is um if for example in this case right if we go back to this example that I had with I start off with the zero zero zero array how do I generate this array versus how do I generate this array right instead of me saying okay element zero zero plus one element zero one plus two all that stuff right instead of doing that one by one what broadcasting allows me to do is I can have only one vector of size one two three and it'll depending on how I do the broadcasting which I'll come to in a second I can duplicate it along the row Dimension or I can duplicate it along the column Dimension and numpy allows for that it'll do that on its own in the back end and so that's really what broadcasting means is I don't need to for example create a new array saying I want to like create a new array to begin with which is already like this and then add those two together I can just duplicate this and get this all right so now some rules for broadcasting and let me just quickly visually also just show what broadcasting will do oh sorry so broadcasting this is a pretty good visual analogy um I had this one by one comma one comma two comma three Vector right um and I wanna basically add let's say only the columns with this one comma two comma 3 Vector so what broadcasting allows you to do is you'll you only pass these two values in and on the back end it'll duplicate this along the column Dimension so let's say I have one two three one two three one two three one two three and then it'll do the addition similarly if I pass it a vector one comma two comma three comma four and I want it to be added to each of the rows instead of each of the columns it'll be able to do that by sort of duplicating it on the back end so this is visually what's happening with Broadcasting all right now some rules so how does numpy know when and how to do broadcasting so the main two rules to keep in mind with for broadcasting is one um it can only happen if all of the dimensions every single Dimension between two arrays are compatible and when they say what is compatible either the dimension values are equal or one of them is equal to one and that is the only rule required so for example I start off with this x array right I have this like a three by four x array um will Y is equal to three comma 1 be compatible yes it will be why because you have three in the First Dimension between the two which is the same and in the second dimension you have four and you have one so those are compatible values and so what this tells numpy on the back end is I'm doing for example an addition Operation X Plus y it knows that okay three and three are the same but four and one are not the same you know one of them has one dimension so I need to duplicate this y along the second dimension which means I need to duplicate it along the column Dimension and once it does that it duplicates it it'll get four three comma four in Array and then it can do the addition and it does that really fast so it's better to use broadcasting in this way but then for you to create a separate array already duplicated and then add them similarly I have this Z array which is one comma four what x into Z will do is first I'll check okay three comma one okay is that compatible yes because you have three in one dimension you have one in the second and four and four are compatible okay so say I know that these two are compatible in the second dimension I'm going to change anything in the First Dimension it'll know to duplicate them basically so in order to duplicate Z and so add it three times in the row Dimension create a separate array and then multiply those two so this is giving you an example of saying I started off with X I have y and then the final shape will be three comma four so a lot of times in deep learning um you will have the same um because you'll have different batches of different images coming in but you want to apply let's say the same weight Matrix to all of them and instead of duplicating that weight Matrix a hundred or even like potentially depending on the size of your batch size like a thousand times and then adding those together you use the same Matrix and it'll know okay if I'm going to be duplicating over the batch Dimension it'll do that for you on the back end so it's use a lot of times in deep learning because of this and basically in your second homework that's basically what you'll be doing implementing a feed floral Network in numpy and it'll say you have like this W Matrix yeah this like B Matrix which is a by we'll come to those in class and it'll ask you to implement their numpy because that's basically what you're doing is if you have this input image you have a weight Matrix which will somehow scale it to an output and that weight Matrix will be applied to multiple images in your batch and those images can be different but their sizes will be the same and it's optimized for that okay um so this is just more examples of sort of the same thing your final thing that you'll be coming to is a size of three comma four um let's see this one's sort of the example that I showed right here right which is that I have this array of flight say zeros I have this numpy like this B array of size what size were they would this be yes good because you have one outer list and inside this you have one inner list so it's just basically one row and then three values inside so yes and so would this be compatible yes and so it'll know basically to duplicate um over the row Dimension and so you're going to get duplicates in the row Dimensions you're going to get one two three one two three one two three and that's what's Happening Here um so these are for example a little bit sometimes when it says more complex um Behavior what this basically just means is that like if I have this B Vector which is three comma one if I'm doing this B plus b dot transpose by the transpose is just changing the dimensions and switching them so if I have a two by three Matrix uh transpose will be a three by two Matrix um what that means visually is something like your row and rows and like column Dimensions will get switched six goes to I believe it's like one two three four five six so like three row rows versus like three columns um and what this is just saying is that uh a three by one and a one by three um both of those vectors will be compatible because remember in each Dimension it's either the same or one and so it knows to duplicate uh over both of those dimensions and that's what's Happening Here uh okay so I think we are right at time um and what I would recommend is basically playing with variations of this for broadcasting and see December the two rules for broadcasting is just if it's compatible it's either the same value or it's one and whatever is the one dimension is what's going to be duplicated over on the back end so yeah it's not going to be compatible if they're divisible for example right so if you have like let's say six and three that's not compatible um you can reshape it and then see if you'd like to have one there's tricks you can use um where you're sort of thinking like on the back end how do I want this data to be multiplied you can maybe reshape everything into like an eight one like one by eighteen Matrix and then multiply everything and then reshape it back that's what you can do but you can never just directly for example six by three make that compatible okay um so I think let's wrap up this one's just a quick example of another use of efficient numpy code um quick note never preferably don't use uh Loops whenever you're dealing with large data matrices mostly because Loops are almost always about a hundred times slower numpy is usually very very efficient as this is just an example of what you can accomplish with numpy and same thing using Loops so what this is saying is that I have an X Matrix of size thousand by thousand and I want to apply you know let's say I want to add everything from row 100 onwards um with a plus five so visually what that will look like is something like I have this full Matrix and I wanted everything here basically to be add with plus added with plus five um then in in the loop format I can basically Loop over the First Dimension um of 100 plus and do that or numpy I can basically do what's called numpy.a range which will generate um integers in like we see one two three four five six all the way up to that hundred value in this case it's between hundred and thousands let's start with hundred hundred one hundred two all the way two thousand in the First Dimension and then just add that with five so this is just an example of how you would switch from using Loops to using numpy and it's a lot lot faster 
","['', 'Python review session', 'Introduction to Python', 'Why Python is used for deep learning', 'Setting up Python environment (mentioned but not covered in the video)', 'Variables in Python', 'Data types in Python', 'Type casting in Python', 'Boolean values in Python', 'None value in Python', 'Lists in Python', 'Functions in Python', 'Conditional statements in Python (if, else)', 'Comparison operators in Python', 'Indentation in Python', 'Arrays in Python (NumPy)', 'Indexing arrays in Python', 'Slicing arrays in Python', 'Concatenating arrays in Python', 'Negative indexing in Python', '']"
"and so today I kind of just want to cover the fundamentals of Pi torch um really just kind of see what are the similarities between pi torch and numpy and python which you guys are used to at this point and see how we can build up a lot of the building blocks that we'll need in order to Define more complex models so specifically we're going to talk today about tensors what are tensor objects how do we manipulate them uh what is auto grad how pytorch helps us compute different gradients and finally how we actually do optimization and how we write the training Loop for our neural networks and if we have time at the end then we'll try and go through a bit of a demo to kind of put everything together and see how everything comes together when you want to solve an actual NLP task all right so let's get started so if you go to the course website there's a notebook and you can just make a copy of this collab notebook and then just run the cells as we go and so to start today we're talking about Pi torch like I said it's a deep learning framework that really does two main things one is it makes it very easy to author and manipulate tensors and make use of your GPU so that you can actually leverage a lot of that capability and two is it makes the process of authoring neural networks much simpler you can now use different building blocks like linear layers and different loss functions and compose them in different ways in order to author the types of models that you need for your specific use cases and so pytorch is one of the two main Frameworks along with tensorflow in this class we'll focus on Pi torch but they're quite similar and so we'll start by importing torch and we'll import the neural network module which is torch.nn and for this first part of the tutorial I want to talk a bit about tensors one thing that you guys are all familiar with now is numpy arrays and so pretty much you can think about tensors as the equivalent in pi torch to numpy arrays they're essentially multi-dimensional arrays that you can manipulate in different ways and you'll essentially use them to represent your data to be able to actually manipulate it and perform all the different Matrix operations that underlie your neural network and so in this case for example if we're thinking of an image one way you can think about it in terms of a tensor is it's a 256 by 256 tensor where it's has a width of 256 pixels and a height of 256 pixels and for instance if we have a batch of images and those images contain three channels like red green and blue then we might have a four-dimensional tensor which is the batch size by the number of channels by the width and the height and so everything we're going to see today is all going to be represented as tensors which you can just think of as multi-dimensional arrays and so to kind of get some intuition about this we're going to spend a little bit of time going through essentially lists of lists and how we can convert them into tensors and how we can manipulate them with different operations so to start off with we just have a simple list of lists that you're all familiar with in this case it's a two by three list and now we want to create a tensor and so here the way we'll create this tensor is by doing torch.tensor and then essentially writing the same syntax that we had before just write out the list of lists that represents that particular tensor and so in this case we get back a tensor object which is the same shape and contains the same data and so now the second thing with the tensor is that it contains a data type so there's different data types for instance there are different varying level of precision floating Point numbers that you can use you can have integers you can have different data types that actually populate your tensor and so by default I believe this will be float32 but you can explicitly specify which data type your tensor is by passing in the d-type argument and so we see here now even though we you know wrote in a bunch of integers they have a decimal point which indicates that they're floating Point numbers and so same thing here we could create another tensor in this case with data type float32 and in this third example you see that we create another tensor we don't actually specify the data type but pytorch essentially implicitly takes the data type to be floating points since we actually passed in a floating Point number into this tensor so pretty much at a high level tensors are like multi-dimensional arrays we can specify the data type for them we can populate them just like numpy rays okay so now great we know how to create tensors we know that ultimately everything that we work with all the data we have is going to be expressed as tensors now the question is what are the functions that we have to manipulate them and so we have some basic utilities that can help us instantiate tensors easily specifically torch.zeros and torch.1s these are two ways to create tensors of a particular shape in this case tensors of all zeros or tensors of all ones and you'll see that this will be very helpful when you do your homeworks typically you'll you'll want to just need to just create a bunch of zero Matrix and it'll be very easy to just specify the shape here without having to write everything out super explicitly and then you can update that tensor as needed another thing you can do is just like we have ranges in Python so if you want to Loop over a bunch of numbers you can specify a range you can also use torch dot a range to be able to actually instantiate a tensor with a particular range in this case we just looped over the numbers one through ten you could reshape this and make it one through five and then six through ten that's another way to be able to instantiate tensors and finally something to note is that when we apply particular operations such as just simple python operations like addition or multiplication by default they're going to be element wise so they'll apply to all the elements in our tensor so in this case we took our tensor I think this one was probably from earlier above and we added two everywhere here we've multiplied everything by two but pretty much the pi torch semantics for broadcasting work pretty much the same as the numpy semantics so if you pretty much have different Matrix operations where you need to batch across a particular Dimension pytorch will be smart about it and it will actually make sure that you broadcast over the appropriate Dimensions although of course you have to make sure that the shapes are compatible based on the actual broadcasting rules so we'll get to that in a little bit when we look at reshaping and how the bra uh how different you know operations have those semantics in this case we have to define the I guess I'm not personally aware of how you would Define kind of a jagged tensor that has unequal dimensions um but typically we don't want to do that because it makes our computation a lot more complex and so in cases where we have you know for instance we have different sentences that we turn into tokens we might have different length sentences in our training set we'll actually pad all the dimensions to be the same because ultimately we want to do everything with Matrix operations and so in order to do that we need to have a matrix of a fixed shape um but yeah that's that's a good point I I'm not sure if there is a way to do that but typically we just get around this by padding okay so now we know how to define tensors we can do some interesting things with them so here we've created two tensors one of them is a three by two tensor the other one is a two by four tensor and I think the answer is written up here but what do we expect is the shape when we multiply these two tensors so we have a three by two tensor and a two by four tensor yeah three by four and so more generally um we can use matte mole in order to do matrix multiplication it also implements batch to matrix multiplication and so I won't go over the entire review of broadcasting semantics but the main gist is that the dimensions of two tensors are compatible if you can left pad the tensors with ones so that the dimensions that line up either a have the same number in that Dimension or B one of them is a dummy Dimension one of them has a one and in that case in those dummy Dimensions Pi torch will actually make sure to copy over the tensor as many times as needed so that you can then actually perform the operation and that's useful when you want to do things like batch dot products or bashed Matrix multiplications and I guess the final Point here is there's also a shorthand notation that you can use so instead of kind of having to type out matte mole every time you can just use the add operator similar to numpy effectively that's kind of where we get into how batching works so for example if you had um let's say two tensors that have um some batch Dimension and then one of them is M by one and the other one is one by n and if you do a batched matrix multiply to those two tensors now what you effectively do is you preserve the batch Dimension and then you're doing a matrix multiplication between an M by one tensor and a one by n so you get something that's the batch Dimension by m by n so effectively they're kind of more I think the full semantics are written out on the pytorch website for how the matrix multiplication works but you're right you don't just have these cases where you have two two dimensional tensors you can have arbitrary number of dimensions and as long as the dimensions match up based on those semantics I was saying then you can multiply it alternatively you can do what I do which is just multiply it anyways and then if it throws an error print out the shapes and kind of work from there that tends to be faster in my opinion a lot of ways but yeah that's a good point all right so yeah let's keep going through some of the other different functionalities here so we can Define another tensor um and kind of one of the key things that we always want to look at is the shape so in this case we just have a 1D tensor of length three so the torch dot size just gives us three in general this is kind of one of the key debugging steps and something that I'll try and emphasize a lot throughout this session which is printing the shapes of all of your tensors is probably your best resource when it comes to debugging it's kind of one of the hardest things to Intuit exactly what's going on once you start stacking a lot of different operations together so printing out the shapes at each point and seeing do they match what you expect is something important and it's better to rely on that than just on the error message that pytorch gives you because under the hood pytorch might Implement certain optimizations and actually reshape the underlying tensor you have so you may not see the numbers you expect so it's always great to print out the shape and so yeah let's uh so again we can always print out the shape and we can have a more complex uh in this case a three-dimensional tensor which is three by two by four and we can print out the shape and we can see all the dimensions here and so now you're like okay great we have tensors we can look at their shapes but what do we actually do with them and so now let's get into kind of what are the operations that we can apply to these tensors and so one of them is it's very easy to reshape tensors so in this case we're creating this 15 dimensional tensor that's the numbers 1 to 15 and now we're reshaping it so now it's a five by three tensor here and so you might wonder well like what's what's the point of that and it's because a lot of times when we are doing machine learning we actually want to learn in batches and so we might take our data and we might reshape it so now that instead of kind of being a long flat and list of things we actually have a set of batches or in in some cases we have a set of batches of a set of sentences or sequences of a particular length and each of the elements in that sequence has an embedding of a particular dimension and So based on the types of operations that you're trying to do you'll sometimes need to reshape those tensors and sometimes you'll want to particularly sometimes transpose Dimensions if you want to for instance reorganize your data so that's another operation to keep in mind I believe the differences view will um view will create a view of the underlying tensor and so I think the underlying tensor will still have the same shape reshape will actually modify the tensor um all right and then finally like I said at the beginning your intuition about pytorch tensors can simply be their kind of a nice easy way to work with numpy arrays but they have all these great properties like now we can essentially use them with gpus and it's very optimized and we can also compute gradients quickly and to kind of just emphasize this point if you have some numpy code and you have a bunch of numpy arrays you can directly convert them into Pi torch sensors by simply catch casting them and you can also take those tensors and convert them back to numpy arrays all right and so one of the things you might be asking is why do we care about tensors what makes them good and one of the great things about them is that they support vectorized operations very easily essentially we can parallelize a lot of different computations and do them for instance across a batch of data all at once and one of those operations you might want to do for instance is a sum so you can take in this case a tensor which is shape five by seven and it looks like that's not working you can take a tensor that's shaped five by seven and now you can compute different operations on it that essentially collapse the dimensionality so the first one is sum and so you can take it and you can sum across both the rows as well as the columns and so one way I like to think about this to kind of keep them straight is that the dimension that you specify in the sum is the dimension you're collapsing so in this case if you take the data and sum over Dimension zero because you know the shape of the underlying tensor is five by seven you've collapsed the zeroth dimension so you should be left with something that's just shape seven and if you see the actual tensor you got 75 80 85 90. you get this tensor which is shape seven alternatively you can think about whether or not you're kind of summing across the rows or something across the columns but it's not just some it applies to other operations as well you can compute standard deviations you can normalize your data you can do other operations which essentially batch across the entire set of data and not only do these apply over one dimension but here you can see that if you don't specify any dimensions then by default the operation actually applies to the entire tensor so here we end up just taking the sum of the entire thing so if you think about it the zeroth dimension is the number of rows there are five rows and there are seven columns so if we sum out the rows then we're actually summing across the columns and so now we only have seven values but I like to think about more just in terms of the dimensions to keep it straight rather than rows or columns because it can get confusing if you're summing out Dimension zero then effectively you've taken something which has some shape that's Dimension Zero by Dimension One to just whatever is the dimension one shape and then from there you can kind of figure out okay which way did I actually sum to check if you were right numpy implements a lot of this vectorization and I believe in the homework that you have right now I think part of your job is to vectorize a lot of these things so the big Advantage with pi torch is that essentially it's optimized to be able to take advantage of your GPU when we actually start building out neural networks that are bigger that involve more computation we're going to be doing a lot of these matrix multiplication operations that it's going to be a lot better for our processor if we can make use of the GPU and so that's where pytorch really comes in handy in addition to also defining a lot of those neural network modules as we'll see later for you so that now you don't need to worry about for instance implementing a basic linear layer and back propagation from scratch and also your Optimizer all of those things will be built in and you can just call the respective apis to make use of them whereas in Python and numpy you might have to do a lot of that coding yourself all right so we'll keep going so this is a quiz except I think it tells you the answer so it's not much of a quiz but pretty much you know what would you do if now I told you instead of you know summing over this tensor I want you to compute the average and so there's there's two different ways you could compute the average you could compute the average across the rows or across the columns and so essentially now we kind of get back to this question of well which dimension am I actually going to reduce over and so here if we want to preserve the rows then we need to actually sum over the second dimension um they're really the first uh zeroth and first so the First Dimension is what we have to sum over because we want to preserve the zeroth dimension and so that's why for row average you see the dim equals one and for column average same reasoning is why you see the dim equals zero and so if we run this code we'll see kind of what are the shapes that we expect if we're taking the average over rows then an object that's two by three should just become an object that's two it's just a one-dimensional almost a vector you can think of and if we are averaging across the columns there's three columns so now our average should have three values and so now we're left with a three a one-dimensional tensor of length three so yeah does that kind of make sense I guess is this General intuition about how we deal with shapes and how some of these operations manipulate shapes so now we'll get into indexing this can get a little bit tricky but I think you'll find that the semantics are very similar to numpy so one of the things that you can do in numpy is that you can take these numpy arrays and you can slice across them in many different ways you can create copies of them and you can index across particular Dimensions to select out different elements different rows or different columns and so in this case let's take this example tensor which is three by two by two and first thing you'll always want to do when you have a new tensor print out its shape understand what you're working with and so I guess uh I may have shown this already but what will X bracket zero print out what happens if we index into just the first element what's the shape of this yeah two by two right because if you think about it our tensor is really just a list of three things each of those things happens to also be a two by two tensor so we get a two by two object in this case the first thing one two three four and so just like numpy if you provide a colon in a particular Dimension it means essentially copy over that dimension so if we do X bracket zero implicitly we're essentially putting a colon for all the other dimensions so it's essentially saying grab the first thing along the zeroth dimension and then grab everything along the other two dimensions if we now take uh just the zeroth along the element along the First Dimension um what are we going to get well ultimately we're going to get now if you look uh the kind of First Dimension were these three things the second dimension is now each of these two rows within those things so like one two and three four five six and seven eight 9 10 and 11 12. so if we index into the second dimension or the First Dimension and get the zeroth element then we're going to end up with one two by six and nine ten and even if that's a little bit tricky you can kind of go back to the trick I mentioned before where we're slicing across the First Dimension so if we look at the shape of our tensor it's three by two by two if we collapse the First Dimension that two in the middle we're left with something that's three by two so it might seem a little bit trivial kind of going through this in a lot of detail but I think it's important because it can get tricky when your tensor shapes get more complicated how to actually reason about this and so I won't go through every example here since a lot of them kind of reinforce the same thing but I'll just highlight a few things just like numpy you can choose to get a range of elements in this case where we're taking this new tensor which is one two one through fifteen rearranged that's a five by three tensor and if we take the zero through third row um exclusive we'll get the first three rows and we can do the same thing but now with slicing across multiple dimensions and I think the final point I want to talk about here is list indexing list indexing is also present in numpy and it's a very clever shorthand for being able to essentially select out multiple elements at once so in this case what you can do is if you want to get the zeroth the second and the fourth element of our Matrix you can just instead of indexing with a particular number or set of numbers index with a list of indices so in this case if we go up to our tensor if we take out the zeroth the second and the fourth we should see those three rows and that's what we end up getting yeah again these are kind of a lot of examples to just reiterate the same point which is that you can slice across your data in multiple ways and at different points you're going to need to do that so being familiar with the shapes that you understand what's the underlying output that you expect is important in this case for instance we're slicing across the first and the second dimension and we're keeping the first uh the zeroth and so we're going to end up getting essentially kind of the the top left element of each of those three things in our tensor if we scroll all the way up here we'll get this one we'll get this five and we'll get this nine because we go across all of this the Earth Dimension and then across the first and the second we only take the first uh the the zeroth element in both of those positions and so that's why we get 1 5 9. and also of course you can you know apply all of the colons to get back the original tensor okay and then I think the last thing when it comes to indexing is conversions so typically when we're writing code with neural networks ultimately we're going to you know process some data through a network and we're going to get a loss and that loss needs to be a scalar and then we're going to compute gradients with respect to that loss so one thing to keep in mind is that sometimes you might have an operation and it fails because it was actually expecting a scalar value rather than a tensor and so you can extract out the scalar from this one by one tensor by just calling dot item so in this case you know if you have a tensor which is just literally one then you can actually get the python scalar that corresponds to it by calling dot item so now we can get into the more interesting stuff one of the really cool things with pytorch is autograd and what autograd is is high torch essentially provides an automatic differentiation package where when you define your neural network you're essentially defining many nodes that compute some function and in the forward past you're kind of running your data through those nodes but what pytorch is doing on the back end is that each of those points it's going to actually store the gradients and accumulate them so that every time you do your backwards pass you apply the chain rule to be able to calculate all these different gradients and pytorch caches those gradients and then you will have access to all of those gradients to be able to actually then run your favorite Optimizer and optimize you know with SGD or with atom or whichever Optimizer you choose and so that's kind of one of the great features you don't have to worry about actually writing the code that computes all these gradients and actually caches all of them properly applies the chain rule does all these steps you can have shocked all of that away with just one call to dot backward and so in this case we'll run through a little bit of an example where we'll see the gradients getting computed automatically so in this case we're going to initialize a tensor and requires grad is true by default it just means that by default for a given tensor python pytorch will store the gradient associated with it and you might wonder well you know why why uh why do we have this you know when we always want to store the gradient and the answer is at train time you need the gradients in order to actually train your network but at inference time you'd actually want to disable your gradients and you can actually do that because it's a lot of extra computation that's not needed since you're not making any updates to your Network anymore and so let's create this right now uh we don't have any gradients uh being computed because we haven't actually called backwards to actually compute um some quantity with respect to this particular tensor we haven't actually computed um those gradients yet so right now the dot grad feature which will actually store the gradient associated with that tensor is not and so now let's just Define a really simple function we have X we're going to define the function y equals 3x squared and so now we're going to call Y dot backward and so now what happens is when we actually print out x dot grad what we should expect to see is number 12. and the reason is that our function y is 3x squared if we compute the gradient of that function we're going to get 6x and our actual value was 2. so the actual gradient is going to be 12. and we see that when we print out X talk grad that's what we get and now we'll just run it again let's set Z equal to 3x squared we call Z dot backwards and we print out X talk grad again and now we see that I may not run this in the right order Okay so here in the second one that I re-rad we see that it says 24. and so you might be wondering well I just did the same thing twice shouldn't I see 12 again and the answer is that by default pytorch will accumulate the gradients so it won't actually rewrite the gradient each time you compute it it will sum it and the reason is because when you actually have back propagation for your network you want to accumulate the gradients you know across all of your examples and then actually apply your update you don't want to overwrite the gradient but this also means that every time you have a training iteration for your network you need to zero out the gradient because you don't want the previous gradients from the last Epoch where you iterated through all of your training data to mess with the current update that you're doing so that's kind of one thing to note which is that that's essentially why we will see when we actually write the training Loop you have to run zero grad in order to zero out the gradient yes so I accidentally ran the cells in the wrong order maybe to make it more clear let me put this one first so this is actually what it should look like which is that we ran it once and I ran this cell first and it has 12. and then we ran it a second time and we get 24. yes so if you have all of your tensors defined then when you actually called out backwards if it's a function of multiple variables it's going to compute all of those partials all of those gradients yeah so what's happening here is that the way Pi torch works is that it's storing the accumulate accumulated gradient at X and so we've essentially made two different backwards passes we've called it once on this function y and we've which is a function of X and we've called it once on Z which is also a function of X and so you're right we can't actually disambiguate which came from what we just see the accumulated gradient but typically that's actually exactly what we want because what we want is to be able to run our Network and accumulate the gradient across all of the training examples that Define our loss and then perform our Optimizer step so yeah even with respect to one thing it doesn't matter because in practice each of those things is really a different example in our set of training examples and so we're not interested in you know the gradient from one example we're actually interested in the overall gradient so going back to this example What's Happening Here is that in the backwards pass what it's doing is you can imagine there's the X tensor and then there's the dot grad attribute which is another separate tensor it's going to be the same shape as X and what that is storing is it's storing the accumulated gradient from every single time that you've called dot backward on a quantity that essentially has some dependency on X that will have a non-zero gradient and so the first time we call it the gradient will be 12 because 6X 6 times 2 12. the second time we do it with Z it's also still 12. but the point is that dot grad doesn't actually overwrite the gradient each time you called out backwards it simply adds them it accumulates them and kind of the intuition there is that ultimately you're going to want to compute the gradient with respect to the loss and that loss is going to be made up of many different examples and so you need to accumulate the gradient from all of those in order to make a single update and then of course you'll have to zero that out because every time you make one pass through all of your data you don't want that next batch of data to also be double counting the previous batches update you want to keep those separate and so we'll see that in a second all right so now we're going to move on to one of the final pieces of the puzzle which is neural networks how do we actually use them in pi torch and once we have that and we have our optimization we'll finally be able to figure out how do we actually train a neural network what does that look like and why it's so clean and efficient when you do it in pi torch so the first thing that you want to do is we're going to be defining neural networks in terms of existing building blocks in terms of existing apis which will Implement for instance linear layers or different activation functions that we need so we're going to import torch.nn because that is the neural network package that we're going to make use of and so let's start with the linear layer the way the linear layer Works in pi torch is it takes in two arguments it takes in the input Dimension and then the output dimension and so pretty much what it does is it takes in some input which has some arbitrary amount of dimensions and then finally the input Dimension and it will essentially output it to that same set of Dimensions except the output dimension in the very last place and you can think of the linear layer as essentially just performing a simple ax plus b by default it's going to um it's going to apply a bias but you can also disable that if you don't want a bias term and so let's look at a small example so here we have our input and we're going to create a linear layer in this case as an input size of four an output size of two and all we're going to do is once we Define it by instantiating it with nn.linear whatever the name of our layer is in this case we called it linear we just essentially apply it with parentheses as if it were a function to whatever input and that actually does the actual forward pass through this linear layer to get our output and so you can see that the original shape was two by three by four then we pass it through this linear layer which has an output dimension of size two and so ultimately our output is two by three by two which is good that's what we expect that's not shape error but you know something common that you'll see is you know maybe uh you decide to you get a little confused and maybe you do let's say two by two you match the wrong dimension and so here we're going to get a shape error and you see that the error message isn't as helpful because it's actually changed the shape of what we were working with we said this was two by three by four under the hood Pi torch has changes to a six by four but if we you know in this case it's obvious because we instantiated it with the shape but if we didn't have the shape then one simple thing we could do is actually just print out the shape and we'd see okay this last Dimension is size four so I actually need to change my input dimension in my linear layer to be size four thank you and you'll also notice on this output we have this grad function and so that's because we're actually Computing and storing the gradients here for our tensor yeah so typically we think of the First Dimension as the batch Dimension so in this case it said n this you can think of as if you had a batch of images it would be the number of images if you had a training Corpus of text it would be essentially the number of sentences or sequences um pretty much that is usually considered the batch Dimension the star indicates that there can be an arbitrary number of dimensions so for instance if we had images this could be a four-dimensional tensor object it could be the batch size by the number of channels by the height by the width but in general there's no fixed number of Dimensions your input tensor can be any number of Dimensions the key is just that that last Dimension needs to match up with the input dimension of your linear layer the two is the output size so essentially we're saying that we're going to map this last Dimension which is four dimensional to now two dimensional so in general you know you can think of this is if we're stacking a neural network this is kind of the input Dimension size and this would be like the hidden Dimension size and so one thing we can do is we can actually print out the parameters and we can actually see what are the values of our linear layer or in general for any layer that we Define in our neural network what are the actual parameters and in this case we see that there's two sets of parameters because we have a bias as well as the actual um the actual linear layer itself and so both of them store the gradients and in this case um you know these are these are what the current values of these parameters are and they'll change as we train the network okay so now let's go through some of the other module layers um so in general nn.linear is one of the layers you have access to you have a couple of other different layers that are pretty common you have 2D convolutions you have transpose convolutions you have batch Norm layers when you need to do normalization in your network you can do up sampling you can do Max pooling you can do lots of different operators but the main key here is that all of them are built-in building blocks that you can just call just like we did with nn.linear and so let's just go I guess I'm running out of time but let's just try and go through these last few layers and then I'll wrap up by kind of showing an example that puts it all together so in this case we can define an activation function which is typical with our networks we need to introduce non-linearities in this case we use the sigmoid function and so now we can Define our our Network as this very simple thing which had one linear layer and then an activation and in general when we compose these layers together we don't need to actually write every single line by line applying the next layer we can actually stack all of them together in this case we can use nn.sequential and list all of the layers so here we have our linear layer followed by our sigmoid and then now we're just essentially passing the input through this whole set of layers all at once so we take our input we call block on the input and we get the output and so let's just kind of see putting it all together what does it look like to define a network and what does it look like when we train one so here we're going to actually Define a multi-layer perceptron and the way it works is to define a neural network you extend the NN dot module class the key here is there's really two main things you have to Define when you create your own network one is the initialization so in the init function you actually initialize all the parameters you need in this case we initialize an input size a hidden size and we actually Define the model itself in this case it's a simple model which consists of a linear layer followed by an activation followed by another linear layer followed by a final activation and the second function we have to Define is the forward which actually does the forward pass of the network and so here our forward function takes in our input X in general it could take in some arbitrary amount of inputs into this function but essentially it needs to figure out how are you actually Computing the output and in this case it's very simple we just pass it into the network that we just defined and return the output and again you could do this more explicitly by kind of doing what we did earlier where we could actually write out all of the layers individually instead of wrapping them into one object and then doing a line by line operation for each one of these layers and so finally if we Define our class it's very simple to use it we can now just instantiate some input instantiate our model by calling multi-layer perceptron with our parameters and then just pass it through our model so that's great but this is all just a full red pass how do we actually train the network how do we actually make it better and so this is the final step which is we have optimization built in to Pi torch so we have this backward function which goes and computes all these gradients in the backward pass and now the only step left is to actually update the parameters using those gradients and so here we'll import the torch.opt-in package which contains all the optimizers that you need essentially this part is just creating some random data so that we can actually decide how to fit our data but this is really the key here which is we'll instantiate our model that we defined we'll Define the atom optimizer um and we'll Define it with a particular learning rate we'll Define a loss function which is again another built-in module in this case we're using the cross entropy loss and finally to calculate our predictions all we do is simply is just call model on our actual input and to calculate our loss we just call our loss function on our predictions and our true labels and we extract the scalar here and now when we put it all together this is what the training Loop looks like we have some number of epochs that we want to train our Network for each of these epochs the first thing we do is we take our Optimizer and we zero out the gradient and the reason we do that is because like many of you noted we actually are accumulating the gradient we're not resetting it every time we call Dot backward so we zero out the gradient we get our model predictions by doing a forward pass we then compute the loss between the predictions and the True Values finally we call law stop backward this is what actually computes all the gradients in the backward pass from our loss and the final step is we call Dot step on our Optimizer in this case we're using atom and this will take a step on our loss function and so if we run this code we end up seeing that we're able to start with some trading loss which is relatively high and in 10 epochs we're able to essentially completely fit our data and if we print out our model parameters and we printed them out from the start as well we'd see that they've changed as we've actually done this optimization so I'll kind of wrap it up here but I think the key takeaway is that a lot of the things that you're doing at the beginning of this class are really about understanding the basics of how neural networks work how you actually Implement them how you implement the backward pass the great thing about Pi torch is that once you get to the very next assignment you'll see that now that you have a good underlying understanding of those things you can abstract a lot of the complexity of how do you do back prop how do you store all these gradients how do you compute them how do you actually run the optimizer and let pytorch handle all of that for you and you can use all of these building blocks all these different neural network layers to now Define your own networks that you can use to solve whatever problems you need 
","['', 'deep learning framework [deep learning framework]', 'tensor [tensor]', 'numpy [numpy]', 'multi-dimensional array [multi-dimensional array]', 'data manipulation [data manipulation]', 'neural network [neural network]', 'gradient computation [gradient computation]', 'optimization [optimization]', 'training loop [training loop]', 'PyTorch vs TensorFlow [PyTorch vs TensorFlow]', 'torch.nn [torch.nn]', 'tensor data type [tensor data type]', 'torch.zeros [torch.zeros]', 'torch.ones [torch.ones]', 'torch.range [torch.range]', 'broadcasting [broadcasting]', 'reshaping tensors [reshaping tensors]', 'vectorized operations [vectorized operations]', 'list indexing [list indexing]', 'autograd [autograd]', 'SGD optimizer [SGD optimizer', '']"
"hi everyone uh welcome to the 224n hugging face Transformers tutorial um so this tutorial is just going to be about using the hugging face Library it's really useful in a super effective way of being able to use kind of some off the shelf NLP models specifically models that are kind of Transformer based and being able to use those for either your final project your custom final project or something like that just using it in the future so these are it's a really helpful package to to learn and it interfaces really well with pi torch in particular too okay so first things first is in case there's anything else that you are missing from this kind of like tutorial the hugging face documentation is really good they also have lots of kind of tutorials and walkthroughs as well as other kind of like notebooks that you can play around with as well so if you're ever wondering about something else that's a really good place to look okay so in the collab the first thing we're going to do uh that I already did but can maybe run again is just installing the Transformers python package and then the data sets python package so this corresponds to that hugging face Transformers and data sets um and so those are really helpful the Transformers is where we'll get a lot of these kind of pre-trained models from and the data sets will give us some helpful data sets that we can potentially use for various tasks so in this case sentiment analysis okay and so we'll use a bit of like a helper function for helping us understand what encoding is uh what encodings are actually happening as well so we'll run this just to kind of kick things off an important a few more a few more things okay so um so first what we'll do is this is generally kind of like the step by step for how to use something off of Hocking face so first what we'll do is we'll find some model um from like the hugging face Hub here and note that there's like a ton of different models that you're able to use there's bird there's gpt2 there's T5 small which is another language model from Google um so there are a bunch of these uh different models that are pre-trained and all of these weights are up here in hugging face that are freely available for for you guys to download so if there's a particular model you're interested in you can probably find a version of it here you can also see kind of different types of models on the side as well that for a specific task so if we wanted to do something like uh zero shot classification there are a couple models that are specifically good at doing that particular task okay so based off of what tasks you're looking for there's probably a hugging face model for it that's available online for you to download okay so that's what we'll do first is we'll go ahead and find a model on the hug and face Hub and then um you know whatever you want to do in this case we'll do sentiment analysis and then there are two things that we need next the first is a tokenizer for actually you know splitting your input text into tokens that your model can use and the actual model itself um and so the tokenizer again kind of converts this to some vocabulary IDs these discrete IDs that your model can actually take in and the model will produce some prediction based off of that okay so um so first what we can do is again import this Auto tokenizer and this Auto model from uh for sequence classification so what this will do initially is download some of the you know key things that we need so that we can actually initialize these so what do each of these do so first the tokenizer this Auto tokenizer is from some pre-trained tokenizer that has already been used so in general there's a corresponding tokenizer for every model that you want to try and use in this case it's like cbert so send like something around sentiment and Roberta and then the second is you can import this model for sequence classification as well from something pre-trained on the model Hub again so again this corresponds to sentiment Roberta large English and if we want we can even find this over here um we can find it as um I think English yeah large English so again this is something we can easily find you just copy this string up here and then you can import that okay we've downloaded all of the kind of all the things that we need some kind of like binary files as well and then now we can go ahead and actually you know use some of these inputs right so this gives you some set of an input right this input string I'm excited to learn about hogging face Transformers we'll get some tokenized inputs here from the actual tokenized things here after we pass it through the tokenizer and then lastly we'll get some notion of the model output that we get right so this is kind of some legits here over whatever classification that we have so in this case good or bad and then some corresponding prediction okay and we'll walk through what this kind of looks like in just a second as well a little more depth but this is broadly kind of like how we can actually use these together we'll tokenize some input and then we'll pass these inputs to the model so we'll talk about tokenizers first so um so tokenizers are used for basically just pre-processing the inputs that you get for any model and it takes some raw string to like um essentially a mapping uh to some number or ID that the model can take in and actually kind of understand so tokenizers are either kind of like are specific to the model that you want to use or you can use the auto tokenizer that will kind of conveniently import whatever corresponding tokenizer you need for that model type um so that's that's kind of like the helpfulness of the auto tokenizer it'll kind of make that selection for you um and make sure that you get the correct tokenizer for whatever model you're using so the question is uh does it make sure that everything is mapped to the correct index that the model is trained on the answer is yes so that's why the auto tokenizer is helpful so there are two types of tokenizers uh there's the a Python tokenizer and there's also like a tokenizer fast that the tokenizer fast is written in Rust in general if you do the auto tokenizer it'll just default to the fast one there's not really a huge difference here it's just about kind of like the inference time for getting the model outputs yeah uh so the question is the tokenizer uh creates dictionaries of the model inputs um so I to think it's more like I think the way to think about a tokenizer is like that um like that dictionary almost right so you want to kind of translate almost or have this mapping from the tokens that you can get from like this string and then map that into kind of some inputs that the model will actually use so we'll see an example of that in just a second so for example we can kind of call the tokenizer in any way that we would for like a typical Pi torch model but we're just going to call it on like a string so here we have our input string is hugging face Transformers is great we pass that into the tokenizer almost like it's like a function right and then we'll get out some tokenization so this gives us a set of input IDs so uh to answer the earlier question these are basically the numbers that each of these tokens represent so that the model can actually use them and then a corresponding attention mask for the particular Transformer okay so there are a couple ways of accessing the actual tokenized input IDs you can treat it like a dictionary so hence kind of thinking about it almost as that dictionary form it's also just like a property of the output that you get so there are two ways of accessing this in like a pretty pythonic way okay so what we can see as well is that we can look at the particular the actual kind of tokenization process almost and so this can maybe give some insight into what happens at each step right so our initial input string is going to be hugging face Transformers is great okay the next step is that we actually want to tokenize these individual kind of uh individual words that are passed in so here this is the kind of output of this tokenization step right we get kind of these individual split tokens we'll convert them to IDs here and then we'll add any special tokens that our model might need for actually performing inference on this okay so there's a couple steps that happen kind of like underneath when you use an actual we use a tokenizer that happens at it a few things at a time one thing to note is that for fast tokenizers as well there's another option that you're able to get to so you have essentially right you have this input string you have the number of tokens that you get you might have some notion of like the special token mask as well right so using Char to word is going to give you like the word piece of a particular character in the input so here this is just giving you additional options that you can use for the fast tokenizer as well for understanding how the tokens are being used um in the from the input string okay uh so there are different ways of using the outputs of these tokenizers too so one is that you know you can pass this in and if you indicate that you want it to return a tensor you can also return a pi torch tensor so that's great um in case you need a pie torch tensor which you probably generally want you can also add multiple tokens into the tokenizer and then pad them as however you need so for here for example we can use the pad token as being this kind of like pad bracket almost and giving the token ID is going to correspond to zero right so it's just going to add padding to whatever input that you give so if you need you need your outputs to be the same length for a particular type of model right this will add those padding tokens and then correspondingly gives you like the zeros in the attention mask where you actually need it okay and so the way to do that here is uh you basically set padding padding to be true you can also set truncation to be true as well and so if you ever have kind of like um more uh any other kind of like features of the tokenizer that you're interested in again you can check the hugging face documentation which is pretty thorough for what each of these things do yeah so the the question is kind of looking at um looking at the the hash hash at least and whether that means that we should have like a space before or not so um so here in this case um yeah so the in this case uh we probably don't want like the space before right just because um we uh have like the hugging like I don't know hugging is all one word um in this case um generally like generally the uh for like the tokenizers generally the output that they give is still pretty consistent though um in terms of how the tokenization process works so there might be kind of these like you know instances where it might be contrary to what you might expect for kind of how something is tokenized um in general the tokenization generally works fine um so in most cases kind of like the direct output that you get from the hugging face tokenizer is sufficient foreign okay awesome so one last thing past the adding kind of additional padding is that you can also kind of uh decode like an entire batch at one one given time so if we um look again we have like uh our tokenizer we'll initially have this method called like a batch decode so if we have like the model inputs that we get up here this is the output of passing these sentences or these strings into the tokenizer we can go ahead and just pass like these input IDs that correspond to that into the batch decode and it'll give us kind of this good this decoding that corresponds to all the padding we added in each of the particular kind of like uh words and strings um and if you want to you know ignore all the the presence of these padding tokens or anything like that um you can also pass that into skipping the special tokens gotcha so this gives like a this is a pretty high level overview of the how you would want to use tokenizers I guess in your um in using hugging face so now we can talk about maybe how to use the hugging face models themselves so again this is this is pretty similar to what we're seeing for something like initially using a tokenizer you just choose the specific model type um for your model and then I and you can use that or the specific kind of Auto model class where again this Auto model kind of takes almost the um like the initialization process it takes care of it for you in a pretty easy way without really any too much overhead um so additionally so um for the pre-trained Transformers that we have they generally have the same underlying architecture but you'll have different kind of heads associated with each Transformer so attention head so you might have to train if you're doing some sequence classification or just some other task so hugging face will do this for you and so for this I I will walk through an example of how to do this for sentiment analysis um so if there's a specific context like sequence classification we want to use we can use like this the very specific kind of like Class A hugging face provides so distilbert for sequence classification alternatively if we were doing it using distilbert in like a mass language model setting we use distilbert for Mast LM and then lastly if we're just doing it purely for the representations that we get out of distilled bird we just use like the Baseline model so the key thing here or key takeaway is that there are some task specific classes that we can use from hugging face to initialize so Auto model again is similar to kind of like the auto tokenizer so for this it's just going to kind of load by default that specific model and so in this case it's going to be just like kind of like the basic basic weights that you need for them okay so um so here we'll have basically three different types of models that we can look at one is like an encoder type model which is Bert a decoder type model like gpt2 that's like uh performing these like uh you know generating some text potentially and encoder decoder models so Bart or T5 in this case so again if you go back to kind of the the hugging face Hub there's a whole sort of different um different types of models that that you could potentially use and if we look in the documentation as well so here we can understand some notion of like the different types of classes that we might want to use right so there's some notion of like the auto tokenizer different Auto models for different types of tasks um so here if again if you have any kind of like specific use cases that you're looking for then you can check the documentation here again if you use like an auto model from pre like pre-trained you'll just create a model that's an instance of that or model in this case root model for the burst Burt based case okay Let's uh we can go ahead and start one last thing to note is that like again the particular choice of your model matches up with kind of the type of architecture that you have to use right so there are different these different types of models can perform specific tasks so you're not going to be able to kind of load or use Bert for instance or distill Bert as like a sequence to sequence model for instance which requires the encoder and decoder because distill distilber I only consists of an encoder so there's a bit of like a limitation on how you can exactly use these but it's basically based on like the model architecture itself okay awesome so let's go ahead and get started here um so similarly here we can import so Auto model for sequence classification so again this is we're going to perform some classification tasks and we'll import this Auto model here so that we don't have to reference again just like something like distilbert for sequence classification we'll be able to load it automatically and it'll be all set alternatively we can do distillburt for sequence classification here and that specifically will will require distilbert speed the input there okay so these are two different ways of basically getting the same model here one using the auto model one using just explicitly distiller cool and here because it's classification we need to specify the number of labels or the number of classes that we're actually going to classify for each of the input sentences okay so here we'll get some like a warning here right if you are following along and you print this out because some of the sequence classification classification parameters aren't trained yet and so we'll go ahead and take care of that so here similarly we'll kind of like walk through how to um how to actually you know train some of these models so the first is how do you actually pass any of the inputs that you get from a tokenizer into the model okay well if we get some model inputs from the tokenizer up here and we pass this into the model by specifying that the input IDs are the input IDs from the model inputs and likewise we want to emphasize or we can you know show here and specifically pass in that the attention mask is going to correspond to the attention mask that we gave from these like these outputs of the tokenizer okay so this is option one where you can specifically identify which property goes to what the second option is using kind of a pythonic hack almost which is where you can directly pass in the model inputs and so this will basically unpack almost the keys of like the model inputs here so the model input keys so the input IDs correspond to this the attention mask corresponds to the attention mask argument so when we use this star star kind of syntax this will go ahead and unpack our dictionary and basically map the arguments to something of the same keys so this is an alternative way of passing it into the model both are going to be the same okay so now what we can do is we can actually print out what the model outputs look like so again these are the inputs the token IDs and the attention mask and then second we'll get the actual model outputs so here notice that the outputs are given by kind of these legits here there's two of them we passed in one example and there's kind of two potential classes that we're trying to classify okay and then lastly we have of course the corresponding distribution over the labels here here right since this is going to be binary classification yes it's like a little bit weird that you're going to have like the two classes for the binary classification task and you could basically just choose to classify one class or not um but we do this just basically because of how hugging face models are are set up um and so uh Additionally you know these are the models that we load in from hugging face are basically just Pi torch modules so like these are the actual models and we can use them in the same way that we've been using models before so that means things like lost dot backward or something like that actually will do this back propagation step corresponding to the loss of like your inputs that you pass in so so it's really easy to train train these guys as long as you have like a label you know label for your data you can calculate your loss using you know the pi torch cross entropy function you get some loss back and then you can go ahead and back propagate it you can actually even get kind of the parameters as well um in the model that you're would probably get updated from this this is just some big tensor of the actual embedding weights that you have okay we also have like a pretty easy way for hugging face itself to be able to to calculate the loss that we get so again if we tokenize some input string we get our model inputs we have two labels positive and negative um and then give some kind of corresponding label that we assign to the the model inputs and we pass this in we can see here that the actual model outputs that's that are given by a hugging face includes this loss here right so it'll include the loss corresponding to that input anyways so it's a really easy way of actually calculating the loss just natively in hugging face without having to call any additional things from a pie torch Library and lastly we can actually even use um if we have kind of like these two labels here again for positive or negative what we can do is just take the model outputs look at the legits and see which one is like the biggest again we'll pass that and take it to the ARG Max so that'll give the index that's largest and then that's the output label that the model is actually predicting so again it gives a really easy way of being able to do this sort of like classification getting the loss getting what the actual labels are just from within hugging face okay awesome so um well last thing as well is that we can also kind of look inside the model um in a pretty pretty cool way and also seeing what the attention weights the model actually puts uh the attention weights the model actually has so this is helpful if you're trying to understand like what's going on inside of some NLP model and so for here we can do again I where we're importing our model from some pre-trained kind of pre-trained model model weights in the um the hugging face Hub we want to Output attention set output attentions to true and output hidden states to true so these are going to be the key arguments that we can use we're actually kind of investigating what's going on inside the model at each point in time again we'll set the model to be in eval mode and lastly we'll go ahead and tokenize our input string again we don't really care about any of the gradients here um again so we don't actually want to back propagate anything here and finally pass in the model inputs so now what we're able to do is when we print out the model hidden States so now this is a new kind of property in the output dictionary that we get we can look at what these actually look like here um and sorry this is a massive output so you can actually look at the hidden State size per layer right and so this kind of gives a notion of what we're going to be looking like looking at like what the shape of this is at each given layer in our model as well as the attention head size per layer so this gives you like it the kind of shape of what you're looking at and then if we actually look at the model output itself we'll get all of these different like hidden States basically right so um so we have like tons and tons of these different hidden States we'll have the last hidden State here so the model output is pretty robust for kind of showing you what the hidden state looks like as well as what attention weights actually look like here so in case you're trying to analyze a particular model this is a really helpful way of doing that so what model.eval does is it sorry question is what is the dot eval do um what it does is it basically sets your and this is true for any PI torch module or model is it sets it into quote-unquote eval mode so again for this like we're not really trying to calculate any of the gradients or anything like that that might correspond to um like correspond to some data that we pass in or try and update our model in any way we just care about evaluating it on that particular data point um so for that then it's helpful to set the model into like eval mode essentially to help make sure that that kind of like disables some of like that stuff that you'd use during training time so it just makes it a little more efficient yeah the question was uh it's already pre-changed so can you go ahead and evaluate it yeah you you can so yeah this is just the raw pre-trained model with no no fine tuning so the question is like how do you interpret um these shapes basically uh for the attention head size and then the hidden State size so um so yeah the the key thing here uh is you'll probably want to look at kind of the shape given on the side it'll correspond to like the layer that you're actually kind of like uh looking at so here um like when we call we looked at the shape here we're specifically looking at like the first first one in this list right so this will give us the first hidden layer all right the second gives us a notion of kind of like the the batch that we're looking at and then the last is like so this is like some tensor right 768 dimensional I don't know representation the corresponds there um and then for the attention head size it corresponds to like the actual query word and the keyword for these last two here but yes so um but for this you know we would expect this kind of initial index here right the one to be bigger if we printed out all of the you know all of the layers but we're just looking at the first one here so we can also do this um for um you know actually being able to get some notion of how these different how this actually like looks um and plot out these axes as well so again if we take this same kind of model input which again is like this hugging face Transformers is great we're actually trying to see like what do these representations look like on like a per layer basis so what we can do here is basically we're looking at for each layer that we have in our model right and again this is purely from the model output attentions or the actual outputs of the model so what we can do is for each layer and then for each head we can analyze essentially like what these representations look like and in particular what the attention weights are across each of like the tokens that we have so this is like a good way of again understanding like what your model is actually attending to within each layer so on the side if we look here maybe zoom in a bit we can see that this is going to be like corresponds to the different layers and the top will correspond to these are across the the different attention heads okay this will just give you some notion of like what the weights are so again just to um to clarify so again if we maybe look at the labels sorry it's like a little cut off and like zoomed out but so this y-axis here like these different rows corresponds to the different layers within the model oops um on the x-axis here right we have like the um like the different attention heads that are present in the model as well and so for each head we are able to for each at each layer to basically get a sense of like what how the attention distribution is actually being distributed what's being attended to corresponding to each of like the tokens that you actually get here so if we if we look up again um here as well right we're just trying to look at like basically the model of tensions that we get for each kind of corresponding layer the question is what's the the color key um yellow is like higher higher magnitude and higher value and then darker is like closer to zero so probably very Navy is like zero so what we can do is now maybe walk through like what a fine-tuning task looks like here um and so first like uh in a project you know you're probably going to want to fine-tune a model um that's fine it's a and we'll go ahead and walk through an example of what that looks like here okay so what we can do as well is all right what we can do as well is use some of the um the data sets that we can get from hugging face as well so it doesn't just have models it has really nice data sets and be able to kind of like load that in as well so here what we're going to be looking at is uh looking at like the IMDb data set um and so here again is for sentiment analysis I will just look at only the first 50 tokens or so um and generally so this is this is like a you know a helper function that we'll use for truncating the output that we get and then lastly for actually kind of making this data set we can use the data set dict class from a hugging face again that will basically give us this smaller data set that we can get for the uh for the train data set as well as specifying what we want for validation as well so here what we're going to do for our like mini data set for the purpose of this demonstration is we'll use I make train and vowel both from the IMDb train data set uh we'll Shuffle it a bit and then we're just going to select here 128 examples and then 32 for validation so it'll Shuffle it around it'll take the first 128 and I'll take the LA the next 32. um and then we'll kind of truncate those particular inputs that we get again just to kind of make sure we're efficient and we can actually run this on a CPU okay so next we can do is just see kind of what does this look like it'll just again this is kind of just like a dictionary it's a wrapper class almost of giving you know your train data set and then your validation data set and in particular you can even look at like what the first 10 of these looks like so first like the output so we specify train we want to look at the first 10 entries in our train data set and the output of this is going to be a dictionary as well which is pretty cool so we have some the first 10 test text examples that give the actual movie reviews here um so this is the given in a list and then the second uh key that you get are the labels corresponding to each of these so whether it's positive or negative so here one is going to be a positive review 0 is negative so it makes it really easy to use this for some something like sentiment sentiment analysis okay so what we can do is go ahead and prepare the data set and put it into batches of 16. okay so what does this look like what we can do is we can call the map function that this like uh this small like data set dictionary has so we call map and pass in a Lambda function of what we want to actually do so here the Lambda function is for each example that we have we want to tokenize the text basically so this is basically saying how do we want to you know pre-process this um and so here we're extracting the tokens input IDs that will pass as a model we adding padding and truncation as well we're going to do this in a batch and then the batch size will be 16. hopefully this makes sense okay so um next we're basically just going to um uh do like a little more modification on what the data set actually looks like so we're going to remove the column that corresponds to text and then we're going to rename the column label to labels so again if we see this this was called label we're just going to call it labels and we're going to remove the text column because we don't really need it anymore we just have gone ahead and pre-processed our data into the input IDs that we need okay and lastly we're going to set it the format to torch so we can go ahead and just pass this in pass this into our model or Pi torch model the question is what is labels so um so label here corresponds to like again the first in the context of sentiment analysis it's like just yeah positive or negative and so here we're just renaming the column okay so now we'll just go ahead and see what this looks like again we're going to look at the train set and only these first two things and so um so here now we have the two labels that correspond to each of the reviews and the input IDs that we get corresponding for each of the reviews as well lastly we also get the attention mask so it's basically just taking the what you get out from the tokenizer and it's just adding this back into the data set so it's really easy to pass in the question is we truncated which makes things easy but how do you want to apply like padding evenly so here if we do pass in so first is like you could either manually set some high truncation limit like we did the second is that you can just go ahead and set padding to be true and then basically like the padding is basically uh added I based off of kind of like the longest um like longest sequence that you have yeah so the question is I guess doing it for all of them all the text lists evenly um so again it just like depends on like the size of like the data set you're you're like you're loading in right so if you're looking at particular batches at a time you can just pad within that particular like batch versus like yeah you don't need to like load all the data set into memory pad the entire data set like or like in the same way so it's fine to do it within just batches yeah the question was how does uh how were the input IDs like added and uh yeah the answer is yes it's basically done automatically um so we had to manually remove the text column here and that kind of like this first line here buy it um like if you recall like the outputs of token like at the tokenizer it's basically just the input IDs and the and the attention mask so it just is smart enough to basically aggregate those together um okay the last thing we're going to do is basically just put these so we have this like data set now um that looks great we're just gonna import like a pytorch data loader typical normal data loader and then go ahead and load each of these data sets that we just had I mean specifying the batch size to be 16. okay so that's fine and great and so now for training the model it's basically like exactly the same as what we would do in typical pytorch so again it's like you still want to compute the loss you can back propagate the loss and everything um yeah so it's it's really up to your own design how you do uh how you do the training um so here there's only like a few kind of asterisks I guess one is that you can import specific kind of optim Optimizer types from the Transformers uh package so you can do atom with weight Decay you can get a linear schedule for like the learning rate which will kind of decrease the learning during the learning rate over time for each training step so again it's basically up to your choice but if you look at the structure of like this code right we load the model for classification we set a number of epochs and then however many training steps we actually want to do we initialize our Optimizer and get some learning rate schedule right and then from there it's basically the same thing as what we would do for a typical kind of like Pi torch model right we set the model to train mode we go ahead and pass in all these batches from like the the data loader and then back propagate step the optimizer and everything like that so it's I pretty pretty similar from what we're kind of like used to seeing essentially awesome so that'll go do its thing at some point um okay and so I so that's one potential option is if you really like pie torch you can just go ahead and do that and it's really nice and easy um the second thing is I that hugging face actually has some sort of like a trainer class that you're able to use that can handle most of most of these things so again if we do the kind of like the same thing here this will actually run one star model is done training um like we can create the our you know our data set in the same way as before now what we can what we need to use is like this import of like a training arguments class so this is going to be basically a dictionary of all the things that we want to use when we're going to actually train our model and then this kind of like additional trainer class which will handle the training kind of like magically for us and kind of wrap around in that way okay anyways so if you can okay I think we're missing a directory but um I think yeah pretty straightforward for how you want to train yeah um so for for here at least again there are kind of the two key arguments the first is training arguments so this will specify have a number of specifications that you can actually pass through to it it's where you want to log things for each kind of like device in this case like we're just using one GPU but potentially if you're using multiple gpus what the batch size is during training or the batch sizes during evaluation time how long you want to train it for how you want to evaluate it so this is kind of like evaluating on an Epoch level what the learning rate is and so on so on so again if you want to check the documentation uh you can see that here there's a bunch of different arguments that you can give there's like warm-up steps warm-up ratio like weight Decay there's like so many things um so again it's basically like a dictionary feel free to kind of like look at these different arguments you can pass in but there's a couple key ones here and this is basically this basically mimics the same arguments that we used before in our like explicit Pi torch method here for hugging face okay similarly um what we do is we can just pass this into the trainer and that will take care of basically everything for us so that whole training Loop that we did before is kind of condensed into this one class function um for actually just doing the training so we pass the model the arguments the train data set eval data set what tokenizer we want to use and then some function for computing metrics so for here we pass in this function uh eval and it takes eval predictions as input basically what this does is these predictions are given from the trainer passed into this function and we just can split it into the actual legits and the labels that are predicted or sorry the ground truth labels that we have and then from here we can just calculate any sort of additional metrics we want like accuracy F1 square recolors and whatever you want okay so this is like an alternative way of formulating that training Loop okay uh the last thing here as well is that we can have some sort of callback as well if you want to do things during the training process so after every Epoch or something like that you want to evaluate your model on the validation set or something like that or just go ahead and like dump some sort of output that's what you can use a callback for and so here this is just a login callback it's just gonna log kind of like that information about the the process itself again not super important but in case that you're looking to try and do any sort of callback during training it's an easy way to add it in the second is if you want to do early stopping as well so early stopping will basically um stop your model early as it sounds if it's not learning anything and a bunch of epochs are going by and so you can set that so that you don't waste kind of like compute time or you can see the results more easily the question is is there a good choice for the patient's value um I think it just depends on the model architecture not really I guess this is It's a yeah pretty up to your discretion okay awesome and so the last thing that we do um is just do call trainer.train so if you recall this is just the instantiation of this trainer class we all trainer.train and it'll just kind of go so now it's training which is great it gives us a nice kind of estimate of how long things are taking what's going on what arguments do we actually pass in um so that's just going to run and then likewise hopefully it'll train relatively quickly okay it'll take two minutes we can also evaluate the model um pretty easily as well so we just called trainer.predict on whatever data set that we're interested in so here it's the tokenized data set corresponding about the validation data set okay hopefully we can pop that out soon um and lastly so if we saved anything to our model checkpoints so hopefully this is um saving stuff right now yeah so this is going to be is continuing to save stuff to the folder that we specified and so here in case we ever want to kind of like load our model again from the weights that we've actually saved we just pass in the name of the checkpoint like the relative path here to our checkpoint so notice how we have some checkpoint 8 here right we just pass in the path to that folder we load it back in be tokenized and it's the same thing as as we did before there are a few kind of additional appendices for how to do like different tasks as well there's appendix on generation how to define a custom data set as well how it's a kind of like pipeline um different kind of like tasks together um so uh so this is kind of like uh using some a pre-trained model that you can just use through kind of like the pipeline interface really easily um there's like in different types of tasks like Mass language modeling but um feel free to look at through those at your own time and uh yeah thanks a bunch 
","['', 'Hugging Face Transformers library is useful for using pre-trained NLP models', 'It is especially useful for custom NLP projects', 'This tutorial covers sentiment analysis using the hugging face library', 'Transformers are pre-trained models available from the Hugging Face Hub', 'A tokenizer is used to convert text into tokens for the model', 'AutoTokenizer can be used to automatically select the correct tokenizer for a model', 'Tokenizers can be used with padding to ensure all inputs are the same length', 'Batching is a technique to process multiple data points at a time', 'Distilbert for sequence classification is a type of model from Hugging Face', 'Attention mask is used to specify which parts of the input the model should focus on', 'Trainer class simplifies the training process', 'Early stopping is a technique to stop training when the model is not improving', 'Checkpoint is a saved version of the model during training', 'Pipelines provide a way to use pre-trained models for specific tasks', 'Mass language modeling is a type of NLP task', '']"
"so today I'm delighted to introduce our first um invited speaker is Dao Aquila um there has also been um as well as being invited and I'll tell his background um he's also um in the symbolic systems program has been an Adjunct professor and has been involved with some students in that role as well but in his invited role he's originally from the Netherlands where he even learned some logic among other things back in the old days but in more recent times he's been um a prominent um deep learning researcher for a number of years he worked at um Facebook now meta in the fair unit and was involved in various ideas including retrieval augmented Generation Um after that he then spent some time at hugging face he's become interested in looking at multimodal models which is what he's going to be talking about today and we welcome Dara it's great to have you thank you very much foreign yes uh yeah thanks everyone for coming I understand that you get points for being here so you're not really here for me but uh uh thanks for coming anyway so I'm going to talk about multimodal deep learning uh it's gonna have an NLP focus of course that's for discourse but it's also because otherwise I would really be talking for uh many more hours than I have time for here so I'll try to really keep it focused on on the things that I think will be most useful for you to learn and so the first thing you should understand is that this whole concept of multimodality is is kind of ill-defined actually um so if you go to the dictionary you'll see that it means having or involving several modes or modalities or Maxima um and and so what mode here really means is so it could be mode in the very generic sense or it could be a very precise sense of the mode of a statistical distribution um and so depending on the paper you're reading in some cases people really mean this statistical sense in other cases people really mean this sort of very vague concept of a modality where it really means the type of information that you're getting so an example of modality in that case is an image or speech signal or audio in general or even affection so smell or or things like that so in this uh lecture we're just going to focus mostly on text because this is an NLP course and we're going to focus on images mostly as the other modality to to keep it simple all right so why does it matter why do we care about multi-modality um and so there are a couple of really good reasons in general for this uh the the first one is is about faithfulness so if you look at how we humans understand the world how we make sense of what happens in the world uh that is very multimodal uh right so we we perceive the world not just using Vision uh or just audio but we synthesize information across all of these different modalities and that's how we understand the world and each other um there's also a very practical uh argument for doing it it's because the internet is multimodal right so if you go to I don't know uh like Facebook or something like that like it rarely happens that it's just text or just an image there's usually a combination of multiple modalities and then the the final good reason uh that we're just starting to hit now if you're if you're really following where the field is going we're kind of running out of Text data for these large language models so uh one interesting way to uh keep scaling on the data side is to make use of all of these other modalities right so if you can have your language model also watch all of the videos of cats in the world it's going to understand the concept of catch cat much better and that's what we want to have in these models we want them to understand the world in the same way that humans understand it um so right now multimodality is really one of the main frontiers of this New Foundation model uh drives that were all in right now there's a thing called the mcgurk effect let's see if it loads up uh but uh so uh what what we'll see when this loads is uh this guy over here and uh we'll have the same audio effect uh being played so the audio is exactly the same and this man is going to say something like and so you're hearing a b there I think if you look at my mouth because that's what I said uh but if you then change the video to where he says with exactly the same audio you're going to hear the other version um so unfortunately I can't really like swap in the different audio here so you have to trust me for it we might suddenly start hearing a guy saying all right um so um uh multimodal applications so when we have multiple modalities we can do all kinds of uh interesting things and as I said most of the use cases we have on the internet they're all multimodal um and there are some really kind of obvious things we would be interested in if we have information from these different data sources right from different modalities uh so obviously we might want to do retrieval so maybe given a bit of text we want to find the right image or maybe given some image we want to find the right text for it so we can match them up obviously we can also do this in a generative setting so then we have image captioning which you've probably heard of we can do text to image generation so that's image synthesis and so stable diffusion everybody in the audience here has probably seen that then we could do a visual question answering where we have an image and text and then we need to generate some new text we have multimodal classification where we have image syntax and we need to have a label for example whether something is hate speech or not and then in general we want to be able to have a richer understanding of information which means that we combine images and text and then use it for Downstream applications that require better understanding or better Generation Um so this field really is super hot right now uh so there's this uh this nice paper title I predict that this paper is going to do really well in terms of citations just because it has such a sideable title I think a lot of people are not actually going to read it and so I mean I've been in this field for quite a while now and people have been saying this for a really long time I think Chris would agree that so for decades people have been saying that multimodal is the next big thing uh but now it's really true I think all right so uh the outline for uh what we're going to be talking about so first I'm going to tell you a little bit about early models then we're going to do a bit of a deep dive on some of the specifics then we're gonna go over a particular type of fusion contrastive models or Lake Fusion then we're gonna go through a little bit of the history of multimodal foundation models then we're going to talk a little bit about evaluation a little bit about other modalities and then I'll make some predictions for the future and hopefully maybe give you some cool research ideas or things to talk or think about all right so um obviously like there's a lot of work that happened before deep learning but I think if you want to start from like the Deep learning Revolution and what was happening in images and text then a good starting point is for example Wasabi uh or device uh or Richard zoker who you who you've probably heard of has done some really cool early work in this that really pioneered a lot of these ideas uh and the basic uh gist of this is that we have a vision model on the one hand we have a language model so this really I mean the first lecture of this course I think was about word embeddings right so that's just your basic word embedding model and now we need to figure out how to align them in the same multimodal space so the way you do that is you get some sort of similarity metric right a score function or like a kernel function if you're thinking about this from a support Vector machine literature perspective and now you need to figure out uh in a Max margin or margin loss um uh how you want to align these two points in your embedding space right so things that are similar you want to bring them closer together things that are not you want to bring them further apart and if you do that in this multimodal embedding space that means that you can do interesting cross-modal transfer where you can take the word embedding for something like Auto or like horse and then you can find close images in the embedding space to that thing and now now you've solved the retrieval problem so this is a really nice early application and I think a lot of the stuff that I'm going to talk about in the in the early slides you're going to see this income over and over again you're going to see it get kind of reinvented with fancier models but it's basically all the same stuff um so you can do cross modal transfer where you have images and text but you can also combine them together so that you get a multimodal word embedding uh and so this uh just gives you a more accurate representation of how humans understand words word meaning because when we think about the word Moon uh or cat or something we can go to Wikipedia and read that a cat is a small carnivorous mammal that people like to keep as pets or we can just go and look at pictures of cats and now we understand what a cat is right and I would argue actually that for a lot of people the picture of the cats is much closer to the meaning of the concept of cat um so uh some some early work where people were trying to do this is from Rooney at all where they did multimodal distribution or semantics using this very uh elegant approach called bag of visual words so just like who has heard of bag of visual words very few people okay so it's it's surprisingly simple and so I kind of like it it's nicely elegant so you take a picture of a moon in this case I think you can see it in the back too right so uh we use an algorithm like sift uh to find interesting key points so it's sort of where the difference between the pixels and the pixels next to it where that difference is Big those are sort of the spots you want to be looking at and for each of these key points you get feature descriptors so relatively small vectors like 32 dimensional events are kind of on the on the implementation of this and what you can do now with these feature descriptors is you can cluster them using k-means and then you assign every one of these points uh so you can count how often they occur right so in this picture of the Moon we have like uh actually the count is oh yeah so there are three like red dots right so that's why the Red Dot one is three so what that gives you is a uh an idea of the visual words very similar to the original bag of words model that you hopefully have heard about maybe in the first lecture um so that's the visual equivalent of the textual thing um and so if you do this and you then concatenate or you apply sud to fuse the information what you get is a word embedding that is much more representative of human meaning so you know as reflected in the data sets that people used to care about at the time so after that there were a couple of people me included who tried to take these ideas and then really applied deep learning to them so some of the very early versions of this use convolutional neural networks uh and then you can transfer the features from uh your your confnet and you take your word embeddings which you've seen in the first lecture uh and then you can concatenate them now you have a multimodal work vector or you can do something slightly fancier so you've seen the skip gram model you can also try to do skip gram predictions onto image features right so when you see your work like cat in some contexts like the cute little cat said on the Met then when you see cat you also want to predict cat pictures so super easy ideas but it turned out that this gives you much richer work representations uh so that's kind of cool but obviously words are very limited what we really care about is not words but sentences so uh then people started really looking into sentence representations and how can we figure out uh how to get compositional understanding in the sentence representations and how to how do we align that with images um so the loss here is very similar to what we saw with works and pictures but now we just have a sentence encoder right um and so there's some really cool early papers from Andre karapati and Richard Soaker also had some work here um and then you know so the basic idea is just that instead of having these word embeddings we now have an lscm in these papers or some other kind of recurrent neural network or in the case of this one recursive neural network and then we try to align the features together um and so so these three or four papers are actually very important than this one by me is less important but it's still kind of interesting because uh we showed here that grounded sentence representation so if you actually just use this part here as a sentence encoder for NLP tasks the ability to just predict pictures from it already gives you a really good sentence representation right so so just by predicting pictures you can sort of imagine what things look like and that gives you a really good meaning representation which you can then transfer to I don't know sentiment classification or something else um and then of course uh once we have census encoders uh or then we also have decoders and and so when the sequence to sequence architecture came out which you've probably also heard about in this course uh what you can do instead of having a text encoder for like your Source language if you're doing machine translation is you can plug in a confnet uh instead of an lstm encoder and now you can generate captions so that's exactly what people did we used to have all of these fancy diagrams in our papers then where we explain the lstm and how that works probably people don't learn that anymore these days they do yeah very good they might make a comeback I think you know at some point uh Transformers are going to go away we'll see um and uh so uh one of the things that that people figured out in machine translation very early on is that you can do alignment of words between your Source language and your target language and you can do the same thing actually with images right so if you want to align a word in your uh in your generated sequence with something in your picture then you can do the same uh use the same approach for that and that approach of course is called attention right so you know you've learned a lot about detention probably in this course and and so yeah that was one of the the building blocks of these systems as well where you can do very interesting things uh and really see that when it has to generate stop for the stop sign that is really actually looking at the stop sign right so there's a really cool alignment going on there um in these models um and so the the final kind of early model we should talk about a little bit uh is Gans uh who here is sort of Gans okay that's a lot a lot more than bag of visual words I guess that makes sense um and uh so so yeah the basic idea of again is really that you have this generator and discriminator and you want to have the generator uh generate images that the discriminator cannot distinguish uh uh from uh so it cannot distinguish fake and real images right and if you do that you can actually condition that on the piece of text uh and then you can generate images uh using some some uh text prompt right so that's what what uh kind of the the first versions of stable diffusion we're doing things like this and you know it's all the a natural progression to that model um so those were the early models um maybe do people have any like burning questions about this or does this all make sense all right so let's do a bit of a deeper dive then on on in particular on features and fusion so those are really the kind of core building blocks for for all of this multimodal stuff really but before we go there maybe very briefly like if all of this multimodal stuff is cool and sort of useful and and doesn't look that difficult you know like why aren't we all doing multimodal things and so why why do we focus on specific modalities and I think there are a couple of problems just to be aware of uh so one is modalities can sometimes dominate especially text is much more dominant than Vision or audio in many use cases right so uh you can already just have a model that picks up on the tech signal and basically learns to ignore the image completely which actually happened embarrassingly for visual question answering we'll get to that so visual question answering you could do that without actually looking at the picture um the additional modalities can add a lot of noise so it makes your machine learning problem more difficult uh you don't always have full coverage right so as I said if you look at Facebook posts sometimes you have text sometimes you have pictures sometimes you have both but you don't have a guarantee that you always have both so how do you deal with that um in many cases we just really weren't ready it was too complicated to implement stuff and also just in general like how to design your model really to uh to combine all the information is actually quite complicated so in order to to uh you know to maybe drive the home that point home a little bit um so featurizing text I guess we all know how to do that by now especially sort of in the age of Transformers and before in lstm sorry we just said like you have your batch by your secrets so batch size by sequence length by embedding size right so it's always like a 3D tensor and that's how you encode your textual information when you pump it through your neural net um and so with images it's like trickier because you can just kind of look at the patches but then if you do convolutions you're kind of like shifting over the image and then you're aggregating right um and in many cases you don't really want to be this uniform you want to have something that actually looks at the things in the picture right so this is called region features where you would use an object detector as a first step for processing your image and then you would have a confident backbone that encodes the features for that particular sub image like this guys like skateboard or something it has its own like vector representation right um and then in terms of dense features we now also have Vision Transformers so we'll just very quickly go over that to make sure we're on the same page so there are all these models like YOLO is a really good one if you haven't heard of that yet uh so we're at YOLO V7 now I think create I don't know uh so there's a new one coming out every every other like year or something but the basic idea is that we get these bounding boxes uh for things in the images right actually segmentations with the bounding boxes is what people tend to use and they they have labels right so this is labeled like Backpacker or something and so you can do this as a pre-processing step on your image to get a much richer representation of what is really in that image which you can then pump into your system as we'll see later and and so then how you encode the information that is in these little bounding boxes or actually in the image itself in general we just use a standard comp net for that and so this probably feels like super obvious now but uh in 2014 when people were starting to discover this it was really very surprising that you could just use off-the-shelf continent features to really replace the entire computer vision pipeline so people used to do all of this very fancy sophisticated stuff and people you know spend decades on trying to refine this and then it was all thrown away and replaced by a confnet that does all of that stuff for free um and so the cool thing you get there is that you can transfer very easily across different tasks so you don't you can have a very generic confidence and then use it to all kinds of very specialized uh things like spotting buildings in Paris for example or flowers or other stuff um and then of course in the age of Transformers um how far how far we're already quite a while and this is only the first Transformer actually uh in the the slide deck so uh you know we're making good progress uh so Vision Transformers are what we would use these days to encode the images uh where you have these flattened patches and then you would do uh kind of the the standard birth architecture maybe as you would know it from this course and then you do classification right so this is all like a standard Transformer everything standards except now your input here is not words or tokens it's patches of an image and then you classify that all right so then we have a bunch of features and now how do we combine the information right so let's say we have two vectors u and v uh so you know it sounds easy right to how how we could could combine them it turns out that they're actually very many ways to combine them so I I don't think it's it's really useful to go over all the different ways here um but you can do very simple things right so obviously like uh inner product or similarity is what you would use if you want to do cross-modal things so if you want to embed things in the same Vector space uh but you can do sort of fancier uh projections on top or different combinations that are kind of linear uh or you can do multiplicative things where you uh multiply the components element wise or you do some sort of gating over the different features you can do attention you can do fancier buy linear things you can do very fancy compact bilinear things so there there's really a wealth of literature kind of on all the different ways you can combine two vectors and and so uh this is called multimodal fusion and most of the literature on multiple modality is essentially about this question what is the best way to do fusion and that's it um so so I think within that discussion it's maybe useful to distinguish between different levels of fusion so you can do it very early where basically you make sure you have the different features and then you just kind of uh in in the sort of modern sense of attention you would attend to everything in all the features from the beginning you can first treat them separately and then combine them or you can treat them as completely separate and then you only combine the final scores right and so there's the so that's kind of what we would call Early fusion and then sort of my my invention for calling the middle part would be sort of middle fusion and then you have late Fusion uh where you really just combine the scores or the logits but you don't really have any interaction between the information from the different modalities um so you could do really fun stuff with multimodal Fusion so this is a paper I really like film um where uh you have this sort of very special uh feature Maps this sort of f here and it gets gets modulated by a multiplicative Factor so this gamma and an additive sort of bias Vector this beta and you have a different one for every layer of a resnet that is conditioned on some encoding of the thing you're after uh so in this case are there more cubes than yellow things so we have some Vector representation for that and we use that Vector representation to modulate the resnet blocks at every layer of the confident um so you know you can really do very fun things where you're sort of modulating one network with the other one and really try to have them learn uh as much as possible from that all right so um let's talk about late Fusion then so late Fusion is what we would Now call contrastive models uh but the basic idea is that we have this similarity score so we have the two kind of we process the modalities completely independently and then at the very end we do some combination uh and the most famous uh instance of that uh these days is clip so who's heard of clip okay so clip uh from openai and so it's again exactly the same contrast of loss that that we've seen in all these early approaches um it does kind of negative sampling uh but then in batch so you just have a batch you have two things that are aligned right so like this the first piece of text and the first image they are aligned so this is the right answer and I just want to make sure that I rank this thing higher than all the alternatives right and I want to make sure I rank this thing higher than all the Alternatives so it's a very very simple idea uh really really nothing special about this architecture that that was sort of invented here but what made this uh thing so cool was first of all it was Transformers and it was Transformers all the way so your text encoder would be a Transformer and your image encoder would be a vit image encoder so also a Transformer um and it was trained on lots and lots of web data so Alex Radford is really a genius at creating very high quality data sets and he he created I think 300 million image text pairs for this data set trained a bigger model on on it than people used to do and then we got this amazing model out of it um and so so uh moving away from the words there to the sort of texts that you would see on the internet right so the caption uh for an image on the web it's not going to say dog or cat it's going to say a photo of a cat doing something something right so uh that that means that you can do kind of zero shot uh label predictions where you have a photo of uh and then you need to figure out what uh the right label is for a given image using this kind of prompt right so the the thing you know you probably all know about prompting large language models and so you can prompt vision and language models in in very much the same way and do zero shot generalization um so if you want a really really good paper I would recommend that you read this paper this is really one that's going to teach you how to write really good papers it's thorough and it's really worth a very close read I think if you're interested in this view um and so I think when it came out uh actually on imagenet itself it it didn't really outperform resnet right so so you might think oh yeah actually it's not all that special but what really made it special was that it generalized much better to these other data sets right so this uh this resnet thing here is pretty terrible at some of these kind of adversarial versions of imagenet and clip is super robust to that so it's just a way better image encoder in general um so uh very very quickly after clip there was this paper from Google uh using a line um which was basically exactly the same idea uh you know the field is not really that creative at all it's like the same idea but then you just keep like throwing more data and more compute at it and it often works much better so that's what they found here too and 1.8 billion image taxpayers instead of 300 million gives you a better model surprise um but uh so it's still very cool and and what is really cool I think is that there's this organization called lion um uh where uh they've they've started this open source Collective to create really high quality data sets um and so the lie on the initial data set um was uh how many examples in the initial lineup 400 million right he knows I know that he knows um and uh so so now there's a much bigger version of lion uh that's even multilingual and it has five billion examples right so uh stable diffusion was trained on sort of the image the English subset of this thing uh and that's one of the reasons that it's so awesome it's because it's just seen a ton of data uh and that really makes your system a lot better so if you're looking for like the ultimate data set to play around with uh with your own ideas if you have enough compute obviously then you should really look at this data set all right any questions about up until this point nope all right um so that then we'll we'll move on from late Fusion to kind of middle Fusion early Fusion uh and this really is kind of the core uh of what I think a lot of people in the field right now or if you're interested in getting in this field or if you're going to go into industry and you're going to be using this stuff like this is what you should really understand and and again like the idea is sort of Stack onto each other so I've kind of uh sequenced the slides to give you an idea sort of of how the scientists kind of came up with the next step uh and you can really see the architecture just get slightly more and more advanced but basically a lot of it is just more data and more compute uh again um so uh who knows how bird works everybody should raise their heads so um uh yeah so so Bert is kind of so canonical I think everybody kind of gets out Burke works right so I don't think we need a real refresher uh but uh I think you can think and so the reason I have to slide is because I want you to think about if you have a bird model and you have a bunch of images how are you going to turn that Bird model into something multimodal right so so there are a bunch of like obvious things you could do given the kind of features I told you about in the sort of fusion process so you know how are you going to do that does anybody want to like say something like if you're doing classification and then just concatenate it to whatever encoder like maybe an a n or whatever you're training on the data concatenating okay exactly yeah so so you can take take the confnet features and classifier token from bird concatenate them and then classify uh for like a catheter or something like that or whatever the thing is you're interested in yeah yeah so that's one thing you could also like take the confident features and like give them to the bird model in lots of different ways right uh we can use the region features so um and I think a lot of people uh when Burke came out who were working in in vision and language processing were thinking exactly about okay so do we do like middle Fusion late Fusion do we do early Fusion how do we do diffusion um and so there were a lot of papers all coming out basically at around the same time where people were doing versions uh of this because so Bert was really kind of the Innovation and then everybody sort of just plugged it into their own thing because of hugging face Transformers and things like that so um the first thing is uh visual bird um this was one of the very early ones where you have this image and people would do uh object detection on this so you get like a hat and a racket and a shirt and things like that so you can just really take these features and then plug them into your uh your Transformer model and then you you try to like recover the features and so this really is probably like the simplest way to do it right um and so this is what we call a single stream architecture where you have all of these kind of concatenating the the original input features and then putting them through the same Transformer what you can also do and that's something that this this uh model called vilbert did is where you have two different streams so you essentially have these two parallel Transformers but at every layer uh you kind of give them a cross attention right so or co-attention as they call it but it's basically like so you just make sure you have an attention map that spends both and then you just do your full normal Transformer layer again um and then so this you can train just like your regular Bird right so you uh you have your your masked model Mass language model here and here you do sort of some equivalent of that and then you also have your next sentence prediction which you probably remember from your birth lecture um but instead here we're saying okay is this image aligned with this piece of text or not um there's also lexmart I mean there I could go on forever there are like 100 papers that that came out that did this all at the same time so Lexmark had a different cross-modal output encoder a bunch of different uh ways of encoding the positional information right so you could say okay I just have a bunch of bounding boxes that are featureized but I don't care about where they are in the image so it's just kind of like a just a bag of uh bounding boxes or you could say I found it here like this is the particular like top left and and bottom right coordinate and that's what you featurize into your network um you can also do something even dumber and I can say that because this is my paper um where you just take the the image itself you put it through a resnet and then you uh do a little bit of pooling on the final feature maps and you just get give those feature Maps too Bert um and so you then need to distinguish between like your text segment embeddings right and your vision segment embeddings um but so this actually works surprisingly well you don't have to do any uh any additional training you can just take Bert out of the box initially you freeze it you learn to project into bird token space then you unfreeze your resnet and then finally you unfreeze your birth and now you have a very good multimodal classifier on the problem you care about so a lot of these other papers they're doing what they call multimodal pre-training where first you have a bird model and a resnet so they're kind of unimodally pre-trained and then you couple them together and then you have a multimodal sort of intermediate every pre-training step before you fine tune it on the problem you care about uh and what we showed here is that you don't really need that actually in many cases so it's a very strong Baseline um you can also go to the the pixel level completely so so that's what they they did in this other paper called pixel bird where they it's basically exactly mmbt uh so the the previous uh supervised one but here they do do the the multimodal pre-training step and show that I think for vqa it helps a little bit um so there are many of these birds uh doing sort of visual things uh people really tried everything uh here's another one called unider where they they added a bunch of different losses uh we can really talk about this for a very long time uh We're not gonna do that I'm just gonna kind of talk you through some of the more interesting ones so this one I think is quite interesting built because here this is really the first instance where we are completely gone from uh confident features so we don't do any any pre-processing on the image no regime features no backbone that it featurizes the uh the the parts of the image we care about we just have these patches of the image so really integrate we flattened those patches we just pump them into the Transformer straight away so this really is like sort of burnt and vit together in one model and this worked really very well um so that that's been the trend uh so here's a here's a nice uh very long list of all those all of these different models and what they do and and so really the distinctions are just in what is the text encoder that you use so do you use birth or something fancier or better Roberta uh what is your vision encoder so in many cases you have these region features so you would do an rcnn style thing or you could just do a resnet or a vit you have different kinds of fusion so either single or dual stream as we talked about right so visual birth or vilbert different pre-training tasks so Mass language modeling image text matching there's a bunch of like funkier ones you can do so and then finally you can do multimodal pre-training on all of these different data sets that have aligned data um So you you're probably wondering okay so what is what is really the interesting difference between a lot of these and uh so I have another recommended paper that if you're interested in this space you should really take a look at it's also a really well done paper where uh they uh they unmask multimodal pre-training so basically they say if you take all of these little model inventions and you train these different models on exactly the same data in exactly the same way it turns out that they're all basically the same uh so that's a lot of kind of uh you know wasted effort on the part of the field because everybody is saying like oh my model is better but it's actually just because you trained it on different data and there's no real sort of model Innovation uh going on in a lot of these things so I don't mean to sound discouraging or anything like that but you know like uh I think that's why this paper is really nice and really important is because it just shows us what really matters so this is also work that I I did myself uh called Flava uh with uh with my team where we wanted to take these ideas really to the limit so a lot of the things that you've seen now uh so the visual Birds and the vilberts and things like that they're all about multimodal questions so how can we do visual question answering uh something like that where we just have these two modalities we only care about problems that always involve these two modalities and where we want to go and this is this is kind of the basic premise I think of foundation models in general is that we have one model to rule them all right so this one model can consume data from all of these different modalities and you can synthesize across all of these different modalities and then do useful things with that information um so so with flavor that's exactly what we tried to build so we wanted to have one Foundation model that is good at vision and language and computer vision and natural language processing is jointly pre-trained on all of these different data sources so it's also trained on just CC news so common all and book Corpus so it's very good at the sort of things you would expect Bert to be good at it's drained on imagenet for image data so it's good at the things that you would expect is a kind of basic image model to be good at and then you have this PMD data set that we created out of publicly available uh image text pairs that we also train it on so this PMD data set is really just if you take all the data sets that were ever created that have image text pairs that are publicly available so unfortunately the clip data and the Google Align data and all of these data sets they haven't been open source so this is before rely on uh where so now there's a good alternative to this um but so this PMD data set if you combine all of these image taxpayers you get 70 million of them so that's still pretty decent size and then you can take all of this data basically to solve all of these problems that we know we care about in these different fields so you can do multimodal reasoning you can do language understanding you can do visual recognition all with exactly the same model and that's that's a very powerful idea I think if you uh like if you work at a company like Facebook you don't want to have different models for all kinds of different things you want to have one model that you can really use for everything that's going to really make your life a lot easier um so the exact architecture here is that on the one hand we have this image encoder where we take the image we encoded as patches and we just do what we call mass image modeling but it's basically Mass language modeling and then just on the on the image tokens right uh and then on the other side we have the mass language modeling uh on on the language so your regular sort of bird thing and then we have a multimodal part where all of this information gets combined uh so we have a mass multimodal modeling loss term uh where you can also do image text matching so this is like your bird next sentence prediction thing and then we also have a global contrastive loss which is exactly like a clip so if you do all of this stuff it's just all Transformers all the way down and it's sort of a very elegant way I think to combine a lot of this information um and when you do that you get something that can really do a lot of things very well uh so I'm we're not going to talk about that table is just way too many numbers but uh so just trust me we were pretty thorough generating uh the table here and so over 35 different tests if you compare flavor to all kinds of different ablations in terms of clip models then this is just a much better way to to get to this information so I think this is a nice example of like where we're probably going to go with the field uh in in the near future um so the other Trend that we that we see very obviously in the field right now is that everybody cares about generative models right so you know language models and and you know image generative models there's just a trend where we want to be generative we want to move away from this contrastive discriminative uh stuff to the more interesting more richer representations maybe that you get out of generating sequences or or images um so this uh Sim vlm paper was one of the first ones where they really had this separate decoder that was trying to generate or kind of complete captions which they showed gives you a lot richer representations um I think this is actually the current state of the art now it's called coca so a lot of these uh models they all again look very similar uh but in this case now we're starting to really see these text decoders so initially with clip I think that's also what they were trying to go for like open AI being a company that really likes generative models but they couldn't really get it to work and I think so it took us a while as a field to really figure out how to do this the right way um and so right now we're really kind of in the age of language models right and uh so uh one of the interesting things you can do with language models is just keep them Frozen and then learn how to project into the language models so uh the mmbt architecture I talked about where we had this bird model we kind of kept it frozen and we learned to learn to project into the bird token space you can do exactly the same thing but then with a much fancier model uh or something like T5 even where you just have an encoder decoder or some kind of generative part of this you keep that thing Frozen uh and then you learn to project into the token space of that Frozen language model uh and then you can do lots of fun stuff it turns out so what they show in this paper is that you then get few shot Learners uh so all of the things you see with gpt3 where you can just give it some kind of in-context examples and it's gonna figure out binding uh kind of on the fly so it says like this is a accent this is a blicket so what is what is this and then it gives you the answer that is that it's the decks so it really learns in context how you decide the feature mappings which is really kind of solving the the grounding problem that a lot of this multimodal stuff uh started with so I think that's very cool and then uh probably one of the the coolest papers right now or models right now that you might have heard of if you follow the field is flamingo uh out of deepmind where they take a chinchilla language model um and uh so this is really an optimal language model and now you have this Vision encoder uh that encodes multiple different images uh that you can then do reasoning over and then kind of autocomplete so what this gets you is just a much more powerful model because you can do uh you know your generative over lots of different images so like it's really like step wise you can see it right we started off with very simple Transformers and now we're actually at something that that is starting to get pretty complicated because we have these building blocks like a perceiver resampler where we have a a bunch of different images that we featureize and now we need to compress the information because sometimes we have three images sometimes we have five images so we want to make sure that we can compress it so that it's always ready for consumption and by the next layer of the language model and then so this paper again is a really good paper to read because they actually so this is not me this is not my code this comes from the actual paper so they just have the diagram together with the code so that you can really understand what it's doing which I think is is really uh great um and so once you you have your perceiver resampling step what you then do is you do a gated cross attention this is how you implement it um and uh so this gated cross attention you do that before your Frozen language model layer so you really just have a frozen chinchilla language model and you learn to kind of modulate the information that goes into that language model you propagate the gradients all the way back you just don't update the language model so you're really kind of trying to figure out like how am I going to design my signal so that my language model can can do the most with it right how am I going to combine the information so you'll notice that now we do it before the layer right in a lot of other stuff you would do the attention after the layer but here you do it before um so uh karpathy I think more than 10 years ago had this this image it's Barack Obama kind of setting his foot here on the scale to make somebody think uh like uh you know they're they're a lot heavier than they really are uh so this is obviously funny to us um but not to an AI system I think unless it really uh understands the scene and so that's why karpathy uh at the time said this would be a really good visual Turing test like if a system can figure this out then it's actually really smart um and so obviously it's been a bit of a challenge for everybody working in the field than to get something that actually works on this and uh so Flamingo as it turns out kind of gets the joke um but uh yeah so it's it's a bit unclear if it really gets the joke because if you read this conversation it's sort of kind of getting steered in the right direction right but um at least we're making progress let's put it that way um and then so in Flamingo you still have a lot of moving Parts but you can really take this almost through the full extreme where you try to freeze almost everything and you just want to learn this kind of mapping between your image encoder and your language model or your image encoder and your encoder decoder architecture and all you really do is just the projection between the two right so there's this nice model called blip 2 where they they experiment with like opt for the language model in Flint T5 for the encoder decoder architecture and this just gives you amazing results it gives you uh really complex captions and things like that without any real direct supervision on the captions itself which is pretty impressive I think so that just shows you the power of language models in general um so here are some examples uh so it can really do like different things from captioning to reasoning to visual question answering to like like location detection uh so you can have a long conversation with this system this really is is kind of the future where we're going right where we're going to have a chat GPT but it's also going to be able to see the world in a way um and so so I think an interesting thing so you've probably heard of like Chain of Thought prompting and things like that where you ask the language model like let's think step by step um and you can tell a vision and language model uh generate a rationale for why uh why something might be the case so you generate a potential explanation for what your answer might be and then after that you ask it to answer the question and it turns out that if you do that sort of multimodal Chain of Thought prompting then the system gets much better uh and and so you know this was like the new state-of-the-art on science QA or Benchmark like that just because it learns to unpack the information right and so uh I think we're really as a field just starting to figure out what what the potential is of this and I think this paper is where they also showed that multimodal Chain of Thought prompting really gets you pretty amazing results and they show uh very nice results on Raven matrices and like very complicated kind of IQ tests the things that that humans are supposed to be really good at but you have to be a pretty smart human to really be good at this and this system Just Nails it um so you know we're making super fast progress and we started off from a very simple birth model that was able to look at some pictures and now we're getting to these very sophisticated Foundation models so that was my my short history of multimodal foundation models um so how much time do I have left all right okay plenty of time um yeah please questions one of the images they just looked like they were um boxes passed through and kind of no sense of shape in them yeah yeah so so I I think the the history of computer vision has been very similar to the history of natural language processing where we thought we needed all of this structure and all of these different things and it turns out you can just throw it all away and just have a big Transformer over the patches sorry yes [Laughter] you mentioned a couple times like Model T person or what does that mean yeah uh yeah sorry I should have explained that better maybe so it just means that um we are not updating the weights um so uh like if we go to uh this era I think is a nice example so uh we have frozen self-attention so that just means that we when we do a forward pass we go all the way to whatever we want to predict we get some gradients we take them all the way down but we only update the non-frozen layers right so here the gradients actually do get updated but these just never change and so the reason you want to do that is because otherwise you're going to drift way too far right so then you're going to kind of destroy all of the cool stuff your language model has learned because you're just going to focus on on the small data set that you're training it on so you want to preserve the abilities of the language model but you want it to become good at the thing you care about other questions is there a benefit to doing like that really your Metal Fusion as opposed to like only doing leg Fusion I think yeah so so I mean we're going to talk about evaluation next but so it really depends on the the tasks that you care about um and so I would say the earlier is always the better if you can afford it uh and so like clip is very efficient to train it's very late Fusion right at the very end so there's no interaction between the different modalities um and so that's really good if you want to be very efficient and if you want to be like for training it's it's much nicer right uh but if you want to have a richer understanding of the the multimodal signal then you want to do earlier Fusion so it's yeah it's always a trade-off right images are just a lot more data than text so how much more difficult are these to train and um how much bigger does like the image processing have to be compared to uh the language model yeah so so um images are are more complex in a way but but they're also kind of higher bandwidth representations right so there's a lot of kind of like just pixels that our brains just abstract away right it's really about the scene that you're seeing and like you're not really thinking too much about the pixels themselves um so so like John Lagoon likes to say that uh language is just a kind of low bandwidth uh a proxy for a language of thought which is much richer and much higher bandwidth and like he thinks probably visual I'm not so sure um but uh so uh yeah I I don't think that there's necessarily a difference between kind of the scaling laws that you see in these systems um or at least we still have to figure that out we'll kind of talk about that towards the end as well so how to put your bias just like the natural one oh yeah they have terrible biases yeah um so yeah so some people are actually working on on this uh who are in this very room but uh so these models can be very racist also in what they generate or or the kind of predictions they make so uh if you have an Asian basketball player standing sort of like this with a basketball very obviously there then the model will think that he's playing ping pong because he's Asian I'm not joking so uh so so these models uh yeah just like all neural networks right this is really a big problem and one of the the most interesting problems that that you should be working on if you're a student and you want to make a difference is how do we get these systems to be much better at these sorts of things probably examples you show like the model interpret from the content of the image so really we want to understand the content for a video so what actual challenges you might see like what improvements we can make uh to um yeah so so um you're asking about the attention Mass sort of right yeah so you can use the same idea for for videos uh and you just look at the video and and so these systems are are so good now the object detectors are so good you can really track objects kind of real time as they they go through your video and so you can try to check how that aligns with your attention mask in your model um so so a lot of uh like so videos I think are sort of interesting but they're also not really interesting because you can very often just sub-sample images and solve the images rather than having to deal with the complex video um good job all right maybe one more one more question and then we'll go do some evaluations yeah so these multi-mole models when um you only provide let's say you only provide a single source of media to the same only text or vision how does it perform in that case because it's obviously more geared for multi-million cases yeah so I mean that's one of the giant shortcomings of a lot of these models is that they're really just built for multimodal stuff and so what if I don't have an image right uh and so uh I mean that that's why we did Flavor because we want to have one model that can do all of that stuff um and that's why in in mmbt so the supervised multimodal by Transformer we actually have an analysis of like how robust is this model to missing images or missing text uh but uh so I think a lot of a lot of folks working on these early visual bird models that were kind of myopically focused on vqa uh which is actually a great segue to what I want to talk about next uh so so it really depends on the the tasks that you care about as I said right and so I I think if I'm gonna tell you about multimodality I also have to tell you how you're going to check that the multimodal system is actually good at multimodal things and so that's the the topic of evaluation which actually is a super important topic and a lot of people they want to be cool and build big models uh but I I think it should be way cooler to do proper evaluation of these models especially if you're in Academia because you only have limited gpus anyway right so what what what can you do sorry I don't want to rub it in with it no um so um so how do you check well um there's this amazing project uh so like imagenet really changed like the history of deep learning I think and this other data set Coco I think also really changed especially vision and language but also I think Vision uh in general where they uh have just a bunch of main sort of multimodal tasks so these images are very richly annotated with all kinds of different things so like the segmentation of the objects the bounding boxes the labels of the boundary boxes they come with like uh sort of a different pixel granularities it's a huge data set uh it's very fine-grained uh annotated in terms of like the categories that it has and then you have five captions for each of these images um and so this this really was the first data set that unlocked a lot of sort of vision and language processing at scale because you had your picture and you had your caption and now you need to figure out okay how do I give the right caption for this image so that's image captioning or can I retrieve given some piece of text the right image or the piece of uh or the image for the piece of text so there's a bunch of very impactful data sets that do this stuff that we already talked about lion but Coco really is the main one still I think that a lot of people kind of use as the canal core instance of this data set category and then the other thing that people really care about in vision and language processing is visual question answering um and so the there really are a bunch of academic groups who are or have been so focused on this task that they didn't really care about anything else and that's why you see a lot of models that are really optimized just for multimodal and nothing else uh and you can see that kind of reflected in the citation counts as of last night 3 A.M um where uh so the vqh just has way more citations than uh image captioning data sets even right and so what you do here is you just have an image and then people ask very simple questions so annotators right they they ask these simple questions they give the answers and now we want to be able to answer these questions with machines and as I alluded to earlier one of the the kind of embarrassing uh backstories of this data set was that the initial version of the data set was actually found to uh to have images not really matter at all so you could just look at the question then it could have something like how many slices of pizza are there um and so well not in that particular case but in almost all of the data set the right answer for how much or how many questions was too so if you just predicted two to every how much or how many questions you got like 70 accuracy on the accounting category so careful data set uh or evaluation Benchmark design is also really a skill and you really need to think about what you're doing you can't just like set some data aside and evaluate it on it you have to really think about what you're doing um and so there's gqa by by Chris actually which is also just a I I think a a better designed version of this data set maybe so you might want to use that uh these days um they're also kind of um a very targeted data sets that really try to measure one particular thing and I think one of the things we really want to get at with these models is what we would call compositionality right so we want to be able to really take the parts and and reason about the whole and understand the relationships between the different concepts so clever was a very clever data set uh that was designed really to to measure the the compositionality both on the language side and on the vision side so you have to understand the relationships between all of these different objects in the images uh so that's been a pretty impactful data set I think for really uh forcing people to think about compositionality but a lot of these data sets uh really had big problems uh so so one of the problem is you know uh they were too easy uh so vqa is sort of like plateauing out we can talk about that a little bit too it wasn't really realistic so you could solve vqa and that's probably going to make some people's lives better you're all like trying to process the means how I can see everything okay let's get to the memes first then so um so uh obviously so these memes are not actually in the data set um so I could put some really hateful memes about sort of Hitler or something which are in the data set but that would be less fun um so uh these are mean meme examples to kind of uh demonstrate uh how the data set was constructed and and so one of the problems we had as I said like vqa the V didn't really matter what we want to have is a data set if we're if we care about multimodality specifically it's like how do we get a data set that you can only get right if you are good at multimodal reasoning and otherwise you're just going to screw it up um and so this is what we came up with is if you have a meme like this one love the way you smell today I mean that's not very nice if you send this to your friend right um so um but uh so it turns out that that if you just swap out the background now it's a very nice thing to say right uh and like this one is you know I don't know you're maybe a bit weird if you like this but uh there's there's nothing wrong with it right um and so it's the same for this one here like look how many people love you with the Tumbleweed that's really sad and like you know if you if you if you change just one word suddenly it's like a really nice thing to say right um so so if you want to solve this if you want to classify this correctly for the meanness then you have to really understand multimodal reasoning you have to understand the relationship between the image and the text in order to get to the right label right and so it was really constructed by Design to do that um and uh so how we did it exactly is we we use some really uh highly trained annotators and then uh one of the big problems with a lot of these data sets is that uh nobody really knows who owns the meme for example right so somebody makes this meme now they technically own a copyright and so when I made this data set I was working at the Facebook and they were very afraid of copyright things so what we actually had to do is uh we had to pay people to make new memes um and and so so not from stretch so we could show them kind of the actual examples and then they had to try to find images that were were uh kind of corresponding to the original Source image and tried to recreate the meme but now with an image that we could buy from Getty um and and so we gave a lot of money to Getty uh so that we could then release the data set uh to the public so that people could do actually research on this and understand for their multimodal models whether they're good or not um and so we really tried to make it so that we had these benign co-founder benign confounders uh sorry um it's a startup world with co-founders um so um so the co-founder here is obviously that you have your original Meme and then you have uh your confounder where you swap out one of the modalities in here you have the other one right so we had our annotators do that as well uh and so this led to a really nice data set I think uh because it showed some of the intuitions that I think a lot of people under field had which is that multimodal pre-training doesn't really work is that an alarm um so multimodal pre-training doesn't really work uh and so all of this stuff that people have been doing with all their fancy visual work models actually turned out maybe to not really be that useful anyway and so maybe it got you like one point extra right from visual birth to like a different visual birth like less than a point uh just just by doing that multimodal pre-training um so that means like we still have to figure this stuff out right this this data set is far from Salt and we we still have a long way to go despite all of these fancy models and and you know a new paper coming out every week that does something new like we're not there yet um and I think that's encouraging uh especially for you like when you uh you can go out and solve it um so um what we did with this data set is we organized the competition we had 100K in price money uh to try to see what people could come up with um and so there there was a lot of nice work coming out of that and we've really kind of managed to to crank the numbers up by quite a lot um but the the solutions were slightly disappointing so I don't know if you've ever used kaggle but if you want to really win on kaggle you just have to Ensemble the hell out of all of the different models that are in the current state of the art and then you're very likely to win right and so that's that's what happened here um where you know there wasn't really the fundamental breakthrough we had maybe been hoping for so uh that still uh needs to be built I think um so this other data set I just want to kind of briefly talk about so so the theme sort of of this section is like if you make a data set think about it very carefully uh because you can really be very creative with this and really really measure the things you're trying to get at so um this this data set winner ground we were trying to figure out okay how good is clip actually so it looks really amazing and it's way better than things that were previously there but does it understand compositional relationships in the same way that humans would understand it or is it sort of just fitting onto the data distribution and it can be very good at the head of the distribution but it's terrible at detail and you can probably already guess where this is going um but uh so so just to give you an illustration of what is in this data set you would have some plants surrounding a light bulb or you would have eight light bulb surrounding some plants so notice that the words here are exactly the same words but in a different order right so uh and and so the visual depiction of these words is very very different so if your model your contrastive model is actually good at understanding the Visio semantic or the yeah visual linguistic compositionality uh of these these uh these uh examples then then you can get it right but again if it's actually just overfitting on the data distribution that is seen and it just kind of is biased toward what it sees often then it doesn't really get it right and so one paper uh that we use as a source of inspiration for this work is uh this paper here Order word matters pre-training for little uh so we actually found that the order of words doesn't even matter that much for General pre-training very often uh which is also kind of a scary thing right so this is deep learning for NLP we think that you know language is really important but these models can can reason about language even if you shuffle all the words um and so that's that's probably not what we want to have and so that that doesn't tell you something about how great we are as researchers it tells you something about how terrible our evaluation benchmarks are right and that's what we need to fix um so so what we did with this data set here some other nice examples like there's a mug in some grass or there's some grass in a mug like these are very different pictures right and so for us these are trivial like so like you know what's the difference between a truck fire and a fire truck they're pretty pretty important I think also to get that distinction right um so um guess what um state-of-the-art models often perform below random chance so uh uh you know as I said we still have a lot of work to do which is good um and and so when this paper came out that I I think the the reaction was was really nice and uh so when dolly two came out um which so you've probably heard of Dolly too right so it's sort of like stable diffusion but then before stable diffusion um and so this was really the the first model that really showed like just how impressive these generative models can be uh when they're when they're creating images so this is there's a mug in some grass uh you do have to kind of cheat a little bit because you have to add digital art here uh if you if you don't add that then it breaks down completely right uh so it's sort of prone attacking I think or sort of tuning on the test set but okay you know um so this is pretty good right so so it's definitely is better than I think a lot of people would have expected even a couple of years ago um but it's not perfect because uh people on the internet like to take more pictures of spoons than Forks um so if you say there are fewer uh spoons than Forks or there are fewer Forks than spoons it just really like spoons more um you know and so maybe it's like the Matrix or something I don't know but so uh spoons are just nicer so uh so again what you can see here is that these models really are just reflections of the data that they're trained on right um and uh yeah so models are getting better but if you've looked at stable diffusion like it still can't count fingers and things like that right so again uh there's still a lot of a cool work to be done any questions on evaluation no okay so let's let's talk about other modalities then because so we've really just been focused on images and images are great there are lots of images uh on the internet and and so that makes it sort of an obvious thing to focus on it's also I think if you look at our brain like vision is a very dominant modality right so how we understand the world is very Vision driven uh but it that it that doesn't have to be the case so there's all these other interesting problems that involve different modalities and so the most obvious one is just speech or audio right so after after CN comes hearing um and and really we could do another lecture just like this just on speech and audio and there's lots of interesting stuff to talk about obviously we don't have time but uh I'll give you another uh nice example of how Amazing Alec Radford is at creating data sets um so so there's this whisper model that came out of open AI not too long ago which was trained on 680 000 hours of multilingual multitask uh Speech data so speech with transcriptions um and they they trained this very fancy uh thing on there which actually is not very fancy at all it's just the long male spectrogram so how you represent the audio signal and then you feed that into a big Transformer so this is sort of your encoder self-attention here right and then you have your decoder where you have your cross attention and then you just generate the sequence so this is encoder decoder basic Transformer model but your input is uh convolutions one-dimensional convolutions over the log mail spectrogram and so there's lots of papers that do very similar things uh there's there's models like wave to VEC that try to turn the wave signal into vectors or you can discretize it in lots of different ways um so there's a wealth of literature then I think one of the funny observations actually is that you can just reduce audio to Vision anyway right so so that's what you could sort of argue this log mail spectrogram does but so not to toot my own horn but in 27 I I did this paper where we showed that you can just take a real audio sample turn it into a a kind of a spectrogram really just a spectrogram so what does the spectrum of the the audio file look like feed that to a regular content like an Alex net even and then that gives you amazing auditory features so now you can use this to distinguish between violins or guitars and things like that so you know maybe you can just reduce all of this to Vision so one question maybe you could ask is that can we also reduce language division or Vision to language you know that could so that's sort of what people are thinking about today um so we talked about the video there was a question about video so a lot of these ideas also extend pretty directly to video but now you just have more data right so like Flamingo already had a bunch of different images in it you can do Flamingo over videos probably a lot of the images are pretty useless for what you're trying to do with this video model right so they're they're too similar it doesn't really add all that much information so you want to sub-sample the frames so that you get the most useful information out of your video uh and so there's a bunch of approaches that that kind of take the keyframes and then you just do a standard joint vision and language Transformer encoder thing on top of that so this is kind of becoming hopefully by now a very familiar recipe right um and so there's this so Merlot is a nice architecture that does this and then they came up with Merlot Reserve kind of a silly name where they also added audio to this model so this is now a tri-modal model right and so you know for going towards this Foundation model that can consume all of these different modalities uh all in one go and that's really like a clear Trend in the field um another very interesting Direction I think where in the field we were very excited about this for a while but we I think it's it's sort of uh gone now because it's too difficult to create lots of high quality data in this setting but what you can do is you can have simulated environments uh so this is a paper from deepmind from 2017 where they had this agent walk around in the Maze and then it could have natural language instructions they could also generalize to like decks and Blicks and different sort of groundings to the and assignments that that you could do in that environment so this is a super interesting Direction I think in the long term because this is how humans learn language right like we walk around in the world we interact with our environments we have all of these different perceptual observations we synthesize them in our brain we manipulate objects we change our own Viewpoint and that's how we learn everything we know about the world and so our our language is very intricately connected to that world and how we observe it um so I think that that might make a comeback at some point in the future um you can also do other stuff so especially with this kind of conditioning or text that that we're seeing a lot of right so like so you know Dali 2 and stable diffusion and all of these different things and the original again we talked about at the beginning you can do the same thing but now you're generating 3D Point clouds right so this is a 3D Corgi um using a corgi and so this this prompt can probably become much more complex over time and you can do like sort of AutoCAD design and just say like give me a house and it's just going to design the whole house for you uh so you can just like tweak The Prompt and things like that like that that's all coming or or even already here in many cases um so so the final modality I I just briefly wanted to talk about is uh olfactory embeddings sure um and uh so olfaction means smell if you didn't know um and uh so it turns out so my PhD thesis was about grounding uh semantics in uh different perceptual modalities so a lot of my work started in vision and then it's like okay now audio is sort of the obvious next one right so you can learn the meaning of violin and then maybe you can learn that violin like what a violin looks like and what it is and what it sounds like and that's going to give you a richer representation but for a lot of these words well it's actually very primitive to their meaning is what they smell like because uh in our brains that's really one of the core areas of one of the oldest areas in your brain uh so uh what you can try to do if you want to complete all of your perceptual modalities is you can try to build olfactory embedding so it was kind of a joke paper I did but um the funny thing is it actually worked um so uh there's there's a catalog this Sigma Aldrich fine flavors and fragrances catalog where you can look up words like melon and pineapple and then it's going to give you all of the chemical compounds that produce this smell or taste and so if you do that then you can count the occurrences and then you can sort of do SVD or something like that only to to get it to be a bit more of a real embedding model so now you get smell embeddings smell vectors and then you can compute similarity judgments between these these smell so turns out Apple smells like pear uh and you know the chocolate and cocoa and sweet and coffee are sort of related right so you get these clusters of different smells just based off of their chemical compounds so this bag of chemical compounds model gives you a very rich representation and so if you look at all of the words that are concrete enough to have smell right so like if you have a word like democracy in there that doesn't really smell like anything right so you you ignore democracy and you just focus on on the things that smell um or that good smell I guess um and then so the the really interesting thing to me is that you know this is is much more correlated with human similarity judgments than the linguistic vectors we had at the time right so so for a work like apple like you can just get a word Vector like you've learned in your first lecture uh and and so you can do like you know Skip gram and things like that but that that thing is not going to be as correlated with human similarity judgments as this bag of chemical compounds model so that's that's pretty interesting right so even something like smell where maybe we think you know this doesn't really matter if you really want to understand how humans understand language then maybe you want to include this in your foundation model too and but I would start with other modalities all right um okay yeah sorry yeah uh so where to next uh I'll just I think I've already said most of this actually so One Foundation model is going to rule them all um and we'll so I mean there will be many of these but a lot of them are going to have very similar traits I think um we're going to be looking at scaling loss and trying to understand really what is the relationship between the different modalities which one do we want more of that sort of stuff we're going to have retrieval augmentation this thing is going to be really huge if you've heard of rag or if you haven't you should look it up uh so all of these parts of these models can also be multimodal we need way better evaluation and better measurements we already talked about that too and that's all I have thank you [Applause] 
","['', 'Multimodal deep learning', 'NLP', 'Images', 'Text', 'Understanding the world', 'MCGurk effect', 'Audiovisual integration', 'Retrieval', 'Image captioning', 'Text generation', 'Visual question answering', 'Multimodal classification', 'Downstream applications', 'Foundation models', 'Contrastive models', 'Lake Fusion', 'Early multimodal models', 'Image encoders', 'Text encoders', '']"
"today I'm delighted to introduce us our final guest speaker um Bean Kim um being Kim is a staff research scientist at Google brain if you're really into googleology you know those funny words the beginning like staff sort of says how senior you are um and that means that being's a good research scientist um um so uh I I discovered at lunch today that bean started out um studying mechanical engineering at Seoul national university but she moved on to uh I don't know if it's better things or not but she moved on to computer science and did a PhD um at MIT and there she started working on the interpretability and explainability of machine learning models um I think she'll be talking about some different parts of her work but a theme that she's had in some of her recent work that I find especially appealing as an NLP person is the idea that we should be using higher level human interpretable languages for communication between people and machines and so welcome Bean looking forward to your talk um and go for it thank you thank you thanks for having me it's honored to be here it's the rainiest Stanford I've ever seen last night I got here last night but then I'm I live in Seattle so this is pretty common so I still was able to see the blue sky today I was like this works I really like it here so today I'm going to share some of my dreams chasing my dreams to communicate with machines so if you're in this class you probably agree you don't have to that large language models and generated models are pretty cool they're impressive but you may also agree that they're a little bit frightening not just because they're impressive they're doing really good job but also we're not quite sure where we're going with this technology in 10 years out will we look back and say that technology was net positive or we will say ah that was catastrophic we didn't know that that would happen and ultimately what I would like to do or maybe hopefully what we all want to do is to have this technology benefit us humans I know in 10 years time or maybe well 20 years or earlier he's gonna ask me he's gonna be like Mom did you work on this AI stuff I watched some of your talks and did you know that how this will profoundly change our lives and what did you do about that and I have to answer that question and I really hope that I have some good things to say to him so my initial thought or an instill so or current thought is that if we want our ultimate goal to be benefit Humanity why not directly optimize for it why wait so how can we benefit there's lots of different ways we can benefit but one way we can benefit is to treat this like a colleague you know a colleague who are really good at something it's called it's not perfect but it's good at something enough that you want to learn something from them one difference is though in this case is that this colleague is kind of weird this colleague might have very different values it might has very different experiences in the world it may not care about surviving as much as we do maybe mortality isn't really a thing for this colleague so you have to navigate that in our conversation so what do you do when you first meet somebody there's someone so different what do you do you try to have a conversation to figure out what how do you do what you do how are you solving decades-old protein folding problem how are you so how are you beating the world gold champion so easily What It Seems are you using the same language the science knowledge the language that we use atoms molecules or do you think about the world in a very different way and more importantly how can we work together I have a one area that I really want to talk to and it's alphago so alphago beat world of gold Champion Isador in 2016. Isidore is from South Korea I'm from South Korea I watched every single batch it was such a big deal in South Korea and worldwide I hope and in one of the matches alphago played this move called move 37. how many people watched alphago match matches and how many people remember move 37. yeah a few people right and I remember the nine Don commentator who's been like talking a lot throughout the matches suddenly got really quiet and he said hmm that's a very strange move and I knew then that something really interesting has just happened in from my eyes that this is gonna change something the South Fargo has made something that we're gonna remember forever and sure enough this move turned around the game for alphago and leading Alpha go to win one of the matches so go players today continue to analyze this move and still discuss people talk about this is not the move a human would Phantom so the question is how did alphago know this is a good move my dream is to learn something new by communicating the machine with machines and having a conversation and such that Humanity will gain some new angle to our important problems like medicine and Science and many others and this is not just about discovering new things if you think about reward hacking you have to have a meaningful conversation with somebody to truly figure out what their true goal is so in a way solving this problem is a superset of solving AI safety too so how do we have this conversation conversation assumes that we share some common vocabulary between uh that that exchange to exchange meaning and ultimately the knowledge and naturally a representation plays a key role in this conversation on the left and we can visualize this on the left we say what this is a representational space of what humans know on the right what machines know here in left Circle there will be something like this dog is Fluffy and you know what that means because we all share somewhat similar recovery but on the right we have something like move 37 where we humans yet to have a representation for so how do we have this conversation our representation space needs overlap and the more overlap we have the better conversation we're going to have humans are all good at learning new things like here everyone is learning something new so we can expand what we know by learning new Concepts and vocabularies and doing so I believe will help us to build machines that can better align with our values and our goals so this is the talk that I gave if you're curious about some of the work we're doing towards this direction I highly recommend it's a YouTube video I clear keen on half an hour you can fast uh do a best feed but today I'm going to talk more about my hopes and dreams and hopefully at the end of the day your hopes and dream is there so first of all I'm just gonna set the expectation so at the end of this talk we still don't know how the move 37 is made okay sorry that's going to take a while in fact the first part of this talk is going to be about how we move backwards in this progress in in terms of making this progress in our journey and still very very small portion of our entire Journey towards understanding move 37 and of course this journey wouldn't be like a singular path there will be lots of different branches coming in core ideas like Transformer helped many domains across they will be similar here so I'm going to talk in the part two some of our work on understanding emerging behaviors in reinforcement learning and all the techniques that I'm going to talk about is going to be in principle applicable to NLP so coming back to our move our dreams and hopes and dreams 37. so let's first think about how we might realize this dream and taking a step back we have to ask do we have tools to First estimate what even machines know there has been many development in machine learning last decade now to develop tools to understand and estimate this purple circle so is that accurate unfortunately many Recent research showed that there's a huge gap between What machines actually know and what we think the machines know and identifying and bridging this Gap is important because these tools will form basis for understanding that move 37. so what are these tools how many people familiar with sale in cmaps a lot but you don't have to explain what it is so saliency map is one of the popular interpretability methods for Simplicity let's say an imagenet you have an image like this you have a bird the explanation is going to take a form of the same image but where each pixel is numb with associated with a number that is supposed to imply some importance of that pixel for prediction of this image and one definition of that importance is that that number indicates how the function looked like around this pixel so for example if I have a pixel I XJ maybe around XJ the function moves up like the yellow curve or function is flat or a function is going down like the green curve and so if it's flat like a like a blue curve or red curve maybe that feature is irrelevant to predicting bird maybe it's going up then it's maybe more important because the value of X increases and the function value goes up function value here like a prediction value so let's think about what are the few ways why this Gap might exist there are fewer is not exhaustive they're overlap a little bit but helpful for us to think about maybe assumptions are wrong so this alien again these machines that we train Works in a completely different perhaps completely different representational space very different experiences about the world so assuming that it sees the world that we do just like we do like having the gesture phenomenon there's few dots humans are have tendency to connect them maybe machines have the two maybe not so maybe our assumptions about these machines were wrong maybe our expectations are mismatched we thought it was doing X but it was actually doing y or maybe it's beyond us maybe it's showing something superhuman that humans just can't understand I'm going to take a deeper into one of uh some of these our work this is more recent work so again coming back to the earlier story about Salient cement we're going to play with some of these methods now uh in 2018 we stumbled upon this phenomenon that was quite shocking which was that we were actually trying to write some different people again people of Christians here but we were testing something and we realized that train Network and untrained network has the same very similar as alien cmap in other words random prediction and meaningful prediction were giving me the same explanation so that was puzzling we thought we had a bug but it turned out we didn't it actually is in this thing indistinguishable qualitatively and quantitatively so that was shocking but then we wondered maybe this one-off case maybe it still works somehow in practice so we tested that in a follow-up paper okay what if the model had an error one of these errors maybe it has a labeling error maybe it has a spheres correlation maybe that had Auto distribution at test time if we intentionally insert these bugs can explanation tell us that there's something wrong with the model it turns out that that's also not quite true you might think that oh maybe superior's correlation another follow-up work also showed that this is also not the case so we were disappointed but then still we say you know maybe there is a there's no theoretical proof of this maybe this is again a lab setting test we had grad students to test this system maybe there's still some hope so this is more recent work where we theoretically prove that some of these methods very popular methods cannot do better than random so I'm going to talk a little a little bit about that I'm missing one person oh I'm missing Tang way in the author list I just realized this is also work with pangwei so let's first talk about our expectation what is our expectation about this tool now the original paper that developed this method IG and Shop talks about how IG can be used for accounting the contributions of each feature so what that means is that when the Tool assigns zero attribution to a pixel we're gonna say okay well pixel is on used by the function and that means that F will be insensitive if I perturb this X and in fact this is how it's been used it in practice this is a paper published in nature they use the shop to figure out the eligibility criteria in a medical trial what we show in this work is that none of these inferences that seemed pretty natural were true and in fact just because popular attribution methods tell you anything about it attribution is X you cannot conclude anything about the actual Model Behavior so how does that work how many people here do Theory proof or few great I'll tell you I I learned about Theory proving from this project as well so I'll tell you like the way that that we pursued this particular work is that first think about this problem and we're going to formulate into some other problem that we know how to solve so in this case we formulate this as hypothesis testing because once you formulate in the hypothesis testing yes or no there are lots of tools in statistics you can use to prove this so what is hypothesis the hypothesis is that I'm a user I got an attribution value from one of these tools and I have a mental model of ah this feature is important or maybe not important then the hypothesis is that whether that's true or not and what we showed is that given whatever hypothesis you may have we can you cannot do better than random guessing invalidating or invalidating this hypothesis testing and that means yes it's sometimes it's right but you don't do hypothesis testing if you cannot validate yes or no you just don't because like what's the point of doing it if you just don't know if it's as good as random guessing right and the result is that yes for for this for this graph it's just a visualization of our results if you plot true negative and true positive and line is random guessing because this is the worst method that's the best method all the equal distance is this line methods that we know shop in IG all all falls under this line of random guessing that's bad news but maybe maybe this still works in practice for some reason maybe there were some assumptions that we had that didn't quite meet in the practice so does this phenomenal hold in practice the answer is yes we did we now have more image graphs and more bigger models but here we test two concrete and tasks that people care about in interpretability or use these methods to do recourse or spiritual correlation so recourse for those are not familiaries you're getting a loan and you wonder whether if I'm older I would have a high chance of getting a loan so I tweak this one feature and see if my value goes up or down very reasonable task have people do all the time pretty significant implications socially so for two of these concrete and tasks both of them boil down to this hypothesis testing framework that I talked about they're all around the random guessing line over worse than random guessing so you might say oh no this is not good A lot of people are using these tools what do we do we have very simple idea about this so people like developing complex tools and I really hope you're not one of those people because a lot of times simple methods work who comes Razer but also simple methods are elegant there's a reason perhaps a lot of times why they work they're simple that you can understand them they make sense so let's try that idea here so again your goal is to estimate a function shape what do you do well the simplest thing you do is you have a point of interest you sample around that point and evaluate the function around that point if it goes up maybe functions going up if it goes down maybe functions coming down right so that's the simplest way you can kind of brute force it but then the question is how many samples do we need so here this is the equation that you're boosting you're lifting this line upwards that way by adding that additional term uh it's proportional to number of samples the more samples you have the better estimation you have makes sense and differences in output how much resolution do you care do you care point zero point one to point point one to point two or do you only care zero slope to like slope one that's resolution that you care about and number of features of course so if you worry about making some conclusion based on function shape sample easy so can we infer the Model Behavior using this popular methods the answer is no and this holds both theory and practice we're currently working on even bigger models to to show just like again you know again empirical evidence that yes it just really doesn't work please you know think of think twice and three times before using these methods and also a model dependent sample complexity if your function is kind of crazy of course you're going to need more samples so what is the definition of how do we characterize these functions and finally we haven't quite given up yet because these methods have a pretty good root in economics and and sharply values and all that so maybe they're a lot narrower condition where these methods work and we believe such such condition does exist we just have to figure out when once we figure out what that condition is then in given function I can test it and say yes I can use shop here yes I can use IG here or no I can't that would be still very useful so ongoing work before I go to the next one any questions yes do the findings you have about the these models like does it only applied in computer-bit models or does it applies any model and that has a function yeah very simple simple actually simplest proof that can show simply any function this holds any other questions it's wonderful yeah this relate to you a lot but like it's almost seems like for the last couple of years they're being at least dozens maybe hundreds of people writing to people through the Shipley values I mean it if you're guessed that most of that work that's invalid or that a lot of it might be okay because the the point of a condition where it's all right right off of you being there so two answers to that question my hypothesis testing results shows that it's random right so maybe in the optimistic case optimistic case 50 of those papers you hit it and on the other side on the second note even if maybe shop wasn't perfect maybe it was kind of wrong but even if if it helped human at the end task whatever that might be Health doctors to be more efficient identifying bugs and whatnot and if they did the validation correctly with the right control testing setup then I think it's good you know you figure it out somehow how to make this noisy tools together work with human interlude maybe and that's also good and I personally really like shop uh paper and I'm a I'm a good friend with Scott and I love all his work it's just that I think we need to narrow down our expectations so that our expectations are better aligned all right I'm going to talk about another word that's a kind of similar flavor now it's an NLP so this is one of those papers just like the many other papers that that we we ended up writing one of those Serendipity paper so initially Peter came up as an intern and we thought we're gonna locate ethical knowledge in this large language models and then maybe we're gonna edit them to make them a little more ethical so that was a goal and then we thought oh the wrong paper from David Bowie and I also love David's work and let's use that so that's the start of this work but then we start digging into and implementing the realm and like things didn't quite line up so we do like sanity check experiment after sanity check and we ended up writing completely different paper which I'm going to about to talk to you about so the this paper the Rome for those who are not familiar which I'm going into detail a little more detail in a bit is about editing a model so you first locate a knowledge in a in a model like the Space Needle is in Seattle that's a factor knowledge you locate them you edit them because you can locate them you can mess with it to edit that fact that's like the whole promise of it in fact that's a lot of times how localization or editing methods were motivated in their literature but what we show is that this assumption is actually not true and to be quite honest with you like I still don't quite get why this is not related and I'll talk more about this because this is like a big question uh to us this is a pretty pretty um active work so substantial fraction of factual knowledge is stored outside of layers that are identified as having no knowledge and you can you can you can see you can you will see this a little more detail in a bit in fact the correlation between where the location where where the facts are located and how well you will edit if we edit that location is completely correlated uncorrelated so they have nothing to do with each other so we thought well maybe it's the problem with the definition of editing what we mean by editing can mean a lot of different things so let's think about different ways to edit a thing so we try a bunch of things with a little success we couldn't find an editing definition that actually relates really well with localization methods like in particular with ROM so let's talk a little bit about Rome how Rome Works super briefly there's a lot of details missed out on this side but roughly you will get the idea so Rome is Magneto 2022 uh they have what's called causal tracing algorithm and the way it works is that you're going to run a model on this particular data set now counter effect data set that has this Tuple subject relation and object the space needle look is located in Seattle and so you're going to have a clean run of the Space Needle is in Seattle one time you stole every single module every single value activations and then in the second run which they call corrupted run you're going to add noise in those Space Needle is or or the space then then you're going to intervene at every single one of those modules as if from by copying this module to the corrupted run so as if that particular model was never interrupted never a noise was never added to that module so it's a typical like intervention case where you pretend everything else being equal if I change just this one module what is the probability of having the right answer so in this case probability of the right answer Seattle given that I know it's the model and I intervened on it so at the end of the day you'll find graph like that where each layer and each token has a score How likely it is if I intervene on that token in that layer how How likely is it that I will recover the right answer because if I recover right answer that's the model that's the module that's stored on knowledge really reasonable algorithm I couldn't find technical flow in this algorithm I quite like it actually so but but when we start looking at this using the same model that they use GPT gptj we realized that a lot of these facts so so Rome uses just layer 6 to edit because that was the supposedly the best layer across this data set to add in most of the factual knowledge is stored in layer 6 and they showed uh editing success and whatnot but we realized the truth looks like the graph on the right so the red line is the layer 6 their extension paper called memet and it's multiple layers that's the Blue Line blue region the black bars are histogram of where the knowledge was actually peaked if you test every single layer and as you can see not a lot of facts fall into that region so in fact every single fact has like different regions that where it peaked so layer six for a lot of facts weren't the best layer what the editing really worked it really works and we did we were able to duplicate that results so we thought what do we do to find this ethics ethical knowledge how do we find the best layer to edit so that's where we started but then we thought you know what take a step back we're going to actually do alternative check first to make sure that tracing effect the the tracing effect is the localization rip implies better editing results and that's when everything started to falling apart so let's define some metrics first the edit success this is the rewrite score same score as roam paper used that's what we use and the tracing effect this is localization is probably you can beat the due to the slide so when we plotted the relation between tracing effect and rewrite score the local uh the the editing method Redline applies the perfect correlation and that was our assumption that there will be perfectly correlated and which is why we do localization to begin with the actual line was yellow it's close to zero it's actually negative in this particular data set that is not even on correlated it's like anti-correlated and we didn't stop there we were like we were so puzzled we're gonna do this for every single layer and we're gonna find R square value so how much of the choice of layer versus the localization the tracing effect explains the variance of successful edit if you're not familiar with r squared r squares like a think about it as an importance of a factor and it turns out that layer takes 94 dressing effect is zero zero one six and so we were really opposed that we were like scratching our head why is this true but it was true across layer we tried all sorts of different things we we tried different model we tried different data set it was all like roughly the case so we were at this point we contacted David and we start talking about and and we resolve them they acknowledge that this is a phenomenon that that exists you know so apart from the layer the other way in which localization can happen is are you looking at the correct token is that the other like corresponding yeah yeah in this graph the token so the added benefit of the rest of the localization could only help you look at which is the correct subgroup token is that it yeah yeah and so looking at any of the software tokens it sort of finds what I should think of yeah yeah just layer layer is the most biggest thing that's the only thing you should care if you care about editing layers in fact don't worry about localization at all it's extra wasted carbon uh climate effect yeah so that was that was our conclusion but then we thought you know maybe the particular definition of edit that they used in the room was was maybe different maybe maybe there's exists a definition of editing that correlates a lot better with localization because there must be I'm still puzzled why is this not correlated so we tried a bunch of different definitions of edits you might inject an error you might uh you might invert reverse the tracing you might want to erase effect you might we might want to amplify the fact all these things like maybe one of these will work you did it so the craft that you're seeing down here is R square value for four different methods and this wasn't just the case for Roman memory it was also the case for fine tuning methods that you want to look at the difference between blue and orange bar represents how much the tracing effect influenced our Square value of the tracing effect as you can see it's ignorable they're all the same you might feel the effect forcing the last one has a little bit of Hope but still compared to the impact of layer choice of layer it's ignorable so at this point we said okay well we can't locate the ethics no ethical knowledge at this project we're going to have to switch the direction and we end up doing a lot more in-depth analysis on on this so in summary does localization help editing no the relationship is actually zero for this particular editing method that from what I know is pretty state-of-the-art and the counter of counter effect data it's not true are there any other editing methods that correlate better no but if somebody can answer this question for me that will be very satisfying like I feel like there should start something still be something there that we're missing but causal tracing I think what it does is it reveals the factual information when the Transformer is passing forward I think it represents where's the fact when you're doing that but what we found here is that it has nothing to do with editing success those two things are different and we have to resolve that somehow but a lot of insights that they found in their paper is still useful like the early to mid-range NLP representation last token there they represent the factual something we didn't know before but it is important not to validate localization methods using the editing method now we know and maybe not to motivate editing methods using via localization those are the two things now we know that we shouldn't do because we couldn't find a relationship any questions on this one before I move on to the next one you're not shocked by this I am shocked by this I'm still so puzzled like it should be there should be something I don't know all right so in summary of this first part we talked about why there the Gap might exist and what she what machines know versus what we think machines now there are three hypothesis there are three ideas assumptions are wrong maybe our expectations are wrong maybe it's beyond us there's a good quote that says good at good artist still I think good researchers doubt we have to be really suspicious of everything that we do and that's maybe the biggest lesson that I've learned over many years that once you like your results so much that's a bad sign like come back like go home have a beer go to sleep and next day you come back and like put your paper in on your desk and think okay now I'm gonna review this paper how do I criticize this one what do I not like about this paper right that's the one way to look at criticize your own research and and that will improve your thinking a lot so let's bring our attention back to our hopes and dreams it keeps coming back so here I came to realize maybe instead of just building tools to understand perhaps we need to do some groundwork what do I mean well this alien that we've been dealing with trying to generate explanations seems to be a different kind so maybe we should study them as if they're like new species in the wild so what do you do when you observe a new species in the wild you have a couple ways but one of the ways is to observational study so you saw some species in the wild far away first you just kind of watch them you watch them and see what are they like what are their habitat how they what what do they what are their values and whatnot and second way you can actually intervene and do a control study so we did something like this with reinforcement learning setup I'm going to talk about these two papers first paper emergent behaviors in multi-agent systems has been so cool who who saw this hide and seek video by open AI yeah it's so cool if you haven't seen it just Google it and watch it it's so fascinating I'm only covering the tip of an iceberg in this but at the end of this hide and seek episode at some point the agents reveal a discover a bug in this physical system and start like anti-gravity flying in the air and like shooting hiders everywhere a super interesting video you must watch so lots of that and also humanoid football and capture the flag from deepmind lots of interesting behaviors emerging that we observed here's the my favorite one but but these labels so here these are labels that are provided by open AI running and chasing for building and ramp use but and these ones were that oh human or humans when painstakingly one by one watch all these videos and label them manually so our question is can we is there better way to discover this emergent behaviors perhaps some nice visualization can help us explore this complex uh complex domain a little better so that's our goal so in this work we're going to again treat the agents like an observational study like us new species then we're going to do observational study and what that means is that we only get to observe State and action pair so where they are what are they doing or uh yeah what are they doing and we're going to discover agent Behavior by basically kind of like a clustering the data that's all we're gonna do and how do we do it pretty simple a generative model have you have covered the Bayesian generator graphical no gotcha okay so think about hi then also what you teach yeah so this is a graphical model um think about this as a fake or hypothetical data generation process so how does this work like I'm generating the data I created this system I'm going to first generate a joint latent embedding space for that represents all numbers that represents all the behaviors in the system and then I'm gonna for each agent I'm going to generate another embedding and each embedding when it's conditioned with State it's going to generate policy it's going to decide what it's going to do what action is given the state and the embedding pair and then what that whole thing generates is what you see the state and action pair so how does this work well and then given this you build a model and you do inference to learn all these parameters kind of same business as neural network but it's just have a little more structure so this is completely made up right this is like my idea of how these new species might work and our goal is to we're going to try this and see if anything useful comes up and the way you do this is one of the ways you do this is you optimize for a variation lower bound you don't need to know that it's very cool actually if if one gets into this exponential family business uh it's very cool CS 228 okay so here's one of the results that we had it's a domain called mujoko here we're going to pretend that we have two agents one controlling back leg and one controlling the front leg and on the right we're showing that joint embedding space Z Omega and z alpha while video is running I'm going to try to put the video back okay so now I'm going to select this is a visualization that we built or online you can you can go check it out you can select a little space in agent one space and you see it maps to pretty tight space and Agent Zero and it shows pretty decent running ability so that's cool and now I'm going to select somewhere else in agent one that maps to kind of disperse area in Agent Zero it looks like it's not not doing as well and this is just an Insight that we gain for this data only but like I was quickly able to identify ah this type mapping business kind of represents the good running behavior and bad running behaviors that's something that you can do pretty efficiently and now I'm going to show you something more interesting so of course we have to do this because we have the data it's it's here it's so cool so we apply this framework in the when ai's hide and seek this has four agent it looks like a simple game but it has pretty complex structure 100 dimensional observations uh five-dimensional action space so in this work remember that we pretend that we don't know the labels given by open AI we just shuffle them in the mix but we can color them our results with respect to their labels so again this is the result of Z Omega and z alpha the individual agents but the coloring is something that we didn't know before we just did it after the fact you can see in the Z Omega there's nice kind of pattern that we can roughly separate what human what makes sense to humans and what makes sense to us but remember the the green and gray kind of everywhere they're mixed so in this particular run of open AIS hide and seek it seemed that those two representations were kind of entangled the running and chasing the blue dots it seems to be pretty separate and distinguishable from all the other colors and that kind of makes sense because that's basis of playing this game so if you don't have that representation you have a you have a big trouble but in case of like orange which is fort building it's a lot more distinguishable in hiders and that makes sense because hiders are the ones building the fort then Seekers don't build the fort so we're in just a little more entangled in Seekers perhaps if Seekers had built more separate for building uh representation maybe they would have win this game so this work can we learn something interesting emerging behaviors by just simply observing the system the answer seems to be yes at least for the domains that we tested a lot more more complex domains should be tested but these are the ones we had but remember that these methods don't give you names of these clusters so you would have to go and investigate and click through and explore and if the cluster represents super superhuman concept this is not going to help you and I'll talk a little more about the work that that we do try to help them but this is not for you this is not going to help you there and also if you have access to the model and the reward signal you should use it why why dump it so next part we do use it I'm going to talk about let's work with Nico and Natasha and Shay again so here this time we're going to intervene we're going to be a little intrusive but hopefully we'll learn a little more so problem is that we're going to build a new multi-agent system we're going to build it from scratch such that we can do control testing but at the same time we shouldn't sacrifice the performance so we're going to try to match the the performance of the overall system and we do succeed I had this paper collaboration with Folks at Sanford actually here in 2020 where we propose this pretty simple idea which is you have on your own network why don't we embed Concepts in the middle of the bottleneck where one neuron represents three the other represents stripes and just train the model end to end and why are we doing this well because then at inference time you can actually intervene you can pretend you know predicting zebra I don't think three should matter so I'm gonna zero out this neuron and feed forward and see what happens so it's particularly useful in the medical setting where there are some features that doctors don't want we can cancel on and test so this is the work to extend this to RL setting it's actually not as simple extension then as we thought it came out to be pretty complex but essentially we're doing that and we're building each of the concept bottleneck for each agent and at the end of the day what you optimize is what you usually do typical PPO just think about this as make the make daughter system work plus minimizing the difference between the true concept and estimated concept that's all you do why are we doing this you can intervene you can pretend now agent 2 pretend that you can't see agent 1. what happens now that's what we're doing here we're going to do this in two domains first domain how many people looked at the uh saw this cooking game before yeah it's a it's a pretty commonly used cooking uh domain in reinforcement learning very simple we have two agents yellow and blue and they're going to make soup they can bring Three Tomatoes they get a war they wait for the tomato and bring the dishes a dish to the cooking pot they get a reward finally their goal is to deliver as many soups as possible given given some time and here Concepts that we use are agent position orientation agent has tomato it has Dish etc etc something that's immediately available to you already and you can of course tweak the environment to make it more fun so you can make it that they have to collaborate like you can build a wall between them so that they have to work together in order to serve any tomato soup or you can make them freely available you can work independently or together whatever your choice first uh just kind of send you the check was that you can you can detect the emerging behavior of coordination versus non-coordination so when the impassable environment when we made up that environment and suppose that RL system that we trained worked they were able to deliver some soups then you see that when we intervene uh this graph let me explain this is a reward of an agent one when we when there's no intervention so this is perfectly good world and when there was an intervention this is the average value of intervening on all Concepts but I'm also going to show you each concept soon if you compare left and right you can tell that in the right when we intervene reward deteriorated quite a lot for both of them and that's one way to see yeah they are coordinating because somehow intervening and at this concept impacted a lot of their performance but this is what what uh what was really interesting to me and I'm curious anyone can guess so this is the same graph as the one you saw before but except I'm plotting for intervention for each concept so I'm intervening team position team orientation team has tomato etc etc it turns out that they are using or rather when we intervene on team orientation the degradation of performance was the biggest to the extent that we believe that orientation had to do with subcoordination does anyone can guess why this might be the position there's orientation yes just a clarification question on the orientation is that like the direction that the teammate is producing yes so it seems like orientation would let you yes yes that's right yes where were you when I was when I was pulling my hair hair over this question yes that's exactly right and initially I was really puzzled like why not position because I expect it to be positioned but exactly that's exactly right so the orientation is the first signal that an agent can get about the next move over the other Asian because they're facing the pot they're going to the pot they're facing the Tomato they're going to get the tomato really interesting intuition but some too obvious to some but I needed this graph to work that out and of course you can use this to identify lazy agents if you look at the rightmost uh yellow agent our friend just chilling in the in the background and he's lazy and if you train our religion there's always some agents just hanging out they just not do anything and you can you can easily identify this by using this graph if I intervene it it just doesn't impact any any of their Rewards so the second domain we're going to look at a little more complex domain so this is uh it's studying inter-agent social dynamics so in this domain there is a little bit of tension this is called a cleanup we have four agents they only get rewards if they eat apples just yellow things or green things or apples uh but if you don't clean the river then Apple stops through all so somebody has to clean the river and you can see if there are four people trying to collect apples you can just stay someone else's to wait until someone else to to clean the river and then collect the apples and in fact that's sometimes what happens and Concepts here again are pretty uh pretty uh pretty common things position orientation and and pollution positions Etc so would we first plotted the same graph as the previous domain it it it it tells a story so the story here is that when I intervene on Asian one it seems to influence Asian too quite a lot if you look at these three different uh graph reward how reward was impacted when I intervened on Asian one it's agent three and four are fine but it seems that only agent two is influenced same with idle time same with the Intel agent distance so we were like oh maybe that's true but we keep wondering there's like a lot going on in this domain like how do we know this is the case so we decided to take another step so we're going to do a little more work here uh but but not a lot we're going to fill the graph to discover interagent relationships this is simplest dumbest way to build a graph but again I like simple things so how do you build a graph well suppose that you have you're building a graph between movies this is like not what we do but just to describe what we're trying to do we have each row if we want to build a matrix each row is a movie and each column consists of features of these movies so length Jungle of the movie and so on and the simplest way to build a graph is to do a regression so exclude I I throw and then we're going to regress over everyone else and that gives me beta which is kind of coefficient for for each of these and that beta represents the strength between uh strengths of the edges so this movie is more related to this movie and not the other movie and ta-da you have a graph it's like dummy story there's a lot of caveats to you shouldn't do this with them a lot of times but you know this is the simplest way to do it so we did the same thing here instead instead of movie we're going to use intervention on concept C on agent n as our node and for to build this Matrix we're going to use intervention outcome which wouldn't happen to be available without our framework for reward resource collected and and many other things and when you build this graph at the end of the day you get betas that represent relationship between these interventions okay so I had a graph of that Matrix apparently I removed before I came over but imagine there was a matrix there is a nicely highlighted between agent 1 and 4 and that only contradicting the original hypothesis that we had and this is the video of it so when we stared at that Matrix it it turns out that there's no High Edge strong edges between agent one and two so we were like that's weird but there is strong edges between agent one and four so we like dig deeper into it watched a lot of uh a lot of sessions to validate what's happening and it turns out that the story was a lot more complicated the ones orientation was important for four but when that fails agent 1 and 2 kinda gets cornered in and you can see that in the graph agent 4 kind of get a get agent one and four uh sorry one and two blue and yellow agent kind of gets in the corner together they kind of get stuck and this is simply just accidental because of the way that we built this environment it just happened but but the true the raw statistics wouldn't have told us this story that this was completely accidental in fact there was no correlation no coordination between agent one and two but only after the graph we realized this was the case now this might be one-off case but you know what a lot of emerging behaviors that we want to detect a lot of them will be one-off case and we really want to get to the truth of that rather than having some surface level statistics so can we build multi-agent system that enables intervention and performs as well the answer is yes there's a graph that shows the red line and blue line roughly a line that's good news we are performing as well um but remember these Concepts you need to label them or you should have some way of getting those Concepts positions and orientation there might be something that we would love to extend in the future before I go on any questions you shy [Music] cool all right so I did tell you that we're not gonna know uh move uh the solution to move 37 I still don't okay I still don't but I'll tell you a little bit of work that I'm currently doing I'm really excited about uh that we started thinking you know what will this understanding move 37 happen before within my lifetime and I was like oh maybe not but I kind of want it to happen so we start this is all about research right you started carving out a space where things are a little resolvable and you try to attack that problem so this is our attempt to do exactly that to get a little closer to our ultimate goal or my ultimate goal of understanding that move 37. so before that how many people here know Alpha Zero from T my yes Alpha zero is a self-trained uh self-trained chess playing machine that beats that has higher yellow rating than any other humans and beats stockfish which is arguably no existing human can beat stock fish so in the previous paper we try to discover human chess Concepts in this network so when does concept like material imbalance appear in its Network which layer and when in the training time and which we call what when and where plots and we also compare the evolution of opening moves between humans and Alpha zero these are the first couple moves that you make when you play chess and as you can see there's a pretty huge difference left is human right is Alpha zero it turns out that Alpha zero can master or supposedly Master a lot of variety of different types of openings openings can be very aggressive openings can be very boring could be very long range targeting for long range strategy or short range very different so that begs a question what does alpha zero know that humans don't know don't you want to learn what that might be so that's what we're doing right now we're actually almost um we're about to about to evaluate so the goal of this war is please teach the world chess champion on new chess superhuman chess strategy and we just got yes from Magnus Carlson who is the world chess champion he just lost the match I know but but you know he still he's still champion in my mind he's still championed in two categories actually so the way that we're doing this is we're going to discover new chess strategy by explicitly explicitly for getting existing chess strategy which we have a lot of data for and then we're going to learn a graph this time a little more complicated graph by uh using the the existing relationships between existing Concepts so that we can get a little bit of more idea of what the New Concept might look like and Magnus Carlson uh so my favorite part about this work I talk about carving out my favorite part about this work is that the evaluation is going to be pretty clear so it's not just like Magnus coming in inside say oh your work is kind of nice and and say nice things about our work no Magnus actually has to solve some puzzles and we will be able to evaluate him whether he did it or not so it's like a kind of success and fail but I'm extremely excited this kind of work I can only do because of Lisa who is a champion herself but also a PhD student at Oxford and like she played against Magnus in the past and many others chestplates in the world and she's going to be the ultimate uh pre-super human filtering to filter out these Concepts that will eventually get to Magnus so I'm super excited about this I have no results but it's coming up I'm excited yes generator because it's already so many puzzles out there so I'm assuming that there's probably something new what are the problems puzzles are actually pretty simple so the way that we generate concepts are within the embedding space of alpha zero and given that because Alpha zero has really weird architecture so every single latent layer in Alpha zero has the exact same position as a chessboard that's just the way that they decide to do it so because of that we can actually identify or generate the board positions that corresponds to that concept and because we have MCTS we can predict what move it's going to make given that board position because at inference time it's actually deterministic of the whole lot plus zero thing so these we have a lot of board positions and that's all you need for puzzles you give up board position and then ask Magnus to make a move we explain the concept and then give Magnus more board positions and see if we can apply that concept that he just learned for example right but it seems like you're kind of underneath yeah so the if I were to ask stockfish to solve those puzzles that were a different question because we're interested in whether we can teach human not stockfish stockfish might be able to do that's actually interesting uh thing that we could do now I think about but our goal is to just teach one superhume like if I have for example 10 000 superhuman Concepts and only three of them are digestible by Magnus that's a win that would be a big win for for this type of research questions all right yeah so oh so wrap up small steps towards our hopes and dreams we talked about the gap between What machines know versus what we think machines know three ideas why that might be true the three different maybe angles we can try to attack and answer those questions and the the bridge that Gap we talked about studying aliens these machines in observation study or control study there are many other ways to study your species uh and I'm not an expert but anthropology and other Humanity studies would know a lot better more about this and maybe just maybe we can try to understand move 37 at some point hopefully within my lifetime through this chess uh project that I'm very excited about thank you [Applause] you talked about interprecility research that costs NLP vision and RL um do you think there's much about first taking certain interpretability techniques from one modality into other modalities all right so it depends on your goal I think like think about fairness research which uh Builds on strong mathematical foundation and that's like applicable for any questions around fairness or hopefully applicable but then once you if your goal is to actually solve a fairness issue at hand for somebody the real person in the world that's completely different question you would have to customize it for a particular application so there are two venues and I think similar is true interoperability like the theory work that I talked about shop and IG are used across domains like Vision texts so that theory paper would be applicable across the domain things like RL and the way that we build that generative model you would need to test a little bit more to make sure that this works in NLP uh I don't even know how to think about agents in NLP yet so it will need a little bit of tweaking but both directions are fruitful John has a question I saw the recent work in which some amateur go players found a very tricky strategy to trick up I think it was alphago and that seemed like a concept that humans know that machines don't in that Venn diagrams about that yeah actually it's funny you mentioned that Lisa can beat Alpha zero pretty easily and it's a similar idea because uh if you you kind of know what are the most unseen out of distribution moves are and and he she can break Alpha zero pretty easily at least I guess that if Isa Dole had known something more about AI then maybe he would have tried to confuse alphago but the truth is you know it takes a lot it's a high stake game like he said oh it's like a the famous star worldwide so he wouldn't want to make a a move that would be seen as a complete mistake like the one that Magnus made couple of days ago that got on the news feed everywhere that he made this Taco century-wide mistake and that's that's probably hurts any other questions zero for example I just like building machine learning lazy's games really well um well these work that I've presented are pretty you um but there has been a bit of discussion in in the robotics applying potentially these two Robotics and of course I can't talk about details but um uh things that reinforcement learning in the wild people worry about or some of the surprises right if you have a test for it like if you have a unit test for it you're never going to fail because you're going to test before you deploy I think the biggest risk for any of this deployment systems is the surprises that you didn't expect so my work around the visualization and others aim to help you with that so we may not know names of these surprises but here's a tool that help you better discover those surprises before someone else does or someone else gets harm um this is kind of an open independent question but I was wondering we're talking about a lot of ways in which we try to kind of visualize or understand what's going on in the representation inside the machine but I was wondering whether we could turn it around and try to teach machines to tell us like what using our language is what they're doing and their representations like illegal representations of ours and then get the machine to do the translation for us instead of us going into the English yeah great question so it's a really interesting question because um that's something that I kind of tried in in my work previous work called testing with Concept activation vectors so that was to map human language into machine space so that they can only speak our language because I understand my language and just talk to me in my language the challenge is that how would you do that for something like Alpha zero like we don't have a vocabulary for it like move 37 then there's going to be a lot of missing valuable knowledge that we might we might not get from the machine so I think the approach has to be both ways we should leverage as much as we can but acknowledging that even that mapping that trying to map our language to machines is going to is not going to be perfect because it's a kind of proxy for what we think like a penguin is there's a psychology research that says everyone thinks very differently about what penguin is like if I like a picture of penguin everyone was thinking different penguin right now right Australia has the cutest penguin the fairy penguin I'm thinking that right I don't know how many people are thinking that so given that like we give we're so different machine's gonna think something else so how do you bridge that Gap extend that to 100 Concepts and composing those Concepts it's gonna go out a while very soon so there's pros and cons I'm into both of them I think some applications exclusively exclusively just using human concepts are still very helpful it gets you uh halfway but my ambition is that we shouldn't stop there we should benefit from them by having us having them teach us new things that we didn't know before but like um I don't know but like trying to locate like specific strategies in the embedding space What are the alternatives I guess I don't know the Alternatives just because I feel like the wrong thing that's possible so like it's like some transformed space of our embedding space in Alpha zero maybe it's a function of uh applied to that embedding space so thinking about that as a raw Vector is is a is a dead end could be uh we'll see how this chess project goes in a couple months I might I might rethink my strategy but interesting thought yeah so I'm a Psychology major and I do realize that a lot of the stuff will be trying to hear like at least this is how we can figure out how our brains work and so I think that this would there be um stuff that um use that's applicable to internal networks and on the contrary youth English means interpretability it's in the studies of neural network will help us understand and stuff for our own brain yeah I talked to Jeffrey Hinton uh you know he would really like this so I believe I believe you probably know about this history I think that's how it all started right the whole neural network is to understand human brain um so so that that that's that's the answer to your question interesting however in my view there is some biases that we have in neuros Neuroscience because of the limitations of tools like physical tools and availability of humans that you can poke in I think that influences interpretability research and I'll try to give you an example what I mean so in you know cat near the the line or the horizontal line and vertical line neuron in cat brain so they put the prop in and figure out this one neuron detects vertical lines and you can like validate it's really cool if you look at the video the video is still online yeah what is it yes yes yes uh so why why did they do that well because you had one cat and a four poor cat and you had uh we can only prob a few neurons at a time right so that that implied a lot of few interpreportable research actually looked at or very focused on like neuron wise representation like this one neuron must be very special I actually think that's not true that was limited by our ability like physical ability ability to prop organisms but in your network you don't have to do that like you can apply functions to embeddings you can change the whole embedding to something else override so that kind of uh is actually a uh obstacle in our thinking rather than helping yeah okay maybe we should call it there um so for Thursday when you're not having uh lecture on Thursday um there'll be Tas in me here so if you have any you know last minute panics on your project so I think we might have some straight inside to help you we probably won't actually um final lecture cs224 in today [Applause] 
","['', 'machine learning models interpretability and explainability', 'communication between people and machines', 'AlphaGo', 'move 37', 'representational space', 'conversation', 'sample complexity', 'causal tracing algorithm', 'editing factual knowledge in language models', 'localization of edits', 'intervention', 'emerging behaviors', 'multi-agent system', 'Alpha Zero', 'human vs machine concepts', 'embedding space', 'neuroscience', 'interpretability research bias', 'cat brain vertical line neuron', '']"
,[]
