Transcript,Keywords
"welcome everybody I let me remind you
NO LAPTOP rule please no laptops no handguns in the lecture theater thank
you all right and let's point out I got it to work it turns out actually the new
MacBook Pros have a problem and no problem they don't work with all
projectors but this is VGA so this seems to be working all right all right this is good
all right good and one quick things I told my students that a lot of people
like Python my TA they apparently they were right like it was it was close but
it was you know but one thing I forgot to mention julia has the same syntax as
matlab so if you know matlab eurozone Oh Julia does that change things okay who's
in favor of Julia now well he's noise oh yeah all right
comparison Python all right it is still a tie! all right well at
least some there's some excitement now about Julia I spent all Christmas
 vacation doing it okay good so we left off last time was what
is a machine learning so I don't know if you recognize this guy in the background this was
actually my motivation to go into machine learning. I wanted to have my
own terminator to beat up my brother so that was you know when I was 15 or
something ... you know still working on it okay good so let me just give you
a high-level overview of my view of what machine learning is so basically you all know  traditional computer science or at least you know something about it and
in traditional computer science you have a computer which is this thing in the
middle and you have some data which is the input and so the data could for
example be an mp3 file write or some music files or video file or something
and you want to generate some output and where the programmer comes is the
programmer writes a program that generate that basically takes the data
at the input and generates output out of it so if you for example want to play a
music file mp3 file well you know given that you've taken a programming course
before all right how would you do this well you would look up on Wikipedia what
are the specifications of an mp3 file and then you would sit down over a long
weekend and you could probably write something in Python or and see rather
busy takes the file decodes it generates the white waveforms and so on and it's
not a big deal right after a couple days you will have a program that takes that
date that data and generates the I'd output that's fine that's not the kind
of problem that I am confronted with as a machine learning person so the kind of
problem that I get is something like this right so someone comes to me from
the med school and says here this fMRI scan of this person and I would like to
know if this person has Alzheimer yes or not right and the problem is there's no
Ricka pedia page to look up how to generate in how to extract that
information out of you know an fMRI scan right so if you're the program at this
point you know you're not to happen but it turns out it's a very specific type
of problem so what I can do is I can say well like
we help you with this particular image yet but there's something else right so
if I ask the physician to go back in busy look at the past files and look at
fMRI images that he or she has collected maybe five years ago well you know let's
say within the last five years that actually has become apparent if the
person has Alzheimer or not right so he can take the past images that you took
from people a long time ago and you can now annotate them and say well it turns
out this one did have Alzheimer and this one did okay so what I can do is
essentially I can collect data for which I know what output I want all right so I
say this guy basically he isn't as good of a person the answer should be yes he
has Alzheimer here's a person she does not have Alzheimer etc and that's
exactly when we can do machine learning right so machine on ebay he turns things
upside down as input you stick in the data and the output that you would like
to know like you would like to have for that data and the machine learning
algorithm then crunches this data and outputs a program right which if it had
given that data as input would generate that output okay does that make sense we
basing senses we're replacing the program and this program sometimes
literally can be C code right often it's not off to some numbers because that's
just more efficient but you can convert this into you know into C or whatever
programming language code you have it's usually not very pretty right it's very
long and cryptic and so on but it is you know you could go through it and it
makes sense and then of course we do the obvious thing right we go back to this
doctor that came to me the first place and now I have this program and now I
stick in the data point that he actually or she you know wanted me to analyze and
now with this program generates an output and that is my prediction and we
call this part your training and this part here testing all right any
questions we will go through this and much much more detail in you know in the
future by the way just one morning this is the last PowerPoint side I will get
after this actually everything will be back board and so
suppose initially and people came up with this this seems like a crazy idea
right can you even do this right this you know like the program by like you
know the computer busy programs itself right it seems nuts but it turns out
over the years you know it really really works and nowadays actually what's
fascinating about this is a can write programs that raise the you know a lot
of work to right now you can make it automatically but on the other hand
that's the most fascinating part you can write programs that you don't even know
how to write but you can generate programs that you wouldn't even begin to
know how to write that program so machine learning was formally introduced
it's actually dates back before this but the the first formal definition that I
am aware of comes from Tom Mitchell 1997 so he wrote and let me just write this
be this out a computer program a is set to learn from experience II with respect
to some class of tasks he and performance measure P if it's
performance a test the T as measured by P improves with experience II got that
the short version is algorithms that improve on them tasks with experience
right that's you know without the e NP and you know that's very very simple
right why does this improve if you go back here where's things improving well
if I give this more data here I can basically generate a better program
right and I would do better okay so that's the idea the more data I give it
the better the program is and that's why we call it learning all right okay let
me go through a little bit let's just walk down memory lane and give you a
little bit before we get dive into how to do this and a little bit of history
of how this all started and where I came from so arguably the very very first
well that some dispute but the the first algorithm that learned from experience
was Samuels checker player in 1952 it wasn't really kind of what we would
consider machine learning now but it was basically the first time someone came up
with the idea to you know to actually learn you know basic to stores and moves
in the database and actually as you play more you eventually get better checker
since then actually has is a solve problem so it was a benchmark for AI
this time there was no mentioning of machine learning machine honey did not
exist yet so the field first actually there was a
you know at this point you know at some point people at first called artificial
intelligence and eventually machine learning emerged out of this the
checkers actually became as benchmark task for a I for a while cuz Alex there
was a a world champion of checkers he was amazing he was probably the best
human to ever play you know it'll never be someone please check us that well he
was undefeated for you know over a decade
it was ignore one was ever close to him the big dream of the AI field was to
finally beat this guy and they tried many attempts and turns out you know he
always beat the computer and eventually actually I forgot which here exactly it
was they finally had a program that beat him and he died very short thereafter
and so I looked into this and trans Alex he's not a coincidence that he died very
shortly thereafter turns out actually he had terminal cancer and he at the time
because he was already in his death bed he was in a hospital he was dying in a
few days left and someone rushed in and was like beat this computer program and
when they beat him like oh yeah you take this died I don't know I feel like
that's you know it's got a dark point in the history of AI I could have you know
let him die undefeated like really anyway checkers is actually now and
solve problems so for anything a position there's the optimal position
the optimum move is worked out and white will always wear all right so that's
it's no longer a challenge it's not a game in that sense what I would call the
first machine linear I wear them actually comes from Cornell in 1957 by
Frank Roosevelt so he invented the perceptron which was there's something
people talk in their you know in detail I would devote a whole lecture to it but
this is really an algorithm that it's still used today and was really
revolutionary at the time and ultimately this is what an artificial neural
network or were deep learning is today so there's actually very little
difference to what he invented back that so here at this basically his insight
back then is you know it's busy ninety percent of the way there so he you know
water he was really missing was computation of power so he did not have
this large computers that we have nowadays and so he was really a
visionary and this is ingenious algorithm that we will talk about very
soon at the time though one thing you have to keep in mind this is I will not
talk about this too much but it's basically he could only train very very
small networks because he was very constrained by the speed of computers
and as 1957 think about this probably the in all the computer power the entire
world is less than your phone right and so he had me buddy mostly analyzed was
like a single layer perceptron is a very very limit accurate but when this came
out there was a huge at this time there was a huge excitement about air because
this was something new all right like you could I've rhythms I could could
learn from experience you know they could play checkers right to some degree
right and so on the people were really really excited it was a lot of funding
for it a lot of funding research and you know generals were already dreaming of
you know armies of robots it said I mean Elizabeth generals do right it's dream
about this we have nightmares of that kind of stuff
but so but you're not actually it was a little bit to it so what happened was
something called the AI winter this was the AI boom people were really really
excited that was the first day at home we're really excited about AI am and
this came out and everybody was talking about it it was articles in the
newspapers and so on and then came a book by Minsky and Papert
and I don't think they had the intention to kill AI but they did and they
actually analyzed these algorithms and they came up with a very very simple
data set that everybody could understand the X or data set it's very very simply
can draw it on to it and they could say that the perceptron has rows and blood
shows that the single layer perceptron can never learn that date and it seems
so simple space is just as four points they could you know to a positive to a
pluses to our circles they could say you could never actually learn to
distinguish between these and it seems like such a simple thing that everybody
understood made if they can't even learn this right how can they ever become
killer robots right like you know there's a disconnect here
and so what happened is actually funding for
e I collapsed like you know drastically in the United States and in most
countries around the world so this is called the AI winter and people still
refer to this in space even you know people got really disillusioned but
they're so they're more people did as they said well wait a second we don't do
AI we do machine learning it's totally different and you should find that it's
the same people that you know so that was really actually care and this is the
sarcastic way but it's kind of where I came from right so and but it's to some
degree the fact that the old that you know they gives if some research area
dies there's also a moment for facing new approaches right that be maybe
before and weren't taken seriously because it wasn't kind of it wasn't part
of the way people thought right it wasn't along the lines of people people
thought suddenly new approaches would take you seriously and that is
essentially how it would place machine learning so on one hand you know if you
want to get funding you have to say you have to do machine learning but on the
other hand actually there was also a difference and I would say the
difference is twofold number one AI was too we're very focused on the human eye
base hair like we want to build something like a human and it was very
focused on problems that humans think very very hard and try to solve them
with algorithms machine learning is more the bottom-up approach so even you start
with a computer and it's more like basically what I've told you earlier
right you base you try to find a program to learn something from data there's not
really you know machinery per se does not really have the ambition to be you
know create humans essentially okay it's something like a job and it can be very
different approach and that's very freeing right because you can do things
that you know well that's clearly not what's going on in the brain but it
works really well on these kind of you know on computer hardware that we have
and computer hardware is very different from the human brain another thing
that's very very important is that machine learning focused on statistics
and optimization and not logic so know much computer science traditionally is
very entrenched in logic right there's a very strong connection between you know
most algorithms right away you argue about algorithms is a through
logic and that is a you know that was a natural choice to think about AI and if
you look at old movies like Star Wars etc right the way AI you know is
presented is as completely logical beings right there everything they say
is like c-3po is always logical right because that's how people thought a IR
actually to be honest if you read these old papers some people thought that at
the time that humans are completely logical right like you know it's not in
the brain everything is very obviously they didn't
have many friends but um it seems like ultimately the wrong approach right
people try that you know that some point is like people inventing fuzzy logic
because they realize you have to have uncertainty right sometimes we just we
have believes about something it's not you know you don't have a clear truth
like it's you know people really made these statements like is the light on
yes or no and if the light is on then I don't do anything if the light is off
then I switch the light on all right but sometimes you're not sure right about
something and the right approach to do that is really statistics I mean
statisticians have dealt with uncertainty for you know four
generations right so before when that wasn't really taken into account it was
you know I'm not saying he is not old statistics is not statistics at all but
machine learning was definitely a much more Cystic or and optimization based
approach okay so there's a very interesting point in 1994 and where I
would say this was a pinnacle point because it was the first time but people
really kind of realized how powerful that ideas right that it really works so
you have these moment sometimes researchers are working on things and
they're in their own bubble but occasionally they see people around the
world notice what's going on and that might those moments was in 1994 but
Jerry T's hour of an Iban wrote a a backgammon player so I know people have
heard of backgammon it's actually a very very popular game around the world
almost every country except the United States yep people don't play but it's
actually it's a you has a huge following in a huge part of the world it's very
very competitive there's World Championships etcetera and
what Jerry did is he wrote a program that basically just
check if a move is legal and that's easy and then he you know the tie the time
the way people thought game playing works is use the minimax algorithm that
those people have taken AI they know how this works basically what you do is
that's it you know that comes you know you play checkers and you would do this
he bassy say well you go through every possible move that you could make and he
say if I would make that move what would my opponent do if I was the
opponent would make that move but what would I do
I would try this move its Arab and you always flip the board and at the end you
say well who looks better now and if now my opponent looks better than that's a
bad starting position and if I look better than that may be good so he did
something else he instead took an artificial neural network which is
basically you know what percent would roast got invented in 1957 and he made
later later that the new network made the decision which move to choose so he
said you know here's a list of all possible legal moves the new network
chooses which one to play and so when he did this the algorithm may totally
random moves clear didn't work all right doesn't do anything but then he did
something really really cool he took the network and he let the
program play against itself and so you had two copies of the same program and
the letter play against itself at the end one of them wins right there pretty
random and make pretty random moves but one of them wins and you tell that one
what you did was good and the other one you lost what you did was bad right so
on base he tries to so reinforce the network to do but even average you get
positive feedback you try to let more when you get negative feedback to try to
that less and so you had that feedback loop babies just let this run and then
he went home and he let this thing run play against itself and he let his rule
that's a run overnight whatever next morning comes back it takes the program
it tries to play against it and he loses the program beats her had overnight just
play against itself but no other input has learned to play better than he did
so he goes around the deport you know is this office right challenges other
people like play my program right and the beats
every single one of these people so he gets really excited right so what he
does is he calls the world you know the organization with a world championship
in backgammon and he says like you know have this program and it can play really
really well you know backgammon really really well and I would like to
challenge the world champ and so you know okay they were curious and the
world champion was interested and so they actually made an appointment and
they paid against each other and in the meantime by the way he you know he kept
let that program play against itself a couple hundred thousand times a million
times and the two at the faceoff and the program won and that was something that
took people completely by surprise right and people go up to him after was like
well that's amazing right how does the program do it
he's like I don't know because I've learned that itself in fact Jerry by you
know it's not such a good backgammon player action he actually talked about I
talked with him about it he actually said well afterwards well you know
because then he you know he had to talk so much about backgammon and so on he
actually started to get a really into it but at the time he really wasn't a good
play at all like the program was much much better than he was and without him
actually putting any of backgammon knowledge into it and in fact the
amazing thing is that doing that face off with a world champion the the his
program made a very interesting move very interesting opening that the world
champion thought was was crazy I was not of a good movie at all and so
but it turned out it was actually a very good so the world champion actually
asked for a copy of that program and started analyzing it and then actually
realized that's actually an opening that was undervalued actually turns out there
was a new opening that basically this program discovered and it's now actually
in the backgammon folks so after this happened
his bosses at IBM we're really excited about like this was awesome but why did
you choose backgammon no one cares about back-end so they did they are let's do
chess and so they again put you know this time they put a big team together
and a lot of resources and the challenge Garry Kasparov you know you know world
champion at the time and probably one of the best player
the history of humankind and their to matches against them in the second match
actually the first time Gary Kasparov won and they challenged him again in the
second time deep blue won no fairness actually this was less about learning
approaches was more of a minimax traditional AI approach because they
played it really safe but the evaluating if a position is good was that
now machine learning is everywhere and you may not even know right this here is
a search engine is Yahoo I don't know if you've ever seen it so search engines
for example let me you think about what search engine is a completely ridiculous
problem right we have billions of web pages and you type in three words and
you expect based on these three words that the search engine knows which one
of these billions of web pages you want all right clear that cannot work right
how but does how does it work well the reason is that humans actually have very
predictable expectations right of what you want to see etc and you know a lot
of bad pages are not very interesting etc and so the way this actually is done
this really is actually learn so but with Google and Yahoo and Bing and so on
what they do is they employ hundreds and hundreds of people that actually goes
the search engines amazing whenever you search something on Google there's a
small chance it gets to one of these people and what they do is they actually
label web pages and say this was a good answer this was not a good answer this
was a good answer this was not a good answer and then they you know basally
put all this into a big learning algorithm the learning algorithm then
learns what's you know what's a good answer and what's about it of course
spam features is a great example for something that that's that it's really
really hard to write a spam pit and it's almost impossible right for to reactor
you know for two reasons a what is a spam email well a spam email is an email
that you don't want all right well what does that mean hey well kitty some
people want these emails right otherwise it would respond to these evil right so
for example emails about stock right or stock options or something for me though
we spam because I'm not really interested in this but Matt you know I
know someone was actually you know finance right well for them actually
it's not right and here also train your spam filter there you know whenever you
get emails from your mother-in-law right you can just spam you know train it to
say that's not so whatever you think is a spam right it's a very very subjective
definition that's one problem the second problem is that the moment you would
write a spam filter that's hard right ins that be say like whatever you you
know for example the word viagra right I know this is some I know what that is
spam but like for why there was a lot of viagra spam and so you know it's very
simple Google people but ever you have the bird viagra in the email that's
probably spam right it's very rare that people actually you know email people
are like you know I love viagra is on the you know you know we do that good
maybe some people do but but that doesn't work right because the moment
you do this then you know the spammers will just misspell it so they busy spell
of one instance of an eye or different you know something slightly different
and then you still get the message across because it's adversarial behavior
right so people would actually immediately then change the way they are
spamming and now you have to rewrite your whole program the nice thing about
learning is you just take the new data and you're just retraining back right
five minutes later you have a new spam filter that actually is now adapted to
the to the new new way span looks or you can actually do online training that
basically that's the way we do it works is every time you get a spam email you
train a little bit and so it always keeps up-to-date another example here's
news right so Google News actually learns what you're interested in
apparently I'm an interested in Lindsay Lohan well I guess it's not super so it
will learn your parents right and basically suggest new stories to you and
so one thing you know that I guess a lot of people talk about those a lot of
research here at Cornell is for example self-driving cars right and so that's
also something that really I think you know a big part of why that is possible
now is actually to machine land right so a lot of the sensors have been around
before was there some also some new developments but the fact that it's you
can basically you know learn from from how to drive a car safely really kind of
was a game changer in this respect and there's something by the way where I was
surprised I remember a couple years ago someone asked me
maybe ten years ago as I'm asking far are we away from self-driving cars
now is that yeah it's going to take 50 years and you know next year basically
people start a drive a crowd so and the reason is actually because suddenly a
lot of things come together it's a sudden you get these leaps because
suddenly a lot of little problems get solved and then you put them together
and suddenly you can do something big and when will it stop
so there is of course we have this this carrot that's kind of dangling in front
of us that's the human brain right so the human brain really just is a big
computer it's just a very different hardware but we know humans are really
really good at learning and the principles are very similar the actual
implementations of the algorithms are very different
but the good thing is we know there's basically we are not even close to that
right so we can do much some people claim this nowadays the deep learning is
now better than when you human brains I completely disagree that's they're very
good at you know very specific things you can train deep nerves to be you know
if you have a very specific task you can train algorithm to be better at that but
humans can do so so many things so that's certainly in some sense good news
that keeps me employed and the the search in some sense means there's you
know there's still a lot you know we still have a lot to do a lot of
opportunities the other thing is machine learning now is you know at the
breakneck speed is being applied in all those various so people now in biology
and chemistry etc like 10 years ago when I got my PhD when I talked to people
outside of my computer science and I told them I didn't mean learning nobody
knew without words I think they thought I thought about doing building machines
you know this was something that was completely unknown and now people in
biology chemistry I you know tell anybody you do machine learning and they
overlay oh okay yeah you know we should talk so it's raising out you know
suddenly you know people in feels that have nothing in common right use the
same algorithms to analyze the data and make predictions and that's extremely
powerful and it's still very hard to design machine learning algorithms and
there's not many people who can actually design new algorithms that are
meaningful and useful as a small community but there's many many people
who can use them oh wait oh yeah I have a little more okay good so let me just
talk about one of the different types of versioning there are and there's on a
you know very roughly there's three types of machine learning that's
supervised learning and unsupervised learning and reinforcement so this class
is really only about the first one so in this class we will talk about supervised
then supervised learning is exactly what I just told you earlier with the fMRI
images so I give you data and I know the answer the data and I would like to have
a function that goes from that data to that answer to that output for example
spam filters and search engines etc unsupervised learning is when I give you
data but I have no labels I just tell you like here's some MRI images do you
think that something interesting going on here all right maybe you can find
some patterns that repeat or something like this that's a okay class machine
learning for data science is mostly covering this that's basically we're
alternating these two classes there's also reinforcement learning and
reinforcement learning is when you do something for a while at the end you get
feedback and say you know that's good oh that's bad and that's what but Jerry's
are used for its backgammon so that's mostly covered in the AI course and in
the robotics course those are the desert there's a significant overlap in the
methods that we using it's just the way the feedback is incorporated is
different apparently machine learning is number one skill that employers wish for
these days unfortunately I don't remember where I got this far so so
someone claims that on the internet so but there was a list of 12 IT skills
that employers can't say no if you close there now a little bit old but you know
Bill Gates once said a breakthrough machine learning would be worth 10
Microsoft's now I'm waiting for that machine learning is the next Internet
machine is the next hot thing and so on bad rankings today are mostly a matter
of machineries that's really very true machine learning is going to result
learn in a real of evolution machine learning is today is this continue with
you all right good that's the last slide and from now
on we will switch to blackboard any questions about the stuff that I just
said um good all right and so while I do this I can hand out notes of the way
people do this is Irie don't like it when people take you know I write
something in the blackboard and you know the entire class writes down what I'm
saying and no one pays attention so what I'm doing is I give you exactly my notes
so I yeah I just know I have no information advantage in some sense so
you can just pass these back and if you free to annotate there's a downside so
you get my notes in my handwriting and I apologize right now you'll a minute you
know what I'm talking about so good alright okay so if you only have
15 minutes today and but let me get started um a little let me at least get
started guys so enjoy this is the headline of the
entire class actually so the entire class ovo a supervised learning and what
I want to do at least today is to formalize the set up so what's the
high-level goal and what have you given what are we trying to do and let me just
write this down an automatical notation and and give you some basic terminology
that people use throughout the course so make sure you you know you familiarize
with this yourself with this because this is exactly the stuff people you'd
be using the entire class so so on a high level but supervised learning
attempts to do is make predictions from data so the set up is you're given a
data set and we call this D of data points and their labels so we're given n
pairs of points where my X is my data and Y is the output that I would like to
generate we call that the label click on that the feature vector so in terms of
spam filtering that would be the email and this here would be its add a spam or
it's not spam all right so all right you know in terms of stock market right does
he has busy some preaches about a stock and this here would mean up or down I
try to predict as a stock up let's talk about so these data points are sample
from some distribution and if X I Y I Sam who's under stab you
should P P is the it's a distribution that we have no access to it's the
distribution that only God herself knows right it's basically where the data
comes from I had a v-8 access to this distribution and everything would be
easy but we don't right there's some probability distribution that if I would
pick someone here randomly and take an MRI scan of that you know their brain I
basically get you know you get an MRI scan if I would do this repeatedly right
there's be some distribution of MRI scans that I get with that process okay
it's a some strange distribution that some from from Mother Nature okay and we
observe data from this but you know in terms of emails this is the distribution
you know from which emails are sampled so everybody may have a slightly
different one right so some people sign up to the newsletter cetera so that
changes the distribution of the emails that arrive in your inbox okay and so
what we would like to do is take data from this particular distribution and
learn a function that basically goes from X to Y okay any questions all right
so this kind of data is either you know there's multiple cases the most common
case is either a time series for now let's assume it's not a time series for
now just assume these are iid so these are drawn iid anyone know what iid
stands for I think all of you together got it right who knows it raise your
hand yep that's right better so they're all from
the city distribution P and bacey knowing one of these won't tell you
anything about the other ones if you know the decisions okay good
so what are these XS and one of these rights so let me just quantify this
so these X's come from a space but I called curly ax and the Y's come from a
space called curly Y and so in this class
he axes are typically a d-dimensional space of real numbers so basically what
we're doing is we say we take our data and we summarize our data in as the
numbers so it's a vector that has D numbers and that summarizes everything
we know about this particular data instance here's an email you could for
example imagine these are the words of the email or something right and I'll
show you in a minute how to do this so for now we assume you can always do this
and it turns out very very awfully candidates in fact if you think about it
if you store your data on the computer what you computer what is it do right it
has a long file of numbers right well that's a vector right of length T so you
know it's always possible somehow all right
so and we just cause our D Israel's and it's D dimensional feature space that's
about me referred to and the X is basically a vector that represents the
ithe data sample so the eyes email or the eyes MRI scanner why I they can you
read this Novak okay not so good you know what I write it's a bigger chalk
okay Lloyd - okay I would try to remind me tomorrow morning to bring bigger
chalk all right okay awesome so so why I is out of the label space
curly why and that's amazing the answer that you would like to know is as I that
you know for example the despairing does not span etc
okay let me give you some examples for what Y can be let me oh that was the
coast all right okay let me give you those some examples so for example one
is binary classification so examples of Y so binary classification is Kelly y is
either 0 or 1 or you can also write it as minus 1 or plus 1 and so an example
here would for example be email spam filtering right we basically say minus 1
is spam and plus 1 is not spam so you have exactly two options and for every
email it's one or the other another option is well you know for
example face detection you take an image is that a face yes or no right so your
camera does this detects faces there's a little window that goes over the face
over the image and tries to find a face there's methods among the most common
settings in machine learning another one is multi-class classification so here
Kerli why you have multiple classes so I call this K different classes that can
be 1 2 3 4 5 right so let's say for example one option would be that I take
a articles on the web and I would classify them as either sports you know
or politics or you know but gadgets something reviews or something like that
so right so busy have different categories and you want to know if her
sample in Google News right they ask you are you interested in entertainment are
interested in politics are you interested in tech news or something
right and so what they do is they go through the newspapers of the world and
they classify each article that just came out is one
of K classes and the one made for example to be sports and you know K is
politics any questions about this raise your hand if you're with me alright
awesome cuz that another one is regression and regression Y is just the
real numbers that's one way of doing regression so here you don't
differentiate between different categories you try to hit those specific
number so for example I don't know if people know Zillow zero is a webpage we
can actually go to anyone any location in the united states take on any house
and it will tell you how much that house is worth and and so that's interesting
if you want to buy a house you can look around and kind of check the
neighborhood's how much different houses cost right but it's a relatively hard
task so what they try to do is they busy say you know well you know how many
bathrooms does the house have what's the square footage right what's the
proximity to the schools what's the ranking of the schools etc right all
these kind of different features so these are the feature items that I have
my vectors in different dimensions and I try to predict how much the house
hopeful and whenever new house gets sold I see put that add that to my data and I
retrain my Iowa alright so but the answer could be anything between 0 and I
know a couple millions theoretically I guess and another one is like for
example if you would like to predict the height of a person either the age of a
person all right good any questions about classification about this binary
regression with classification etc all right good yeah hi I cannot I don't have more
does anyone have two three you're not allowed to sell them I will post them on
Piazza how about this and you can just print out your own who does not have one
raise your hand oh that's terrible the oldest I see they're all in the same
location got him look at the neighbor okay someone's really nice you could
just pass it pass it back and then look at your neighbors but they it's only
three minutes anyway left okay so that is why right so basically if you have
you have our X and we have our Y so Y is what you're trying to predict and he
just says kind of either be you know a set of numbers that you're trying to
shoot for - 101 or zero one can be you know K numbers that's you know K
multi-class classification or can be real numbers you know you know if you
want to do regression now what about X let me give you some
examples of X and well let me give you first one it's for example you know
patiently and so one thing that's really really big nowadays actually well it
won't be anymore I think but but you know one part of the Affordable Care Act
I guess was you know there was a lot of incentives for hospitals to not to not
discharge patients that then come back very very quickly and so that's that's
generally not considered in the interest of the patient right so a few measly get
discharged from the hospital but then a week later you have to go back in well
probably you shouldn't have left the hospital
alright that's usually what generates more costs for the insurance and so
basically there were penalties for hospitals if the patients go back to the
hospital within six weeks I believe there was some some time period so a lot
of effort was made saying predicting if a patient will come back in the next six
weeks and so is the participation healthy enough that he or she can be six
weeks outside on his or her own and do fine because then after
six weeks you're probably good okay and so well you could do is you could basis
a about my ex I is basically the specifications office patient so one
thing you may want to stick in is the key a binary features 0 1 are you male
or female right so he it could make a difference who knows and the other one
is the the H right so you know here's like person 76 years old was the age and
years right so that's another way of describing is the height right 183
centimeters of course you know you don't know what that means but you know you're
six feet or something I don't know how many hands I don't know some you know
height in centimeters yeah you know and you now basing that's a you have some
blood pressure and you know heart rate or whatever and now you try to predict
basically does the person will the person come back yes or no I didn't just
teach a story data from people that you that did come back it did not come back
and you train you algorithm that way so another very common one is text
documents alright so you here you have an email for example and so that doesn't
look like a vector that looks like you know straight numbers as are a string of
words and so what people do is that something called a bag of words bag of
words about representation but you basically say well word orders over ate
it right I just you know who cares about in which order you put in the words down
let's just only look at the word occurrences which words are in the text
and turns out that works surprisingly well so and then you can actually take
the following you can take a vector that basically is d dimensional where D is
the number of words in the English language works out alright so I don't
know how many words there are in the English language maybe a million right
and so then you start with the first word not vertical what it doesn't matter
right so ant right and down here you have zebra right and so then for every
single word in your email for example you just say how many words that occur
so for example the word the you may have five times right in the
word moose right seven times right I said I don't know
right and and no right and zebra you didn't talk about etc right so that
because you just you discount the words in your email and that is a vector of
just word counts and that is a vectorial representation off your email turns out
that works surprisingly well so that's an easy you know usually you can
identify it's an email as spam or not spam or a news article you know if
that's politics or or sports right you can detect that very well just in this
representation which makes sense right so if you talk about baseball a picture
or something it's probably sports you know if you talk about Washington DC
your president something it's probably politics why don't we leave it right
here and I see you again on Monday 
","['machine learning', 'computer program', 'learn from experience', 'improve performance', 'algorithms', 'data', 'program generation', 'prediction', 'supervised learning', 'data set', 'labels', 'feature vector', 'spam filtering', 'email classification', 'stock market prediction', 'MRI scan', 'distribution', 'iid', 'bag of words', 'text documents', 'vector representation', 'healthcare', '']"
"all right quick word about the exam that was just to get you quiet all right and please please those who haven't dropped off the placement exam table at the end of class and now we have a three-minute advertisement to the our project we have the other we have some new competition resources semester we have a new cluster with about 100 CQ force for me to pick the memory so we had a 2.7 gigabyte text file that basically crash from most of our teams last week on it punchy spark as you can see the file that I uploaded it's about 400 but using sport tix and it can dousset these kinds of really fun data science stuff into comfort robbie keane learn more about on wednesday and if you have any questions email cornell data science at gmail.com thank you thank you [Applause] Thanks all right terrific all right and who has who still needs a handout from last week from last time okay it's a little tricky it's random over there enemy how do we do this let's pass them around okay sample here hey wow you guys camping you pass some backwards I don't know who it doesn't have any all right oh sorry that's some extra here [Music] all right everybody's lent Co all right some people still leave the handouts how's that possible this is 370 I gave a 370 but some the new people arrived I'm sorry I don't have any more I have a lot of places he sounds maybe I don't know like who who doesn't sitting next to it ok breeze raise your hand on the time who needs one now look around you who your neighbors also raise your hand oh yeah so you can I'm clustering and okay could you maybe just ask the people in front of you or behind you if they'd be very nice and maybe give you one of those at least every other person should have one okay um so the placement exam is equated tonight so hopefully tonight or tomorrow you will get an email with from velarium that will invite you to be on the on the web page or you get email saying that maybe you should take it again soon by the way this class is likely to be offered again next semester all right by the way I got a new torque torque I got the big chalk okay last time we talked about the machine learning setup so if you remember we had we said we have a data set D that contains n data points o n pairs a feature vector and of label and that was in from the space curly X curly y McCurley x is the space of all features and curry y is the space of labels and these are drawn from a distribution which is elusive to us so these are drawn from done distribution pins I'm going to ask me at the end what does that distribution p right seems are like usually have a Gaussian distribution or something all right and this is really this is a distribution that you can't write down as some distribution of Baisley of Mother Nature or something right so if I would for example go to outside in on you know Cornell campus right and take pictures of students faces right so you know let's say I want to make a face classifier and you know I go outside the first person I meet I just asked them could have take a picture of your face right Soviet but okay and that would be a distribution rights as a random distribution of who I ran into and if I would do the same thing somewhere else in some other country that's a I would do this in Beijing I would get different people right so it's a different distribution P okay always there's a distribution we don't know it's something that's you know we don't control we knew this distribution would be really really easy right because then you could actually just take P of Y given X all right let me just stick in our X and we predict the label Y but we don't know P so therefore we have to do this whole of machine running and by the way there is a that's important things that when you can you collect your data you have to collect from the right distribution actually they're the one that know now that I just said the face example I was like see a great example of Nokia so I don't know if people have remember Nokia used to be a company that makes phones and and and they had one thing they want to do is they want to put in a face recognition system so when people take a picture a camera a picture with the camera it automatically detects the face and so usually when you have a face in the camera that's the person what do you want to zoom into and what you want to make you know sharp etc and so they trained their face recognition system you know with random people ways it took a random people from the streets of Helsinki and the promise of course you know Nokia phones are sold everywhere in the world and so one thing they you know they forgot it's that basically they didn't have any people they were not Caucasian so and then next even you ran it in the United States african-american you know here's a stand in front of camera I think it doesn't see me doesn't see me right and there's YouTube videos they were really funny because basically the problem was it was trained on a different distribution then actually was applied and that's very very important that when you take your training data it should be the same distribution that you and that you actually intend to deploy it on afterwards in this case actually was quite embarrassing for the company and they had to update the software cetera okay so we basically have this data we assume this data is given to us and we you know a big you know getting this thing that's not always easy but in this course we won't get you know I won't get too into much detail how to actually collect this data we just assume somehow you can sample that data so you know last time I had a few examples I you know one from hospitals where you have you know patient data and I said you know for example it could be a vector that this drives a patient and why could for example be the question will this patient return if I would release that patient now would he or she return to the hospital within six weeks that's a typically good question another one is text documents this could be an email and why could be the question is that email spam or is it not span and finally how far did we get is that how far we got yeah okay good so let me do one more I just want to say image so X could be an image this is exactly for example the face problem right so face detection for example so well image actually is a bunch of pixels so if i zoom in it looks like this and these pixels have certain colors so now we have my my face here's my face something like this and so if you have a 6 megapixel camera for example I think the eye latest iPhone has 6 megapixels well really buddy hips so that means you have 6 million pixels each pixel actually has 3 different colors so it's consists of three numbers the red value the green value and blue value usually you have around 255 values for each one of them and that's 24 bits and in total so then you can write your vector X eye as basically a vector that takes all these different pixels and says we keep the red value the green value the blue value of this pixel the red value the green value the blue value this pixel and so on right so this is my for example 0.3 0.2 0.8 that's these numbers describe this pixel here in the top left so you know if you have six million six mega pixels and this leads six mega pixels would lead to a representation that's an 18 million dimensions now that's not often not a very good representation of images but so for a long time actually people had much much better representations in the last couple of years actually people went back to actually just representing pixels as pixel images really as pixels and we will get to this at the very end of this class let me talk about convolutional neural networks okay any questions about the feature space X and representation of data as vectors so the key is basically you know whatever your data is we assume at this point that you can just take your data and take each data instance and represented as a vector X and we assume that every vector has the same dimensionality and and so the text document be a pair of words and and so on all right oh yeah one also one distinction away that I still want to make is there is dense and sparse vectors so let me just actually annotate my notes I just realize this so we have dense there's a sparse and a dense representation means that we use you know for almost every example if you use every single one of these dimensions there's always some value here a sparse representation means that the majority of these dimensions will be 0 and only a few of them are filled in so for example you have this in text document so if you have a text documents and last time I said the way we do this is we just take you know a huge vector of it each dimension corresponds to one word in the dictionary so you know we have twenty million that's all right so a million words that say in the English dictionary then if you have a little email well almost every word in the English vocabulary will not be in your email okay and so if you pick a word randomly you're almost certainly will not be in your email right in fact most of our language we just use the same thousand words over and over again so almost all of these will be zero and there's a few counters basically we say the words there actually is quite common alright so that's actually that's a sparse representation and that's important because the nice thing here is that these if you actually would store this you wouldn't actually still those you would just store only the entries that are nonzero and and so sometimes it allows you to do machine learning on very high dimensional data and because it's part so you can still feed it in memory so the important thing is to understand what stance and what's parts so sparse means you have many many zeros and you're in your future vector and dense means typically don't alright so in the first lecture I had this cute little picture off the computer and you write a program and so on so let me just go back to that picture and use the notation that I introduced to just make that a little bit more formal oh it's still like a tune but so this here is my computer I stick in the data and I stick in the output that's what I had and hope you people remember this raise your hand if you remember what I'm talking about alright good so my data here is basically the X 1 to X n these are for example emails and then for each one of them these are my training denizer I know what the answer is right I know this one was a spam email this one is not an spamming and so on so then I stick in my y1 to yn ok ABC correspond to each other and phasee what I would like to have an algorithm that if this is the input this should be the output and so my machine learning algorithm the ML algorithm is then going to crunch these numbers and output your program right which we call H so H is really mathematically it's just a function and so hopefully if you take any H of X I you hopefully get why I back in almost all cases so that means you did a good job if you're learning and so now that's what's all we call training and then after we're done we take you know we move to testing and testing is now we take a new instance ax or which we don't know the label we take this program H we put it in a computer and the computer computes as H of X which is prediction and hopefully that is actually the y that corresponds to that X all right any questions raise your hand if you understood it all right awesome good I like some people like yeah all right move on okay good it's good I'm moving on I'm moving on I don't want to lose anyone the first week okay one thing yeah one thing is important is this this will only work and there's a you know I'm saying this yeah until you can't hear any more but it's the only work if this X is drawn from the same distribution is this right so these here these X's are drawn from this distribution P this here oh there has to be drawn from the same distribution otherwise it will not work okay good so where are we now you're trying to do machine learning and hey let's go all right good all right so when you want to do machine learning the first thing you have to decide is what is my label space and what is my data space okay so you take this data you generate these vectors we just give you a bunch of examples you also generate your labels why do you have to understand what problem are you in it's a regression problem does anyone remember about a regression problem is what defines a regression table yeah well you you do have labels but they're real numbers and that's right you don't discrete labels yeah and so regression you have real numbers you know binary you have 1 or 0 you just have to double options a multi-class classification you have K options that you want to predict and you also have to write a feature representations you have to somehow come up with a good way of capturing your data and make sure you include everything and there's one more thing you have to actually decide what do you function H and so like so far I just said oh he learns on program you know H but that you know it's not gonna be magic right so there's like different kinds of programs that we could learn and most of this class is actually about you know what kind of what kind of options are there and when is you know algorithm a the right thing when is I wouldn't be the right thing etcetera so we define a set curly H that is the hypothesis class and that's actually this year's called hypothesis that's why it's H so the basally say well leap you have some set of possible programs possible functions that we could learn and we try to find the best one out of that and choosing h is usually something that's not automated some people are trying to automate it but that's usually not that's something that's your job that's you job as a data scientist it's given a problem making the right choice of what your curly Ages right what kind of functions should we try to learn given that particular data that particular problem and then the argument of Xin learning algorithm stop is out of all these possibly infinitely many functions that I could learn take the best one that fits your data that'll be con learning so let me give you a few examples examples of and we will go through all of these in the next couple of lectures is for example decision trees just to give you a little sneak preview of what a decision tree is this is a tree is one that actually it's a tree well let's say for example in the hospital data if you want to predict to someone coming back within the next six weeks so what you do is you repeatedly spit on some features for example say you know is the age greater than you know 65 right and if yes then go to the right no go to the left right you say well this day you know let me have for example is blood pressure are very high right then yes go to the right no and let me say here probability of returning is very high maybe it's 90 percent probability of the person returning the person here becomes maybe say is the age you know greater than 30 if that's no then you know maybe it's you know let's say it's unlike if the person returns and so on so you build a tree that basically you put your data through and ultimately what you're doing is you're petitioning your space in two different instances and say if you fall into these down here and the sleep down here you're likely to return to the hospital if you fall into this leave you're unlikely to return the hospital maybe that's a decision tree the question is how do you learn these decision trees that they actually have provable guarantees etc and we will do this in a couple thank you another one is linear classifiers and some of you may have heard of these perceptron for example that I mentioned last time is exactly a linear classifier then we've artificial neural networks neural networks whatever also called deep learning these days and support vector machines so these are probably the most common and are the most famous ones machines and so one thing I really want to hammer into you is that there's no best algorithm right there's no like that's a bit mistake that I see especially like a few years ago super vector machines were super fashionable everyone want to do everything super vector machines now the same thing with deep learning right if people come to my office and say like you have this data and I want to use deep learning right that's not how it works it really depends on the problem depends on your data which one is the right argument and so hopefully by the end of this class you would have a good understanding it's actually a very principled way of deciding what the right I wasn't so it's not just a you know trying the one that you like the most is a thing all right let's come up with our own I wear them but they're really terrible algorithm there's no you know there's only the third lecture so all right so terrible I'm with number one so lean is always gonna write this down someone's gonna take a screenshot and put it on Twitter or something this is what Professor Weinberger's teaching yet yeah so here we go pick any age of art of age randomly randomly and hope it works well right can always evaluate it right you can basically see on the different data sets like you know does it seem good predictions right so you just put a random decision tree for example and just evaluate well that's clearly not gonna work very well and what's the problem right the promise well you know the Space Age can be very very large it can be many many possible functions you could learn and it's very very unlikely the one you're picking is going to be the one that's you know right for the data so you probably can you guys only expect me what would I see if I deploy this what would happen there's an obvious quite answer it's not very hard it's not a trick question anyone brave enough to say the obvious but it would be terrible but you would get a very high error right because babies like running a random program on your data right it would not spit out the right answer can anyone think of us an hour that's actually where they could still work yeah that's right so if your age is restricted enough right but basically what do you pick it doesn't really matter all that much that's very weird right but in some settings that could actually be the case so if that's the case then it's actually you have a huge advantage which is very very fast right so picking one edge randomly it's reasonably fast okay give me I'll give you another example here's another chair of algorithm that's gonna do much much better he has terrible algorithm number two here's what we do we try out every single age in this in my hypothesis class and pick the one that works best on the data so I go through my my set of functions that I could learn and try every single one and assume for now it's finite because for the program you know number of programs is actually effectively finite because we only have so much decision you know if you minimize how how complex they can be and then we use the one that's best what's the problem with that one why is that a terrible word yeah and what do you ever write like there's maybe more programs in that set and then there's you know elements you know whatever like electrons in the universe alright so you know we'll take you why so so that's not a good job at either so we have to have some way of taking this function out of the set of possible functions okay and that's exactly where the loss functions come so let me just explain it see if not functions and here the last functions do loss functions evaluate for any age class how well does that H work on my dataset so let me give you a very very simple like the most common example is the 0 1 loss and the 0 1 loss is called 0 1 because it's either 0 or as 1 and so here's how it works so we run this over a function H which we have want to test and then some data set D and we do the following we go over every single data points X I Y I in D and you take the Delta function we say H of X I equals to Y I who knows the Delta function ladies your hand not many ok there's a function what it does this is one from the 10 years over 1 for discrete variable it's basically the same thing it says if this here is true then output a 1 otherwise output a 0 right so this function is either 1 if H of X I equals y I 0 otherwise and actually so it doesn't take my notes it should be not equal that's the accuracy because as ice claimed its era okay yeah you want to describe in English words what this function measures yeah yeah accuracy and not anymore right because I just made the cost up but yeah if it used to be accuracy right so if I if I say if it's not the same and I count those as actually the number of times I'm wrong right but you're right like originally it was the accuracy right so Beasley but ever X I H of X I does not equal why I then I have a counter one otherwise that's 0 I divide that by the total let's say I have a hundred examples three of them I get wrong so three of them I have you know I have a one here oh the other 97 I've zero except me develop a hundred so three percent of my training samples are wrong okay does that make sense and so once you can see already lower is better right we want to drive the loss to zero that's why it's called loss that's a negative thing is it's a bad thing and the last functions are always non-negative so if you have a loss if you drive it down to zero that means you're making no more mistakes as that's universal that's across all last far so this is a convention okay so lower is always better and if you a zero then is if you you've made no more mistakes any questions about this alright let me give you another one square lots a squared loss so the square loss it's again a sum of all my data points and to the following I say H of X I minus y I squared than average can anyone tell me an example where that may be when that you know we want to use that one yeah deep linear regression that's right alright so for any regression problem really right so if I try to estimate a house price of something and I don't have a Z say well here's how much I predict that house is worth and the image is really worth right and so if I'm off and I Square this I may say well the more I'm off right the more I have to pay with my knowledge right and if it's zero bit of my exactly hitting it then I have a zero loss so then I'm doing really really well okay any questions all right let me do one more and the absolute loss it's 1 over N and sum of all my data points now I just have the same thing but I take the absolute value so again the same thing you know I'm trying to save is how close is my prediction to the true value and if it's a little higher or a little lower I have to pay for it can anyone think of an example where I may prefer the square loss over the absolute loss where I want to prefer this absolute loss over the square loss why don't we actually buy it I can give you a few minutes so justice I'll give you one minute discuss it with your neighbor and see if you can come up with an example [Music] all right any suggestions when would you yeah prefer one over the other it will and you want to you squared because you say if you get one wrong like really really wrong it would actually mess up the whole neighborhood it could be could be typically outlier proms go the other way so I mean you're saying it's busy whatever you saying the right thing saying that you know if basically getting one really really wrong all right basically square law Stars viewer off by a lot right you might have only one point is off by a lot that's a lot worse and having a lot of little points off by a little okay so you have ten points off by one they Eve give me a penalty of one if you have one point off by ten or ten squared is a hundred right so that's much much much worse right so we'll always make sure that everything is busy you know roughly you know there's no number that they're really really wrong and that's good except for can anyone think of any scenario when it's been that's really really bad any ideas so one month yeah that's right so one thing is for example I give all these houses here rather than Bill Gates decides to move here is Iike right if I was a mention for you know five hundred million dollars right and so all you will try to do with your function is get his prize exactly right right and if the student housing is off by a couple hundred thousand doesn't matter right it's not a big deal right well that may not be you know we all who cares about building a house in that case how do you actually basically what you're doing is many many people will have larger loss right we'll have an inaccurate prediction just for the sake of one guy who basically gets it right all right so in that case you may want to actually go to the absolute Lots all right another setting is if if some things could be wrong all right so that's a some some how this may you know some patient he someone sells their house to their brother-in-law right and they sell a big mansion for one dollar right because it's like some tax benefit or something or they you know people selling it to get you know in case they go bankrupt they don't want to own it anymore but they're really selling it to their spouse or something so now you have this when the tax record you actually have a mansion that's worth one dollar right and so the squared loss would try to make you know charge with everything to get this prediction right now I would say okay I'm really really far off i square this I'm off even more right and would screw up everything whereas the absolute laws would say well you know it wouldn't amplify that too much okay so it's a trade off actually and we will get to this more in a few lectures but yeah this was a great examples okay good so let me come up with so okay so learning is basically the process of you know you give the learning algorithm a function H and I choose choose an age little H in my hypothesis class all right so that's basis the idea and as the answer machine learning algorithms you say like you know I want to learn your networks or something and which one is the best new that's basically the learning process and these last functions are basically there that we can evaluate how well different functions are enabled guide us like you basically metalhead find some clever ways of cutting through this very very large set of possible programs the possible functions to be could learn such that the little age that you find has a small loss in our data set now it's all might come along and say wait a second if what we really want right is find a function that a small loss on our data set IV could come up with a really terrible idea there's terrible algorithm number three here's how we could find our you could define our function we just defined the following wave say H of X do the following you say if this ax actually equals x I oh sorry X I for some X I element of in our data sets so if there exists the data data point in our data set that matches exactly the input then we output Y I and other ones otherwise we just output y1 can anyone tell me what the loss of that algorithm is on the training data set so by now computer if I take this H and take any of these last functions on my data set and my training set now assume all my data points are unique so I have only X only X I only once in my train data set what would the last be of this I yeah zero yeah I guess you did arrive so he wouldn't make any any mistakes right so for every single data point in my data set I would look at what the label is I would busy find it in my data set and would output exactly that label right so the last would be exactly zero right so that seems like a great algorithm right are we done that's the end of this class in your home right we solve machine learning if on an algorithm that always gives us your loss no matter what we do that's the problem yep that's right it's a terrible error than why because if you stick in anything that was not in your data set and doesn't know jack right Oh which is outputs by one basis just memorized it's like the difference between people you know before an exam right they're just memorizing everything and then they write it on the exam it's not everybody but some people right then but then the new problem came along they don't know what the answers right so that's exactly what you're trying to avoid it's memorization yeah and so we what you're saying is our chain data comes from some distribution and our test data should come from the same distribution and how do we know that's actually the case and so that's trust that we have in the data scientist that's right so we just by the ways you you had adhere taxes all right yes or that is a problem there's a very big problem actually and I give you a famous example again of someone who made such a mistake actually was the the US Army actually they trained a classifier that classifies between civilian vehicles and military vehicles and so what they did is they it took pictures they sent some on ours take pictures of military vehicles so the guy just drove around base and took pictures of tanks and jeeps and stuff and they drove it down you know downtown and took a pictures of normal people's cars some civilians cars and trained a classifier and they realized they got zero percent error and so they fade really good it was like awesome and we can use this right we know targeting and all this that's awesome right so then they actually you know fed really good for them about themselves for a couple days and then they deployed it and turn out it didn't work at all that's fifty percent error it's like one coin toss right didn't work at all the question was what was wrong well what they did is the guy first actually drove around base and took pictures of military vehicles and then they see a few hours to do a lot of pictures of civilians of the civilian car so at this point the son had said a little bit it got a little darker so all the only classifier there is like bright images where military vehicles and dog images were Servilia cops right and it worked really really well the defense made an example of the distribution not being the same right that's just you know that was sample from a very different distribution so yes you have to be careful okay good so there's this problem but you know we still have this guy here so this terrible algorithm number three there's memorization would do would it give us zero loss so if we only use these loss functions to basically measure how well we're doing on our training set this will still seem like a really good idea right gives us your loss so we have to make sure that we don't arrive is something like this and actually one thing you will see later on is that most machine learning algorithms are very much capable of memorizing the Train dataset and that's okay as long as they still generalize so in this case this doesn't generalize at all and so let me tell you what generalizations any questions by the way any questions about this all right generalization and here's what we just went wrong or you would like to have is a function H and you know H such that for any X and y drawn from the distribution P so in case you don't know that notation that's this invert a upside-down a means for all so for all X and y Johnson's diffusion P we want that H of X is roughly y it's very often by I'm not going to formalize it so what we did is that this memorization algorithm only worked for data data points in the air in the data set D but did not work in the general case if you have any X and any y Jonathan P it would not work and this is what we call the generalization so what do we really want right what we have here up here as we measure the loss on the training data set so we took the sum over all the data points in the training data set for each one of them he said this our exes are why there's a prediction on X there's a prediction it is our true labor why are they the same right so either in the absolute loss and the squared loss or are they literally the same that was the zero one loss all right and now what we would really want is something else you would want for any x and y drawn from this distribution P which is elusive distribution P which we don't have access to for those we would like to have that a H of X matches fine so what we would like to have is the following that if we draw if you take our function H and we compute the loss on any x and y that is drawn from this distribution P the expected loss should be very small and this is called the generalization loss let me go through this one more time DeBakey saying what's the expected loss of what's expected value of this loss function if it's applied to a to any x and y drawn from this distribution P so if you would go out there and take a new data point x and y I'd stick that in your function asked to compute the loss what would be the air so what would be the value what's the value that you would expect the probability here is over these xn by Jonathan T ok any questions about sorry ok one more time so what we computed here up here is basically the average loss right if you so this is basically let me just this is the average loss of all data points in our chained innocence we summon over all our data points in the train data set and we say what is the loss for this particular train eight point and be just average then okay can all the view that says you know essentially you know if you would pick a point randomly from your train that I said what's the expected loss all right so here's something else you're amazed he's saying so that's good right but that here we can run to the memorization problem that be Bayes the only do well on the train data set but we don't do well on data points that are not in the train set so what we would really want is if you draw any X&Y doesn't have to be from your training data set but it has to be the same type of data that isn't your training it has to be drawn from the same distribution as the data of which your data set is made up I took any one of these X&Y stick them in my last function then this H should have a low loss right Alyssia basely computes the loss that you would get in expectation grazie handed that makes sense okay so this is great why don't we just minimize that and the answer is we cannot write because we don't have this P and so we can't possibly this is all you would like to minimize but we can't do it because we don't have access to P all we have is these measly and data points drawn from P that's all we have so what do we do and here's the good thing if when you one minute you want to hear the good thing all right I'll give you the good thing in one minute and the good thing is we can estimate this as we can't compute it we can approximate it how do we approximated here's how we approximate let me get the data set this is my data set like you can imagine like these nothing x1 y1 x2 y2 etc I take my data set and I split it into two portions training and test so the moment I collect the data I split off some part that I call a test set and the rainy part of the training set and this data set I put on some d0 disc that I put in a safe you know balled somewhere in the basement right no one has access to it and then this part here gives us data eight data scientists I mean the data scientists now learns an algorithm and as far as the data science concerned he doesn't know he or she she is she doesn't know about the test set right so he or she just knows this training set and he or she now trains the algorithm to do well on this data set and maybe you get really no last year that's very encouraging the moment the data scientists come back with some function H we can test how well it really does how do we do this you apply it on this data set okay does that make sense because Lena sine has never seen this data set this is a fair estimate right this is sampled from the distribution P we know this because the data a sample distribution P so then basically we have an estimate of how well we would do with you go in the real world right and I do this literally right survive data set I don't give my grad student everything all right so I keep something and then you can always evaluate later on all right so that's this year is to estimate what we really want and this years to guide our algorithm and if you do this then you would actually realize that the memorization algorithm is terrible algorithm number three wouldn't work right it would give you zero error last year but it could be a really high last year all right so now you know it doesn't work okay so you you file your data scientist you get a new data okay good and we pick up on that on Wednesday 
","['', 'machine learning setup', 'data set D', 'n data points', 'feature vector', 'label', 'distribution P', 'Gaussian distribution', 'real-world application', 'Nokia face recognition system', 'training data', 'test data', 'sparse vector', 'dense vector', 'image representation', 'convolutional neural networks', 'machine learning algorithm', 'prediction', 'generalization', 'memorization', '']"
"and one thing I want to reco warn you so once you post the video recordings there's a temptation of students to say oh you know what I'm just gonna stay in bed and I'm gonna watch them at home it's pretty boring to watch lectures at home so most students who want to watch them ultimately then don't get around to doing so so it's better to go to lectures but you're welcome to watch them online someone had a question in the front No okay it's resolved all right good another thing is so at this point everybody should have received an email either invitation that you were to Bo karyam or an email that maybe the exam didn't go so well and their combination is that you should drop the class and if you have not yet received an email please contact you know why--i Piazza contacted TAS so that means either please also check your spam folder the VOC areum invite is sent automatically through karyam so it looks like spam so please make sure you find it by the way please all close your laptops come on guys it's the one rule I have no laptops all right no iPad either all right any more questions about velarium placement exam or anything else yeah right unfortunately the way it works is you have to first is to the partner and then you decide on the project that then you start with the project you cannot reach we saw later on that then the reason is because we have had problems in the past but people put people on as partners the day before or something you know as a favor or something you know it just gets complicated the best there's a clean solution the ever ever the title and you have a short description of the project that you can look at okay any more questions okay good so last time we talked about the general setup of machine learning we have data but then a set D that consists of pairs of basically vectors feature vectors and labels and our goal is to find a function you know that from a vector X predict spot right so if you have X I and outputs y ideally let's see the vector and this function H is chosen from a hypothesis class the hypothesis cast said of all possible functions that we consider so someone at the end asked me what is this curly H seems very abstract what is it and so let me give you an example so I showed you a little teeny little example of a decision tree right decision trees who want algorithm but you basically split on a feature you for example say is the person male or female right is the first how old is that person you know age is the person you know over 21 yes or no right and then the ng basely you may can make a large tree like this and at the end you make a prediction saying yes the patient will return within 60 days or no somewhat you know you could ever know or something that's miss so in this case the capitalist curly age will be the set of all possible trees you could imagine right so it's actually an infinite set because you could I guess you know well actually the finite feature said actually this finite it's very very large I'd larger than you think even larger right it's very large so but you know that doesn't matter just have to find some algorithm that finds the ride age out of that okay they ride little H okay so basically what we're doing is we saying by choosing curly age all we're doing is we're saying oh I'll be using decision trees right that's basically that's what that means and then we try to find the best one that is learned on that date so the learning is basically choosing this one particular instance of all possible functions out of the set of functions any questions about this raise your hand that made sense okay raise your hand if I'm speaking too fast Billy all right I've watched my own videos like Bao I'm talking fast but I'm trying and trying to slow down ok good so then we said right great right we have this data fed and we try to find the age that has minimum loss on this data set so loss what was the last function again last function was some function L if you stick in an H and we stick in our data set and it tells us how well we're doing right so baby says this particular function age has the following error makes so it make me know makes that fraction of that percentage of examples indeed it gets wrong right oh that's the average error we are making in predicting house prices or something and we would like to be at this function L be small so ideally the idea of machine learning is we try to find an age that minimizes this function all right you remember this crazy and you remember it all right good good good so then he said okay well that's great right we came up with this really cool algorithm that gets zero loss all right and that was the terrible algorithm 3 and what it did it just memorize the data set and when you stick it in ax it just looks up the X in the data set and then outputs that Y all right and that gives you zero loss because for every X Y all in the dataset you output exactly the right way it's great all right we're done so what's the problem with this the problem is that only works on data that is from this theta so it doesn't doesn't generalize and so that led us you know to the next notions that what we really want is actually not a function a function H that - Alhassan this dataset what we really want is a function that minimizes the loss on new data and how do we write this if I this what we really want to minimize the expected loss if we were to apply our function H on a new pair of points that are drawn from the distribution P from which our data set was drawn so basically if we take two new data that if you take a new data point X with label Y which we know but but what would the air what would the last be and that's great that's exactly what we want right this time you're right this is exactly what you want that really means this is 0 that means if you get a new point you get always get it right it's awesome the problem with is is we can't possibly compute it because we don't know the distribution P the distribution P is this elusive distribution from which our data is sampled we don't know where the data comes from and or at least we don't know what the distribution is and so we said well it's elusive you can't actually compute this but there's good news and the good news is we can approximate it and this is exactly what we left off last time we take our data set D and B cut it into two parts and we call this part here D training and this part here D test it there's a deep and what we do is the moment get this data said we should chop off this D testing part and be copied onto some disk and be buried somewhere underground we don't tell anyone else where it is so no one's allowed to ever look at this and now we train our algorithm Prime you know only on this this data set on the training data set and then ultimately if you want to know how well it really works we compute the loss on the test set all right so that's the key and because that's basically you know it's an unbiased estimator of how well you know of the actual real last I be trying okay um there's a few pitfalls and that's very important is you kind of split it any way we want and actually a lot of grave mistakes of machine learning have been made but people splitting between training and testing the wrong way so tell you one example so I used to work in industry and one thing our research team did is we put a spam filter an email spam filter and so it was one day where one one of us may have been me I'm not saying what's really really excited right because you know the new algorithm got 0% training 0% testing here so the evaluating algorithm here you tested yeah you got every single one right not even like spam is soft right awesome take this stammers but what was the problem the problem is I spit split the training set randomly in testing and training but turns out that spam is very very similar all right so people send out a spam message not just to one person but to one hundred thousand people so you have the same email many many times so if you split randomly every single email that wasn't a test set was also actually in the training set in one form or another so our algorithm basically learned do the right answer for all of these then it's all the same emails again essentially and got zero so the right case and this because there's a temporal aspect to your data is to split by time say like when was the data collected and you have your time axis here this time and you say all the you know data collected up to Thursday comes in the training and Thursday to Sunday goes into test not something like this all right and then it's an accurate setting because you can no longer as you get no information from the future okay and that's actually essentially what you want but you want to change the spam filter that's trained today to work tomorrow that's the idea so if you have a temporal component you always have to spit on time if you don't have a temporal component and the data is really sampled iid then the right thing to do is shuffle them randomly right and it's a good exercise to always do this we've seen I've certainly seen cases one except you know a case where this was not me this time but was someone else where the problem was that basically the data was just stored in a way that first we had all the points with label 1 and then he had all the points with label 2 and some ones have split it and so all the tests that was all the same label so that of course doesn't give you an accurate estimation of the test so if you if it's iid data with it you know uniformly at random if it's tamper all data split at some test point as always I'm done time point and everything the future goes in to test those other cases and medical care uh one mistake that I've often seen is that people take data set like multiple data samples from each patient and they sample those uniformly at random of course you can't do that either you have to move the entire patients some patients into tests and some patients into training the idea always is you know you train on a set of patients then you test on a set of patients so the important thing is be really really careful when you do this but if you do it right it isn't it's a great estimation of your test there any questions none all right so I guess let me just I guess you have it right in front of you right so the two points you know that I really want to make is uniformly random if data is ID and by time if there's a temporal component very often okay well very often people do one additional thing and practice they don't just split into training and test they make a second test they actually split into three sets they call this validation anyone have an idea why you may want to do that yeah that's right that's right so if you actually have many many models right and you train them and you only have one test set right so here's one thing that's really really important if you evaluate a model on the test set that is an estimation of how well your test is so how about you generalize about your generalization error is but only the first time you do it if you then train another model and you get a different test error and now you pick the better one of the two actually now you made a decision that there depends on these particular data points and that actually may no longer be reflective of the true error let me give you an example so let's say we have face recognition data set and we are a critical classifier you get 99% correct so maybe in this test set you only make two mistakes right so now we can tweak my algorithm retrain it and now I only make one mistake on this test set so I got a half my error right that's amazing all right should I really pick the second algorithm of the first maybe it's truly better but there's also a good chance I'll just meet some tweak that just get that particular person right right and it may not actually you know be be true in general right and so let's say I try a hundred algorithm 100 algorithms right they're all a little different they're all around you know making one or two mistakes right if I show you many many algorithms it's got to be one of them that coincidentally makes no mistakes and if I pick that one it seems like I'm doing really really well but actually what I really did is I just over fit it right over fit it to the test set so I made a choice that really probably only applies to this does that make sense so in certain some sense if I look at this data set too often then I'm also actually training algorithm on the test set and I'm losing the fact that actually it's unbiased any questions yeah so here comes the comes the point so if you have the right agent set what you really do is you take your argument you take your multiple algorithms you evaluate them on the data validation set you pick the best one on the validation set and then when you finally find a champion you value it on the test set and that's the error you tell your boss right because that's the true error that you can expect entry but area life right and usually what you see is that the training error is much lower than the validation error and the validation error is lower than the test error right a little bit because you made some choices based on the validation set it's not just actually different models you know often people also have parameters right so for example if you have a decision tree how deep should the decision tree be right there's some trade-offs you'll learn about this that's the kind of stuff you decision you make on the validation set but then the key is on the test set only you can only evaluated once you write a paper and be really really strict about it right so people are very annoyed about you know when and you publish an algorithm for example and then it turns out actually you know you evaluated many times the test said and you actually pick the algorithm that's the best because then but you know people notice that immediately because you take the algorithm apply it on a different data set and you get a clearly higher error so it's easy and it's problematic any questions yeah right and it will happen on the prediction set right but then you still take the best algorithm or you still stick whatever algorithm you think is the best and apply it on the test set and now you finally get it you get a true answer of what you're a true estimate of what your generalization error oh isn't that net it's true that it's absolute true and so one thing people do actually you know people talk about this later on you can actually change it once but this is training in this is validation and then train again with this is training in this is validation or something so you make different splits all right to get different violation errors and people talk about this in more detail but yes that is a problem and actually one thing that lately has become a fashion is a compute the error on validation and then actually add a little bit of noise - it's like perturb it a little bit and basically say well there's a little bit of overfitting so I can overcome this by adding noise we will talk about this in a few lectures yep so so you can reuse it so here's what you do you train your data algorithm on on training and then you evaluate it here and then you try a few different models right and you valued here okay once you pick the best model you retrain it one more time on the entire data set and now you test it here but now you stop now you throw away your computer right you're not allowed to touch a do anything more because you've used to have the test set anymore yeah the bay in the test set yes the test that is when he's done right I mean one thing you can do that what we did like 49 was an industry we had a test that on spam data be evaluated here the moment we used it we already collected the next one all right so he always had a new test set that we could actually use for about it yeah so if you have a temporal it a component and your data then you have to spit it by time oh if you actually have time series they actually oh then it gets a little but usually it depends on the case on the other setting but the one thing you have to be careful of is that your algorithm cannot get a glimpse of the future because in the actual test setting you would not be able to do this and that's that's often a pitfall that you actually realized oh the algorithm used some information that I got from the future yeah wipe the memory oh oh I see so you basically want a glimpse at the test set couldn't just you run it again so the problem is not actually Iowa and the problem is you all right so because you glimpse that the test set that informs you right it's gonna be really hard to say oh I totally ignored this now afterwards I made you know like that the fact that you know this right you learned something from this you learned that one set of algorithms works but a new networks like better than to a and decision trees and that actually will change your decisions in the future and at this point you are cheating equals right so because you actually you make decisions you know based on the test set so now if you evaluate again right you basically is using it twice but you first looked at it and said what's gonna work well and then you say oh look here this worked well but does that make sense yeah yeah oh yeah that's right so so you can make many more divisions that's actually we will get into that and we will discuss this formally and then we get into the how you you know all the little details yes that's a good question so the validation Sado the test data should be not too large and not too small and that can I leave it so that's usually people do an 80/20 split and it depends how much data you have all right so it also depends on noisy your data so usually then you want to look at the variance and so on and you want to see yeah if it is attested is that it's very very large and you get a very good estimate of the translation error but also you burned a lot of your data that for training all right so that's basically your trade-off and these are the tricks you can do and that we can we'll talk about in a couple thanks yeah by the test set you basically you know you look at it and then you poke you eyes I don't do it but yeah then you're no longer allowed to look at it so unless you basically say here's my own thing in my outcome is the following right a bit of a basically you know I haven't I'm comparing these algorithms and these old you know what I'm trying my my point is that here the errors that they make on the test set then that's fine alright on the test set and then actually conclude that's right and then keep keep iterating over this right so that that space the point right does that make sense so as long as you as you lay open everything you did right he said I looked at the following five algorithms here random all the tests said here the five of them that's fine right because ultimately you didn't make any decisions you didn't change any behavior afterwards right there's something said but if you try the first algorithm and then based on that you just saw a second algorithm and a third algorithm now actually you're adapting to it and the moment you adapt to it you you actually overfitting yes yeah right right so we will get to this so one thing you can do is here for example let's say you have you know I wrote them on problem once but we had 11 data points right suppose a medical trial it wasn't a try but what was you know experiments was very very expensive each data point cost a lot of money and involved taking MRI scans of people and so on and so what we did is basically you know we can only leave one person out of testing right now that's that's weird right because either we get that person right or wrong so you get zero 100% but what you do is you basically always train on 10 and leave one out and you do that 11 times and then you average and we say the average error is the following and that's fine that's totally kosher you just have to describe what you did does that make sense okay one last question on this because there is more to machine learning yeah oh there's an art there's this a German art they look a little different all right hey so I said again give you a function that's right so here's what he did you try some some age and you get a function Rhino here right and they say well that's not good enough right your boss tells you you want to deploy a spam filter and you better have no more than 0.5 percent error all right do you run your thing and you're doing point you're doing okay point seven percent error but it's not good enough she lying darn right let me try something else as you try some other function you try more complicated networks etc run again now you get point six okay let me tweak it a little more right until you get point four and they say okay now I think I've got it you tested here you get point five awesome does that make sense okay good let me move on [Applause] at one quick quiz okay let me just let me just say this so if we have let me just say it and then you can because I don't have the answer on the sheet so if you have the loss on the test say it what is this there's the sum of all the data points in the test set yeah 1 2 n loss of H comma X I comma Y I the question is why does this as n becomes infinity becomes exactly this can anyone tell me why that is so I claim that this year becomes this as n goes to infinity so Bayes II know that's that's really what we want right saying that the test that is really large then group your computer is exactly this maybe give you a minute to discuss it with your neighbor why is this converging to that [Music] all right who can tell me why and what is the statistical law yeah that's a weak law of large numbers so that big law of large numbers baby says that exactly this right the average of random variable the chance the expected value in the limit all right so remember that's the weak law of large numbers says exactly you know busy.we assures us that what we're doing is right okay any questions okay good so by then if I don't see you you raise yet there's too many people here so please say something right I just you just say anything you want and she said this last time is that last time I taught this class that you can say anything you wanted one one one student said like viagra we talked about spam filtering so it made sense but all right so what have you talked about now that we have talking about the data set D the feature presentation so you take your data represent as features you have labels you're on a function H they goes from X to Y we know we have to split the data so that's all really really good so one thing I haven't talked about is how to find this hypothesis class H right and here's a great idea why don't we just you know during this course figure out what's the best algorithm let me just only use that one well even better why don't I just figure out what's the best algorithm then I only teach you that one all right and then we call it quits why does that not work and let me give you an example on this example is actually is a party game are you ready for a party game oh come on what kind of party is this all right okay so here's my party game I give you a bunch of data points and instead of training machine learning algorithm and I want you guys to be the Eiger okay so I I give you a bunch of points this is my training it this gives my ex and this is now why so tell you for this x value for this value if I the answer is this way okay and I give you a bunch of these points okay and that's our training data and now becoming test a let me look at tested oh I'm gonna say I look at this point here that's here let me let me caught it make this more complete one two three poor so where do you think is the world value at exactly this point of X who wants to volunteer any brave people yeah around to ah sorry totally off its for how about this point here for oh come on SWAT anybody else yeah tae-il close Thank You negative so I know what you thinking right your parties suck vana so what's the problem right what's the problem like oh I can't you learn that's right you guys are terrible at this I'm sorry but hey Mikey the function looks like this actually Zhou here went down negative so what's the brach right so you made an assumption I mean you saw these points of it assumption you mean assumption that the function smooth clay most of you did this and that's the key to machine learning so if you have an adversarial setting right if he actually had some setting where basically I try to predict something but you have some adversary that was me in this case and I could always come up with some other answer machine learning is not possible right I would not know that it's I mean you have to assume that the world is nice I know you have to make some assumptions about the data set otherwise we cannot make predictions the whole whole process of prediction is you know the whole point of predictions is that we assume that basic things keep you know behaving the way they did and different functions or different algorithms make very different assumptions and the no free lunch theorem is we call it a mission Basel II states you can actually formalize this but this there cannot be any function that's the best for everything because you have to make some assumptions otherwise you can't do well in prediction and then I can generate a data set that violates exactly these assumptions does that make sense and therefore my algorithm will do arbitrarily badly okay so there is no one algorithm that as well as everything it cannot exist and I'm just I'm not formalizing this lecture I'm just giving you the intuition and a big part of this class is to understand what assumption each algorithm is making I didn't a big mistake that I see and people apply machine learning in practice is that the Bayes they have one favorite algorithm that they like right for example it's deep learning or it's random force or something and they're just loved it isn't that algorithm they use it for everything right every respective of what assumption that makes if these assumptions actually hold in that dataset so what I'm what I want you to get out of this class is to you know sharpen your vision in that respect and look at data sets and first analyze you know what are the properties of this data set but this particular theta I said what assumptions can I make all these assumptions do this is I'm so Holt which algorithm leverages these assumptions and makes good predictions and that is the most important most important aspect about machine learning and but most people do not do right actually so any questions about this yeah so people so there is algorithms that you it's called meta learning and basically what you do but essentially what you're doing is just what a data science system would do if you if you don't know what assumptions you can make then basically what they do is they spit up a training in a validation set and the test set so they take their training and validation set they try out a whole bunch of different functions and just see which one gives the best air and validation and then they train that on test so that just a purely empirical you know way of doing it but if you think a little bit enough about it then typically it helps a lot to kind of rule out algorithms to begin with if you say well they make this assumption that assumption this does not hold in this data set all right so don't waste your time good question any more questions all right awesome so that means we can actually move to the next topic and I can talk about the first algorithm I said let me do me give some to the middle thing well I'll give you some more yeah these to you [Music] okay who does not have a handouts ha and can it somehow make a cross you know to the upper half of the classroom yeah maybe alright well I can already get started ok the first algorithm is also one of the oldest algorithms in machine learning by the perceptron it's called K nearest neighbors and it comes from 1967 is that correct yeah 1967 like over in heart all right so let's say we have data points this here's my data by the way can you see this in the back raise your hand if you can see this okay a third table you have data points and we have crosses and so these are I just have a binary classification problems I have crosses and higher circles and have a new data point let's see this is my new data point I would like to know is this a cross or is this a circle what would you predict anybody it's a cross whereas a circle it's a cross that's right and why do you think it's a cross it's very close to crosses that's right and that is the assumption that nearest neighbors is making so now given that you know we just played this awesome party game you now know that the first thing you have to do is when you look at the algorithm is thinking about what assumptions is that algorithm making and K nearest neighbors makes the assumption that data points that are similar have similar labels it doesn't have to be the case you can easily think of a data set but that's not the case at all but if that you know that's the assumption we are making we say data points that are very close together are likely to have the same name and so here's how the algorithm works in a nutshell you look at this data point that you don't you know this is your test point you don't know what the label is and now you look at the K nearest neighbors with K could be 1 it could be 3 some usually want to have some odd number so that's a vf3 this one that's one and this one so you pick these three years neighbors and nearest neighbors in this case all three of them are crosses so you let them both this guy votes cross this guy votes cross this guy let's cross so unanimously decide it's a cross if the test point was here and you would actually have these three as nearest neighbors then you would say okay I have two votes that it's cross and I've won both at it's a circle so I'm still saying it's got maybe less confident but I'm still saying okay does that make sense raise your hand if that makes sense all right so it's a super super simple algorithm that's surprisingly effective let me formalize this algorithm so we have a test point X and you have a data set D so we look at the set of K nearest neighbors so how do we define this you say you have this data set s of X which is a subset of D and s of X such that the cardinality equals K so we have to have exact exactly K data points in there k plus 1 is too many and now we say for every other points X Y Prime that are in D but not in s X so every point that's not in the set of nearest neighbors what do we have to have you have to have that the distance between X and that prime X prime must be greater equal the distance the maximum distance for any point in as X to our test but one more time so what are we saying you be saying if you take any point in our data set as X and compute the distance to our test point take the maximum so the point that's furthest away every point that is not an as X has to have a distance that's larger than that so in other in colloquial English what we are saying is if you're not one of the K nearest neighbors then you should be further away than the case nearest neighbor crazy Hannah that makes sense awesome okay good stuff yeah well you could have pointed exactly the same right but you can only have K nearest neighbors right so that you have to choose it's like it's like choosing between momma and Poppa or something and what're you gonna do it's like like them both all right so we have a few more minutes okay so the key about nearest neighbors okay nearest neighbors so they key behind the genius neighbors is that it's only as good as its distance metric okay so if the distance metric reflects similarity and labels then it's a perfect classifier right so from the distance you basically say you have a zero distance if the label is the same and the distance of one of the label is different well then it's perfect because the nearest neighbor will always have the same label as you the nearest neighbor will always have the same label as you so the key behind nearest neighbors is to use a good distance metric and this whole field of metric learning that actually says well we can actually learn these metrics so you're not going into this but the most common metric that people use is the mimic cotton mill minkovski Minkowski Mankowski so its finest and she's ever even kelskiy I where's he from Russia so we go over all the dimensions and we take the odd this here's now the earth dimension maybe I should write it like this the earth dimension of X minus the dimension of Z to the power of P 1 over there's the minkowski this is most commonly used now a very quick quiz for you guys three special cases P equals 1 B equals 2 he becomes really really large I claim these are distances that you're well Avera discuss it with your neighbors with your favorite neighbor maybe two and what are these three metrics [Music] [Music] all right who knows the answer first one P equals one what is this that's who's brave yep Manhattan distance right P goes to for someone else yeah Euclidean P goes to infinity yes something yeah sorry they say one more time nada and I don't understand it unfortunately yeah maybe could be yeah it's the max that's right it's the max its max so the max the max of what it's a maximum difference of two dimensions right and why is that well I'll think about it right P is now very very large right very large so now we we take all these these differences and raise them to the power of P now to imagine one of them is a little larger than the other all right so we take the difference in these different features right so what is the difference with a age difference in weight or something let's say the difference in age is larger the difference in weight you raise it to the factor of a thousand right so let's say you have you know the difference in age maybe thirty two to a thousand plus the difference in you know weight maybe six to sixteen over 15 mm well this is so so so much larger than that right that doesn't matter at all right this whole it's completely dominated by this right does that make sense so there none take the whole thing to one over thousands or that's basically just 32 I mean this is this is epsilon this is so tiny compared to that thing it doesn't do anything does that make sense so as P gets larger and larger this effect goes it becomes more and more pronounced than essentially you just have no matter you know we basically just have two maximum difference yeah well the turbine why does it become this the same way I don't understand it sorry why are we I might be comparing to my zippy oh because Paisley okay so he is like why are you doing this and the answer is there's the Minkowski distance and the nice thing is if you have an algorithm that implements this right just with different values of P you get a whole family of different distances right and that includes the common distances that basically people think of and so the you know they are Euclidean distance and having distance all right I'll just taking the feature that you know they vary a lot of one feature and that actually is the most important does that make sense and you can you can't take any value for P or any non-negative value okay so that's that's that's one reason why it's so popular it's because it's actually very fairly general all right I have a short demo but you know what I'm already one minute over so why don't we start next time in the demo but can you someone please remind me that I haven't shown you yet the demo 
","['', 'Machine learning', 'Loss function', 'Generalization', 'Expected loss', 'Hypothesis class', 'Feature vectors', 'Labels', 'Decision tree', 'Training data', 'Test data', 'IID (independent and identically distributed) data', 'Temporal component', 'Validation set', 'Overfitting', 'Generalizability', 'Minkowski distance', 'Manhattan distance', 'Euclidean distance', 'Weak law of large numbers', '']"
"alright ok good so we I hope you enjoyed the last two lectures actually do this every year that I have a little bit of a math background this time I was out of towns of my jaikant Iran was a nice and sub for me I know some people knew all the stuff but for every person who knows everything that's probably five people who you know forgot something and it's certainly valuable because we will dive into statistics very soon project 1 and project 0 are both still out project 2 will be out very soon so please if you haven't started as project one yet don't fall behind right so the projects are overlapping so don't that all accumulate any questions about logistics will carry him or something I know some people just joined today today's last day of adding if you don't yet have a kerime account please that the TA is no on Piazza and hand in your placement exam alright so we were talking about the K nearest neighbor classifier and if you remember correctly the assumption that we made was although the assumption the K nearest neighbor classifier makes is that similar points have similar labels right si idea is very very simple you have a test point and what you do is you just look for the nearest training point and you just steal the label from the nearest any pawn you assume well if it's really really similar then you know it likely has the same label as the test point it's a fair enough assumption and then I was like well that's great let me make sure you meet a little proof when we show that actually if you have enough data if your data goes to infinity and goes to infinity then this classifier only can it most have twice the error rate of the Bayes optimal classifier which is the best you can possibly do alright so that's a great sign so that's all very promising however then that's the upside then comes the downside and that's the curse of dimensionality and let me just go through with it over this one more time maybe the curse of dimensionality what we said is but basically look there data that was drawn from a from the unit square here or that you know a hypercube so this is old distance one and this is of course already three dimensions because it's hard to draw D dimensions but these really large but we may see a Schumer kV draw data uniformly at random from you know within this cube within this hypercube every length no edge has length one and so then the question was well imagine you have any point in here and I would like to know what's the smallest little cube that encapsulates this is length l and it claps capsule if the K nearest neighbors of this point okay so this is in some sense you know you find the key nears neighbors you draw a tiny box around it and one question we looked into is how big does this little box have to be all right and well the the math was quite simple well we can't compute the volume of this little box just as L to the D all right D dimensional space and have a cube with length L and we know that this this little box contains K points out of n well these are uniformly distributed we can say well roughly you know this is K over N that's roughly the volume over all has to be the same as the ratio of the points right because of the uniform distribution and so far do you remember this is a raise your hand if that's that's clear okay awesome and so then the question was alright you know if you have this relation we can now solve for the size of L and we get L is roughly K over N to the 1 over D and we can now solve for this let say K is 10 and n I think we said to a thousand so this is no 100 to the power of 1 over D and we can solve this for several values of B and here comes the shocker for different values of D when we have D equals to the is not a big deal as 0.1 so roughly this out square right you know the box about that big and four D equals ten is 0.63 so this means the box is now already that large and for now D quotes that take you know 100 or it's 0.95 five and thousand it becomes 0.995 four so basically when the thousand dimensional space we have this box from which the data is sampled and for any given point if you just look at the K nearest neighbors right you say how much space does this box contain that contains only my K nearest neighbors it's essentially in the same size so what does that mean all right so think about this why and why is this so troubling right why am i why am i making such a big fuss well think about all right so the basically means that this entire space must be empty right so here a thousand points well ten points are but you know this whole entire interior there's only ten points right all the other 990 points are you know just you know here yet right squeeze between these two boxes all right right why is that so problematic it's so problematic because think about it what the fact that this little box is so big can only mean that basically these K nearest neighbors where are they well they're also here in the edges right otherwise you could draw a smaller box and the points that are not the K nearest neighbors where they're where they they are between these two boxes right so let's say here's my point well the K nearest neighbors here and the points are not the K nearest neighbors are right next to them right so the K nearest neighbors are not close at all right there's no notion of closeness right they can't be close they're really far away at the edges all the points are really far away at the edges right all points have roughly the same distance from each other they're all at some crazy you know it's crazy distance you know they're all of the edges are really far away from each other it's somewhat counterintuitive our brain is just so made for three-dimensional spaces and three-dimensional spaces things behave very differently three-dimensional spaces you know the drop points randomly they would be in the interior in high dimensional spaces that's never the case that the interiors empty there's never anything there everything is far away from each other and that means that our assumption of K nearest neighbors that nearby points of the same label is nonsense right because there is nothing nearby everything is far away from each other and everything is about the same distance far away from each other all right so that K nearest neighbors are here but some other many many other points are right next to it right in the difference in distance is nothing all right so it seems unreasonable to say you know here the K nearest here's a nearest neighbor and here's another point and clearly this point should have the same label as this guy but not this guy i despite that they actually have no real you know they're not significantly further for a far apart let me give you one more intuition why that's the case so why is everything so you know at the edges think about the following way think about it like if I draw a point randomly in this cube right if I draw it uniformly demandin what am i doing I could just draw every single edge every single coordinate independently right so I first draw this chord let's say like what do I ran and up here then this coordinate and up here this Gordon appears then I kind of take these three directions and now I have you know I have a point okay does that make sense crazy hander that makes sense okay all right so now I'm saying I'm doing this over and over again we have a length of one and I'm drawing drawing a chord BC I take one chord enough the other and I basically draw you know some numbers you're uniformly at random between 0 and 1 in order to be in the interior I can't be at the edge so what's the probability that I'm not at the edge well let's define the edge as saying it's epsilon apart from from you know from the 0 from the 1 ok so this is my interior which is 1 minus 2 epsilon and the edges is this epsilon and the these are my two edges okay and if you look at this if absolutely small it's very very likely that I end up in the interior so that's why your intuition is the low dimensional spaces if you draw some points randomly in a cube you end up in the interior but now comes you know the change is that if you actually do this in D dimensions and D becomes large so what's the probability that I'm ending up in the interior well it's exactly 1 minus 2 Epsilon okay that's the probability that I end up in the interior that's in one dimension what's the probability that I end up in interior in every single dimension it's 1 minus 2 epsilon to the power of D right because in every single dimension I cannot be at the edge because if there's a single dimension which I'm at the edge or now I'm an edge point okay does that make sense raise your hand if that makes sense awesome well what is this this is less than 1 right and any number less than 1 to some large power heels to 0 damn quickly so this is you know very very quickly to the 0 that means you can never the probability of hitting the the the interior is basically zero any questions yeah thank you why I did it all right I'm just telling you this doesn't work right high dimensions doesn't work but then I run ten years names on pictures and pictures have like thousands of dimensions right so what's going on and how can this be possible right what's happening well the key is that pictures are not uniformly distributed so they actually a very very different to stupid and so here comes the key so in general if you make no assumptions about the space and you have high dimensional data kenya's neighbors will not work but it could be but you have high dimensional data so your high dimensional space R is high right but actually in this high dimensional space lies a subspace it's much smaller and your data only lies in that subspace and you never actually draw any points that are off that plane you could for example imagine you have a two dimensional plane that's embedded in a thousand dimensional space well then K nearest neighbor still works because your data is essentially just two-dimensional you just have a very high dimensional ambient space doesn't matter that who cares if that's the case you're fine so the assumption of K nearest neighbors is you have you have a low intrinsic dimensionality that's what it's called and so you have that either if you data lies in a subspace in a low dimensional subspace or it lies in a low dimensional sub manifold so what is that right you could have in a high dimensional space but your data could be you know there's a surface up here that's curled or something let me draw this so here you have the surface like this and the surface may be you know ten dimensional but you actually in a thousand dimensional space and the surface can be curled up in anything so you really need the four thousand dimensional space to represent your data in this case you wouldn't you could just project this on the two dimensions you're fine all right so seems silly to have this in a thousand dimensional space but in this case you may need the whole dimensionality because this the surface itself explores the thousand dimensions but the data never leaves the underlying manifold which is actually low dimension right and that's the key and because the manifold is no dimensional well let me just explain what money I don't even heard of manifolds before raise your hand if you've heard of n the manifold is basically mu there's a surface that is low dimensional and has a rehmannia mit has many different definitions but the one we use is basically that you it has two properties number one it's locally Euclidean so if you were a little little ant right you live here congratulations and now you move around right you would have no idea that you that this is actually a curse place right if you just look at this it looks completely flat right which is actually by the way you're laughing but actually that's exactly what we do right so we humans live on a big sphere right which is actually a mannequin right and so this is why for centuries or millennia people were convinced that the earth is flat right because you have you a little tiny human on this gigantic earth it looks it looks flat locally right so locally Euclidean space is totally appropriate despite globally actually it's not Yuki I such this field so that's what you have on a manifold so manifold locally you have Euclidian these Edison's the valid globally they are not valid so if you for example take the distance between this point at this point right and this point in this one you could let me accede to a better example so you could have the manifold actually curl up like this alright so then actually this point in this point these are actually closer in Euclidean space and this at this point but actually on the manifold if you would want to go from this to this point you would actually have to traverse all the way around here alright so the true distance between these two points actually much much much large so globally Euclidean distances don't work on manifold data but that's okay because for K nearest neighbors we just do local distances you only look at the K nearest neighbors and that works right so what you've basally half years that all the points in this region we have one label all the points in this region have one label all the points of this region if you have a point here you find the nearest neighbors and there they're close any questions about this yeah and you should that's exactly right hey so so you can for it so this is you know there's all sorts of tricks how to actually estimate if you lie on a subspace on it's a manifold subspace for example is just you know I know people heard of principal component analysis or a singular value decomposition raise your hand if you've heard of those yeah okay so that's essentially what these algorithms are doing is they find a new coordinate system that captures in a very most the most of the variance of the data in this case for example you would find a coordinate system actually be centered here that actually would only have two dimensions and you would see that in the remaining dimensions you have no data anymore so you can just drop those manifolds a little trickier so you have to what people do is you draw a little spheres around you keep doubling the spheres and you see how much bigger does your data get so and let's say you have a point here you can draw a little little sphere around and you measure how many points to 1/2 inside the sphere and now I double the sphere and make it twice as large and I count how many points do I have now well if it's a two-dimensional surface if I double the sphere then I should get four times as many points but if I have a three dimensional surface I get eight times as many points right and so on so you can actually use this to estimate the intrinsic dimensionality of your data items exactly right and so if you can use neighbor algorithm doesn't work why that's a good test to do any other questions one thing that also helps is to think about what is the true dimensionality of your data so for example in faces you think about faces you know images of faces may have ten thousand pixels right but clearly you don't need ten thousand attributes to describe a face right so if you describe someone let's say you know a new boyfriend or girlfriend to your mom right you wouldn't say like okay well the top left pixel is green right and then what you would say you know you get let's say someone steals steals your your wallet you go to the police right and they have someone who sketches sketch artist all right he or she could probably with maybe you know 20 or 30 questions make a pretty good picture of the person that you have in mind right that means that maybe face is lying at 30 40 dimensional space right but not in the 10,000 that you you know that the pixel representation has any more questions all right let me show you a little demo [Music] and by the way it's a good pre-processing step in general right whenever you work with data it's a good pre-processing step to first try to reduce the dimensionality right a lot of algorithms scale pretty badly you behave badly venue data is very high dimensional but a lot of datasets don't need the high dimensions that they come in and so the first two in PCA or doing some other dimensionality reduction algorithms is typically well there should be the one of your first tools in your tool sets okay and second to this work oh I see oh yeah awesome whoo okay the first thing I want to show you is actually a demo that IOU in some sense that I wanted to do last time and I didn't get around to it's actually a K nearest neighbor so I just want to show you visualize the K nearest neighbor classifier for those who you know you essentially that's the same as what you have to do in the homework but let's say we just ran away the data set I don't know what I know let's try to do something yeah maybe I know a banana another banana all right and how does an umbrella alright good and then let's say we have some I don't know see that's why I'm not an artist all right so there's my data set I have circles and I've crosses if I now run the K nearest neighbor classifier and so what do you see here is the Kenya's neighbor classifier with different distances so I use different distance metrics and this is the one nearest neighbor classifier this is large enough so and what I draw here is basically read as the the region any point in the red region would be classified as a cross in any point inside the blue region would be classified as a circle and the widely is wide we just write when you're at the edge so it can go either way and and you can see that you know what the difference actually makes here in terms of distances in this data set actually it's not so drastic but for example here with the city block distance and you know for example you get this weird stripe here that's actually still blue and someone asked back in the last week what happens when you increase K right and so you can do this now we can actually increase K to 3 and 2 5 and so on and so what you see actually in this case is that I drew more circles and then crosses so what happens actually the circles become more dominant around around where they are but actually eventually the decision boundary should smooth out so maybe may I do a slightly suboptimal dataset and he mentioned he just becomes the mean label and so you see that you know here now all the top is everything it's blue because the average is actually blue and as we keep doing this and eventually the basis just overrides the Union and that's advantageous if some of these these points may be errors right if they basically may be mislabeled right and you don't want to over react to them and say well globally still that seems like a blue region because there's a lot of circles here right and eventually Anna any questions about this demo any of the data set you want to see that I will attempt to draw all right let me show another demo this is the curse of dimensionality and this is also on your sheet but let me just explain what I do here yeah so what I do here is I draw data points within the cube like I did exactly what we just talked about right I take this hypercube and I draw endpoints randomly and then what I do is I come to compute the distance between any two points and what you see here is a histogram that theythey says how many points have that distance from each other right so the maximum distance you can have from each other is exactly square root of two all right that's exactly if you're on the opposite end right you to each other corners right and this is exactly square root of 2 all right so that's of course only two points like the ones you know or the ones very few points that I write on the edge and the bulk of the points in two dimensions like the most of them this is like the y axis here's how many points have that distance how many pairs of points of the distance most of them have a distance of 0.5 and now as we increase the distance the dimensionality what happens is this this you know the shape can get squished together and so in ten dimensions already you have hardly any points they have less than you know 0.5 distance that's amazing right so basically that means there's no neighbors anymore right no points are within point 5 distance of each other everybody gets far away from each other all right this is the the fascinating part here and to K nearest neighbors works great in this regime I think they'd say here's my point and some my neighbors really close and therefore we have the same label right but what happens in 10 dimensions that disappear so you have no neighbors anymore so like it's like the suburbs you know everyone is spread out right and so 100 dimensions gets even more extreme in 10000 dimensions you have this ridiculous like distribution right there basically there's no neighbors at all it is like farms in Kansas or something right they're really far away from each other right and there's really nobody has any neighbors at all right and so now you can see that it just it's just ridiculous to say you know oh one point it's slightly closer to the other point but they all have the same distance from each other at every point as the same distance from each other there is no such thing as nearest neighbors we can't run nearest neighbors in the state of any questions yeah yes yes yeah it'll be same yeah see I use l2 here yeah but it's the same effect with other metrics yeah and so sometimes you can make it work with other with with special let's for example for images you could have certain metrics that work better that but usually what they do is actually they take advantage of the fact that your data truly lies in a low dimensional space right so maybe yeah first up through a transformation to bring that out or something right but it's really a matter of the space yes sorry jeez you ask questions alright let me think about this so the Alan thing you know I'm busy would say you look at maximum distance and oh no it's still the same though right because the maximum distance would still be very very large right because it would be very unlikely that for every single dimension you know there isn't one dimension for which you have a very large distance right so by the same is the same with the product you know if you understood the argument of the probability distribution that I made earlier right it's the same thing all right so in some sense you're drawing yeah you can talk about offline but but your basic join two points you see that over and over again at some point they will be far away from each other okay any more questions is that a question are you raising that you waving oh no okay if if I'm sorry one more time if you did I guess I don't send the accuracy of what is perfect oh no no I see I see oh I see what you're saying okay I see I see what you think so what he's saying is the following so like okay here all the points are really really you know all the points have the same distance right but they don't have exactly the same distance right they just very very very little but let's say I have a super precise computer right you know get the latest super awesome computer has like you know 1000 bit precision floating point numbers so I can tell the difference between that point and that point right then I can still use nearest neighbor no the answer is because these two points like these are two points they have this distance they're just not close right that's basically the the problem right and the assumption is that similar points have same neighbors right they're actually really far away from each other so who knows if they share the same label or not yeah sorry but yeah that always depends I mean really it really it's always about intrinsic there it's also let me put another way if your data does not have a lower intrinsic dimensionality then you shouldn't analyze it because it's probably not interesting but theta that's uniformly sampled is just not very interesting right there's nothing you know what pattern you're trying to find there there's no pattern that is uniform right so in some sense all the data you're trying to e but you actually try to make something reasonable okay you know try to find something reasonable has some low dimensional structure right by the way I can I've low dimensional I on a lot of mine subspace it can be on a low dimensional manifold the other thing is actually could have clusters that that's another thing yeah okay maybe last question yeah it has something to do with that of drawing it from the OSE because I have this tail here well no it's just because they are confined to a to a hyper hyper box let go I have a cube sorry and so the hypercube you always have a maximum distance you can have and just shut off oh oh that it goes up oh I see then I think that that's more a function of that the yeah that the mean distance approaches the maximum distance well no no is this actually it is a factor of a property of the curse dimensionality right because the bases all the points are at the edges so if you're all at the edges then you have maximum distance from each other right so we're in this very first case I said a very few points will have exactly square root of 2 distance it's only if you're at the exact opposite corners of this two dimensional square right well that's very unlikely right but in a high dimensional space you're always in the I opposite opposite corner of some edges right so it is a property of dimensionality and in this particular setup yeah okay very last person all right [Music] no no I'll give you the example so imagine a teleporting classifier but I want to know I pick a random American and I want to know what baseball team they're cheering for right well if I pick a guy and he or she lives in Chicago all right well I don't know what baseball team they cheer for in Chicago right but I look at his neighbor and his neighbors a huge Cubs fan right I say well he's probably also cups man okay but now in this case you you know your distribution is that should I speak to neighbors one and one is in Cleveland then the other one in San Francisco all right they're all really far away we just don't tell me anything about this person in Chicago anymore and that's the problem yes the one in Cleveland is a little closer than the one in San Francisco arguably but it's still wrong right because basically that the labels changed quicker then actually you know you danced your sample density okay any more questions I'm very proud of myself there's my very first baseball-reference actually I know nothing about baseball thank thank you thank you my wife would be very proud of myself all right engine all right okay so k-nearest neighbors make this assumption that nearby points are have similar labels can anyone tell me some advantages and some disadvantages of K nearest neighbors when you think about it like what what could be a problem with K nearest neighbor so I showed you that besides the curse of dimensionality that we just talked about if impractical you know we showed that if n becomes really really large this truly becomes a very good classifier assume you have a reasonably intrinsically low dimensional data and you collect a lot of data right you know because you know as n gets large the K nearest neighbor classifier becomes really accurate what could be a problem you know yeah yeah I'm gonna skip you because you always said something yeah so if you have many many classes is that what you're saying so if you have many many classes you've even more data right that's right absolutely but let's say okay well you know we go all out right we just say okay let's collect more data right like we haven't you know trip all the datasets and that's correct as much data as we can to make it really accurate right um you can't even do this I'd say you know collect data until we get 0.99% AI percent accuracy why could Kenya's neighbors be a bad algorithm in practice yeah oh so yeah it may not do very well on bond boundaries well you just have to collect even more than it's collect even more than all right good yeah sorry assume the name of the discrete fair enough let's assume the labor the discrete let's choose if it's not the speedy after the regression you can still do that though yeah let's assume the assumptions right I want to hear practical think about it you have to coat it up like those people project one already why could it be about now that you've said five times would be a click even more data right so so you made me collect more and more data right so now we've collected a ton of data alright one-one Killian data points yeah yeah that's right so you have to sift through all your data points right so you have to you have this one Killian data points and you have to compute the distance to every single one other all right it's gonna take a really long time I would sleep you know for those who computer scientists of you the the complexity during test time is order n times D so for every I go through every single data point of my training data set and compute this turns and D dimensions okay so people know Oh Big O notation raise your hand that make sense okay awesome that basically means like my algorithm if I my algorithm scales linearly with n so the e how long it takes me to Rama algorithm is a linear function of n so if I would double my data set my algorithm takes twice as long so that is prohibited right because you know think about Google or something they have classification problems on billions of data points right you can't every time you make a prediction comes with every single one right and then you know if you for example a log on to Google News etc so um that that's a real limitation of K near Snape so as the K nearest neighbors becomes really really good if n becomes large but it comes also damn slow right and becomes really really large and so in most applications you know the time that you take during inference inference is the time and you actually make a test prediction you know actually matters right yet someone has to pay for this computation let me show you another Eiger than that doesn't have that downside and actually algorithm that was invented before k nearest neighbors and it was invented here at Cornell University and it was actually the first machine learning algorithm it was the perceptron by Frank Rosenblatt 1957 yeah the perception makes a very different but equally easy to understand us um it says okay well assume you have some data and you have basically exes and yet you know nods and crosses here my assumption is that there must be a hyperplane so the line that separates the one class from the other right so in this case we assume the only F for now we just assume we just have two classes so the tube dining reclassification problem we say they exists a hyper plane such that all the points of one class lie on one side of the hyperplane and all the points of the other class lay on the other - other side of type of a now I know what do you think all right that seems like a crazy coincidence hey you know what if bond circle was over here while the whole thing goes out the window alright doesn't work any but turns out actually I think about about the curse of dimensionality told us right so in high dimensional space data points tend to be far away from each other right so low dimensional spaces this is a this doesn't hold very enough in high dimensional spaces is almost always holds in fact actually one thing we will do later on the core of course we will actually take our data and map it in an infinite dimensional space and infinite dimensional spaces you can show that this actually always holds and you can always find such a hyperplane just kerbs think we know this is hard to visualize that you know infinite dimensional spaces but but it becomes very powerful so in some sense the perceptron is kind of the opposite of key nearest neighbors right so K nearest neighbors what you want to do is you want to use it in low dimensional spaces for two reasons a because the curse of dimensionality doesn't kick kick you and B it solves a lot faster than low dimensional spaces because computing all these distances is slow all right like scales linearly the number of dimensions perceptron you must do in high dimensional spaces because that's where they're not that assumption is satisfied right it's so it's important to remember these things because as a data scientist you know you should I get an intuition of which I wear them works what data sets okay good so assuming that such a hyperplane exists but the perceptual algorithm does it it tries to find and so the assumption is just this there's a search hyperplane and our goal is to find a hyperplane not the same any of them and mathematically how do we define a hyperplane a hyperplane is defined by the vector W and offset B and since the following set did you find a hyperplane as I said H is a set of all X such that W transpose X plus B equals 0 so that always defines a hyperplane has one less dimension than the ambient space and so in this two dimensional space the hyperplane is one image any questions at this point raise your hand that makes sense okay good so we try to find a hyperplane and so all the car points of one side one label on one side points of the other side and as a class on the other side and now what do we do doing tests during test time doing test time that's it you get a new point this is my mystery point I don't know what it is what do I do I just look which side of the hyperplane a lie on and the nice thing about this is it it's always the same speed right no matter how many training points I have computing which side of the hyperplane anion is just you know actually computing this right so you just compute W transpose X plus B and you look at the sign if that's this expression is greater than 0 then you lie on one side if it's less than 0 you lie on the other side and if it's exactly 0 you lie on the hyperplane any questions yeah so yeah so this just this this particular algorithm does not work for continuous data so we make two assumption the data is binary and there's a linear hyperplane that separates it so it's linearly separable yes you can extend it to multi class we will get to this later and there is of course other algorithms that are also linear for regression but they're different algorithms yeah any more questions good question yeah it resides exactly on the hyperplane then you get out a $1 coin you flip it if it's heads you say here tells us it any more questions so it could be that a test point lies in a hyperplane right so during training it will always find a hyperplane that separates then exactly if it exists but a test point could very well lie on right and then you just well you're just returns out yeah okay yeah so the day doesn't consent do you say our exes are here and circles are here well in this case you your and your assumption is doesn't hold right you're basically your there is no hyperplane that separates these two not in 2d as a very simple trick to make it linear acceptable and we will get to this in a few in a few lectures right so if you map this into a high dimensional space and it's immediately usable and that's what we will do so you can still run the perceptron you just have to a little trick before yeah hey it's not it's not in it if that you know would be a crazy kook yeah they're always many as almost many infinitely many yet all right um okay let me go let me tell you how to find this type of thing so one more time we assume our labels are binaries let me just formalize this we say our label set by is minus 1 and plus so these might be our X's and these must be my circles all right so we have two different classes and I would like to find a hyperplane such that all the x's are on one side although the positive points on one side all the negative points on the other side and for now this is assumed someone told me that such a hyperplane exists all right so the first thing is you have to learn two things you have to learn W and we have to learn B and that's a pain and you have to learn two things that's just you know but be much nice if you could only learn one thing so we can easily do this and it's a very simple trick how to get rid of the B so let's just do this let's assume there is no offset and it's very simple trick you just say all our data X I it's mapped to X I and mu2 a little one below it so we have a vector with all the previous dimensions of X and then a little one below it and whatever vector W we are learning becomes the old work the W and a little B alright and I claim that the inner product between this vector and this vector is exactly the old W transpose X plus P does that make sense raise your hand that makes sense okay awesome all right so this little transformation is so basic what we do is the first thing we do is just take all our data where's ido1 add one more dimension with all ones and now we just find that vector of is one more dimension and the last dimension is actually our B okay so now we define the hyperplane that's something else you just defined the hyperplane s W X such that W transpose X equals zero okay so if we busy assume that the B doesn't exist anymore and just absorb that in the data okay so we just make that little transformation they can just you know learn this one W instead geometrically what's going on geometrically what we are doing is if they see say that our hyperplane now has to go through zero right our hyperplane always goes through the origin but there's no offset anymore so what W do is shows you the angle of the hyperplane or the orientation of the hyperplane and B is the offset so now we have no more offset but we add at one dimension to our data set right so in some sense what we did is we moved all our data in this additional dimension we moved all our data one off right because in this in this one dimension we said it all to one so if beforehand we had a data like this if we have X's here and zeros here now we just take this whole plane and move it one over here so that we have X's and O's here all right this is the entire data is basely in this space here okay and beforehand we could find a hyperplane like this now in a three-dimensional space so we find a two dimensional hyper plane and it basically looks like that so I had to face it goes through here all right so this here's my two dimensional hyper plane and if you put if you look at this this the plane that the data lies on is exactly the same solution okay that does that make sense raise the handle that makes sense awesome good stuff all right if two more minutes let me maybe let me stop here and be on Monday we continue with the perception 
","['', 'K nearest neighbor classifier', 'Bayes optimal classifier', 'Curse of dimensionality', 'High dimensional space', 'K nearest neighbors', 'Distance metrics', 'City block distance', 'Circles and crosses dataset', 'Mean label', 'Demo', 'Hypercube', 'Error rate', 'Pre-processing step', 'Dimensionality reduction', 'PCA', 'Face recognition', 'Perceptron', 'Linear separator', 'Support vector machine', '']"
"all right hello everybody hello good hi alright so last time we talked about we finished off the curse of dimensionality and then we went to the perceptron and then the K nearest neighbors actually curse dimensionality k nearest neighbors and the perceptron a few questions that were asked afterwards I make quickly relay so one person asked you know why is that so bad if you have you know everything is far away from everything else and so for ones actually yeah well it someone asked the question well how many points would you need if you actually want to have a small neighborhood right so we had that little box and we said you know this here's my little box with a size L and when we showed is that if the dimensionality becomes very large then this box becomes larger and larger and basically it takes up the entire volume one question that you may ask is well if I want to L to be 0.1 right if I want L to be small how many points would I need all right and it turns out it's very easy to show that this you know the number of points you need actually grows exponentially right it's actually on your nodes so it in fact that it scales with 10 to the D it's the D is the dimension that that's how many points you need right this order has a constant but so in the mat you know if you have a thousand dimensions by ten to a thousand right is absolutely gigantic right so that's more than there's electrons in the universe right and the moment you have anything that's more than electrons needed worse it's useless all right it's basically infinite so that's another way of looking at it another question that someone asked me was if you have two two points so with the curse of dimensionality what we see is here's my test point and the you know the nearest neighbor may be here but the sake nearest neighbor maybe over here and they about the same distance but I can actually find out that this year's a nearest neighbor I've have a really really precise computer I may be able to find out that this is my nearest neighbor and I know that this here's an X and this year's O so why couldn't I then assign an X to this point because I realize this here is the nearest neighbor this here is a little bit closer than that point ok does that make sense so the question was well if you can still figure out which point is closer even if they are far away from you why couldn't you still do nearest name and the reason is that it really doesn't tell you much about the data right so all you know about the data is you have the assumption that nearby points have similar labels so you know this is an X so you know around here is all axis and this isn't own you know around years or lows and you know another thing at some point it switches from O's to X's but you have no idea where it switches but it could be that it switches here right that at some point here basically goes over to Oates right and you allowing squarely in the old territory right or it could be that X is all the way up to here and you know here's where it switches then you're squarely in the X territory right the promise you really cannot infer anything about the label of this point if you know all you know is over there X's over there are O's and you have no idea what's in the middle does that make sense Razia under that makes sense awesome ok good those are two little thoughts will do convey about the curse of dimensionality quick thing about the project so some people report their problems with look for aquarium it seems to be that the most common problem is that people click commit without clicking safe first so that's I guess that's not really expressed in the user interface so you have to first say and then you have to commit otherwise you get through the previous version submit it to the leader board so please be conscious of that and also today project 0 is due and project to project 1 is still running in project 2 is being shipped out either tonight or tomorrow morning and they'll be on the perceptron any questions about the projects or logistics No okay good after we I scared you with the curse of dimensionality oh Jesus still oh my handwriting it's amazing it's prefilled actually that's that's exactly what I want to talk about so we talked about the perceptron it's the idea the perception is you have data points that either X's or OS and the assumption that you're making is that you can have you know there's actually a hyperplane that you can squeeze between the different classes so in this case we assume there's only two classes that's called these the minus one and these plus one there's a plus one so there's the binary case and that may seem restrictive that you can you know find a hyperplane between these clouds but actually in high dimensional spaces that's not very restrictive and high dimensional space is actually that's very very very likely all right and that's in some sense basically you're taking an advantage of the curse of dimensionality okay and this is actually yeah okay let me erase this part so last time we defined the perceptron I showed you that it's just a hyperplane so when we have a point X what we compute is just W transpose X plus B that tells you how the how the point X stands in relation to the hyperplane which is defined by W and the offset B and if this is negative then you're on the on one side if it's positive you on the other side so actually in this case this would be this way all right raise iana that still makes sense okay all right so the perceptron algorithm assumes that such a hyperplane exists and provides basically a very efficient algorithm to find such a hyper plane or a such hyperplane one trick we did is we said we do the following transformation we take our X and we just so everybody just add a dimension of one of constant one and our you define a new W it's called this W bar or something and X bar X bar is our original X with a 1 as one additional dimension and W bar is our original W with just one more constant that one more dimension that we're learning and the nice thing about this is if you take the inner protect the inner product between W bar and X bar and that's the same thing as here so if you learn our W bar then you can just look at the last dimension that gives us our B so then we can basically decompose our W bar into W and B and why is it that pertain to someone else now why are we doing this right it's like it's the same thing and that's exactly the point it's exactly the same thing but the derivation is a lot easier because you don't have to futz around with that B all right so we'll prove something in a few minutes and it's just annoying to always carry that be around with you the same in the code it's just annoying to update it so you just there's a very very simple trick just get rid of all the updates to be etc okay yeah so well and I guess is linearly separable that is that I'm not shown us and the question oh it's very different I can use name I see well Keeney's neighbors can around go around curves and stuff right like this so K nearest neighbors also has a decision boundary let me let me look is a good point let me let me explain this and so when you do K nearest neighbors and this is actually something I guess I showed you this example with the pink and blue they a in a set right so one thing you can basically say is for me every single point you could say if there was a test point would use classify it as a circular would you classify it as a as a as an X right and ten years neighbors therefore all that he finds a decision boundary is thy something sighs disempowering public it was like like this right between all the O's and all the X's right and so you could actually have something you know we have another thing carved out E and Ireland or something right the perception makes a very very restrictive assumption on the decision boundary it says this decision boundary has to be a hyperplane so that that's it's very restrictive I have a trans out actually sufficient high dimensional spaces right but the advantage is that now you just have you just have to store the one vector and the button and the bias of the hyperplane where's Ford K nearest neighbors in order to encode that decision boundary actually have to store every single training point right and you have to compute the distance which is a very slow and memory inefficient algorithm so they'll be busy simplify the hyperplane get a lot of memory savings you know a lot of savings for it but there's a trade-off because we couldn't model something like this okay you're a good question any other questions okay and the same thing by the way like you know that the K nearest neighbors we meet this have this nice theorem saying that if n goes to infinity you know we will have most have it twice as many errors as the Bayes optimal classifier well that's impossible to prove for the perceptron why I can even tell me why that's impossible why couldn't you possibly ever prove something like this for the perceptron all right I'll give you a minute to think about it discuss it with your neighbor [Music] [Music] okay any suggestions so I would like to show the baby yet the training set becomes infinitely large my perception will make very few few errors why can't I possibly prove this when is it going to break down yeah that's exactly right right so if you have a data set you know you could have a data set that the distribution is such that you have positive you know points here negative points here positive points here negative points here and so on right imagine you theta they do a sample from some kind of sine wave right well there's absolutely no hyperplane that separates the O's from the crosses right and the more data you get you know you won't do any better if you error will not improve if I tell you know here's really a lot of crosses right like here's a lot of oh right it won't help you because there's no hyperplane that separates the only crosses right the moment you put your hyperplane here we're gonna get all these guys well why do you say everything is oh you're gonna get all these guys wrong and the more data you have like you know it's it's all proportional value just sample more points so you're not actually change in the air okay so nicely but keen ears neighbors it works for all data set where K nearest neighbors would work here but perception would not does that make sense raise your hand of that make sense okay awesome so here is the perceptron algorithm it's very simple so you start out with W equals zero so you say initially my weight vector is all zero you know that will always predict zero that's neither positive nor negative all right and just a reminder I assume that my Y eyes are either plus 1 or minus 1 so every data point has a label plus 1 or minus 1 and then I say you know I can either say well let me write it the way I have it here Y true it's a loop forever that's what's not great coding but so we said you know any consumer may see what we're doing is we just repeatedly go over the entire data set and they count how many points we missed classified so M here is the counter of how many points I miss classified if I make it pass to my data so initially I say okay so far I haven't got misclassified a single point yet now let's go over my data set let's try every single data point and see if it rice lies on the right side of the hyperplane alright so I have my data set like this yeah my ex is here my OHS I initially have a zero I have a pains I will get every single point one but so the algorithm is basically every time I get a point wrong I adjust my hyperplane and then I basically you know loop over the data set and the moment I don't make a single mistake anymore that means you know if everybody see all the crosses line on this side of the hyperplane or the ozone that is set up to hyperplane then I know I've actually found a hyperplane that separates the two and then I can stop so that's the high level intuition so this year is the count of how many mistakes I've made and now we say for every X comma Y element of D so we loop there's a for loop or for you now you can buy this annotation that you're familiar with then you basically say okay now I classify this point and so here's what I'm doing I now say okay does this point lie on the right side of the hyperplane if it does I don't do anything and if it does not then I increase my counter because now I made a mistake and I adjust my hyperplane such at this point is now hopefully correct but that's the idea that's the idea behind it so how do I do this I say if Y times W transpose X is less than 0 less equals 0 Y times W transpose X is less equals 0 I claim that that's the case if and only if I get the point wrong can anyone explain this only one person why does this mean why W transpose X so I have a point X with label why I call him John and I want to know that's this point lie on the right side of the wrong side of the hyperplane now I'm but I'm computing is y times w transpose X and save that's less equals zero that I must have gotten the point wrong otherwise much gotten right who can explain it ma okay give me the first that's exactly right so you say these guys should you have a positive label these guys with negative label if I'm on this site and I have a positive sign so W transpose X here W transpose X is greater here W transpose X is less than zero and what I want here I want this to be greater zero here I want to be less than zero right so I want these signs to align right so here I wanted to be negative and here I want to be positive here the label is positive here the label is negative so what I do is I multiplied right so if I get it right here this is negative x minus one is positive if I get it right here this is positive x plus one is positive okay so if it's less equal than zero then I know I got it wrong and then what I say it's very simple I say my W becomes W plus y times X so what does that mean Y is either plus 1 or minus 1 so it's a part of this point I'm adding it to W and if it's a negative point I'm subtracting it from W you can think about it in the following way if it's a positive point then I'm adding it to W that means the next time the inner products going to be a little larger all right so hopefully it will make it positive and if it's if it's a negative point then I subtract it from W that means next time the inner products going to be a little smaller so hopefully then it's going to be negative times the negative minus one it's going to be positive and the other thing I'm doing I'm increasing my counter M is M plus one so this counts how many mistakes 1/2 and I say at the end so I do this a loop over the entire data set that's M equals 0 then break and then you can end so but if you what we're doing one more time we initialize our W to be zero that's going to get everything wrong everything wrong then we just loop forever and here's what we do everything a loop we set our count initially to zero and now we go through every single data point in our sample we say which side of the which side of the hyperplane do you lie on the positive point should lie on the positive side the negative points are on the negative side if that's not the case then the update our W vector we try to reinforce the positive points to become yet larger at the point in the product the negative points they have small inner products and we increase the count of how many points we get wrong this time around and we keep doing this until we make a full pass over the data set without a single point run and what does that mean well if you went through every single data point and every single one was in the right side of the hyperplane we didn't make any updates that means it must now be a separating hyperplane any questions yeah let me let me get to this in a few minutes but well yeah in some sense what you can think about is the following right if now W plus X so W transpose X becomes W plus X plus X all right well that equals the old W transpose X plus X transpose X right gbz increase the inner product by that amount okay any more questions it's one second let me just bring the projector down yep now this is inside the loop it's inside this loop Lizzy goes through the whole data set if he had made a mistake then you go again over the data set you reset M equals zero when you go again over the data set but the moment you make a mistake you update it so you could actually feel ready go the data set in every single time you update it right the only thing you stopped it if you haven't made an update for endpoints does that make sense yeah so though we will get me here proof in a minute in a few minutes we will prove that that actually will converge very quickly so this is actually you didn't just make this up and you know it actually has a very well you know motivated derivation yeah sorry so depends depends on how many data points you have I mean come on there's 1957 like you know we are glad I'm getting something right I mean you will eventually I mean that's already but all of Machine does like your training you try to minimize the error on the training set there's other mechanisms you do against overfitting this was not actually a concept yet at the time yeah we will get to this any more questions all right good so here's a little demo so um what I can now do is I can draw a bunch of data points and I can then train the perceptron to classify them let's first draw some positive points so here my posit I think positive comes first I'm not sure and that's choice I'm negative these are negative okay whatever and so let's hope that's linearly separable okay if it loops forever it's not linearly separable all right let's see and so here's the first thing it does that initially had a zero vector you can't see anything and then it encounters this vector here right so these are the positive points and negative prints so pix11 arbitrary vector first it's this one it realize it gets it wrong so what is what does it do with ads x2 that vector and what you see here blue the blue line the blue arrow is actually the error after I've made that update so it points exactly to X Y is that because my initial vector was zero zero plus X is exactly that vector okay does that make sense so this is my origin and X is basically the vector from here to here so I'm adding it to its there's Nam and now I'm actually have a hyperplane great right so everything here is you know it's classified positive everything is classified negative so now I'm this is my current hyperplane my vector W this red thing now I have another point this one down here I get it wrong right this guy's positive it should be on the red side but it's on the blue side so what do I do I add this point to my vector right now this vector Plus this vector becomes this vector ok does that make sense so that's my new hyperplane now I have a new hyperplane I look at this guy oh I get this guy wrong - right so this is on the wrong side so I again add this this is my original vector the red is my original hyperplane vector I add X to it now I end up here so now I get this guy wrong again I already had this one but you know now I get it wrong again right so I keep adding to it and that's can someone please pray this is yeah ok good good and to now actually end up with hyperplane that actually you know here all the positive points on one side all the negative points on the other side right and we were proven a little bit that whenever there is such a hyperplane the algorithm will converge to society any questions about the demo any data said you want to see yeah there's there's infinitely many and wars have been fought over which ones are the best we will I will expose you a little bit to this later on but that was the first algorithm that found any so at the time was amazing that you could find any and the moment that had that happened people we're like wall - better than yours so there's a lot of data it turns out there are actually better ones in numbers so one that you can see if let me just make a really easy example you know here for example I've just you know three points on this side I have a really large margin between these right and one thing you gonna see is it's gonna do it very quickly right so the algorithm takes a lot longer when there's very little wiggle room right because you kind of overshoot it over and over again and that will make sense once we look at the proof right so in this case it immediately found it with one update any more questions I can show you another example before we do this I just want everybody to have a moment of silence that we think about the time it took me to make this demo [Applause] all right so here here's what we have so so he had handwritten digits of zeros and sevens so I only take zeros and sevens turns out that's linear acceptable because they don't really look very much alike and so that's in the space these are 16 by 16 pixel images sorry 28 by 28 so 760 my 16 I was 256 dimensions anyway so it's based in a 256 dimensional space and this is my training data set these digits here right so this is my entire training set is a small training data set and what I'm showing you now here is this box is the weight vector right so the weight vector is also in the same space so I can visualize it as an image is a 60s by 256 dimensional vector I can plot it as an image okay why not so right now it's all zeros right so that's gray white is positive you know that's kind of negative so right now all zeros means gray okay so now I go through my first image this 7 and I misclassified I classify that as a 0 because 0 times whatever this pixels are is 0 so I get this point wrong so what do I do I add this point to the data air to the classic air to the weight vector and this is my new weight vector okay actually I subtract it sorry because it's a negative example does that make sense raise your hand of that demo makes sense ok now I reclassify all right so these are now the reclassification Sakuni but I'm classifying everything is negative ok that makes sense right because so far I've only seen one example and it was negative so if I take the inner product of this vector with all my entire data set every example is negative so now you give the next example before when everything was classified as zero that's why they were all here so the y-axis here is basically for every single image how I classified it it's up here it's positive if it's down here it's negative so now I'm classifying this guy here well I'm classifying is negative as a 7 but it's actually a 0 all right so I got it wrong so what I have to do I have to add this to my weight vector all right so this is what happens all right so now I actually have this kind of it's half the 7 half this year right if I now reclassify things you can see now this guy's classified positively right let's now go to the next oh it's already everything is correct well that was pretty easy ah let me do it one more time because I speak you know you gotta make it worth it yeah okay here one more times I have the first one I get it right I get it get it wrong so make it positive like one so here's actually mine here's my weight vector so now I get the seven wrong so I subtract two seven now the things that miscast a reclassified now I go to this zero this gets classified correctly next zero gets classified correctly seven gets classified correctly everything's correct awesome feeling pretty good and now this guy oh what's going on it's still correct I think the top left corner no maybe the thing is wrong okay maybe it's the center okay so I have to update my vector yeah and I move this guy up oh that was too much right so now I have to get the seven wrong so after zip you know subtract it again and do we have it I think we got it all right good doesn't make sense [Applause] yeah question yeah so the negative that the points that have negative label these points have minus one so these aren't subtracting and these here that wants these zeros they have a positive label so I'm adding them so what I do is W becomes W plus y times X so Y is either plus 1 or minus 1 all right that's why I'm adding these guys and I'm subtracting these guys yeah good question any other question yep well if it's not linearly separable it's just going to loop forever I said that's the problem that there's no there's no check of is that even possible but you just assume it's possible yeah oh that's right yeah so Y is actually the Y of that particular data point absolutely right yeah yes that's V we've evolved a great deal since 1957 so that there's much much more efficient base right and that was making you know this was the first algorithm that to do this yeah so later on the course you basically take this intuition and I will show you much much more efficient ways of doing this that's good question though it's extremely inefficient yeah any more questions all right let me close this [Music] okay good so now I thought we spent a few minutes I just want to have my home the geometric intuition on a little quiz so there's just two parts to it so if you turn you your you actually have this printer that's the same thing oh I don't have it there you go so the question number one is you have a point X you have to draw it like yeah here's my origin and here's my weight vector W so remember that the hyperplane always goes to zero right why does it always goes to 0 as 0 who knows why doesn't happen train I always have to go to zero yeah that's right so we added the 1 and that B we forced it to be that way right bigger better we incorporate the offset as the additional damage so this here's my picture I have x fw i want you to draw so this guy is misclassified it should be positive I want you to draw the new W after an update that's the first first quiz I want to do is just to visualize this and the second thing I want you to do is think about how many times imagine I would encounter the same point over and over again in a row how many times could I get it wrong it does that make sense so please spend 3 minutes with your neighbor and figure these two things out [Music] [Music] [Music] [Music] all right please raise your hand if you've got both answers all right keep thinking all right well the first question so first guess is my first initially my hyperplane is defined by this w and so the happening goes to zero right so it's basically the vector that goes to zero and I thought it so that the plane so it's talking onto the vector W so this here's my hyperplane not only come to this point X and I get this point X wrong so this point should be positive but it's negative what do I do what does the hyperplane look like after the update can anyone show me what I have to draw I see people drawing the - okay good so here's what I do right I look this vector actually you know this is actually a vector X right now become W becomes W plus one times X is the positive points of one time so I vector addition Macy works that way right so you add W plus X this is max let's translate it and this is my new W then and so I have to draw an orthogonal hyperplane so this is my WT plus 1 WT okay thank you to everyone crazy Hannah that's what you got all right good so the second question is well what if this point doesn't actually have to be correct now right it could be it could still be wrong but there's no guarantee that after one update you actually get the point correct so imagine that we visit the point again how many times could we possibly get the same point wrong I could it be that we just keep updating there's one darn point we never get that right right yeah that's exactly right so let me just derive this so you basically say my you know W transpose X what does that become that becomes W plus X transpose X okay then we know this is less equal zero all right so if we make a mistake until we make an update and it's still wrong let me make another update what would do that would that be well that would just be adding X one more time okay so let's say after K updates we've added X K times 2w okay and we're still getting it wrong so what does that tell us about K but if we solve this what we get it's W transpose X plus K times X transpose X is less equal zero and what does that mean what we can solve that for K that means K is less equal minus W transpose X divided by X transpose X so that is so this is negative right so that's a positive value stability shows there's an upper bound how many times you can get K wrong right so that is some positive number and after that we have to get X right right so there's only a finite number of times that we can get this particular point wrong right that's very encouraging that means we're doing something right right so Z right this is just if you just look at this one data point I agree so if there's good this is just to simplify the argument right now right the proof that we will do in a few minutes will be over entire data set but you're exactly right so well at least we showed something right so if I give you one data point and I show it to you repeatedly eventually it will get it right right I mean I said you know it's a minimum requirement but it means it's learning something yeah if you're exactly the opposite right what would happen so here's my zero this is my W and then my X is actually exactly say X is minus W right so my X is here but what happens if you add that right you actually get the zero back and then the next time you would actually get exactly this vector oh if it's zero if it's zero it's actually still then you get exactly the equality here right so when it's zero you say it's still wrong it has to be on the right side if something lies on the hyperplane you still consider that wrong and in part you need that because you initialize with zero so initially you classify everything as zero right so it's actually a common mistake and people implement them it's perceptron so you must have less equal otherwise it immediately converged I mean it's easy to detect you all right any more questions yeah sorry moved it now another way to come at them no no you don't have to the origin is always zero no no no no and that's just one more dimensions you just them high dimensional space right so that's just there's some dimension in which the vector actually X really lies in that you know one off I guess some dimension up in the hype in the into the blackboard whether the vector X lies lies one remove no you could have two two-dimensional you could have one dimensional data but in this case yeah so this I mean in this case you would always have a third dimension if you did this two dimensional ya de ya this was basically he can uh give one dimensional data and I add a second dimension okay all right who's ready for the proof someone at least one person good all right convergence proof so this most but what made Rosenblatt famous and you guys are here in Cornell half a century later has changed the world we live in alright so the question is why does W why does the perception algorithm always give us a separating hyperplane if it exists right and so we won't get through the entire proof today what I really want you to do is they be novels will get started with a set up and what I really want you to do is read through it before Wednesday it just you know one evening maybe not Valentine's Day you know but you know maybe there's snack we read out through the proof and just make sure you understand the different steps right it's okay if you don't understand everything that's what the lecture is for but it really helps a lot if you've looked at it beforehand okay so here's what we assume right the assumption of the perceptron is that they exist the W that can separate it we call this w star the exist of U star such that every x and y element of our data set we have Y W transpose X is greater than 0 all right that's our assumption now we're not saying we will learn that w start and we will learn a separating hyperplane but what we will show in the proof is that we will get closer and closer to the W star and then eventually will be so close that we will classify everything correctly that's kind of the idea all right so so far that's just the assumption that's just what we know the foundation of the perception algorithm and now we say okay well we know thus W star exists so one thing we don't want to deal with is a scale of W actually W star is separating hyperplane well then actually five times W star is also a separating hyperplane all right you just rescale the hyperplane right it's B just rescale about any positive constant it's the separating hyperplane so clearly there's infinitely many separating hyperplanes right if they exists one so that's not very interesting right so in the picture that bassy see it says the following if you have our X's here now O's here here's our hyperplane well there's one that has you know error like this another one has an arrow that's twice as long right that's exactly the same hyperplane so we just fix one scale so we say be rescale w star such that W star the norm equals one okay that's not a limitation we can always just take any W star divided by its own norm and then they get exactly this okay this is we just assumed W star equals one and we know that exists because they exist a hyperplane and now we can do something else we can save all let's we scale our data so that's somewhat a trick so basically this is just to simplify the proof later on all right so basically what we can say is well we can always just take our data in multiplied by a constant right the same you multiply this w star by constant so what we assume is that for every X I the norm is less equal 1 for all X for all our the Xin biiian artists so all our data points right hola these are feature vectors we assume that they're norm is less equal one can you always do this that seems can fish right so this was still I think people bought this one do you buy this one raise your hand if you buy it all right good some people have concerns you should have concerns actually this is it is totally kosher but so here's basically what we can do we can say busy save all so month in some sense basic but you could think about it you can just take the entire data set rescale it to make shrink it find a separating hyperplane and at the end expand everything again right so you've easily have some magical device you just take this data set we make it really small are you just multiplied by a small constant we find a hyperplane here now we take all of this all the points and all the hive grain and modify it back alright so we busy month we multiply by alpha and then meet multiply 1 over alpha so you can always go back between these two - just a rescaled version of the other it doesn't change anything does that make sense raise your hand if you'd buy it now another way of looking at it is if we have so in order to I have all my axis alright the base e what I'm doing is I take my largest X that's like in some X has the largest you know let's say our mica equals max over X right sister Omega is the largest norm of any of my data points and now I say everything a data point becomes X I times 1 over Omega so I rescale it now this is satisfied right because the largest X now has exactly the norm one then everybody else is a smaller norm and but this is a non-negative constant so I have this one hyperplane here that says everything you know this is my MA my separating hyperplane every point lies on the right side if I just multiply this with 1 over Omega this still holds right because that's non-negative so I'm actually not changing anything in the same way that can rescale W I can also rescale my axis and just to make things simpler I now basically take my data and I rescale it such as everything is within a within norm one another way of thinking about this is that base you have my axis in my nose and I basically rescale and let's add a on a circle with size one I did that that's the idea and they'll be only doing this because it's convenient during the proof right and this does not not infringe on the generality of the of the result let's leave it here please read over your over the proof you can maybe you can make out of it at valentine's power all right think about it it can be very romantic if it's read the right way 
","['', 'Perceptron algorithm', 'Curse of dimensionality', 'Hyperplane', 'Linear separability', 'Binary classification', 'High dimensional space', 'Inner product', 'Weight vector', 'Bias term', 'Margin', 'Update rule', 'Misclassified point', 'Training data set', 'Zero-one loss', 'Convergence', 'Perceptron learning rule', 'Perceptron demo', 'Handwritten digits recognition', '28x28 pixel images', '']"
"come everybody quick reminder project 2 is now out I know you couldn't wait and project 1 is due this Friday please if you haven't gotten started by now I hope you have gotten started by now please start immediately you have two late days for every project and for the homeworks please everybody close their laptops thank you even they also pretend they can't hear me I could try different languages with their laptops to my hands and swash nae danke all right thank you all right the last time you talked about the perceptron last time we talked about the perceptron does anyone remember what was the assumption that the perceptron made on the data yeah another was you you know everything anyone else yeah that's right the data is linearly separable but I mean that in a positive way don't hope you're not offended right so we have our data set crosses at and zeros we assume that we have two kinds of labels minus one and plus one so let's call this one plus one this one you know these zeros are minus one and we assume that there exists a hyperplane that separates these two writes the hyperplane it's just the line and two dimensions some plane and higher dimensions yes question VI see if you don't want to classification you're under regression that's exactly how you do a question but there's a little bit more to it because you can't just use any type of mine there's no because there's no separating hyperplane anymore you actually have no classes right but but yes there's something along these lines people get as very soon all right and the perceptron algorithm is very very simple this this hyperplane is you know is defined by this vector W and technically is actually you know the hyperplane is the set of points X such that W transpose X plus P equals 0 we can collapse that into just W X that's you know if you just add one more dimensions that we talked about this last time and so you know without loss of generality we just assume that we we have no B and then the algorithm is really really simple right so you basically you go through your data points one by one and you check do I look at lie on the right side of the hyperplane and what do I want if my label is plus one then I want I want W transpose X should be greater than 0 and if F minus 1 I want W transpose X be less than 0 y equals y equals so this means I always want Y times W transpose X to be greater than 0 all right so that's that's the same thing and that's exactly the condition we're checking so you go through one data point you know at a time the order is actually not important you check if this condition is satisfied and if it's satisfied then you move on to the next point that means you know rely on the right side so we don't have to do anything the moment this is not satisfied so if we have a less equal 0 here then we do an update and B basically to the following update if y equals plus 1 and we say W becomes W plus X if y equals minus 1 let me say W becomes W minus X and that again translates into W becomes W plus y times X in both cases alright any questions yeah ok damn did anyone hear anything I just said it says that battery all right all right want to test better thumbs up thumbs down thumbs up better yeah okay good awesome thank you thanks let me know luckily I had batteries with me it's amazing okay good so what we want to do now is we want to prove that this is actually a reasonable algorithm that means if there exists such a hyperplane so if there's a single W star that does separate them then we will also find them right find W the dust so it doesn't have to be the same one in fact if they exist one there's you know usually infinitely many but you want to show that they you know that we will converge that this is this update actually will not loop forever and you will terminate at some point ideally in a finite number of steps so not just in the limit and that the you know whatever be a right a converge to that separated now the one thing is you know about the moment we do stop we must have found a hyperplane because that's our stopping condition right so we say we loop until basically this is satisfied for every single data point that means the moment the algorithm stops you must have found a solution it could be that we never stop all right so let's let's make sure the questions like you know it can be somehow show that this algorithm cannot loop forever and so last time he basically said okay well let me set that set up this proof and we made a few assumptions to the first assumption is how we ended the lecture was well they exists of W W star you know that separates such that for for every X comma Y in our data set we have satisfied that Y W star X transpose is greater than 0 so we always lie on the right side then we made one more step we said ok this is just the assumption of the perceptrons that's not a big deal is just formalized and then the moment X is 1 w star they actually must exist infinitely many why is this because if you just take W star and multiply by any non-negative number any positive number and this will still be satisfied right so if I multiply this year with any cons an alpha alpha squared zero and this is still true right so I can just define any other W star prime equals alpha times W star okay so for any Alpha basically I can find another hyperplane that has exactly the same property and essentially what I'm doing I'm just rescaling this double right so I can just make this longer what it doesn't change anything okay so there's infinitely the exists one there must be infinitely many and and in fact they can't have any possible scales of what we say is very simply just pin down one of them let me say W star the norm of W star equals one okay and we can easily recover this a moment if I have any of them we just divide we just set alpha to the one over the norm right and then we basically rescale it such that the norm is one any questions about the step so we now basically happy now just this w via has exactly norm one right that's my stuff okay so that's the first step that we did an any Kinnaman a little bit further we said well actually be content you don't just have to do this for X and for W we can also do it for X right the same thing we can also multiply X by alpha and nothing changes so we can also rescale all our data as long as we scale every single data point by the same scalar and has to be positive so we can do exactly the same trick and basically what we assume that you know X I the norm of X I is less equal to one for all I so essentially what we're doing is we divide every X by the norm of the point of the largest noise as possible any questions about this raise your hand if you're okay with these two steps okay not everybody any questions about it so let the geometric intuition was the following we have our data data like this and what we're basically doing is we re scaling the scale of this such that the entire dataset falls into a circle of unit one of radius one so some data points gonna lie right on the edge that's the one that's farthest away at least one and our W vector is going to go exactly from zero I mean is a little prettier here this here's my zero my W vector has exactly norm one right so it will also the W vector will also go to here right and once I found this I can rescale the whole thing again to my original data data set yeah that just certainly makes it easier you could also do it otherwise but yes this way just multiplied by a scalar all right so I don't know if you've seen the movie Honey I Shrunk the Kids have you seen this it's a little like this you just shrink I always think then you can make it big again yeah oh because you shrink everything by the same amount the entire world gets shrunk but you shrink everything right so if you shrink everything then the relation between any two points is the same we shrink everything find a hyperplane and then beyond shrinking there okay any more questions yeah know so these can be rescaled independently actually because W actually has additional property that actually you know any any rescaling of W will still be a separating hyperplane because it goes to the origin yeah oh yeah you shrink all the XS then you can still actually shrink the hyperplane anyway without changing changing its orientation that's exactly right right so you basically say X becomes X X I becomes X I divided by the the maximum norm of any point right so therefore the largest one has exactly norm one any more questions okay so if you buy this which you should then we can move at who managed to look at the proof before this lecture raise your hand well that song huh all right congratulations to you guys alright so so here's the idea what we want to show is that we know there's this vector W star it starts from zero here's the origin this W star and we initially we have you know some vectors W and then basically we keep updating this w and the first thing you want to show is that W becomes closer and closer to W star alright so you know if you could show that busy W becomes more and more similar to W star well then you could actually make an argument eventually it's the same stubbing star that's kind of one one way of looking at it and so what we will look into is the quantity what happens to W transpose W star after an update okay so when we make an update to our W vector how does W change in relation to W stuff okay that's the first thing we look at and what we will show is that that will actually always become larger so every time you make an update this quantity will come larger now that's good but it's not enough why is it not enough how could you make this trivially larger without actually getting closer to W star any ideas so I'm showing you every time I update W this becomes larger that's good that seems to suggest that these two vectors become more similar in the product measures the alignment like house you know how close they are but actually it's not enough so it could be a pathological case but this becomes larger with every single update yet we're not getting any closer I saw you I give you a give you three answers per lecture all right all right yeah it's right so you could just rescale top right so basically you know one option could be that you know imagine you could just every time you just update you you update W by a factor two or something right and this he always becomes larger by a factor two but actually the the relative relation between these two hasn't changed at all right but the first time that we use this next time W is twice as long and then four times as long right if you haven't actually made any progress our hyperplane hasn't changed at all right and and so for that we look at the quantity W transpose W how does that change right and so in the case that you just pointed out what would happen this would grow really fast but this would also grow really fast so really just the norm of W become really large and what we can show is that this he grows fast and this he does not grow very fast and then we can actually prove put these two results together and say if that's the case if W becomes more you know the inner product between W w star becomes larger yet we can show that it's not the case that W is just growing then the only option that we have left is that W tilts towards W star and eventually they define the same hybrid ly any questions about the high-level intuition all right let's get cracking all right so let's first look at you know when we do an update what happens an update that's the following by the way please pay attention now it's so much easier to follow it now in lecture like this is definitely on the exam right so this is kind of such a beautiful little proof so irresistible these are resistible wait what the hell okay here we go all right okay this is mine so when we do an update w becomes becomes W plus y times X okay that's the first thing we have to note the second thing we have to note is that the only reason we didn't update was because we misclassified X right so what do we know therefore we know that Y W transpose X must be less equal zero right otherwise you wouldn't have done the update okay raise your hand if you're with me at this point okay awesome right so we know these two things so we're making we know this about x and y and W and we'd be making this be doing this step right W becomes this and the first thing you want to now look at is if we make this update how does that affect these two terms right and I think I started with W transpose W stars let's do this one first so the first question is how does that affect w w transpose stars or how does it affect our relationship of W of 8 vector with the the holy grail weight vector that we know actually separates the data and so well what we do is we just substitute the new our new W in here all right so we now know this what does it become this term now changes right which way does have changed now have W plus y times X Y a transpose W star okay that's that's basically that's the change that happens raise your hand of you with me okay is gonna be a lot of raising your hands but you know I just want to make sure I don't I don't lose you guys alright good so that's great now we can take us apart right and say well that's the same thing as pulling this in here and Polly good years of u W transpose W star plus y times W transpose star x okay now without looking at the notes what do we know about mr. and now be careful there's a star this is not W this is a star so this is not our original W this is the W star that you easily want to find someone said it yeah huh it's positive why is it positive that's right right so this here is kind of you know this here is the golden boy here right so this guy gets everything right right so this means the only way that's possible is if this term here is always positive okay so this is greater than zero right where am I so D okay so then and oh shoot sorry I forgot forty oh sorry guys I forgot one I try to be really quick here sorry one one quick one quick you know that I a what they forgot to define is the margin okay good so what is the margin you will need the margin so the margin is remember we have our data points X's and you have arrows well there's some point that closes to the hyperplane right there must be some of them I could say this guy here is the closest to the hyperplane right everybody's on the right side but one of them is going to be closest right could be the two are actually equally close right that's fine let's pick one of them you can find gamma to be the because that's the margin basically in some sense the distance to the hyperplane right because everything is norm one actually turns out that's just the minimum in a product with the hyperplane so x transpose W star for X comma Y element of T so this is just a constant and we will need it so the important thing is like this is this greater than zero it's not greater equal zero the margin is actually something we will use throughout a machine learning if they never we define hyperplanes one thing you want to think about is you know how large is that margin can you give you an intuition like what do you want like you know let's say I can find a hyperplane you know have two hyperplanes one is a very small margin is a very large margin which one would I prefer it does not matter any ideas yeah that's exactly right right so so the intuition using videos if you have a smaller margin or I think BC you would carve a hyperplane right here then if you get a test point it's just a little different from this point then it could be on the wrong side of the hyperplane so typically a large margin it's desirable in machine learning and so we will get into this is actually the whole family of large margin classifiers but for now let's think you know just think about this notion it does actually that's why I put this placement exam you remember computing the distance to a data point you know the margin basis says how close are we a boy a what's the distance to the closest point in our dataset okay good all right so so now we know this is greater than zero and in fact we know even more right if you look at W transpose X well that's exactly what this thing is right and we know that gamma is the smallest such terms so gamma is the smallest in a product with the with the W star so we know that this must in fact be greater than gamma okay does that make sense so this is basically essentially what this is is the distance of x from this hyperplane w star and we know the minimum is gamma so it must be lit greater than gamma I had a greater equal gamma okay does that make sense any questions and so that means what we can do is we can say this is greater equal then W transpose W star plus gamma so why is that important so what if we just jump we've just shown that if we make an update the inner product of the hyperplane that we're looking for with the hyperplane that we want to find grows by gana at least by gamma it may grow more right but every time you make an update this quantity increases by least gamma because what we showed is that basically this the updated quantity is greater equal than the old quantity plus guy okay any questions raise your hand if that makes sense okay awesome okay this is a big deal by the way right because we're showing if it's not that actually it like one thing it would be bad if it kind of increases a little bit and you know increases less and less and less as we keep going right no the beautiful beautiful thing is we actually have a concrete number we say it would grow at least that much every single step right that's that's very powerful statement all right so he comes the second one what happens with W transpose W all right so that was the second thing it will come straight about and well what do we do we do the exactly the same trick we plug in B now we do an update so this you know becomes the following right and you plug in the new definition of W in here so now we get W plus y times X transpose W plus y times X okay so all I did is I take W transpose W so what's the squared norm of my hyperplane and for each W are putting the new W so that's the new expression alright so once again I expand this so this becomes W transpose W plus you know two times why W transpose X plus y squared times X transpose X all right okay so let's look at this this is good right this here's again the old term plus some stuff right it's exactly what we had here all right basically said you know and we update this we get the old term plus some stuff and the stuff turned out to be at least gamma so the question is what is the stuff here right in this case so the F W transpose W now you can look at these terms here and then let's see what we can figure out about right so the first question is what do we know about why W transpose X any ideas without looking at the notes what is what is this it's I don't have someone someone said it yeah what is this term yeah it's negative right why is it negative that's right right so W transpose X so maybe the reason we made an update is because X was on the wrong side of the hyperplane and the only that means W white sabi transpose X must be negative right that's the test for our if you do an update or not all right so we know this is less than zero all right good and second step is y squared what do we know about Y squared all right this one yeah so it's 1 and what do we know about X transpose X that's right let's see go on right how do we know this that's right by the V we made it that way all right so we kind of you know now you know why we did it all right good so now if you know these things while we can actually plug these numbers in right and we can actually say well that is less equal then W transpose W this term here sorry one second plus one all right ok good so wait it's correct yeah okay good so this year is basically okay this year's let's equal 1 and this year you can you can only make this smaller if you basically you can only make it larger if you drop it right so you can just drop it and then we have less than W transpose W plus 1 all right any questions raised II handed that makes sense the last that make sense all right good good good all right good so here's busy what we did right B said you have a lower bound he's saying this one here changes at least by gamma every time this point here the inner product with itself squared norm changes it most by 1 every time ok so we have a lower bound with one upper bound of the other and now we basically what we want to do is you kind of you know one kind of grows grows a certain speed the other man grows can't grow faster than the opposite bound on the speed and behold the basically they converge at some point and you can see that that must conversion and I find a number of steps right so the define animal steps basically it can only go on for so long right if this is true that's basic the idea and let me move this up so that you can keep these two inequalities all right who's ready to move on all right good stuff so let's say we make em updates right so we've run the algorithm and we're still running and we've made em updates so after M updates there's now in there like in the TV show you know we said like 10 years later right like em updates later and here's what we say well we say m times Ghana I do have the same they make sure yeah so M times gamma is less equal then W transpose W stock why is that true yeah maybe we have to be careful W transpose W star you're almost right yep that's right so the first thing is W starts from zero so initially it's zero right and we've seen up here the W transpose W star grows at least by gamma every time right so after n updates it must be at least n times gonna okay so W Delta star W star must be greater equal m times gamma okay because every one of our M updates made this quantity larger by a forever by an additive term of gamma it doesn't make sense Razia no that makes sense okay awesome okay so one more time W transpose W star is the inner product with the hyperplane that we want to have so every single update it increases by gamma now mate M updates so it must be at least n times gamma all right good so now we can say I try to go really really slowly through this so that equals the absolute value that's easy actually because it's non-negative all right so you can just pick the absolute value around it by not right but what we can now do is the following you can say well that equals less equal then the norm of W times the norm of W star and does anyone know what that inequality is without looking at the notes anyone know it causes rights right this cauchy-schwarz inequality okay good so you have to that's one inequality as my money one of five inequalities in computer science that you should know know about if you don't know this one please look it up the basis that's exactly this if you have two vectors W W star you know a B then actually basically they're in a product must be less equal than the norms multiplies actually it's very fairly easy to prove okay who is with me raise your hand all right awesome good the only a few steps away it's so close okay what do we know about W what do you know a w star the norm of W star it's one right this is one so you just drop it right this is w this is no wasabi okay good so this equals square root of W transpose W why is that it's the norm if the definition of the North right that's you know that's just what it is ah and now comes the last step so W transpose W wait a second what do we know about our chance was W you've done M updates we just showed something with da B transpose W right that's like some some vague recollection wait just 10 minutes ago we showed that W transpose W close by the most one every time okay so this here can be at most M right because we've made em updates okay does that make sense this must be less equal the square root of n okay raise your hand if you're still with me all right good good good awesome so close so now you've showed something we showed that M times gamma is less equal than the square root of F so that actually is very interesting because M is the number of updates we made and we showed it an equality bassy prove it inequality about this M times gamma it's less equal than square root of M but we can now solve this for gum if I am can square both sides we get M squared gamma is less equal M sorry I'm sorry gamma squared thank you thank you thank you good point or you can have it you can divide both sides but actually maybe you divide both sides but that's good and that's actually ya better than you actually get scared of M gamma is less equal one so you know if you now flip it around you have easier 1 over gamma squared M is less equal 1 over gamma squared now I came it down why are we done why is this a big deal this results made Rosenblatt famous yeah that's right so earlier on we said well after M updates right in the moment I said this you could prove wait a second M can't be larger than 1 over gamma squared right so you can't make more than that many mistakes well gamma is some value whatever it is that say it's point one right one of it's point one right you know what do you get tens do you know one hundred updates right that's it after that you're done mm-hmm so that's the extremely strong result i busy shows that the algorithm converges in a finite number of steps and the number of steps only depends on how far away you know w star is from from the closest data point now first any questions any questions about this last step raise your hand if you you think you've got it alright awesome so I've one question it's not just one w star there could be many W stars which one would you pick yeah that's right right the one with the largest margin right so baby says the perception converges like you know if you take any possible hyperplane but you can choose any W star you want right take any possible hyperplane if you take the one that has the largest margin right so that's basically right in the middle in this case actually what is the optimal hyperplane right it's not this one B probably something like this right here's my X here's my all right you probably have something like this like this right it has maximum distance to both data sets so they're both classes okay and if you take that hyperplane and you measure how close you are what the margin is all right that basically bounds how many steps you need or in other words if you dataset it's really very well separable so data points are really far away from each other that means you can place a hyperplane with a large margin that means even converge very quickly that's on the other hand if you dataset looks like this see if your X is right and the X's and O's they're kind of hanging out right and I think that's right the really really close together right that means the hyperplane here has a tiny margin man has been right that actually will take a very long time right which intuitively makes sense because you have the wiggler dried into between these two two cents yes no it doesn't define are you hyperplane that's basically the only guarantee you have so could be that this really awesome have a clean exists and the one you find is this yeah right that's correct and so what we will do very soon we will talk about support vector machines which will actually find exactly the time that's you know the site a different optimization procedure but it was invented 20 years or 30 years later yeah any buds good question any more questions yeah that's that's right so so for a maximum margin hyperplane you will always have the same margin on both sides right if you didn't if one was further away then you could always move the hyperplane a little bit and actually get a better margin yes yeah so this is your question what if you didn't do rescaling if you didn't ruin rescaling essentially the same thing holds right because you could do if you just rescale the data said you're just rescaling all the updates actually everything still works out that's all right there would be some constancy yeah that depends on that's right that's right yeah I guess is the maximum is the norm of the maximum difference yeah yeah and mother that's yeah I mean all I'm saying is that you can't have most of em steps after that basically this inequality can't hold anymore that's basically what it means like you know these two inequalities can't hold for longer than that many steps these two that the busy the W transpose W grows by the most one all right so the payee you know the implication here basically tells you gives you an upper bound of how about W W star is and what I'm gonna have you squares so so think about the following you basically have a function of W transpose W must grow at least a certain rate right let's actually Tommy tries to stop you grows by you know so judges pose W must be less equal than this must be in this region right and W done suppose W star must be greater than some other function it must be you know greater than and gamma or something alright and for these two to hold I guess you know so what about we Beasley did here is he busy said sorry let me just yeah as also I guess let me just think about the intuition so the I guess you have gamma times m and 1 times M ah sorry you know what I can't figure the division outright on another fly but essentially what you're doing is you have an upper bound and lower bound who meet up at some point yeah yeah so yeah I've added one of you yeah so so that's that's a good question right so you busy saying we assumed that W star exists what if it doesn't exist right so did the cheeky answers you run the perceptron algorithm you wait two days and if it hasn't converged then you know it didn't exist probably there there's better wait so once we learn the perceptual SVM algorithm you can actually solve for this yeah exactly and you busy can buy there's no optimization problem and this optimization problem has no feasible solution so you can test for that very easily yeah sorry there for someone else first yeah answer I can't understand your question it does not guarantee conversion so if W star does not exist then you have no convergence then then it will always converge yeah after that many steps it must because you can only have that many updates so this means it must converge basically after this after this many updates you can't make a mistake anymore yeah well if gamma is really really big right and this actually yeah and that is not a bad thing right actually means you come with conversion much faster and then who asked the question someone yeah so the perception work spread on high dimensional data yeah it's not sitting right in low dimensional a that actually it's unlikely that you can find a hyperplane that separates to class okay maybe last question yet so it depends on the scale of the years I'm not sure if that's true but it depends on the separable ax T of the data all right that's that's really the case is that make sense so I forgot who asked the question yeah yeah well so don't play again ask them what you saying if you've been saying few things that it's really large videoid we need our scale so the thing you have to at the beginning have to scale it down to this unit circle so they have to scale down a lot and therefore they're gonna be really close together but that doesn't it's not really true right you could have data points at a really really large scale but really far away from each other and I scare them down to the unit circle and then you have some points here at some point here right so maybe I scaled them down by a factor a billion right to get here it doesn't matter that the margin is huge right it's really about how close together are the points of different labels that's really what it is okay um all right so the actually you won't have time for the next topic but let me just tell you a little story so the and a little bit of history of this so when the perceptron came out in 1957 it made a huge splash right this was gigantic this was all over the news it was considered the first time the people this was by the way near uninspired like it seems a little weird right now because the hyperplane right but the idea basically is that you know what a neuron basically you know either fires a doesn't fire and so that's either positive or negative and you take all its inputs basic a linear sum of the inputs that never spin of the idea with whether it's came from and so this was celebrated as the first nerve cell the first artificial neuron right in the newspapers were full of it like you know everybody was reporting about this and it lets a huge AI boom I even get an article about that I linked to from from the web page in the New York of 1958 that actually discussed the perceptron and it talked about everything there is like remember it it ends with this beautiful sentence saying of what does the Cape what is the perceptual not capable of but will it ever understand love or human sex drive and didn't say I'm confused about what they've you know author was confused about there's something kitty something he's not getting but you know either about the perception or about you know something else the but the important thing is it was read as the hyperplane in a couple years for this to catch on and marvin minsky actually then famously had a book which he actually called the perceptron so he actually want to write a book about it and show how awesome it is but he showed us the mutations and he showed this very very simple data set we said well here's a data set that a perception can never learn right give positive points here and negative points here right it's called the XOR problem and this is a data set with four data points and there's no hyperplane that separates the circles from the x's right and it was not mean-spirited it was really just about you know clearly that's a limitation of this assumption that things are liberally acceptable but when people read this they'll be like wait a second it can't you kind of think it's this right I thought I'm gonna have like armies of robots right like you know I want to have my next husband should be a robot or something you know and people got so disillusioned that actually the entire funding for AI dried up so I talked about this in the first in the very first lecture this was the death of a the first death of AI right when people realize you know at the beginning that hurt or what is an artificial neuron all very soon we will be able to build brains they are gonna be artificially gonna outsmart us and you know people we talked about what do we do do we have two laws against this and them easily can this very very simple example say well we can never solve this one with a perceptron right and it was such a disillusionment that AI basically docked and people stopped working on AI people started working machine learning and that's what we you know continue with next next lecture 
","['', 'perceptron algorithm', 'linearly separable data', 'hyperplane', 'perceptron convergence proof', 'perceptron update rule', 'norm of a vector', 'rescaling data points', 'geometric intuition', 'margin', 'large margin classifiers', 'historical significance of perceptron', 'Marvin Minsky', 'perceptron limitations', 'XOR problem', 'death of the first AI winter', 'machine learning', '']"
"all right welcome to another lecture of machine learning please put your laptops away also right before fall break I'm the only thing between you and fall break where's our spring break oh please what is it this February break whatever you guys caught it's all the same thing all right project one is due today this however to slip days so I will not actually focused on the leader board yet yes you have two more days to improve your position there one thing is we did find some people are complaining that they ran out of memory on Julia that actually turned out that was if you could reproduce this so we now app the memory so now there's no longer you be we quadrupled the amount of memory you have during grading so that should no longer be an issue and also there was another issue that well karyam every time they they graded on julia they used the totally new fresh julia installation and what happened was the moment you started actually installed all the packages so for ten minutes it was just installing packages and then it was starting to grade since it's been ten minutes installing packages and then 10 seconds grading that's now fixed so we contacted book here and by the way via the first course to use julia i'm okarians so there's a few of these little hiccups but that's also solved now all right any questions about the projects in logistics yeah oh you actually get points for it right so the way we set it up now is you actually do get you know it's basically accuracy times five is the points you're getting so maybe originally what we used to have is that whenever you beat me you get extra points which is also what we want to do well karyam is still working on implementing that yeah that's right so currently we can't do that yet yeah sorry that's right yes there's also fame and glory if you're number one all right so last time we talked about the perceptron just quick reminder perception was a we made the assumption that the data is linearly separable and every time we see what we do is we go to the data set every time you make a mistake we make an update and then we showed we made this cute little proof let me show it the following that and you can take the norm of our W which is you know I guess this is our W the norm of our W can be the dizziest number of updates then we know that the norm of W grows larger than gamma times M but must be bounded above by square root of M okay so that's and we showed this within cautioning a Cauchy Schwarz inequality so that means that if the number of updates increase the norm of W has to be within this shaded region so kind of moving around here but because they intersect we at this point there's nothing to go anymore so this here's the possible largest number of updates we could possibly make before getting a contradiction and that was basically the proof that we made right so the first part of the proof was we showed that norm of the he's always larger than M gamma M the second one was that we busy show that it's less equal in spirit of M so it must be inside the shaded region and because but every update we move further to the right at some point there's no place to go anymore so the we must have converged all right that was the perceptron today we will talk about something completely different but basically if done the K nearest neighbor algorithm is what's the first intuition how to learn theory the assumption was points that are close have similar label the perceptual make the assumption that you can't find a separating hyperplane and today we go back to the roots and say well what you know what is our data come from our data it comes from some distribution P of X comma Y all right that was the very very first lecture that we said we draw our data from this distribution and one thing we talked about this was very brief during the proof of the K nearest neighbor algorithm was there well if you had that distribution if you had access to this distribution then you could do really really well right so then you could use the Bayes optimal classifier and so if you have this distribution you could face then solve for P of Y given X and then you just think in your X that you have for any given feature vector any test point and you get the distribution of a whine you pick the most likely Y all right so that that will be called the Bayes optimal classifier back then we use it in the context that we show but that's as good as you can possibly do and the K nearest neighbors is not much worse as n goes to infinity so today I want to revisit this from a different perspective and that is well if we have access to this distribution then we would have an optimal classifier right so the moment we have this distribution we have an optimal classifier so a logical thing to say as well we don't have access to the distribution but maybe we can at a we can approximate the distribution from the data and if you get an empirical estimate of our distribution then maybe that's still good enough to then actually use the Bayes classify so basically I stay okay we don't have this but you can estimate it from data and then we just you know go along from there and some sense someone could say that's actually what all of machine learning is about in some sense what we're doing is you know all machine learning algorithms in one way or another try to estimate the probability of Y given X it has a little bit of a subtlety or or is or estimate P of X comma Y I'm on a another and there's two different types of family of distributions one is you optimize a estimate P of Y given X directly that's called discriminative learning and one is you try to estimate P of X comma Y or you decomposes into P of Y given X and P a P of X and sorry it's again either decomposed best way or you can decompose this way P of x given Y P of y and this up here is called discriminative learning and this appears called generative learning so these are two different approaches of machine learning fifteen years ago it was a big question which one is the right approach and people fed is really strong about it actually like I remember one person is like well I would never trust someone with us discriminative learning right nowadays and this is kind of gone away I would say like actually turns out that I think the scrimmage of learning like most approaches nowadays that discriminant and so most of what we're doing in this class is you busy just the NPC you learn some function given at predicts why but it's it's good to keep in mind you could also go the other way around you could also say given a label try to predict about my feature vector would look like and then you can basically see by which label you know what is what give me the most you know that the feature that kind of looks most like what I actually okay anyway so the bottom line of what I'm trying to tell you is let's just try to estimate the distribution from data so let's just take our data to this one purposes estimated situation and then once we have the distribution well then prediction is trivial any questions at this point okay well the moment you have a probability you can immediately convert this to just a label right you just take each label you have a probability you go through all the labels you pick the one has largest probability and you I'll put that one in some applications the probability is essential and so for example a self-driving car it's a photography car you have to see some object in front of you you want to know is it a squirrel is the pedestrian right if it's a squirrel you want to run over if it's a pedestrian you do not right it's very very important to integrate this probability with everything all the other sensors you have so let's say the vision apparatus sees something right but you also have a radar that see something and so on time off to integrate all this information right if the vision apparatus says it's definitely a person the radar says is definitely a squirrel you don't know what to do right but if they both give you probabilities then you can actually make some you know a well-informed decision you always have to take into account how bad would it be if I ran over a squirrel or how bad would it be the run over pedestrian right so you probably rather break actually in this case but all right any more questions another one actually just that just because you guys know Siri and so on one setting we're probably it is a very important is actually speech recognition all right but you basically you have two machine learning algorithms so one basically says just you know X is what I hear is the sound I hear and why is the phoneme that you know it's currently being set so it's like it's you know basically like a letter you know the other one is you know take into account what I said previously and you know what word is likely given what I just said right so I say you know I like ice cream right it's very my ice cream could also be ice cream as in screaming right but it's very unlikely that I say I like ice cream that doesn't really make any sense in that context it would probably be ice cream right so you want to integrate it's to basically get you know one thing it's basically saying what is how likely is that I say every word given the past that I've said the other one is how likely is a word given but I just heard and you integrate these two together all right anyway that's that's a second it it's just a little thing all right any more questions okay so what I want to talk about today is how to actually estimate probabilities from data and some of you may be well familiar with this who's heard of maximum likelihood estimation folks heard of Maxim a post or a posteriori estimation okay very few okay all right all right so here's let me just come back start with the simplest possible distribution and we will later on connect this to the setting we have the simplest possible conditions just you just have P of X we don't know what P is but we observe some X's all right so there's no Y right now it's just a one-dimensional problem and so assume you know here's a setting oh by the way I realized after I put print that I accidentally didn't print the right notes I put it at the draft so there's typos in there so one Snickers for the person who finds the most typos I already found five or something just now looking at so okay so here's here I think so imagine you kind of go through you grant ads you know whatever Alec right you find an old you know very valuable coin from ancient Rome and because you're curious you want to know if I flip this right what's the probability that I get heads or tails all right so that's crazy the distribution the distribution is the probability of getting heads if I toss a coin I don't know this distribution but I can sample data from it all right that can be flipped a coin and record how many times I get heads and how many times I get tails so let's say you do this and you know observe the following thing is like heads tails tails heads heads tails tails tails six that's one more head somewhere whatever okay so I for time heads and six times tails all right the question is what do you think is the probability that I get heads or tails any estimates yeah yeah based on these four these four out of ten throws being heads you could say it's a point for probability that I get heads right and so how do i estimate this well what he did was the following he said well the number of times I got heads divided by the total number which is basically what we called an ax right so that's that's my approximation you know maybe a reasonable thing but it may also be that you just unlucky right like who knows you know could also be 50/50 right we don't know but let's just stick with this so the question is this thing this seems to make a lot of sense intuitively can we somehow derive this formula so that you know that certainly seems reasonable but is there some principled way of deriving this and while this is where Emily comes into maximum likelihood estimation and and MLE you basically say the following you say okay I have a distribution and this is the beauty has some parameters and and I apologize it should be a semicolon not a conditioned on that's actually a terrible mistake okay so you have some parameters and I'll fix it when I put it on the web and you have some data and what I yeah I don't know what these parameters are so these parameters in this case are what's the probability of observing heads the data is in this case the data that I collected you could call this here x1 and this here's my X N and the question is for every single theta that I plug in here I would get a different probability that I observe the data that I actually observed and the principle maximum likelihood estimation is that I say well given that observe this data which the which parameters would make it most likely that I observe what I observe all right so that's the idea and so in other words what we're saying is we say theta equals Arg max P of D theta that's the baby I say I want to find my data at my theta such that the probability if this really is the case that if I would I then actually draw data from my this distribution the probability that I observe exactly what I observed is maximized and it seems like a reasonable thing to do yeah no no no max finds the actual value the max would give you the probability that is maximized on the most likely theta and Arg max actually which theta gives you the highest probability you guys keep me on my toes good any more questions alright so we can now do this you can say all right what is the distribution that we have well we actually either have we have n independent events and they can either be heads or tails so you have a binary problem here we have two different values for our features well that's a binomial distribution right so you can basically say well what's the polynomial distribution is the following P of D semicolon theta is the following it's nh+ and t choose NH times theta to the power of NH plus 1 minus theta empty any questions raise your hand if that's clear all right awesome grazie and if you fall asleep some people are lying I see people asleep alright good so you want to find the theta that maximize maximize this distribution well one thing we can do is we can now say well actually you know maximizing this is the same thing as maximizing log of this why well because log is a a function that's actually increasing so whatever maximizes the log or the Maximizer if it's inside the log it's a monotonically increasing function and so we want to find this and so why do we take the log while a there's two reasons a math becomes easier and B as a computer scientists to always take the logs anyway and the reason we take the largest because if you might defy many numbers like computers cannot represent small numbers I think very small probabilities are very hard to represent a floating-point numbers so what you do is you always take the log instead of multiplying small numbers you take the log in these add them up and computers a very precise with precision all right that's like the 101 of a numerical stability okay so we always take a large just because a be a computer scientists and B be also I want to make the math easy so if you do this then we get the log of this this thing up here NH plus empty choose an H that's just the constant doesn't make any difference and then you get NH times log of theta plus NT times log of one mentally okay so far so good any questions raise your hand to get with me all right awesome ok good and all right let me get now Oh make sure I follow the steps that I have am i ok good I think okay good so you can now set this we don't want to solve this so you want to say weeks you find the maximum how do we do this so you remember from from high school how you maximize a function take the derivative and ten to zero right so what we get here is NH over theta minus because we have this guy here and T over one minus theta and if you equate that with zero we get this so take the derivative and if you solve those you get theta equals n H / n H + empty ok any questions it's going to be a quiz in a minute so this is your chance anything up yeah aigoo be a minimum but you can check and it turns out to be a maximum yeah there's a good question up you're right okay any other questions all right so this is actually mostly review most of you have probably seen this before so okay I claim you can actually use this to let me just Eva have time for the quiz let me first do the Bayesian approach okay who's hurt Bayesian statistics versus frequently statistics raise your hands Bayesian Bayesian statistics versus frequentist statistics raise your hand if you've heard about it okay good raise your hand if you think you understand what the war is about a very scary xkcd company guarded yes all right so I will explain that in a minute so the so this is good right next to likelihood estimations is a reasonable thing to do can anyone tell me when does may fall down like when may this bright break apart this seems like a reasonable thing to do right can anyone tell me when could this be unreasonable when could be run into a serious problem yeah let's a view only that's right so that's a B we tossed the coin right we toss it ten times we only have heads right well boys you only have tails right that's right so in this case you only have tails well and we have zero probability of you know actually you know being head so that seems pretty unreasonable right and why is the why why did that happen right so a B put too much trust no data and they're very very little data right so if you have very very little data and you may get some totally extreme solutions but you may see no are not true right you know it's not there's not a zero probability that you get has all right but you could easily get it right or think of the extreme case you say oh my sample size is 1 right you flip it once is it heads oh it's always heads right so clearly it does not work for small numbers of N and so one trick around this is to say all right here's what we do we basically we assume that before we started we've already seen M tosses of heads and M tosses of tails so for example you say oh I believe my initial probability is 50/50 let us say okay well I just add M tosses of heads to it and and tosses off well let me do this mmm had em head plus and tails so what my am here is actually hallucinating tosses that I never had made okay and the idea is well it can be say well I assumed that I initially have a bunch of of tosses that are their heads I get five tossed upper heads five tosses the details right so then this is this is originally close to point five right so now if I have to probably I just toss it a single time it turns out heads well then I actually have five over 11 right which seems pretty pretty close to 0.5 still okay does that make sense so I'm basically I'm cheating right I basically say well I don't trust like when I have very very little data I don't trust this estimate and I'd rather trust my prior belief but I actually think you know it is right and so basically what I'm doing is I'm hallucinating a few samples from my prior beliefs where I say well my private leave is 50/50 so I have equal probability of getting a head else so I just make up a few more coin tosses from that distribution and then I start then I start doing the nested and the nice thing is now actually these problems go away okay this is called smoothing alright is something that frequentist have used for a long time any questions about this the most common one is where you just cost once moving so this really just gets rid of the the of the problem that you have zeros right because the problem with zeros is that you you know if you modify the probability of 0 everything blows out right so you always just do this that's passed once moving yeah so it that really depends on you and the more you do the more you're biasing it was you're you're probably it's a good question yeah typically and in most applications and actually the homework which will come out very soon this project number three you actually you know typically you should do Plus once moving any more questions this makes a lot of sense and was something that Laplace himself actually invented one hundreds of years ago but since that is a very nice way of deriving it yeah hey I can't understand you sorry the acoustic in this room was really bad so you can give me but I can can never hear you so you already Basin you already you're five minutes ahead I will get to this in five minutes but now I'm just saying you busy just have some probability of you know yes the news name is samples of a heads or tails that ends up being actually a prior distribution over the parameter theta sir would you say yeah uh-huh what if you don't know all the possible okay what does that mean now I see well you usually you always have to specify all possible outcomes right that's just yeah but but it could be that some are much less like in others and so you could never observe that one so with the plus one smoothing you say well I pretend that in my data I saw everything at these points and so that's extremely helpful with with words right so people use this for spam filtering and you basically we will do this in the next lecture you basically look at how often what's probability of seeing a certain word given that an email is spam or not spam well most words you will notice a huge tail of words that are rarely ever used in the English language right English language has millions of words most people only use ten thousand right and so what you see is great to just say all these words I observed once in spam and once or not spam so just to avoid put divisions by zero okay any more questions okay does everybody understand that that fix all right that's very simple fix to they avoid these degenerate cases this okay so that's no gay it turns out that's actually it's not just a fix that's actually turns out it's a and that's the solution of that's the map solution and that's the Bayesian approach of doing this estimate so macro maximum likelihood estimation is frequentist statistics and maximum a-posteriori approximation is bayesian statistics and so frequentist impatient statistics is busy two schools of thoughts within statistics and it's a little like a religion right like religious wars right so they basically they agree on almost everything except a very like there's a small change definition but actually statisticians for citizens that's a really really important difference and I mean there are certainly many people you know who a Bayesian would never talk to a frequent right or would never trust their children to be you know in the same room with you know only vice versa right and in fact I see the we had actually here at Cornell he actually had a fistfight break out in the stats Department between basements and figurative like only a couple years ago right so so it's still it's still very alive right and people like just at some point they couldn't take it anymore I had to punch someone in the face right over the difference of the definition of a random here and let me let me tell you what's behind so here's what we do so Bayesian busy says so as a frequency has to be the estimate the probability of the data given theta okay that's that's all you want to maximize you say which data which parameter makes our day that most likely now that's all fine invasions agree with this now here's the difference the patients make and I'll be careful right it's a small but crucial difference patients write it this way all right I just might give a moment of silence to appreciate the difference wars have been fought over semicolon days it's straight the vertical fine now what does this mean this means that theta is no longer parameter it's a random bear and conditioning and bases what's the probability of D given theta theta is suddenly a random variable that I can condition on all right so bayes's if you do Bayesian statistics you allow something like theta to be a random variable frequentist strongly object to this and the reason is theta there's really no no event right no probabilistic event no sample space where theta is associate with any outcome and so the question is like what is the definition of a random variable and this is just a parameter right so frequentist says there is no event that's that this is just not a random variable it just doesn't fit the description of a random variable and basically goes like well doesn't matter right and that that's really all there is to it but you know if you you know you can't fight with people with this and but has interesting implications so if you're not Bayesian and you say well this is a random variable then you can actually say well there's a distribution that over theta which would to figure this makes no sense right because what is that distribution it doesn't exist as the parameter that there's no event but you can draw theta as well but well let's just go with it so what Bayesian spacely say this distribution encodes your belief over what theta should be and the Mize is important it's important that if you if you have very very little data one thing we just saw was just pointed out to us is that if you do maximum likelihood estimation you could end up with a very bad estimate of theta and I guess bigger this would say well you just you know tough luck you don't have much data so what else can you do write a Bayesian would say no no no I have a just I have some belief over what theta should be right so basically there's some proper distribution from which I draw Ceyda and ah the basically the fact that I get this really really extreme theta is extremely unlikely under this distribution so I have to discount it right even if my data says the probability of heads is 0 but my private leave tells me that's a really unlikely event all right so I just don't believe it and so I incorporate this into my distribution and into my estimate of theta does that make sense so the nice thing of Bayesian statistics is it allows you to incorporate prior beliefs like you basically you've got feeling right or something you experience into into your estimation process now ultimately frequencies do this - right because they do disperse one smoothing I just removed it right and ultimately that actually turns out to be they're very similar right because why can you do just add one to it essentially actually turns out that's exactly the same thing and but that's basically different so if people talk with you about Bayesian visits frequency statistics it's very very subtle one thing that's really important Bayesian statistics has nothing to do with Bayes rule right that's just the guy was you know I had a long life and you know invented many things and the Bayes rule is named after him and all the Bayesian statistics but frequencies totally believe in Bayswater once actually heard someone give a talk and he said like oh yeah but you know the advantage of Bayesian slits that we can use Bayes rule no no but everybody can use Bayes rule it's it's a true rule fundamentally accepted any more questions all right so um so the pee-on taking the meters okay so if you have P of D given theta that's the likelihood of the data that's what we minute maximize with NLE this here is my prior over my parameters all right so that's where the Bayesian comes in we say now theta is a writer and variable and if you can do this then they can also flip it around you can say what's the probability of theta given the data and that's the posterior and that is what we estimate with map so we basis a given that we have data what is the most likely parameter all right so it's just me flipping things around earlier we said which parameter makes our data most likely another way of doing it is to say given that we have the data which parameters most likely right it's not the same thing it's related that's not something any questions so this is ml e maximizes this and map X maximize this and these are related and I mean they are basically believed to base P of theta if B equals P of D given theta times P of theta over probability of the data okay so the posterior is the likelihood times the prior over normalized okay this is Bayes rule but everybody is allowed to use Bayes rule any more questions okay good so the question is what you'll be used for this distribution right so this was pretty clear this was a binomial distribution right because we have n independent tosses they can either be heads or tails that's exactly head-on you know this is like there's no no doubt about there's a binomial distribution what about this one right what's our belief over theta what could theta right and so well if you actually had a head of coin right well what would you believe that the probability of heads to be right you probably think is around 5050 maybe it's fifty five forty five because it's an old coin right it could be a little biased in one direction it's very unlikely to be you know 0.9 that it's heads right unless it's um you know some bizarre coin ISM thang so you basically say well you know my theta here maybe you have some distribution that looks like this right or something that looks coughs um but actually Gaussian is not a good because that would go into negative but you don't actually did I cannot be negative so one distribution actually but people usually use in these cases Dursley distribution or specifically if you have two two events that actually have a beta distribution please tell them I'm busy Thanks all right let me let me formalize this no I'm never gonna okay here's the so let me just phone lies this so the probability of theta this is the beta distribution as the following is Theta to the Alpha minus 1 times 1 minus theta to the beta max one over normalizing in general what the Giusti distribution gives you is a probability I never have to get it let me H not get into this because otherwise you won't need it later on so weight is just a special case ok and so that the nice thing about the beta distribution basically is gives us a value between 0 & 1 right so so the theta will be a well-defined distribution that's the important point so the theta will be a well-defined probability distribution right and depends on the parameters alpha and beta the distribution can look like this between 0 & 1 that's what for example mean that it's very likely to be low or large or it can easily look like this maybe say it's somewhere around the center and this is usually what we want in our case and the alpha and beta actually could correspond to how many hallucinate it well if we get to this how many hallucinate it causes me would it make but let's just let's assume you take this distribution we say okay so one more time right we're trying to estimate theta from the data but we say well we we have some prior belief over theta we say theta is drawn from a beta distribution so this is kind of it gonna look like this year then once we have this we can immediately say okay well now actually you would like to have the theta that is most likely given our data and that is because of Bayes rule right that's actually proportional to the probability of the data i given theta times the probability of theta so this is if you divide by the data then actually this is exactly this is exactly the Bayes rule it's the Bayes rule relates this term with these two terms that doesn't make sense raise your hand of that makes sense okay awesome all right now we want to maximize this thing wait a second what's going on here right this here basically is you know NH plus MT over a choose NH theta 2 NH theta 2 NT times this thing here right forget about the normalization constant this is alpha minus 1 1 minus theta to the beta minus 1 we can now take these terms and put them together and what do we get this equals cleaner to the NH plus alpha minus 1 1 minus theta and T plus beta minus 1 so all I'm doing here is I'm now collecting all the terms so if you have theta to the NH feder to the alpha minus 1 I just collect these to add them up add up the exponents you do the same thing and sorry this is a 1 minus and this is what I'm getting and as anyone realize what we're doing here what does that correspond to if you now solve this for theta what do we get if I now solve this maximize for theta what do I get there's almost the same thing that we had just a few minutes ago just all these alphas nice paid us what do we get that's right any just replaced by any to alpha minus one so what do we get we get theta equals NH plus alpha minus one over n H plus N and T o plus alpha plus beta minus two okay so what does that mean that's exactly the same thing as smoothing right so basis means we take alpha minus one hallucinate it head tosses that give us heads in a cough a minus beta minus one hallucinatee tosses that are tails so turns out actually frequentists and patients have been doing the same thing over and over again now you know for years it's just they the like with their different approaches they actually ended up at the same same solution raise your hand if that makes sense okay so what I really want you to take home here is some people use this throughout this class is that we will have properties abuse ins that we estimate from data it will either do Emily what will you do we'll use map and the map is levy s to be try to maximize theta given D and Emily each other estimate maximize D given theta one is Bayesian the other one is frequentist it really doesn't matter for us but it's it's important to know what people are talking about when they talk about you know this is disagreement with in statistics about what the right approach let me end with one last thought that's the Bayesian approach to call make theta a random variable allows the enormous amount of flexibility and that's why Bayesian skew so strong about it right like you know you'd say like why do they care right like you know ultimately they're doing the same thing well if you're Bayesian you don't actually have to maximize you don't have to there's multiple ways of flying thither right what we did is said what's the theta that maximizes theta given the data right so given the data that we observed what's the most likely theta but that's just one way of doing it right here's another way of doing it that XT is kind of the true Bayesian approach if you basically say well actually if I really want to estimate a new label or something right then I would like to say what's the probability that something is heads given the data right I don't care about the model so what I then say is I say well actually that depends on my model so for every possible theta I can say what's the probability that I get heads and theta are given give the data and let me let me go through this in a few seconds I can see that's probability of heads given theta comma T times probability of theta given D so this here is the important equation so if you truly Bayesian then actually allows you the fact that theta isn't that your model parameters are now a random variable allows you to integrate out your model right and that is why people are so obsessed about alright this gotta be the most beautiful thing you've seen all day hey what are you saying is my prediction right no longer depends on any model what I'm saying is I take all possible models in the universe and I say if I had that model what's the probability of this prediction times the probability of actually seeing that model given that the data that I have hey cheer up it's so beautiful hey and you know what I let you go on your February vacation with this equation in mind see you next week 
","['', 'machine learning algorithms', 'estimate the probability of Y given X', 'discriminative learning', 'generative learning', 'Bayes optimal classifier', 'maximum likelihood estimation (MLE)', 'coin toss probability', 'binomial distribution', 'likelihood function', 'log function', 'frequentist statistics', 'Bayesian statistics', 'maximum a-posteriori (MAP) estimation', 'smoothing', 'alpha and beta parameters', 'random variable', 'Bayesian approach', 'integrate out the model']"
"I hope you had a nice February break good with one person did please all close your laptop's ok so last time we talked about the following thing we said well we have a machine that I'm using you have a data set D that is strong from some distribution P X comma Y right we have n data points this is the power of n so there's this distribution that we don't know about and from this distribution we have iid drawers that I basically make our trains with end-to-end drawers that our training data and this distribution will forever remain elusive to us right this is something we don't know so for example if you have images of you know faces or something this would be the distribution of saying if I go outside and I take a picture of someone right the probability of getting a certain face we know with a certain label that's maybe the identity or something or student or professor of whatever and so that's some random distribution but it's not a distribution that you can have think off and you know typically now one thing you could do is you could think well if that's not too much you know to complicate of distribution we could approximate it right we could define some distribution that we understand that has parameters theta and we say well we try to fit this distribution here to match the distribution that you know we don't know okay and once we have this distribution then we could you know use the Bayes optimal classifier right so all our problems would go away if we had access to this distribution then you could just use the Bayes optimal classifier and we would get perfect prediction right perfect in a sense we couldn't do any better maybe we minimum possible error so what we're saying is okay well that's that's you know we can't have this but we could estimate P let's call this P theta the some distribution that we understand and has the following parameters theta and we try to match this certified this theta such that this distribution matches this this elusive distribution as much as possible and well we don't have access to this distribution but we have access to n samples this is our training data so the question is can be somehow find the parameters theta such that this distribution here which we understand you know would give rise to our training data with high probability and so we talked about two day or two different methods to do this the first one is maximum likelihood estimation and maximize likelihood estimation we said theta we find theta as such that it maximizes the probability of the data that we observed right so you easily say now I want to find the parameters theta such that if I plug these into my model right the distribution is very likely to give me the data that I observed I don't that's the case and for all means of purposes these these distributions are probably pretty similar right because if I sample data from them I get you know pretty much you know very similar results okay so that's the idea behind maximum likelihood estimation so you want to maximize the likelihood that you observe the data that you did observe the second thing is maximum a posterior a posterior II estimation every say theta is we flip things around we say given that we observe the data what's the most likely theta so we say I want to maximize the probability of theta given that we observed the data now the crucial difference here is that he appeared as a random variable where seals not here's the parameter and I made this clear by actually putting it kind of as a sup index year so this year is a frequentist approach we say parameters of parameters we fit them we're here we go into bayesian territory we say well this could also be a random variable it's not associated with any random event there's no sample space we could sample theta but let us you know let's not let us stop and let the stop us you can still treat as a random variable and then you can have a distribution you know today over theta and that's the prior distributions we basically say in order to do this we have to have some prior to stab you P theta that be defined and then we can actually do this fitting yet then finally the last thing is that's the truly Bayesian approach that's saying well actually these are you know these are just mapping you know these are estimating theta for a purpose right why why we even interested in estimating theta because ultimately you would like to make predictions ultimately once we've done this then we actually what we want to do is we stick our theta into our distribution we say for a certain certain test point you know X what's the probability of Y right that's the only reason we are doing this so why don't we just do this all along and that's the fully you know that doesn't me call this fully Bayesian approach it's basically we say well if you just want to make predictions then what we could actually do is the following we could say the probability of Y given x equals the integral over all possible models P of Y given theta times P of theta given the data and so here basically we get rid of the machinery model and that was what I was so excited about last time I said this is the most beautiful thing in something this is the holy grail I'd say a lot of machines I think people have kind of spent the entire career trying to find you know how trying to solve this equation maybe a Z say what actually machine learning right it traditional approach machine learning is you have some training data you fit a model that's my theta as yet these are the parameters of my model and then using this model I make predictions and the fully Bayesian approach what you're saying is my prediction averages out all possible models right and it weighs every possible model you could conceivably think of by the probability of obtaining that model given that the data that you observed we will get to this in a little bit like in a couple of lectures but it's just the prime and Samson's keep this in mind right almost always this is impossible all right this integral is impossible to solve but for very obvious reasons because these are very very hard terms in here to estimate but there is actually at least one algorithm but you can solve this in closed form it's a very beautiful algorithm and so occasionally it works but what I really want you guys you know take home from last lecture is M le maximum likely this to me and Matt okay well you basically they well there's two different ways of estimating estimating a parameters from from data any questions about this it's gonna be a quiz in two seconds yeah the distribution you choose stays constant so that that's just a distribution that you get out at the end yeah you just get a distribution out that's right right your marginalize out basically there yeah any more questions all right so what we then did is my hope was to put this connect lip this around No to have space I don't have space okay do you mind if I raise this any objections I hope you remember this right well you know what why don't I show you a demo right now so here we go I have everything set up why not okay so here's what we did hey what's going on okay perfect so then what we did is we look at the simplest possible case right the simplest couple of cases we just have P of X so we don't even have any Y now and this is just a toy cars a coin toss so you find some old ancient coin you don't know if it's exactly biased and you want to know what the probability that this particular coin gives you heads right so that's the distribution you don't know now you sample some data as you toss it ten times and three times you get heads and seven times you get tails and now you would like to know well based on that data can I estimate what the probability of heads is and so we derived this and we what we found out is that with a maximum likelihood estimation the the answer of P of x equals heads was the number of heads divided by the total number right and if you do map then actually what we get is P of x equals head equals the number of heads divided by the total number let me take this number of heads plus the number of tails plus some terms here so you actually have unloosen ated heads and hallucinate it yeah sorry I lose that heads so I loosen eight the heads and the news they tails that was have you put a beta prior over the distribution so here the the distribution that we had that we put over theta basically translated into saying me hallucinate some tosses so essentially what we're doing when we do map we say well we have a prior believe over coins right so even if all our Casa tosses are heads I still don't believe that there's a hundred percent chance of getting hats right that's just my experience in life tells me that right that this may be actually an unlikely event right so what you do is you specify a distribution over your model beforehand and you say that's what I was I be prior to seeing any data this is what I believe my distribution of a theta will look like and then as I get more data I basically you know shape the distribution you know base the update this distribution over theta so this here's the distribution p of theta given the data so basically in some sense of if the data is the empty set you start out with some institution that you believe our priority and so in terms of the coin toss that just add the base it just means you just add a couple of imaginary results you busy say well I start out with saying initially you have a 50-50 distribution so I'll just say I have ten heads and ten tails and that's up what I begin with and then I start tossing them let me code it up a little demo and let me just run this so here's what I'm doing now as I say I'm running oh yeah it's mine Julia okay good so what you see here is maximum likelihood estimate and map and on the x-axis years how many coin tosses I observe and what do you see here's the estimate of my probability theta that's the probability of getting hats okay so basically after how many after you know just like a this is just you know one one but it's very you know this is like a couple of coin tosses basically you could very extreme values right but then as if this is a very thick by the way after seeing 10,000 tosses it tells us they all kind of approach 0.7 which is the true distribution right so I rigged the system saying this coin in reality has a 70% chance of resulting in heads and a 30% chance of resulting in tails so after I toss it 10,000 times you know my estimate is basically right around very much around 0.7 so after 10,000 you know coin tosses I'm pretty sure it's a you know it's a 70/30 ratio and the same thing with the map in this case basically what I did is I said I'm not at my map parameters with 0 in 0 so I actually map is basically the same thing as Emily right so I said I'm not hallucinating any coin tosses in either direction what you see here on the left is my prior belief so when I'm saying I'm not hallucinating anything what does that mean that means I have absolute an uninformed prior that basically means probability is just a it's just a constant you know here so it's basically just saying I know whether there's one point zero but it's basic I look at the shape so the shape basically says any profit this year has please the probability of a desire give it a particular value of theta and this may be how likely I believe it is and up front I'm saying everything is equally likely I have no prior belief I can now run this again and say okay but I'm hallucinating one heads and 1 tails to begin with I'm running this again and this is what I get right so what do you see you on the left this here's my believe now right this is now chanting to beta distribution with parameter alpha 2 and beta 2 that means I really believe it's a 50/50 chance right that basically means I mean I'm hallucinating an equal number of heads as tails and what you can see here is Emilee is unchanged because it doesn't affect anything but map here in this case base because I hallucinate it one coin toss is heads a monkey winters as tails I start out with 0.5 and basically you know here we have much fewer extreme values any questions about this this demo any questions about what what I'm showing here the different graphs yeah for the left one is actually a PDF so this is the probability density function yeah yeah sorry over time no smoothing family which is that would be exactly map right so smoothing for Emily is map yeah any questions yeah then actually that the Friars uniform right so look at this fire here right now I'm saying the prior is basically I have a strong believe it's 50/50 all right there's some chance you know it's spies into either direction it's very unlikely that it's a 1 or 0 right and the reason is that is because I hallucinate it one heads and one tails already right so it can't be that it's all heads or all tails right it's not really possible right um if I let me just show you this one more time if I make the 0 0 then basically my prior belief just becomes a flatline all right I see everything is equally likely I have no no idea what it could be okay does that make sense it's called an uninformed prior I'm not biasing the system in any way any more questions like it out for example go up and say well what if you do 10 10 right it's now I've just to loosen it a lot more and what do you see is that the prior becomes a lot sharper right because I had 10 tank tossed already of heads 10 causes of a of tails I'm a pretty strong believe it's 50/50 now it's much more unlikely that's even the point 9 range right and what do you see here with map is at the beginning it's very much dominated by these hallucinating throws right so it believes maize leaves sticks to the point 5 until it sees more and more heads and eventually moves up to the point SAP okay one thing you should observe it's important is in the limit if I see it you know a lot of coin tosses it doesn't matter right they both always end up with the same solution right because at some point the reality is gonna dominate my you know my data right but if you have very little data then actually the data is dominated by these hallucinating tosses and so one that you can see now it's because point five is actually not so good right at the beginning it's a little bit off right it could it make this even more extreme if I do one hundred and hundred or something so now actually you see it's kind of like it's really really confident right that it's point five right which is not true right so the distribution here is I'm - it's like a the confident teenager or something right it's like it's really really confident I know it's this one right you're wrong right it has to see a lot of data to actually then get convinced side of reality I apologized any teenagers in the world thank you and so you can see there's actually not a very good estimate but you need after hundred samples but you're still way off right and actually 0.5 you know you ways away here right but Emily is actually a lot better right so the important thing is that map estimate is good when you have little data but if you actually have the wrong prior right then it's not good at all all right let me up easily then it can take you longer to converge another option is also let me show you another example here where I'm actually running it sorry ma'am I I'm here you go and I could also just do three and seven right by actually or two and eight let's do this two and eight but I say only hallucinate not many but I lose say two heads and a tails right so I'm not sure about it but my private leaf is there actually the probability of heads is pretty low right in this case I'm Way off it's actually the other way around right you can see it leaves a lot of data to move up here okay on the other hand if I makes you right let's say I'm actually correct and I do 7:3 for example which is exactly correct right then actually you see that map is really really good right I'm starting out exactly where I'm supposed to end up and you know I just get a little bit of variance here and very soon I'm a peter out right if I can make this even more extreme as I just seventy thirty seventy well it's a seven hundred come on let's go all in all right so I'm really really confident and alright so I'm really confident this is the solution and it ends up being the solutions then in this case of course I'm dead all right any more questions about this demo yeah so well it depends right so if you are right with your prior if you're close then it will come greater might faster but if you're wrong right if you do the following if you do you know eight hundred two hundred oh shoot what did I do Julia stopped well that's terrible one more time yeah so so he I'm actually oh wait this is the moment right in this case I'm very confident that it's that it's down here right any that it's busy a 20% chance of heads but it's really a 70% chance of heads alright then you can see doesn't converge for a long long long a long time right even after a thousand samples it's still way off look at this year this is a thousand samples right here's where we are might be at point four the answer is 0.7 right it's really pretty bad right where's Emily at a thousand is actually pretty good right it's here the thousands actually right around point seven right so it really depends on your own your prior and you can bias the the answer basically so map is the only good if your prior is this makes sense yeah by the way people have been at these endless discussions instead of statistics in statistics if people you know if you're allowed to have you know use Bayesian statistics or not and if these priors are reasonable right and it's part of this people have actually done experiments where they want to figure out is the human brain is a frequentist as a Bayesian right and two people did these bizarre examples where they they ran out to random people on the street and told them you know I have a have a cake in the oven how much longer should I keep it in their head is now impossible question you don't know what cake you don't know how what oven you don't know how hot the oven is you don't know how long it has been in there right the question makes no sense and people say 20 minutes and so basically what they want to prove is that well we have some prior but you don't use no it's not you know it's not a decade right so it must be you know it's probably 20 minutes sounds about right I don't know what is it banana cake wall at 20 minutes so those are kind of the you know psychologists have have had Phase II had a lot of fun like trying to find out like you know do we have phases or do we do frequent statistics and it seems to be that occasionally actually we do have we do show tendencies of Bayesian statistics but we actually have you know we do have certain prior beliefs you know that we basically used that's off by the way also related with our prejudices right and and and so on we basically have it jump to conclusions because of our private lives all right choc chocolate all right any more questions okay so let's connect this now to the more interesting parts of machine learning so last lecture we said okay let's take the simplest case repeal we just set P of X what I want you to figure out now is what if we go a little bit more interesting right so what if we actually don't have this set a which is not very interesting what if you have a following setting UPF Y comma X right and so X can take on a few discrete values and Y can take on the pewter screen values and what I would like you to figure out now and together with your partner is the following is what is the probability of P of Y given X right let's say you have data set now it's not but from this and how do you would you use Emily to estimate the parameters theta for this this distribution okay and just a reminder in P of X what we did is we do the following P of x equals little X we said the following the answer is we sum over all the data set in our a you know if there's any set up whenever it's I equals x divided by n okay so this here is the function that either gives one if this is the case of zero if not so this here basically is the number of heads right number of heads if this years heads okay so we said we count how many times we find X in our data set divided by the total that ratio is the probability that we actually see X okay does that make sense raise your hand that make sense okay good what I would like you to find out is what is the probability of Y given X in the setting where you now have a joint distribution of two variables label Y and a single feature X okay I give you maybe three four minutes please discuss it with your neighbor and try to figure out what is the you know what is the MLE estimate wants to volunteer the answer anybody yeah good you're brave okay good very nice so what what are you saying is the following we have a data you have a data about data we want to condition y on x equals x right so in some sense but what he's saying is that's not very different than what we did before with the coin tossing except that now we're only looking at the cases where X equals little X all right so that's what we conditioning on so beforehand basically we had data where we said you know we have a bunch if they don't want to know what's the fraction of of times we actually had heads now we do the same thing we said what's the fraction yeah that we have had so that we have class label see but only within the subset where x equals the legs or let me put it another way so we can actually say know if they have the following right this is the this here's the set where x equals x this here's the set where y equals y and what we interested in is this year out of the circle where x equals x okay so in some sense we don't care about the case where X is not equal X all right I can also raise the end of that makes sense okay good another way of looking at this is the following that is P of y equals y comma x equals x divided by p of x equals x that's just the definition right now these two we can solve this is exactly what we did before right this is down here this is some of our I equals 1 to N I of X I equals x / n right that's exactly what we had before and up here we now have not just one condition we have two conditions right so well no big deal right so we say I is X I equals x and y I equals y over N and the two ends cancel and what we get is exactly this ratio we sum about all the points is exactly what he said x equals x I and y equals y I sorry why I equals minus I X I equals x divided by the total number of points X I equals X all right so so one more time we go through our data set we say how many times do we see X and out of those times how do we OH how many times to be also see y equals y all right and that ratio basic tells us given that you've CX x equals x what's the probability of Y right so one thing you do is you can reason say ball just this one more way of looking at it you just say well I take my data set I throw away all the other samples where X does not equal X right and then I just estimate the probability of y equals okay does that make sense all right awesome raise your hand if you've got that result all right nice nice nice all right if we do if we try to estimate P of mic what's Y given x equals x right what we said is we just take our data set and we throw away everything you know all the data points you know where X is not equal X right well in the one dimensional case that may not be that bad right but imagine you have a high dimensional vector alright so you have a hundred dimensional vector so then really you know what you're saying here is you're saying that you know x1 equals x1 x2 equals x2 and so on x hundred or XD equals x d right so if you have a hundred there D dimensional vector you're specifying exactly every single value of the data point and you throw away all the data points that are not exactly the same okay so essentially what you're saying is well for a test point how many given a test point you know if you want to know the label of I what you're doing is you just go through your training data and you say how many times have I seen exactly that test point right and what what was the label when I saw it and you can see very clearly that you know if D is reasonably large you will never ever have seen that data point again right before right so I mean if you have a face recognition system or something and you want to know you know let's say you know it's a picture of a person is that a student or is that a professor right what you would do is you say every single pixel has to be exactly the same right have I seen that exact image before right and what's the professor or was the student right and then you get out put that label so really there's not a very good algorithm right that's kind of you know you're not actually you know you were basically always have this empty set okay so that's why we can't really do this in practice but there is a rescue and the rescue is the naive Bayes assumption who's heard of naive Bayes before okay good all right so the problem is that estimating P of X y equals y given x is really really hard right and we not really we can't really do it because we maybe tie together all these different features and say when we have this when have you seen exactly this combination of features before right and the answer is almost always never right so we can't make any predictions all right so naive Bayes turns things around so in naive Bayes and you will yeah there's actually by the way knife phasers put in this way an I face you turn things around see baby say well instead of estimated P of Y given X I just use Bayes rule and flip things around so I say P of y equals y given x equals x my Bayes rule is just P of x equals x given weight in between to do it yeah given y equals y times probability of mic was right divided by probability of P of X all right so this is just a normalizer don't worry about it this is P of y equals y that shouldn't be a problem okay because we usually only have a couple labels so for example in the example of you know it's a personal student or a teacher a student or a professor right well this is the prior probability if I take any one randomly is it the professor as a student that's just the ratio of students with its professors that's not a big deal if that's a spam filter right that's basically the spam filter this would be is the probability is the email spam or not spam right so that's just you can just estimate this on your own inbox it's not a big deal this here's tricky right busy given a specific email and let's say X are all the words and the email is based a long vector of words is that spam or not spam right you've never seen that email before so we don't know so we have to you know this we can solve this we don't worry about for now and the question is what about this guy all right well it doesn't seem any easier right give them the labor what's the property that we get exactly that email so what's probably that we have you know given that it's spam what's the probably that we exhibit exactly that email it's really hard to estimate right you've never seen that so naive Bayes makes a crucial assumption and you will see the minute why it's called naive knife pay says that the features are independent given the label so let me explain what that means that means that says P of x equals x given y equals y equals P this is a vector with D dimensions so what knife Bayes assumption says is each feature right is indeed independently distributed it can inform each other given the label so this equals alpha equals 1 to D P of X alpha equals X alpha given y equals y so this is the alpha damage right that takes on this particular value okay and I'm multiplying all these these different features you know multiply all them together to get the system let me let me make this clear in an example so let's say you have an email and the email can either be spam or not spam what knife phase assumes is that the way the email was written is that all the different words in the email are completely independent of each other given that you know provided that I know the label so in some sense you easily have a monkey who just presses certain words but he has two different typewriters right so that's the each button gives you a word and you have one typewriter for spam and one typewriter for not spam and so the monkey takes raizy at the beginning you make a decision do you want to mate by the spam email or not a spam email and then let's say you wanna buy the spam email then you just take that typewriter for spam emails and you just add words randomly to the email right according to some distribution that's the probability given that damn what's probably that this particular workers okay and you just draw them wrap any questions yeah sorry bye oh why they dependent they're not independent this assumption makes no sense right but if we make this assumption and we can solve it so you know sometimes you have to cut corners you're cutting a corner yeah it's a huge corner right so yes if you look at you emails right hopefully the emails you're getting are not independent words right but turns out if you make this assumption then actually it's computationally tractable and it turns out empirically that often actually this works fairly well that's as many data scenarios make this assumption you're trading off something why are you trading off you know busy make assumption that doesn't really hold on the other hand now you can solve it with you efficiently and turns out you not it doesn't break things too much too badly but yeah it's a very good point any other questions can anyone think of a scenario where it's actually bad actually holds yeah okay I see I see what you're saying so he's saying there's a very extreme assumption right why do you make everything independent from each other vikon just makes like partially you know it's like some parts of it independent of each other it makes the subject line independent of the main thing something I've never seen this it probably is too much it's not worth it there's probably other things you can do with your computation that it's more effective but it's a fair thing to ask anyway can anyone think of any setting where actually where they actually are independent of each other yeah I see so same analysis sentiment analysis it's the case for for example where you have a a texting you want to know is it positive is a negative it's a actually a scenario of a machine on a use lot and while I'm at it actually it's actually a billion-dollar industry and so what people do is let's say the new iPhone watch comes out what people do is they just quickly write programs that quickly goes to the blogs and check other you know are they about the iPhone watch and are they positive or negative much faster than any human could read them and they try to get in sense of do people like this or not and if they do like it then they buy Apple stock and if they don't like it they short sale Apple stock and people can make millions of dollars the billions of dollars actually using these algorithms so you know think about this if you make billions of dollars let me know office hours 9:00 a.m. Monday and so the question is are they actually independent I would still say they are not independent right because even if you say is a positive really you still the fact that I say Apple means it's probably pretty likely to ever say watch afterwards right so there's this they're not you know doesn't always it doesn't really work for language yeah pixel ologies are very correlated right so if I have a green pixel somewhere right the likelihood that the one next to it is part is also green is pretty high right they're highly correlated typically yeah sorry cards okay I think you see it so here's what I'm taking away for this he's thinking and sometimes in the right direction you think in terms of it as a graphical model so if people who's head of graphical models before okay that you took so for those people take your Africa models they says what we have here is we have a vector a label Y and that label Y gives rise to the different features and so the you you may have this independence if you actually have that these feed the value of these features are just truly caused by the label okay so you know for example at cell you have some some medical you have some disease right why is do you have a disease or not and these are symptoms right but these symptoms are actually caused by the disease right so telling you that I've symptom x1 does not tell you anything else about it makes two if I already know if the person has the disease or not right because that the symptom is completely caused by the disease all right so in some sense thank you typically there would not be independence right so that's a you have a you have a disease that may make you cause coughing may also give you a rash right the fact that you have a rash may tell you something that you are starting to cough because it may tell you that you have that disease which causes both but if I already tell you why is you have that disease or you don't have it then the two are truly independent of each other okay does that make sense so if there's a causal relationship right then actually this is typically the case okay any more questions all right so let's just work through it how we do the estimation when we make that assumption oh so yeah okay yeah you could you couldn't so the question is basically he said I'm not just assuming they're independent I'm also assuming they are IID actually right they actually in identically distributed you don't have to go there you could actually make them it could actually make it more complex by having different distributions yeah and we will get to this actually next lecture that's right it's a good point good catch yes all right good so um well if you make this assumption there's teeny little assumption then actually it turns out everything works are very very beautifully so let me remind you of what the Bayes classifiers right so the Bayes classifier says for vector X what I'm outputting is the most likely late right so I'd say art max Y P of Y given X all right so that's just the Bayes classifier I'm saying for a certain input X I look at my distribution which I now know and I pick the label Y that is most likely and that's what I'm outputting what I don't know this distribution but I can use Bayes rule became that that is Arg max PR x given Y P of Y over some normalizer and you know I said early on don't worry about it now you can see why you don't worry about because it doesn't contain Y right so it's just a constant okay this is P of X you don't have to compute it it doesn't matter because you're just looking at Y maximizes this this term okay so we can just take it out this does that make sense Razia no that makes sense okay awesome good and now you want to maximize this thing here we make the naive Bayes assumption so the key now well that is the same thing as art max over Y the product over alpha P of X I've given Y times pfy and actually maybe I should take this out to avoid confusing there's outside of the product okay so here but all I'm saying is this P of X given Y decomposes into a product over every single dimension all right this he has a nice base assumption now this here still a product of many many small numbers so it's ugly but what we can do is we can do the old trick we just take the lock right so in computer science you never multiply numbers right never ever much by many numbers it's really special probabilities because if you might you might apply probabilities you always get 0 up to the Machine position so what you do is you just say well that's the same thing as taking the log and then you have log of py plus and now you get a sum of alpha log of P of X alpha given Y alpha over given Y and this is easy to estimate and each one of these is also easy to estimate y is each one of these these are estimates can anyone tell me some people it's only one-dimensional right so that's the key right there's only one feature given Y right so that's no problem right so we just check how many times have you seen that particular feature right when we saw Y okay so that's that's no problem at all all right and you know we leave it at that and B see I see you on Friday 
","['', 'machine learning', 'estimating probabilities from data', 'naive bayes', 'distribution', 'maximum likelihood estimation', 'maximum a posteriori estimation', 'prior distribution', 'coin tosses', 'Bayesian statistics', 'frequentist statistics', 'classification', 'classifier', 'Bayes classifier', 'naive Bayes assumption', 'independent and identically distributed (IID)', 'spam email', 'sentiment analysis', 'text analysis', 'features']"
"welcome everybody all right today must be a beautiful day out because at least one-third of the seats are empty okay one one quick announcement so a project which might well be now three a knife base will hopefully be shipped tonight or tomorrow yep although there will be next next week will be three really cool machine learning talks here at Cornell for those who are interested we had one last week two weeks no two weeks ago on computer vision it was really really fun and deep learning I saw some of you went next week we have one on robotics machine learning one and healthcare machine learning even a machine learning for machine learning I will announce it on Piazza but if you want to see cutting edge research these are really really cool talks to attend all right one good things that one thing I actually didn't get around to yet is project one so that's now done and I just want to say we have it's really really fun to see the leader board people moving up on leader board so maybe I want to spare me one minute I look at the top results so Steve Nash and Carl Sagan can you repeat [Applause] where is the team Steve Nash and Carl Sagan can you raise your hands okay can you tell us please please be quiet guys can you please tell us what you did what was your secret sauce okay how did you for everybody also they did feature waiting so when you could be the distance you made it each feature differently how did you find the weights how interesting okay oh wow okay interesting so what they did is basically with the computed weights for every single feature and they use different functions for different data sets so they tried out basically for example a Gaussian waiting so that would wait points in the middle or a point you know X plus y I guess weights the bottom right corner them oh I see I see now okay interesting so there's a whole industry in metric learning so the way you make K nearest neighbors truly competitive is actually by learning metrics so that's basically you know you two guys took the first step into that direction right well you basically you know the idea is during training you actually given some data and now you actually want to learn a metric such that during testing a Kenya's neighbor classifier would do really really well and that is essentially actually how for example facebook does face recognition right so they actually have some very fancy way of computing metrics of faces and so then they basically say if you upload an image and the first to detect faces and then the question is who is that person well they now look at you know when you do a nearest neighbor classifier with this really really fancy metric that actually is optimized for faces and so one advantage they have is because people upload their profile pictures they have many many many faces and they also know who they are because usually you upload a profile picture of yeah of yourself not everybody does but almost always it is so they have a lot of training data for this okay but how about number two that's these ha ha ha witty kitty ha ha ha woody Kitty can you raise your hands are you here alright they're celebrating outside not bad how about team garma it's actually very close at the top the camera's not here either oh here we go oh yeah Team Garmin what did you guys do don't you speak up a little yeah Oh interesting oh wow interesting okay okay cool let's go data augmentation so it's actually interesting because these are completely orthogonal approaches so you can combine them and then you would actually if you joined your teams you would have feed everybody the so just one more time what he's doing is he base he takes the training data and he modifies the training samples ever so slightly such the basically the sample does not that label does not change right so for example if say you have a digit over six right if you just take you know take this image and just move it a little bit to the right a little bit to the left it's still an image of a six right that's that correct and so the advantage of this is you get a lot more training data right and so if you test them which maybe is a little bit to the right a little bit to the left then actually have a better chance of actually finding your neighbor right and this is actually a test that a trick that people use in practice a lot right it's called data augmentations or when people actually for example to do exactly this by the way they take images and move them a little bit they rotate them a little bit they in flip them you can't do this with everything right but for with people you can flip them right and they should still be you know faces etc and this way you basically artificially increase your training dataset it's a very good very good approach and awesome this is very very nice I love that you guys actually Paisley got to the top I actually you know adding adding totally reasonable things to the classifiers okay good so I handed out a little handout who did not get the handout raise your hands everybody got them a one-person okay who is - okay good mr. resolve the problem so there's a very quick little quiz it's not meant to be hard that's actually from an undergrad class an entry-level undergrad class but it's a nice base so why don't you and your favorite neighbor go through this and just try to figure it out one thing is just to clarify the last question do I really want you to use naive Bayes right so the early on when it says CPT cpt means conditional probability table that basically means what if I estimate the probability just given this table later on if I make a prediction that means what if I use the naive Bayes argument so please discuss it with your neighbor and just go through it yeah maybe exceeds well let me just okay so question seven so Susie goes on a date and anyway there's a you know horribly bizarre story but it what's the prior property that the person is good who can tell me the answer come on this is the easiest one yeah what happen is one half why is it one half because you just look at why and without any other knowledge right well fifty percent I'm fifty but half the people are good have the people I've met right so she has a you know just estimating P of Y from the data is just gives you one half probability all right so then she wants to train a knife based classifier and for this she constructed CPT though because EPT is a conditional probability table so that Bay says what's the probability of this this random variable given this random error so the first entry is what's the probability of not sees that the person's not wearing a cape given that the person is good-looking Tommy yeah it's one third why is it one third that's exactly right right so you may see say out of those people like you know you throw away everybody who is not good where Y is false and then out of those were in for y is positive one out of three actually does not wear a cake right so it's one-third and next one is the probability that person is not wearing a mask given that he is good what's the answer here yeah one third it again right this is exactly the same scenario and then question ten what is the value of probability P of not the person is not very okay another person is good if you do +1 smooth it can anyone tell me yeah yeah yeah 205 right so that's basically what you're doing is you say well only one of them actually does not wear a cape but I'm adding one to this that's where the two comes from right so if one plus one over three in total plus one that I added in the positive case of one's in the negative case right that's two or five so here you adding one for each class right so this busy add one heads and here you add a heads and a tail right can you raise your hand if you got that result correct all right okay so now next question Susie spots a date but to her disappointment he already sits at the tables you cannot observe his underwear situation what's the distribution that the person wears as Underpants outside his pants P of you yeah oh I see so this here is the total number of tosses in this case he bef coin tosses we're busy you either get a person with a KA cape or you don't get one with the Cape right so we're adding that's one smoothing means we add two tosses one with a cape among without a cape so the here you counting the people without a cape right that increased by one and the total number of tosses increased by two because he added one without a cape and one with a cape right as having something in there yeah Timmy they have to add one per class right okay what's B of you who's brave but this should be really yeah it's dessert right yeah you've used a third okay good so I for a second I was like then miss some things it's really hard no this is just me no just two out of six achoo right and so is that true about spider-man I guess I don't know okay whatever all right now comes the last one so now she actually does her inference right she sits down with this guy and turns out he does not bear a cape he does not bear a mask that's you know so what's the property that the person is good given that he does not bear cape and he does not very mask so who thinks he or she has the answer yeah one over nine no yeah no Joe's goes getting kills I know 100% oh you got si all over the place no no all right let's do it okay well let me do it here let me do it here then yeah so probability of why give him not can you read this in the back raise your hand if you can beat this okay good come on not math right so this we company observe right if you you realize this is exactly is the lemma you ran into this what we did naive Bayes in the first time in the first place right if you look at the the table you actually never see this right and so you know you want to basically use make use of the naive Bayes assumption so here's what you do you flip it around you say what's a property of the data that observed not see not am given Y times property of Y over the probability of not C not act okay so far so good who has it so far raise your hand all right all right now here on the top we can use the night base assumption right and instead of this term here what we do is we write this is the product over you know well let me just write it up the spear not C given Y times P of not M given Y times P of Y okay so that's that's a knife base assumption that leads to like given Y these two are independent so now because this factors who has it so far is your hand alright okay good now comes the second part down here we have this guy right well how can we actually apply a naive base here right we also have to apply the ninth base assumption and so what we do is we just say well this is given you know given Y prime and B some over Y prime so now actually it tends probably give Y prime so now this perturb your factors again okay does that make sense that this is exactly the same as what we had before you just you just basic you know look at both cases either yeah by you know well let me write this out well we basically get is P off see given Y P of not M given Y P of y plus P of not see given not Y P of not M if not why probability of not why yeah oh okay yeah let me just write this down one more time then no y prime is just okay let me write this one more time so what you're saying is based that P of Y given naught C naught M equals P of naught C naught M given Y P of Y divided by P of not see given not P of not C not am right and what is P of naught C naught M equals P of naught C naught and given y plus P of naught C naught n if not why if not why okay that's the same thing right does that make sense and now we've got everything in this in these nice level of conditional turns and now we can apply a the naive Bayes assumption right so now this year becomes just the product and of two terms raise e how did that make sense okay awesome and then you plug everything in so you know and all right so we can there now do this like times P off Nazi given wine not M given wine if you have not see why not my P Y plus P of Nazi if not why not M given not Y since P of not why okay now the first thing you do is you realize that that's P advise over the same as you just knock it out right you don't need it that's always one half so you can't add it in but it doesn't really make any difference and so P of Nazi given Y i if you have not M given Y well that's exactly what I let you complete computer Leon right that's exactly the answer the question seven in question a Christian 18 question nine so that's basically one-third times one-third down here you have exactly the same you have one-third times one-third and then what about this guy anyone tell me what this is what's P of Nazi given not why it's 2/3 right so two-thirds times two-thirds and that gives you one over five right so once if any questions right so the question is the following well you know this data set is nice right but you know actually the Susie's mom who creates that is that actually it wasn't Susie but anyway yes as we've imprecise um and so you're right right so you know if you go to a real date Wells very rarely do you're gonna date with spider-man right usually is a pretty busy guy right so it's not representative so that's that's actually not a crucial weakness of naive Bayes it's a crucial weakness of machine learning in general right so you assume that the training data is from the same distribution as the testing data right so in this case she's not dating any Superman superheroes then it wouldn't actually probably wouldn't apply but you would have to the fit of a distribution does that answer your question okay any more questions all right okay so let me continue them with knife face where is my chalk and at the end of last lecture someone asked me some a few have some good questions so one question was so we make this naive Bayes assumption all right the naive Bayes assumption basis says that we can that these features are income independent conditioned on the label and the first question someone asked well what if that assumption actually holds right what if that shoe is then naive Bayes a perfect classifier what do you think so if the assumption actually holds do you get the optimal Bayes classifier or do you not have a boat how about this vote if yes now who thinks yes whose things new things no we have low low participation here the answer is yes right so if your if your assumption actually holds but we haven't got any fudging since that assumption by the assumption was the one fishy thing that we did everything else is just you know absolutely accurate right so if your assumption holds then you actually recover the the Bayes optimal classifier right then it's actually very very good so one thing is Emma asked me like well when does assumption not hold or can you said I give you some examples where this is really bad right and I thought about one thing and like one question was also by decomposing P of X given Y into P of alpha P of X alpha given Y right to be over estimated to be under estimated probability and it turns out you really can't say that in general right so it could go either way all right sometimes you may be probably maybe larger and sometimes property may be lower can anyone come up with an example where this year is larger than that can anyone come up example with this year is lower than that so maybe I give you a minute to discuss it with your neighbor I think this is very helpful just for you guys to get some kind of intuitions about this algorithm so coming up some practical example where you say by decomposing this I actually get a larger value than this so overestimate the probability well come up think of some example where you can actually but you're underestimated who wants the one via 20 an example here we have someone brave yes probably oh gosh okay I don't know anything out poker okay probability that you have a full house given that you have three Queens if it is doesn't as net a full house already OSE the features are you have three kings and yep oh I see okay okay so you have two features one is do you have three kings do you have two queens and you say if both these are the case and you always have a full house that's your label the label is do you have a full house yes or no so okay good so both of these are the case then actually turns out you always have a full house right but actually if you if you decomposed it then actually well sometimes when you have three kings you have a full house and sometimes you do not right and so he actually okay good good I like this example that's good that's good any other example yeah handwritten digits you say the different pictures are not independent that's right that's true but this is also pretty low right because he actually saying what's the probability of getting exactly this particular digits right that's actually super super low too right so it's not clear which one is larger which means lower right and any more examples yeah perfect exactly right so let's say you have two events that are both indicative but they actually never occurred together right so a neat example is for example email spam right so this piece is used a lot for email spam but there's different types of spam right they're quite distinct right so for example you have viagra spam that people try to sell you viagra and there's Nigerian prince spam I don't know if you know did Nigerian principal like someone's a guy I'm from Nigeria you know and I have $30,000 that I really want to give you right because I'm from Nigeria and so please I just can't afford the transaction fee because I don't know why I have thirty thousand dollars but the story doesn't really add up so why don't you give me two in a box and then I will give you $30,000 all right and surprisingly many people end up doing this but these are very different spammers right so these are totally different people they have you know there's nothing in common so both of you have the word Nigeria and your email it's pretty indicative that it's spam if you have the word viagra it's pretty indicative that it's bad right so both of these words are pretty you know likely given that it's spam but seeing both of them together is extremely unlikely right there's actually no so far there's not yet been any ones like Nigeria and trying to sell spam and also wants to give you $30,000 right that's so if you actually write an email maybe you talk about spam emails right something that I could send out to you and I mentioned both of these words actually that's actually given that it's spam both seeing both words it's really unlikely right it's actually very very small but if I decomposed it and I say what's probably given that it's spam that it's as you know G RIA and what's probably that let's see viagra it's actually pretty hot right so in this case actually I'm overestimating very nice very nice example all right good so what I want to do today in the last couple minutes and start looking into a few base in cases of my face and so just to make this a little bit more practical and to show you how it's actually used in practice okay so what we said is we have this distribution that P of X given Y and this ultimately what we are I'll be approximating so there you know so far so good that's the first assumption we've made you have to make a sexual assumption and that's what the distribution actually is right and so early on we talked about MLE we talked about the binomial distribution but you could actually talk about many different distributions you know that it could come up in practice and so let me go over a couple let me just see what's the first one to my notes oh yeah so the first one actually is that's the easiest one so if we actually say what if my data has categorical features right so basically I have a feature vector X and each X alpha is element of some some set you know of 1 to K right so my X element of R to the D but each dimension can only take on one of K values so for example you know typical example would be you know patient data and a hospital right and so for example the first feature may it may be are you male or female right that's then a you know Kate was two then another one could be what's your marital status right and so on so these are the typical questions they ask you are you allergic to anything and they give you basically a couple of things to choose from right and that's that's basically all you have right or similarly the example we just had was you wear a cape yes or no right and so in this case this is exactly the thing we talked about in the previous six air and the ML II lecture like a few lectures ago we basically say well the probability of a certain feature you know having a value J given a certain label is estimated as some parameter that's basically you know we had earlier that's the data that was the theta of our coin and we call this index by the dimension and basically actually by the the feature value and by the class probability and these are normalized let me explain this in a second just to make sure everybody's with me so what I'm let me just explain what I mean by this so there's actually really very simple what I'm saying for each feature basically I have a certain I take on one of these K values so in some sense I'm rolling a die with K faces and have some probability for each one of them to come out right and so overall they have to sum up to one right so if I sum over J which is basically the the feature type this should sum up to one and I have a different die for each class and for each damage okay does that make sense so forth so for each dimension that's basically the question I'm asking the feature I'm asking are you male female for example that may just be a coin toss right that's a male you know that comes up with either male or female right but it may not be 50/50 for example in hospitals actually you know because man died earlier and women live longer but you actually mostly in a hospital when you're old right it could be the distribution is more shifted towards women businessman sir right so um but basically for each for each feature for each label you have a different die and this the the probability of the size of the die have to sum up to one raise your hand that makes sense okay so then the question is how do i how do i estimate this and this is exactly the the MLE or map map approach that Vaizey say the probability that given that a half class c and you know this notation is horrible I apologize I hate this intakes now that I'm seeing all right so the probability of of XJ equals J given Y it will see which is my theta J see it was my alpha exercise the alux alpha yeah equals and I may see what I'm doing is I discover all my data points and I say how many times to observe this particular label and how many times do I also observe this particular feature value and then I sum over the total times I observe the lane but I can do smoothing if I want to so I can do plus L plus L times P alpha which is there's just how many how many different parts features there are so this is actually nothing new this is exactly the same thing what we did last time with Emily and map raisi and of that makes sense okay awesome so let's get to the more interesting part so the only thing that's maybe confusing is because of the Thetas but otherwise shouldn't be any surprises okay next thing is what if we don't actually use that binomial distribution what if you use a multinomial distribution that's something you have not seen yet more tiny elastic okay so binomial distribution he said each feature can take on one of K values and basically what we're doing is we're rolling a die that SK for a different you know numbers on it or something and they have different possibility and then that's that's amazing how we assign the feature value and you're trying to estimate that die multinomial distribution that process is a little different and that's the typically use for example for spam filtering actually so most spam filters that you use on your whatever mac books or something actually just use the naive Bayes classifier with multinomial distribution and so here's the idea we draw em words right so you have an email the email has M words and the way it works is you basically draw repeatedly without placement so sorry with replacement so you basically you have a distribution over all possible words so there are some property that you draw the bird the some probably the draw the word a or something right and then what you do is your basis you know from this distribution you just draw em birds and that's your email okay the only the key key inside is that you have different distributions for different labels so you assume that spam emails are drawn from some distribution where for example the word viagra and Algeria are now disproportionately more likely and not spam emails are you now have a different solution so in my case for example the word Killian Wright is pretty likely it's not spam emails because a lot of people send me email addressing obviously addressed to my name right so and then what the spam fruit is busy doing there's estimates these probabilities from the data let me formalize this so basically what you're saying is my each picture alpha can be so yeah can be one can be up to M numbers and M is the one and this year is no longer a feature is a feature value but actually means something to split this actually means how many times this particular word is present in your email okay so your feature vector here is actually not you know that does not have as many dimensions as you have words in the email has as many dimensions as you have words in the vocabulary alright so in the English language that maybe 10 million right so you have a really really large dimensional vector and what you're doing is for each for each one of these dimensions really represents a particular word right so let's say x1 is the word right and then you count how many times to have the word the in my email okay that maybe five right so then the first entry is five right that's you know let's say the word a right that's alphabetical what the second one meeting the word aunt right how many times do you have the word aunt in your email I'll probably not actually right and it's on any of the word all right and all well how many times zero and so until you count you basil II have count of how many times each one of these words your email and the total sum of all these counts is exactly M which is the number of words that you have in your email raise the end of that make sense all right awesome so of course in practice you don't store all these zeros right that's the that's the trick right otherwise the extremely inefficient so the question is what is now the probability of a certain vector X given that I have you know a specific number of words and that my email that I know the label rights is either spam or it's not span and well what I'm doing is that basically again have a probability that I'm assigning to each each word and so here's what I'm doing a busy say okay well the first thing is the order of the words doesn't matter so what I'm doing is I say for each one of these and xD this is how many combinations that because we arrange my words in my email right I could have all the AAA AAA at the beginning and so on and I could rearrange the words in my email alphabetically and would give exactly the same representation right so this just this is you know good old probability theory that you've learned in the middle school something right so this is just basically fat you know considering that there's many different words based off of reordering the email which don't actually make any difference and then you say okay well I'm going now that now I'm going through my my email 1 to D I look at every single word and what's the property that I'm drawing exactly XD of that particular right so theta alpha C here is the parameter that Basie says that given that the class is C given that it's spam or not spam you know what's the probability of drawing word alpha and I drew it exactly that many times it should be an alpha yeah the should be novel right druid that many times okay does that make sense so this year's basically the probability of coming up heads or whatever right Reno but that is the the probability of that words and I came up that many times so this this will be the probability of receiving exactly the email that you observed in that order and if I multiply with this term then I say well if I reshuffle the word it doesn't make any difference any questions Louisiana that makes sense raise your hand if you want to go outside and be in the nice sunny weather alright good see you all on Monday 
","['', 'K nearest neighbors', 'metric learning', 'naive Bayes classifier', 'conditional probability table', 'Gaussian weighting', 'data augmentation', 'feature weighting', 'CPT', 'machine learning talks at Cornell', 'Steve Nash and Carl Sagan team (project 1)', 'team Gamma', 'Woody Kitty team', 'naive Bayes assumption', 'prior probability', 'multinomial distribution', 'email classification', 'spam email', 'feature vector', 'likelihood', '']"
"all right welcome yet another lecture on machine learning please put away your laptops close your laptops iPads game stations Playstations have been ok good we've been talking about oh sorry quick logistics sorry I promised you I promised to ruin your weekend with homework three a project that it has not happened yet and yeah actually maybe a test driving it very very carefully before we rolled it out and we ran into some problems on the auto grading thing so I think it's all resolved right now we're testing it one more time and hopefully you can ship it out tonight also one good thing that happened is we contact with VOC areum and actually book areum is now on Piazza so they actually have now one of the people from valerian from that company is now one of the Piazza TAS so if you actually post a problem with the poky areum he is actually very responsive and they're very very interested in in making this work for us okay good any other questions about logistics all right so we've been talking about naive base right a naive base is a generative algorithm so what do you do is in some sense you have your data let me draw a little example here let's say you have positive or negative points here my positive points and here my negative points in this case it's very easily now nicely separated and what naive base does it actually estimates the distribution of this this data so what you're estimating is P of X given Y right so basically what it says okay well only the points in a label one what is the distribution of these data points and so you know you could please imagine that's kind of a you know some distribution I hope you understand this this painting here this is kind of like a he'll write a probability that you could estimate in the space but turns out that's really really hard because the space is high dimensional so what knife these days does is it says all we make a very simple model assumption we assume these different dimensions are independent so in some sense what are you doing you just take this data your projected onto each axis right we say here now you have two points like this and now you model this okay this is easy to model because the one-dimensional problem then you do the same thing with the second dimension right so you project the data onto here and now you Melissa right and you say the distribution here is basically you know if you combine these two independent product of these two independent distributions okay so that's the idea behind naive face and so when we do it on behalf data there's basically two steps the first step is we take the data if you look at each dimension individually and model this distribution and then once we have it let me basally have a term for this expression here which is a product of all the individual distribution for each feature and then we can do prediction with the base formula okay so let me just fall back onto the Bayes optimal classifier one thing that's important about that and I and I basis that basically this two step approach when you first model the distribution and then you make a prediction right so when you make your prediction you the data is no longer important now you just try to eat the first you model that there's to be a the data for the distribution but then you only look at the distribution and predict P of Y given X using the base formula okay so if your modeling is bad then actually your classifier is gonna be bad the nice thing is it's very very fast most the cases especially you know last time we talked about how to though you know we started going over some examples how you do this so one has been when you have categorical data you features a categorical then we just haven't binomial distribution which obeys just two counting we just count how many times have you seen this particular feature for labor one and how many times have we seen labor one overall and the ratio space the probability of a feature given that label okay so that was Bobby the last time we believe how do we estimate P of X alpha given Y Brighton was based just you know the ratio of how often how many times do we see this particular feature out of all the samples that we had any questions at this point just you know there's just a high level overview one more time to make sure you you mind a reminder of the big picture of nine things all right so what I want to do today is I want to go through more examples the multinomial and and the continuous features and then I want to talk about well actually once we have these two distributions we then have a classifier based on these distributions and the way it works is now we have a distribution here for the negative points of a distribution for the positive points and then we take a some test point and we say what's what's more likely right is it more likely at this point was drawn from this distribution or more likely from that it was drawn from this distribution right that's basically how the classifier works in some way in between is a decision boundary where you say you know at this point it becomes more like this drawn from this distribution and here it's more likely to strong this decision and what does that decision boundary look like so the second part of the lecture I will talk about this and actually I can you know give you the punchline already it actually turns out it's a it's a linear classifier yeah okay all right no question okay no guys all right so we asked me in this by the way one thing we also asked to be TPO by rights the PDF is usually very easy to estimate you just count how often do you see each label but it's just a fraction of seeing that label across all data points all right any more questions yeah if you use the what the orthogonal components oh you mean the axes like the each dimension you mean oh I see so what you're saying is the following what if I change the coordinate right what if I actually now same accordion system look like this right now I do PCA is there now I do naivebayes right and so you now changed your assumption right you're busy saying that after this transformation of this different coordinate system now all the different dimensions are independent conditionally independent that may or may not be true usually it's not true but it may work right typically the the scenarios where you'd use but naive Bayes is with text documents for example there doesn't really make sense of a transformation right because the data is very high dimensional it will be very expensive and in some sense the features you know of each dimension corresponds to a word it's a pretty good representation of your data but yes you could do it yeah naive Bayes for regression and that I don't really know how to do that now yeah any more questions yeah over there oh it doesn't know argue be patient be patient people get to this and I love it yes it doesn't always have to be linear classifier you you got so it's not always it's in several common cases it's the new attachment yeah okay any more Gerson's all right so what we were I think when we left off last time was the multinomial distribution I think I just started describing it right but I didn't get into the equations that correct yeah okay people are nodding so it really was these generative models it really helps to have a model of that you have in mind how the data is generated right so let that sit in some sense why it's called generative model might be basically be estimating this year right so think of some kind of process that takes the label as input and then generates an X right and so think of X is now as emails head so then in the multinomial case multinomial here we basically say the wavy PB write an e-mail on the baby we populates an input vector it's by adding on to the to the different dimensions so we have so what does that mean so each X basically is a very large vector and each dimension here is a count of how many times we have observed that feature in our data instance okay so so explain this last time so in birds typically be half you know you have some some description of the English language via PC have all the words in the English language this can be millions right so if you work for spam filter company as you know like Google or Yahoo was something about you would actually use all the languages in the world right so you actually turns out actually what you need to is you hash them but but but they eventually be amazing you have millions and millions of these buckets and down here zebra who knows right and you count how many times do you have a certain bird in the email so for example five times we saw the three times we saw a and we saw actually twice right because I'm writing a lot ants bite who knows and once about zebra I saw most of these words however I will have never seen right because most emails do not contain those words and the process basically is that you have to think about is you start with an empty email but just trippin this is really how you write your emails right now you start with an empty document and then you add one word after not right so you increase one of these and what you do is you roll a die right you roll a die and say alright what word came out but then you know the word pumpkin came up right and then you you increase you count of pumpkin by one okay and they comes the next word right and now you roll it again it's like oh unicycle right right so you know you increase your counter increase your counter of unicycle etc right so and you do this until you have M words and then your email is finished so that's the process there's the generative process how we assume our data is sampled right now again this is obviously not having about emails but it's a process that gives you something that looks like an email right into that if you make that assumption ultimately classifiers work fairly well right there's an approximation of course okay so the question is any questions at this point so what we want to estimate you know in order to estimate this B's you have to estimate this time right that we're rolling the baby says what's the property that we're picking this particular word okay that's basically says for you know given the label Y what's the probability that I pick word J okay and obviously has to sum up to one because it's a probability distribution and we call so I think I oh wait I think I already talked about this right so let me just write this now one more time so we have the probability of a particular email X given that I have M words and I'm from the class C so that's either spam or not spam is the following it's just the product of FS 1 to M so I roll my die M times and I so this years see here alpha goes over the dimensions there's basically word alpha right you'll see the thetabc is the probability of ending up with read alpha there's some value between 0 and 1 and how many times did I roll that exact Alpha X alpha x all right so that's basically the the probability and then I have to make sure that actually the ordering doesn't matter right so in this case X T I'm just looking at the words I'm not looking at the sequence of the words so if you write you know the and and the zebra or a zebra and V or whatever is the same thing right it's right in our representation this is why it's called bag-of-words right because the word order is thrown out it doesn't matter anymore it turns out to recognize spam from non-spam the word order usually is not important so just words good frequency statistics is enough so let me also multiply by this whole thing by M factorial over X 1 factorial X D factorial so this here just gets rid of the order is not very important it's just a not you know constant okay any questions about this crazy handed this process this makes sense okay some people seem confused okay let me let me explain one more time so I have a Z assume that my data is drawn from a distribution P of X out why that has a parameter vector theta and that parameter vector theta have one for each class right for each class effort this busy mind might die this basically says you know what's the probability data alpha C says that if I have it like so that's theta I'll for spam right is the probability that if ever draw if I want to write a spam email that I randomly draw the word alpha and I roll that die M times and I always look which word comes up and I populate this vector that's today and at the end I have a vector with bird counts and that's my email and what we now want to do is say okay well if you make this assumption now given the emails that we have in our spam inbox and now you're not spam in boxes I can now estimate this die I can basically say what's the you know if you assume there's actually the process how emails are generated now I can look at the data and say what's the most likely die that would have generated the data that I'm seeing and that's exactly mocks maximum likelihood estimates I can do this so I can basically say okay now we want to estimate theta alpha C so Gong more time I want to know what's my estimate of what's the probability that I would see word alpha given that a half Class C so there's given that it's spam or not spam and how do I do this well it's actually quite intuitive I basically say how many times did I see word alpha in my spam emails all right so that that's Phase II that the fraction so I say I sum over all my my my data and say the number of times and so first thing is I must have this class right oh that's a spam emails I sum of all my spam emails and I count how many times this years how many times I see that read alpha in those spam emails and I divide it by the total see y equals C and then this year can anyone tell me what this years all right I'll give you one minute discuss it with your neighbor figure out what does this mean and describe to me this fraction all right do things he or she has to get it out who can describe it what does this mean I should just realize this should be an eye maybe that's through people huh it should be an eye here right I I beta okay okay now that I correct annotation yes I guess not that's not quite correct this busy go over X I yeah that's right that's right so busy it's the you thing is some of our all emails from 1 to N here you say this here's one if it's spam and if it's otherwise it's zero so you only look at the one to the span you could also just say some about all the spam emails and this baby says go over all the dimensions and sum up the count of how many words they are it's a basic go to every single word and say just you know have a cumulative accumulator the base it just says just adds up how many words you have total you know that's actually M right and we assume that each email has M for simplification but there's also generalizes what you know of emails have different lengths so then actually you just you need term here right and so here what we do is we sum over all emails we say again the only some of other emails that are spam and let me say how many times do we see word alpha right so they see what we're saying is well say well in total by the time you go through the entire spam box right and you say the word Nigeria right which is like a common word for spam I've seen that 55 times that's up here I go through all my emails for each email account how many times missing that word so 55 times and then I go to all my spent all my spam almost BAM box I say how many words in total right maybe ten thousands right so then the probability of seeing Nigeria and then spam email is 55 over 10,000 okay so that's basically the estimate so in other words if you would now go see a spam spam box and pick a word randomly from all your spam emails right that's exactly the probability that it's word alpha okay does that make sense and you raise your hand if that makes sense okay and so if you want to do map smoothing it means you have to loosen 8 some words and the way it works in this case is that you actually have to sum up you know people didn't do plus 1 smoothing for example and you have plus 1 here and then so here we actually have two sound plus D can anyone tell me why we have to add D here anyone except them what's your name Arthur ok not Arthur anyone else next time I opinion yeah that's right that's right exactly right so earlier on we had each feature that we said we assumed that in this dimension we see each value once right now actually the dimensions are counters right so what we're doing is we're assuming we're saying we assume we see each word at least once right so we add a 1 here but don't really have to divide by you know add be these the number of words in the dictionary okay that's typically what you use right because if you don't do this someone put since I'm really aware word or you don't misspell the word that you've never seen before and then sudden you get a probability of 0 because you say well the probably that this word appears in span is 0 because I've never seen it before right but that's actually not what you want right so these rare words you know usually one way of dealing with us just is just assume that you've seen each one at least once in either spam or not spam any questions yeah [Laughter] but this is just when you estimate what's the probability that C word out like given that I have spam then you do the same thing for HAMP which is not spam yeah good question do you run this twice right so you may now see why naive space knife in my face is so popular in spam filtering because what do you do when a new email comes in all you do is you go to the different words of that email and increase the counters right so for each word basically you you store this counter and you score this counter and you just increase both both right and then you're done so learning is really really fast you have to retrain anything you have to run anything etc right so that's why most of your software probably uses a naive Bayes classifier yeah sorry oh yeah okay good question so you're saying what if I misspelled Everett right so there's these people I don't know if you remember this there was a time in viagra spam was really really fake and then people will try to misspelling it because exactly this reason people use naive Bayes classifiers and so then the probability of viagra given that it's spam was really high and these it was really we're in a not spam so it was a very good feature right and the spam didn't get through anymore remember people did as they wrote just you know they wrote it like a 1 right they voted this way right I don't you want to ride by a ground but you and then so then actually you know humans still recognize what the word meant but actually did you know did an index anymore because there wasn't actually a known word so people just threw it out right and so well one trick actually what we actually did so when I worked on this we just write a hashing function so actually what we do is we actually don't have a mapping from words to dimensions are getting a not a dictionary lookup what we do instead is we actually have a from a hashing function that takes us word the word has input like you know high whatever and that in that outputs some number between 0 and D and if you anything in the hashing functions or anything you stick in there gets mapped some dimensions so if you if they make up new words then maybe next time you retrain you will actually have populated that word so that's the trick that's also how you get around many languages and those those people kind of suddenly write lol or something that was a new thing that came out ten years ago and you know so you have to kind of these new abbreviations the kids came up with you can just track them this way yeah ah all right okay you're onto me so in the case of collisions you just say ah come on doesn't matter all that much so though he's been saying what if I have two words right like what if I have you know that's a here you have a really really unfortunate coincidence right like the name of your mother-in-law right matches viagra right like it goes in the same thing all right so now all these emails going to spam filter right so there's actually there's actually two tricks around it so one thing is so what we actually really did is we action is hash it twice the two different hash functions and so they approximately uniform the probability of getting a same collision twice is basically zero so then actually if want collide then actually the other one doesn't and then you still fine but it's a very good question yes okay any other questions yeah oh these counters so the question is like if these counters get too large you know do you run to overflow problems so the question like if I just you know send you an email just with you know a million times my Agra in it right then the hope that the counter gets so large that it rolls over and goes to zero all right and then I've got you Fred um I mean you can try it my assumption would be that you use enough bits for these contents that you should be fine and you have to really attack I mean not a no it's a pretty obnoxious attack it's a good question though it's a good question okay any other questions all right let me so this is the case there's actually the most common case for text classification and so spam food is actually usually use my own multinomial classifiers order for project 3 which is coming out tonight you actually have to put implemented a nice Bayes classifier and so that would be one option and I can show you in a minute what what it looks like yeah sorry what is the oh here this this this constant here just make sure that you don't incorporate the order of your email all right so this place he says you go to every single word in a ward and say what's probably that I got that order that many times right but if you would know shuffle the or if you write the email backwards right according to this that's exact same representation right so any order in which you write the email actually has to be incorporated into the probability of this particular vector so that's what this constant does turns out in practice you never have to compute this dog constant ever can anyone tell me why yeah that's right that's right so it doesn't actually matter right if I just want to know ultimately I just want to know I could just do the following what is y equals spam given X divided by P of y equals not spam a not equal span given X right so that's only buddy but I'm looking at if it's than one that I'm going with spam if it's less than one I go with not spam right these two have exactly the same constant so you can just cancel the constant okay so so you never actually ever compute this it's just to be a little bit rigorous all right now I'm gonna answer the burning question that you all have what if my exes are continuous all right that was what you were thinking so so far baby always had counts or categories so what if you actually say my ex alpha is element of some of our right or X element of so these are actually can be real real numbers and so here you can make many many different assumptions so you can be easily model this with any demand I'm a distribution that you want but a very common one is just the Gaussian distribution why the Gaussian because it's so nice so basically what you're saying is you're saying the probability that I see a certain X alpha given Y it's drawn from some couch mister bution you know with Mu Alpha see sorry alpha C square okay so I buy you say I take a Gaussian distribution and it has some mean and some variance and have a different mean and variance for each feature and for each class okay so typically for example if you had a classic you know patient data or something a typical one would be height right so height is something that's normally distributed so here actually the Scouts distribution is a very good you know approximation and you're busy say well you know let's say I want to classify if someone is a male a female well in this case actually if you have exactly that's right we have two different means and two different standard deviations all right and ultimately it's a Gaussian distribution hide as a Gaussian distribution given that you know the gender how do you estimate this and so to just you know get back to that picture sorry here we go so if you busy have your your positive points and your negative points all right what you're busy doing is you're collapsing these onto the axis and you say this year has to be a Gaussian distribution so that's that's the approximation that I made and I say this here has to be a gouge distribution then I multiply these two and I get a Gaussian distribution does that make sense crazy Hannah that make sense okay awesome so how do we estimate this well for a certain certain feature alpha in a certain class see what I'm saying is I say I sum over all changement changement ation okay well there is just do this so a sum of all the points that have that class C and just average and so all I'm saying is ignore this this is her gif just listen to me what you're doing is for each a feature alpha you just compute the average value that's what that is and just could be the average value for even within that class so let's say you know take all the the the patient files of oh man and I compete the average height that's new alpha given that they're men new of heights given that MN and then I take all women and I compute the average height and that's the new of height given female okay so that's that's all there is to it it's really really simple where MC is the number of points in the class and C equals y equals C any questions so this is the likely of maximum likelihood estimation of a Gaussian distribution yeah that's exactly right so his question is what if I have some discrete and some continuous features right that's perfectly fine you can say Sam I modeled with a gouge distribution actually any feature could have a different model right that's totally fine he's just gonna uphold the the independence assumption as you said good - once you estimate the mean now he said variance but what's the variance this is again not a big deal so it was just a definition of the variance all right so we just sum over all the data points that have that label and zip and I just compute the difference from the mean squared right that's exactly what the definition of the variance so hopefully that should make a lot of sense yeah oh you mean by the put a minus 1 here or something so nitpicky I'm the odious ooming independence anyway and oh yeah I wouldn't think it makes a difference maybe I'm sure someone has written a book about it but it's a good question it's a very good question yes um okay any more questions about this then we're actually done with the estimation part so so just one more time because then you get your data and what you do is yes you assumes our distributions you say either this Gaussian or this binomial is multinomial right this has some parameters in the Gaussian you have the parameter mu and Sigma all right the multinomial you have the theta right it so on and then you estimate that from the data using maximum likelihood on that and so these are basically the ways of estimating this please go through this and make sure you understand it I was opposed to a bunch of things on the homepage pointers in the homepage in case there's any uncertainty okay one thing I want to show you whose have a demo now but at the end we're easy and if you want to have it now at the end it's a tie all right okay all right okay good good you know I might run out of time that's the I guess you'd rather run out of time okay so here's the issue maybe I can show you the but so what you need to do for the project maybe I first show you what you have to do for the project so the project this time will be the following I give you a list of baby names so can you read this okay boy stur train so here's here's a list of pay see what I did is I went to some list of baby names and I randomly picked 500 names particular names that are boys there's the United States registered his boys and then we also have 600 names that are tend to be good where - Sierra Mayra destiny Annabella etc right and so the task is the following given a name that's not in that list usually predict if it's a boy name or if it's a girl name right and so what I want you to do in the project is to first extract features out of a name so think about what could be good features I give you some as an example and so and then Trina naive Bayes classifier but you easy then you know try to classify with ear the B just takes a name and then predicts is it a boy is to go and so I can now test this demo here we go I don't know if you can see this okay this is very small right this is very small I don't know how to make it bigger one second they're just hear someone say by agra say and second I don't know how to make it larger sorry Raja huh I guess it's better no this is worse okay this better this better all right so who are you let's type in Stephen right Stephen I'm sure you a nice boy okay so it's got it right right slow you can try Anna right and I'm sure you're a nice girl I know you can say Donald well I'll show you a nice boy Hilary Howard Hilary nice girl okay got that right author you yeah I know you're just you just revealed your name so here a nice boy good you can Danny any names you want to try sorry Killian oh come on yeah I got it right what if I tagged with double-l oh yeah okay it's pretty robust Vicky someone in the family is called Lilian let's see Lilian as a girl all right perfect what is it Pat p80 Pat is it supposed to be a guide enough think it is of course something a tricky right so Robin Robin could be boy or girl turns out you know things if it's a boy but it could Taylor okay Taylor that's a boy I like to think of he all right old you see I've actually works pretty raw I think it's pretty I have nice least can see Timo Vica I know actually so far that's gotten everything right any anyone any moments with any adventurous names Oh Bom all right I'm sure Donald Trump would be very pleased to hear that he's bald anyway all right so any any other ones otherwise can take the department chair that see how that works Fred Schneider okay he's good okay be safe all right any other house once Barak all right sleep Aramis video not yeah that's a boy okay yes that's that's actually more fear isn't it Trump here we go yeah he's pretty meow all right so so this is what you have to implement at the project and then there's going to be a competition where you basically take the test data and then we will see who can actually get the most accurate classifier of course there's an upper limit right because some names can go either way all right let me just take a demo before we actually derive it and B also I can also show you a demo G and B so this is actually now I'm running a Bayesian sorry a Gaussian yeah naive Bayes classifier so here's what I'm doing you just have two dimensions and I now you know draws on some points in two classes circles and crosses and then I look at the decision boundary after training okay so we first take the positive points so that's I take them positive points and then I draw some negative points and we can outrun it and you can visualize the decision boundary here I'm assuming that the features are real numbered and I assume a Gaussian distributions it will be just it and so if I now run this this is what we get right so there's actually there turns out as we will prove in actually there's something we would prove over the whole and homework actually it ends up being exactly a linear classifier right so implicitly by assuming that the features are independent and that they're Gaussian we made the assumption without realizing it that it must be a linear classifier right actually it's relatively intuitive because actually if you have two Gaussian distributions then actually the the line of where you say it's more likely to be classified from one Gauss drawn from wondergirls than the other turns out must be a hyperplane it's relatively easy to show and so one thing you see here it actually get all the points correct right so all the crosses on one side all the circles are on the other side and that doesn't necessarily need to be the case so for the perceptron we showed that actually it will always give us a separating hyperplane if there exists one turns out for the knife base a classifier that's not the case and now can anyone come up with night a data set that I could draw where we actually but that's linearly separable the perceptron would get it right but the naive Bayes is gonna screw up it's not able misclassify some point so maybe think about it for a minute how would I have to draw the points such that naive Bayes will not even get all the training points right so maybe discuss it with your neighbor for a minute just think about it you're trying to trick the naive Bayes classifier think about what it does [Music] [Music] [Music] any yeah all right I think you're the first person you're volunteering a dataset 1x the very right okay yeah okay good good good good good so he's been saying I know were you going with this BAM right you got it wrong why did it get it wrong anyone explain it can you explain it that's exactly right right so this X here's mascara in fact even this is my axis gears in this classify right and what's happening is we made an assumption that these these points here if projected down here are going to be gouged right but actually they're basically very heavily concentrated here right and then this guy over here basically you know ultimately an outline right is so so so unlikely given that you have this this distribution right that well think about the other way we're assuming this thing here is a gouge right so basically jaws a Gaussian around here right and it has a Gaussian for the X is over here so this point here actually is much more likely to be drawn from the Earth's right it would make more sexy would become even clearer if you have multiple holes so let me just draw this again let's say we have you have a bunch of owls here and then you know we just have a few acts to see it but the bulk of the X is really is over here right then if you model this right it basically models these points here the Gaussian and he is also here as a Gaussian and clearly these points are more likely to be drawn from the O's than from the X right and these two points just don't carry enough way to move the mean of the Gaussian over all right so despite that this is completely linear separable by the perceptron if I had the same dataset of perception would actually find out a classifier right here right in between that doesn't make sense any any questions about this yeah that he had right so this is exactly points on this line a 50-50 that's exactly right right so you exactly the ratio is 1 so it could be either one or the other yeah yeah perceptron of knife is so naive is much faster it is just counting the knife pace the perceptron you keep going over the data set actually it can be really slow in terms of convergence yeah and if you have a water distribution right right so the problem here is that they made an assumption that the assumption was the data is gaseous to be the net just doesn't hold right so that that's what breaks it so if you had a different distribution that that's more accurate then it would be bad right so you could ever you know you know mucho modal Gaussian or something all right thank you no Gaussian mixture models I'm but I get some more hairy then right it gets a lot trickier yeah just because easy yeah all right see you all on Wednesday 
","['', 'naive Bayes', 'generative algorithm', 'estimate the distribution of data', 'P of X given Y', 'label', 'high dimensional space', 'independent features', 'project the data onto each axis', 'multinomial distribution', 'continuous features', 'classifier', 'decision boundary', 'linear classifier', 'bag-of-words', 'spam filtering', 'maximum likelihood estimates', 'smoothing', 'naive Bayes classifier', 'perceptron', '']"
"okay as you may have seen the next project is outs on naive bass today we will transition from night bass to logistic regression and to empirical risk minimization and Emmy ball then the next project actually will be shipped out as there will be one more project before the midterm as the police gets delivery overlapping so please get started with a knife pace that you're on track alright so meat we talking about knife pace and you know the general idea is you fit this distribution you know you want to know P of Y given X and you say well that's proportional to P of X given Y times P of Y do people know what that proportional means raise your hand that makes sense okay so it's basically up to normalization right so that they divided by something then you have equality divided by something doesn't matter in this case because we're just interested in in the ratio of two classes all right and so well let's say we have 24 classes plus one and minus one and you know we basically estimate these two distributions individually so what we do is we here we just estimate how often do we see that class plus one Hartman's often we see the class minus one and here we actually have the probability of x given Y and here this is where we make the knife base assumption and we went over several cases last time you know to remote no meal and Gaussian right basically in the Gaussian we take each feature in for the Gaussian and multinomial we assume that each features account how many times we've observed that feature so let us look at the multinomial setting where for a second so assume we you know we have a classifier and we say that the you know what it does it a plus-one okay so when does it predict a plus-one well if and only if P of Y given X my equals plus 1 is greater than Peter y equals -1 given X okay so in this case B is evaluate both of these they say what's the property that my label my my data point XS this is a plus 1 y equals plus 1 has probability plus 1 what's probably that it's postponed the story has a label plus 1 what's the probability that it has the label minus 1 if this one is larger than the Bayes optimal classifier I would say you know go with the more likely one so well this decomposes so we can actually plug in these following terms so we basically say well that's the case only if so why P of my four plus one let me just do a plus one yeah plus 1 it's greater than that you know the same term given minus ones okay so that's the same thing right so I busy saying I'm just taking you know exactly uh this just Bayes rule right and I say this one the probability that basically we get a label plus 1 must be larger than beginner probability minus 1 raise your hand if you're with me all right awesome good stuff I should we do a multinomial distribution so if you remember from last class multinomial distribution we doesn't affect this thing here but effects the assumption you know out there that we are making about the distribution of the features and let me just say these are vectors so we saw from last class this is the distribution here is basically you know we had this M factorial over X 1 factorial XD factorial times the product alpha equals 1 to D theta and for C X alpha so what does it is basically we go through all our features from 1 to D and we say what's the probability that we rolled exactly that feature X alpha times where X alpha is basic the value of that particular feature ok so that's in the spam setting or something we have an email we go through every single word in the dictionary we say what's the probably that we observed exactly that word that many times right given that it's a particular class here the class is plus one okay crazy and of you still with me okay good so then I do the same thing for the other one oh sorry times P of plus one so same thing for the other for the case where I have XD factorial and now I say the product I go over all my features and now I roll the different diet or the minus one die that's now the spam or not this this this band is the not spam die and I say times the probability of Musk okay so all I did here is I plugged in this the definition you know much Nomi distribution in for P of X given the label yes oh yes I can write larger that's right okay good I guess you can't be these doesn't matter because they actually the same on both sides so we removed these constants and so now we're left with this expression right anybody have a product of many of these probabilities on the left and on the right side and so what do we do when we have a product of many probabilities who knows it because you're computer scientists good we take a lot right so that's exactly what we do No mmm all right so now we can take the button log of both sides doesn't change anything right it's a monotonically increasing function so if the log of the left side if the left side is larger than the right side then the log of the left side must be larger than the log of the right side you can take the log because both must be non-negative however baggage must be positive so let's take the log so then we get log of PA plus 1 times the sum over alpha equals 1/2 D log of weight sorry this is X alpha times log theta alpha of plus 1 there's not alpha class this does label plus 1 right y equals plus 1 must be greater than log of P of minus 1 times the sum over alpha equals 1 to D and now X alpha times log of theta alpha and label as minus 1 one more time what I did is I just took the log on both of these sides ok any questions did I forget some term hey there's a plus sorry what's that you're concerned okay good raise hand if you're still with me all right good you're holding on squits good good all right so well either we can do right we can now actually take collect the terms respect to X alpha like we always busy what is this right is the sum of many terms right we have X alpha times some term right we have the same thing on this side so what can we do we can pull these sides over here okay so if that happens well you get the following we say we get log of P of plus 1 minus log of P of minus 1 here what I did I took this time and just moved it over alright and then I said alpha equals 1 to D and then I have I take these two sounds like you know move the right some some some on the right over to the left alright so what I get X alpha times log of theta alpha plus 1 minus log of theta alpha minus 1 and that must be greater than zero here one more time so what I did I just collect the terms right I just subtracted the right side from the left side and so I put the two log probabilities together and I put these two summations together raise your hand if you're still with me all right good this was you're almost there you're almost there so here's what we do we call this year W alpha okay and you call this year B okay so this here's one constant it's just a scalar and this year is for every alpha is one value right so we actually get it from 1 to D is a D dimensional vector so what does that mean tada w transpose X P poor plus P is greater than zero what have you just shown can anyone tell me it's a linear classifier exactly right it's nothing else than a linear classifier if we just have a very specific way of finding the W's right so the perceptron was one way of finding the W and you have the perception update rule and now if base is another way of finding the percept you know the weights of W and you just define them this way but you estimate the Thetas using Emily and then you computed uw's and you be that way question so it doesn't work for all distributions it depends on the app that's right and so actually in the this will be the homework the homework people actually look at other distributions that's yeah good question so you can easily make the solution that doesn't work at all anymore but actually that the interesting thing is you typically the most common case is actually multinomial because it works really well for these kind of text document classifiers and and she's also true for Gaussian naive Bayes yeah yeah ah that's a good question that's a perceptron is anyone from where does anyone remember what happens if you run the perceptron algorithm on a data said that's not linearly separable you're screwed so good you screwed I like it all right it just loops wrap right like it's never ever convergence so what does naive Bayes do can you want Tommy does it loop forever the answer is no because there's no loop yeah they gets the answer it's reasonably good and what does it do essentially it just fits the two data sets right the two two classes with the multinomial distribution and then says if that was true if the data really came from these multinomial distributions then here would be the hyperplane right it doesn't all right but you know but if it was - if the data actually was the way I modeled it then actually I would have a really good time plane right that's basically what it is and even actually if the data if you're modeling is correct it doesn't need to be linearly separable but you could have two gaussians let me show you an example yeah you could have two you could have two gaussians right there bases like and my Gaussian here from which I draw my excess and then I have my my second Gaussian here right draw draw my my circles and these are certainly overlapping right that's okay all right nice Bayes classifier I would model both of these as a Gaussian have used the Gaussian naive Bayes and would then basically find a hyperplane that's that best separates these two gaussians so but raise if I have planted but that's a very important distinction to make it does not find a hyperplane that best separates the positive from the negative points it finds the hyperplane that best separates one distribution from the other and it fits these distributions from the data that's that's that's an important distinction to me any questions yeah so trippy so there's a few advantages of naive Bayes so one thing is this year does not require any loops over the data or anything right so you just this is really really fast to estimate it's very fast to update so that's that's very advantageous the second thing is you don't have this data if you would have a spam filter there's a perceptron that were you really terrible right because occasionally gonna misclassify a spam email that's not spam or something now you dataset is no longer linear acceptable right and basically the half a loop forever head so in that sense that's true right the perceptron is not generally used anymore but derivatives of it are used and actually that's exactly where we are going now the perceptron is a discriminative ivan so what is the script except I was trying to do is try to model Y given X and now faces a gentleman of I wouldn't models X given Y but there's a formulation of this and actually flips it around and I will get to this after this question yeah right that's a very good point that's a big so this no no so there so he says one advantage of the naive Bayes algorithm is that the end actually you get a probability right so you can actually say I at the end I get P of Y given X right I get the property that it's spam and the property that it's not spam right sometimes that doesn't matter I just want to know is it should I move in the inbox or not but sometimes that's really crucial right so for example if your doctor and you say you know I'd say you know I want to do some brain surgery and you want to say is this brain surgery necessary right yes or no right well you may be interested if the probability is just 51 percent versus 49 right may want to do another cat scan and who knows what right so these probabilities basically these these probabilities give you a measure of certainty and that comes for free in naive Bayes actually it's not that every that you have to have a generative model to do this so discriminative model also have these probability can have these probabilities actually the one we will talk about in a minute also has these actually but the perceptron does not someone just had a question like oh yeah yeah so so what naivebayes does right it takes you data and you have points here and you have points here right and then it models these these data points with a distribution that's the step when you estimate the theta for your p of x given Y so you make you you make a decision here you say what your distribution p of x given Y you see that's multinomial right you do this as a data scientist to understand the data right so that that's where your knowledge comes in once you've made that decision now this your distribution has some parameters for example genome is this this particular theta and you fit that theta that's the maximum likelihood okay the first step is you decide which distribution use second one is you estimate the parameters at this point now you substitute the data with the distributions you're now saying if my model is correct this was drawn from the following distribution here these are kind of these these surface plots can think of this as a mountain that comes out of the out of the right ear blackboard and then at this point you don't look at the data ever again but you're now separating just these two distribution this can't go horribly wrong and we saw these examples last time when I asked you guys what dataset would break it right if you for example have something that you have some points here really close by and a lot of points far away right in this case and if you say it's a Gaussian distribution well what its gonna do it's gonna you know basically just you know have a gouge mr. fusion here maybe a little bit of them something like this right and so what about to do we have these points in this distribution this distribution will find a hyperplane somewhere in the middle and basically sacrifice these points right the perceptron would never do this because it separates the data right knife base first approximates the data models the data and then basically using that model gives you the best decision boundary but if the model is wrong it gives you you know gives you a wrong result does that make this thing's distinction kill that's right it would keep updating until it finds a decision boundary you know that catches but on the other hand if there isn't such a decision boundary will just keep going forever but there's 90 pieces like whatever I know ah you guys are clever so his question is the following let's say I take my data set and I I make a prediction you know and I have a test data point and I'm pretty sure by the 19th of ninth base classifier is asked this is you know 99.9 percent confident this is this spam email the fact that I'm so sure right you know means that is you know it's probably right so kind of just take this data point now put in my training data now I have more training data and now every train right it's called what's called self training and it can work the problem is typically that if you move the add the ones that you're really confident about you're actually not gaining much because you already knew about this point anyway so what you really would like to add is the ones you kind of write but they're you out on the edge right of course that's that's a risky business people have gotten a to work though you can basically do this but there's a little bit more to it but but yes in principle it's a good idea okay any more questions yeah sorry do you if you don't have a close form here oh I see this is a ball yeah yeah I mean if you don't have that then gets nasty right you don't need you don't need do you make no modeling assumption you don't need a distribution so that's it that's that certainly advantage of the perceptron that's right right so nice base like the knife base baby you know as an advantage when you have a good estimate of what the distribution is right and ideally of the naive Bayes assumption holds right that's that's where it really kicks in okay so one thing that will be on the homework assignment the next I think it's coming out this week actually you have to derive the following thing that if you actually use a Gaussian like if we just did this for for multinomial if you do the same exercise right we started out with saying what's P of X cubed Y given X and then he went through it and we plugged in the night based formula and plucked in the formula for multinomial distribution and so on at the end rubies they came up with a linear classifier if you do the same thing for a Gaussian naive base then you get the following expression that's 1 over 1 plus e to the minus w transpose x times y where y element of plus 1 minus 1 okay so at this point you just gotta believe me and then if you don't believe me well great you know you can actually verify it so maybe I give you a minute to digest this equation I want you to become one with this equation so discuss it with your neighbor and think about the following let's call this thing here Z right so this w transpose X so Z times y think about the following what does this function look like if Z times y becomes really large it becomes really low negative right really really negative and what does that tell us about this function so think about this discuss it with your neighbor and just get some intuition I ain't gonna draw it in a min so try to draw it right as a function of here's you know this here is Z times y okay who can tell me what this function looks like who is brave enough to even you can to just describe it yeah that's right right so I'd say here's one here's 0.5 so that's infinity it's one that minus infinity at zero right so why is this if this becomes really really large right let me have one over something really large becomes zero so what we actually have two so this is negative you know you might apply with a negative so this becomes really last you can if it's inverse right so at this at this point here where C times y is really really really negative you're multiplying that with the minus one so this it becomes really really really large one divided by something really really large is 0 if Z is very positive then each of them something negative positive is e to the minus 1 million as well as zero right then you have 1 over 1 is 1 ok so the function is here and it's here and what is the divide at the middle at 0 what is it what is it here someone said it it's it's Phase II 1/2 right so it can look like this right ok so the nice thing is this makes a lot of sense right so means that if if I'm really you know intuitively right I haven't a hyperplane W and I have a positive point here it has a positive he knows label 1 what does w transpose x measure w transpose x essentially measures how far away are you from the hyperplane right so says if you're here my positive points here my negative points if you really far away from the hyperplane on the positive side right then basically give you a probability of 1 right and if you're really you know on the wrong side gonna give you poverty of - one okay sorry I move the sorry actually I made it so I can't have steak with the fire here this will actually there should be no higher you're not always multiplying with so this equals positive yeah that's right sorry well you got it still right typically you actually have a plus B here but you can absorb as an additional dimension just like the perception okay so basically you can either basically say there is a plus B or you know that's not so any questions so one more time if you take a Gaussian naive Bayes and you fit it what you get is the following expression here and you may need to get some formula for W in some formula be just the same way that we just did it for the multinomial distribution okay so you take your data you estimate all these little gaussians and then basically you know this gives you a vector W and gives you a B and then what you do is for every single data point you just stick this into this equation and it gives you some value between zero and one and it turns out that's exactly the probability that the class is passed okay all right so this is great but we can do something even better and here's what it is so basically what we're doing is right we get this we've been modeling P of X given Y right we're modeling the data given the label and then we get our W naught B and we get this formula and that separates the two distributions from each other and as we've seen earlier that can be problematic right one of the distributions are not actually exactly Gaussian right then you're not separating the data you're just separating these two gaussians which actually do not really correspond to your data so here comes the brilliant idea number one why don't we if we really interested in P of Y given X right and we know that the Gaussian nice base at the end gives us exactly this this expression right can't we just set W and be esta instead of estimating that the parameters of the distribution can we just estimate W be directly so how do we basically have our data and we estimate this distribution to get these you know get these two distributions here we estimate the participation of the positive or negative points that gives us a double unit B to separate these two distributions well wait a second right we have a closed-form year we have this beautiful formula can be just instead of estimating the parameters of the distribution come just estimate wmv directly and that's exactly what the discriminative approaches so that's what we'll just take regression is and there's an algorithm that's really old statistician have known it for a long time and it's the discriminative pound counterpart of naive Bayes right so the naive Bayes you estimate estimate P of X given Y and a logistic regression yes to me P of Y given X you also estimate P so this is parameters T theta and this yes you know I guess I'm gonna prime and this year has some parameter W and P all right and so now if a space you estimate these two and then you get this right but it'll just regression you're estimating directly and well how do we do this right we want to know the best parameters of WB for this distribution so now in some sense what we're doing we're saying okay well let's just start with what not the knife base gives us knife base gives us this formula at the end I P of Y given X equals the following expression that's a now our assumption our assumption is that the probability of the label given X has this form and now we fit it right that's exactly the same thing as what we did with a knife base you made an assumption about the model and then we fit it in this case we just make an assumption about the probability Y given X and not X given Y that's the big difference but it's the last lecture I give you several examples of how to estimate P of x given Y and what is camuto meal in a Gaussian and a categorical and in this case you just flip it around we say we just estimate P of Y given X as parameter W MP any questions raise your hand that makes sense okay awesome so let's do this right that's jeez I never figured this out okay so what we want to do is you want to maximal divisions to Emily so we want to find the parameters WB that it best explain our data so here's what we do we [Applause] the P of Y given X and now we actually have our W here so this here's our data this is no can just say let me just do this X 1 X m and VFW and here's our data our signal 1 X this is the whole conversation let me let me just let me just do the following so basically what we do is we want to maximize the probability of you can ignore the first one the vector notation I don't think is particularly necessary so here's a we do we go over all our data points from 1 to N and we say what's the probability that I get this particular label right given that I have my W and my ex my ex and my W and if you want to be really really particular or you know correct then you have to make a semi colony because we're frequentist so this here is a this this model here has parameters WMV so one more time I go over every say I assume I have this model P of Y given X that has the parameters W and P and now what I do is I go over my entire data set and say for every single training point right what's the probability probability that I draw exactly the label that I observed given that I had that vector X right so this I go to all my emails to say this here is this particular email turns out of a spam what's the probability that actually is spam under my modeling assumption and I would like to find my W and B to maximize this right so obviously I want to do Arg max W comma P to maximizes I would like to find the WB that best explained the labels of my data any questions all right so here's what we do oh we have a product of many many probabilities what do we do take luck right good good good if you learn one thing in this class take the log all right so we take the log of this whole thing that's the same thing as Arg max W come up be you know sum over I equals 1 to n log now P of W comma B why I given x i sr x comma w and well we know exactly what does this because we made that modeling assumptions we can just plug that in so that equals sum I equals 1 to n log of 1 over 1 plus e to the minus W transpose X plus P and there's an Arg max here yeah okay so all I did is I just took this different this this P there's the probability of my label given the email or something and I just plug in the definition right so this is our modeling assumption we say our label behaves the following way raise your hand if you stir with me alright good thank you I know there's this tiresome but it gives me a lot of feedback alright and oh yeah oh sorry one thing I forgot you actually have to have this particular label why so you multiply with that's why here actually let's get rid of P okay so we can always get rid of B because it's just same as perceptron you can just absorb it in your data okay we're almost there if you now take the log of this expression on the right right lock one over something well that becomes you know the sum I equals 1 to n then we have the log of 1 or log of 1 anyone know what that is well I'll leave it as a mystery for now log of 1 plus e to the minus y w transpose X ok so log of 1 I will let you know it's 0 so what that means is you can just take the sum and move it in here and I'll give this expression and we are stuck all right we have no idea how to find the best Avenue but in there that maximizes the secret this equation that'll be for the naive base whenever we did this at the end we just get some beautiful little formula it turns out it's just like Oh is the ratio of some counts well in this case actually it's W is W that maximizes this function so any ideas yeah get the gradient set it to zero that's right good good good hit the game set to zero it turns out you can try this I'm not gonna do it now because it will fail so that there isn't actually a closed-form solution so it's a very good idea right if you want to minimize it you know maximize a function and try to find the extreme point it's a great exercise but don't get too frustrated when it doesn't work out if you take take the gradient at zero actually there's no closed-form solution for this so well one thing you can do is you can just look at this a minimization problem if you remove them the - so now it's Park Min of W so you can think about what this function looks like right and it turns out it's a function that actually is come back so it's a function it looks something like this right and here's my W and this years the function that I'm trying to minimize and turns out Benes function is convex and try to find that you know try to find the minimum we can use things called hill climbing methods so basically what we do in a nutshell we start with some W and then which we base the estimate which direction goes downhill and they keep going down okay okay good so we will get to this next lecture let's try one more thing all right this was Emily what if we do map right these emily is not the only thing you have our in our tool chest right so Emily via stuck and we have to do something more complicated here right so we were really excited maybe have this better discriminative way of doing naive Bayes but turns out actually you know it's actually not that you know it's more complicated what if we instead of Emily be used map does anyone remember what nap what map does what are we maximize here so we maximize probability of P of W given the data right so that's the difference when should we try to find our max of da be given the data right and that is proportional to you have the data given W times P of W so that was exactly what map boss so this year is exactly the same thing as Emily and P of W is something you have to make up so we say well maybe we kept this base a prior belief so basically what map does is to be saying Emily V said which which W and B are most likely such that our data becomes most likely here we saying which W is most likely given all the XS and the Y's that we observed and so we don't know how to do this but we use Bayes formula we flip it around and we say you know what's the which W maximize our likelihood that's the same thing as Emily times this prior over W and basically kind of you big you remember this Thanks so this is the thing that we the monkey difference is now W is a random error that's okay you have to have come up with some random distribution what W is like and so those weird right there's a hyperplane right so what what distribution should our hyperplane be from and so but typically in pivot typically what people say is well it would be good if it's not far from zero why is this because you can make really really obviously how far you are from zero is a measure of how complex the hyperplane is that you learned let's face it the assumption so if you have to go really far away from it if you make something with very large weights then that basically means you have to learn something more complicated something really really simple would be something that has very very small numbers and it's very close to zero so that that's an idea and we will get to this a little bit and more detail but you know a typical thing would be that you actually say you know the W is actually normally distributed so you actually say P of W comes from some normal distribution right with zero mean and some some standard deviation so this here's a vector there's also something that bill with your homework so I won't be ruining your fun but if you actually plug in this distribution and derive the map estimate what you will get is actually exactly the same formula with a small change here that you actually have plus lambda times W times W for some lambda so when you do map map you get basically the same thing it's just one completing air additional air just as of turn and what this does is you minimize this as well you're minimizing this term here which says explain my data and you're minimizing this term here which says give me a simple solution it doesn't help us being stuck we're just as stuck as before let me give you a little demo so that is not too dry [Music] [Music] fine oh here we go all right good good okay so you only have a little time left so here's what I'm doing let me once again draw some points in two years let's say you know it does I'm positive points and some negative points and let's do it maybe this way that you know I'm kind of fooling this that this is certainly not a Gaussian distribution right because I have these outliers here and now what I do is I fit a Gaussian as our knife base Gaussian knife base and what you see here is basically it it calcifies most of these points correctly it based the estimates this distribution and this distribution as two gaussians and put the decision boundary right between them but it you know the year two points right these two x's are in the minority here and they basically didn't get modeled right the model doesn't explain them so because we're not there separating the data we're just separating the model that we've learned naive Bayes it doesn't really work here but one thing we can do now is we can actually say let's optimize the decision boundary well you know the way we just that so let's maximize the likelihood of the decision boundary and this is what happens oh it's magic isn't it oh here we go BAM and now actually the two points you know the two classes are perfectly separated from each other okay so what we're doing is we shifted from basically going from something that just estimates the distribution so something that actually you know estimates the likelihood on the data anymore datasets that people want to see maybe time for one more who wants a chance we'll just take aggression yeah overlapping okay good good good that's it so you guys I mean of course you're trying to torture this pure little uh naive Bayes so let's do this we have some positive points something like this and now we have some negative points where shut Jordan so there's not linearly separable is this fair do you want to be even meaner I don't know yeah all right okay here we go look at this you want even up okay all right here we go so that's a what what now faces us actually it's not that bad actually because they're approximately Gaussian right in this case logistic regression cannot separate them but it will adjust the hyperplane and I guess what it does basically there's more points here that actually crosses so it corrects the hyperplane you have fewer misclassifications by moving it over here right so the likelihood of your data actually is better if you move the hot plane a little bit to the left alright I leave you with this to think about I see you all on Friday 
","['', 'Naive Bayes', 'Logistic Regression', 'Empirical Risk Minimization', 'Maximum A Posteriori (MAP)', 'Multinomial Distribution', 'Bayes Rule', 'Classifier', 'Label', 'Feature', 'Hyperplane', 'Gaussian Distribution', 'Linearly Separable', 'Outlier', 'Decision Boundary', 'Likelihood', 'Perceptron', 'Kullback-Leibler (KL) Divergence', 'Expectation-Maximization (EM) algorithm', 'Softmax function', '']"
"all right welcome last time we talked about logistic regression [Music] thank you know I come oh thanks - neat I'm a gasping oh okay sorry about this [Music] it's not good luck I'm sorry okay so last time we talked about logistic regression and that was basically the way we derived this was by saying well what in a naive face you know learns P of X given Y and P of Y and you put this together to get P of Y given X and in order to do this tractive levy here we make the nice base assumption well turns out in certain cases this P of Y given X actually if you do nice base UK actually get a very interesting formulation you get a linear classifier in particular in you know the gaussian case which is something you will prove in the homework this will have a very specific form will have the form 1 over 1 plus e to the minus w transpose x times y and this is called a sigmoid function and so well one thing we thought is well if it has that form you know anyway why do we fit this function by actually fitting P of x and y why don't we just instead assume this form and then actually optimize the likelihood for P of Y given axis is what we really care about ultimately what we really care about is making the prediction of Y given X and so we looked into this so it's my mic on I feel like it it's not on is it on thumbs-up thumbs-down no it's not on want to test better alright okay good yeah people the mic is off or if I start speaking German let me know so it happens occasionally so let me said okay well let's do this right let's go over they decide to do maximum likelihood estimation that's basically sum of all our data try to maximize P of Y am i given X I and so we maximize the following maximized over W the product over all points P of Y given X I we took the log we did all the stuff yeah and eventually what we arrived at is a loss function that well I know it by heart actually and we somewhat want to any take log of one plus e to the minus W transpose X I Y I and that is what we want to minimize okay so we forehand whenever we minimize something we get a nice closed form solution like you know embedded map and Emily this time you to map it Emily and we arrive with something like this all right and we don't know how to continue at this point by the way please close your laptop's thank you well not all is lost so the nice thing about this that the problem is you know this function we don't really know what to do but the nice thing is this functions actually very well behaved all right it turns out it's actually continuous and it's differentiable and actually since I've even come backs so what we do it what we do is we call this scene here our last function our loss of W and so we can just view this as an abstract problem we have a function of W and we would try to find its minimum okay there's something you've done ever since high school I believe right it's basically this just you know good old analysis given a function where does it have its minimum point all right any questions at this point some people asked me some interesting questions last time that I just want to relay to everybody so I make this point to that P of Y given X is actually that's what we really want to minimize and so it's actually logistical question seems to be the better thing to do you know instead of naive Bayes right so then the obvious question is why did we spend three lectures now you're talking about naive Bayes if you should just do doing be doing logistic regression all right and actually it's not so clear-cut right it's not that logistic regression is always better than naive Bayes and the reason is quite simple when you don't have all that much data let's say you in a regime like for example you do spam filtering right well there's actually two reasons there's two reasons why you want to pay for an I face for spam filters for example the first one is an I face is really really fast you don't have to Train anything here we still have to figure out how to me to find the minimum the second thing is imagine you've only seen a couple of emails right the problem with logistic regression is you have a very very high dimensional space and you're trying to predict Y given X and this there's infinitely many hyperplanes and so essentially what it will do it will be extremely biased towards these by these few emails that you've seen right so if you're just a very few data points it will actually not give a very good classifier because it will essentially over fit and we will talk about overfitting very soon all right naive Bayes is not so prone to that and the reason is because here you make an egg out an assumption you actually you add additional knowledge right that you put in you're saying this distribution has to be gaussian and by doing this you're reducing the number of hyperplanes you could possibly learn drastically right that is that's a bad thing if you have a lot of data if you have a lot of data then why would you reduce the number of hyperplane that you can learn right just learn the best one that separates your data but if you have very very little data if you only have a couple of data points right then there's just too much degree of freedom and you're going to learn something that is you know overfits too much to these few data points then actually naive Bayes is actually much better now turns out for spam filters most people are pretty lazy and they very rarely spam is not spam and one thing you want to do is you want to give them a good spam filter right at the beginning when they tried your product for the first time right so we actually knife base texture typically what people use in this case so it's still you know it's still used very heavily it's just when you have a lot of data then actually typically to this aggression it's the way to go any questions about this no all right so here today I will post a look into this this abstract problem of saying you have a function L of W you know and I want to minimize and this is a very general thing and turns out that many machine learning algorithms not that not just with justing aggression many machine learning algorithms can be written as in exactly this form right and we call this our last function so what this basically does it it measures how many points do you get wrong right that's essentially what this function does so all right so what does this function look like I told you it's continuous differentiable and convex so it kind of looks like this is something else right so it's kind of like a like a bowl that means it's convex it doesn't have any jumps in it that means it's continuous and you can't compute the derivative so it's differentiable and we would like to find this point here right that's our target okay so the way this works is there's many many many different ways of doing this and optimization Theo is its whole own field of mathematics but I will tell you today the basic principle and essentially most algorithms don't deviate too much from that so here's the idea right what we do is we say well initially we really don't know where there's many moments we have no idea which parameter setting is the mineral right if you knew it we had a closed-form solution we'd be done we don't know so what do we do we just assign W to anything we want it doesn't matter all right whatever just make it all zero so all random varies right you know put in your own your birthdate encode it in the in the weight of W so you have now some point here which is my initial W 0 okay so basically this here's my axis and see as W in this year's the loss of tab okay so initially I just said you know my loss to anything I want it doesn't matter and so now the important thing is I would like to know which direction does this the minimum line right and so basically that he defines like some some direction that I want to walk into is call this s right and I basically want to you know identify which direction do I have to go and then I take a small step in this direction and then I get a new point and then I look again right that's the basic idea behind hill climbing in this case we're not climbing we actually tie to find the minimum so function minimization okay any questions at this point all right yeah question is there more handouts there should be more handouts I printed as I printed enough and there's a lot of empty seats who has two handouts who has more than two sorry oh this whole thing hasn't got any oh I'm sorry wait but but I'm confused are they somewhere if you know someone who has the handouts please point in that direction people people are playing if he was actually oh I'm sorry how does that way that's really bizarre I had 300 copies there's not 300 people in this room now someone leave someone leave with a big sack full of notes I'm sorry I don't know how to resolve this yeah there's nothing I can do I'm sorry alright I will guard them more carefully next time there will be online so yeah oh but then I have a mad rush at the beginning okay well I could okay I could put them in the entrance and find some algorithm okay okay good my suggestions please post them on Piazza if you have some convergence rates actually I but you know appreciate okay good all right so we have this function to minimize the problem is we don't know what this function looks like right so we know it kind of looks like this but it could be nasty right and you also don't know how much of a step you should take right if you take too much of a step then we may overcome it you know right now it looks like this but the function could also look like this right so then if you here we take a step that's too large we actually end up over here right which is not what we want or if you take a step a little bit too large view over here which is really really hard high right so how do we find this minimum and the assumption is that you know what we do is a very very simple trick as basically we say well this function is really really complicated and we don't know how to minimize it but let's just assume the function is was easy it was really simple and for example linear or quadratic and if the function was linear or quadratic we could actually find the minimum very easily right and so basically what we're doing is we we say well you know here let's just fit a line and say assume this year was in line right well if the function was just a line then it's easy to figure out a misdirection you should go right just go in the direction where it goes downhill okay and you just take a step in that direction and then you you hear you can you fit a line and you basically look at this again right so that's that's the idea more formally what we're gonna do is we're gonna use Taylor's expansions who's heard of Taylor's expansion raise your hand Oh perfect beautiful all right so take this expansion is quite simple it says you can approximate this function L by saying L plus s is roughly the function W plus the gradient at W times s right so it's like this is just this times s so what's going on okay good thank you [Music] right so basically what we're doing is we say at this point like a potato expansion by Z says is this function is continuous and it's differentiable what does that mean that once again if you're a teeny little ant you're sitting right here right what you would think is that the function is actually a straight line right if you're just small enough right because the function is smooth right if you just zoom in enough right at some point you will actually not feel the curvature anymore right and this is exactly the effect if you have planet earth right so it's globally has a huge curvature but actually if you just look here actually looks damn straight right so that's basically what we're doing here right we say locally actually this function the approximation that the function is linear is not a bad approximation and in fact this is not just the only thing you do we can actually say well we can approximate even better if you add a second derivative and then we basically say well this is plus 1/2 s transpose H of W s and cbz say what's the function value at W and then I go a little bit in direction of s if s is just small enough then it's linear right then the base is a little larger then I guess you know a better approximation actually is if I take the second term into account as well then what I'm doing essentially I'm saying the function is not just a line is actually a parabola locally right so it looks like a parabola ok any questions do people know what the age of W is if the Hessian who's heard of Hessians outside of history okay so the hessian base these are matrix that baby just says the IJ entry is the derivative with respect to well I mean let me write it down so H I J is the derivative you know of L with respect to W I and W J is actually the second derivative generalized to you know functions with multiple inputs okay and so actually tailor approximations as if you just you know you take the first derivative second derivative and the third derivative and so on you know they could eventually eventually make it arbitrarily close if the function is you know differentiable enough we will stop after first and second derivative right because the reason is the first and the second derivative these are actually if you just look at the first derivative it's a line and we know how to minimize lines and if you look at the second derivative it's a parabola and parabolas are also very very easy to minimize in fact we can do it in closed form right so those are the two these two is this approximation basically is simple enough that we would know how to minimize the function now the trick however is this approximation only holds when s is really really small right so you kind of say oh the function is roughly this that's minimize this thing on the right right and then you're done that's not gonna work right so for example if you have this you know parabola some some what you would do is you would jump here and then you would actually find the minimum here well that's obviously wrong right so the assumption only holds for small steps right so basically the way it works is we approximate the function as the Taylor expansion we minimize we take a step towards the minimization of the Taylor expansion and then we approximated again and we repeat that process and that's basically how these function minimization processes work okay any question yeah why don't you make s believe a small end well why don't we just use the linear approximation okay good question so he says this Hessian I don't like the Hessian right what have they done why don't we just use the linear approximation if s is small enough that's a sufficiently good approximation and that's actually what most people do alright there's a reason why you may occasionally want to use the head shift the reason actually is that you have to take many many many mini steps and so sometimes it's the case that you would have to take 10,000 mini steps or you take two steps for the hair shoe so it can be a lot faster right but it's not always the case and I we will get to this in a minute okay so the first gradient descent is first up is gradient descent a lot of you I'm sure I've heard of it is the most general framework of minimizing a function that's essentially exactly what you just proposed green descent basically what you're saying is the fact this this vector s that we want to add to our function to go downhill is just the negative gradient right it's just the gradient w times x times a small step size times a negative sorry so we had where is it sorry one sec ah [Music] so here we had this little picture that we Daisy have our w0 and then we take a step s and here I'm telling you a gradient descent we just say s is the gradient at W times some small learning rate and then we negate the whole thing and so why is that the right thing to do alright well if you just look at this approximation then it's very very easy to show that actually if you take this step it will always take a step down we will always reduce the function value let's just test this for a second so a loss function L of W plus s which is now this term here so plus minus alpha G of W okay so now we have our original W because I'm telling you this here is the step this is basically this this step in which we want to take from our original W and I'm claiming this year should be less than just L of W so it should be less than the value that we started from right if that's if that's true then I showed you that we by taking the step we went down here so we know that this is roughly if alpha is small you can make this as small as we want and this yes is small so we can make alpha small enough that the Taylor approximation holds then that is L of W plus this years or I guess - in this case minus alpha times G of W this is s times G of that okay so all I did here is I plugged in minus alpha times T of Java you into the s up here right and then we get this term here raising hand if you're still with me awesome and well what is that let's just look at the sine of this thing this e is the square of a rect vector so that's greater than zero right alpha if you just choose alpha to be small but positive so let's do this off as great as zero right that's a skinny parameter so that whole thing is still positive and then leave a negative so this thing here is negative right less than zero so we take we take the like the Elif double austell of W and we subtract something from it so that's clearly less than L of tavi okay so what I just showed you is that no matter where you start right no matter what you starting point W is you just compute the gradient multiplied by the small numbers and if you subtract that from your W you're taking a step downhill and so if you do this often enough you will eventually come to the minimum and so that's basically the idea right why will you eventually come to the minimum wait a second why is that guaranteed can anyone tell me yeah because the function is convex right so that's right so it could be if the function was not comebacks it could look like this right many of you start here you go down here go down here go downhill here at this point what happens here here in the stationary here that waiting to 0 right so you stop all right so you think you're you're done but actually you know this can be obviously deep here as you can connect really bad but because the function is convex and I think this was actually on the placement exam to show this you don't have to worry about this right so you keep going down keep going down keep going down and eventually you will stop why are you going to stop because at the minimum the gradient is zero right so that point you you know you basically know that you found it right you're not moving anymore okay any questions about this process No yeah how sorry how do you be alpha good I was waiting for the question that's exactly right right how do you find alpha we said alpha has to be so small the Taylor approximation holds well how are you gonna do that and well one thing is you can just err on the side robbery alpha is too small right if alphas really really small the until approximation will certainly hold the problem is then you're taking tiny tiny tiny tiny steps and it will take forever right and it may take your whole lifetime to minimize the function right so then actually it's probably much better to wait a little bit and buy faster computers in ten years and then just start then so the question is how do you find this alpha and and this is actually something that the community has thought about for a long time and I think what I when I started my PhD it was still consider the dark art and people thought well you just use voodoo right so everyone has their own kind of mechanism right now all you know eventually I start out with point one and then eventually I decrease it right and you had your secret sauce now you know fast forward a little bit it's actually very well understood so for once a really safe way of doing it is Phase II just saying alpha is some constant divided by T but T is the number of updates you've already taken if you do this then it will provably converge and the reason is no matter what your constant is in the beginning but eventually this will become so small that Taylor series holds and so you don't have to worry about it and you know you can make it can you you can make your constant large enough that it will converge eventually yeah that's not very good because it's still very very very slow in the last two three years people have actually made some sufficient progress here and I can damn it what is this is horrible okay and nowadays that people do is actually much more sophisticated and I want to want to quickly go into this it's called a de Gras oh my god and only a couple it was only invented a couple years ago but it's basically really taking the community by storm and the idea is very very simple the ideas well here's how it works right if you have a very very it depends really on what your function looks like right if your function looks like this and you're here right you want other to be very very small because alpha is too large you're gonna jump over this this thing right if you function the other hand looks like this right then you want to have alpha pretty big right because being spun pointing this direction you want to huge steps right it's not so steep and so essentially the mechanism I'm just going to explain to you as a heuristic I'm not gonna but it turns out X you can derive this very very nicely and it's very principled it is very simple what you do is as you do your optimization you keep track of the gradients that you've had in the past and the basically what you're doing is you say the following well imagine you have different features right so you have some features that are basically tracked and you know you have medical records some attract in millimeters so you hide in millimeters there's a very large numbers I don't know why you want to do this but you know and or you let me give it this better example let's say I have spam spam filter I have word counts right some birds will be very very common right like the and a and so on right these are stop words but even you know banana is gonna be much much more common than maybe hippopotamus or something right so words have different bird frequencies so what happens is that basically the gradient of Ariel across the different features right and so really there isn't really one good learning rate for every single feature so ideally for each feature you'd have a different learning rate right but now we made the problem even worse right before we just had one learning rate how do you fix this one this was a dark all right now we have to fix a thousand or maybe a million learning rates right every single dimension should have its own learning rate and so here's what I regret does it base says as we keep optimizing so hey I put in my notes that's right as you keep optimizing what we do is you have this algorithm that we say now we repeat they could be evaluate you know lfw sorry the gradient of W and G equals DL DW that's a vector right and then what we had before is we said okay G W becomes W minus alpha times G right so that's the gradient descent algorithm and we don't know how to do this alpha so here's what we do we do the following we just we start with some vector s which is initially is zero we put our for loop and then every time you complete a gradient we say s becomes s plus G squared and can you explain what I mean by this where the Julia people I think every single energy is squared okay so you can either write this as this so you know so or you can write it as this G all right so lazy you just take every single entry and square them but you keep it a vector you don't sum over them okay so it's not G transpose G is G dot star G so what you're doing is you be easy to keep track of on you know an average a like home how large the gradient walls and past's in past steps and then what are you doing is you take the grain that you have an element-wise you divided by the square root of the Sun plus some Epsilon so essentially what this means is when in the past I had a feature where the Grady was very very large right then this is very very large right because I square it and I sum over all the different changes so that means if I divide the gradient by it the steps that's going to be really small all right so if I very steep and some feature them in some dimensions I will have very steep surfaces and for those that take small steps in other dimensions I have very shallow surfaces I take large steps okay and that has really become kind of the beauty of it is you still have some some constant here that you can set but doesn't really matter you can just set it to one and then you just get rid of it right ultimately it sets itself right so busy takes past gradients and it learns how to set the learning rate any question yeah oh oh I'm sorry is that true oh I see Oh OSI does my notes are wrong actually right OSE in the nose that should be SS instead of GS I and then there's notes I call it I call this s and this SS which I blame on momentarily insanity okay any any more questions yeah Paul why's Epsilon oh it's just because in case something is zero imagine you have some feature we just have no gradient ever right that also means that feature doesn't matter right but that means s squared would be zero and you would be divided by zero and everything would blow up so you just add some constant to it and then you get a very large learning way that probably doesn't matter because then rate is zero right yeah good I mean in some sense because you just added this write the gradient must be zero here so it doesn't actually matter what you do in that case right good question yes any more question so X are just ten to the minus five or something oh yeah the very back yeah [Music] oh good good good right so so the question was well actually we're just getting closer and closer and closer and as we get really really close to the minimum what's gonna happen the gradients gonna come you know go become zero right here the zero so here it's gonna become really really really small right so what will happen we actually take really really small steps so when we actually ever really get there right so when should we stop this whole whole algorithm and so you do exactly this you basically keep track you say this here's my T plus one this here's my T and you say you stop if you know if the norm between T plus one minus WT is less than some some small Delta then you break then you stop right so usually always have some color please say if my weights have not changed the last round then I'm stopping right the important thing is there's really something that people didn't realize for a long time it's enough people use these concepts of optimization for your mathematics and optimization theory they're really concerned of getting exactly to the minimum by accuracy up to ten to the minus ten or something in machine learning we don't care about this right because ultimately what we want to minimize essentially is just the zero one loss right leg end you just want to have good predictions and if you get a little bit closer probably doesn't make any difference right so you know this Delta can be sufficiently large okay any more questions okay good so that was grading descent now come in the Hessians so great nice and it's really nice the only thing is and there's actually the step that you said it's really nice getting close to the minimum but once once you're in Lewisville inity it slows down so the the most of its time almost all its time it will spend you know nose to the minimum okay because that's when the gradient is small so the question is can be vitas apps like once we are close to the minimum 10 we get right to the minimum you know up to some Delta that we can stop and this is exactly where the the second order approximation all right so so far what we said is you know we just approximate there's a lion amazing step you take a step in direction off this line the slope now we're basically going to take a parabola and so seconds so the idea is quite simple you can just say exactly this function here and just minimize it with respect to s we say wait Taylor's approximation tells us our function is approximately this way now don't cross this out any what kind of just minimize this there's a quadratic function and what's exactly the step size that I would have to take to minimize this function so what I'm saying is I have some W here I find this parabola now I pretend my function really is the parabola and parabola sickens minimizing closed form so I can just find the minimum of the parabola right that's what I'm on so forth and this here is my X further the the difference from W that I have to add to get right to the minimum of the parabola and once I'm here that I again fit a parabola I find a minimum again I fit a parabola and then I'm done Newton's method converges superfast blazingly fast I just need a couple of steps like if you take 10 Newton steps you know you're you're you know you're gonna be the absolute you know a numerical accuracy usually when you have low low accuracy numbers I get machine learning a few Newton steps typically get you there very very quickly so let's just quickly do this so you want to minimize this respect to s so minimize this function with respect to s and what do we do what do I do when I minimize a quadratic function it starts with gray and ends with the end I think someone's that a gradient it's like a few the gradients it's back to s this here zero this is g fw this year is now I have to take matrix gradients if you're not familiar with matrix gradient look at the matrix cookbook I think I'll link to it from my from the home page as that's H of W times s and you want to set this to zero so this here is the gradient of this function with respect to s that's zero what do we do well it's quite simple right so we just put this on the other side so s equals G of W minus 2 u fw and to get this function this matrix over there we just take the inverse right H of W inverse nice we divide by this that's what you have to do so Newton's method what you do is you just take your random vector W you compute the gradient and you compute the hash stream and you multiply the gradient with the inverse Hessian and that's what you add to your function and that basically gets you down really really really really quickly so Newton's method is awesome there's our one dance there's one downside and this downside is most of the time it doesn't work at all so most of the time what happens is that when you when you have a function that's really really flat right what you're now doing is you're approximating that with a parabola and that parabola could look like this or something like this and now you're taking a really large step that's every function looks like this something now you take a really large step right this here's my s and if you take reading our steps Taylor approximation doesn't hold anymore right and so what you're doing is you're shooting off into no-man's land right all right let me give you an even better example let's say have a really any function like this right function like this this is my convex function right so it could very well be that my parabola that I'm using here looks like this right approximating this way so I start here I jump all the way here to that minimum all right now I end up here right not approximated again with a parabola and I end up here right and I'm spiraling out of control in no time so Newton's method really only works if you're very very close to the minimum already nice and your function is not well I show you some examples why not show you some examples so the best typically what I recommend people to do is to Grady descent for a while and then just before you end take two or three Newton steps right and then you're basically then you sort it right because then you have basically at the minimum any questions well people buy that I also do is they sometimes just take the diagonal of the Hessian that also works reasonably well those are called second-order methods or there's conjugate gradient that approximates it yeah well in some sense you're solving for the whole thing right so it's no longer if you don't scale it I think you still wouldn't get any little convergence guarantees actually in some sense the benefit would be gone what is this does not mean all right okay good all right so here's a how much time you have okay and so first I just want to show you a little so okay so first a little example of just good old Grady descent so here's my function and the red point is where I initialized it and the middle is actually control a function so that the minimum was actually here at zero right and so now I can be put on the different different step sizes and see how long it takes me to converge right and so the first thing I can do is I can just set the step size to find one all right and now you can see the different steps I know if you can see these can you see them it basically takes all these these red lines here from here to here basically is a step right so all these different things so I took all the way eighty-three steps right so took eighty three steps to go from here to here and then it basically converged up to some Delta I can do the whole thing again with a smaller step size then actually it never converges at some point I just said maximum number of iterations has reached so I never get there right if I make my step size larger let's say I just do you know five right why not then what happens this happens right so here's my function weight and this year busy you see this here's my function out here and this is kind of Jupiter and this is you know like Neptune or something be way out there in outer space right oh my gosh come on where is it I see something I'm hopeful [Music] all right so I'm really really out there all right so you get the point sorry oh yeah central 195 great yeah that's more than electrons in the universe right so we are really really out there right so I didn't even take my step size that large right I was just a little bit lighter right and be have rights did I shoot past the minimum all right so setting the step size is important I can also show you some Newton steps so here she had the demo1 Newton so here's another example there's a beautiful function and now Newton the beautiful thing about Newton's method I don't have any step sizes to choose from right so I can just you know I just run it and I converge so we can first say okay well let me just start at 0.3 right and at 0.3 my conversions nice steps right well I'm pretty accurate here very very accurate if I you know say you know let's say I or this is all point three I start up here okay that's pretty steep so if I started you know whatever it's two right then I would only take eight steps and I'm actually here right and the nice thing is every single step you X usually get one digit off of significance of accuracy what if I start at nine right and so that means I'm starting over here and I do my Newton search right then BAM right what happens well I'm actually oh and I'm still fine wait I'm still fine this is amazing okay let's do seven that's 210 okay now I diverse right so now actually if you zoom out if I try to zoom out which I can't so one thing you can see wet is the red line actually okay I can't zoom out I'm sorry I don't know what happened basically I'm again in outer space right so the difference is the Newton step the equivalents in some sense like gradient descent you have to set a step size right Newton said only works at certain starting points right the good news is you can check very easily if you made a mistake right if you're lost and suddenly a lot larger than you know oops right Newton step doesn't work here yet just keep doing a few more gradient steps alright I get using the same thing one more time in 2d now Simon's over ah shoot here we go here we go so what you can see here is actually a function that I'm trying to minimize alright so so what you can see here is basically the black line is basically gradient descent here's my function at this basic function value this number of steps I'm taking and what do you see is the gradient sent very quickly gets close to the minimum right but then takes forever to do this little last little bit right to get from here to here right takes a hundred steps right so that's exactly the problem with gradient descent right so you get close to the minimum relatively quickly but then just takes forever to finally get there all right so Newton steps actually is very very good right in just a couple steps actually it's really at the minimum if you look at the the plot here on the right jex you can see this is the function here's where I started so gradient descent takes these sorry greenie sense green ones that actually takes one step here and over and over and over you know them very quickly it's actually close but then here it actually takes forever right whereas Newton steps actually very consistently gets towards the minimum and then you can sue in any further Newton step basically just takes two from here one more step whereas the grading is sent here probably takes 90 more steps right so if you're close to the minimum you want to take this method let me do one last one before you guys leave does the here's basically if I started at a bad point so here's the same function and again you see here I'm actually the red line is Newton steps so I my function is down here and very quickly in outer space right I basically completely blew it and what happens I just started as you know so here's what Newton step does right so grading descent very nicely goes to the minimum when Newton steps like oh my gosh right like it's like so what do you do in such cases and the answer is you just take a few steps with gradient descent and then you do Newton steps right so here's what I did I took took a couple steps of gradient descent and then I switched over to Newton and then I'm okay again all right so please remember this the main take-home point when you do gradient descent to adaptive gradient to the other grad and when you get close to the minimum so maybe after you know a couple hundred steps or 1000 hundred steps switch over to Newton's method then you converge in no time 
","['', 'logistic regression', 'naive Bayes', 'maximum likelihood estimation', 'loss function', 'gradient descent', 'convex function', 'alpha', 'stochastic gradient descent', 'AdaGrad', 'learning rate', 'features', 'gradient', 'squared gradient', 'Epsilon', 'minimum', 'convergence', ""Newton's method"", 'step size', 'good starting point', '']"
"all right welcome please close your laptops all right so few logistic thinks so I see people are working hard on their naive based projects that's awesome the next project is already done and we're currently testing it on two TAS and it will be shipped out tomorrow that's the plan so please don't fall behind homework for that we after the midterm so no more homework but one more project before the midterm okay so the last year so yeah okay so last time we talked about we started with logistic regression which is essentially saying you have P of Y given X right and we had give it this very specific form you know 1 over 1 plus e to the minus W transpose X Y if Y is minus 1 or plus 1 and so the question was can't be just you know optimize this directly such that we basically make as few mistakes as possible basically essentially we do make a maximum likelihood estimation so basically saying go over dataset and say for every single data point X we look at the label Y or we would like to do is that the dataset that we actually observed is as likely as possible and so we did this you've entered the motions when we came up with this last function let me you know we're basically stuck on in the last function was the following was you know we sum over all want to end and then we basically had log of 1 plus e to the minus W transpose X I Y I all right and so that was the function where we don't have a closed form solutions or the question was how do we minimize this we know this loss function somehow says you know the lower that is the better the lower this is the more likely is our data and so then the question was how to be minimized isn't a nice thing about the last function is convex and differentiable and continuous so what we can do we can just do local hill climbing and so we talked about two different methods the is the function looks like this we want to find this point here but unfortunately we don't know where it is if you knew it you just jump there but what we can do is we can just take any given point and evaluate the last function here so we look at this point here we say okay well we know what the function is here because you can compute it and you can also compute the derivative and sometimes even the second derivative in this case you can also compute the second derivative so if you compute the derivative then we can actually look at the local slope and say well if this was a linear function then we have to go in this direction so let's do this we take a small hop in this direction now we end up here again we look at the derivative you know we would have to go in this direction to go downhill you take a small hop and we keep doing this basically and then we eventually arrive here that was that's just good old gradient descent and then he also talked about how to set the step size so how big should the step size be how how big of a step do you want to do there's a very effective algorithm called a degree at that sets this adaptively that's what the adder comes from and then finally we said well if you actually have if you know if the the luxurious situation that you can actually compute the second derivative then you can actually approximate this function not just as a line but also as actually a parabola you can minimize the parabola and then actually you know we can get take more aggressive steps that converges really really fast the order of 10 maybe 20 steps but the problem is actually if it doesn't converge it blows up completely and that's not a problem because you know if it blows up it knows that you know you know you know it blew up if this function value suddenly went up you took a step and now the larger value is larger so then what you do instead is you take a few gradient steps and then eventually just switch over to new this man okay good I hope you remember this so a lot of machine learning can be written in exactly this form so we have some algorithm and it turns out we can actually just massage this algorithm into saying ultimately all we have is this function sum sum function this is the specific function of logistic regression and turns out many many algorithms can be written in exactly this form you just say but ultimately you just have a function and this function measures how well I'm doing and lower is better right and so usually you can't go negative so let's just maybe think about this function for a second this here's our logistic last function and M I claim it's actually somewhat an approximation of the zero one loss of a zero you know it lose the approximation of the zero one loss so really what we want is that to say we go over our train did I said and then ever we get one wrong we want to want to change that we want to penalize how many examples we get wrong and you know when you get something right then that should not incur a loss right so in some sense a great loss will just be I go over my chain data points and I some how many examples I get wrong that's the zero one loss the 0 1 loss is not differentiable it's not continuous so we can't really optimize it so essentially that's why we are doing all that stuff right if you could just optimize the zero one loss you know that we would just do that directly so a lot of these algorithms essentially what they're doing is just approximate approximating the zero one loss so maybe I give you a few minutes to you and your neighbor just think about this part of the function and think about what how does it behave and you make a mistake and how does it behave when you get something right right and maybe discuss it for a few minutes with your neighbor and just get some intuition what happens when you ways to get a point right and what happens when you get a point wrong got it right be an intuition when I get an example correct then what taught us this function here I'm summing over behave yeah so that's right so the only thing is if you can really coast you at your W actually it's well you give you very close you mean in terms of alignment yeah yeah so actually but essentially what you're doing is you have this this is my coordinate system we have some hyperplane here this here's my W let's say I have a positive point so Y is is positive if I get it correct then W transpose X I is also positive that means I'm lying on this side of the of the space so that means that W transpose X I Y I is large right so the further I'm away from the hyperplane the larger that is are you saying it exactly right then this charge it turns to 2 to 0 because it Y to the minus something large that turns to 0 very very quickly it's exponential decay and what do you end up with log of 1 what's the log of 1 0 right so this goes to 0 as I'm basically on the right side of the hyperplane what happens when I get it wrong who knows the answer here yeah yeah I think you're saying you're saying the right thing so basically what happens is when I get it wrong then why I has a different label and W transpose X I so then for example the point is positive but I'm actually lying here right so W transpose X is negative but the point should be positive so this means this thing up here is negative and if I take my t5 with a negative number this V becomes really large right because it's greater than zero in this case okay this is when I get it wrong now e to something greater than zero blows up really really quickly it becomes very large all right so this here becomes very very large now here comes the interesting part right so this could be some large number let's say it's five thousand as I write e to the something then I'm adding 1 to it what is it adding 1 to do nothing right there's nothing right because this number is really really large adding 1 to a large number it's not very interesting right and then you can you take a log of that well what is that approximately it's just this term up here right so that's actually all you get so you busy just get you know W transpose X I write which in this case and SpyEye negative right so as the last becomes negative this is just the log of an exponential which is basically the identity right does that make sense the log of e to the something it's just the something okay so log of a log of e to the a equals a right now if I have log of 1 plus e to the a well that is still roughly a right because this year is usually much much much larger than the one that doesn't really change anything all right the difference between log or 5,000 log of 5,000 one it's just you know doesn't make much of a difference so so basically what that means is I get more penalty so this is I'm getting it wrong right this means these two signs don't agree this is negative times something negative is positive so the loss is larger the further away I'm lying right so this point should be on this side and the further I'm away here right the larger than you know magnitude that in the product is the more I'm paying for it all right that's basically what it is all right so there's a notion of if you get something wrong and you could get it more wrong you get it more wrong if it's further away from the hyperplane in the wrong direction so one key inside here is that this is linearly it could also be code radically growing that I am really really wrong I pay for it more right so in this case this is just a linear penalty any questions yeah yeah it doesn't actually make much of a difference because you can just take this loss and just divide it by any constant right and if you optimize it respect to W you get the same thing right you can just rescale everything the scale of OHS if you have some ax the larger scale and some of the smaller so yeah it's going to be dominated by those with large scale yeah yeah any more questions okay good so it's it's worth thinking about these these last function a little bit like what they actually intuitively are doing that gives you insights of how they behave and when they are the appropriate thing to use right so in this case for example logistic loss is pretty good in settings where you have mislabeled data but occasionally you have lost functions we have mislabeled data and here's not a loss function right it's actually also commonly used that's very suppose this one right each of the minus W transpose X I Y AK that sneaks financial loss is this Spacey what's inside the log here right that's not the last function the Israelis now it's somewhat similar to logistic loss function is just exactly what we haven't said the log but she have you get something wrong right it really beats you over the head right because yep you know you're raising it you know you're exponentiating so if you have data that occasionally is mislabeled right for example in biology or something you know you're never quite sure about actually say you can only guarantee that 80% of your labels are actually accurate you can't use this last function because it's gonna totally obsess over the few points that gets wrong there's logistic loss actually it's very lenient that way okay today we will talk about a different last function that's called a linear regression that's quite simple and also abbreviated as OLS don't even know what SS LS stands for ordinary least-squares that's right so ordinary least-squares ordinary least-squares okay and so far but you see what we looked into logistic regression by the way logistic regression is really really misleading and every year if people are the exams they get this wrong but ask them what's a classification algorithm much a regression algorithm people say logistic regression is a regression I would write that's not true right statisticians just doing this to mess with you all right come on like that I have fun too it's logistic regression is the classification algorithm and linear regression is the regression algorithm okay so just because it's called logistic regression does not mean it's actually a regression I would it's different reason and so today I want to talk about regression so regression is when the label why I a drawn a basically from the real numbers so we don't have a lake we don't have a you know two classes any more than you would like to separate instead we have some continuous value that we're trying to hit for example if house prices you know and I have some features about a house and I would like to predict what's the value of that house all right so this is what Zillow for example is doing or you know very experienced I mean medical settings you have many many different applications of this so what we are going to do is you're going to make two assumptions right so when you model data you have to make assumptions so before and Viet classification algorithms we have data positive and negative and we made for example the assumption there's a linear hyperplane that separates these two today we will also make a linearity assumption but it will be different I'll be saying okay we have data and there's no longer two classes right but we have x and y so first you have an X we have some value Y and we assume that this correspondence is linear so there are some line that roughly models that okay so then once we know this line then we can take any other value of X look up what the value of - okay and this could of course be multi-dimensional doesn't have to be a single image any questions raise your hand if you're still with me all right awesome so now we have to make a second assumption the first assumption is we're going to model this with a with a linear function all right the relationship between x and y the second assumption comes in well actually the day that doesn't really lie on a line alright it's gonna be off the line as you can see here they're a little above and below so what we have to say is well what is the distribution right what's the noise model that's usually what people caught right so in some sense you may see saying well and you know in the in Fantasyland this would actually be the true linear function but actually our observations are a little bit noisy and so in practice you don't actually exactly get this you don't hit them exactly on the middle of the lines and a little bit off and so what we assume here is it as a Gaussian distribution so saying that actually most likely is that you're right here but actually for each value of x you basically have a Gaussian distribution of what your value of y should be or more formally the assumption is that basically our Y I equals W transpose X I so that's the linear part plus a little bit of noise if you a little higher a little lower right and Absalon I is drawn from a Gaussian distribution with zero mean and some variance so those are gouge distributions that basically here along the vertical axis and so it's very unlikely that you know for this value you know for this value of x I have a value of y down here right because the mean of my Gaussian is up here okay so that should be somewhere around here or I can actually if I take this model here I can also write this differently I could also say well that essentially is the same thing as saying Y eyes are drawn from a Gaussian themselves with mean W transpose X I and Sigma squared okay so that's the same thing and I can either say there's a function w transpose X and I put noise around it or I say there's basally many many gaussians and the mean of that Gaussian kind of moves along that that's slope and so that's the same statement you okay any questions all right why don't we make this assumption that it's linear because often things are linear all right it's good that's kind of if you have a data set but things really are linear that's great if you can capture it because linear functions are really easy wide assume is not a Gaussian noise well a because it's really easy to deal with and mathematically and also often be half an hour ocean noise right very very often there's the center of limit central limit theorem says that you know if you have many many different random variables basically they're there means are approximately Gaussian distributed Oh okay all right so this is now assumption so if you make these assumptions then basically we get a distribution for any if you get a certain X I would you know we would be now automatically get a distribution of Y so Y I give an X I W like W here describes our model which is basically the slope and this here is our feature vector and the question is like this here baby describes our house and we would like to predict what's the value of this house right this describes how many bathrooms do you have you know what's the square footage what I like you know how close are you to a local school what's the rating of the school over there like there's all these features that people come up with and we say well that actually equals this Gaussian distributed so this actually equals just one over square root of 2 Pi Sigma squared e to the minus W transpose X I minus y squared over 2 6 squared okay in this year's just I mean I hope you recognize this there's just the good old Gaussian distribution right that you remember from high school okay so any questions at this point awesome so we would like to now do the following and this is actually the last lecture Brown think I'm going through this so this is the last time but I'm going to exactly this exercise just one more time just to drill it into your heads how do we ask in my double alright how do we figure out what the right W is if you just have some data given to us now we made these two assumptions that it's the relationship between x and y is roughly linear and we have maybe Yoshi noise how do we find the W that best explains our data there's two approaches can anyone tell me Emily and map I think people said it all right so the first one is Emily a maximum likelihood estimation we may see say okay I would like to find the Dhabi you that maximizes the probability of my thing so I go over all my data points and I say for each one of them the probability that I get this particular label that I observed given the feature vector that it has right and I completely computers for every single at a point now I want to tweak W such as the data that I observed should be as likely as possible alright so in some sense if you think about where are we oh here up here all right basically what you're doing is you have your data points and you kind of want to adjust your hyperplane such as it kind of goes through these data points right that basically none of these data points is really really unlikely right if one point was really very unlikely then you're multiplying with zero it's something very small so that doesn't you don't want this okay so who is note in front of them can you raise your hand if you have notes in front of you okay awesome who did not get any notes awesome so here's what I want to do I want you to just you know you and your neighbor we have the derivation for Emily right here printed out and I thought maybe it's beneficial if you just you know go through it with your neighbor and just go through every single step and make sure you can explain to each other you know how we get from one line to the next and then I will go through it in three minutes who thinks he or she is digested at all some people not many ok well let's let's go through it so if they say okay we go through our data sets and for every single data point we say given this w what's the probability of observing the Y under the feature vector that it has and you know we basically to get the probability of the entire data set that we've drawn you have to multiply them all because they a ID drawn so the first thing we do is what we always do when we have the product of many many probabilities taken a lot right so that's the same thing as let me just write it out this way as the sum I equals 1 to N is the arc max of log probability yes that's just an old trick and all right sure why not so now we've got this log of P well now comes the time where we actually have to substitute in what P is or Pierre conveniently is written right here so we can't just take the log of this expression here and samo all rights that's its Arg max and now the log and sum of a log of this expression here that's a log of a product of a product which is the sum of two logs so that's the log of 1 over square root 2 Pi Sigma squared plus the log of e to the minus something or log of e is actually just the identity right so actually just turns out to be this stuff in the middle all right so minus 1 over 2 Sigma squared W transpose X I minus y squared any questions at this point and so first we the reason we can take the log here is because we're doing the Arg max that's important right the whatever parameters maximize this they also maximize this these two expressions are not the same but they have the same maximum now there's maximum of the same plot at the same place so then we just take the log and we get these two terms now here's the thing we take the maximum over W right and if you look very very very closely you're gonna discover that there's no W here right this is W free so what we can do is we can just exit it doesn't matter it's just a constant it can just if you add a constant to a function it doesn't actually change we are the maximum right so that's just you know a breeze they have a function here to find whether you know where is this maximum is going to add something to this function right then it just looks like yeah the maximum is exactly the same point like if I just move it up and move it down so this term here forget about so then you know because we're fancy you can move the salmon here why not okay so actually we can move it in here and now we're in the second to last thing now we have one over m is 1 over 2 Sigma squared times this expression here and now comes the last thing you can say well actually this 2 Sigma squared actually it's just a constant once again right doesn't make any difference so we don't need it the negative term that means we are maximizing wait a second only news is maximized but if you want to minimize right so let's just modify with minus 1 right so you might have a with - bar now we're minimizing it makes us feel a lot cooler right this constant we don't like 1 over 2 Sigma squared right that's kind of a stupid constant if you just get rid of it and then you get you get the follow now we are almost there all right so this looks pretty good is to square us and everything there's only one one thing you don't like about this last function this is totally correct and you can minimize this and you get the right answer the only problem is if you know you tell someone I have the following lots I have a loss of 7.3 right it's really meaningless why is it meaningless because you're summing over all your data points so it really depends how many data points you have right so what you would like to have is actually what's the loss on average all right so we just put a 1 over N doesn't make any difference but the nice thing is now when you compute the loss actually it tells you exactly how far off are you on average for a particular example alright so now it's really really interpretive all right so if I tell you I predict the house prices right and my average loss is you know whatever 10,000 dollars or something right then I have to you know that basically means how that's how much I'm off on average for a certain house square actually so that's the you could take the square root then you know exactly you know how much you often have it does that make sense any questions all right so the beautiful thing about this function does anyone know what type of function is this it's a parabola right so it's just so nice things parabolas they actually very very well behaved they're convex differentiable continuous they even nicer right because parabola is actually you can compute the minimum precisely right and sometimes you do this sometimes you actually use gradient descent on you so all of you use Newton's method you jump right to the minimum actually if you take a parabola and approximate of the parabola then you get exactly the answer so Newton's makes Newton's method actually just jumps to the minimum with one step and but if you do the reason you often want to do gradient descent is because V data is very high dimensional that actually computing the hash gene or something actually is very expensive so it's actually faster just to do gradient descent okay any questions about this alright then we can move to the next part and so maximum likelihood estimation is only one way of doing it there's another way of doing it and that's map right maximum a-posteriori estimation and what does map to an app you say I want to and find the W that's most likely given my data so I would like to find P of W given all my data which is Phi 1 X 1 Y and X n and the way you find this orbit we just you know when we divide that we just call this the data set D and when we divide this we just able that's actually the same thing as you know P of D given W times P of W divided by some normalized okay so so map just assumes that W is a random variable and basically tries to estimate the dependent variable the probability of this really know the value of the spread of any variable given the data so one thing we need here is the prior so this here is the tire is our prior belief of what W should look like now that's really weird right what do you think your job W speeds the slope of this function right what does it look like well you don't know right so what do you say is well if you don't know you always say well it's probably Gaussian so you say well a star from some Gaussian distribution with zero mean all right it's um you know sum-sum isotopic variant so basically in each direction you have the same variance of I squared but this is something you know really means while I have no clue sometimes you do have a prior right sometimes it happens that for example let's say you try to do a spam filter right W could be the classic you know a spam filter and you want to estimate how spammy Adada is but actually one thing you could do is you could for example train a spam filter for everybody I take pull all the data together and train a spam filter that's not going to be great because that'd be easy just means what is spam what does an email it's defined as spam by everybody right but your personal taste may be different so it doesn't take into account information about you so then actually what you can do is you can say you know like you call this w0 you can actually have a Gaussian around this value got w0 right so that's actually this code transfer learning like I can say well I trained a classifier for you but I have a prior belief that you are not that different from everybody else right and that's basically what the prior encodes in this case we don't know it's a real estate it's got a zero mean okay so why don't we do the same exercise for more time I give you four minutes you now turn it you know the node around you can see the derivation of map and please go through it with your neighbor maybe this time take the other neighbor or you know or the same neighbor I don't really matter all right any questions raise your hand if it's crystal clear that's someone all right so a few people let me go through it so the idea is very simple I say what's the probability of W given my data that's y1 x1 and so on now the first thing I do is I use Bayes rule that's a breeze Bayes rule to flip it around and I say the normalizer just the constant it doesn't matter that's the same thing as saying probability of my data given W times PFW unless he is actually exactly likelihood function thus he is just Arg max this is the product of all my data points P of Y i given X I comma da vu times P of W now why is that does that stat makes sense to everybody raise the end of that makes sense ok so the idea in some sense is basically what I'm doing is I'm just using the chain rule right so just saying well that actually equals I can also write it as P of y1 yn given X 1 X N and W times P of W right and so but here comes the key that each why I only only actually pens on each exile right so the other exes actually don't matter right so there's actually these Y's are actually only you know there is the independent of each other so I can actually write this as a product over the different write so I can write this as a product I equals 1 to n P of Y i given X 1 to X n comma w you have W okay so these are all independent from each other because each one only their iid drama but actually each I I actually really only depends on X on it doesn't actually depend on the other ones but the other data points don't have any influence that's because given that one X I that's all the information I need right so I can just actually remove all of these and just put an X I okay that's that does that make sense for a Z and that makes sense okay awesome so if we do this then we did once again take the log rights because that's what we do best and let me take the log of this and this here is exactly the same expression that we had earlier all right that was exactly what what how to the MLE so we get exactly the same term at the end so what we arrive at is argument 1 over 2 Sigma squared 1 to n W transpose X I minus y I squared and here on the right P of W what was that that was a Gaussian distribution right so P of W equals 1 over square root of 2 pi you know what's the tau squared e to the minus W squared over 2 tau squared right because it's a Gaussian that's actually around zero zero mean so if we take the log what we get is the following we get 1 over 2 tau squared W transpose done but W transpose W is this it so this is just the log of P of W log here up top now this is inside the Sun the Sun bassy sums over all data points and takes this this constant here so we have this constant n times so if you take it out of the Sun you say the Sun is only about the first time let me have an N here all right does that make sense crazy and if you're still with me all right awesome good now I take the whole thing indeed multiplied by 2 divided by n multiplied by Sigma squared and then I get exactly the answer 1 over N W transpose X I - why I squared plus lambda times where lambda equals Sigma squared over top yeah oh yeah yeah that's just that's just so I video of oh yeah sorry that's just the constant right so X X it doesn't depend on W probability of oh yes the question was - P off why'd I just drop P of X 1 X and given W right well actually these X's don't depend on W right so they actually just just a constant this is the probability of X this is probability this you know doesn't it doesn't it's not effective acts by W so that's the same thing you can remove this W right and now it's actually a constant that and the optimization doesn't make any difference in a very good question I have a demo maybe out of time so maybe I start next time of the demo can someone remind me that I have to start with a demo okay thank you see you all on Wednesday 
","['', 'logistic regression', 'maximum likelihood estimation', 'loss function', 'gradient descent', 'convex function', 'differentiable function', 'continuous function', 'Hessian', 'zero-one loss', 'approximation', 'intuition', 'regularizer', 'maximum a posteriori (MAP)', ""Bayes' rule"", 'likelihood function', 'chain rule', 'independent and identically distributed (iid)', 'Gaussian distribution', 'mean squared error', '']"
"welcome everybody okay this is better all right few logistics things there I hand it out so I hand out the handouts already so please make sure you have a handout also probably the next project which is I believe project four is that correct project three is nine phase yeah so now you project three is still going strong I see a lot of submissions it's very very nice project four is you're still testing a few aspects and be talking with Bukharian right now just make sure everything works seamlessly so it may be delayed by a few days so maybe late by until Friday you will get your full two weeks so this is not eating into your time also add the the CEO of voc Ihram just email me he has seen all your complaints and all your suggestions Piazza thanks to those who posted something there they're taking this very very seriously and they are very interested in fixing things so please continue to post things there they actually they found some bugs and their software and hopefully most of these things will be addressed very soon all right any questions about logistics the other thing is the exam is just around the corner by the way all right so we have two more weeks it's less than two weeks so please get started hey please I just want to remind everybody the exam is a huge part of the grade like if the biggest failure mode in this class is to do really well the projects but not do well on the exam so please take it seriously okay so last time we talked about yeah question yeah bikuni try to find one date where everybody can do it it may be a Sunday it's really hard to forget everybody in the same room and yeah please follow the Piazza posts in that regard I guess if you if you can't if you missed the makeup exam then it will end up being an oral exam in my office so that's kind of the way it works alright so last time we talked about the squirrel us so the square loss was basically linear regression but we said you have our data and we make an assumption that the data is somewhat you know lies on the line right like this is basically a line is a good model of that of that data how that the better set has a linear relationship between x and y and then we made a second assumption that well of course the points won't be exactly on that on that line so if they're off we actually assume you have Gaussian noise so they put a Gaussian around raising this this has the probability of being here it's kind of a Gaussian sent around the with the mean which is the position off that line we derived this and turned out actually this last function is a parabola and once we went through all the notions that just turned out to be quite simple the loss function was faces just well let me write it down here the loss function end up just being some of our points W transpose X I minus y squared and if you do that the MLE answer so this is basically the last function you're trying to minimize with gradient descent and the MLE or the map answer is you basically add a regular rise to it yeah lambdas lambda x squared this is if you have Matt okay so one thing is this is a parabola so turns out actually has coast form solution that's a function that a parabola you can just find the minimum in closed form this is essentially actually doing a Hessian update if you just do have used the Newton's method and take one step with Newton's method OHS then actually you get right to the minimum we can quickly derive this I just want to spend three minutes deriving this one question people ask me is you know do people actually like you know if there's a the coast form solution why don't we just always use the closed form solution why don't we actually just gradient descent and the answer is as you will see in a minute the closed form solution requires d squared memory and is actually D cubed in complexity so I'll get to that in a minute but it turns out that you know for large enough dimension that's actually is prohibitive so let me just just derive this real quick what's the closed form solution is let's say you have to find a matrix X where X transpose is the following x1 xn and y transpose is y1 yn right so X is basically a matrix where every row is a data input is exactly the kind of matrices you use in your projects then I can write this last function as a by the way please everybody kills your laptop's thank you and you can write this last function as following there's just x times W minus y squared all right so that's the that's the last function if I just write it in matrix notation crazy handle that makes sense all right awesome so I can write that out and I get XY w minus y transpose X W minus y so that's you know so far nothing interesting here and if I multiply this out you get the following it W transpose X suppose xw at these two terms then I have minus twice y transpose X W plus y transpose one alright so this is just completing the square and that's not my function there's a this is a parabola you see basically this year is a positive definite matrix and if you now want to find the minimum we just take the derivative and set the derivative to zero I take the derivative of this function here with respect to W all right any questions so far so one more time I took this last function I just divided the matrix notation just because that's cool right and then I write everything out and make me a matrix notation this makes it very obvious if there's a closed form solution and now I have this function here this is just a function of W right it's a function it tells me the last with respect to W if I want to find the minimum I take the derivative with respect to W and set that to 0 all right this is something you've been doing your whole life right and so if I do this and the derivative of this function becomes X transpose X W alright this is this quadratic term this is by the way if you're not familiar with taking derivatives like two vectors please look at the matrix cookbook it's something that's very very useful if you're familiar with it if you're not if you're uncomfortable with it you can always go back and take the derivative respect to every single WI all right so that that always works it's just a lot of notation a lot of summation so it's actually it can you know when you do this kind of work it can can save you hours and hours of time if you were just familiar with taking derivatives spective matrices and okay so this this is the term you get here this because we have W in both sides just ends up being x transpose x times W and we get minus two y transpose X and then you know the W there's a linear term so we just get Y transpose X there's the derivative we set this equal zero so if you put an equal zero here that the same thing is put an equal you raise your hand if you still with me all right now to get rid of the tutu's because we don't need them now on both sides and now we have some matrix times W equals something well we just multiply with the inverse of that matrix so we get W equals X transpose X minus one I'm sorry I made a mistake here this is X transpose Y X transpose okay after transpose and that's the solution so there's actually the closed form solution to this last function so if you can compute this then there's nothing else to be done right you just jump right to the minimums you know that's basically if your function is a parabola and you can just you eat jumps right here and then your doc why don't people always do this the answer is this this matrix can be gigantic and inverting it a gigantic matrix can take forever right so there's two problems a you have to store this in memory which you may not be able to do and then also actually inverting inversion is very expensive in practice actually you don't do the inversion the way actually you can also buy this as a linear program and then you can do it a little faster but ultimately the complexity is the same the complexity is he cubed so cubic complexity is always a problem so what that means is basically if you double your data it becomes a times small a times slower and so that's that's what cubing complexity the complexity is detail acute right that's the and this is the space complexity so there's a space there's computation crazy and that makes sense to you okay that's something basically that's gonna you know Elementary you know computer science the idea is basically whenever you have an algorithm you try to argue and how much slower does the algorithm Gatz how much more space do you need as you increase your input size and here this is a function of the number of dimensions that you have right so for example in Stanford during my economical example there you actually have millions of dimensions potentially because these are the dimensions are the number of words in the English language so if you have millions of dimensions and this year becomes prohibitively expensive so what you do is just to grade it sand and that actually works really well any more questions about this quest yet oh it's just because when you take the derivative you have to actually take the a times W if you take derivatives back to W it becomes a transpose so that's the matrix cookbook yes yeah okay any more questions all right today we will do something much cooler even cooler than linear regression okay so then we'll talk about support vector machines all right so one more time we the last like to be talking about regression let's go take a step back once more time hey all the time and talk about classification so our labels by eye at this time either plus 1 or minus 1 and just as before we make the assumption that the data is linearly separable so we have theta you know if you have some crosses and positive points with some negative points and you would like to find a hyperplane between these two now one thing you already knows like if you you know if he knows that your hyperplane exists we have an algorithm to find such a hyperplane who remembers the name of the algorithm perceptron right so the perceptron finds us a hyperplane but you have no guarantee what hyperplane it finds right and turns out actually there's infinitely many right if they exist one there's infinitely many so you could except this one or this is a hyperplane or this is a hyperplane etc and so people have been wandering ever since it was possible to find a hyperplane the obvious question is well which one should I find right that's where the perceptron is weak why do you have no guarantees on what you find what the SVM does it actually finds you one a very specific one and I would argue that's the one you do want to find and so if you remember correctly in the in the SVA the perceptron proof what we talked about is yet this notion of margin lazy said you know the how many steps you need to find this hyperplane is inverse proportional to the margin so what you want is the data to be far away from each other it has to remind you the margin for hyperplane is the distance to the closest point all right that's that's the margin I call it gamma and the intuition behind SVM says that if I want to find any hyperplane the best hyperplane is actually the one that has maximum margin and that's the idea so originally actually they were phrases margin maximum margin classification and that's not just a good intuition you can actually you know theoretically derive that this actually is you know likely to generalize very well so the idea is if I have a hyperplane that's kind of really far away from the positive points and really far away from the negative points then if my positive points are negative points drawn from the same distribution they won't be exactly the same points like my test points that I'm actually that I actually care about and I actually deploy this thing I'm gonna get see slightly different points right let's see maybe this point at this point right hopefully people you know there will be someone in that region worthy with the axis for my training X's are and if I move my hyperplane as far as possible away from these points I give a lot of margin of error right so they basically if they're here and that's not a big deal right because I actually have this large margin in which they could fall into if instead I drawn the hyperplane like this which is perfectly fine it separates the training data very well but now if a test point is here it will actually be misclassified right and dutifully that seems like the wrong thing to do right because that point is still very close to these guys does that make sense so the idea is if we find the maximizing hyperplane and and it's actually it is defined as the hyperplane that has the maximum distance to its closest point and that's always exactly the same on both sides can anyone tell me why that is the case why do you have the same margin on both sides one person already knows it yeah sure that's right if one side was closer than the other then it's a here's a little larger than what I could do the margin is actually the distance to the closest point so let's say my hyperplane is this and this guy is closer than that guy but what I then can do is actually I can my my the margins defined is the shortest distance to a data point then I could find a better hyperplane that's moved a little bit over here all right that increases this much so by definition the distance to the closest point and in the positive class and the distance of the closest point in the negative class must be exactly the same okay any questions at this point all right so an SVM SR where invented I think in 1994 by Karina Cortes and Isabella Guillaume and vladimir vapnik at 80 82 to this day they are the most popular machine learning algorithm and are extremely successful this it's also one of my favorite algorithms there's one little caveat they actually it's actually patented so you're not allowed to use it but besides that it's great or but we put another way if you use it and you know you just can't admit so there's actually a patent troll company that owns the patent and if you have ever you know we're to admit that you use SVM's than any product that makes money they will sue you quicker than you can say support vector machine and so it actually Oracle for example they foolishly agree you know admitted at some point they advertised our databases now have support vector machines and bam I think it's you the next day but you can always say we're just using a learning a linear classifier that has a maximum gap between the hyperplane and the closest point right so if you avoid the language a little bit then it's very hard to prove that you're actually using it okay good so let me just formalize this notion of margin we've talked about this a little bit already but let's just make this crystal clear so we have our hyperplane our hyperplane is the following set is the data points except of W transpose X plus P equals 0 all right so this is my hyperplane index by W and P so far that should be that's let's place the set of all points that lie right here in this middle and what I want you to think about a little bit is what is the distance to a point from a hyperplane and this was you know you may recognize this it was actually on the placement exam for exactly that reason because I wanted you to kind of get a little bit of a head start just thinking about the geometry so that say this years my hyper plane defined by the vector W and now I have some point X and I would like to know what's the distance from this point X to the hyperplane and well the first thing I can do is it say well and I can project this point X onto the hyperplane that's basically this point here has called this X P and then the distance from the hyperplane to this point is exactly the distance between these two points and we call this this vector here T any questions so far raise your hand if you with me all right good so one more time I have this hyperplane W this hyper plane defined by W I have some point X I would like to know how far away is that point from the hyperplane what I do is I project the point onto the hyperplane that's the basis the closest point on the hyperplane to X and now I measure the distance in these two points X P and X I call the vector between these two deep so then I can say x equals sorry X P equals X minus D okay so if I take X as a track D from it and I get exactly my point XP all right any questions all right so then what do we know about X P we know that X P lies on the hyperplane what do we know about points on the hyperplane by definition these are the points where W transpose X plus B equals 0 so what we know is that W transpose X P plus B equals 0 all right that's just the very very definition of points in the hyperplane we know X P lies on the hyperplane so far no big problem now we can plug into our definition of X P in here so what we get is w transpose X minus D plus B equals 0 and now comes the question what is this B and here's one interesting realization that D is actually ORS I didn't draw it that way must be parallel to W so because it actually points away from the hyperplane what points exactly your thumb to the hyperplane is this vector W right so actually D is basically a rescaled version of W it must be all right because it's parallel to it so I can say instead of D I can write d equals alpha times W raise your hand of you with me awesome so just plug this in here and what I get is w transpose X minus alpha times W plus B a zero and that's all I need because here B is given W is given X is given so the only thing I don't know is alpha so you can solve for alpha all right and if I do this what I get I get I solve this follows that alpha equals W transpose X plus B over W transpose W so what I'm doing here well I know maybe I give you a minute to digest this and kind of verify this any questions raise your hand if you want me to move on awesome okay good so I hope was that if you've all done it for the placement exam then we can go through this quickly okay great so now we know exactly what alpha is that means we know exactly what D is I so D is alpha times W so D equals this term here I offered em - w alpha is w transpose X plus P over W transpose W times that let's play see that's a scalar that's my alpha and that's my dog alright so now what I want to know is what is the length of D night and that's basically well that's a computer's what's the norm of T so the norm of the equals D transpose D and the square root thereof and that equals okay so I guess I have yes ice let me write down alpha square W transpose W because these alpha times W lizards are squared and I people nodding maybe just not I don't have to raise your hand all the time because I'm bigger is nothing okay thank you take the ALF out alpha squared you can pull that out and then you get square root of W transpose W and now what is this well what's alpha one more time that's this term up here that's my alpha sits W transpose X plus P over W transpose W that's my alpha times the square root of W transpose W okay one more time I take my D I have a formulation for D I want to know its norm norm was just the square root of a square vector I spot a square now I plug in the definition of Debaters alpha times W squared there's our alpha del W I square the whole thing I get alpha square times W squared I pulled the Alfa out I get alpha times the square root of W transpose W and now alpha is this term here if I multiply that with the square root of W transpose W I can just divide by it and thus this term disappears and that's ultimately what I get so it is W transpose X plus B over the norm of W which is what this here's the norm of W so this here is the distance of a point X to the hyperplane and if you remember this from geometry when you with the high school that's exactly the formula you learned any questions ready to move on crashing already move on okay person yeah it works in all the spaces we care about yeah it doesn't have to be oh sorry I mean that's just the it has to be in this is Euclidean space it's so it doesn't have to be two-dimensional or something but this can be n dimensional or D dimensional that's totally fine and it also works in actually in general Banach spaces etc and this is this will become relevant in a few lectures good question though yes any other questions okay awesome so now this is the distance to a data point what we want to know them the smallest distance to a data point and so we can just define the margin of a hyperplane you find but W and B is the smallest such distance so the smallest W times X plus B over the normal answer that space you go over all the data points in the data set and you compute the distance to the hyperplane for every single one of them and you say what is the smallest one and which one has the smallest distance okay maybe take a second and digest everything all right any questions all right so what last VM does it says well a second right so we can define the you know this is the formula of the margin of a classifier this gamma here and what we would like to know is the hyperplane that has the largest has the largest margin right and this is this is what the margin is so why don't we do exactly that why don't we just say you want to find the W you want to find the W which maximizes W comma B gamma W this place it says you wonder find W that maximizes their margin so if you just leave it as this what do we get all right so maybe I'm give you a minute to think about so are we done at this point if we just say this is my optimization problem I just want to find any hyperplane WB which has a maximum margin so I'll give you a minute to think about it with your neighbor why is this gonna blow up in my face all right who knows that this is my dataset I'm trying to find a hyperplane that maximizes the margin where should I draw the hyperplane why does an optimization going to give me yeah you could draw it here that's right and so if you want to maximize this since two data points just gonna go push it away to infinity all right that's a great hyperplane really large margin to both classes but that's not what we want you want to have a brain that actually lies in between the two classes and it's actually separated so we have to somehow force this this maximization to also do something sensible and we do this with a constraint this goes into constraint optimization problem so a constrained optimization you may say well minimize this function and as I maximize this function subject to a certain constraints the swallowing has to be satisfied and the constraint is that for all our data points I you have to have that why I W transpose X I plus B must be greater equals 0 and so we had this before and when we talked about the perceptron this here we see says which which sign of the hyperplane you lie on right if you're positive and in one side you're positive on the other side that's negative but if you multiply with the label the positive points if the positive points are the positive side and that's positive and if the negative points in the negative side that's also positive so if this constraint is satisfied for oh I then you must lie between them and then actually we get the right justify any questions yeah are you looking ahead I'm not there yet right good point so we'll get there very very soon all right so we want to solve this right now we don't yet know how to solve this all right and so this is actually where people get to in a second so you want we know we want to maximize this function subject to this constraint and now just pay attention for five more minutes right let me this is only three steps so you're gonna learn something really really cool being able to solve these things makes great conversation pieces at first dates cocktail parties all right so now you plug in the definition of gamma which is basically just saying you want to find you know here you want to find you know the minimum X element of T W transpose X plus B over W XW like this is the absolute value okay so now we have something really nasty right oh god I'm gonna put on some glass so it's gonna protect our eyes it's a maximization of a minimization that's nasty so let's not be scared just keep going what can we do alright the first thing we can do is we can pull this W transpose W out of the internal minimization and if a maximization of a minimization but double transposed over here is not actually a function of X so we can pull that out so you pull that out sure why not W transpose W so now we just get this term here so minimize W transpose X plus P over X and then we multiply the whole thing minute maximize that over W that's still pretty scary right but now comes the cool trick and the cool trick is that if you have a hyperplane right and does he have plane is defined up here but this is my W right may set us all the points such that W transpose X plus B equals 0 what I can do is I can multiply my W and B by any constant right it doesn't change a thing okay does that make sense so if I take this equation here W transpose X plus B equals 0 if I rescale W and B by any positive value it makes no difference but it's gonna be exactly the same set so what I'm doing is essentially just rescaling this vector here w right you can have infinitely many scales here it's always exactly the same hyperplane so that's a degree of freedom that we can utilize it currently there's no unique solution so we don't care which scale we get and it doesn't really matter we just want to find a separating hyperplane so we can just fix a scale all right raise your hand if that makes sense all right awesome good so let's just fix a scale that's really convenient to us and here's the scale you're gonna fix we're gonna say okay well we want a very specific scale such that the minimum W transpose X plus P actually that's just this well here minimum equals 1 so you'd recent say we just rescale it exactly the way that this here is one it's good trick all right so now this whole thing is just what and so now we're left with just you know the maximization with 1 over W transpose W right and we just had normal constraints to talk about this so so the objective function is now much much simpler okay any questions so we still have to DVF you know we have committed some sins here we still have to deal with this this constraint in the future but but for now just just you know easily pushed it down as a constraint yeah why because there's always I guess if I give you some X and B right I can always rescale this in any way such as the minimum what the margin is exactly want I think whatever your margin is I can just divide my W and B by exactly that value so this term is 1 all right it gives me the same hyperplane doesn't give me the same solution to the problem but I don't care about actually the the the actual value of this problem I care about the hyperplane right and for my means of purposes that's that's identical does that make sense all right good so now we can you get rid of all this scary stuff now we have why not what W transpose W right now here's a very important lesson that you learned last time right maximization is for losers I have you want to minimize that's put a minimization yeah alright if we just do you know in Reno just say instead of 1 over W transpose W just max a mini minus W transpose W this is the same thing there's no difference right you can either maximize X or minimize 1 over X the same thing okay good raise your hand if you're still with me all right ok good so now we almost there now we just have these two constraints and so now I'm a claim something and I want you to verify it that instead of these two constraints you can actually write something else you can write instead of these two constraints we can actually write that's the case if and only if for all I why I W transpose X plus B this greater equal one so I claim that this set of constraints is identical to these two constraints in particular these two constraints if they are satisfied implies this and if these are satisfied and implies these two and I give you a couple minutes please talk with your neighbor about it and see if you can prove it that these that that's actually exactly the same thing all right who can go one direction or the other left to right or right to left who is brave nobody all right Arthur okay good that's right so this is from going this direction to that direction right he's saying everything is is great equals zero you have to be a little careful actually everything here basically says has the right sign it's on the right side so what I can do is I can actually rewrite this thing here as Y I times this right instead of the absolute value because that's the same thing we said based in my Jeep I'm with the same sign that's here otherwise this would be negative and then I actually know that the minimum here is one right therefore they must all be greater equal one that's another way of saying it okay raise your hand if that makes sense okay good any questions there's some questions yeah so in this case actually you're just going from left to right you're just saying the fact that everything is the minimum is 1 must mean that everything is greater so that's just going from left to right I think what you are concerned about is the other way right is saying going from right to left it could be that the minimum is actually 5 right which is so great a ego wants everything here's satisfied but be a set B of violating this guy here right that's your question ok good so left to right everybody's okay left to right tick ok now right left so assume this year is satisfied for every single data point now he's raising a very very good point right he's saying wait a second assume for everything data point this expression here is greater equal 1 well it could be 5 right Oh could be hundreds all right smallest one copy one hundred so then the C is definitely satisfied everything is you know greater than zero weight equals zero so you're not worried about this constraint but this here seems fishy right he'll be saying they're explicitly the smallest margin has to be one and he was just saying has to be greater equal one so it could be larger than one why is that not an issue we'll figure it out other than Arthur yeah you could and why is it going to be scared downscaled why is it gonna be exactly one yeah that's right because we are minimizing W right so if you had something that's larger than one let's say you have 10 right for the smallest one and every single one of them is larger weight equal ten then what you could do is you could actually reduce your W further right you could just actually divide W and B both by ten your constraints are still all satisfied but you're getting a smaller objective value right and therefore it can't be the optimal solution right so in the optimum this implies that right does that make sense that argument makes sense so the key is because we are minimizing the square of W we are fine any more questions all right so the good news is this problem on the right actually is a linear quadratic function up here right before it disappears this is a parabola and this is a linear constraint that's a quadratic program the quadratic programs is something that our friends in the math department have been studying for 50 years right so they've written all these these you know packages and obscure languages that nobody uses you know to minimize these functions and we can use them or write our own all right demo yeah you called my bluff I made up that demo but no no I do actually have one all right all right thanks thanks for keeping me honest um well it let me first do the demo for this class and then you can still do the other one um why is the banana I don't know okay um so here we go so we have a data point data set so we can now create a data set let me just make some linearly separable data's oh shoot what's going on oh that's good all right and I hear my negative hear my positive points are the other rounds now can I now run my SVM solver and what it will give me is exactly the hyperplane with maximum margin right so you busy see here's the hyperplane and the white lines are basically exactly the parallels that goes through the closest point and you can see they're exactly parallel their accurate distance from each other all right and so the beautiful thing is about you know you just run this QP solver this quadratic programming solver and bam right you get the answer right there's no iterative process I mean ultimately inside that package you have iterative process but it's just such a beautiful formulation of machine learning problem so we've been this came out and people I people hadn't thought of machine learning that everyone thought of like some algorithms that iterate over something right and then basic your inner core test and Isabella Guillaume came along and they just said well you just write this as an optimization problem and attack it gives you the answer and that put the entire feed into a frenzy like for ten years nobody that anything else but quadratic programming I know you know a convex programming I can show you some you know I don't know anyone want to see any other examples I mean the I guess we can do something but they have really close together right that's something but the perceptron will take a really long time so here we have you know very very small margin so the perception would take a long time to converge right but the SVM bear might immediately find it right and so this is what makes SVM so so so popular it turns out there like when this came out suddenly they blew everything else away you get much much lower error most data set right and you could actually run it like it's such an elegant framework right everybody understood it any questions about the demo yeah no no you automatically minimizing both right because you're minimizing the meaning the constraint the basis the closest points the one away but by doing that you have to have equal distance from both classes otherwise you could always move closer to the one that's further away and increase your distance to the closer one all right so the other demo I guess I push that away to next lecture please remind me at the beginning 
","['', 'square loss', 'linear regression', 'Gaussian noise', 'MLE', 'closed form solution', 'gradient descent', 'Hessian update', 'cost function', 'matrix notation', 'derivative', 'minimum', 'constrained optimization problem', 'maximization', 'margin', 'hyperplane', 'constraint', 'quadratic program', 'SVM solver', 'linearly separable data', '']"
"all right welcome welcome everybody please close your laptops thank you okay so we talked about support vector machines and the idea behind super vector machines on a high level it's quite straightforward it's the mic on in the back can you hear me Mikey so if you have once again two classes positive or negative points and we started out with the questions that we'll be having algorithm to find as a hyperplane between those that's the perceptron but the perception just gives us a hyperplane and it won't be nice if you could actually find a particular hyperplane ideally one that's well suited for classification right so and in some sense it comes with the realization that some hyperplanes may be better than others it could have a really bad hyperplane for example this one here right would actually classify this correctly but if any test point you know we're here it would actually classify that as a cross which seems intuitively the wrong thing to do and so the idea behind support vector machines is that a hyperplane is good if it maximizes the margin so or the best hyperplane is the one that actually maximizes the margin where margin is defined as the distance to the closest point now you know of course it has to be a plane that separates the two data set so it has to be between these two and then actually turns out that the one that maximizes the margin is always has equidistant to the closest point from one class and to the closest point of the other class there must be the case otherwise you could move it closer and that doesn't really seem like it's the middle but yeah so that's the idea so the question was how do we find this hyperplane that has maximum margin maximum distance to both sides and what we did is we define the definition of margin and then we just said it's maximize this subject to the constraint that every point is on the right side of this hyperplane so if this is w easier must be on the part of the positive side these must be on the negative side so this wasn't up to a constrained optimization problem and that was kind of ugly so we had to do a little bit of massaging and ultimately we ended up with the following optimization problem which is equivalent in the final solution w chance we minimize the square of W subject to the constraint this means such that and for all I why I W transpose X I plus B is greater equal 1 and that is the SVM optimization problem so the constraint here says every single data point has to be at least a margin of 1 away from the - he must be on the right side that's because that's positive it must be at least one away and what W does spheres try to make w as small as possible so you have to be careful one here is actually really just one unit right it doesn't mean anything because he can divide W and B by a constant all right so by actually minimizing W what you get is basically the smallest w that means such that these are actually this is actually the case so it ultimately what it does you know it basically tries to find a hyperplane that pushes them away as far as as much as possible everywhere so and then we had this little exercise where you basically looked at the two different formulations and we could show that they are the same in the optimal right so before and we actually had a bunch of constraints what we showed that actually you know at the optimal solution that's equivalent to these constraints here so we can't just enforce these constraints here just do ok yes in some sense the way this is like last place if you have a set of all possible solutions that satisfy the constraints it's here we have the optimal and what we showed is there's another set of solutions has the same optimal so that's basically the smaller set is these set of constraints and if you remember what we did last time those are the two basic different formulations okay anyway so this is the SVM optimization problem and some people ask me afterwards like wait a second what if such a hyperplane doesn't exist all right so what if for example but you know there you know for example we had one circle here or something like this right the perceptron would loop forever what would happen this case actually if you optimize this if you stick this in a quadratic programming solver what you would get is actually your output infeasible and there's no feasible set there's no single point that actually satisfies the solution so a natural question is well in this case right is it really the right thing to say well I'm throw my hands up and say I give up or should I maybe still output something that's reasonable that separates most of the points and that's of course something people thought about and actually this was published very soon after the original SVM formulation came the formulation with slack variables and so the idea is basically instead of following you say imagine I can't possibly solve this problem write this for example here we have some points like you know here there's no no hyperplane that satisfies this point from each other what do I want in that case and the formulation that Karina Cortez came up with was she said well in this case I still want these to be satisfied as much as possible but I allow for some violation of these constraints and how do you do this you bassy say okay what I'm doing is I add a little bit of slack here I call the sky and these I must OB with a equals zero alright so this guy here basically says this is something that I'm subtracting from one alright and then I can always make this the satisfied right like whatever this value on the left is I can always subtract enough on the right such that this constraint is satisfied does that make sense raise the end of that makes sense okay so one more time you have this constraint I can't satisfy it but see on the left hand side it's just not greater equal one right so what I do is I say well I just subtract something from the right until it's satisfied and what I want to do is I want to subtract as little as possible so what I'm saying addition to minimizing W squared I'm also minimizing the amount that I'm that I'm adding and that's the formulation so then it's called SVM with soft constraints so it basically means you should satisfy with its constraints but if not well then you just have to pay for it right so you basically have to you know they have assigned some nonzero value to X I that costs you here in the objectives so you will get a lower objective value right so whenever it's possible the optimization will try to find a hyperplane where everything but all these sides are 0 but if it's not possible you know for example in this case it says well this guy for example just has some non negative sign there's nothing I can do about it sorry nonzero sign there's nothing I could do about it any questions yeah oh why would you ever use anime down a good question so well because these are in some sense totally different units right and so sometimes you may really really care so then let me show you the following example right so if I allow if allow there's a man of slack but here's something that could happen right imagine I have some points doesn't look quite as drastic like this but it looks like you know something like a bizarre zero a circle here right what the optimization problem may say is well there's only just one of many many many points that they have many many many points right and I can actually get a much much larger margin overall if I place the hyperplane here and just sacrifice this one sucker right and so if you allow this or not depends on how large you see it so if you crank up you see a lot let's say you make it like you know get it close to you know infinity then it would never do this right because this is one point we know the cost will be so large it wouldn't be worth it right but if you relax this then actually you know eventually basically again in the extreme case if this is zero then you're actually saying what doesn't matter at all right and then it just you know sets all the excise just a negative it's the value here so that actually you know it doesn't care at all about getting points anymore and if you make it smaller than it would say well you know they're to sacrifice some points as if we get the maximum point and that is important if you suspect that your data may have mislabeled data points and so forth so if you think your exam of my data points are pretty noisy right and I'm not even sure about it in biology often that's the case when in Applied Sciences when you say well you know say I classify certain you know whatever protein into something and you know it's just not an exact science only eighty percent of my Labour's are correct then you would actually put it aside a small C here because you simply say well I have to allow some slack but what I really want is the general trend does that make sense good question good question any other questions yeah yes a good rule of how to set C and the answer yes and no so so C is what we call a hyper parameter it's basically it's not a parameter that the machine learning algorithm learns it's a parameter that we are setting as the user and that's kind of your intuition about the data get put in here often what people do in practice is they take a training they're set to take a holdout set to a validation set they train on mode do multiple values of C on the training set and they look at the error on the validation set I'm gonna pick the value of C that has the lowest error on the validation set and then they retrain on the union of the two sets and typically that works in two steps the first thing is actually you try to get the order of magnitude right so what you do is you try like 10 to the minus 4 10 to the minus 3 you know all the way to maybe a hundred and you look at which one is best then you explore around that order of magnitude let's say 0.1 is good then you maybe try point two point three point four point five and you know maybe 0.09 and so on right so that's that's called a telescopic search so you first look at you know just that the order of magnitude and then you zoom in and look again in that order thank you very good question yeah any other questions all right okay so now we have this optimization problem with these quantities size that's still a linear problem we still only have linear constraints and Listia still remains a linear problem so we can just stick that in in a quadratic programming solver we can actually also do something else and what we can do is you can basically say well given this but I know that the excise would always be as small as as possible right because I'm I'm penalizing them so um I can actually solve this for my folks are right so and what I can do is they can basis rewrite the loss function as the following well let me actually first divide them so basically what I can say is well in some sense I know works sides right sucks I equals is either is either 0 if Y i w transpose x i + b it's great equal 1 but if the constraint is naturally satisfied the optimization problem will always put a 0 on side because it has to pay for it and otherwise it's less than 1 then we can also solve for it because then we can just say well in this case sy equals 1 minus y I W transpose X I that's P okay any questions I'm so pleased he was saying well what is the optimization problem do with this RI well if this constraint is naturally satisfied able to set it to 0 so if the constraint is satisfied it set it to 0 if it's not satisfied then actually basically has to make up for the March right basic says this point is not one away so what it has to do is has to make up for this margin that's that's what excites alright so that's people tries assigned to and that's except we can just solve for that directly right sucks I must be the following man it would never set it any larger because then it would pay for you know pay extra raise e handle that makes sense ok good so if we know this then we can actually just stick the excise wait damn it never notice into the objective so what is our objective our objective is minimize over WB and then we have W transpose W plus C times the size oh sorry C times the sum overall size but we know exactly books is and that is it's the max of 1 minus y I W transpose X I plus B comma 0 we are not you're not anymore he's given up on that in the so that's correct that's correct but we have you're paying for it right so in the optimum and the objective function on the very top right you actually have you're summing over all the values of X I and you trying to minimize that function so whenever possible of it try to make size as small as possible right so what you're describing would exactly happen when you have the scenario but as you know over here we price one cycle is one circle it's up you know with the crosses and the base just sacrifices that one right so if you have an outlier that's mislabeled for example it's good question okay so let me just make this clear so this I I is 1 minus y either my either this expression on the or 0 and if you think about it that turns out to be exactly the max of 1 minus y I this here's the prediction with 0 and do you want to spend maybe give you one one-minute Union neighbor please discuss why this here is correct [Music] [Music] all right any questions larger than 0 but less than like if the negative side chunk is less than yes that's right that's right so what he's saying is what if what if this term here is it's what are this term here is not great equal one so it's not zero but it's good equals zero all right so then in some sense actually this here means we're classifying correctly this is this greater than zero that means we have classifying the point correctly but if it's not greater than one then you're still paying for it and so what setting is that that's the setting right where you basically have these are my points this is a hyperplane here's my margin and I have one outlier that's here and it's basically too close it should be outside the margin of one but it's not so I'm paying for that one that's basically what it is okay any questions about this is this clear raise your hand that's crystal clear okay good all right so this is actually this is very very nice now because what does that mean this here is actually just a loss amount and this here's what we call a regular and if we just have a last function of our W became just do gradient descent alright so we don't actually have to go to the math department and get the QP solver we can just actually you look at this this optimization problem bizza it's almost inferential a everywhere turns out if you square it then it becomes differentiable so you can do that some people do this and now we can just use great you know Delia a technically correct a sub gradient descent and just optimize this and it's just like logistic regression any questions about this alright um unfortunately I don't have my laptop you today I have some some technical difficulties but the I can show you you can just look at the printer the figure on the printout [Music] all right so this is also on your seats what I'm showing you here and I hope you can see this this here is a data set of you know binary data set of crosses and and circles and the first and one thing you notice like this data set is kind of especially designs actually basically the cost is all here the circuits are here in one little circle is kind of this outlier right lies here and so if we so this is linearly separable and if we set our C to be large we set C to 100 then what it does it really penalizes any slack so we'll try to find a nonce it gives a solution with zero slack and that's exactly what it finds so what you see is here this black line is the hyperplane and the white line is basically exactly the margin of one okay and it draws the margin baseed scales that have to plane exactly in positions that exactly such that the closest crosses here lie on the mount a on the high pair on the margin of 1 and the closest circle eyes on the margin of 1 and now one thing you can do is you can actually solve this again and reduce your C right so if you're basing now maybe you use starting to suspect that your data set maybe is a little noisy right and maybe that point actually should be a cross right who knows but it seems little suspicious that at one point and so if you now lower your value for C what happens is you can see the numbers we listen to 10 that's still basically the same thing you can see the white line is actually a little bit outside of it so this circle is now inside the margin if I lower it to 1 what does it do now it's allowed to give a sign a little bit of slack right and basically and I'm giving it in some sense of slack budget that's how you can see this and what it does it assigns the slag to this point here and to these points here right so it moves the hyperplane more in the middle and you see this guy here is still classified correctly but it's really really close to the hyperplane right so that encouraged some cost but it's willing to take it because it doesn't have pay that much for it anymore alright see it's basically how much about how much we multiply the slack variables does it make sense for easy handedness makes sense to you okay awesome now we can go even further we can say okay well now you know you know we're going you know there's a sale right it's like you know snag variables become really really cheap so it can allow a lot of slack and what's happening is see see here it actually puts this hyperplane smack in the middle right so they've an average you have very little slack but you know it doesn't care about these three points and these couple points yes okay so what it does it actually is more concerned about having a large margin right and less about getting every single point correctly and then if you do this even further at this point actually it assigns a really large margin you see W is tiny now this here's my little W right so the margin is very very large and in fact every single point lies inside the margin right so in some sense the point of the margin you know it's no longer you know than something the point of the margin is that it separates to two classes so in this case I would say the slack variables you know it's too cheap right it kind of over over does it a little bit all right that's basically the effect so as a practitioner you have to find you us data scientist your job is to find the right see any questions about this this visualization yeah so that the white lines is exactly the line of every boy that lies there has exactly a margin of ones there's AB is a distance of one from the from the hyperplane all right so W why W transpose X plus B is one in that case so and that's what we want right so that's basically we want all the points to be outside of here right so so in some sense the idea really is that you want this solution here but you're allowing it to basically mess it up a little bit if that's too strict so in the you know probably you know if you were actually concerned about this data said this year I would guess gives you the best test error right because probably this year is an outlier this one circle and probably most of your circles lie here most of your crosses lie here so we're having that the line here in the middle is probably you know probably going to generalize the best right the danger here is with this solution is that it's actually quite likely that another cross could end up somewhere here and you will miss classify those right during test time that's the idea any more questions all right okay so then we're moving this year in something is the perfect segue so if you look at this this is exactly the setting that we looked at when we talked about gradient descent everybody said what if you just have a function this here's my function that I'm minimizing this is a function with respect to W right and I'm trying to find its minimum while we use gradient descent or second-order method etc and it may be surprising that this does SVM that we started out with something very very you know with an intuition we want to find the maximum margin hyperplane then we you know derived this quadratic program formulation and it turns out we can even write that as in loss function right so in exactly this form that we can use the gradient descent similar to logistic regression and as it turns out a lot of machine learning can be written exactly that format so very very often we can just write it ultimately what it boils down to is even if you come up with a probabilistic intuition or with some you know for example here's like is the geometric intuition you end up with some function that you're trying to minimize and typically this function has two terms one that we call the loss and the other one that we call the regularizer and the properties of these two are that the last function typically you know involves X Y and W and what the last function measures is how many mistakes you're making so this function here is large if you make many mistakes on your misclassifications on your training data whereas the regularizer does not involve the data typically and does not involve the labels just involves the parameters you're fitting just involves W and what the regularizer tries to do is tries to penalize overly complicated answers right in an SVM that translate in exactly into finding the maximum margin hyperplane but if you take a step back actually turns out that this is exactly what you discover many machine learning algorithms and the idea is based the following you have a between do I want to get every single point right on my training data or do I want to get a simple solution right and there's a principle called the Occam's razor principle and there's actually a specific term in learning theory but ultimately if I just you know paraphrase it the idea is you know what would typically the simplest answers tend to be tend to generalize the best right so you can't come up with really really complicated answers that work on your training data but often then actually you're you know you're doing something that doesn't work anymore in the general case so what we would like to do is try to find a an answer that explains our training data and is as simple as possible and in this case simplicity is measured by the norm the squared norm of our W vector turns out that translates into having a large margin if you think of it from the SVM perspective but you can also just think of it in terms of just plain you know smaller armies is a simpler answer mmm okay and any questions about this so this means that what we typically do with a baby typically write this as we try to minimize some w w our parameters minimize some loss function of W plus some regularizer of W and this year some lambda and actually let me just write this like this yeah the last function also goes over my data point onto n last function over and how did I write our H W of X comma Y I so what's HW e HW is the pricked predictor given my w I have the following classifier for every single X and my loss function basically measures how close am I to the true label all right now sum over all my data points off we have a 1 over n here and here you have the regular riser of W regular wise that measures the complexity of your of your solutions and like did you find a really really complex solution or did you find it solution and usually this is large if you find a very complex solution lambda trades off between the two this here here we have a sea of one thing we can do is we can just write lambda equals one over C and we get a lambda here and one here alright men's exactly this format this here is my R of W and this here's my last function L okay and we saw exactly the same thing we did logistic regression where we also actually had the last function which was basically this log loss and the regularizer emitted mammoth map estimation then we actually got once again W transpose W is the regularizer any questions all right a lot of theory of machine learning you know it's build up on top of this framework which we call empirical risk minimization so the idea is well really what I would like to minimize is this loss over my you know over my test data I can't do this so what I do is that minimizes loss over my training data I will be overfitting so I'm adding this regular visor to favor simple solutions and we will see many many examples as the course continues now of exactly this framework so one thing I would like to do today is give you a little overview of a few last functions that are quite popular and machine learning that fall exactly into this paradigm and then maybe next lecture we will move on to the regular risers okay so okay this here's our framework we based you want to minimize a loss and be all that regularizer for now we just care about loss and the first loss that's quite popular is actually one that we just looked at and that's the hinge loss we call this hinge loss and the hinge loss look is basically just max and that's exactly what the SVM does 1 minus HW of x times y I comma 0 ^ P so this is exactly the last function that we had earlier and here where H W of X was thus W transpose X plus B that's my H of X so I'm writing explicitly in writing this as H of W because it doesn't have to be a linear classifier actually so later on this class you will have nonlinear classifiers with the decision function is more complicated but we can actually use exactly the same framework all right so I'm just doing make me a little bit more general in this case so if V of P goes one that's called the SVM if P equals two and that's the squared loss SVM and the reason people sometimes use the square P equals two is because then it's differentiable so some solvers only work with differentiable losses and those actually turns out actually you can actually find a closed form solution in that case actually because it's parabola okay now the loss that we've already seen is the log loss and that as follows the loss is log of 1 plus e to the minus y a ya why i HW x i and this here is the last that we saw and artistic aggression and so the nice thing with is the connection once you understand these last functions and the properties and the regularize and so on you can really mass mix and match and kind of for a particular problem you know assemble your the last for the the machine learning algorithm that's best suited for you so the nice thing about the law class is that it's pretty you know behaves pretty well in terms of outliers actually similar to the ballistic air to the hinge loss but the one thing that that's very sweet here is that actually you get these well calibrated probabilities so if you actually use say P of Y given X equals 1 over 1 plus e to the minus y HW of X then these are actually well calibrated probabilities which is very nice there's a confidence estimate of how how sure are you that a certain class is classified as one or minus one all right let me do you show you two more maybe I draw here so what is the exponential us exponential o is just e to the minus HW of x times y this is exactly what's inside the log here so there's the exponential loss we will visit this this is very it's a big part of the algorithm called adaboost which is on the most popular machine learning algorithms and it has some very nice properties so the exponential loss if it's kind of a little bit of you know like if you make any any tiniest mistakes right any point gets misclassified this is e to the you're raising this to the exponentiating this so this is kind of the loss that loses its nerves right like if anything goes wrong a little bit it just freaks out at you right and it will it will move the entire decision boundary right just to get one little outlier right right because the loss of getting one point wrong is so tremendously large right if you get if this year is positive so the you know these two don't match ensign this just blows up so ridiculously quickly right that it makes sure that no single point gets misclassified if you don't have noisy data then that's a good thing to do and can in terms of adaboost you will see it will give us very good guarantees that for example the algorithm converges very quickly and oh you know actually gives you zero training error very quickly the downside is if you have noisy data it just you know it blows up in your face typically the final loss function is one that we've already seen that's zero one loss and as you all know this one is Delta of H W of X I does not equal Y I that's just a 1 or 0 so the Delta function is just 1 or 0 that's the function that space just defines the training error in practice you cannot approximate it you cannot minimize it but it's still important because that's typically how we measure the training error ok what I want you to do is you have a little field below you I hope on your notes and I want you and your neighbor spend a couple minutes and try to sketch each one of these functions just to get some intuitions what they look like and I would like you to sketch it in terms of Z which I think on the sheet I called x equals HW of X I times y so if you write so what's the last in terms of Z so if she becomes largest he becomes small of Z is 0 how would you draw each one of these last function so please put in the hinge loss logistic loss exponential loss and zero one loss [Music] who's ready with all of them raise your hand oh come on guys to give you a few more minutes once you're done please think about the two questions below which functions are strict upper bounds on the zero one loss as you ignore the strict I'll just upper bound since you're one loss what can you say about the log loss and the hinge loss as ZB goes to minus infinity [Music] all right so maybe cuz I'm going to scribe let me start with a zero one loss does that look like yeah that's correct how about the hinge loss yeah that's right exactly right and this you know let me just show all of them so this is the answer so zero one last year whenever whenever y times H of W is positive that means I get it correct and the loss is zero the moment it gets negative it has a loss of one right and this he has a point of discontinuity yeah the hinge loss is the red line the red line has zero loss when it's correct and greater equal one right that's very important and so the moment you get too close to zero like there's a busy a danger of misclassifying in this here's the margin right so what's happening here when you and you're really far out it doesn't care about you right you're correct you don't know contributing to the loss but the moment you're getting too close to the hyperplane this years when you're right on the hyperplane when you're too close to the hyperplane the loss goes up goes up goes up goes up right and the moment you cross the hyperplane you know it keeps going up linearly right so this is exactly what pushes it out if you think about like well you know what's going to happen the points get pushed out until they they reach exactly this distance of one which is exactly the wide line that I drew early and the pictures okay so it tries to push everything out from there then you have the exponential asks what is the exponential loss to the exponential is really really aggressive right so if you get it wrong like the the hinge loss that if you get it wrong well that's that's that's that's per head right and you have large loss but if you could instead and prove another point and move it down here or you have one point up there that you move to the right it doesn't really make any difference right so the exponential loss he can see how it freaks out right this point here - - right is that somewhere up here right so if you can improve this point just a little bit and move a little bit to the right move a little bit towards the hyperplane the lots going to drop tremendously right so whatever Chucho try to do everything possible to get this one point right that's really far away and so the exponential loss will always focus on the point that's kind of most misclassified right in a justice hyper rain rain accordingly whereas the hinge loss kind of points to focus on the closed points right it kind of moves them out here for the for the exponential as these points don't matter very much because they're so insignificant compared to I get by the way you have to look at the absolute loss you have to lose look about look at if I move a point to the right how much does the lost go down that's really the important part right so here I can move that loss down a lot if I go a little bit to the right because it drops drastically wherever move here to the right the exponential loss doesn't go down very much at all and then the log loss is this black line here that's kind of you know that's the most chill in some sense it but that's the same thing that one thing about the log causes is not an upper bound of the 0 1 loss so if you one thing is with all these other loss functions you compute the loss the hinge loss and then launch the logistic loss and if you know that the loss divided by n let's say is point two right then you know you can't have more than 20 percent points misclassified right because it's an upper bound of u01 the logistic losses that's not the case right sometimes it's very nice so but in all the cases there so the for example you know hinge doors exponential loss of you you know if you drive it down to close to zero then you know you must have every single single point and correctly classified and last lecture was what what happens with the logistic loss of log loss and the hinge loss as Z becomes very negative one you can see here these two easily become parallel lines so also the hinge dog is more aggressive here it's temperate at some point actually four points that are really really misclassified the penalty is just linear so actually they are both very well behaved when it comes to outliers all right and we will continue this on Monday 
","['', 'Support Vector Machines (SVM)', 'hyperplane', 'maximum margin', 'constrained optimization problem', 'slack variables', 'soft constraints', 'cost function', 'SVM with soft constraints', 'quadratic programming solver', 'gradient descent', 'hinge loss', 'logistic loss', 'exponential loss', 'misclassification', 'outliers', 'regularization', 'binary data set', 'linear SVM', 'kernel trick', '']"
"ready please put your laptops away thank you one little thing so the exam is in one week and one day I hope you are feeling ready please don't start too late as I said before number one so is the mic on can you hear me in the back mic on can you thumbs up no one to better okay so exam is next Tuesday please start early some people ask what's going to be on the exam the answer is everything everything that we have covered we are posting or I think it's already posted exams from previous years on Piazza under resources if you click on resources on the top and you can see them there we separated the months with within without solutions so basically first try to do it yourself without solutions and then look at the solutions and please please please start your shuttle ready with the studying you know one one common failure mode in this classes that people do very well on the projects but that actually don't take the exam seriously enough exams is 50% of your grade so please keep that in mind alright so now I do this purposely because I machinery really is kind of half theory and half it is you know practical chops right so you can also have people who know the theory but can't actually get anything working so I really want to kind of have this course cover both of these two sides of machine learning in order to do well you have to do well on both of them any questions about logistics okay good and last thing that's the project the last the next project will really be released today I've been promising it several lectures we ran into some bark that actually sometimes appeared when we tested it so we always have some TAS pretend they're students and it turned out you know we actually had to communicate with velarium and they actually had to change something in their back-end to make sure it didn't happen anymore so delay things a little bit but ultimately you will get the same amount of time you'll get two weeks starting the moment it gets released it's not it's basically erm so the idea is you have to implement them on a bunch of these last functions so hinge loss logistic loss and so on it doesn't take very long so it's a good preparation for the exam but you can also finish it afterwards if you feel like you know you'd rather study you know the lectures okay so last lecture we went over empirical risk minimization and so the idea basically is that machine learning very often we can write the learning algorithm as saying what we are trying to do is we try to find some parameters we call these w that doesn't have to be W these are busy our weights and what we are minimizing essentially is some loss that goes over the data set you know I think I wrote it as H W of X I come on why I plus some regular rise and so here H W of X is busy the prediction of data point X I this here is the true label and we have some last up easy says how much do I have to pay for getting a point wrong or even right but not right enough it's a Bayesian that maybe not confidently enough and the regularizer here is the second term that penalizes really complicated solutions it turns out if you're in a high dimensional space then typically even with linear classifiers you typically always find a solution that makes knots no single mistake anymore but it tends to be overly complicated right so basically you learn some rules that just you know rely on the noise in your data which doesn't mean anything so by making trying to come up with simpler solutions you effectively obtain better generalization all right the last time we talked about the hinge loss the hinge loss which was also what we saw in SP ends then the resisting loss and the zero one loss and the exponential loss so these are kind of typical famous classification loss functions today I want to move on to regression and say regression bassy says we are in the certain regime well why it can be a real number alright so for example you're predicting a person's height or you're predicting the value of a house based on its features etc right or you you know predicting the temperature rights based on previous temperatures like you know net cetera and and so again we have many common loss functions I just will go through them and maybe make a few comments on them and the important thing is that all of these basically fit into the same framework so if you understand how to optimize one of them you probably know how to optimize all of them so the first one is one that we've already seen many times that's the square loss and we've seen there an ordinary least-squares and enriched regression and the loss is very simple loss of the following it's penalized h w of x i minus y i squared and so that's our loss function and so you know in some sense nice thing about square is is always positive so if you're off you're either below or above why are you trying to predict that say a price of a house right if you have too high or too low it does matter right get the same penalty the other property of the last function is that because it's squared is if you're more off that gives you a much larger penalty right so if you're off by 1 1 squared it's just one if you're off by 10 10 squared is 100 right so this quadratic relation forces this last to focus more on outliers so if something is really busy make sure that nothing is really wrong and if you would just say ok well I'm just trying to say you know I mean over take all the people in this class for example and look at their height and I would try to predict what's you know what's the the height P that minimizes the square loss across everybody does anyone know what I would get probably the answer I'm trying to find some value that would an average minimize like here my age Bobby of X is just one number right he ignores the X right so ignores the W it's just one number I'm just outputting one constant and I'm trying to fit that constant what would it be any ideas the average that's exactly right right so and what the square loss does it tries to fit the average all right that's basically what is aiming for if you actually can fit exactly the average then that's actually you know that's the optimal solution in terms of the square us all right you can't do better than the average so estimates estimates me and so you know for example if you do income prediction right and that'd be one example but the average may not be well-suited right so because this income is not is this distributed based on the power law distribution so there's a very very few people who own as much as everybody else and so if you know for example if you take the average income in the United States all right let's say Bill Gates makes a good investment and you know increases his his his income by 10% right something you would look like everybody's income and up by quite a bit on our but it is not very meaningful because only one guy made more money so but on other hand you know in some other settings the mean may be totally appropriate but if you don't actually have such crazy outliers the nice thing about the square loss is that it's fully differentiable everywhere so it's very very easy to fit with gradient descent and of course if you you know Hessian then it's become just just one step um beginning on your picture and if you just minimize this with a linear classifier then it just becomes ordinary least squares this is exactly what we had before any questions about the square loss all right so the next one is the absolute loss and here the loss is it's almost the same but it's the absolute any idea without looking at the notes of what it estimates this does not estimate the average what does it estimate what is the minimum error in absolute terms I would try to predict the some quantity here that estimates the height of everybody in here and I want to minimize the special absolute terms I'm I'm off by as little as possible it's not the average what could it be median right the median and so for example in the income example that would make it out more sense right so now Bill Gates gets up very richer wouldn't wouldn't change the median at all yeah no no no oh sorry yeah so if you compute the average here it doesn't give you zero it's just as good as you can do with the constant right you can't do any better that's that's all right and you can do a lot better if you do something different than a constant if you actually try to predict something for the X you know take the X into account but here's one more thing right for every single X let me just expand on this but every single actually have some distribution of what's the probability of Y given X I right and so basically people you're trying to predict this actually the mean label given X I right that's basically what you try to predict of this distribution what's the average that's what the square loss is trying to predict okay it's conditioned on your featuring and here given your X are you trying to predict the median label this is down here does that make sense okay good so for example let's say I try to predict height in this class right yeah if I would just take a constant just hard to say what's the you know what's the best you know one number what's the height of my students in this room right here this would be the average height in the median height but I could do better if I take any features in the account right so for example one obvious feature would be gender right we all know that height is highly you know agenda certainly affects height right so then actually you know what I would get is for all the guys here would get the average height for a guy and all the girls I would get the average height for a girl okay that would be the best prediction if I only have one binary feature yeah how does it go from okay they're not yeah it's quite simple actually so what you do is you put in some constant here and you minimize this so you just take the derivative and set it to zero what do you go find is actually the answer is exactly the media they mean this one is a little not quite as trivial any more questions all right and okay so this one this one is often better because what the apps the advantage of the absolute loss is that if you have something that's really off right let's say I'm trying to predict house prices and one house is you know I'm here if the car the most houses are not that expensive right but some guy actually you know build a nice beautiful mansion that Lake right and that actually costs ten million dollars right so what the square loss would do if I'm off here by just 1 million right well that's a lot that's a million squared is this just you know gigantic right so would spend all its energy trying to get that one mansion correct and would be happy to sacrifice all the other houses in Ithaca that's right so that's the problem with these you know with the squaring after us because the absolute loss wouldn't actually you know wouldn't consider the this mention any different the problem of the absolute loss is that this is not differentiable at 0 and you're trying to get this to zero so right at the point where you're trying to get it you it's not a friendship ball and that makes it a lot harder to optimize and there's a beautiful kind of best of both worlds and that's called the Google us and who Balazs one of my favorite losses it has the following you either pick there so you pick this you'd be basically if any very very close to zero because that's where it's differentiable but if you're far away from zero that's where this here becomes problematic because you're squaring the differences so you actually I tell you you know you have very large losses then you actually switch to this one and you can basically construct a continuous function that way the following way you say I hope you can see this 1/2 H of X I minus y I squared if the absolute value is less than some Delta and otherwise we have delta x minus delta 2 so otherwise so what does this do this here says if the loss is the absolute or was less than Delta then square it which makes it basically the same as the colleague loss so around zero you make a quadratic but then at some point basically you just continue linearly so for large losses you just get the penalty of an absolute loss so if someone is a millionaire or something you're not actually squaring that in addition to the original large loss so that's that's very outlier friendly and and that's typically the best of both worlds but surprisingly many people don't know it many people just use this loss of this loss and most of the time I would say the boob loss actually is the preferred choice and who knows just one way of doing this there's another one that's very very similar and that's called the law cosh loss and just for those people are not familiar with it the cost function is the following fee of X plus e to the minus x over 2 maybe think about this for a second what happens with this function but it gets very very large and what happens when it gets very very small very negative yeah yeah ok yeah so the question is when watch what you think what should you Delta V and so that's the one dancer than velocity they have this choice to me Delta equals two here we go I mean it did and it doesn't matter all that much right because in some sense I mean by picking this Delta u baby say how much of the square loss do you want how much of the absolute last day you are right so if you think there's really drastic outliers that make that so small right if you think they're what we won't be that many make it larger yeah how do you can with this expression so you basically just connect these two functions that said exactly like you basically have a quadratic function then you say up to a certain point and then you busy solve for what's the first derivative and that's exactly how you continue it I'll make you draw it in two seconds well I see in one minute okay so Lizzy has the cost function if we think about this is actually a pretty symmetric function right sothanks becomes very positive if the X takes over there's just e to the X right if X is very negative right then this here goes to zero so it doesn't matter but if this here this goes it's e to the X right so this is negative the negative sign cancel out this becomes again e to the X so it's basically kind of each of the X in both directions okay does that make sense raise your hand if you have an intuition what that function looks like okay good right so this may see you just e to the X but instead of actually that's kind of what each of the X looks like right you just say well that's going to make this and the nice thing about what we can now do is the log car section just says we take the log off the course of X H of X Y and I'm gonna have a quiz in a minute that you have to think about what this function looks like so the nice thing about this is this is like a Hoover loss the nice thing is differentiable everywhere and the Hoover loss actually right at the point where this equals Delta it's not differentiable but that's that's okay because that's a point but you don't really care about very much the downside of this is that you have to compute these functions which is a little slow if you have to do it millions of times you know computers are not very good at computing and exploitation and logs etc okay so why don't we just do a quick exercise so please you know on your on your sheet right below this table you actually see a little little graph so why don't you just quickly draw all four of those functions and she managed to draw the human loss with Delta equals 1 and Delta equals 5 so to Hoover losses the law kosh the absolute in the square loss oh no no it's actually a constant delta oh good point yeah oh sorry someone does that's a really good question here use Delta this is not a function it's not the Delta function it's just the constant Delta Y Delta just means a small number yeah thanks for clarifying this [Music] [Music] all right who thinks he or she has all of them no who thinks he or she will have all of them soon all right amazed s get started okay good and I can draw them here okay who knows what the function this is this is easy that's the easiest one it's Creoles alright good next one hey oh yeah here we go what is this one absolute right exactly gets harder oh it takes a long time to compute that function wait oh okay damn it so which one is green which one is blue yeah greenness is uber what the delta 105 one yeah that's right and blue so blue is I believe also tell who got us yeah and the lock cost is actually actually well sorry the green was actually LA Cotte and red and pink are hooba there's basically almost no difference between LA cashier and and Hooper right they're very very similar function that's why it's hard to tell them apart it's not impossible so here you can basically see so once again right the way you have to look at these last functions is but you have to look into if I have a point that somewhere here right I can easily spend energy I can adapt my parameters do moving closer to loss zero right and the gain that I'm getting is how much the function goes down right sleeve or the absolute loss the black line if I'm here three right at some point that's off by minus three if I correct it by one point you know that the improvement is the same as if I have a point here and move it by moving one to the right okay so there's no incentive of looking at the points that are really off and improving those like improving anything helps the same way whereas a quadratic loss if I'm off you know here right this point here if I move one block to the right I get a huge improvement right whereas if I'm here I only get improve right does that make sense how much you go down so that's what do you have to think about right so moving along these lines or what the last ones will do is it will take at the points a look at the points that are really far out and try to move them in because then it can drop the loss a lot on average I'll take it any questions about these dogs foxes yeah so seems somewhat arbitrary it is somewhat arbitrary there's this in some sense is the choice you have to make is the data scientists right that is kind of left to you which one is the best loss and certainly something you know weighing in to how noisy is your data etc right is a big factor in this case but yes you know it's people have written books about each one of these last functions though it's you know the theory is very well understood but in practice is still kind of it mostly comes down to how noisy your data is I guess I remember when I used to work in industry we had actually you know we had fierce discussions about which loss function is the right one thank you know we almost got into actually real arguments yeah yeah yeah it's not faster so nowadays that's maybe not so much of a problem anymore but computing log gosh actually is computationally pretty expensive if you just think in terms of cycles computing cycles that squaring is really easy it's just one multiplier right okay yeah it's not even sorry shift shift yeah so it's super super super fast takes one cycle one few nanoseconds whereas taking the exponentiation in the log is much much more expensive yeah yeah yeah there they are already on the website yeah any more questions sorry Turner shift you have to multiplication what's wrong yeah okay good there was monix the author's not here today but here's extreme P bugging me to show you this one demo that I promised you last time so there's actually I don't want to pretend that I had a demo and then actually not show it so this is actually I never I owe you a demo on Ridge regression so it's just as as relevant now as it was back then because its base is just the square that's right so here's basically richer guessing that's a you know it's basically just a minimize the square Lawson have an l2 regular riser so here's you know the data sent let's say I'm just drawing some data points you know it helps when you make that sound and and now you I fit this data set with a casa que regresar and here's here's the line that minimizes the square loss right so basically what it does on average basically it just be off very low right and this is for very small lambdas I do very little regularization if I increase my regularization then the you can see here what happens I'm oh my gosh no so now I'm increasing my regularization what happens if AZ flattens this out right and the baby says simpler solutions are parallel to the horizontal axis so in the extreme case I do a lot of regularization that just ignores the data and just gives me a straight line all right so that's basically about this what this does but the effect of this regularization is and I can also show up maybe I draw one example just do it one more time with with them some outliers and let's you know do this and let's make some point here right and if I now fit this you see that actually it moves this whole line right a lot over towards this point here okay so it really is really a bad estimate for these points yeah right and that's only because of this one point but maybe the measurement where as like typical and then in biology or something you measure something you might invite that moment a big Mack truck drives past you lap right and everything shakes and so this is kind of the one point where the Mack track kind of you know distorted your results right and so the square law basically falls for it right and basically moves like all of these guys now get worse predictions just because of this one outlier yeah well so the problem is it doesn't really work right because the prediction is actually the overall prediction across the entire over dimensions it's not actually clear how to do that right yeah it doesn't really work and it's a good question yeah you're right you may have some some dimensions are very noisy now that's not right the right probably the best way is to use the lock harsh loss are the coolest yeah any more questions okay okay let's move on to regularization so what's regularization regularization is raising this additional term so we try to minimize the error and but what we've already seen this right that actually when we did in logistic regression we did map inference or in the OLS we that map inference right and then what we got is the second term where in our case actually our of W was always W transpose dot right in fact we saw three classifiers where we had this regular doesn't even remember the third one one is rich regression the second one is logistic regression with map and estimation what's the third one nobody someone as IAM thank you thank you oh my gosh yes SV app right so the SVN the last actually became that a regular rise of his become the hinge loss plus the l2 regular is someone asked me a really good question at the end of last lecture and said so with the SVM right and I'm paraphrasing the question but it was a great question so like it initially we started out saying we have this data set right where we have these crosses you have the positive negative points then we would like to find the hyperplane that has maximum margin but at the end when we looked at the final optimization problem you know it just minimize you know minimize W transpose W is subject to the constraint that why I stopped you transpose X I plus P is greater than equal 1 right so why does that maximize the margin somehow you know the intuition was very clear at the beginning and he you said like you know every single step made sense but then at the end we arrived at this thing that doesn't seem to maximize the margin and so you know for example if I now would take this hyperplane here right that also seems like a good good classify a classifier according to this this optimization problem and let me just make that connection because I think it's a very good point right turns out no no this is actually a suboptimal solution in this case and so why is this because this constraint here basically says that the inner product with W must be at least one for every single data point that means this user space there can be to draw a line on each side they say there is a line all the points to the right of this heaven in a product of at least one ok or negative one in this case it is that those lines make sense raise hand that makes sense ok awesome this is my W right and now here comes the point what does the last function say the last function here says minimize the square of W so what I wants to do is take this error and make it shrink it right then what happens then if I have an inner product right this is this is basically the line with the inner product W transpose X but speedy gives exactly you want now I take my top of you and I shrink it and make it shorter what does that mean for this line right it means that the line moves out right it has to because now the points X have to be further away right to get the same in the product because I'm multiplying with a smaller number right so if W get smaller X has to be larger right to get the same in a product so if I said shrink W and that's exactly what the subjective is trying to do it says give me the smallest possible W right so I'm doing this what happens this is this margin do you know moves out but in this case if I have this hyperplane the the margin can't move outwards anymore right because this X already lies here this is o lies here okay so I have no more wiggle room open against the corner right so I can't shrink my W anymore okay where's on the other hand if I would now rotate the whole hyperplane right and I'm here that actually have more wiggle room you don't actually get they can actually push them out even further by don't actually in this case actually the maximum Arjun I played with Bobby look like this be somewhere along here right actually have if this and this this line right so if I make sure exactly in the middle then I can move these out as far as possible and what that means is I should get the smallest probably a possible W squared okay does that make sense so it's actually exactly the same thing right it's just a different way of looking at all right so with this constraint and basically saying you know try to to you know move these lines where the inner product is one as far away as possible that's what this optimization problem says but don't go any further you know like the moment you hit any data points you have to stop and that means you you will arrive at the maximum margin solution yeah yeah yeah so essentially this is basically that's right so though if you now put this in there this is basically hinge loss right and so the nhd have the last function here plus the regularizer yeah that's right so in the soft constraint where is it okay good any questions okay so I certainly regularize on the SVM it's pretty clear but the regularizer does write the regularizer basis says don't just find me any hyperplane find me the hyperplane that gives me maximum margin so in general a regular rise that can be viewed as saying I want to have a simple solution so this year's ocean also know an ocean of simplicity all right saying the simplest solution actually is the one that's right smack in the middle and typically it's a little more general than that and let me actually before we get into this let me just tell you a little bit a little trick of optimization theory so we have the following problem or let me write it here can people read this in the back can you give me a thumbs-up okay good so this here's the optimization problem you're trying to solve now for those people who've done after ization theory and you may know that actually this year is facing something you know it's called the Lagrangian formulation of an optimization problem that actually has constraints so what I really mean is you can rewrite this guy as something that looks slightly different you can say that's the same thing as saying the following and minimizing over W the same function L of H W of X I comma Y I right I'm minimizing the loss subject to a constraint such that our of W is less equal than beef and so for every lambda they exists a B such that those two are equivalent if every be there exists a lambda that those two are folks right so it's not obvious what that is but but you just trust me it exists okay does that make sense so why is this the case because my lambda is large enough I will arrive at some solution for our of the okay so if my Landers large enough based eternal minimizes loss but it will also try to minimize our of W right which is also some function right the larger lambda is the more I'm trying to make this part is small if lambda is small that I don't care about this then I mostly want to make this putt small so lambda B says how much do I weigh off these two functions now if I solve this for given lambda then basically you know I get some value for R of W and I can make that value exactly might be right and now if I run this optimization problem I get exactly the same answer okay does that make sense the other way around is not as obvious so once you have a B you have to try out a bunch of Landis but turns out there always exists any questions about this who believes me raise your hand okay good so gullible no it's actually true it's true all right so let's just view it as this right so that actually what is our regular riser regularizes constraint right and so what's the regular rise that they've been looking at so far that's the l2 regular right so R of W equals W transpose double and it's be looking from this perspective right that this actually has a very neat interpretation it means that W squared must be less equal than some B for some B that you specify so what does that mean that means you're somewhere in the space of W right so this here is my W wand it's my first entry of W this is my second let's say my W is just two-dimensional okay then I can draw it right so any point here is basically a solution that's aw okay does that make sense that's the force curve of W that's the second coordinate of the other okay wait raise your hand that did that make sense okay now here's what we say we have some function over W even if you're trying to minimize so that's some function you know in the space let's say it looks like this right and here's the minimum okay so this is kind of a valley alright you're looking at and we tried to find a WBZ you know when we do gradient descent we start with some W when we walked down here until we hit the minimum okay that's just minimizing the loss this here's my last part what the what is this year this says this w fw w transpose w is less equal than b what does that mean that means the sum of squares right w 1 square plus w 2 squared that's less equal to b what does that mean that it's a circle that's exactly right right you have some circle here of radius B squared I guess right and my W must lie inside the circle actually no it's a sphere at a hypersphere have some ball right I'm saying minimize this function here on the right right minimizes lost function but the solution must still lie inside this ball that's what my constraint is telling you that's what the regularizer is doing ok and so what what about to seek waiting descends trying to do right it's trying to get to this point here but it's not allowed to right you're saying you have to lie inside this ball so what is it going to do it will find the closest point that's basically to the minute and it will end up here okay does that make sense and as you increase your B at some point it doesn't matter anymore right if you really really large and and you will just end up here right so if your B is that basically means your lambda is so small that the regularization has no effect anymore right that's the equivalent of you but if you lander is large enough then actually it will not find the optimal solution it will find the optimal solution under the constraint that you're inside this ball okay that's that's the idea behind regular l2 regularization any questions about so the B that's right so for every London exists to be if everybody existed London it's also a yeah yeah that's exactly right so in practice what you do is the question is how do you find lambda you just search for different values of lambda on some holdout set you see which one does best right and so the problem is that basically if you minimize just on the train data set why'd you get a little barrier you always get lower error if you decrease Yolanda right the lambda will hurt the regularization hurts your training performance but it may increase your test performance and we will get into this very very soon actually after erm is done we will talk about the bias-variance decomposition and then we will get into exactly this trade-off right you know between regularization and actually minimizing a more regularization less regulation etc any more questions yeah that's exactly right right it's a small small Nam that means larger P right in basis zero lambda means internet P the base e means this is no longer an active constraint that can be anything you want it's exactly right any more questions okay so I guess we are out of time I will continue on this next week a 
","['', '', 'Empirical risk minimization', 'Loss function', 'Regularizer', 'Classification loss functions: hinge loss, hinge loss in support vector machines, squared loss, zero-one loss, exponential loss', 'Regression loss functions: squared loss, absolute loss, log loss', 'Least squares regression', 'Average prediction', 'Power law distribution', 'Huber loss', 'Delta function', 'L2 regularization', 'Ridge regression', 'Bias-variance trade-off', '']"
"everybody all right so a quick thing on Tuesday as you hopefully know this point is the midterm if you haven't started this point please realize that you're late in the game and get started as soon as possible last time we talked about regularization and I showed you that the L to regularizer is basically saying I have a ball oh wait this is not on it's on no is it on now better yeah well to regularize that says you have a ball around the origin and maybe let me move this up after all so the yell to regularize has a ball around the origin and what you're essentially saying is you see the solution it has to lie within this ball and this is how you control complexity please close your laptop thank you this is how you control complexity so essentially what you want to avoid is that you get a really really complicated solution where some values that really really large and others are really small so if you shrink everything it basically has to come up with some simpler solution that's that's the optimal intuition and you're restricting this space right instead of actually finding a solution anywhere in the space you just say well I has to be lot lighter than this ball and so you trying to minimize some function and see this is my function here's the minimum all right so ideally if I didn't have any constraints I would end up here but my worry is that that would actually that could over fit with the training data so what I'm saying is find a solution to this function but stay close to zero and that typically helps with generalization gives you a better estimate you know for your test data and also actually that's actually what the map estimate gives you if you have a prior Gaussian prior over your your vector another norm and another regularizer that's just as popular is the l1 norm so the l1 norm is the following you just regularize lambda times the absolute value of W and if you draw that as the same kind of image what you get is not a ball it's called the l1 ball the l1 ball actually is not a ball at all as corners it looks like this like this the l1 ball it's kind of like a diamond right and so you're restricting you to trying to find the optimal solution and you're restricting your set to be inside this l1 ball that's the idea can anyone imagine what are the one of the advantages of the l1 ball what are the properties of the element ball like why would you do this why would you not always use a circle ideas well this is this plan that basically finds how big that that area is right you can make it as large as you want doesn't have to be uniform yeah that's right increase the sparsity that's exactly right so so what it does and let me explain this for everybody else what happens is it's like you're minimizing this a function right and let's say this function is up here the minimum is up here okay so ideally what would you want right this here's my table you want well he does he might have you to value the best thing would be to w2 pretty large and w1 some small value okay so that's the that's the optimal solution if no constraint was was enforced now I'm enforcing matter one constraint and now if you look at this object let's say this objective function looks something like this right well you will see that actually the optimal point will be exactly this corner here right because because it has these pointy edges the pointy edges kind of you know can penetrate this function a little bit further until the optimal solution will be this value here and what does that mean I cranked up w2 as much as I can and I said W 1 to 0 not to a small value to 0 right and so that's what what ends up happening here in l2 you don't really see this true you would end up somewhere here pretty very close like you would have a very very small value to of w1 but it wouldn't be 0 and the reason people really really like setting things literally to 0 is because then you can rule out that whatever feature w1 you know whatever dimension that corresponds to contributed anything to the prediction so if you make predictions and biology and you want to know you know for example here's my you know DNA or something right and and I want to predict there's a person half you know is likely to get cancer yes or no what I would love right if most of the way to actually zeros if I can do well by a prediction well and then actually most of my weights a zero then I know all of these things actually don't didn't didn't actually contribute to the prediction right and that makes it much much easier for me to understand how the algorithm arrived at the prediction okay so what people like especially in Sciences people actually care much more about understanding the prediction that actually the extra prediction itself and so therefore they love these sparse vectors they're in a lot of things set to zero and so with very high dimensionals what you will see is that actually you know in three dimensions this kind of you know had kind of goes out of the it's out of year and so these entire edges all have w10 so in high dimensional spaces this thing consists of many many edges and almost all of these edges have the majority of the weight set to zero so yeah your answer will always almost always lie on one of these and so you will get a very very sparse answer your weight back table set almost always to zero yes right so what this thing's gonna do is this this will set something non zero to every single dimension right so I guess what you're saying is why not train this and then just look at the largest way to something and see you know and the problem is you don't know wait a small weight makes you still contribute a lot right only once it's zero it's really kind of off let's switch stuff the people do do this occasionally that they actually then look at the top base but why not just use the other one regularization is actually to be honest the optimization is more annoying to that one regularization but ultimately the predictive power is usually not not worse is that going to answer your question the nice thing actually and we won't get into this now but actually if you do i-12 regularization one thing you can do you can actually solve in closed form for the values of lambda such that exactly the first suite of feature becomes nonzero the second feature becomes nonzero and so on so just to make this clear if my lunder is very very large then this little diamonds going to be really really tiny and almost certainly I will sit in exactly one corner so only a single feature will be nonzero and that also intuitively makes sense if you think about your last function that's going to be a single feature that's most important to predicting the label you know if you can only spend very very little weight what it's going to do it's going to put all the weight on that one feature all right so that's what the l1 regular visor does and so as you basically one thing you people do is they kind of grow this ball slowly slowly essentially lowering lambda and then kind of see which features pick first which pictures features pick second and so on any questions one downside of the l1 regularizer is that it's not strictly convex so that means there's actually you could have for example imagine you have two features that are identical then it doesn't matter how you know how much weight you put on one or the other and you could put all the weight on one feature all the weight on the other feature or half on both or any combination thereof and they're all exactly the same solution that's one downside and that's annoying so what people do in practice and this is called the elastic net it's that you do a little bit of both your lambda L 1 plus mu L 2 and the Mew is very very small so if you do this then actually because this year is strictly convex and there's a unique solution that cannot be multiple solutions although it is it yeah yeah how do you excite this constraint bassy says minimize my loss subject to is less equal sum budget and this project corresponds to lambda so last lecture I said that is equivalent just minimize lfw plus lambda times w those are the same that's the same thing the same optimization problem there's just the Lagrangian formulation so I can go back and forth between these it's just these are nicer because now actually you can draw the set and you say you have to sign that inside that set Li inside the set but those two are exactly the same thing in fact actually if you solve this problem and if you just take the norm of W that is exactly B and if you think about it what is the absolute value right like absolute value of W equals the sum I da Bo that's cos alpha for a be used to basically I'm penalizing every single W the same way everything that I mentioned the same same amount so I'm saying I try to find a simple solution that's my goal that's why I'm regular rising I'm saying I'm you know you don't just minimize the last you minimize the loss but find a simple solution yeah he finds implicitly as saying the sum of all the weights must not be larger than P but it's okay to put all the weight on one dimension that's okay that's not more complicated than putting a little bit of weight on all the different dimensions right that's okay here I do something else here I say it's the sum of our a alpha W alpha squared so here I'm saying if I put all my weight on one dimension that's worse than putting a little bit of weight on all the other you know and every single dimension because I'm squaring right so if I put you know Mike one weight really really large a square this that's a lot more than many many small squares so it's a different different definition of complexity and so this definition of complexity basically leads to you know the classifier putting all its weight under a few features and that's very informative has the disadvantage that it's more brittle right so if you for example imagine have a self-driving car something try to classify something and they put all its ways and a few sensors if these sensors then drop out then actually you know your prediction is off whereas this one can be more robust because it spreads out the way any questions yeah it does not solve you differentiability problem well the last thing that solves is that one thing you could do is you could actually if you have to in our features that are highly correlated then you could put weight on one feature on the other feature on both features and that's the same thing according to this thing where it's this thing only has a solute in the unique solution imagine you have two identical features the optimal solution in terms of the l2 norm is to put the same amount of weight on both at the moment you would want would be larger because you get Greece's increases quadratically you're actually paying more for that so that that's what that does so the elastic net is basically kind of combines the robustness of the solution with the ability for l1 norm to to actually identify features yeah yeah same thing yeah any more questions about this yeah yeah but the problem his answer is saying the following imagine I have two identical features wouldn't be the best thing if the answer would be to just drop set one to zero and yes at the element to a large value but that's not what the l1 norm is doing right the one room you have no idea what you're doing because they're all exactly the same right so depending on your implementation and that's what scares people right because two people run basically the same you know optimize the same optimization problem they get different results and you don't know which one is right right what the promised people would like to have deterministic answers and so by doing this now you have you know now if you run this and I run this we must get the same solution no matter what solve are you using and if you use MATLAB of Python doesn't matter yeah you fit your model you minimize the nut model about you you add this complexity penalty basically right and this serves two purposes a it avoids overfitting there's the bias-variance tradeoff it's just the first lecture after the midterm and the second thing is that you will always discover free which features are responsible for the prediction right it shrinks the weights yes so okay good so he's asking very good questions saying wait one second if I'm only if I'm a biologist and all I care about is basically you know the first day okay about is which features are you know are useful but then actually I really want to have a good predictor now these things are in conflict with each other right so I have to make a tight budget be I say I want to have my classifier inside this small area right that's good because that will tell me which features are responsible for the prediction it will set 0 to most features but also but those that I'm pill is selecting it will also shrink those right because evil in this case we'll put all the way to w2 but ideally w2 should be this value up here right so I want to learn the w1 is maybe not not relevant or w3 maybe is not relevant at all right but I still actually don't want to cramp w2 Stein here too much and so he's saying why don't we just run this first then be identified which features are useful and then we run without regularization or with l2 regularization on on with these features and that's what people actually do in practice so if you'd invented the 20 years ago famous now okay any more questions that was going to the end off of erm please look at the notes I wrote down to it two or three famous cases one is called lasso lasso a squared loss and the other one is elastic net if you add an L to regularizer to it so if people refer to lasso or elastic net this is what these are so basic just the square loss with aiyla I'll run regularizer that's lasso and l1 and there to is elastic net and then you also have SVM and of course logistic regression you already know maybe I show you a very very quick demo for a few seconds and then we can then I actually want to do a little bit of review for the exam just to get you guys ready okay what I want to show you now it's a small data set iron I don't think you've seen it yet it's small you know some some data set actually from life sciences and what do you see here if let me just explain this just a attention for a second guys so what I want to see but you want to show you here is basically if I run if I run the classifier there's a squared loss with l1 regularization that's called lasso and I crank up a bit a lower lambda so if lambda is really really large this here's the first solution so the x-axis here is basically whenever I kind of lower my lambda just enough such that one more feature becomes unlocked okay so my phone my lambda is infinity knowledge that every feature is zero okay so everything is at zero and what do you see here these lines are the weights associated with the different features so if I lower my lambda a little bit the first weight shoots up that's the first feature that it picks okay so basically at the beginning it right here I now advisee allow it just a little bit of weight right yeah to distribute it over the weight vector and what it does it picks one feature and puts all the weight on that one all right so that's the most important feature and then I lower lambda further and what it does it keeps increasing that feature but it also increases another one that's the second one all right and these are some features they're not necessarily dimension one or two these are just over the vector right here you see the same thing does he is the weight vector so what you see here is the first weight does all greens let's zero everything is zero and then the next row is the weight vector it's like the weight he is incorporate as color then you may see see here the first value up here that's nonzero that's the first feature that basically some main assigned to and then as we keep to the right what you see is that basically the features get more and more pronounced right more and more weight is distributed across the different features and so here that's the same graph as this one this just basically you see the value you know as negative or positive points here along these lines so here at the end basically what this line tells you is that this particular feature has a value of minus 2 ohms minus 0.2 okay because that graph makes sense right ahead of the graph makes sense okay good and what do you see you on the top right is the error and one thing you can see is as I'm lowering my lambda right the training error the loss goes down very very nicely right that makes sense right because actually now I'm not constraining myself into a ball anymore at the beginning I'm constraint is very very tiny more now I'm growing this and growing this so eventually I basically you know I can get closer and closer to the actual minimum what do you ever see is when you look at the test error and that's the yellow line well of let's look at the training error first training error is the red line that all the gist goes down nicely but the yellow line is the test arrow and that actually goes down up to here and then it goes back up again right so actually the lowest value is actually at point five it with five features so here so you only need five features right to do really well on the test set afterwards the other features they help you on the training set but they don't help you on the test set anymore okay and that's exactly what the biologists want to know they want to know which five features right are the ones that really help me on the test set right because those are the ones that really generalize right the other ones are just fitting noise right something that's particularly specific to that particular data set okay so when you work with biologists so you know in life sciences make these kind of graphs then find the cutoff point and they're gonna get really really excited when you then tell them it's these three genes something that explained actually everything about you know the entire test era I might test data or the accuracy my test data can be explained just by the these five genes or something or this protein or something like this any questions yeah oh yeah well you got me there I'm I can't remember it was something with something with iron I don't remember if it's like in the blood or something sorry selector braum I just think about the abstract data and yeah lambda this is actually this but this here's the number of features that are nonzero so always decrease my lambda until the feature-based becomes nonzero and turns out actually there's the algorithm called least angular regression that we won't cover in this class but that algorithm actually tells me exactly what the next value for lambda is true P so text you can solve that in closed form it's it's correlated with with B but it's basically that that's right it's just that it's somewhat it's there may be different gaps you know B may suddenly increase a lot until you get the first feature then it may just go a little bit further up to the second feature right so it's really the number of nonzero features that's what the x-axis is but yes you're right as that goes to the right B goes up as well it's just not linear right yeah so the loss is the actual loss that I'm minimizing is the square error but in this case actually it's a classification problem so actually have a training error as well and those are not the same thing so if you actually see that the last keeps going down but the training error goes up that means I'm I'm overfitting to the last function right I'm putting too much energy trying to get the square loss right when I really care about the training error okay any more questions yep yeah if you only care about prediction then I guess this is like looking at the feature vectors less interesting right also may help you debug things you may still want to use lambda right because here the best test error is actually at lambda you know at five features so you still want to do regularization right oh do you the otherwise you don't know when your fitting noise when you just memorizing training the data and you know when you still generalize it any more questions about regularization right let me turn out that actually is a funny thing that elastic net which is I guess people use it actually can be solved it's the same thing as an SVM there's a very interesting relationship that actually a student of mine discovered two years ago and we call it's spent that you can actually reduce the elastic net exactly to the support vector machine so for any problem of this kind you can actually construct a new artificial problem that has no meaning and if you feed that into an SVM the weight vector is exactly the one that you get here and so the nice thing is SVM's can be solved much faster all right what I want to do today is a little review okay good so I saw you make two teams so how much undergrads again grad students that's not good who is undergrads raise your hand for grad students oh that's that's rough do we have anyone who's not it was neither he add them to the grad students how should we other grad students are you willing to take the challenge yes okay okay good all right let's do this undergrads those brats together you know street bragging rights okay does this work yes this worked okay good okay so because the grad students of you so the way it works I do people know Japanese yeah yes okay great I don't but if you can tell me how it works so I guess the people bid on on these fields and then this how many points they get if they get it right so the Iranian versus someone says for example SVM's for forty prompted questions get more you know get harder as you go to the higher points and then I read the question the moment I'm done reading anyone can answer it you raise your hand and you make a loud buzzer noise it goes like e all right so many inspectors that for a second okay one two three good good don't be shy maybe everybody can each individual can only answer two questions like the fiddle undergrads can only say two questions grad students can only answer four questions all right that's fair let's put some regularization on okay grad students go first who wants to go first any grad student you don't have to answer a question you just have to say which field I should read it's pretty safe yeah sorry KN for ten ok he's playing it safe all right here we go the era of one year's neighbor classifier is at least as bad as twice the era of the Bayes optimal classifier and goes to infinity who is the Hector Herzl was it you tight well two of us - good okay other undergrad or grad okay good ten points yeah I took Moses back right otherwise it would be wouldn't be not as useful of a bount right oh you can barely see this let's please it's supposed to be dark okay okay you can pick the next one [Music] nice fifty all right ready go oh here we go when is the knife based decision boundary identical to logistic regression okay good you almost got it can someone help am i allowing some other grad students to help Oh your undergrad or grad you great okay now the undergrads are that's it the other guy yeah that's exactly right right so it's nothing we assumed it but it's actually true right that's what yeah I will post those yeah yeah okay okay you can ask the sorry the person who just answered the question you can ask next one you can take a pic the excellent SVM 50 all right ready go why shouldn't you incorporate the bias as a constant features as a constant feature when working with SVM's just let's make this clear and we had a perceptron algorithm you just incorporate the bias as a constant feature right we just added a feature of one just why can't you do that but the SVM wait I didn't hear a buzz announce that has come on okay good under Greta Greta right okay the undergrads get a chance to answer it there's 50 points why can't you do this so if I would just absorb the yep ah exactly that's it that's it right undergrad I presume oh boy so the answer was that because we're minimizing W transpose W right that's actually you know we're maximizing the margin but a crucial part of this that we are not minimizing P squared and so if we would actually minimize W transpose W but you absorb to be in there right then that would be the same thing as minimizing W transpose W plus P squared in your objective all right and that's actually not maximizing the margin they're actually saying I'm maximizing the margin plus I want my decision boundary close to zero right which is not what SVM's do does that make sense so B actually has to be free right B is not minimized very nice okay next question which one do you wait let me just yeah sure so who's talking oh yeah that's that's right that's part of that constraint that's right for it to actually become a linear classifier yeah that's right so the Gauchos tributed with the same variant at last both countries a country's classes yeah Natacha right I'll do you can pick the next one canine 50 you guys I like it you get over the big ones all right ready set go how would you modify the K nearest neighbor algorithm for regression some of that good okay sure I gladly undergrad ID okay okay this vote you've scared the grad student so much they say and yes exactly right right so one thing you can do is you can just take an average off the K nearest neighbors right and that gives you a value and so even better actually if you take a weighted average so actually if you just take you know a sign of weight to every single point so you do the following I sum over all my neighbors I element of my neighbor's Wi-Fi I and then I normalize the whole thing some of our I element of n WI right so they have to sum to one otherwise it's not a weighted average and so but you know one thing you could for example have this you know this way WI could be the distance one over the distance X minus X I or it one thing that people often pick is exponentiated so basically what you do is you want to give less way to points in a further way that's all you can define your own weighing function typically that improves that improves results all right next question general 5050 all right ready go for each algorithm name one assumption that it makes on the data cleanest neighbors naive Bayes logistic regression and svm okay keep going independent given the class name very very important yes condition [Music] the almost AI mean it's very very close so yeah I mean I don't know okay you know what I give you you know I give you half the points how about this in part also to motivate the grad students to kill 25 here we go we can't just beat them up like so yes K nearest neighbors data points the clothes you know similar points of similar labels that's that's a good one naive Bayes you know features are conditionally independent right and oh just ik aggression you said the right thing actually it's actually a little less strict to be honest in practice I would count it because we didn't talk about it in detail it doesn't have to be a Gaussian distribution actually it also works for any distribution of the exponential family Josh was one of such example and SVM linearly is linearly separable yes except when you actually have slack variables and it doesn't have to be right then actually still gives you you know it gives you a reasonable answer but it on the other hand the assumption you're making is that a linear classifier you know is a you know it's a good classifier right that you can fit a hyperplane which is it gives you reasonable results which is a big assumption right that could be totally wrong right could be that the function is totally you know a dataset is totally not a ball yet but I mean you know you just can't do it with a linear hyperplane right so in that sense you know let let me count it another assumption is that the you know a larger margin actually generalizes that be something specific transients yeah so you know it was basically correct okay good Arthur one more you can pick one here I'm 50 okay I like it I like it which loss function was used in each case two different three different examples the squared hinge loss with C very very small square hinge loss with C large or the exponential loss that may have been taken from an exam actually I'm not sure okay good down the grata grass awesome all right go for it come on be all cheering for you the first ones are really good smell see that's correct that's correct exactly perfect whoo all right it's the comeback all right so just to make you know a few indication why there's obviously the case so here the first one but you know it gets basic everything wrong right headache it's horrible all right the decision boundaries not between these different classes it doesn't care at all about getting the points right you just care about having a smiley smiley w it's a tiny W which is exactly here you know the W you can't even see it it's so tiny right so that's you know basically you have almost a zero loss in front of your your constraint and you in front of your spec variables so these two must be the other two and I guess what you see is that the hinge last year basic tries to enforce a margin between these two you see one point is on this side so that should be here right survey Z gives up on that one that's a lot harder for the exponential loss it also has to give up on that one but it will be moved much much closer to that point just because it gets exponential penalty all right svm 440 okay why is the SVN margin exactly 1 over the norm of W you have to do the buzzer sound okay trick no no that's just Max maximizing will maximizing the marginal it's sure that it's equal on both sides but the question is why is it exactly that darling yeah it's exactly right grad student undergrad grad all right all right you guys are coming back it's getting getting exciting so so the answer is very simple is the margins actually W transpose X plus P times I I guess divided by the norm of W but we doing that motivation set well let's fix the scale of W and B right to any particular value and what if we do we fix that exactly such that it's 1 all right and this was just to make the optimization problem easier so essentially it's it's the answer is a because we fixed it that way or B its enforced by the constraints at the constraints basically ensure that this must be the case and by the way if you just look at this objective that's the size of the margin now it becomes clear by minimizing W squared actually maximizes the margin right you're basically minimizing they stop at the norm of W yet but the margins exactly one over that so if you minimize the norm of W you know basically must blow up right you're exactly maximizing the module all right which one do you want to pick erm for 4002 do my grad students taught me to make that sound I don't have to know what it means um alright so there's no shoot I shouldn't have shown you the question before you can actually now baton member wait this was a grad student so you can now bet but who who asked that question okay you can now bet as much as you know as many points as you want on this and if you get it right it will double them so you got a double you know to be honest the best strategy but you can still choice you can how many points do you wanna bet yeah everything you want to go everything so base the waiver did you bid a certain number of points and if you get it right you doubt we get it double them if you get it wrong you lose them that's correct you get nothing all of it you gotta go all-in that's the optimal strategy you gotta win Oh [Laughter] 115 okay good so now only the grad students allowed to answer right but if you get it wrong we lose everything name two distinct reasons why you might not be able to use Newton's method on a particular data set independent of convergence so I'm taking away the one that made no convert give me two reasons why you may not be able to use Newton's method only grad students only grad students yesterday okay okay so we do teamwork okay sure it's the first one okay first one okay first one if the last function is not wise differentiable that's correct second one I can think of at least two more yeah it's exactly right all right so if the data set is really really high dimensional it may not be feasible to compute the hash it all right I'll give you the points awesome oh and mayor's are not be invertible right you actually have to invert the head okay now it's getting really really tight and I guess they have 230 and oh yeah yeah they are this is like the election like Khan expect don't take it too personal sorry and alright who can pick I guess that yeah you were the first one so nine face forty all right ready set go map estimation with plus one smoothing in naive faith with base with C classes and a categorical distribution with K categories per feature is equivalent to adding how many hallucinate examples to the training set you got a new sedated at least native see no yeah it's C times K right because every single Casa for everything category you have to have loosely assume that he see that exactly once right that was the answer grad student okay that's how many points forty oh my gosh the grad students are winning there was that was the Daily Double yes that's what it is actually by the way that's how and Watson actually won so what's next e they knew they have no chance against the human players the IBM Watson they actually much much much better the human players but then do one thing the observer watching the videos is that humans are terrible at betting correctly and actually finding the Daily Double and so that was actually the that entire team working on two strategies a betting optimally for the Daily Double but you can actually solve for exactly and be the other thing was actually to find the Daily Double so Daily Double is always hidden somewhere and what they did is they actually analyzed all these past videos and they realized that that's the distribution is not uniform at all and so they actually incorporated that and bought some strategy and so what's that a much much higher chance of getting the Daily Double and then actually capitalizing on that's actually exactly okay um who was this uh suppose you yeah nine phase four forty all right I we just had that one sorry Kanan for forty all right why does Kenny all still perform well on high dimensional faces and handwritten digits data who was the first one you were the first okay that's right that was thirty forty right so the answer is the data at high dimensional but that actually is not what matters is the dimension sorry that the the space is high dimensional but the data actually intrinsically is not high dimensional all right you can even take a low dimensional data and just add representatives I'm really high dimensional space make everything else zero then just rotate the space arbitrarily then it will look like the data is really high dimensional when it's really not right so K nearest neighbors won't change okay general over 40 all right which algorithm would you expect to result in lower test time classification error ten years neighbors or linea SVM a on text classification by topic be patient spinal signs given the patient's vital signs predict the health status your fifteen features when C sixty thousand handwritten digits calcify the squeeze a business eight so on each one of them which one do you think will do better SVM or K nearest neighbors yeah let me do the out of water okay Susie SVM SVM KNN well okay tell me why do you have a good reason then I have that count right I'm with you I'm with you the questions be so he has a fear so the point here is that this is very low dimensional data right so typically a low dimensional data linear classifiers do not work well right just because there isn't just much much space you know many options to to place a hyperplane so the first one definitely linear classifier text documents a really high dimensional right you always find a good no they just a linear classifier is amazing at this and but the other two typically are not very high dimensional and tend to have nonlinear decision boundary so kanan tends to work better how about this I give you I give you 30 I give you 30 points they give 110 to the grad students all right no no no not the first one is a little bit sorry are you a grad student I want him to be a tight race all right I just leave it at this I just say you know I feel like 10 points [Applause] okay okay we already have kind of out of tummy if you do one last question so there's the last one okay you didn't ask last one KN for 30 all right true or false and nearest neighbors on a data set with n training points and D dimensions takes order n times D computation true is a tricky question [Laughter] this is set up the discretion to set up yeah yeah exactly right so yes yes what it is right so the because usually you accept me right so usually the computation is K times K times D so it's n times D to comply the K nearest neighbors actually past the bookkeeping of what the K nearest neighbors are which is some some little tree heap tree but XE if you take all the points as your training as your neighbors then actually all you need to basically always predicting the most common label so you actually don't need to look at your data at all right it's a cynic a surprise there's a grad student grad undergrad or grad right okay good well it was a tie [Music] 
","['', 'L1 regularization', 'L2 regularization', 'Sparsity', 'Optimal solution', 'Ball', 'Complexity', 'Overfitting', 'Generalization', 'Regularizer', 'Feature selection', 'High dimensional data', 'Weight vector', 'Zero weights', 'Lasso', 'Elastic net', 'Squared loss', 'SVM', 'Logistic regression', 'Training error', '']"
"hello everybody so my initial plan was to go over the last year's midterm today but then TAS were like well the students are gonna do this anyway so typically we don't like this when teachers do this so I asked them what they would think is most useful and they said why don't you just go by every single lecture for a couple minutes and kind of say what the important points are that's that's not a good idea you wonder if deputy things I posted the solutions to the jeopardy quest exam so one key question is we could still go through the whole class forwards or backwards in time and the idea being is we ran out of time at the end we either missed the last lectures of the earliest lectures any preference let's have a vote who wants to go through it so the advantage of going in chronological order is that it's easier to kind of say how things build on you know there's actually I mean there was a reason I ordered it that way on the other hand if you run out of time he wouldn't miss the last couple lectures which may be most relevant or most maybe the most uncertainty about because we haven't had homeworks and on the other hand the others that's the reverse so let's have a little boat so who wants to go through it in chronological order raise your hand and who wants to go in inverse order it's a tie okay well that is tricky okay first time my life that he actually had a tie now I don't know what to do what's the question ok good ok these address we go through chronological or probably we try to be really fast so how many minutes we have in 45 minutes how many lectures do we have 10 right ok so you have four point five minutes after your my time man so you can you know every time put the four point after four point five minutes it came in four minutes and thirty seconds pay me to move on and I try to go to the first one so faster so this way we can save up some time okay please and every single topic feel free to ask me questions so that the at the beginning we talked about the general machine learning set up and sorry I don't have big chalk today actually I apologize I know somehow someone stole the big chalk so I have to buy some new ones yeah okay um and yeah yeah so so this whole class about supervised learning and in the beginning basically just talked about the general framework a few important things is the data set that we draw as if X I Y y1 and so on up to xn yn right they are drawn from sun distribution P and this distribution left on iid right so each they're all independent from each other and they're drawn from the same distribution and but typically goal is to predict by given X all right and so in order to evaluate how well we are doing and to train algorithms we split the data 50 into typically three three data sets we have the training data set we have the validation data set and you have the test set and so the idea is basically you take your train data set and on your train did I said you train your algorithm and then on the violation the I said you can estimate what the errors and the validation is that because you didn't touch it that's an submit of the true error right so ultimately what you care about is the expected error what's the probability that if I would take a new point from this distribution P what's the property that I get it right right or another way it put another way what's the expected error I would get if I draw a point from this distribution P and so you can't measure this because you don't have access to the distribution P but you have you know more data than you need right so you take your data set D you reduce it you only train on some of it and then these remaining points they're basically drawn from this distribution P right so we can just use them to evaluate how well your algorithm does and so all of ideation says did you now compute the error can anyone tell me if I have any validation and test yeah so my vision is there as you make sure that you don't have a fit of it that's true to the trained ear set but why why do you even need split off one more data set yeah that's right so you typically you evaluate the I wouldn't another validation data set then you realize darn right the error is too high all right so what do you do you try find ten different algorithm let's say you had the perceptron now you get an SVM or something all right then you train again you evaluate and then you see how that doesn't work right maybe my C was not large enough right let me increase C so you run it again test it here right you tweak your hyper parameters until you're satisfied right but now you've looked at this thing I said so many times but this is no longer actually an unbiased estimator of your true test there so that's why you you split up another data set a little bit of a test data that you really only touch once right so that's the idea so once you're confident here this is kind of a sanity check right so let's say that you know you need an error of 1% right that's what your boss told you the first time you run to get the error of 2% that's pretty good but I have to go get it lower so now you tweak your parameters to get air of 1.5% right you tweak your parameters again change the algorithm a little add some features to something else right now you get you know one point one percent right there you keep doing until you get with 0.8% that's pretty good then you test it on test it I said you get 0.9% that's probably what the truth right is right does that make sense okay so you know please keep in mind that the validation error basically you know these these two are basically you know meant to be estimate of the true expected error in the test error if you dataset is really me Raj is actually you know approaches exactly the test the true expected error that you're trying to minimize the just also called the generalization error the generalization error is the expected error for new data from this distribution okay so that's that's kind of just the set up and just make sure you Yuri truly understand how does it stand and by that that's done okay any more questions about this general setup then I move on to Canaan n yeah training error is always the error on this data set after you train okay it's after you train right so basically and typically that's much slower than your testing huh alright so we know for example SVM right if the data is linearly separable it will always find a hyperplane that you know has zero error right but that actually may not be in general the case okay any more questions okay good so then he went to cane years neighbors rights okay nears Davis is one specific algorithm and actually let me just just talk about the different perhaps I wouldn't let me learn so we have this distribution P over X and y and so ultimately we want to give it an X we would like to make a prediction over Y right so that's exactly this distribution P of Y given X right so that is if you have this distribution then it's easy you can even tell me what you do when you have this distribution what classifier to use yeah use the Bayes optimal classifier right so that that'll be the optimal choice to do once you have P of Y given X so you know one way to do machine learning is try to estimate this probability distribution and then you know then but the Bayes optimal classifier does just picks them you know the most likely Y for a given X another option is to learn a function f of X right which actually approximates exactly this so as I said this is Arg max Y P of Y given X right so base is hanging in this case I don't learn a distribution maybe it's too much work to actually learn I really just a distribution over the label really all I care about it's just the one label that's most likely so a lot of algorithms also just then a function of these is given X tell me which label is the most likely name right so if you have this of course you can always get this function but for example K nearest neighbors doesn't actually give you a distribution it just gives you a function you can make modifications to get a distribution but typically it's just this this cause discriminative learning that's also generative learning and gentlemen of learning instead you learn P of X given Y and then P of Y and that ultimately the same thing if you have that then you can just use Bayes rule to flip it around yeah so that they are this right there this compare this absolutely yeah I mean you never learned what's given the labor what's probably diff is up next right you never learn that at all this human of learning does not mean that you're not doing probabilities right you can learn a function that actually gives you probability of Y given X and you can chain SVM actually very easily into an algorithm that returns your P of Y given X thus cumulative says you you stick it in put X in and all you learn is the the probability of Y all right so thinking about the following write a discriminative algorithm given all measurements about a human will tell you is that human male of female write a journal of algorithm would tell you given the gender generate a human right what are the feature representation of a human right well SVM doesn't do that at all right does that make sense yeah no perceptron does this right so this gives a hyperplane if you on this side is positively otherwise it's negative there's no notion of what's force given by what's a certain yeah sure sure let me do the following let me say discriminative generative and then you also say parametric nonparametric okay so let me just quickly talk about parametric was nonparametric what's parametric was nonparametric and I grew this parametric if you have a finite number of parameters that you're learning that you're fitting from your data so the number of parameters is the same no matter how much data you have even term the example of a parametric algorithm yeah perceptron right so you've easily happen number every dimension of W and B those are your Perret parameters right then you take your data and you learn these parameters a nonparametric algorithm is something but there's no fixed number of parameters you're learning the model size how much you're storing grows with the number of theta you have typically any one time an example of a nonparametric algorithm yeah K nearest neighbors right so K Gnaeus neighbors you just store the whole data set and so you know the amount of storage that you need grows linearly actually with the data set all right that's not the case for SVM perception that you've you know upfront how big your model s that's it right if you get more data you're just going to learn a finer ma sure okay let's just go through them K nearest neighbors where does it sit that's okay nearest neighbor this year right what's nice a perceptron someone else not author parametric is committed right perceptron SVM nice base I'm sure someone said it generative and permitted right there have them all I'll just take your question yeah where's that it's same thing logistic and of course like you know lasso and elastic net and so on so all these linear all these erm algorithms fit in here yeah yeah yeah yeah yeah now you're getting fancy yes you could do that that's right so what he's saying is he could actually take the do naive Bayes keep the data around right and then actually do nonparametric kernel density estimation or something entity estimation to estimate the probability sure you can do this but but everybody else just forget what I just said yes okay good okay you have to move on to the exact okay good so well this was kind of in between it's kind of across different lectures so let me cover 10 years neighbors the gaminess name is a really really simple algorithm we just classify everything you know by a majority vote of heath nears a if it's nearest neighbors the key is that K nearest neighbors relies heavily on the quality of the metric right so one thing you have to remember is what's the assumption of K nearest neighbors similar points of similar labels the metric has to reflect this assumption right if you have a bad metric so I said actually the you know the measure similarity in some way that's actually not compatible with the label it breaks right oh it's not you know correlated with the label and then we went to that so that the second one thing you of course have to know is how the proof works with you know why does K nearest neighbor why does one year's neighbor you know why is the error upper bounded by twice the Bayes optimal error it's relatively simple two-step argument and then there's the curse of dimensionality the evasive States actually you know in very high dimensional spaces the notion of similarity is somewhat you know can be bad it defines alright can be ill-defined there are data points no it typically happens that you know everything is really far away from everything and there is no notion of similarity anymore oh and put another way you would need exponential amount of data to obtain a local local similarity and then of course the question is why does it still work on images and digits and the answer is because the data is actually not extrinsically haider intrinsically high dimensional and it's just lies in the high dimensional space you have a question yeah right cheeky answers of K nearest neighbor still works intrinsically low dimensional you you can test it so if you basically look at the nearest so one thing you do is you for every single data point you look at the K nearest neighbors for some data points do with the K nearest neighbors and you basically there's a bunch of different steps you can do you could just do local PCA I don't know if you've seen PCA before so to actually estimate locally right that's design a low dimensional space globally it doesn't right so I mean if you do PCA and you realize global Eli's non dimensional space you just have a bad representation but it could be that locally it's low dimensional there's also something called the doubling dimension the babies there's like as I keep doubling a radius around my points like how many more data points do I see in this so I have a data point I say well but then the radius are how many points we have inside this and now if I double our right how many more points do I have what if the data is two dimensional right then it's going to be four times as many points if it's three dimensions gonna be eight times as many points right see connects to compute the local dimensionality exactly it's EE it's called the doubling limit okay any more questions yeah a parametric are with with generally passed an unpromising algorithms yes absolutely yeah that's you know there's always exceptions because some fancy stuff but yes typically that's the case yeah sure but the number of parameters that you're learning does not grow if I make you dataset ten times larger the final model that you're storing is not any bigger all right for every dimension though you know busy just have some some description of the property the parameters of that distribution for that dimension so name them same number of parameters KN if I give you more data you store more data so the model is the data right so you X you have to store more you know you need more storage that's right that's exciting right okay good perceptual algorithm so the important thing is essentially the perceptron algorithm is a that is linearly separable if it's not section I present X all right so you have to understand what happens when it's not linearly separable it loops forever right and you have to understand the update what what does it do right it looks at one data point at a time and then if it's positive it just adds you know if it's misclassified if it's correctly classified doesn't do anything if it's misclassified if it's positive it adds it if it's negative it subtracted really simple all right it keeps looping over it the beautiful thing is it converges in a finite number of steps you have to know how to prove this right and what they're bound this the second you know little nugget in perceptron is there's this bias term how can you get rid of the bias term you just add one additional dimension all right and just absorb B in the vector all right why is it scale-invariant we talked about this as well right so you can just rescale the data set and that was a little trick to make the proofs if that doesn't change anything about the convergence any more questions about perceptron otherwise they move on that seems pretty easy okay yeah sorry yes that's what should be it was a typo yeah that's the why the why that's a either plus 1 or minus 1 so yeah ok ok so then we talked about so after the perceptron we went into your back into the setting of AZ said well you know one way to do machine learning is actually just to estimate this distribution P of X comma Y right so once we had that distribution we could just use the Bayes optimal classifier right and so the first thing we did is we just talked about how do you even estimate you know distributions from there alright so and so what we talked about is maximum likelihood estimation MLE and we talked about math alright so this is the two ways we in this class estimate distributions from data and so the way typically works as we assumed this distribution P has some specifics on some specific forms or could be by normally a multinomial etc distribution and then that has some parameters and then we fit these parameters right so it means the adjust the you know the parameters of the distribution such that actually it fits what we observed in our data and what does maximum likelihood estimation do it basically says what's the problem you know maximize the probability of the data given the parameters right so these data here are parameters of the distribution and we say I want to make my parameter such that you know the data that I observe becomes as likely as possible some impose with the questions like what's the difference between probability and likelihood and so the way it's used in this class is that likely the propertied is really the same thing it's just you can't really say what's the probability of a data set right if you have a trained inner said you can't see what's probability of the data set does make any sense right because theta that already exists it doesn't have any probability it's like asking what's the probability of me I don't know right I'm here so so what do you say that's the likelihood right that's something that already happens right so in hindsight kind of how likely is it that I've seen that thing that I'm actually seeing so be distinguish between this by saying either probability of something that hasn't occurred yet and likelihood of an event that has occurred you just retrospectively analyze the the likelihood that happening okay so mle just maximizes theta Z maximize this given theta and map goes the other way round is Emily and map says what's the probability maximize the property of theta given the data all right this is what what map and so there's a crucial step here it's very subtle but in this case actually theta God gets promoted from just a you know set of parameters to actually a random berry so we use a and the views Bayes rule to make this P of D given theta times the property of theta divided by the normalization this year the p FD given theta is the same as here that's why when you do map with M le you almost go through the same steps right most of the time because you're maximizing this so you maximizing des right so when you take the logs that's always the same part plus some stuff on the right right when you do math right and that plus some stuff almost always turns out to be a regular bat right because it doesn't actually involve the data and only it involves the parameters right so you basically get the norm or something over the parameters right that's basically where these regular rises come from that's one way or to derive them any questions about Matt versus Emily then very briefly we also saw the what's the true Bayesian way of doing this we didn't really get into the specifics but you should know roughly what it is and that's you know in some mazie saying for a given Y in a forgiven X what's why you know what's property of Y and the way this is can you eat this still is it's too small Razia thumbs up okay good so basically what we're doing here is you say okay well we assumed you have some model but we don't know what that model is we integrated all possible models you say what's the probability of Y given X and a certain model and then be saying what's the property to have this model given our data right and so this he has a test point yet there's my test point that's point and then be in GPL possible I think so this year so basically think about this integral right so you basically say given my data there's some probability that I end up with a certain parameter set what map does it just says what's the most likely one all right what we're saying here is there's many different choices and we sum them all up and each one of these models give us a different problem addiction then we average them out and the way every one of them by the part by the likelihood of that actually that happened so in sometimes ebz say yeah they compute this integral and say you couldn't want to have a baited average of all models or a map you just say I just take the most likely model and return that answer right so this is kind of the the pragmatic Bayesian approach and ideally you know what you want to do is the Bayesian is this this but you could you're typically can't solve this integral is very very hard so what you do is it is just math right the way it's often done in practice is you just evaluate this you know for maybe a couple thousand values of Thetas and then you average those it's a sampling approach okay good any more questions about Emily map page mr. speaker test and we can move on all right naive Bayes so now that we know how to estimate probabilities the idea was well can't we just now estimate P of x given Y or P of Y given X right can we just estimate this from the data all right so that was the idea right awesome let's just do this maybe a dot right you know go home the problem is this doesn't work then e1 tell me why this doesn't work what would you need for this to work you want to estimate yeah you would yeah that's right you would need you know busy an infinite amount of data an exponential amount of state to estimate right so so basically what you would do is you would look through your database took all the X's that match exactly this accident see how often did observe this particular Y right he's basically he's right after every single test point I must have seen during training already ideally multiple times such I can actually estimate what's the probability for a given label right and that's completely infeasible so we can't do this so what does naive Bayes say say well we just flip this around we say this is P of X given given by P of Y divided by some normalizer and this doesn't change anything doesn't make it easier but this is you know and obviously this thing we could estimate that's really easy we just count how many times to BC each class that's easy read this thing here I tell how many spam emails tell me not spending less we see this thing here is also hard but we can make the simplifying assumption that this year factors it's a P of X given Y actually equals you know into conditionally independent so we can say this is you know product of alpha P of X alpha given Y doesn't have to be exactly this actually can also just be a function to be honest it just doesn't have to be you all the probability so I've seen the multinomial distribution these are actually if you're very very careful you realize the donation normal so it's just functions but doesn't really matter like if you just say these are conditionally independent that's exactly everybody understands what you mean that's correct any questions about naivebayes yeah is any yeah okay so the question is you know I mentioned multiple times in lecture that this is typically not true right and that's correct so why does it still work in practice and the the essentially well basically it's not true but it's all it's not totally off right so if your distribution is okay all right then or it may not matter all that much right for emails for example the word order doesn't really matter right so that's maybe why it's most wrong right it ultimately turns out it can easily detect if something is spam that even if the email was written backwards in any particular order I think that's the best that can can give you basically right and that base is over a strict assumption yeah good quick yeah oh well so there's two things right so the first assumption you're making is this the second assumption now is one of these factors right and typically we say these are now independent probabilities for each different dimension so each dimension now it's basically not estimate that individually and now we make the second assumption we're saying well what is that distribution right before example say that's my motor normal Gaussian or something typically that's where really breaks down right so if you assume a Gaussian distribution but they're not Gaussian distributed right then horrible things will happen right and this is exactly someone actually posted that question of Piazza right if you have data points like this and so that's if you have one more cross here right what is naive Bayes do right so naive Bayes now looks at each dimension individually so this two-dimensional thing there doesn't matter for a face just look at the first dimensions will project everything down here all right so these are all the points basically projected down so I just collapse just project them onto this this x-axis and then I I feel like you know it's a I'm chipping gouge distribution I fit a Gaussian around the circles after a round stuck out on X right and now comes the crucial part I'm not this you know once I have these Gaussian fitted I don't look at the data ever again I just use the data to estimate the parameters of my my distributions so now I'm you know I do this on the other side as well so he actually have my my you know actually I forget these distributions here right and what about you would will actually find a hyperplane to basically separate this these two distributions which you know will look something like this if you put them back together and it will find a half I breathe here right so despite that is linearly separable and actually not separate this point correctly because once I estimated the distribution that guy was such an outlier basically you know I basically distribution the best have made it still here and so that's typically where things fall apart right so if your data is that actually is not Gaussian stupid or I know not would know me or something that's a bad estimate then you would get very different distribution that your data and then actually your classifier will just do the wrong thing right and a good test could be if you actually take these distributions and use sample points from it do you get stuff that looks like your data right if your assumption is right then you will get exempt it looks exactly like your data nice if you now get points so you know in this case you would get points here but you would never get that point right that's really really unlikely right so that's that's kind of where things fall apart right so apparently and your data said you actually have a little bit of probability max here as well someone asked when does logistic regression and naive Bayes get the same result right and that's basically so just take a question and that's the next lecture now says okay instead of actually estimating this V me you know estimate this right but use the same same form for this that we actually get from naive Bayes right which is this 1 over 1 plus e to the minus why W transpose X so this is my W in this case WUSA does the same thing right so logistical question fits the points naivebayes separated the desert aggression separates the data the data points naivebayes separates the distributions which it pit on the data that's the crucial difference right those two give the same result if your data distribution actually fits your data because that is the same thing right if the data is actually drawn from these two distributions then doesn't matter if I'm separating the distributions or if I'm separating data points drawn from this distribution that's the same thing as long as n goes larger becomes large enough right so asymptotically as n becomes very large if the data assumption holds that means nice base assumption holds and my modelling assumption for distributions hold those two I breathing's are identical and often they are not and therefore in our logistic regression actually you know tends to work it tends to work little better yeah yes yes good question so is there ever case when I face is better than the logistic regression the answer is yes and typically it's when you have very little amount of data they are very middle a little amount of data then actually logistic regression can easily over fit to the few data points in half right but so you basically have some distribution here but you only have very few points drawn from it right so it's really not clear what this distribution looks like so what now phased us is if you actually bite by giving the distribution a very specific form by saying it is Gaussian for example right or by saying it's multinomial or something like this right you are introducing additional structure into the problem and if you're right about this then you're helping the learning algorithm converge rather very good converge better give better decision boundaries right so if you have very little data that additional knowledge will help you right and you will get a better classifier if you have a lot of data it would hurt you right because if you have a lot of data now what you're doing is you're saying this has to be a Gaussian form but if you've sampled a million data points from this right if there's no point why do you have to restrict it to be a perfect couch right because just had the data speak for itself right so that's the advantage of the of the logistic question so typical even you have a lot of data logistic regression does better if you have very little data it's took advice to use ninth base yeah and then they condition yeah because this this is no longer are you there's no longer but nine piece month yeah yeah how would you ever use of this regression oh as a the question is why would he ever use the destroyer aggression over SVM one thing that's really nice about logistic regressions gives you well-defined probabilities this SVM doesn't as the end just gives you a prediction right which way on the hyperplane Eli on some people like combine both SVM's sometimes give you slightly better prediction performance they'll just see aggression and so what people do is they take the output of SVM's it's just one one feature right like this the prediction of the SVM not where you lie on but the W transpose X plus B from the SVM that's one dimension for every feature ever every data point and they stick the atom to a logistic regression classifier and train one more time but then you can also get well well-defined probabilities for SVM's that's actually that's that idea it's actually called Platts game so it was invented by John Platt and it's become very famous actually the famous algorithm yeah good point so is it again yeah multinomial also that's right so all this it makes it something that's actually from exponential family yeah so code emoji nope not multinomial or Gaussian right and by using naive Bayes you actually make that specific so you're putting in more information which helps you if there's very little data if you're right that's basically what it but that's also by logistic regression is more flexible if you see a lot of data yeah why does it give value find probabilities because that's what your estimated right you're estimating P of Y of X W so you're basically you know you're maximizing the the the likelihood of your data so SVM there's no probability at all right at the end you just see either on the right side the positive side or in the negative side there's no probability at all so the downside of SVM is it's hard to read certainties from this right just tells you this is Cass one discuss a minus one but you can't really tell if it's really sure about this so if it's not really sure about this so if you have a gel self-driving car it tells you this is a pedestrian right you don't know how sure it is about this right there's not a pedestrian right maybe it's 49 percent sure it's you know yeah you know fifty one percent you is not a pedestrian right you may still want to slam on the brakes not to run that person over right so that that's why these probabilities are important to this for decision-making yeah you can but actually that's not part of the optimization problem there's nothing in the optimization problem that tells you that points is a really far away because a scale-free if you have two different SVM say right and you compare them it makes no sense to compare basically right because it's the scale is just conveniently sets this is the optimization problem becomes easy not that its interpretive all yeah any other questions all right okay good if you've gotten logistic a question yeah for the acid so it works surprisingly well on all kind of data all right so if it's just you know it basically finds a linear decision boundary right so if you just look at in the ERM for a forum it's just another way of defining a loss function yeah okay so one thing we don't know just take a question basically we minimize this directly and then we realized once then we realized to be defined this we actually should be put on the MLE we get a function let me want to minimize but there was no closed form so this was kind of you took a little detour for a couple lectures Frazee say if you get a function how do you minimize that function right so in this case you know there's a few things I told you about this function a it's differentiable it is content and it's come back so that's nice so that means we can do local Hill Climb right so the function looks something like there's some salad bowl shape thing it doesn't have to be symmetric and so and we want to find the minimum that's the maximum likelihood estimate for logistic regression and so we talked about a few little algorithms how to optimize this so you start with some points to P you say W is all zero but it doesn't matter it can be anything you want and then basically what we do is we say well we have the form of this function we just don't know where its minimum is so what we do is we do a Taylor expansion it's a Taylor expansion either becomes linear so if it's linear and it's really easy to figure out which way to go to go to reduce it and as long as your step size is small you can actually this approximation is not bad so you take a small step and then you get Allen up here now you again estimate with the linear function you take another step and now again you estimated right and you base the every single time you just take the gradient and go in that direction that's gradient ascent Newton's method goes a little bit it's a little more aggressive what it does it fits a parabola and it says the quadratic so base it takes a second-order Taylor expansion finds the minimum of this jumps to it and that does this repeatedly the downside of Newton method is so the upside of Newton method is it converges very quickly few steps in your convergence maybe 10 steps the downside of Newton's method is that there's no guarantee that you're converging right because these steps can be pretty large and so that's the problem but the gradient descent you can force your way in steps to be really really tiny but yeah Newton's method the nice thing is these steps are very fast that's the good stuffs are really large so in a few steps you at the minimum but it can also be that you're actually unlucky right and the you know the parabola you're fitting looks something like this and the minimum was over here and then you know at this point your function is really large and so now you're diverging very very quickly all right the second downside of the Newton's method is that you have to compute the hash which is a D by D matrix and can take up a lot of memory and can be very very hard to compute and expensive to compute and then you always have to invert it so inverting a d by d matrix is order D cubed so that can be very slow yeah as I explain one more time yeah yeah yeah so you can test and you cannot really go back and go back to gradient descent that's what progeria period people typically do so they do a first order the Taylor expansions the gradient descent until they get bored then they do Newton's method and if they if their works out there it's great and if they diverge then they just undo this and go back to where they started use methods you know they've saved that that vector beforehand and just keep doing green descent and let's try this again a couple times that's ok see just do a step it mi Rae's do Newton's method yes great right no all right never mind let's made a few more steps make sense ok so yeah so many conversions that can be way faster way will be way faster so if you day that's not very high-dimensional the questions that that could be a typical exam question when do you want to use one remember you when do you want to use the other right so if your data is not too high dimensional right it's a fifteen dimensional data well 15 by 50 matrix is nothing takes you know a couple of milliseconds to computer and work right no problem at all right and now you've done a few steps right but you might have many many data points so computing the gradient every single time at Z if you have 100 million data points right but the only 15 dimensional computing the gradient takes a long time right comparable comparison computing the Hessian right I mean you know that takes the same amount of time you go over the data set right but and inverting the Hessian is nothing right and so if you just do 10 steps of Hessian inversion that's nothing compared to taking maybe you know 5000 gradient steps right you have to go over your data set five thousand times so okay then we also talked about autographs so don't forget autograph so one thing about gradient descent is that you have to have the step size and one thing the autograph does is basis that's a different step size for every single dimension and it sets it automatically it's very neat that way I guess you all did it for your homework so I guess you know pretty well at this point yeah are there cases yeah of course but these are very very large you know I'm sorry if if B is larger than N or something right so then actually get some invertible or not invertible matrix yeah that can also be the case that's right so if actually that's not a reason not to use hashing and other knows not to use utens method one thing you can do we can approximate Newton's method by just computing diagonal off the hessian and that's easy to invert right and that's very easy to compute it doesn't take much memory that's a very common trick but that's then similar than a diagram that you actually basically have a different step size for every single damage all right then with a linear regression so then here regression in some sense continues everything we've done you have data and we try to find a line that's a that's best explains the data we just said the noise here is Gaussian so then the moment we have this couch noise we can actually use our framework MLE or math and what we get is just the squared loss if you use Emily or we get square loss with L to regularize ever me do this math so it's very simple and that in some sense then alright so now I'm did run out of time the last couple things so SVM any questions about SP ends so SVM's SVM say how do you you multiply the maximum margin hyperplane and one thing we showed is that basically you know if we do a little bit of math and turns out the margin here becomes exactly 1 over W over the norm of W so what we can do is we can just maximize the norm of W aw so minimize know of W this way we maximize this term here and we maximize the margin right so the SVM then says maximize the norm of W and then subject to the constraint at all the points on the right side a hyperplane and exactly one off so make sure you know how to derive this constraint right it's just a few lines of derivation yeah yeah yeah no because because then actually some points move in here right and so you basically say these are basically absorbed by the slack CBDs saying the margin minus the slack is 1 all right so this here AC would then have some value here on the side i which you assigned right see busy saying now we have a slow soft margin the margin basically is the distance of the point you know minus the amount of slack that I allow this point to reach to get inside the margin that's the definition does that make sense okay good and then the ya why can we not observe the be into the W vector very simple because we have the constraint uyi W transpose X I plus B greater equal one right so here we could just absorb the B into W right wouldn't make any difference right we just have one more dimension we have one more mention here here it would be a problem right because what are you doing here what is this this yes you actually maximize the square this here's the sum over all W alpha squared right if you now absorb the be in there you're changing just the definition here but then you're also adding a term B squared here which is a different optimization problem yeah so here okay good guys you've got me and so so typically when you use the district regression use it on very high dimensional data often you don't even need a B so you just drop it keep it to stop it yeah that's really the short version of it all right but an SVM the nice thing about SVM is this very nice definition of finding the maximum margin hyperplane that's that's what they are you know trying to do and that that's what this doesn't do anymore but you're right but the holistic aggression the kosher way of doing it it's also not to not to do this if any when you use regularization at the moment you do regularization you have to be careful that you don't regularize you be yeah this was to save paper oh yeah it may have been a mistake yeah so typically there's a peon yeah okay shall we stop I guess we're four minutes over right okay good luck everybody eat fruit and look through the notes and look to the past exams 
","['', 'Machine learning set up', 'Supervised learning', 'Data set', 'Independent and identically distributed (iid)', 'Expected error', 'Training data set', 'Validation data set', 'Test data set', 'True error', 'Generalization error', 'Hyperparameters', 'Bias', 'Regularization', 'Nearest neighbors algorithm', 'Maximum a posteriori (MAP)', 'Bayes rule', 'Naive Bayes', 'Gaussian distribution', 'Linear regression', '']"
"we will start today with a very short announcement by robotic systems and we're looking for so if you're interested in to talk to us find out more about what we do or please join us know awesome thank you [Applause] all right I hope you both survived exam looks like actually a lot of empty seats today we were talking about the most important topic in machine learning so there's two reasons you should really really pay attention today wait because that is super super important they will transform the way you think about data science so if you understand today's topic it's the bias-variance tradeoff is what I would usually have as part of the midterms last lecture midterm because the snow-day I we didn't get around to it the bias-variance tradeoff is what distinguishes like children from adults it's the difference between people just play around with algorithms or people really know what they are doing so please please please please pay attention today well the next couple weeks will all be from viewed from the lengths of the bias-variance tradeoff if I succeed it will shape the way you think about machine learning and data science the other reason for a practical reason why you should pay attention is because you're rolling out the next project in a couple of days and we will only give you one week because we don't want you to do it over spring break so don't want to ruin your spring break but the project is all about today's lectures so if you pay attention today it's a really easy project if you know what you have to do okay so bias-variance tradeoff the big picture of the bias-variance tradeoff is that what we will look into today is the generalization error so far but what we've done is go with minimize the training error and basically hoped that what we do will lead to a low generalization error today we will look at the generalization error and decompose it and understand where it comes from and if you understand that then you're much much more informed to actually make good decisions about how to get classifiers to generalize well okay so the setup is the same as usual we have our data points x1 xn yn and so just for because it's easy that holds it holds generally what we are doing today but it's the derivation is much easier if you do in the regression setting so what we are saying is why I is element of r okay so we have a regression function that's if you want to pretty cow surprises or something all right so the first thing is that V and today's gonna be a little more theoretical so you have to pay attention not a nod to get lost but it but it's well worth it so these data points x and y are drawn from some distribution P right so V FP of X comma Y and how do we get this data set you get this by drawing endpoints Rapinoe iid from this distribution right so that's basically assumption that's about all of machine learning hinges on right so that's the for example you take any emails or something in your distribution is the distribution of incoming emails right so so these are basically you know the emails in your inbox okay and we would like given actually like to predict Y now one thing is important is that for a given vector X there may not be a unique Y alright so for example X may describe a house right and so the features could be how about the square footage is how many bedrooms you have and I don't know how many stores there's up or whatever right and say you have 10 features of this house well it could be an identical house you know a house sorry a different house it has identical features right that gets sold for a different price right so for Jordan X we always have a distribution over Y alright so that's basically just if you factor this you can just say this is P of Y given X you know P of X I think this year it's busy for certain X what's the probability of Y okay so the first thing we will do today is just remind you of something that we've done before saying for certain X you know in terms of regression what do you want to predict in classification we had this Bayes optimal classifier what do you want to do in regression and so I claim you you know typically a good thing to do is to predict the expected label so saying we call this Y bar of X so Y bar for XYZ says oh sorry go so for a certain X this is given X right given a certain X which why should you predict there's many different choices and what you want to predict is the expected Y right and so it's just the integral of all possible wise times the probability of seeing that Y given X so let me just explain this one more time so given a certain X which why do you want to predict it's ambiguous right you could have a feature vector that describes a house and you know sometimes it's maybe worth 550 a thousand dollars sometimes seventy thousand dollars who knows right and so you know a reasonable thing to say is what you want to predict is you know the label that I would expect right so if I would draw basically infinitely many houses on average I would get the following the following value which is also another way of saying I just integrate out all possible wise and weigh them by the probability that I see this Y given X okay any questions at this point raise your hand that makes sense okay awesome you've still with me awesome good so we call this the expected label we're just going to talk what I'm gonna do now is I'm going to set up a few things we'll put them all together right so you just have to remember what these things are this is the expected label so that is what we want right given X we would like to have the expected late okay good now we can go on so let's say we have this data set D and what do we typically do we have machine learning people all right at this point you know you're one of us so what do we do we have some algorithms a machine learning algorithm and what is it I wouldn't do he'd be sticking the data set the trainee has said and outcomes some function H right because it's the hypothesis so we call HD HD this is the classifier that we're learning this is a function that takes an X and outputs some value by and and what is this this is the output of some machine learning algorithm a that takes this input the training data okay so for example this could be a perceptron right you stick in the data set D and outcomes sound classifier which in this case is a linear classifier is just that that's defined by the choice of a let me use either it could be an SVM or something any questions all right so this is what we do in machine learning right Beasley come up with these algorithms a that's what I do all day okay so given that we now do this so given that we now have a training that's at D that we drew from this distribution P and we have these n data points you know stick them in our algorithm we get our H we can now say well what is the expected error that we get right so this is the generalization error this is what we're interested in so we can measure the training error on the data set D that's the training error but what we really care about is the error that we would get if you see some new data points right some new houses some new emails etc so that's the expected test error and now here comes an important part expected test error given HD and this will become you understand in a second while I am emphasizing this so much right so given a specific classifiers you train your SVM now outcomes some classifier H why does the expected test error given that classifier and that's not very hard to compute right so that's basically saying okay well I'm just looking at any new X Y that's drawn from this distribution P and I measure the error let's just use the squared loss so we say that is basically the expected value if you draw x and y from P of HD of X minus y squared all right any questions at this point yeah question ok anybody else yeah y square loss because for regression that's a very reasonable loss and it's also the the proof that I will do in a few minutes is a lot easier when you do the squares yeah that's a good question yes any other questions yeah not if you look at the what I'm saying in principle is true it doesn't pop out as nicely what I'm showing you but yeah we'll get to this in the end you can ask that question again yeah okay any more questions yeah that's that's right yeah so they're there yes so this is basically you know this is the generalization error given HD that's the important part right so for a specific classifier right this is the expected test error oh yeah we can't compute this there's this you know we're doing a little theory right now right so this will this will become practical in a few minutes right so that's just yeah mechanics this would be to estimate this you basically sample into the many points and then evaluate it of course you can't do yeah you kind of lose okay but this is what we would like to minimize and well how do we compute this this is just the definition of X they should we just plug that in so we just integrate over all X we integrate over all Y and let me say H D of X minus y squared dy DX that's that's that's the same thing this is just you know my son is doing that right now he's in second grade all right and she's not all right so any questions yeah I said he's in second grade that's why he's here we go good point X comma Y yeah thank you any more questions any questions okay good so this is what we would like to understand better right there's always been looking at right now but actually let's just take a step back all right let's take a step back for a second we have this data set D and the status that G is drawn from the distribution P and now to get our H and now you want to know the test error of age all right what we may also be interested in this what is actually a test error of this a-here right so if you basically just you know design and algorithm right and you actually want to argue about this in principle not for one particular output H so let me put this another way H is actually a random bear so we have D like each one of these X and Y is a random variable and we draw n of them to get the set D so the set D is also a random variable it's a set of random variables so it's itself there at random variable and we stick that into some function let's assume now it's deterministic then they get some age ages a function of a random variable so it's itself is a random variable okay does that make sense raise your hand oh that makes sense okay awesome so there's some distribution right if you would do this over and over again take a different trained ear set D drawn from the same distribution you will get a slightly different so there's some distribution over functions so what we can do is actually let us compute the expected test there we can also compute the expected function H that we would get right and that's the following the expected classifier we call this H bar is the following if you just take the expected value of these data sets D which are drawn from P to the power of n n times drawing from P and let me just call that algorithm right so basically that's you know we call this a of D and that equals you integrate out all possible training data sets and let me take HD probability of D DD can anybody tell me how would you estimate this Mouse how would you estimate this value any ideas yeah and average the outputs right so basically that's exactly right so today's the average the HDS that you get and so you know if you have enough data sets that it becomes exactly this that's the weak law of large numbers okay any questions so let's Paisley's s this quantity what's H bar H bar is basically if I had infinitely many training sets like on average let this for every X this is still a function right for every X that outputs one value and that is the average prediction that I get right across all classifiers right where infinitely many training sets okay any questions yeah so if you would have you had a classification yeah I think it's lagging yeah right so then you basically would have to kind of take the mode right the most common label or something so that's that's why I said like it's very nice in regression right because it's just average I didn't have to deal with you know how do you yeah that's right how do you combine multiple classifiers that's right good point oh yeah sorry the data said well it's it's a random process right CBC draw and data points and that gives you a data set D right so you have some distribution over what this data set could look like does that make sense less busy you know I mean a set can also be a random variable there's nothing nothing unusual about this in the same way that a function can be a random variable so you can just do this you can sample many sets why'd you just sample another 10 data points to get another set and you can compute what's the you know which data set would I expect etcetera right I mean it's you know it's just unusual but but it there's nothing it's still just a random process right okay any more questions yeah oh very good point good good good so all right good you guys are too smart and so yeah yes maybe so what he's saying is you know if I had infinite amount of data why would I just chop that up into many many little data sets and trade them and average them if I just take one very large data set and fair enough right so in this case we want to argue about something that in practice you would just do this right that's one advantage actually the variance of this will tell you something how uncertain you are and if you had many different data sets in this case it will help us argue about it but yes in practice you would probably just combine the data set and learn one big classic like you know then one very accurate classifier rather than average many okay good good questions yeah so this is justice I'm just defining this in some sense there's only come in handy very you later on right so just I'm just introducing the notion that there can be an extra spacing something like an expected classifier right yeah h-bar is not a number h-bar is a function and so you're averaging many different functions each one of these is a function and you have you multiply them by weight and you're summing that up right so I mean if it's a linear function but I mean be careful why this could be highly non it's could be the decision trees this could be something nonlinear it's really you know it's a video function oh yeah last question Oh n ^ n because we saw draw n times so that the distribution D T is drawn from this distribution which is basically just P to the power of n which means you draw n data points from P that gives you a set of end points yeah good question all right now I guess you have it all on your sheet so all right so now comes the next point is that if age is a random variable then what we could actually do is we could find the expected error of our algorithm a right no longer conditioning on H all right early on B basis attribute a our data set we train a classifier H we want to know what is the generalization error of this classifier right that's a little unsatisfying because if you invent a new algorithm right you don't want to know what's the expected error of one particular output of the eyelid right you want to know how well does your algorithm do okay and that's a right so a is really kind of SVM's in general right that's what it would a is and some procedure to come up with classifiers and how do we do this well we say we really have to integrate out H as well might be to say okay we go over all our data point x and y you want to know what is the error on this on such a point in expectation but we are also drawing HS in ran that's what we saying you also have D drawn from P to the N and let me say what's the error of HD of X minus y squared so one more time let me just walk you through it one more time the expected error of our algorithm a it's computed the following way right so first I draw a data set D a training data set the first thing you do is you draw a training data set then you train your algorithm then you get an HD then you take the text test test point x and y and compute the error right and how large is that era and expectation all right so if you would do that 10 million times so 10 million times we get a new training asset train a new classifier and then draw one more point a point where x and y test it on those well in what's the average error that's the generalization error of the algorithm any questions yeah maybe too fast yep so you can approximate this by splitting and we will later on do this that's right so this is basically the same thing that we had before but now we also have this integral over D so we integrate out every possible training data set that it could have probability of XY probability of D dy DX D D okay good any questions all right and this is exactly the term that we will now decompose all right so this I claim this is what we're really interested in right when we make a choice about which algorithm to choose or when we design new algorithms etc right what we would like to know is for a certain data distribution P how well are we going to do you want to pick the algorithm at the lowest such error okay yeah no x and y is the test point all right so these these are n data points drawn from P and that's the n plus 1 data point on da from P so it's totally fine there any minute all right good who's ready raise your hand I'm very reluctant okay hmm so now if you turn around so we're now decompose this era and though it's important instead every single step because the next few lectures will all hinge on this lecture now it looks a little scary a little intimidating don't worry it's really really easy it's just the very the basically do the same trick we have two tricks and we do them twice each one okay before we get started any questions alright so we want to decompose this expression here all right so that's basic says we integrated out all X Y or D what's H D of X minus y all right so if I basically know take a random training set train our algorithm get HD what's its error an expectation now maybe I raised this year on the right to get some real estate all right so here is the trick I'm doing I just take this expression here and I subtract and add H bar of X does anyone remember what was H bar of X the expected classifier right yes so just take this I say okay well that's the same thing and over X Y and D where I take HD of X minus H bar of X plus h bar of X minus y and then I square the whole thing okay does that people believe me right so I'm just adding something and subtracting it right away like clearly I'm not doing anything yeah but now I can do something beautiful I can just put parentheses around here and around here all right that's still I'm not still not doing anything okay any questions all right so the trick is now I can actually you know computer at the square and look at the individual terms so what we want to do is you want to analyze and get to the bottom of where does this arrow come from right why do we have such a hair a high error and so if I now complete the square then first look at the first term this is the following er over H of x1 seconds there's actually a mistake here shouldn't be H bar right right just ever that's just a plus B squared is a squared plus B squared plus 2 a B right this here's my a this is my B okay that's just you know you're still in middle school yet so this he has a squared plus B squared plus a B - a B HD of X minus H power of x times H bar 4x minus one okay this is a squared this is B squared this to a be any questions yeah probably yes yes it should be good point thank you yes any more questions okay so far so good now okay this term is cool this term is cool this term here I claim is zero and so maybe I give you in your neighbor two minutes to convince each other let's try me on the right actually is zero you can look at the derivation on the notes right below it so I understand why we can get rid of the middle term well why can't we make the same argument to get rid of this term because essentially what we're do saying is that's the thing - it should not be a yeah I understand that but if the argument basically here is that if you take the thing - its mean and take the expected value of that at zero right but that's what we're doing here anyway it's an Exedy are independent sorry one second like we could split the expectation value of x comma D into X book expectation of X and expectation of B and then make the same argument correct and no no because it's in a square this is squared Oh okay raise your hand if you understand why it's zero okay it's a lot of you good good good all right so basically what we are saying is we just take the expected value over x and y of this term and and D so I forgot D here XY are not independent right that's the data points label of course you know they're independent I'll be bad but D and X of I like the XY pair and D they are independent of each other so what we can do is we can write this as phase the first collect say a of XY and then we have e of D and then in here you basically now if you just you get HT of X minus H power of x times this the second part of the product H power of X minus y and so the point is that this here is 0 Y is the 0 because expected value is linear right so this here is actually you know this is an expected value of a d this is not this D doesn't appear anywhere this is just a constant right so we can write this as e of d HT of X minus H bar of X all right that's the same thing we can just pull that out of the expected value what is this well that's exactly H bar of X right so we get X bar of X minus H bar of x equals 0 and we multiply this with this term everything disappears ok any questions about this we will use the same trick in a few minutes second time so that was the the first trick was to just add this term and subtracted the second trick is to discover that this here this cost term must be 0 okay who's ready to move on I'll be almost there guys you're almost there it's so close ok so this here is 0 big fat zero awesome so now we're left with these two terms so we just show that the error of an algorithm a consists of two terms and if sum of two terms the first one is the expected value of HT minus H bar of X and the second one is H bar of X minus y now there's y squared sorry both of these squared okay so let's move on okay good so if you look at the first term you will already see something wonderful shining through here and let it what is this this H H of X minus H bar of x squared right so it's basically what are these bar X is the mean function value and H of X is the one of your particular random variable those are just the classifier you just drew and if you square the difference for random variable its mean what he get is the variance alright so this is the variance of basically your prediction all right so this is nice we can interpret this very well me to get to this in more detail in a minute and he V step the second term and the second term is H bar of X minus y so let's just look at only this term right now and decompose that expected by H bar of X minus y squared and we have this over x + y and now we do the same trick again we just do the same thing right we add and subtract the mean but any of these in fact if you had eight years of e subtracted H bar and added and subtracted H bar now we have Y here so what do we do we add and subtract Y bar so it's exactly the same step same argument you're making me say this is H of X minus y bar of X plus h of X oh sorry a one second that's my bar of X y bar of X minus y and the whole thing squared okay guys any questions who is still with me good good you're holding in there good tough you're almost there so if ad saying the second term is interesting why it's basically the average prediction - the label right so we can decompose this further that's it that's interesting in itself but let's decomposes further so what we do is we you know add and subtract this average label off an X that's the expected label of a net of a spur 7x and it's just be precise this over X Y and there's no more D is that correct yeah okay and if you write this out we get the following one more time you get the first thing squared with it which is H power of X H power of X minus y bar of x squared again this is a plus B squared so this is a squared now we need B squared which is y bar of x - y square plus 2 a B and I claim this is again zero this is zero right why is this let me just draw it here one more time it's exactly the same argument as before if you take these the expected value of the product of these two what do we get here we get h-bar of X sorry y bar of x and y we take the expected value of this the expected value of y is y bar right so we actually get these to cancel out so do you want me to go let me go with sure so how do it is a little so here's the idea if I said G of X comma Y so this time they are no longer dependent have to be a little careful so what we do is we say the first go over X and then you go over Y given X Y given X this is how we can be decomposed into two different expected values that's that's the same thing and then we look at the product of these two terms that's Y bar of X minus y and then the second term is H bar of X minus y bar don't worry about this term here on the right but here what do we get expectation basically say first is expected value by X and then ever Y given X that's the trick and now give an X all right what's the what's Y well Y bar of X in this case actually a constant right does this doesn't involve the VBA via take the expected value over y this term here does not contain Y it's Y bar of X as a function of X has nothing to do with Y right don't be fooled by the fact that there's a y in it right so this here is a constant right so this is just Y bar of X minus the expected value Y given X of Y and what is this that's exactly the definition of Y bar of X so we get Y bar of X minus y bar of X is 0 all right so this thing once again disappears right into some good feeling any questions all right and now we are actually there this year's zero and this here was only the second term so let's just go all the way back to the beginning let me say that this is the left term plus the right term which we decompose into these two so let's at the left term here that was e of x comma d HD of X minus H bar all right so just all right and now take a deep breath because vo as we just showed is that the expected error of an algorithm you compose into these three terms and let's look at each one of these so this one here what is it right I already gave it away early on so we you know but if you take this particular HD that we drew from the training data set and compute the difference like differently there's not actually about getting right or wrong this just how different is the prediction of a given da a classifier H from the expected classifier right so this is the variance of the classifier now this make sense is not about getting it right around this just how much do my if I draw if I take two different training sets and train two different classifiers how much do they vary right BOTS the variance across these predictions that's exactly what is this right this is the prediction of the average classifier there's the production of this particular classifier and I subtract these two and square that's exactly definition of of the variance raising handed that makes sense okay good what is this thing yet this year's y bar minus y right what is that well you have a data point and has a label Y but the expected label is y bar okay so if that's very different that means your problem is really really hard right because your data does not have the labels that you expect it to have you can't do better than Y bar of X right that's the expected label right that's the label Vaizey that this this feature vector should have right so this here is the noise does that make sense so if you have really noisy data right that means that the same feature vector the same description of a house right could sometimes be worth you know a hundred thousand sometimes to be worth a million all right you have a lot of discrepancy that's exactly what this captures right for certain acts what is the average label that we get what's the expectedly we get and what is it really right if I draw all right so that's the the nots and what is this thing this is called the bias square and what is the bias squared the bias bad race it says if I had you know infinitely many trained ear sets right and get the expected classifier right and noise doesn't matter right I'm not trying to predict the Y for X I predict the expected label right so noise is no longer an issue how much error would I still get and what does that capture that basely captures how much my classifier is biased towards some other explanation that it's not really in the data and so for example the data could be nonlinear right it's not a linear decision boundary or let's say you have regression here so it's just not linear but I'm fitting a line to it right and no matter how much data have right no matter how many classifiers I'm averaging I will always make some mistake and that's just because my classifier is biased towards a very specific solution right and more data will not convince it otherwise that's the bias of the model any questions my now they old square right all I see well it's because this is actually the noise of that Lots right there's the variance yeah I guess it's just what we call it yeah so exci let me just give a better explanation because the bias could also be it could be above or below but if you square that goes away yeah no I mean you know basically the ottoman era that you're getting right it's based in the sum of these three errors yeah know what oh this is more about you understand it like it shouldn't actually plug in data in here right this is this for you to understand that the error is actually you know is the sum of three very precise things it's the bias the variance and the noise of Jagoda are yours or the bias and variance of Y algorithm and the noise of the data right in those three parts you know that's all there is to it to the error and so what we will do in the next couple of lectures actually I will talk about how to reduce each one of these right and here's the data science the scientist if you have a classifier that doesn't it's not accurate enough it's your job to determine is your bias high if you know is high is your variance high right and then do exactly the right measure to reduce the value that's actually large right each of these is quadratic right so if you didn't reduce the wrong one and this is actually what you see all the time in practice my people have algorithms that don't work and they have a bias problem and what do they do instead the decrease noise or decrease variance right which is not their problem right so you have to basically sharpen your mind sharpen your eyes for what's really going on when you train a classifier let me give you I just gonna do a little exercise before turning around do not turn around do not turn around the the notes I want to give you a little bit of an analogy and the solution is in the back here so here's what I want to do imagine you're throwing a dart at a name I want you to think about for two minutes that we still have what are these pictures look like if you are basically if you have high bias right so you haven't yep you're throwing darts at the this goal you're trying to fit it here okay let me give you one right yeah this is low one second no variance high variance low bias high bias okay so if I have low variance and no bias forget noise or now like no noise would be if this thing is shaking so let's say you're in the front row throwing darts at this thing right just think about it don't do it if you have low variance and low noise what happens is you actually a very little error right you're gonna throw all the darts right here at the center now here's the question if you have high variance you know we can decompose this error now in variance and defense right so what happens when you have high variance what happens when you have high bias what if he happens if you have both discussion for one minute I'll show you okay we out of chance let me just show you what what I mean so if you have low variance no bias you always hit this exactly right if you have low bias and high variance what does that mean I'm just you know so if you have low bias this means an expectation you're dead on right but your different throws they vary a lot right so it looks like this but if I would average oh my darts I will be right here in the center right and some may be here right but and you know so in expectation my darts are right here right but I have high variance so sometimes I'm a little bit to the right sometimes a little bit to left little Buffalo but no right but as no systematic error right again I have no bias I'm not biased towards any other goal right I just I'm shaking a little bit right need a drink or something so yes here's the other scenario what if you have high a a I wear my la vies high bias and low variance hi my is low variance means I'm not marrying a lot right I'm always hitting the same point with laser accuracy but unfortunately you know my glasses are a little off and I constantly hit this point here right with amazing accuracy right like I'm not and my variance is very low but I'm just biased to watch a slightly different solution okay that's this error here and the worst of all words is when you have high bias and high variance that means an expectation you're off and you actually vary a lot right so you've easily really going for this goal here right and you don't even hit that one very well okay awesome eel siege on Friday 
","['', 'machine learning', 'generalization error', 'expected label', 'classifier', 'hypothesis', 'training data', 'algorithm', 'test data point', 'expectation', 'splitting data', 'bias', 'variance', 'high bias low variance', 'low bias high variance', 'high bias high variance', 'analogy', 'dartboard', 'low variance no bias', 'expectation value', '']"
"welcome please everybody close your laptop's I hope everybody got a hand out because of San Patrick's Day they are in green in Germany some Patrick's days today okay so last time we talked about the bias-variance tradeoff which i consider one of the most important topics and machine learning and data science it's really a topic that comes from Statistics this is something that statisticians basically first derived and the idea was the following last time he basically came up with all these night I brought it up here just to make this faster he basically looked at the expected test there are often algorithm a but we basically say we look at to integrate out any possible training data D and then we look at a test point X with label Y a me say what's the probability that we draw this justice train did a D then this x point a this test pair x and y and then given that what's what's the error that we get right and if you integrated all of them that's exactly the expected error of that algorithm from that data distribution for a given training data set size and so we looked at that error this is this term here on the left and this is just you know easily a squared square squared error he expects to expectation of a squared error but then he looked at this a little bit more closer closely and decompose this error and we found something pretty pretty amazing that this term decomposes into three very interpretable terms right and let me just you know go through every single one of these one more time so the first one is the variance right the variance Pacey says how much do my classifiers vary right if I take you know basically in some sense I classify as a random you know random variable right I'm drawing a classifier which is basically the processes I draw data set and then I train a classifier so I can also view it as drawing a classifier so if I draw various classifiers how much do they vary from the mean right this is exactly the variance of these classifiers so I mean it's amazing right like if you just think about this for a second that this square term decomposes some the sum of three different quadratic terms it's an each one of these is beautifully interpretive all right so this is going to help us a lot when we try to make machine learning work in practice so please really pay attention to this and make sure you understand every single one of them so so one more time this buddy says how much do my the classifier is itself very right and this here is the noise so here the important thing to realize is that when we minimizes square loss which was the assumption here we made here the square loss is minimized at the average label so the average label is exactly this guy here right so there's the expect or the expected label in this case right and so what our entire algorithm is trying to do is trying to find a classifier that reliably for certain X outputs Y bar right that is the goal of the machine learning classifier and what the noise is measuring saying that even if I'm hundred percent correct right every time of hitting exactly y bar right I still have some error because the data points don't actually always have the expected labels right they can vary themselves and that's this term here is exactly the variance across the label okay any questions about the noise about the barians yeah that's exactly right yeah [Music] yeah this average is they don't have the same data set size I mean in this case basically a I didn't actually go into data set sizes all right but it's still the case if people have a very very large data set and you would you know in the typical case actually your age would approach approach this H bar and so then actually so that's exactly right of you so here here's actually why I can tell you it's very very simple right so in some sense this captures the variance of the data as well right so if I would actually sample let's say I'm sampling a billion data points right and then I'm sampling another billion data points right well these two sets won't be very different right does that make sense it would be extremely unlikely that the first billion data points are completely you know very very very different than the next billion data points where there would be a billion times I'm drawing from this corner of the space and here a billion times are drawing for this corner of the space it's not gonna happen right so when the data sets are small then actually if you draw many data you know different data sets they will vary quite a lot but as your dataset gets very very large the next time you draw a different data set you will basically get the same one does that make sense so that actually USU totals automatic what that tells us is as my data set becomes large this becomes very small because of my classifiers won't very very much anymore very good point very good point and okay so then you have the noise and then we have the bias so what does the bias say the bias says now we take the expectation this is independent of the of the label Y and independent of the training data set this is basically you know take away assume you actually get the average classifier right we basically converge to this right how much error does that still make right so even even the expected classifier right is not good at hitting the the label it's trying to aim right and that's because we make some assumption that is not true in the data right so no matter how much data we have you be averaged over an infinitely many classifiers right so infinite amount of data I still we're not hitting the expected label of the data points and why is that because for example we assumed that the data is linearly separable when it's really not or something like this right so it's really important to kind of like it's really beautiful how these different three different terms capture completely different aspects of our algorithm in of our data so you don't have an idea how to reduce for example noise right so this is kind of these two are down the algorithm but what do we do about the noise turn any idea sorry say reducible it's a pessimistic view ya figure out outliers sure so if you maybe say some terms but then you actually change the data distribution might see would actually say like you know you change distributions on P Prime when you say be exclude certain certain points right you know data cleaning yeah add more features right so that's actually very that's actually a really good one right so why bar here is a function of X right so X basically describes your data that's really important it's like if you try to predict that price of a house right well if you only have like three features number of bedrooms number of bathrooms and square footage right well then actually you know the amount of the just based on these features for any given house right there could be a large distribution of prices right but if you have much more fine-grained features then hopefully eventually you actually going to pick up the nuances of the pricing right and the variance will go down so this is actually there's not something that's just given to you as a data science scientists can influence each one of these terms yeah no no you're not right the variance does not change right more data does not make any difference the expectation of a distribution there's no notion of youth data set D right no this is this is expected value right there's like in expectation this this has really nothing to do with how maharaj ji' dataset is there's really something about the distribution but be careful the distribution is in with a space you know described by your features right so that's the one thing used under your control right the the feature space itself okay anyway yeah right so so here comes the interesting bit right so what he's saying is as I'm if I add more features right and I make this Ian smaller right I may make this one larger right and that's exactly what happens a little like whack-a-mole all right you know this game whack-a-mole I don't but it's kind of like that so you press one down and then something else comes up I think that's kind of that's what I'm picturing so so that's exactly the problem right so that it's very very hard you know the moment you bring one down to zero or something right the other one's going to go up right there's always a trade-off and that's exactly why we call it the bias-variance tradeoff right the beauty of it is and this is where your power comes in the realization that each one of these terms is quadratic right so one will always dominate the others I know very likely so what do you want to find in equilibrium right so it could very easily be that the whole thing is large because bias has massive and these two are very very small right then you have the chance because it's quadratic to reduce pie is a lot and only make these to go up a little bit right and that's basically yeah that's usually actually what you're trading off its variance and bias this is the - right so if you get one down the other one goes up but it doesn't have to go up by the same amount and that's the skill that you have to learn as data scientists and people talk about this exactly in the next couple of lectures so no more questions one thing I want to show you is the project that's coming out today oh I see I see a here excitement so good stuff so QP is this it this is connected to the kernel oh that was connected to the kind of okay okay one second finds variance here we go okay so so this is still ders the prototype so this is actually not exactly what you will see so one second is this running okay good so here's here's what we do we draw a data set so what I want to show you is that the error is the sum of these terms and this is in some sense what you're going to test in this in this project right to really kind of see like a gnat the air are really decomposed into these three terms and then how large is each one of these three terms as we vary the settings of a classifier so please pay good attention you only have seven days to complete this project so here's the key so that what we do this time is we draw our data from a very specific distribution P that we know right so in this case we are playing God right so we know the data distribution which is typically not the case but by doing this we actually have control of each one of these terms we compute them very very exactly you know to arbitrary precision so here's what we do we may see George I have to Gaussian distributions the red one and the blue one and I say all the blue points are plus one and all the Wet Ones and minus one I believe that's the labeling or zero one I don't remember and so now we draw a certain number points and the first thing is you know if you would now if you know these distributions right then you could actually use the Bayes optimal classifier right and the Bayes optimal classifier give you predictions and these points that I've highlighted here with these gray circles are the ones that the Bayes optimal classifier would get wrong and so because there's an overlap for example here this is a blue point but it's much much more likely than it's actually red based on its location so the Bayes optimal classifier would say that this point is red despite that it's actually blue right and in fact in this case it makes a nine point seven percent error right so this is basically the overlap between these two distributions and that's you know what can't even tell me what that measures of these three turns the noise right that's exactly the notes right so that's the stuff you can never ever get right right that's basically the expected label here would basically be read right but actually it turns out it's blue right so there's no you have no chance you can't possibly get this right right based on this two-dimensional representation okay so then what we do and I'm going to pretty score this by that you don't see the solution what's happening here this is I know people gonna watching it on the video in slow motion oh I see okay wait one second I didn't save so one second I have to say it all the time okay so and let me just tell you what I'm doing is still running so so here's what I'm doing I take this data set yeah so here he is okay so now you can see it's funny so basically what I'm doing is I draw initially I draw a lot of data from this data set and then basically what you have to implement in the project is a term to actually compute bias compute variance and compute the noise all right and then actually sum these up and the way we as summate these is by sampling so let me first show you the the examples so the classifier that we're using here is actually just a linear classifier that's kernel eyes so kernel eyes we haven't done this yet we will do it in two lectures so it's basically just to make put linear classifiers on steroid and make them really really powerful so we need to do this because otherwise they they don't have they're too simple so don't worry about it for now it's just volume is just a linear classifier let me just take the data and project it into some other space where they where data is always linearly separable and so then we have a regularizer and so this year this regular vizor lambda is set to two to the X so as we go to the right basically we have more regularization okay and if we have more and more regularization what are we doing we're forcing the classifier to be simpler and simpler right so if you regular buys a lot at some point we just get the all zero classifier right if we here Aguilar Rises you know two to the minus six that's very very tiny and so here's what's happening that's the only thing we're changing so if you're changing the regularizer so basic from very very small to very large and one thing you can see here is oh basically the red line here is the bias the black line is the variance and the green line is the noise of the data and if we add these three up right then we get the dotted line and the dotted line is very very close to the blue line and the blue line is the actual error of your making alright and the reason they are not exactly the same is because I set it up so it actually runs pretty fast because you guys need to run this on velarium so the way we actually estimate the bias and the variance here is by actually as a for example the variance what you do is you learn one classifiers right and then you basically compute the average what is the average classifier and then you compute the variance from these hundred classifiers it would be a lot more accurate if you would train a thousand classifiers but it would take ten times longer so okay and similarly the approximate the bias and similarly approximate the noise okay so here you see them very very beautiful trends so the first unfortunately this box is right over this blue line but you see something very very nice that you know you can really see how these basically these two lines dancing around each other and so you know they're basically the same thing up to the approximation error that we get because we only be sampling only a hundred functions and so the second trend you see and that's you know also very beautiful is that the bias here the red line right as we regularize more the bias goes up right so the proportion the portion of the error that is explained by the bias gets larger and larger as we regularize more right so basically what happens here is that the model is too simple right and that is why we have this large error right so you know a huge portion of that error is basically the you know the area up to the red line that's does the figure make sense to everybody any questions about the figure yeah yes you can absolutely have this you can you know and that's not good so that's that's called a terrible classifier I think that's the technical term so in this case you don't see this right because we're trying to trade these two all right so so basically if you make the wrong assumption about the data and then you have a you know high variance classifier right and so this little value in going that's usually you try to avoid these situations I will show you in a minute how to recognize this and okay so basically yeah the second one is variance and one thing you see variance is very high here right so what's happening here here the the classifier is not regular rised and so a large proportion of the error that making on the test set right can be explained by the fact that it's accentual e memorizing the training sense right and that's why there are so much variance across the different classifiers right so yeah between 100 classifiers compute how much they vary they vary a lot because each one over fitted to the very specific training set that it was trained on I think says that does that make sense so that's the failure mode on this end and here's the failure mode on this end it's basically that the classifiers are too simple I mean the classifiers are very simple they don't suffer from that problem anymore the variance here is low right they're too simple to memorize the data set right but on the other hand they're also too simple to generalize well and the noise in this case actually is independent of lambda and think that's typically the case right because lambda is just the modeling assumption so the noise is really is the function of the data set so that's actually constant just become estimating it every time so in some sense you see how much the estimation bear is here which is roughly also how much this jumps around all right so the the noise is basically you know always there and there's little we can do in this case if we just change the trade-off between regularize it how much we regularize and how much we focus on the last and so what you as a data science is you try to find the lowest point here right so in some sense actually somewhere here right the point where the variance is low but the bynes is not quite large enough to you know to basically mess you up by being too simple and so you basically want to find the sweet spot in your set any questions any questions about this project of the stem okay all right let me move on then yeah Kristen there's always a line one more time yeah right so the Bayes optimal classifier can anyone tell me what is the variance of what is the bias of the Bayes optimal classifier so the variance is obviously zero right because the Bayes optimal classifier is you know computer based on the distribution and the distribution you know the training set in some sense has nothing to do here right so there's no variance across classifiers there's zero right and the bias is how much does the expected classifier differ from the expected label the prediction of the expected that's fine differ from the expected label well that's exactly what you predict with the Bayes optimal classifier so you also have zero by zero bias right so what you're left with is the noise so that the noise is something that even the Bayes optimal classifier gets is that does that make sense so I'm a time variance is basically this right h-bar there's the variance right the Bayes optimal classifier always outputs the expected classifier it doesn't it doesn't use training data it uses the distribution so this here is zero right then the bias is following the bias is H power of X minus y bar squared right the Bayes optimal classifier what does it do in regression in regression you output exactly Y bar of X for every single X that's what it that's the output of the Bayes optimal classifier right and you always get exactly so that's also zero so the only thing you're left with is by the noise and the noise has nothing to do with much model you using so you still start with that yes it's just always there like yeah yeah but they how you how'd you get rid of the noise you cannot remove it alright so Buick Lee you just observe the error which is the sum of these three terms okay any more questions alright so what I want to look at today is basically the setting you know let's just go back to erm and so this is the setting that we just saw in the project really basically say you want to minimize some parameter W if you have some loss function that we sum over you know well I think X comma Y W that's lambda times some regularizer and this could you know it's typically the l2 norm for example something like this right and so the first thing I want to basically now depict us the graph that we just saw what happens when we change our laugh right and so I would convince you or try to convince you that lambda is actually one way of trading off between two different concepts and we call these underfitting and overfitting and let me just draw a little you know this is very dirty let me just draw a little picture if you want to look at the error as our lambda changes and so in the project we looked at the test error and the three different terms let us for now just look at the test error in a training error right and so as lambda becomes large and this here's the error so as lambda becomes large if lambda is it's very very small right then basically we're putting all our weights so here's our last function right so assume lambda is zero if number is zero then this optimization has no weight on the regularizer and the only thing we're trying to optimize is the loss on the training dataset right this means the training error must be very very low because we're not doing anything else but minimizing the training error and as lambda gets larger if lambda is really really large what happens then this term is complete the Optive dominated by trying to find a simple solution right in the extreme case you have an all zero vector well the old zero vector is basically as good as random guessing and so your training error will be really high right so your training error basically looks like this it's not it's not linear let let me let me draw this again anymore like this ok this here's my training error and my testing error well yeah the two things so clearly if my lambda is really really large and we turn the old zero vector my testing error will also be really large because I just returned a bogus classifier if my lender is really really small then the problem is I'm just minimizing the error on the trained ear set and I'm focusing too much on the training data set and so what I get is the following I've easily get a decomposition so this here is my test error and this is exactly what you will observe in the in the in the project what we just saw that you know basically as my lambda is too small the test error is high then there's the sweet spot in the middle and then if it becomes too large you're over regularizing and the test error goes out again where's the training error the training error only goes up as your regular isin 1 and we call this regime here overfitting and this your under pit and so what's basically happening here is if you look at the you know the bias variance you would do they would have to draw the bias variance curves then basically in this regime here the bias the variance is really high and it goes down this is the variance and the bias goes up this is the bias and so the noise in this case is just constant so we don't look it until the question is how do we find the sweet spot speeds okay any questions about this yeah it doesn't only apply to which a regularizer oh I see yeah this is generally I see what it's saying so this is generally applies but if you have the wrong regular visor you are increasing bias more I presume yes yeah yeah so so the question is what happens if you make the wrong assumption about the regular Rises if you for example you can have Gaussian noise then you get the two regular visor and so on and the curves looked is similar but you can actually get affected by how quickly the bias see arises yeah any it's a good question very good question any other questions ok good so because lambda here is the hyper parameter of our model and that basically balances this trade-off and so how do you find the best lambda and the the typically actually let me just characterize this too a little bit more so overfitting I think you already talked about overfitting is the variance is high that basically means we are memorizing aspects of the training dataset that are too specific right so you know the example would be I take images of let's say students and faculty or something but it turns out that students are disproportionately more often you know the pictures just happen to be in front of trees or something and faculty you know there's fewer fewer pictures in front of trees more in front of buildings that's a total coincidence but the classifier would pick up on it just because it happens to be true in this data set but it does not generalize right so these are kind of the the things that basically happen in this case the training error is very very low but that does not translate to the general setting in the in the under fitting setting what happens is that you fires even bad on the training dataset and so that's that's the key part that I give a certain data set you know the data point and even those that it has seen it can't do correctly right and that's because for example in terms of over regularization my classifier is constrained to be on a small ball and it just doesn't have the expressive power to you know even get it every single training point right okay so how do you find the sweet spot London yeah and the simplest thing that people use is called cross validation or k-fold cross-validation and it's quite simple basically what you do and it's also called grid search actually so that's two different things but with search and key for cross-validation and so the first thing is great chart and that's really really simple so here's what you do you just say well which lamda is good here's what I do I just take my training dataset I stood it up into training and validation and then I just try out a whole bunch of different values of lambda and for each one of them I record the error I get and then I pick the lowest one and I say the lowest one is reached here and that's the lambda I pick all right so you know each one of you would have invented this and you would be famous right if you've done at 40 years ago but unfortunately it's too late at this point but so you know it's very simple process right so you try out there all the different values of lambda evaluate you know the classifier on the validation set and pick the lowest one ideally you want to try as many put a different point of lambda as you can different values of lambda as you can the problem usually is that it takes a lot of time to train the classifiers you have to find some trade-off and one thing that people do is they do telescopic search so telescopic search is basically first year a very gross setting of what the right lambda is then once you pick the best one you and typically this can be order of magnet so you basically you do this not a real logarithmic scale so you try out lambda equals 10 to the minus 5 10 to the minus 4 10 to the minus 3 and so on the 10 to the 5 or something right so you try out the different orders of magnitude just to get an idea of what's the right regime of your lambda then you pick the best one let's say in this case it's the best one here then once again you try out 10 versions of lambda but just around here and then you pick the best one which may be here for example all right so there's a two-step telescopic search the first you locate where should i search and then you search you zoom in in that regime of the hyper parameter axis any questions about this okay so it's really simple there's one danger and the danger is if you try out many many different values of lambda what is going to happen is that you will inadvertently over fit over the validation set that's that is problematic right so what will actually happen some lambdas may work better just on this particular validation data set just because it only contains a couple of hundred points right and you have to be careful right of those hundred points maybe you only get five wrong right so if you get one more wrong right that increases your error from five to six percent but it's only one data point that's now more wrong right so is that really something you want to you know rely on right so you have to be very careful all right so if you validations that is too small you may actually very easily make erratic decisions that you know basically you know give you a lower error on the validation but again it doesn't generalize to the true test yeah so that's all right good good so that's exactly what I'm leading to so the answer for this is k-fold cross-validation and so k-fold cross-validation what you do here is you say well I don't just pick one validation set actually every single data point in my data set should be a validation point and so here's what you do you basically don't just make one split you make many splits so basically what you do is you divide your dataset into K there buckets and you say K minus 1 of them are used for training and one is used for validation and I can do this exactly K times right I can always leave out another bucket so you know the first time I leave out this but this bucket here is then I trained classifier here evaluated here then I leave out this bucket here I train a classifier and all the others and evaluate it here okay so now for every single lambda I don't just get one error one validation error I get actually I get K validation errors and the beauty about this is about is that a it's much much more accurate and B I also get standard deviations right so I know how much does actually varied across the different validation data sets okay so I can tell me if one point is a little better than the other but that's well within the noise that I guess C anyway it doesn't make any difference any questions about this yeah so the test set is the one you only touch once and that's basically what you report to your boss and that's that's the true estimate of the air yeah what do you then do after your boss approve is you train one more time but you stick in the test set and you don't tell anyone okay and then that's the final classify you move a push-out MedX there's a little bit better because there's a little more data right that's okay as long as you never use that test set again yeah how do you choose K good question k stands actually for Killian and larger is better now though the larger the larger K the more buckets you have the better the estimate of validation the problem is the slower it is so typically what you want to do is it something's a trade-off of how much time how exactly accurate you want to find the best lambda and sometimes how much does your boss pay you in a bonus you know when you actually get the lowest possible error that's in some sense but as to how much do you really care about it it also depends on how big your data set is so one thing is in the extreme case K equals K so in this that's the same thing as it's called leave one out cross validation so in this case actually you basically have you've run it n times and you only leave one point out and you just see do I get it right or do we get wrong but because your average overall end point n times you actually get a very accurate estimate of the error and the only problem is it's really really slow if n is large so that's however often used when you did a set a small right so for example I've you know I've used medical I've worked with medical images and their each image you know MRI scans of people's skulls and brains you know each one of these costs several thousand dollars so we could only afford you know maybe eleven right so what we did is we trained on ten and left one out and we did this exactly eleven times and that basically then gives us you know the space the our cross-validation in this case but very good question any other questions all right awesome so so one way to regularly regulate overfitting is regularization another one that's also very very common is over early stopping and so early stopping is something that basically means exactly what you think it does it basically is a part of the optimization process so when you try to find your parameter vector W you basically don't optimize all the way to the to the end you just stop early so for example at say you do gradient descent right so one thing you know typically but with the way we described it as you keep doing gradient descent you keep improving it until your loss basically doesn't change anymore that's that's totally fine but one thing you could do is you could just stop early right and return basing some observe optimal results and turns out that's a very deep connection between regularization and early stopping and is actually quite simple similar can anyone think about why that is maybe give you a minute to discuss that with your neighbor so what's the deep connection between regularization and early stopping so think about or just think aggression or something [Music] [Music] okay any any suggestions what is the connection yeah that's exactly right so in some you know in both cases basically we control how much we minimize the last function right and one is basically we stop it at the edge of a ball right if an l2 ball and so you can't go any further right that's the regularization point of view and so we can deal with this it's this thing here we start with some W it's around here this year is our last function you're trying to minimize we're trying to get to this bottom here in regularization we place it put a glass wall around here and you know our minimizer you know moves down but hits the glass wall and can't get out of here all right let's place the regularization busy says you have to stay within the ball of some radius where's the early stopping base these days you move you move move it eventually you stop Oh another way of looking at it is if you basically you know if you would only take 0 gradient steps I didn't let's say you start with the old zero vector but you typically do with last functions then you actually get an old you know a vector with 0 norm right and as you keep adding gradients to it right like the first time you the body you do the first time why'd you say W becomes W you know minus alpha times G but alpha is a very very small number that's the bit that's the zero vector initially so now you add a vector a times some very small constant so the norm is tiny at the beginning right because alpha is tiny does that make sense so after one step you still must have a very small norm right if you would take the norm of this you could take the Alpha out of the norm off as a very very small number so the norm must be very small right so you need as you base the two Grady descent for more motivations slowly you're no can grow okay and the regularization your base is saying I just put a cap on the norm so there's a very very clear relationship between these two and so basically whatever that the x-axis in some sense inverted so more iterations means less regularization so the way it works is the test error looks the same but actually the training error looks like this right so so this here's the training in this case the test so so as we keep going people more you know make fewer and fewer mistakes because that's exactly what we're minimizing with our last function right but at some point you know at the beginning what we are fitting generalizes to the test point but at some point we will start you know learning things that really don't generalize any more and then be overfitting and then we move on so if you think about it in terms of if you think about it in terms of variance right if you had two different data sets right what would that look like if you have two different data sets the last function would look a little different right so see this is my data sets one this here's my data set too right so they have slightly different different minimums okay this would be actually an extreme case but what happens at the beginning at the beginning we kind of you know that you know we're moving up in this general direction so at the beginning the variance is very small I just take a few steps of gradient descent my my my parameters won't matter Avon very very much because they're all going in this direction where the minima are he does that make sense whereas as I keep keep iterating and I'm minimizing my loss more and more eventually you know I'm going down to exactly the minimum of this particular loss function of this particular data set right and I'm not ending up at this and at this minimum which would be a different training set right so as I'm minimizing more the variance between these points becomes large okay because I'm getting more and more directly into the minimum that really only is the minimum for this particular data set and a different data set actually this may be actually not a great solution okay does that make some raise your hand that makes sense okay awesome okay terrific I will see you all on Monday 
","['', 'bias-variance tradeoff', 'expected test error', 'square error', 'variance', 'noise', 'label', 'data set size', 'classifier', 'average label', 'reducible', 'outliers', 'data cleaning', 'features', 'Bayes optimal classifier', 'approximation error', 'regularize', 'overfitting', 'k-fold cross-validation', 'validation set', '']"
"hello everybody hello okay please put away your laptops okay quick thing about the exam so ultimately I think it went very well a lot of students seem to do very well it was 90 points in total nobody got a perfect perfect score but several people got really really close to people got 88 out of 90 that's certainly a very good sign of all the distribution is extremely smooth so pretty much every single point assignment you could imagine up to some level you get no I don't have those right this is the oh yeah this is just the sort it there's not even the CDF though there's just every bar has one student I just started you guys that's what that is so this here's the top two students have 88 and then basically got sort of enough points so ultimately if you're between 60 and 88 you're doing you know I'm not worried about you down here these are very concerning I just want to remind everybody today is drop dead deadline I mean I know I didn't I'm just being honest yeah and and I'm also worried if you have below 50 points like 50 points are lower I mean you know these this region here it's very low all right consider this as like the exam if you admit if your final exam is not much different that's 50% of your grade so I am concerned in that case if you decide to stay in the class please come to office hours and we can chat and see how maybe debug what went wrong here and the I mean clearly other people have had you know less problems on this exam so no to try to figure out what went wrong if you are in this this you know in this region here right certainly below 45 it doesn't look very good any questions about the exam only online then Piazza yeah yeah Solutions online so for those some people took it late until the TAS didn't heroic effort of trying to get basically as many graded as possible before the dry adopter drop deadline I did not ask them to do this but they actually did this you know because they thought it was important to you guys so they basically came in Saturday and Sunday mainly graded pretty much the whole time to get everything done on time and so unfortunately the late exams are not graded yet but the solutions are online so maybe take a look and estimate how well you did in case you want to know yeah that's that sounds accurate as a homework for oh I think that's incorrect that's the one that they just beat the homework for somebody just came out of yeah oh I see yeah I know that that's that's a mistake so the project is due the day before midterm oh the day yeah sorry the day before what is it called Spring Break yeah okay if you can maybe post something on Piazza that I guess yes any other questions okay awesome so overall XD yeah you pretty well and it makes me very impressed with the first question so I think the median score was 100 percent of the first question so that that's very very impressive over half the class got every single question right all right what actually got people was the naive base for some reason apparently naive Bayes was the hardest one which I wouldn't have predicted to be honest okay so William Smith still talking about the bias-variance tradeoff and so last time two lectures ago we basically showed you that you can take the error if you minimize the square loss but it generally holds more or less and decompose into three very interpretable terms the first one is the variance second one is the bias of your classifier and the third one is the noise that's inherent in the data set and let me look to these curves and the curves can looked a little like this last one we looked at was early stopping where we said there's two things either you know you do regularization where you increase your lambda or you can change the number of iterations that you're optimizable and if you start with an old zero vector than that's actually a pretty good way of regularization as well and so what you get here is that basically as you integrate more the training error goes down and the test error goes down then goes up again and so this years again a sweet spot sweet spot and so what's happening is here basically the test error is too high in the car that's underfitting so you're not fitting the data set you know well enough yet do you have more we know eight improvement and here you're overfitting overfitting means we are learning too much about the training data but it doesn't generalize the test data anymore so those are the two regimes and to another practice there's these two things that are very similar to one it's basically changed the lambda the the weight in front of the regularizer the other one is to change the number of iterations for which you're optimized typically or this there's one advantage that you have when you do this and you change the number of iterations and turns out it's a lot faster in practice can anyone tell me why it's a lot faster to find the sweet spot when you change the number of iterations rather than trying out many different values of lambda any idea so actually maybe let me just remind you of what I'm talking about so you are you're minimizing you have to wait back to W that's in the two dimensional case this would be the first time ition is the second dimension you start with zero and you have this last function that you're minimizing last function looks something like this here's the minimum so as you optimize you kind of move along this path to get here and early stopping basically says well I don't go all the way there I just stop somewhere early and regular ionisation says if you do l2 regularization then it says I'm staying within a ball of some radius and that's dictated by this lambda so and and so basically once you hit the ball you basically you know you make no more progress you stick you stake it and if you do l1 regularization this ball just turns into you know a diamond right so that's that's the side of different shapes so if you want to find the sweet spot one option is to basically we talk about cross-validation so what we said is you take your trained in asset and you split your train data set into K parts right or you know one thing you just do we can in the original we just do one for then you just have training and validation if you do K cross VY additionally do this K times and you train your data you casa fire here and you evaluate it here okay and that's basically how you can get a peek at the test or the validation error as an approximation of the test error right so in some sense you can basically looking yeah these kind of plots all you have observe as a training error but when you evaluate your classifier on the validation data set then you can look at what the testing error is over you know an approximation of the testing error and then you do this for many different values of lambda so and you pick the one that actually has lowest error on the radiation set now it turns out that's a lot faster if you do this method can anyone tell me why yeah that's right that's right that's exactly right but see you don't have to do this right so when you when you try away when you try to find the right value of lambda what you have to do is you basically retrain your classifier over and over again and then once it's strange its converged you look at the validation error you say okay for this value of lambda this was my validation error and you try out ten different values then you pick the best one when you do early stopping here's what you do we just optimize just optimize once and every couple of iterations us teammate the validation error you just keep a track right if it's tasty you save save the weights and some file and say this was the violation error and then you keep optimize and keep optimizing all the way until the end and then you look into your files and say which one was the one that has the lowest validation error it was this one here so that's the one I'm picking okay so once we basically you get all these different values of n okay so it's very very fast and so that's with expensive models like newer networks this is what people do all right so you just train your newer network once for a couple thousand iterations and then at the end you say okay well after 700 maybe that was the best you know best model to that I started overfitting so you use that as your final answer any questions about this sorry well it doesn't work for all methods right so not all methods actually have an optimization like this so for example SVM you may just call it solver like a quadratic programming solver then there isn't really a notion of early stopping in that sense yeah so it's not space not always applicable any other questions yeah Holly said you and what you do exactly this right you basically you train your classifier in every couple iterations you evaluate the classifier you have right now on the validation set and you keep note of what the error is and that's lazy this curfew right because the approximation the validation error is going to look very similar to this and then you basically evaluate this over and over again and you say after what after you're done you go back and say that was the best one and so you saved the weights you save the parameters at this time and that's what you return does that make sense how did the save time because for everything about you do them all in one step as you be just go through it and you keep iterating if it'll be over if you charge different values of lambda you have to every first every value of lambda you have to do the whole optimization until convergence I so the other plot is this if you basically say what's my value of lambda right if my lambda is very very large my training error it's the other way around right so my training error goes up wait how does it go so it goes this way right this way and right so so here actually try all the different values of lambda but for every single value actually have to do the whole optimization wrist here you just do the optimization once and you get all the different values for three that's the difference okay any more questions yeah how do you know it looks exactly like this and the answer is I'm lying it doesn't it looks in reality it looks like this hey the principal it looks like so I mean it won't be that smooth but it's pretty smooth yeah I mean you have no it's why do you always have noise oh okay any more questions all right so and what I want to do next is kind of say put you in the shoes of a data scientist and imagine your boss tells you train a new system right so for example to do a spam detection or to predict the stock market or I don't know whatever you want to do right so choose your industry and your boss gives you a lot of data and then she says you know like here's you know you have to train a model and the test error should be less than 1% and so you train your model and your error is higher than 1% right so what do you do and so that's called machine learning debugging and so basically what you need to find out in this case is if your error is too high we've now learned one thing right you have to know which part of the error is too high and so he error consists of three different parts that I added up and this is your overall error it's going to be variance bias and noise or something right and so if you know which one is the largest one then you know what to do in order to reduce the overall error all right and it can be very lopsided but it could be something that you know your massive variance problem you've hardly any bias at all and you don't have much noise right so in this case if you try to reduce the bias you know you won't have any luck I know you try to clean the data and spend a lot of money to reduce the noise behalf right well I won't help you much because you actually had a variance problem so you have to address the right aspect of your of your problem so the question is how can we debug this and so what I'm telling you now is a very very simple trick that you can use in practice and I hope you will and so here's what you do you plot you make a plot and this here is the number of training data training instances you use so you basically take your training data and you just randomly reduce it just sub sample data so this here is basically using my entire training data set and this here is only using a small fraction of it and so for Phase II what you do is you train your algorithm many times first you just train in one percent of the data very little and then 2% and 4% so on and you slowly crank it all the way up training in the full mark and what you want to see is how things change as you increase your data and so this year's error and let's say your boss dictated you here this is the error rate that your boss tells you if you get that much error then we be able to strip it and you get you bonus they move on and get in a promotion etc you may even get a cubicle near the window it's very rare and so basically you have to move your test error below this right in my test area essentially I mean you know in this case validation error so what happens right so the first let us plots the training error so the training error when you have hardly any training instance at all you just have one training instance what's your training error with any respectable algorithm can still look at itself in the mirror I just give you one training example what's going to be the training error zero right so it's going to memorize that one example your whole right otherwise yet you may have a biased problem and so as you increase your at your training data what happens is the training error will only go up because your problem is only getting harder the more data you have to basically be consistent with the harder your problem so you're basically as you know the beginning your training error is very very low and then you keep going keep going keep going going and so here there's kind of what happens and what ends with the test error well the test error if you just have one training sample is going to be horrible right because probably most of the most of the world will not look like that one example you down at it so it dissipates your errors very really high and then test error goes down goes down goes beyond let's count and the important thing is the test error is never lower than the training error so the training error is a lower bound of the test error so this is what your figure looks like okay and depending how much data you have you will either stop here because that's now you've reached the maximum with your data set or you stop here if you have a lot more day okay all right so let me just I just found this this training error and this is testing and I claim there's two regimes in which you could be in that's right there I'm excluding here the regime where you test there as lower than absolute right if you test there is no other nap salon and why are you doing this are you wasting your time I just go to your boss you know show the results and you get a promotion ready you know and move your stuff to the window so so in this case we're always you know the test error is always above the exit right so there's these two regimes so either you increase your train they're setting you can stuck here right and this is the regime number one and this he has regime number two okay and so without looking at the notes one of them is high bias and one of them is high variance which one is high variance which one is high bias who can tell maybe actually discuss it with your neighbor for one minute don't look at the notes is it in the notes it's a very big that's really alright okay nevermind okay giving it away all right Lizzie is high variance this year's high bias it's a high variance hi yes Weiser and so let us just think about this for a second right so what's happening in these two regimes alright so let's first think about this thing right so here we have the high bias regime so what's happening your training algorithm you're using in both cases by the way I assume you use all your training data right so basically you increase your training data and you're stuck here right this this is where you stuck basically you've used all your training there right you move from the left to the right does that make sense so if you're in this regime here the important thing is your training error is higher than absent right that's very very important like your ivory them cannot even achieve the error rate that you want on the training error on the training data all right the data it was allowed to look at so this means something is wrong right you somehow modeling the wrong you know you have the wrong model for this data set right you have a bias problem okay so in this case a few things are important if you're here more training data will not help you can anyone tell me why more training data will not help me in this case I've seen this many times people in this scenario and they go like our error is still too you know too high let's add more it's gonna help always helps it doesn't why does it not help yeah right so adding more training their data always decreases your test there that's good but it only decreases the test error like the gap between the test arrow in the training error test arrow will never be lower than the training error and the training error will only go up if you have if you add more training data Hey so the training thing will go up and the testing will go down right but it will never move freely up salon it cannot write because the training error is about the epsilon already and it will only go up okay so you're shrinking this gap but this is not your problem your problem is not that that gap is too big that's very very important that can save you thousands of dollars if you spend a lot of money on new data and then you realize afterwards it didn't help okay so what do you want to do in this case right I guess I maybe I don't write them all down and say you know so though how do you detect this receipt the important thing is that the training error is above Epsilon the training error itself is too bad and number two the second giveaway is that the test error is only a little bit higher than the training error like there's a small gap and both of them are bad that's a high bias region so if you're not setting what you need to do is you need to make your model class more complex so you have to increase the power of the algorithm clearly your algorithm has a hard time understanding that the shape of the decision boundary on your data set right so for example because you made a linear assumption and it's just not a linearly separable data it data set so in this case what you need to do is you have to change your algorithm to be more complex there's a few ways of doing this so one thing is kernelization people talk about this maybe still today add features all right so something is not expressive enough if you add more features your algorithm maybe you know maybe more expressive so where we have Molly doing out of order yeah and boosting and boosting is something we will cover again in a few classes there's some sense you see what we are doing here right now I'm telling you how to detect the two regimes in the next couple of lectures I will then show you what to do about it so you will cover boosting will cover kernelization and adding new features it's kind of on you any questions about high bias alright so the other one is high variance so what what is the high variance set again to give aways the training error is below Epsilon all right so the on the training error said you're doing great right if that was the error you wanted but the problem is the test error is too high so you have a big gap between training and test error all right so what you need to do is shrink that gap that's basically what happens so all the remedies here will move up the training error and move down the test error and they've a little bit if you're a little lucky then they will basically converge below that epsilon and then you're done okay and if you're unlucky they will converge above it and then you're in a high bias regime right then you have to address the bias and so that means you still have too much bias left so you're trading off one for the other as that's the whack-a-mole thing right - to reduce the same thing by the way here if you address the bias you know if you do criminalization you add features you do boosting what could happen is that now your training error goes way down right the testing arrogance probably also goes down but they could now be a big gap and the test error is still above Epsilon that means you now move you traded up bias for variance so now you have to dress the variance right and so that's in some sense this is this balance that you have to strike so in this case over time the training error is below Epsilon that's good but the testing error is about so there's a large gap between the two so what can you do one thing for sure is you can add more data all right so the high variance case adding more data it's great because adding more data will move training air up and and test error down so that closes the gap I'd hopefully be low epsilon then you're done another thing is bagging that's something we will cover very soon it's pretty cool and so another thing is you can reduce the complexity right so maybe you're using a model that is too complex and it's hard to get the regularization right so reduce model complexity one thing you do decrease decrease regularization so increase sorry increase regularization and here by the way if you have to do decrease regularization to have that here I just have decrease and increase regularization as in terms of model complexity I guess in this case ok any questions about these two settings and what to do yes if your noise is higher than epsilon oh then you blame the person who collected the data that's hard right so if your noise didn't even remember what do you do in case your noise is so high yeah you can't add more features right so that that's one option noise is a function of the features right and you can also do data cleaning so if you do this a couple of times and you're always above Epsilon at some point you seem to suspect that your of these bars right of this variance you know variance bias and noise right maybe that in itself is larger than epsilon right so that I could of course be that's that that's too large and you have to do data clean I'd also look into all the labels correct etc right but it's certainly adding features is a good one yes good question yeah it's the same thing we just divide it's just you know divided by the data set oh I see I see yeah yeah you had yeah if you compare with more data you always have to average yeah yeah yeah but the error is usually a rate right so in some sense the error means if I put a sample a point randomly what's the probability of getting that point wrong that's that's what an error is right if you have more data the training error will always go up if you don't change your model yes and because it's harder or you may stay the same if it's totally separate but it will never go down yes yeah I tell Mon you converge right so this this graph you converge right so at some point of AC doesn't make any difference anymore but more data will never bring the training error down that's the important thing to know but and chance there is always greater equal the training error yeah one more time sorry yeah yeah in this case yeah just to get this graph puts it yes one thing you can do by the way is if you do cross validation then you could actually get standard deviations right and that actually is not a bad thing when you to be honest when I do this graph I do k-fold cross-validation and then I actually draw era bounce around here because that's actually very informative how much does it actually bury yeah yeah no no any more data would not help you write your training error still zero like if you have it's a linearly separable data set and you get a completely right and you have zero training error adding more data if it still is linearly separable it's still zero and if you're really unlucky you get some point that now makes the data set not linearly separable anymore and now you get a larger error but it cannot it is not going down by the way it's always a little noisy if I say it can't go down you know that's up to the fluctuations that you get yeah okay any more questions so please keep this in mind right you will be later on if you use machine learning algorithms in real life right you will be in these scenarios right well you basically I mean that's in some sense all we ever do right if it just works then you just stop you know then you're done but typically the reason they hire a data scientist is because it's not easy and so you always have algorithms that don't perform well enough and your job is it to now improve this algorithm that's exactly why they hired you right so if you now you know use a systematic approach right you can actually you know do much much much better than if you just randomly try different things and so what we will do now is actually go through the different settings so there's a lot of stuff here that I actually wrote down and that we haven't covered yet and that's will be the next couple of lectures so Bayes the attackee very system high variance setting and high bias setting yes you use its per sample so you divide by the size of this data set and there okay let's just move on to kernels I should let me give you a few sorry you over here [Music] all right and what I want you to do just to you know you start with a little quiz so here's basically the idea imagine you have the following data set here my across this and here my zeroes my O's and not across this and as you can see basically these are drawn from some kind of distribution with ik the o's are basically in some circle and the crosses are right of the you know origin right there's some small Gaussian distribution here basically of crosses and the circles are around it and just holding a said it's just two-dimensional so this is an old trick is basically and people came up with this long time ago is to make like a safar is more powerful one way is just to add more features right does anyone remember in which regime do you want to add more features high variance or high bias I heard both of them high bias right high bias so let's assume here in a high by setting and why is this because we use a linear classifier right you your cousin who you usually trust tells you this data is linearly separable don't even look at it trust me right so you you know you invest in this and you by a linear SVM you even pay the patent fee and everything and now you want to train this turns out it does pretty badly alright so what I would do a busy chain something like this right so you get roughly 50% error so you feel like something is wrong so you look at the data set and now you've already invested so much in the linear classifier you won't don't want to give it up right you've already paid the royalties so what can you do right an old trick is to basically you know in this case what do you know you could of course if you know more about the data at how to extract more made-up and more features but one thing you can also do is you can take the existing features and try to construct new features out of it and so basically what you do is you take your vector X and transform it into Phi of X where Phi of X is higher dimensional can anyone tell me a transformation you know of just one more feature to add to make this here linearly separable I'll give you a minute to discuss it with your neighbor I know some people already see it [Music] okay any suggestions yes sorry Oh polar coordinates oh that's good like this one yeah yeah that's right like you just take x1 x2 and just change it basically to the you know radius I guess and what do you need the the anger I guess right the angle so if you just have the radius and the angle radius and the angle then that actually works pretty well very nice I like this a lot that certainly works any other suggestions yeah it's right you could add another one could say x1 x2 distance when the origin is x1 plus squared x2 squared right square distance whatever that's exactly right and there may be others as well so and so the crucial point is that if I basically add more features to it right so if I take my existing features just add strange transformations of them and may make a very easy problem a very hard problem easy ok and so instead of actually taking a much more complicated classifier maybe I just take some you know add some features that's cheap doesn't cost me very much and now I can use this beautiful linear framework right and the nice thing about linear classifiers is yeah for example SVM's right they're very easy to learn you know exactly what they're doing so you know when I don't just do this any questions about this so you have a demo here but the computers acting up yes I have no internet and you would Yeah right so in this case you don't need you don't need x1x2 anymore but typically the idea is you just add more features that then actually make your problem easier right I mean you know it seems pretty balls pretty confident to remove them and that that would really you know assuming this is a very there's a toy example right but you actually happens to know the perfect feature which is you know in polar coordinates basically the radius you know the distance from from the origin typically you don't know that okay so so that's great right so typically in this case we know exactly this is exactly what you what you're saying we know exactly which features to add because we can just look at the data and the data actually it's trivial typically this is not the case right typically we have a data set you have a bias problem and we want to add more features to make our classifier more expressive but it turns out of course we don't know which features to add but it's actually very hard and and so a good thing to do is actually say well why don't we just add any possible features you can think of I asked the rat back and so here's the idea we can take our X 1 to X D and so on transformation we could make and say we take any kind of nonlinear interaction between any two features so the first thing we just say just add a bias term one Y well why not and then we have X 1 2 X D we just keep our old features and then we take any two interactions between it so let's take second-order you know of you know first-order interactions that's by Z x1 squared right and then wait is that correct that's our x2 x1 times x2 then X 1 times x3 x1 times XD then x2 times x3 and so on until you basically get at the end XT minus 1 times XT then you take three any three features are multiply them with each other right and you do this all the way until the end and you get x1 times x2 x XD that's NE d features my desire to get right so bathing and saying there may be some interactions between creatures that are nonlinear right then we have something if this features large in this feature is small that actually tells me something right if I just have a linear model that's really really hard to capture right so what I'm doing is I multiply the features with each other okay in this way I get these you know for example here x1 times x2 all right that's suddenly a nonlinear you know relation that I can now capture can anyone tell me how high dimensional is this vector is I'll give you a minute to work it out [Music] [Music] [Music] pick up okay could anyone tell me yeah it should be to to the party that's right so each one of the features can either be in or not in and so you have a binary decision so you have 2 to the D right there's no internet in this room apparently but I managed to get it on my cell phone so I want to show you a little video so let's see if this works okay awesome so what do you see here it's is a data set you see the blue dots in the middle and the red dots around it right and so the classification boundary they would like to have is the circle around the blue points you know that separates the blue and the red points but we have a linear classifier so we don't know how to do this but I now show you basically I just want to visualize to you there the you know if you may see now add more features you just have to add this one feature that basically is the distance on the origin now this turns from a two-dimensional data set into a three-dimensional data set oh yeah it's the good stuff and now and now we a decision bandit a boundary now it is no longer line it's a plane right and you can fit the plane exactly to cut off the blue part right and if we now go back to the original data set and visualize this decision boundary right this is what we get all right so we get exactly the session boundary want justice enough we thank you all right I see you all on Wednesday 
","['', 'bias', 'variance', 'noise', 'regularization', 'lambda', 'cross-validation', 'validation error', 'training error', 'overfitting', 'underfitting', 'early stopping', 'naive Bayes', 'squared loss', 'solution', 'exam', 'median', 'score', 'distribution', 'project', 'deadline', 'office hours', '']"
"welcome everybody please put away your laptops okay one quick reminder the project on erm is due today you may want to use some some late days I looked at the top scoring teams and I really like people have done very principled things to to get to the top so it's very very nice I will maybe donate some time after spring back to talk about the winning I'll give the winners some chance to kind of explain what they did but it seems like they really know what they're doing any questions about logistics sure okay any other questions all right okay so we've been talking about bias-variance tradeoff and so if you lectures ago we basically established that the error and machine learning decomposes into these three terms you know the noise bias and variance and what we will do now is basically try to debug or give you tools to kind of address each one of these carefully it's the last lecture we talked about how to identify if your problem is a high bias problem high variance problem or neither one of these than it's a high noise problem and now we're getting into how do we combat these and we are focusing for now in high bias all right so high bias crémeux tell me how you diagnose a how you diagnose high bias what happens when you have high bias how do you recognize who remembers yeah hi training errors I did the small gap between training and testing yeah that's the important part right so your training error in some sense it's already too high and your test errors actually not much worse right so the problem is not the gap between training and and testing which is variance averages the high variance but instead actually is that your classifier itself cannot even do it on the training set right that's clearly high bias but could also be attributed to noise but typically it's high bias so one thing we did last time is to say well I gave you this very simple problem where we said we have these circle from the middle and then you have crosses around it means that what we really want to use a linear classifier because we just got a discount on SVM and how can we apply a linear SVM on this dataset and turns out this way it doesn't work right there's no way you can kind of divide these into crosses and knots but one thing you can do if you just do it add some features so in this case actually if you take x1 x2 and you will just I had some features for example x1 you know one person actually suggested a polar coordinates but you know you could also just add for example x1 squared x2 squared or the sum of those two right that's the same thing because if you assign weights to those who stand them up and then actually suddenly this becomes linearly separable right it's really really easy it's a really easy problem so that leaves a makes us believe well maybe that's what we should do right so maybe one good way of addressing high bias is just adding more features and you know of course if you have your data specialists you could maybe extract more features from the data by Vaizey for instance to you if house prices maybe also get the school district or something of the average grades of school district or something that's one that's the kind of the data and creation part we're looking at the just you know that's it you have some features already can be somehow combined these features to create some you know to capture them nonlinear interactions between different features one ideas or in this case it's it's simple we can actually look at the data and you can just figure it out ourselves it's more like a puzzle usually that's not the case right usually you cannot just plot the data and just you know futz around with it instead you know you actually have to do you know you know that's say the data is hundred dimensional while you wouldn't really know how to visualize that in that way so one thing is if you just try to model all the different interactions between different features so one idea is we have this vector vector X 1 2 X D and one thing I proposed does we say well what we do is we just add features between of any possible interaction between any of these features so the first one is just a constant that means no interaction between any features and we you know we say okay this is just the feature itself and let me take any square terms so x1 times x2 x1 times x3 and so on x1 times XD and we have x2 times x3 and so on and you keep making you know all three very modifications until at the end we get x1 x times XD and that seems like a good thing and probably captures a lot of the you know possible nonlinearities you could have in your data set so that's very effective at decreasing bias but there's one downside and that's you know how many dimensions is this and last time you you figured this out very clearly does that even remember what the dimensionality of this vector is yeah there's two to the T right why is it two to the D yeah that's one way up that's right another way to looking at it you just always have X 1 2 X D and each one of them can be on or can be off right and then actually this way you get exactly two possible options for each one of these terms this is the option where everything is off so everything is just 1 and here is basically you know everyone is on so you have exactly 2 to the D possible switches and that should scare the living daylights out of you right because you could easily have a thousand dimensional data set to to the thousandth is more than there's electrons in the universe right there's no way you can possibly write down that vector so it's great but right now it's not very feasible only for very small data sets and let's not get scared by this and let's just continue to pretend we don't have that problem that's kind of looming over our heads right so you want to do this stuff and but let's just you know ignore it for now so what I want you to show you today is you know basically two tricks how to deal with this so you would like to extend the dimensionality of our data by and of combining features with each other that seems like a good approach and the problem is this is really hard I mention all that it is you know the first thing that's really really slow and B that needs a lot of memory so the first thing I want to show you is just a simple trick very basically and I just you know that's there's a lot of mathematical reasons why you want to do this but I now show you just a very pragmatic reason and that's well one thing we can do is if you have a linear classifier turns out we can actually express the little clearly a classifier in terms of inner products and if the only access our data so in the products then you can pre-compute these and therefore you can kind of store them compute them once and then during the training we just save them so we just pull them up so so let me just explain that so let's say you have a matrix K you say K IJ equals x I transpose XJ okay so basically what I'm trying to convince you off and there's part of the homework so I kind of primed you here so I think one one book ago you had to do this already it's well if I can express my entire classifier in terms of inner products then one thing I could do is I could pre compute the inner product and the advantage is that I just have to look them up I don't ever have to compute them and you know one reason you may want to do this is because you already have the feeling these are going to be really expensive these are the paths okay so let me show this that you can actually do this right that basically we can express everything in terms of inner product and so let me for now just look at the square loss just because I like to square us but it also holds for all the other losses that we actually talked about so if I remind you the stare loss is w equals and we sum over all our points 1 to N and the loss is w transpose X I minus y squared okay so what's the gradient of this the gradient and be the gradient descent it's please correct me if I'm wrong now W transpose X I minus y I X I ok so this is the last function this here's the gradient it's called this G ok so far so good now I'm so I did the the notes are a little bit different I just realized just now on the you know as I walked over that maybe I can explain this a little bit more condensed so here's the trick the first thing I want to convince you off is that at all times during our algorithm this was part of the homework if you remember and our W can be written as a linear combination of our inputs so I that W equals one to n alpha I X I for some assignment of off of alphas okay so that's that's a claim that I show you approved to you in a few seconds and their visit shoe right then actually think about this what happens here right in this via he only access our data in terms of W transpose X right here we only exit our data in terms of tau B transpose X well there's a little trivia but don't worry about this for now so let me have W transpose X well W actually is this thing here right so what does W transpose X become let say W suppose XJ becomes the sum over I want to n alpha I X I transpose XJ well this here is just K IJ o here just just to be clear what I'm trying to do here is to express everything in terms of inner products because then I can then I can pre compute the inner product save them somewhere memory and then just look them up every time I access one I didn't actually have to compute it and the nice thing is now my vectors can be very high dimensional it doesn't matter as long as my inner products are pre computed and good to go okay that's not worry about the pre-computer kind of computation of the inner products so all right so when we doing training basically you know the only exit our data to this W transpose X if W actually has this form that it's just you know a linear combination of my axis which I claim is true and I will prove to you in a second then actually everything is good right then W transpose XJ for any vector X J might reindeer set just becomes this following thing we just just you know the sum over alpha I and then K IJ any questions at this point okay good so let me briefly prove to you that off W really is of this form who remembers the proof from the homework Wow maybe wasn't not on the homework it was on the homework right okay okay I'm well on listen Skippy who remembers there was a homework of a couple all right good and okay here you go so here's the algorithm we do gradient descent the square loss is quadratic so it's convex so the beautiful thing is that we can initialize our W anyway we want initially so let's just say initially I do it by induction okay proof by induction so W zero equals the all zero vector that's how I start right I just start there because actually if you have a convex function that you're minimizing integrating descent it will always get to the minimum it doesn't matter where you start but you can start here you can start here it doesn't actually make any difference so I choose the starting point and I choose it to be all zero right and the reason I'm choosing around zero because now I can actually write well obviously that's a combination of my exes what are the values of alpha in this case it's all zero right after I equals 0 for all i okay so check right we're good right at the beginning we start out and our vector W is a linear combination of all our inputs great now we make it update and so in some sense assume basically the case already assume that now Riaan iteration t and wi aw equals sum over i equals 1 to n alpha i XR okay this is just proof by induction now we use the induction hypothesis do you now prove by induction who knows proof by induction okay thank you thank you all right so we assumed this now we do one more update so what is the update the update is w becomes W - step size s times G okay so what is G G is I have it up here is this term here right and the sum of our eyes but 2 n 2 times this term here times X I sum of all X honey well I can call this your gamma I this is just a scalar right the scene of scalar this is a scalar this is just a scalar so my G is gamma I X I okay and so what does this mean this means this becomes this years but W is just alpha ixi that's the induction hypothesis minus s times G well G is this thing gamma I X I and if you write this what does this become this becomes sum over i equals 1 to n alpha i minus s gamma I X I so this is my new alpha alright so this actually proves there's already we're done with the proof right so it is true initially all the alphas are 0 if it's true at the beginning of any iteration it will still be proved with the proved true at the end of the iteration that we just changed our alpha alpha I becomes after I minus s gamma so another way of looking at this is what we could do is we could actually change the gradient descent algorithm by saying initially alpha I equals 0 for all i and now we iterate you compute gammas compute gamma I for you know this is another loop compute gamma I and then update your office by saying alpha I becomes alpha I minus s gamma and now loop around any questions yeah that's fine still just a number right it's just a scalar that's okay right at the end you know this is still just a linear combination of my X's right where the scammer comes from doesn't matter right if you put in the age of my mother-in-law right that's fine right it's the no depends on the age of my mother and my mother-in-law but doesn't actually matter it is still a linear combination of the exes yeah any more questions this is only to Bolivia classifies so what we had be trying to make you're holding on to the linear classifiers we want to make them more powerful right because linear classifier is a beautiful okay any more questions okay Arthur so she comes a beautiful thing so now we can basically we have our linear classifier and we can basically write it instead of actually Co ever computing W right we just have to compute n alphas now why is that a big deal right now can anyone tell me why this is incredibly important when you deal with two to the D damaged think about if we started the lecture by saying we take our feature vector right and we map in just a really high dimensional space and now we learn a classifier there right this is 2 to the D this is ridiculously damn high damage right if we have to store the W vector in this ridiculously high dimensional space you would use up all disks on the entire planet right even those in North Korea right it's only a couple but so what I just told you is we don't have to do this all you ever need to store is n numbers UN alphas right and it's independent of the dimensionality of your data right so the storage we need to do to do this I've written is independent of the data and I mention alt right so that will allow us to go to really really high dimensional spaces actually by the end of the lecture you go to infinite dimensional spaces all right so that you know certainly reassuring that we don't have to store into the dimensional W so in Phase II the way we now - clintus and we just store n data points where n is number of data points s is our n mates these alphas for enzyme of data points so it only scales with n and is always finite right the only a finite amount of data any questions about it ok so we can do the training by just storing these alphas how do you do testing well if you do testing your H of X is there's a test point it's W transpose X right W super super high dimensional we never computed it because we were way too scared about it so what we do instead is we do the following we say well this is just W is just I equals 1 to N alpha ixi and now we just have transpose X okay so now we can order do the testing without actually ever computing double okay so doing testing during training and during testing we never need www different X's right that's exactly right right and you couldn't have asked a better question right now because that's exactly the last piece all right last piece of the puzzle so I can show you that you can also do that so the first magic trick in some sense is that we can train a linear classifier in this extremely high dimensional space without ever ever computing the W that defines the hyperplane we just have to store these alphas one for every data point and we wrote everything in terms of inner product now what he's saying is wait a second right you still have to compute these inner products right so when you compute kij that's X I transpose XJ right or here the test point if it'll be in a product for every single training point with every single test point have you gained anything right that still seems very very expensive all right so here comes the second magic trick are you guys ready all right Arthas ready I want didn't know those are yo all right good so let me just let's just go back to the very beginning of the lecture where I showed you this this description of you all right so let's just go with this for now and say we use this expansion right just because it's very convenient and it would be awesome if you could do it so all right here we go so here comes the magic trick so because this here PI of X X goes to Phi of X now we want to compute the inner product between two such vectors let's call them X and Z so you want to compute this means X transpose Z becomes Phi of X transpose Phi of Z ok any questions at this point raise your hand if you understand what I'm doing ok so base if you're mapping our data from this low dimensional space and this is ridiculously high dimensional space right scares the out of your pants right I'm in this high dimensional space we only have to compute in the products that's the only thing we need to do let's do it so we have our data points now we have to compute then a pair efficient at five acts of piracy what does that mean well we compute the inner product between this vector and the same thing with disease 1 C 1 ZD Z 1 Z 2 T 1 C 3 C 1 C ok so if you write that out what is this alright it's first these two multiplied 1 plus X 1 Z 1 plus X 2 Z 2 plus plus X DZ d plus and so on and so on plus X 1 XT plus a times C 1 CD ok that's a massive sum it has to the D different terms that we sum up but I claim actually if you think about this what are we doing here it's actually exactly 1/2 D the product between these terms 1 plus XK CK that's it and this only takes D iterations this kit takes a few milliseconds so this it takes computing this takes from now until the end of the universe right so well the solar system let's say the Sun collapses collapse into the Sun like you know ten million years I don't know how many years and maybe five billion years this year it takes ten milliseconds the answer is the same thing you can choose right now which one you going to do so maybe spend a few seconds you and your neighbor and convince yourself that these two are actually exactly the same thing [Music] [Music] [Music] [Music] [Music] all right who thinks it's crystal clear why the - those two are the same raise the end all right it's a Fairmont okay so essentially what we are doing right if you basically take these these terms right we have one plus let's say we just have the simple case where you have x1 and x2 we blow this up right this becomes 1 X 1 X 2 X 1 X 2 right and if you take the inner product which we choose X vectors and this would you know correspond to this x1 z1 1 plus X 2 Z 2 right so basically everything you may see take all the costumes right you take one one times one that's the very top one then 1 times X 2 Z 2 then 1 times x1 z1 and then at the end these two cross tabs which is exactly what you get right you get 1 plus X 1 z 1 plus X 2 Z 2 plus X 1 Z 1 X 2 Z 2 and that's basically all possible across them so the number of costumes you have is exactly 2 to the D and that is the exact sort of list that we have here any questions about this so what I just showed you is that we take our data like we do kind of two crazy tricky right we take our data we map it inches at infinity or not infinite like exponentially high dimensional space and we run our algorithm in this high dimensional space but actually we never once compute a single instance in that high dimensional space we can't because we couldn't even afford it but because we only need inner products in this high dimensional space we can compute those very very cheap do you have no idea of what these vectors look like you never have to compute this guy right because you all you ever have to compute is the inner product between two such vectors right you can do training in this in that high dimensional space right and the solution is exactly the same as if you would nap your data up there and it's extremely powerful because now you capturing any possible nonlinear interaction or you know a pairwise interaction between any two features yet you never compute a single one of them and the answer is exactly the same so all you need to compute once is this kernel matrix K IJ equals Phi of X I transpose Phi of X J right for any two points which is just order D computation right so each one of them takes a few milliseconds so it's not a big deal at all so suddenly it took a very simple classifier let's just can make a linear decision boundaries I made it exceedingly powerful another way of looking at this is that all we need from our vector space isn't in the product function and you can also view this as kind of saying we just really find in a product we say the inner product between two vectors is no longer just X I transpose XJ instead the inner product between two vectors is K of X I comma XJ where that is some kernel function right and where that actually equals this right so this here actually equals K of X comma Z and so I just define a new inner product and I run my algorithm with this different inner product and I know this is well-defined because I know it corresponds to some extremely high dimensional vector space in which is just the good old inner product that you all remember right from high school any questions yeah yeah you have to be careful now the dimensions are no longer there particularly interpretable right because you have an excessive number of them and they really kind of capture different combinations of existing features but the notion of the margins exactly is saying that the classifier doesn't change at all yeah the loss so here's what you do where don't oh he raised it sorry okay well let me write down again so then the last was the following right my last was L of W well some of our I equals 1 to n W transpose X I minus y I square R I try to square s square regression ok so now we realize that W can be expressed in terms of alphas right so I can write my hole loss in terms of alphas and W it becomes I equals 1 J equals 1 to n alpha J XJ transpose X I minus y I squared right now I have everything in terms of inner products now I plug in my new inner product function and I just say this is actually K of X I comma X J which is this function up here and now I can just compute the loss and it takes me now sub seconds right no big deal at all super super easy and you take the gradient with respect to alpha and you get exactly the gradient descent algorithm that we wrote down in earlier it works for all other ones we talked about yeah good question and no you just use all of them but because why not right because you could assign zero way to them so he comes the crazy thing right so he's maybe saying the following right he's saying wait a second yeah there's 2 to the D possible features right and some of them have capture interactions between features that I don't care about I don't think there's any interaction between these two features right so why do we blow it up right but it's not a big deal because you assign a weight to each one of them right you have this gigantic weight vector W and you can just assign 0 weights to these right that you don't care now the cool thing is that you never actually compute those those W vector right but implicitly right by choosing your alphas you actually assign your weights of these right this it's pretty crazy that he can do this right this without ever computing the vector yeah sorry if you yeah okay good good so the question is is it linearly its data not always the nearly separable and let me let me show me a few so though let me get back to this question three minutes so one thing I just I just showed you I just showed you this very very carefully constructed feature expansion right that we basically take every any possible cross you know correlation between different features and I showed you that that can be written in a you know in this very compact form right the question is is there other are there other inner product functions that we could use and turns out actually there's tons of them at tons and tons and tons of them and you can make your own actually and if it catches on you can name it after yourself or after some loved one or something so let me give you a few examples off in the product functions that we never use so the first one and they call these kernel functions for good reasons because they're actually what we're doing actually is via by doing this we are actually mapping our data and they're reproducing kernel Hilbert space that actually is defined to that count so the linear kernel function is very simple that's just K of X comma Z equals X transpose Z right if you use the linear kernel the algorithm becomes the good old linear classifier that you're all familiar with that you all like from you know that you did before the midterm another one is polynomial crap so here we say K of X D becomes one plus X transpose e to the power of P actually not the P well I can gone D whatever doesn't matter some constant just raise that's not constant well that captures is actually some sense now you get if P is 2 then you basically can model quadratic functions if P is 1 it becomes the linear kernel exactly plus some constant which doesn't matter right so then again you're just learning linear functions becomes quadratic you learn quadratic functions it becomes you know three than you learn cubic functions etc the most famous one this like this like the Brad Pitt of kernels right it's like people you know sometimes faint when they see it it's like it's called the radial basis function a function kernel yeah RBF solve got RBF and the RBF kernel K of X comma Z it's e to the minus X minus Z squared over Sigma squared like in mitosis this is vector yeah and so that should look familiar to you does anyone remember this never know the cousin after the of the RBF kernel there's a Gaussian distribution right so the RBF kernel is so popular for various reasons number one you can prove that it's universal approximator so what are the years the universal proximity means that actually you can fit any function arbitrarily closely given the few assumptions so going back to your thing your question is now everything linear set linearly separable and the answer is yes but if you use a RBF kernel suddenly every problem becomes linearly separable provided you don't have two identical data points the only exceptions like if you have two data points that are identical and have different labels you can't do it but you know taking that one point away you know that you know which is ridiculous then actually everything becomes so you can learn anything you want it's just a simple linear classifier the RBF kernel corresponds to us to a space that's infinite dimensional right so you can't actually write down Phi of X right it corresponds like they exist some Phi of X such that you know K of X see equals Phi of X transpose Phi of C but this is actually infinitely dimensional so you can actually never write it down completely but you can approximate it pretty well that's arguably the most popular color if you you know in most problems if you use one particular product kernel that is that is the one that usually works best out of the box in one there's a connection with the Gaussian distribution and years in some sense what it does and what the RBF kernel does it takes your data set and puts many many little gaussians around every single data point this here's the Gaussian this here's my training data point right and Sigma basically tells me how wide these gaussians are and what you do is by doing this basically you get some weird space where you basically say how similar are you to any given data point and the reason you can basically approximate anything you want is because if you have enough it's kind of like it's similar to the nearest neighbor proof to some degree but in some sense basically if you have enough training data then actually there's always some trained data point you put a Gaussian around that data point and that then defines a one dimension in your into the dimensional space and does that make any sense so it said you know it's a little bit hard to kind of picture these infinite dimensional spaces but but you know you get used to it it's it's a nice world to live in that's a few more let me just think you know how much time do we have I want to show you a little demo there's a few more kernels I wrote them down few people use them the most common one is is this the RBF kernel now here comes the important part and let next lecture actually we'll go to this and into this a little bit more that you know that the natural question is what is a well-defined kernel all right he just do anything okay just make my Killian crown right and just you know take any kind of function and just say that's my you know take two inputs and say the output is the car and the answer is no it has to be a positive semi-definite function so what does that mean that means if I take compute this matrix K for any set of vectors that matrix set has to be positive semi-definite who knows what's positive some definite means raise your hand it was mentioned briefly on the homework and so a matrix is positive positive semi definite if if and only if there's a few definitions the K is positive semi definite this is how we write this if and only if for every vector Q Q transpose KQ it's great equals zero and that's the case if and only if K can be decomposed into Z transpose Z and another one is if all the eigen values are non-negative and it's real and symmetric in this case so this here is the one you really want this is this should make it obvious why why why you have to have this definition these Z's could anyone tell me why this is why why there you know if you know this is the case then it must be about a very fine kernel can anyone tell me this yeah that's right that's right that's exactly right so each column here in Z is one particular Phi of X right so these these are basically the really high dimensional feature representations right and if I just end up you know based you know now it's inner product between it is inner product function right this doesn't have to be ridiculously high dimensional and the reason is because this matrix case only over a finite number of points and if you have a finite number of points they always like in a finite dimensional space right so you can just do PCA on those points so that's why this doesn't have to be so even if you have an RV f kernel you can take the kernel matrix and compute these Z's that's fine you know so the important thing is for any set of points if you compute the kernel matrix K you can do this decomposition so that's basically that that's that's the only condition you have to have so the beautiful thing about this is that that's extremely it's extremely flexible right so once this came out people went nuts right people in biology defined kernels of a molecules right and over sit DNA sequences right there is no vectorial representation it you don't need it right you just have to define some of the product matrix that's actually always positive semi-definite right so it's extremely powerful you can define in the product over sentences right over all sorts of weird data compare constructs and now you can use all these linear classifiers like SVM's right suddenly they apply to data sets but you don't even know how to write a vector right yet you can define a linear classifier all you need to know that is in principle they exist some mapping from this theta into some infinite dimensional space which I don't know but it doesn't matter because as long as I can compute in the product I'm fine so it's incredibly powerful and when this came out the whole you know machine learning world went crazy this was in 2000 you know this was basically with SVM so because Karina Cortez in is a valid yo and and they be introduced this for their support vector machines and that's why support vector machines became so incredibly popular and for I think 4/5 10 10 years the whole machinery fields did nothing else but you know mapping everything into kernel spaces and analyzing these very very thoroughly let me show you a little demo that you see a little bit what this looks like in practice any questions about I'm including the why you can't have really terrible kernels I guess right the zero kernel maps everything to zero or something so what do you want is that a Colonel says similar points are similar when they have similar labels right and so you can encode that if you know a lot about your dataset the better you encode that the last data you need to train it yeah [Music] ultimately people typically use the the RBF count yeah any more questions yeah as I really still limited by alphas what does that mean that's right no it connects all these last functions are convex so you can you can optimize them to arbitrary precision so you can use the you know second order method like the Hessian and and you can you can make this as precise as your computer can represent it okay so let me show you there's an SVM well let me actually first okay so here's an RBF kernel which regressions the Richard Grieco just draws on points and and now have the RBF kernel so what you see is the following so what I'm changing here is may seem a regularization term and my Sigma and down here actually if my Sigma is very very wide you can't see this because the blackboard is in front of it and this becomes the linear classifier this becomes the gaussians of the RBF kernel really really right but if I just changed my thinking I'll make it smaller and that's that's going upwards and if I regular rise less right and actually you see on top left you get an extremely powerful classifier right so you basically and here somewhere in the middle right you get something that actually captures this this data pretty well right here right I basically come you know I have a variance problem right I'm totally over specialize into every single point of me they have a spike that just hits exactly that point right so I'm memorizing exactly my training test set right but just shows you the RBF kernel can learn anything right but this is too much that won't generalize well but if you're going on here in the middle right you see beautiful curves right that capture your data set very very nicely right so what do you now just have to do is you have to basically find your lambda and your Sigma because you find the right hyper parameters and you can learn any any data set you want let me show you another demo for actually classification so let's make some really nasty data set and what's a nasty data set any ideas any suggestions a smiley face all right good so what are the two classes in the smiley face so these are positive what are the negative points oh wait okay wait I guess I already made it wrong so let me just do this let me just put the other ones all around it here so this guy is like you know smiley person who has freckles okay good so now let's run this right this is now now you're running a linear classifier on this right a few lectures ago you would have laughed at me right hopefully so let's just do this well this is a linear classifier it was just if I mount a linear classifier doesn't do anything the decision boundary actually is so so it doesn't even draw anything it gets a precision errors so this here is if I take a linear classifier and I now show you the prediction values between positive or negative and what you see is that classifies everything at point two because it has no idea what to do so now I run RBF kernel da da right and so what you see here is all right so the blue is busy negative the the cross the positive and the white line is actually exactly the points that are one one in your margin one away from the decision boundary these are my support vectors right so we have the same thing and by the way this is a straight line in this high dimensional space right it's just curved and you know because we are so limited now you know in in 2d and I can now show you the value you know how much a certain point is positive how much is negative if I now this here is actually and I can show ya like I can now actually make you know show you this in 3d you need see a landscape right yeah but you see the smile basically right like it goes negative into these cars out these valleys [Applause] alright let's leave on a high note see you next Friday 
","['', 'bias-variance tradeoff', 'high bias problem', 'diagnosing high bias', 'training error', 'test error', 'linear classifier', 'SVM', 'non-linear interactions', 'feature vector', 'data dimensionality', 'kernel function', 'inner product', 'gradient descent', 'square loss', 'convex function', 'proof by induction', 'linear combination', 'alpha', 'high dimensional space', '']"
"welcome thanks for coming here despite being the last day before spring break and Dragon Day I think the reason I got the latest because I got stuck behind a gigantic dragon and all right please put your laptops away close your laptop's okay so we talked yesterday about Wednesday about kernelization and the idea is we have these linear classifiers that we understand really well and we like them a lot because they have these convex optimization problems and be you know just initialize our loss you know with some weight back to em you just do gradient descent super easy and out comes the solution is very very fast does everything about them is nice except one problem they have really high bias right it's a linear classifier software from high bias and with Microsoft very another Sun one to test better okay good so linear classifiers suffer from high bias and the question is how can we fix this and so the first idea we had was well what we do is we have high bias one option is just to you know add more features so one thing we could do is you could just take the existing features and add combinations of these features to ecstatically inflate our dimensionality and that works really well the downside of this is that it increases your dimensionality of use of your vector and it becomes very slow but then I showed you one thing is that we can actually write this vector W at least for the square loss we can write it as alpha ixi so we can write this vector W as a linear combination of these X's and the beauty of this is that or V actually in this let say B had tries to do this transformation 5x and we would have these fire effects and the beauty of is that we actually never have to compute this vector W instead we just have to store these n values alpha so the beauty is that now our vector waw is independent of the dimensionality of our feature space so we can project our data extremely high feature space and we can still learn it because we just have to keep track of n al first no matter what the dimensionality is so that's the first thing the second thing is then we realized that if you do a square loss and it's true for other losses as well effect for all the losses that we talked about is that the inputs are only accessed in terms of inner products so we only ever compute Phi of X I transpose Phi of X J that's the only way we access these excise and X J's and so the beautiful thing is that we don't actually ever have to take up data and map it into a high dimensional space instead it's sufficient to define our in a product function that computers this in the product and that's all we need to do and very often this is much much much easier exponentially easier and faster than actually mapping the data in a high dimensional space and then computing in a product that's if you have like in many cases you can compute this inner product extremely efficiently and that's what we call our kernel function that's how you call it ok any questions about last lecture yeah no right so a simplest one is a simple counter example the linear kernel x and z is just x transpose c well this can clearly be linear negative it has to be positive semi-definite so if you take n data points and computer bay tricks K such that K IJ equals K of X I XJ this matrix has to be positive semi-definite that's the constraint so you can't have negative inner products any more questions so in some sense but the inner product measures is of larger means they're more similar and lower means they're less if that's typically zero means a talk okay good so what I actually this is a great question you just asked because this is exactly why I want to go today I want to teach you how to construct new currents and basically that it's kind of demystify these kernels a little bit so turnable last time you have the linear kernel is this one this is a pretty famous one this is base if you just use that linear kernel then you get exactly the same result as if you would use a linear classifier you not didn't you didn't change anything you may still sometimes use a kernel function can anyone tell me why why would it be efficient you know why would it be a good idea to you know run your algorithm in the kernel eyes fashion and then what setting is this advantageous yeah that's right that's exactly right if D is larger than n right so if computing these inner products is very very costly because if you know in this case Phi is just the identity but vectors are really really high dimensional but I don't have many of them so I could just pre compute this kernel matrix K all right and then actually just run everything in this the spec so you have this for example with DNA data so DNA data you know typically you don't have all that much because you know let's each data point corresponds to a human where you actually you know how to sequence the entire DNA that's very expensive or used to be very expensive so your n is small but your D is ridiculously high right so in those cases it makes sense just once pay for basically computing all the pairwise in their product and then do all the learning in the inner product space in the kernel you know and the kernel eyes so the second colonel that we had is the RBF kernel that's the cool Carl that's what I refer to as the Brad Pitt of the columns all right so that's here of X comma Z I don't know if Brad Pitt is still cool these days but I guess it used to be X minus Z squared over Sigma squared and so the intuition here is that basically you put a Gaussian around every single data point and we say points are similar they are close by and they're if they're far away and we don't know anything right so this goes to zero very very quickly right so if they're far away then they raise the orthogonal right and she's napping your data into some hyper sphere it's a positive quadrant of that it's an infinite dimensional sphere that's what you're doing in the polynomial kernel and the polynomial kernel is this one where we have 1 plus X transpose e to the power of degree or D whatever this is the degree of my polynomial so these are the most common kernels but you can construct your own and what I want to show you today is some rules basically that actually constructing young kernels is quite simple and it's to some degree a super power if you know how to do this right so if you actually have some specific data let's say you go into you know any kind of chemistry or biology I don't know what your PhD will be or you know whatever like what idea application we'll be in and it helps you know if you can actually take some weird data and be able to construct a kernel that can make learning very efficient without you actually having to squeeze it into a some vector form so the first amazing IV now wrote down eight rules for you for you know how to make kernels there's more but those are the most common ones and the first one is that the linear kernel is a crop right that's very easy to prove it's a positive definite matrix because in a product matrix so that's that's you know you just do that on a you know on a rainy day like today here of X comma Z X chance we'll see that is a well-defined curl that's my first rule so you know that's what we've been doing the whole time sets in terms and all I'm saying this is the value find in the product well duh right that's how you know that's what inner product is D that's the product you learned in high school right the second one is more interesting the second one says ever have a a kernel function K 1 of X and I multiply by any constant that's non-negative sorry that's actually that's positive and if well I kept is zero but then you get a trivial it's a trivial case so if it's not negative and this is still a valid if I'd come okay so the base is just rescaling all my inner you know my inner product by some positive value still gives me well-defined and a product well that's that's you know that should be very it's actually art we know from the definition of inner product it just follows right away please ask questions anytime you know anything is unclear and there will be a bunch of quizzes today by the way so make sure you should know close to someone you like but you can work with them together all right don't move away to obviously so so the second one is if you have two kernel functions if you add them up you still get a comp so it's closed under addition so we have two kernel functions K 1 K do if I just add them up I still get a crown if you take any one of them and multiply them by a constant that's positive I still get a crown okay these are the operations and so basically what we will do later on is we say okay let's construct new curls and what you do is you start with something that you know is a kernel and then you basically transform it into the function that you'd like forests and there's a little more interesting so if you have a function let me just make sure I define this correctly here sorry I didn't define it Soji has to be a polynomial function soviet function K of X if I define my kernel as G of K 1 of X comma Z and I forgot to write down but GS and I believe it has to be a polynomial function of even degree it's a positive coefficient sorry ok that's right that's right ok G polynomial but positive okay that's obviously so you can basically take you know square this cubed etc right sticking it to any kind of polynomial it stays a kernel stares about it event in the product and number five is I can I'm just gonna skip this now I can just take two in a product functions and modify them it still stays a kernel six is an interesting one now it's getting a little interesting success if I have a kernel function and I and I multiply left and right right by any any function f of X on the left and f of Z on the right that's the lock are so basic and transfer my main my features I'm going to do the same transformation for X and z and x that kernel and that's still a kernel that's seven if i exponentiate my kernel that's still a kernel and finally eight if I take it pauses on the definite matrices not actually asked you know I'm looking at the survey right now like and ask people what's the worst part about the course and one person wrote positive semi-definite matrices so that's great ed now this is not me so if you take any positive some definite matrix and multiply it you know right a left from exit Z that's also a comp so that's the identity that you fall back this guy okay good any questions about these rules yeah that's the quiz that's exactly what I want you to do right now so pick your favorite neighbor I'll give you four minutes try to prove that the RBF kernel is a well-defined drop I don't [Music] [Music] [Music] pressin yes it's basically okay of X comma Z equals e to the minus X minus Z transpose X minus C that's the strv FK okay who thinks he or she has figured it out raise your hand some people a little bit a person yeah give you one more minute sorry so yeah Sigma is real it's not a mat it can't be an imaginary all right I think you were the first one do you ever be quiet please everybody yeah exactly right so let me just you know explain it one more time for those people and we thought it was a little faster bathing you start with X transpose C is the linear kernel that's that's rule number one that's about a fine kernel and you say I multiply this by 2 Sigma Square to a Sigma squared that's rule number two it says if we multiply with a positive constant it still occur then I rules use rule number seven where say I can exponentiate it that's now I have exactly the middle part and now I can't use rule number which what is it I don't see you right now all six who number six but there they can multiply this with f of X on the left and F of Z on the right for any function f of X and f of Z and so I define them with you right and then I get exactly this term on the left so RBF kernel in a you know four step process is very simple is exactly well-defined kernel yes thank you very much this very very nice yeah sorry but you know the function always has to be off too far that's okay of X comma Z is defined as extras received yeah that's about a fine Colonel if you make this X transpose X that's not a well-defined kernel that's that's a different person so don't yeah any more questions yeah he has a state the state has to stay apart if you set it to zero that everything becomes one and then you have a function that map's everything to one is that positive definite the old one matrix I know but if he had a polynomial that just raises the inner product to zero then it just really becomes one so any other kernel it B is the kernel matrix that's all once all right I'm not sure that's positive definite is it maybe this I'm not sure so maybe we can yeah we have to look into this yeah it's a minute in some sense only looks at as extreme it excludes that it that silly case but it's also used this case yeah I would assume it's finite I'm not I'm not sure I'm not sure to be honest yeah I don't know certainly holds that it's finite I'm not sure what yeah last question yeah so f of X goes to but as is the poem is like f of X you can swap X and Z right okay let's take the case I'm sure it doesn't work but okay let me do one more little just one more example and so please don't look at the next part of the notes I forgot to remove from the notes I put the solution up and so I will add in the proof for this when I put it online so another kernel I want to show that's often used as a set crowns that's a B if your data is not vectors you know they doesn't sex right you may have that in biology or in language actually yeah bag-of-words or something right and so you have set one and set to right these are not vectors and now I define a kernel as the following is say my kernel between two different sets that one set you is each of the cardinality of the intersection at a PC see how many points that these have in common and then exponentiate this and that's my kernel function and maybe spend a minute you and your neighbor discuss why is that a well defined so once again these are now sets right and I'm saying how many elements do they have in common and i exponentiate that and that is a very fine colonel so please discuss it with your neighbor without load don't look at the assets in the next page okay any guesses any guesses how to prove that this is a well-defined kernel but those who didn't look at the answer so the simple trick you basically say that's one more assumption actually that I didn't make here so in case that tripped you I apologize these sets must be from finite from a finite universe and so if you want to you know look at the intersection one thing you could do is you could theoretically say well I take each set and I might as a gigantic vector and but every dimension correspond to one possible element that that actually that that that has so you're busy say well si maps 2 to X i where X is a long binary vector and for every single dimension you just a so the one if I have that element in there and that's 0 if I don't have it in it and then this year is identical to saying this is e to the X I transpose XJ right and now we just have the first jewel and the exponentiation so the important thing is this may not be computable this may be totally infeasible just the fact that it's possible in principle makes it proves that this year is a value find curve right so it may be easy to look up that actually you know what intersection between two sets it's yeah question maybe right it's just for this thing here not so if you have an infinite dimensional vector you have two infinite dimension vectors you have to show that this actually this inner product is finite so then kind of get into weird exceptions yeah so bag-of-words essentially give you that sometime in bag-of-words are basically these I mean they're you could actually represent these vectors because they're typically very sparse okay good so in some sense I just want to give you a flavor of how to do these proofs and typically always sneak one on to the exam at the end but it's also it's a you know it's not just it's really a good thing to know because if you can actually if you can write a well-defined kernel function alright that's a massive machinery in machine learning that just takes as input a kernel function right so if you can come up with a kernel function there's a you know that's facing the key to a whole world of algorithms that you could apply to your data sets so keep that in mind and please remember how to do this and do it but you have to be able to show that it's a well-defined kernel function otherwise horrible things happen and okay any more questions and by horrible things I mean the loss will become negative so that's that's as horrible as it gets okay good so and what I want to do in the remaining time is show how to take algorithms that caramelize them so yes last oh sorry a question well larger values should mean two points are similar so you as a data expert can basically yet something could verify right it's a data points they're very dissimilar should have small in the product data points they're similar to have larger planets all right so here's the you know because these kernel machines so in order to in order to take a machine learning algorithm and turn it into a kernel machine you know kernel eyes it we need to follow a two-step process the first step is we take this algorithm and we have to prove that you know data is only accessed in terms of in the products and that also during testing time all you ever need is in a product so that's the first thing that's what we did last time that's actually all you need to do like if you look at all papers people try to prove all these represented theorems and so on people ultimately oh that is not necessary it's just there's a very pragmatic test just show that everything is accessed in terms of inner products and then define a kernel function and just substitute that in terms be the product so the first thing is you take your I with me you rewrite it in terms of inner product and then you look whenever there's no product you just swap in the kernel functions hey the kernel function and that's all you need to do so that's two they have another quiz number one here's quiz number one be and maybe spend a few minutes and try to kernel eyes the nearest neighbor algorithm so the nearest neighbor algorithm you check the AL to distance so we try to find the nearest neighbor with the yell to distance and then assign that label can you colonel eyes so please union neighbor discuss it I'll give you three minutes oh you need the rules no you don't need the rules though yeah there was on it yeah all right so who figured it out all right everybody please be quiet yeah yeah actually you can use any kernel you can use any kind right so the important part is you just say for all we need for the nearest neighbor classifier is to find the nearest points according to the Euclidean distance you getting distance is this you take a square root of this actually right and but so let me just write this this years the Euclidean distance but whatever is closest based on the Euclidean distance it's also close based on the squared Euclidean distance right so I can just take this on both sides and square them that doesn't change anything all right so if I take the squared Euclidean distance I still find exactly the same nearest neighbors and if I use the squared Euclidean distance this year becomes X transpose X minus 2 X transpose Z plus Z transpose Z right and hallelujah right in a product in a product in a product right so the entire algorithm accesses our data only through inner products so what we can do is we just stick in the kernel functions here and we say this is K of X comma X minus 2 K of X comma Z plus K of Z comma Z and you can use any kernel you want right any well defined kernel works with this there's now kernel nearest neighbors right nobody ever uses kernel nearest neighbors it's not a very good algorithm and I can explain to you later on why it doesn't make any sense but it's and you know it's just a very simple example how to economize but there's other algorithms where mixes and that the reason actually doesn't make much sense is because it's already a nonlinear so bias is not a problem of Kenya of nearest-neighbor right it's not a high bias argue with them so Colonel izing reduces fires we don't have a bias problem yeah Kenya's neighbor suffers from a variance problem but linear classifiers days off from the bias problem yes that doesn't matter like we're not constructing a kernel function when we kernel eyes an algorithm all you need to show is that every single access to a data point is in an inner product that's what you need to do right hmm the the other these rules were only to construct your own kernel function right so now you can stick in your own K your favorite K funds yeah yeah that's right love it yeah good so RBF kernel you would happens right so the closest the largest it gets is one so you would have one one here right into here depends how close they are they are far away then this becomes small etc yeah okay good let me go with another algorithm so the kernel kernel quick question and kernel regression is actually a very useful algorithm it's very very handy and the beautiful thing is it's one line of Julia there's also one line of Python R of T ok so the idea is the following we just have to square loss will be minimize W transpose X I minus y I squared 1 over n let's just drop the 1 over n just to make it simple there's the last time a minimize last lecture we already showed that accesses data only in terms of inner product if we make the following substitution W equals I equals 1 to N alpha ixi and we showed you that that's totally valid we can do this right so if you write our I'll W this way then everything just becomes terms inner products and today I just want to derive this from the closed form solution so if you remember when we talked about ordinary least-squares we said that actually has a closed form solution so this is actually a quadratic problem it's a parabola and the parabola has nice function we can just compute the minimum to right away it's essentially we take one mutant step and the closed form solution for ordinary least-squares is w equals XX transpose inverse x y where x is the matrix defined as all the you know we have all the inputs kind of stacked in a rows like basically the way your projects work and why it's oh you're wise here I think transpose yeah okay so the beauty of which we care of average regression and ordinary least-squares is that you can compute it with one line of code that's just this line you know the base just take your input you just you know compute the covariance matrix you take the inverse multiplied by your data and by the label and bam this is your W now you would like to kernel eyes this the problem is that this W via the RBF kernel will be infinite dimensional right so we something seems to be wrong we can't do this right but the good thing is and well we know it's going to work because we show the last time but one thing we can do is we can basically substitute for W this trivial as we take this definition of davi if we write this in matrix notation this means you take every X every column here and multiply by you know one alphabet right so basically what this means is W equals x times some vector alpha right so that's exactly the same thing as this all right just if you take this vector alpha and multiply every single column here and sum it up right so that's these two are equivalent so and that's our W so what we can do is we can just plug that in you can just say x times alpha equals this thing on the right all right this has to be correct and now we want to solve for alpha and hopefully what we will get is the expression that only responds in a product they can sneak in our kernel function and so currently not everything in terms of inner product but you can do a cute little trick and that's we multiply both sides by X transpose X inverse times X transpose so we multiplied by this X transpose X inverse times X transpose and why are we doing this because now this year basically dissolves all right this these two evaporate and if you have to do the C on the right side to now this gets a little ugly so it once X I this becomes X transpose X inverse times X transpose and now we have here xx transpose inverse oh my gosh XY and so now I already you know told you this here these two bases disappear if I do this trick and here on the right hand side what I get I get this term here X transpose X X transpose inverse X let's just look at this text transpose xx transpose inverse X I claims that I did it must be why is this well it's quite simple I can just multiply with X on both sides if I multiply with X here and with here then these here disappear and I get x equals x ok so this here must be the identity so this here disappears this year all disappears and the only thing that's left is alpha equals X transpose X inverse transform and we're done why are we done what's X transpose X that's the inner product matrix of X so that's exactly a crime all right that's basically every single entry of that matrix that's the matrix K where K IJ is K IJ equals x X I transpose XJ that's exactly this so basically what we get is alpha equals the in up inverse kernel times y and that's all there is to it so then in MATLAB if you want to solve this am at level and Julia here's the code y equals K X X plus y I believe so that's how you solve so it's literally one line of code and the beautiful thing is now if you do prediction right you this will have you I'll first you just multiply you it original data with it and it's a super super efficient algorithm that's extremely powerful any questions yeah ah good someone got me so when I multiply it with all these terms here I made some assumption that actually this year is in a this here's actually invertible right that actually inner product which is invertible and that's exactly right and when it's not raising what you do is you add a little bit of a ridge so what you do is you say the following alpha equals k plus sigma times identity inverse that's why and that's exactly colonel eyes which request that's exactly very nice point yeah that was just to save ink yes sorry yeah I know that rope universe okay thank you very much and these notes are actually written by students who transcribe lectures is very nice of them but sometimes there are little typo site so that I didn't catch all right good so let me just walk you through this one more time well I know who want to see it one more time raise your hand okay so basically we have the closed form solution for this for this experience here the closed form solution for this equation here is w equals x transpose x times XY z-- that's exactly this term on the left instead of W arise x times alpha and then I multiply both sides from the left with X transpose X minus 1 times X and I'm just doing this so I should a lot of terms disappear right because they're all inverses of each other and so the only thing I'm left with is X X X transpose so X transpose X this term here and this he actually is my kernel matrix so all I get is alpha as the kernel matrix times y let me see if two more minutes what do you do do still go to s VMs or would it go to the SVM with beginning of after the Spring Break pretty hand if you want to do it now after Spring Break it's a tie all right let's do this for me 
","['', 'linear classifiers', 'high bias', 'add more features', 'increase dimensionality', 'vector W', ""linear combination of X's"", 'alpha', 'kernel function', 'inner product', 'square loss', 'positive semi-definite', 'data points', 'inner product space', 'RBF kernel', 'Brad Pitt of the kernels', 'efficient', 'DNA data', 'polynomial kernel', 'construct your own kernels', '']"
"welcome to one more episode of machine learning all right please put your laptops away or close them okay I hope you had a great spring break so a few things I want to go over and so we had the survey and I was really interested in how can i what should I change was there anything obvious to change so turns out it's a little tricky and there were some concrete recommendations but for most things actually most things look pretty good the biggest biggest complaint was actually for karyam I took all the VOC areum feedback and send it to Polk areum some of it was pretty harsh but I you know I did not edit it but a lot of it actually got fixed so you know me working very hard I mean there I'm particularly a very very interested in making this work for us so a lot of people also broad like Bukharian was really terrible and now it's okay and and the majority of uxd one would say we should use it again next year so that's encouraging in terms of most other things it seems reasonably okay so the workload is okay and there's a few people who basically say this is a lot of work but as you know and this is typical food I guess these questions but yet what I'm looking for is is it really lopsided right so in this case basically I have you know actually more people who say it's amongst the lowest amount of workload then there's people who say it's the highest amount of workload so I feel like you know obviously we 350 people there's yeah I can't you know satisfy everybody it seems the workload is roughly all right well if I would change it then I would be the upset one group or the other the most people actually claim they go to class all the time which is surprising gives this we have more people in the class than there's seats in this room and there's many empty seats every time this is just a question to see if you're honest actually and overall I think mo da things seem pretty good so a few things I guess the key as you can see the first part is always bow carry and will carry on will carry him and sodalite I passed all that stuff on I'm not a huge fan of the material oh it's a little I can do and then what would people want to cover everybody wants have deep learning so that there will be deep learning don't don't worry there will be deep learning and examples a few concrete things there weren't very there weren't many things that I could really change immediately that I saw one thing is post the grades on CMS for the different projects so we've done that right away the other thing is make the homeworks more connected to the class we're trying to do this I guess there are some people rumbling about TA some TA is not being prepared so I will talk to them about this and then other than that some people say the cows is way too fast but then there's also a lot of people who say the class is way too slow so I don't know how to do that one okay and then I guess some of the math durations are very dry I'm trying really hard I agree I agree it's try but there's not you know I don't really know how to that's just the nature of mathematics one person said that none of the algorithms are really intelligent I like that one computers are just not intelligent you know actually I'm not even sure if humans are intelligent so it's just the way it is in some sense you know like the media sometimes makes it look like machine learning is you know it's close to becoming conscious or something and no but I think these are really just you know the way machine learning works is you have a train that I said and then the test said in some sense you try to see how similar is that to stuff that you've seen the training set right and that's how all of these algorithms work okay good but otherwise most people seem reasonably content so I'm glad about this and thanks for participating in the feedback one thing I want to go could you want to look at the last project this last second to last project the ERM project and look at the leaderboard and maybe give those people who are leading a small chance to say what they did so number one is storm in Paris storm in Paris right where a storm in Paris okay can you maybe give a brief explanation what you did to get such a high score I guess actually the three bit equivalent class but yeah uh-huh you consider by grants rights okay stop words okay awesome so but basically what they did is two things number one that removed stop words so stop words are words like the an a and so on they could dominate your email but really mean nothing about don't tell you anything about the content so you can just take most common words that don't actually have any you know don't contribute to the meaning of the sentence and just remove them drop those features that helps it's called stop word removal and the second thing they did is by Graham so by Graham's is base you don't just look every word and see if it's in the email but you also look at two consecutive words right so for example if you have Killian you have Weinberger IDE also take you say Killian you say Weinberg and use a killing member right those two together and of course there's many more by Graham's and there's yuna Graham's you singular words but sometimes they can be very important because you know how you use words and can be very predictive you know in if it's spam or not spam those are great great methods to improve your spam filter how about the second team DK where it's DK DK what do you do okay right uh-huh okay same huh nice okay okay good so I guess like you didn't do the normalization did you so I guess normalization didn't didn't help much I guess in this case but oh interesting and maybe had a different support start a stop word list so he did the same things that the the other team did and he did one more thing he took their vector at the end and normalized it and so could anyone tell me why that may be a good idea to do like normalizing bzzz divide every entry by the hue busy of word counts why'd you divide by the total number of words in the text document why is that a good idea well you can tell us oh it was just a technical reason okay good so you just did it because otherwise the code would crash but and that is a very good reason to do this and that's because you know if you have a long email or a short email right there really you know probably is not that indicative if it's one class or the other so if you just divide by the number of words then actually you know your feature vector becomes invariant to the length of the email right and so it's not that you know it's probably not the case that longer emails are more likely to be spam or not spam or something right maybe it is but that's one rationale you know why you were able to do this certainly for for example if you classify news articles right by category if there's a sport articular politics article but the length 3 is just dictated by the newspaper how much space they have so you would always divide by the total number of words because the length has nothing to do with the class of the of the document maybe the last one depth where is the team and there's something else I don't know what that means yourself can you read it hope is nothing dirty okay can someone tell me what that that team and depth are they here I guess they are skipping lecture alright so maybe that's the next thing hi alright I don't know how to pronounce this one either yeah you won't tell me you can anyone that be say what this team is called that I can call them out who speaks the language I don't even know what language I guess it's Mandarin how about Steve Nash and Carl Sagan there you actually won last time right thanks oh yeah I'm just saying okay I see I see I see interesting so I guess what they did is they basically picked special words that they thought was very predictive or certain symbols of spammers is not spam and then they actually has them differently so that they don't collide with the other words and so give those words a little bit of an edge right so that that's certainly very reasonable cool very very nice the one thing I guess that it's all vital but one thing I really like is that these methods that you guys discovered actually the typical methods that spam traders use so if you for example take your Gmail or hotmail or whatever spam filter right they use exactly these things if you remove stop words they you know they use by grants one thing they even do is use you know skip Gramps so skip Gramps are actually you take any two words with you know a flexible number of words in between so and so for example you say like the way you could encode this word one and then star star star by star star star could be anything and then word two and you basically you know whenever you see that pattern you increase a counter and the reason that's very indicative to what's that's good to catch the spam is because family may also actually generated the templates so the way spam emails work actually is it's I don't know well let me I spent two minutes explaining your spam burbs so the spammer is typically what they do is they implicate botnets so there is both database the infected computers that people use you know download stuff on their computer and so their computer gets infected and then with the spot what that does is basically it you know it doesn't show the user ever that anything is wrong but instead it does it's a little program that means it communicates back to the botnet owner and the body they bought that owner can now use the computer to send out spam rather send out emails and so the reason they do this is because they can't send out the spam from one particular message sender because then that IP address would get would get locked up and would be blocked so they have to have many many different IP addresses that many many computers that send out these families the other thing they have to do is they have to make sure that the spam messages are all different so if you would send spam to Gmail right gmail has so many email accounts that if you would say this exact same message a million times right what what all these email providers are doing they're hashing the incoming emails and they realize oh that's the same email already gets ten hundred thousand times so we just block it right and not only this they then go rich we actively sue the email accounts where it was delivered and remove them right so you basically wouldn't get through right so you can't send the same message you know too often so what they do instead is they sent they create templates of emails where you basically say here's the message and try to convey and in between you can basically randomly put in certain sentences and they make sure there's millions and billions of possible configurations of these emails so that no email is exactly the same so that's the idea so you know somewhere you might be so you want to put in sell viagra cheap or something right but what you put in between us can always be different right to regain strength or to you know whatever like oh that's of different I'm not gonna go there now but you know the reason by the way I keep bringing up high high grass because when I I used to work on spam filtering and the biggest problem at the time was viagra span so it's not that I'm really interested in biography anyway anyway so that's basically and the reason reason and because of these templates basically you have to have certain words always occur in the email and some random words in between and so you can capture that let's give backs that's basically one way of a finding signatures in emails all right awesome any questions about this competition the ERM competition and the spam filtering yeah typically not so they are stemming for example you take words you basic go back to the root of the word stuff you know running you just say run you know usually this stuff only helps if you have little data if you have enough data then you see all versions of the word often enough and it does make a difference so in some sense nowadays that we have enough data these methods go away so that used to make a big difference when we had very literally like 15 years ago okay good yeah all right a long time ago before spring break we talked about kernels and in particular the last lecture what we talked about is how do we find a we talked about two different topics one was how did he find a kernel matrix that is so well defined and how to show that a kernel matrix is well defined and the second thing was how to kernel eyes an algorithm so for the first one basically we had certain rules so the one thing he said is so let me just actually remind you I know it's been a long time and so what does a kernel do right so we basically kernelization came from the fact that we had linear classifiers that we liked a lot right so linear classifier says H of x equals W transpose X plus B that's a linear classifier and also great we have very efficient algorithms like the SVM that the perceptron to learn these things but they're also highly biased right the high thing you know they have a very strong bias problem because they are in the good only nonlinear decision boundaries right and so we talked about the bias-variance tradeoff and if your data is not linearly separable then these algorithms will never get a ride even if you have an unlimited amount of data so how can we reduce the bias and the one way to reduce the bias is to extend to map our X into a high dimensional feature all right so if you go to very high dimensional space then it turns out you know your data is much more likely to be linearly separable and so that seems like effective thing to do but on the other hand the problem is now this is really really high dimensional and now you have a very large vector W and so what we did then is we could show that for most of these algorithms you know the data is actually never really accessed as a vector itself it's only ever accessed in terms of inner products so all we need to be a little clever about choosing the spy of X then you can actually define a kernel function such that this for kernel function that just computes the inner product after the day snapped into this high dimensional space very very efficiently I didn't so some examples that we had was the polynomial kernel that goes into an exponentially high dimensional space but you never have to compute it you just compute the kernel function which is very fast the computer or the RBF come radial basis function car which just basically puts a Gaussian around every data point again very very simple to compute in the RBF kernel in fact corresponds to an infinite dimensional Phi of X one question is well can we just come up with any such function K to define inner products and and the answer is no right it has to be a well-defined function in the sense that if you take any set of vectors to take any n vectors and a computer matrix K such that capital K IJ equals K of X I XJ this matrix here has to be positive semi-definite nice it has to give rise to positive definite matrix and one thing we talked about is this basically you know I gave you a drools how to take any kernel and kind of modify that color so once you have a kernel function you can do modifications you can apply modifications to that kernel function and you still get a comma right and so then you can start with the simplest possible kernel which is the linear kernel the linear kernel just says that is x transpose Z that's a well-defined kernel s is the inner product matrix and you start from here and then you basically massage you you kind of function around by multiplying by exponentiating and so on until you actually eventually get to the kernel function you want and one thing we did last lectures that we showed at the exponential kernel we could prove in a few steps is applying these rules that starting from the linear kernel we can basically arrive at the exponential kernel and we maintain the positive semi-definite so the exponential kernel is provably a well defined car all right so that was the first thing we did last lecture and then the second thing was okay now that we have such a kernel function and so you know you're free to design your own for whatever application you have to do by the one thing I just emphasizes one thing that's very powerful about this kind of function is that these don't have to be vectors right so these can actually be we have defined kernels of assets or can define current service strings that's in fact what you have to do and the homework and so once you have such a kernel function how do you now kernel eyes an algorithm and so there's basically two steps the first step is you show that your algorithm only accesses inputs in terms of inner product and the second step is you swap in the inner product for the kernel function right and you have to when you show that everything is accessed in terms of inner product you have to show this during training and during testing right so in both the inference time and during training time you can only use an advice okay and then I you walk last time you walk through one of the algorithms and and that was kernel regression and let me just remind you let me just quickly go through it one more time because I will continue there today and that was basically the it kind of regression so let me sorry the kernel regression is basically the kernel iced version of linear regression if you remember linear regression minimizes the following loss function but actually let me do this w transpose X I - why I square okay so that's just the square loss and if you have a that's you can solve this if you do MLE we get ordinary least squares if you do map then we get a ridge regression which base it has a regular rise and we can write this and I'm just going to quickly one more time derive the kernel version because just the three liner if you want to divide this as a matrix with this here's my matrix X 1 X n and have a vector Y y1 2 y n just be careful that's the other way around that in the this is the transpose of what the way Python or Julia Stewart then we can write this thing here as just minimize X well X transpose W minus y transpose squared any questions at this point okay and if I write out that square well that just becomes oh sorry that makes you do something else now this is my W in there in order to kernel eyes it I have to get rid of this W and so the proof that we showed earlier the W actually is a linear combination of my axis and so what we can write is w equals x times alpha alright which is basically saying I sum over all my alpha X I times alpha I that's my W is just a linear combination of all my axis and you have to show that that if you do this that you're not restricting the number of solutions it turns out you don't because it's just gradient descent and this is proof we showed a couple lectures ago so if you do this if you plug that in for X then actually this year becomes X transpose X alpha so I just for W I plug in this term and now you see X transpose X what's X transpose X there's a matrix that's what can anyone tell me something about this matrix what's the IJ entry of this matrix at its symmetric good good at symmetric what else is it yeah it's the covariance almost it's the inner perimeter right so if I take X transpose X X transpose X IJ equals x I transpose XJ how does that make sense I take my X transpose is this x1 xn times x1 xn right and so the the first column of this matrix is the inner product of x1 with everything else then x2 with everything else and so on so IJ is X I in a product XJ right so what is this that's exactly the kernel matrix right that's K all right okay is just the inner product matrix crazy Hannah that makes sense ok awesome good and now if you minimize this well actually that becomes now pretty simple all right we can just do right here you know if you just complete the square that we get alpha transpose K transpose K alpha minus 2 y K alpha plus y transpose Y I guess and that's what when you try to minimize with respect to alpha how do we do this we take the derivative with respect to alpha derivative here becomes 2 times H or K transpose the same as you say the symmetric - and then actually this becomes 2 K Y transpose so this years if I take the derivative respect to alpha this term here there's a quadratic terms that just becomes 2 times K K times alpha this is a linear term something some matrix times alpha that becomes the matrix transpose this is just good old taking derivatives of vectors and if you set that to zero and then I can move this term to the right and so I get 9 divided by 2 now I get K K alpha equals KY transpose you're almost there I multiply it by K inverse I get rid of this thing I get K alpha equals y transpose I might buy again by K inverse and I get alpha equals K inverse Y transpose and that's the answer so colonel eyes regression as a closed-form solution is just K inverse times y or y transpose depends on how you organize you buy any questions yeah there's any way to interpret the results and use kernel are aggressions any way to interpret the results well what do you want to interpret which features were important yeah not really right they gets lost yeah depends on you know well if you use a linear kernel and this is identical to normal quick question then yes otherwise no any more questions yeah yes otherwise it's not in the product I told so the kernel matrix all it does right it defines it in a product and if it's not positive definite it's not an inner product it's an if and only if statement yeah so otherwise it's just a rent you know it's just not it doesn't solve that anymore but you no longer learning about your function in a high dimensional space okay any more questions yeah last question very good question okay good so his question is how do you actually do this on the test day is that your question okay let me go there and so now we have our alphas right let's go back to take a step back so what is what is the linear classifier linear classifier actually is H of Z Z is my test point W transpose Z right now we don't have W have you can't compute W so what do we do we know W what W is w is this guy here right extra Java so that's x times alpha transpose alpha transpose X transpose Z right what's alpha alpha this vector here what's X transpose Z is exactly what you just said is the inner product of Z of the test point but every single point in my training data okay so what is that I would make a kernel matrix with one more point but the test point that would be the slice of the kernel matrix that basically corresponds to the test point right so we can call this K star right that's a vector in some sense where the eyes entry K star I is Z transpose X I okay it's the kernel or if you do a kernel iced version that's the kernel function of Z and X I like that that's what that and here comes the amazing thing this year is again a linear classifier right so we basically scatter a vector alpha which has n dimensions and the course we just transform our data into basically a long vector of n inner products and now back where we started from with a linear classifier during chess time consider from D dimensions we actually in n dimensions so one more time we had our virtual data that was in D dimensions right we implicitly melted into some ridiculous high dimensional space you know X goes to Phi of X we do the whole learning in this ridiculously high dimensional space that millions of you know infinitely many dimensions what we get out is actually a classifier that operates in n dimensions and that's actually not surprising that makes perfect sense why is this because in this very high dimensional space I told you W is a linear combination of my n training points well n data points can only span an N dimensional space right so we must lie in an N dimensional subspace in this infinite dimensional space and that's exactly what we capture here so ultimately for classification be only here the N dimensional space raising hand of that made sense okay here's the question this kernel regression parametric why is it not parametric I'll give you a minute discuss it with your neighbor [Music] [Music] all right so we have a boat who say it is parametric raise your hand who says is nonparametric who has no idea what I'm talking about so I guess there was there was a majority of non parametric there was a tie between no idea I'm talking about and parametric I guess so let me just repeat what they remind you of what parametric versus non parametric yes so parametric algorithm is one that has a fixed size of parameters that you learned during training time and then you applied during testing right so if you have more data you don't learn more parameters a non parametric algorithm is where the model size grows as you get more training so the example was K nearest neighbor classification so the question is is kernel regression parametric and non-parametric the majority of you say it's nonparametric why is it nonparametric who wants to say anyone so it would explain why it's nonparametric yeah number ominous the alphas are n-dimensional so if you have n training points if n parameters so number of parameters grows with the data sets us that's correct why is it parametric who can explain why it's parametric yeah that's okay that's okay but the question is like if you would train again with more data would you have more parameters so that that would make it nonparametric but it turns out it's also parametric why is it parametric yeah no it's because essentially what you're learning is still a W right you know H of X it's W transpose Phi of X and this is a finite number of parameters all right there's Bayesian sum so if you didn't want to put an ohmmeter kernel there's maybe ten billion that mentions so you can at most have ten billion Ceramics and not more than that right because that's actually that describes your classifier fully so it's somewhere in between I totally if you have an RV f kernel then this representation wouldn't even make any sense because this would be infinite dimensional so you only can use the alpha representation and that grows with the number of training set so it's clearly nonparametric right but if you actually have a kernel that gives rise to a not so high dimensional space then you can also view as parametric generally it is considered a nonparametric algorithm because people typically use kernels that are extremely high high dimensional okay any questions about this alright so now I want to go to the most famous kernel eyes digress and this in some sense how the whole frenzy started with kernelization so for ten years the entire field nothing else but kernel eyes everything right starting from there breakfast right and this was to the invention of SVM the support vector machines became exceedingly popular because that was actually when kernelization came up and so I just remind you of the kernel an optimization problem of SPMS this is the SVM optimization problem he minimized the norm of W and then you know and then I am violation of our parameters that actually say that every data point has to be on the right side of the decision boundary by at least one now turns out you can actually go through this and you can find that this is you know everything is just accessed in terms of inner product but actually it turns out here's a very very beautiful way of deriving a kernel ice-t-- version and that's actually that's why people went crazy I was like you know people just saw this and that was so beautiful they stopped doing everything else right they stopped attending their kids and loved ones and just you know in pursue of this beauty so basically what this is this is a quadratic program and it's convex and so I used in formal classes when I used to teach this class I always used to derive it but it I think some people you know never really recovered from it so III I stopped doing it but I'm just gonna give you the high level and twist so this is a convex problem and the Z is my problem you know for every value of W I get some function you know but some objective value and I'm trying to find this minimum this is my W start has the best possible done and there's a convex problems or quadratic problems because if you know how to solve it there's another thing we know about convex problems if you know a little bit about optimization theory and you know that this is a primal problem and so every optimization problem that convex has a dual problem dual problem is somewhere like an alternate universe right but everything is reversed but everything is up surrounds like anti Superman and Superman something I think you know that's like so the dual problem is a maximization problem so it's some other function if you try to maximize it and these two problems because there this one is comebacks turns out to have exactly the same solution all right and this is something there's a good old known thing from optimization theory right this must be the case it's always the case and this the first thing you learn if you take a class and optimization theory is like given such a problem how do you derive the dual problem right it's like a standard ways of steps to get from the primal problem to the dual problem the dual problem is now a maximization problem but you try to maximize and the reason you may want to do this is because sometimes these dudes dual problems can be quite different from the primal problem and sometimes they can be easier to solve right or there may be some properties that become impossible and obvious through the dual problem and another reason why people do this is that these primal dual algorithms they solve the primal problem and the dual problem at the same time they've easily take a step in the primal problem and then they take this plug this to convert it to the dual problem and take a step here and so on and one thing they do is they always measure the gap between these two solutions and they know when they're at the minimum this gap must be zero all right so this way it's a very very effective way to see that you actually achieve the minimum all right this is what Prime a dual that's like the primal-dual gap if that's Dontos view so when the inventor of the SVM came up the primal problem like this optimization problem the first thing people did was I go what's the dual problem right and so when they derive then they wrote down the dual problem and I'm just going to state it here and then actually what you see is that that actually clearly only accesses the data points in terms of inner products and if a Z that's the following you have n variables alpha 1 to alpha N and the optimum is the function is the following and sum of I J alpha I alpha de y i YJ k IJ but ki J Z inner product between I and J minus the sum over all alphas such that each alpha is greater equals zero and less equal C and the sum over alpha iyi equals zero and and so here is when people saw that yeah in the he basically had X I transpose XJ and they realized wait a second is only accesses the data points in terms of inner product but it was inescapable it was so obvious and so they plugged in these different in the products and they suddenly realized we get much much better results right and so that's kind of where the whole kernelization came from and so actually by the way the next project will be for you to implement the primal and the dual of an optimization problem and one thing you're going to notice is that this of course doesn't you know doesn't contain W anymore and so when people then solve for W and again it's not very complicated to derive but it requires a little bit of optimization theory and so that's why I'm not doing it anymore because it seems like a silly prerequisite for this class you know needed for one lecture and but if you solve for W what you get is the following w equals some of our I equals 1 to N alpha I Y I and X I so that's pretty much exactly what we had just now with ANOVA question the only thing is we also multiply by a sign that doesn't really make any difference so we multiply by the label the label is plus 1 or minus 1 all right and so solving this this dual problem is just as complex as solving the primal problem there's no difference there are some algorithms that are particularly suited for for the dual problem but the beauty of this is and this is actually what gets really really interesting is that this dual problem a science-based it is alpha ice that's basically the weight we assign to every single data point between 0 and C and almost all of them are 0 only those that actually are the support vectors and that's why it's called support vectors so only those points that actually have here in equality or actually have nonzero sy so only those points if you look at the classifier you know here's basically my data set and I have my cat my cat made my decision boundary here yeah the second line here's this margin only those points that lie on the margin have nonzero office what I'm alright I guess not alright and if we now plug in this definition of W H of X equals W transpose X plus P W is just this thing here so we just get I equals 1 to N of I why I K of X I let's call it Z the test point V plus B one question is where does be come from and well it actually it actually turns out if they're very easy to solve for because you know that basically you know that this equation here and in the middle basically you know that this constraint here must be satisfied so if you actually plug in these equations you can actually solve for B so that's the idea have a written on did I write it on the notes oh yeah okay so we know the following if alpha I is greater than equals 0 that's the case if and only if Y i W transpose Phi of X I plus B equals 1 this here is basically a row of the kernel matrix this is basically ki there's the eighth column and you can just solve this for B and that's how you get B so I'm saying this because you have to do it in the in the project so actually maybe I'll show you the project but quick and and I give you a little overview [Music] so the project will be pushed out very abrasive when it's already done be always testing it before windings on set up testing students just to make sure everything runs smoothly so the first thing you need to do is write an SVM primal problem so that's the good old optimization problem that beer that we know right the linear SVM you have divided this is a quadratic programming problem and then you stick it into a quadratic programming solve my just one for Python there's one for Julia and you stick it in and then you run this and this is what you get by the basics we generate the data set that's linearly separable a random one and then the I if you did everything correctly it should be able to classify this you know perfectly that's the first part of the homework assignment you get zero percent training and does here's the output of the this solver so then we give you a data set here that is no longer linearly separable all rights this years now we have positive points and negative points and clearly you can't separate them it's the first thing you do is you run the code that you just wrote through this data set and you will see that it won't work right so this is what you get right the you know well you're setting this guy up for failure it's a suicide mission right so tries to find a hyperplane doesn't work right so then you basically have to convert the primal problem to a dual problem now we give you the dual problem right so you don't have to do this as you have to define a kernel matrix does he has a kernel matrix what you see here is the eyes and the J's input and this is the kernel value can anyone tell me why it looks like has these funky you know Star Wars Rebel Alliance shape any idea because the spiral data and so the data points that are very close together are very similar right and so this here it's actually these are the training points and the C are the testing point so it's basically the they're kind of in order so points that are close together are very similar so if you compute the kernel the RBF kernel that's what it should look like you know this is a good good way to debug it so then here's the dual problem and the question is please implement this dual problem now as a quadratic program so we have two how to do this I'm gonna skip past it and the first thing you now do is to debug this to make sure your code is correct you take the original data set that's linearly separable you solve the dual problem with a linear kernel alright so if you do a linear kernel you should again get out exactly the same linear classifier and you run this and this is what you get okay so otherwise it's hard to debug up you know you know kernel let's actually solving some problems some elusive high dimensional space but if you just use a linear kernel but in a product that's you know ki J is just the inner product between two vectors then you should get exactly the same result as before all right so this is a very effective way to debug your code and then actually you you you run this and then this is what you get once you kernel eyes it right then actually the classifier can do the spiral saying very very effectively and I think that's the last thing there may still be a final competition where you actually are asked to run a cross-validation now you basically try to optimize for the different type of parameters to try to find this matrix here tells you you have one parameter and the other parameter which one gives you lowest error you basically try out different hybrid parameters if you I agree with them and so here red means high error blue means lower error this is a way to find the right settings for your SVM either alright see you all on Wednesday 
","['', 'SVM (Support Vector Machine)', 'Kernel SVM', 'Linear SVM', 'Quadratic programming problem', 'Primal problem', 'Dual problem', 'Training set', 'Testing set', 'Hyperplane', 'Linearly separable data', 'Non-linearly separable data', 'RBF kernel', 'Inner product', 'Cross-validation', 'Error', 'Spam filter', 'Stop words', 'By-grams', 'Normalization', '']"
"two brief things so one thing is one comment that I saw in the survey a lot was people asked for recitations in addition to office hours or maybe ta s go over problems so maybe once every other week TAS would have a session where they explain some topic one more times you know and go over some example problems so I thought that's a good idea and I asked one of the TAS so you know don't the PhD TAS to actually do this kind of a quick poll who would be interested in attending such sessions okay this is paramount all right okay good so we will do this and I guess the first session will be on kernel second one-billion Gaussian processes and then probably boosting and then deep learning I guess one additional things the one thing that people said that I thought was very interesting was that one complaint I also got this X less in the survey but more at lunch talking to students is that you know the spoke areum setup is very neat and in some sense it's all cut out that you just have to do a small portion of the code and the problem is that some students said well I wouldn't really know if I get a data set now how to attack that data set without having Apple karyam set up and so I thought what we do is we do a competition where I give you a data set and that's all and you have to basically is a trained ear set with and you have to submit and the test data set without labels and you have to submit predictions on the test data and this will be for extra credit so that's also an opportunity for those who didn't do so well on the exam to kind of bump up their grade and it will also be a great opportunity for you to kind of do machine learning in the wild right so you actually can just do anything you want just use pull out all the tricks that you've learned and really make it work well it simulate the setting where you just given a data set or now you know see what you can do so we will hopefully push that out by the end of the week maybe Monday next week that's the current plan and this will be a cowgirl hosted competition any questions about this yeah it will be doing the last day of class actually possibly even when the 17s the last day that is possibly do so you have a lot of time and you know it involves everything by the picture pre-processing you know we can use SVM steep learning whatever you want yeah it's gonna be surprised likely discrete class especially so we are still debating you have like a few data sets of your discussing right now which one be mapping but it looks like it will be a classification problem okay good all right so then last time we talked about kernels and in particular we talked about kernel SVM so first actually talked about kernel regression and kernel regression let me just write down that you know we derived this and wait do they used to be as oh wow okay I never realized so we talked about kernel regression and they're basically we actually came up with the following formula that H of Z equals K star transpose K inverse times y so it's very simple where K star is actually the kernel the K star I is the kernel of X I with the test point Z this here's the kernel matrix K IJ equals the kernel of X I XJ and so you just multiply that with the vector of Y so this would come in handy later on but it was just one example of how to Colonel is an algorithm then we talked about Colonel SVM Colonel SVM Becan alized an unusual way we took a took advantage of a well-known fact and optimization Theory optimization theory that says if you have a convex optimization problem then there must be a dual problem which is the maximization problem these two intersect exactly at the solution so they have exactly the same solution and turns out that the dual problem actually accesses the the data points only is in the product and if you solve the dual problem is actually trivial to colonel eyes it you just instead of putting the inner product matrix you put an econometrics and so it's a very very elegant way of colonel izing the the SVM algorithm and this is actually where everything gets started the just to put the equations out one more time I'm not going to write down the dual problem I hope you still have it in front of you but the the weight vector at the end becomes the sum of all data points x y I times alpha I times Phi of X I wear one more time Phi is basically the snapping to really high dimensional space that we never actually compute but as long as we only do inner product with other points it's correct it's okay and then if you actually do a classification of any point Z and what we get is it's the sign of W transpose X which then becomes y ir feh I K of X I come as E Plus P ok any questions about SVM's dual SVM's before I move on oh here the kernel SVM's you know just by doing you know solving the optimization problem then using this here as a you know a classification function extremely powerful algorithm and one thing I want to do today is actually show you you know we actually move on today and we talked about Gaussian processes but I want to do five more minutes on SVM's and just tell you something Kanon eat and so if you you know I want to just examine this equation a little bit so remember we started out this class with the simplest possible machine learning algorithm which was the one nearest neighbor K nearest neighbor classification I ran right into the K nearest neighbor algorithm what you do is you take you know you all remember this right you have theta data like this and what you do is you have a new test point you find the K nearest neighbors and you average the label so that's a DS here plus one this he has minus one so if two x plus 1 1 times minus 1 so that's positive so that you would give this a positive label I can write this decision function of the K nearest neighbor algorithm as the following equation I can say what we do is for a test point Z we sum over all the training points and I'm sorry we do the sign and so assume for now my Labor's are plus 1 minus 1 then I sum of all my training points and I take the label of that training point and if it's a nearest neighbor so I have this Delta function that says if X I is a nearest neighbor of C ok there's any questions about this line so this is the nearest neighbor assignment rule so busy saying I'm just summing over all points and in some sense are busy summing up the labels but I'm only taking those points that are nearest neighbors so this is either one or the zero right so this is 1 1 if 1 if X I is a nearest neighbor of Z 0 otherwise that's that's this function so in this case have advisee take 1 plus 1 minus 1 plus 1 so it's actually a positive label so I assign the positive yeah question that's sign yeah yeah all right any questions now compare this with this equation here where we say H of Z is the sum of all points their label times alpha I K plus some bias okay so some things should strike you right this is really really similar all right it's it's almost the same thing sum of all the data points and here everybody just say well it's only take the nearest neighbor everything is you know on the nearest neighbors are one everything else is zero here we have this term here now let's look at this term alpha IKR excising now let's just first look it's only K of X I see what does K of X I see let's look at the Gaussian kernel the RBF con k of x i z equals e to the minus x i minus Z squared over Sigma squared so what does that mean if Z is really far away so X is really far away from Z right then in this case I'm assigning a zero because it's not a nearest neighbor in this case what am i doing I'd say XII minus Z is really large because they're very far away from each other e to the minus something really very large is zero right because exponential decay right it goes down exponentially to 0 so if points are really really far both of these rules what they say is just disregard them right they don't affect our classification so we only look at close points so if x i- z is small then each of the minus something small while that can be you know reasonably largest right so what is the SVM doing it's actually doing the same thing as K nearest neighbors it's just not making a heart threshold it just doesn't say or we take three nearest neighbors what it does instead it puts a Gaussian distribution around the test point does here's my test point you put a Gaussian distribution around it this here's my Gaussian distribution that's kind of like a bump that comes out of the blackboard and if you get far away and this value here is really really small so these points don't matter right if they're really really close they're very large and so we really do an a weighted average of the neighbors that's basically what it is so Kenya Israel SVM really what it is is a soft nearest neighbor algorithm where instead of saying you have K nearest neighbors you say you know I put a Gaussian around myself right and if I put a Gaussian around me here right you guys here will still get some probability mass right so I'm busy averaging you guys but she doesn't away back you have zeros you don't you no matter anymore I said it's very similar to K nearest neighbors it's just that I'm not have a hard Thresh or a cut-off instead I have a soft cut off that's what the K does what is the Alpha do now remember the Alpha of asically you know weights that we assign but there's one thing that's really important if you look at the dual of problem the alphas are always non-negative all right they're either between 0 and C so what are we doing we're assigning additional weights of these points in fact a lot of them are zero so what does that mean that just means we take some training points and just remove them because we don't need them all right that makes a lot of sense if you have many points here right and there's no other labels well why do you have to keep all these points if you just set all of these to zero and just remove them all right they're not changing anything everything here will be classified as positive so we can just set them to zero all right that's exactly what the SVM is doing it's you know these are the basically the points that are not support vectors those are points that in a region that gets classified correctly anyway just set them to 0 right so SVM is in some sense especially the RBF kernel a smart nearest neighbor algorithm there's two things number one it takes you trained ear set and thins it out and removes points that are obvious right and so you don't have to carry your whole entire data set around all the points that zero zero alpha you don't need and then the second thing is then before a test point that basically has some soft assignment where it says you know if there's some dense region I take on more neighbors if it's a sparse region I take on a few and a diversity any questions yeah yeah so in these high dimensional spaces you typically don't need to kernel eyes right this is only a property of the comp if you don't kernel eyes anymore then you just have a linear classifier right the the RBF kernel does not work so well in in if you a BF journal is affected by the curse of dimensionality just because everything is far away from everything right that's what happens so the fascinating thing is right we started out with the K nearest neighbor algorithm then we introduced something totally different write a linear classifier then we mapped that linear classifier into an infinite dimensional space and what do we get the K Gnaeus neighborhood so what will be is he's showing is that you know this in this infinite dimensional space actually a linear classifier is basically doing what a K nearest neighbor algorithm is doing right it's just better though all right SVM is because you're weighing data points you know by these alphas and because you have a soft assignment it typically gets tends to get much lower error than the nearest neighbor the last of last but not least actually also signup eyes and so that's just as a small detail that you don't really need monster okay any more questions about SVM's [Applause] yeah the question is are there any scenarios in which non kernel eyes SVM's are better and the reasons yes yes so if you have very high dimensional data like for example text documents there's no need for you to colonize I just use a linear sphere right of course that's identical to a kernel SVM with a linear kernel any more questions okay good so then what oh yeah perfect okay good so let me just hand out next thing so I have to warn you a little bit the next topic is not complicated but students perceive it as complicated I don't really know why I'm trying every year I'm rewriting this lecture every year for the last you know I don't know how many years and it's one of the most beautiful algorithms but I'm just warning you please pay attention now it's like it's there's a really high cost if you don't get it the first time reading up afterwards it's quite tricky it's the it's borderline non-trivial okay sorry I gave too many to the these guys in the front okay but just because actually just to wrap this up let me show you one more time the okay this is one more time the SVM demo because now that you understand it better it may make a lot more sense to you so let me just create a dataset here's a dataset that is not linearly separable for example I have a bunch of points around it and so these are my negative points around the circle my positive points are in the center if I now run a linear classifier this is what the linear classifier tries to do right the Bayes he says well here's some positive points I can put this line here and I get reasonable error it doesn't really make all that much sense it's just it's you can't beliee do it but here's what the SVM does right and actually this is maybe a little bit too large of a see actually so in this case actually makes everything support vectors does the only point that drops actually so it looks like I made my C a little bit too large if you look at the 3d plot you can see how you know here it basically carves out this valley in the middle right and so you can now see how it's a nearest neighbor algorithm right it basically wherever you have this this dot here you see it's very very red right that means it's very positive right around these data points and this is exactly this couch and Gaussian kernel that he basically put around all these data points so here it's all all red right so because you're closest to one of these red points and then you get in here and busy in here for these blue points so you know again it's the starkest ride where you have one of those blue points right so it you can see it's kind of a smooth decision function of the nearest neighbor algorithm oh good point the white lines are the the lines where the decision boundary is exactly one so those are the support vectors I can also show you one more demo on so I also show this last time here we have a you know we do a one dimensional plot this is now our a kernel regression yeah and so what do you see here is there are two parameters one is the kernel width that they see says when I do my RBF kernel and how wide is that gouge right so my nearest neighbor algorithm how a house how why does it go and the other part of lambda is basically how much do a regular rise and what do you see here is this year right wait so here in this case basically it's kind of like a linear classifier because I have a very very extremely regular based but this whole thing is very smooth function this is because my my Sigma is very wide so I have very wide gaussians this means in some sense what am i doing i have a nearest neighbor algorithm that averages over many many points right so that change is very little right there's a low variance high bias classifier still despite that it's kernel eyes and if I if I lower my Sigma then these couches become smaller and smaller and what am i doing is essentially Beasley and this is gonna like K nearest neighbor with large K and this year's K Nima's name is a very small K right so what do you see here is that basically right around this data point it gives a large value right but then it falls off already right because you're no longer close to that point so it really just predict exactly what your nearest neighbor does and if you're too far off well then you just falls back to the meet right so it's a very very sharp there's a high variance very high variance by its pacifier any questions about Colonel request yeah this is this is Richard question yeah that's right lambda in sigma lambda is the regularization and sigma is the kernel with so this here's the grid right so you busy I changed my Lander in increasing order to the right so this is low lambda as they go to the right that lambda becomes larges are regular eyes more so these are all flat lines because they're over regular eyes and as you go down and increasing my Sigma so that basically means that make my gaussians wider the kernel wider so that's kind of equivalent to making a you know like if you look at the if it was an SVM it would be a large K now like it you know Kenya's neighbor classifier would be a large K now so you're averaging over many different points okay any questions about this okay all right so um alright and we're not quite done yet with car so the next algorithm X is still a kernel algorithm then then there's the last kernel algorithm and the next algorithm is called Gaussian processes and it's basically an extension of linear kernel regression and I always like typically in my view is that Gaussian processes are kind of the regression equivalent of SVM so SVM's are kind of you know really good classifiers if you have classification problem SVM's are great algorithms and there are no errors they're very reliable they don't really work well for regression there's as many people who've tried to make the SVM regression and it's awkward it doesn't really work right because you have this hyperplane you try to point points on one set another and basically what they do is they say well if you kind of off the hyperplane I still kind of classify you with the same value it doesn't really work it's awkward Gaussian processes are really for regression people have tried to make Gaussian process classification and that's awkward and doesn't really work properly right so there really cannot with say equal partners right they have very complementary strengths and programs and processes to get into this we actually have to think a little bit about Gaussian distributions and gas distribution is a wonderful distribution and one the reason why Gaussian processes is so awesome is because they are all based in Gaussian distributions and Gaussian distributions is really easy to work with there are very very very nice properties and the first thing is can anyone tell me anything that's you know that's truly Gaussian distributed it's like I'm trying to plank this does anyone know anything that scarran distributed yeah kind of people right exactly right height of people is something that is Gaussian distributed right so in fact actually voila height of people is actually not caste but it by modal right so you have man and women and but if you condition if you say given that you know the gender it's a Gaussian distribution and actually actually have a friend who who actually wanted to show this in this class right and he collected some data he thought where can I get data I just want to show my students that humans really have a Gaussian distribution in height right and so what he did is you know he meant to OkCupid and so OkCupid actually it's a dating page that's free and so the data you know that the deal in some sense is the data becomes available for researchers and so he looked at all the you know this is great right because people actually put in their height and all the stuff and the agenda so he just looked at man and he you know collected all you know tens of thousands of man and their height and then he plotted it and distribution looks great right it looks like this and looks like this and okay well that's odd what's going on here and does anyone know what's what's here it succeed let's try that's six feet so so apparently people who are 5 foot 11 generously round up which is interesting actually I think it frustrates me a little bit because I'm exactly six feet and I feel like I'm you know people are kind of I'm not you know I guess that their recommendation for ladies is you know to you can trade off one inch right and get someone who's really honest right if someone five foot eleven that's it that's honest guy so or if you want to talk like you know that maybe go for six foot two or something anyway this couch distributed another one is another thing is IQ IQ is government stupid actually people for them that experiment where they ask people what's your IQ and that also looks nothing like y'all actually the average average person has a way above average IQ according to self reports but so the reason that many many things are Gaussian distributed is because of the central limit theorem so who's heard of the central limit theorem I'll prove it okay beautiful so let me just show you a I made a little demo we can actually show the central limit theorem so central limit theorem says something very simple it says if you have any distribution that has finite variance so that's most distributions it's yeah that's one then if I then sample multiple points and average them this average becomes Gaussian distributed and so I thought well let's test this if it's true so here's my demo and so here's here's what I'm doing so I have a distribution and right now it's a uniform distribution so so here's the uniform distribution so I sample points on this distribution and I have basically a that has you know can I take ten values Oh can take on ten values and each one of these ten values is equally likely so I draw samples from this and average them and then I look at the averages I do these averages multiple times and if I do two averages I think hey a I average two values into that many many times this is the distribution that I get and if I average four values and so on and if I average 156 values you see this is a perfect Gaussian distribution alright so here on the right okay despite that this here looks nothing like y'all just and now we can see like you know this is really hard we can tell break it right let's try to make this distribution look really non gauss right so for example let's make a lot of probability mass here a lot of probability mass here right that's that's the opposite of a Gaussian right you have like you know nothing in the middle but everything on the only and the outside right can even set the say get to zero right like you know that's really get rid of these things oh I just broke it I made a negative but that doesn't work okay okay good I can't make it negative all right let's make these video a large okay good so now I have basically nothing left right and what you see here is like you mean you have few samples right it doesn't look out see but as you actually get more examples like even with this crazy distribution which is really odd by the either one or ten right you're never in the middle all right the the mean actually becomes cows right and so I guess that means if you you know in real world if there's something that depends on the average of many random variables right for example your height right what is your height depend on on your genes right on yeah you know what you eat right well I don't know what else but you know probably many things and and these are you know these are basically the a yoke idea you hide us you know in some sense an average of many factors right so it tends to be Gaussian distributed and another place of people use a lot of cow Moos diffusions actually in the stock market right so the stock up and down you know people make a lot of money predicting this but there's also many many factors that factor into this so and you don't really know what those are but you know you know they're roughly Gaussian right so that's actually that's that's your angle right because you know the distribution you can actually attack okay any questions about this demo and it's actually the number of values that I'm averaging and then I took many many averages and I think my notes may have a mistake there I gonna correct it on the side so yeah okay any more questions okay he comes the beautiful thing right so one thing we have is that many distributions like if random variables are the averages of many other things many factors they tend to become Gaussian right but that's not enough to explain why we see so many gaussians in the real world and there's a second thing the first thing is things become Gaussian as we average them there's a second thing the second thing is the Gaussian is the black hole of distributions once you Gaussian you stay Gauss right you can't escape hey look if you add up two gaussians they stay Gaussian so if you multiply two Gaussian TV stations if you condition on one variable it stays Gauss right if you marginalize something out it stays Gauss right so it's kind of this attractor the sinkhole idea stuck in it and that's a very very important property that we will take advantage of actually make a little demo like actually in the past I've actually proven all these things that it's kind of closed unto all these different operations I thought maybe that's a little boring so let me let me instead I just had a meeting actually a faculty meeting so doing that I secretly made that demo I hope none of my colleagues watch this video but and here we go so here's here's the first couch in them okay good so here's the first thing I want is that that's like it's a yeah I take two different couches these are two variables X and Z so X and Z it's like I'm just drawing points from this distribution and then drawing points in this distribution this is my X this is my Z now I want to look at two new distributions the first one is X plus C and the second one is x times C what do you think these two will lie maybe I give you a minute to discuss it with your neighbor and think about where will ecstacy Mabel X x cb [Music] [Music] [Music] all right any guesses where is X plus C who wants to guess I want subscribe or x+ Xia's well it's gone all right black hole that's right that was it any guesses where is X plus Z yes it's around zero that's right it's kind of between these two right and maybe a little wider than the blue one and let's see it here we go right well it's not exactly than around zero because this guy's they're not actually exactly symmetric so this guy's a little further off right because it has no variance right or you can see the beautiful gouge distribution right so what I'm doing is in some sense I take a sample of value here sample value here and add them up right so that's it's going to lie somewhere here in the middle all right where is x times Z yes I ignore you guys you can see ions are too many questions does that say is a compliment yeah sorry - Jones 20-30 in here yeah I told its yes it's here but actually the spread is much much right right so here the variance here this mostly affects the mean right but if you might apply them all right if you could actually get a large large negative value here a large positive value here that could give you a very large value or you could have minus 10 here plus 10 here Thai cuisine your value of minus 100 right so this is a very spread out distribution okay so the important properties that I want to go over today that we will take advantage of is that the Gaussian distribution is is closed under for four operators the first one was normalization well that's that that's interesting actually it's like it is already normalized and so it sums to 1 well that's that's a trivial one the second one is actually let me just bring up the course notes because I after what I'm going to show you the demo again in two seconds there's no point going to the blackboard okay here we go so so we have this first thing that second thing is marginalization so what does that mean if we have a Ellis or the one thing we looked at let's just do them out of order some nations the summation we've just seen let's say we have two distribution the first one is y and y Prime they're both Gaussian then if I sum them up y plus y prime becomes a new gautsche distribution where I sum up the two means and I sum up the two variances right so that was the Green saying in the middle of the red and blue right so the mean was kind of in the middle in the variances get a little you know the variants get a little wider which is exactly what you said then we have two more marginalization and conditioning so what does that mean marginalization means if I have two different if I browse distribution over two values right and now I marginalize one of them out so I've you know P off what do I call them here and why am i B why a and Y beam that's a two dimensional Gaussian right and if I if I now compute the probability of P of Y a what is that well that's the integral of a by B P of Y a-comin YB YB right so that is again a Gaussian distribution okay so if I have a high dimensional Gaussian distribution and I take any one of the dimensions and basically collapse it right so well I have a demo I can show it if I basically take out all the mass on one dimension and just sum them all up squeeze them together what's what I get back is a Gaussian distribution let me show it to you see it was a large it was a long faculty meeting so I actually it's multiple multiple demos so this is a two dimensional Gaussian right so you have X 1 and X 2 and this here's the heat maps that I've been drawing on the whiteboard all along right so imagine this for example this could be you know and you know we get to this example later on but I've mentioned this in the past in our housing prices right so say I want to sell my house right but my neighbor has basically the same house in the same street and if my neighbor that says x1 I'm ex - right so these are correlated in this case right so you look at this to the oceans tribution what you see is that if the value of x1 goes up the value of x2 also goes up I can actually make you a 3d plot here's a 3d pop so now you can see very nicely right they're basically saying the value of one affects the value of the other right so these two are tied together okay and there's a two dimensional Gaussian distribution and if I now marginalize it what am i doing right let's say I want to know the value of x2 I don't care about x1 I don't care about my neighbor's house right you know I want to say my own house right so what I have to do is I have to sum up over all the possible prices that my neighbor could could sell his or her house for I can show you how this works so let's say I know they do this and I okay wait one second so here this is just the same thing on a different scale and and now I am summing up all the all the different columns so I'm just summing up summing up something up something up something upsetting you up carry it all the way to the end and what do I get Dada right right here this this is the Gauss distribution I do you always stay Gaussian right that's the beautiful thing people take advantage of exactly that property later on let me do Gaussian processes okay so the the last thing that the Gaussian distribution is the last operator that we can do is conditionally right this one here and conditioning says if I have a two dimensional Gaussian and now I owe the value of one one of the values the second one stays down so going back to the example of the of the my house of my neighbor's house right there's some Gaussian distribution that says you know they're tied together right and now my my neighbor moves and sells her house right well that now effects the price of my house but it remains couch so this is basically the following I have a gouge distribution of it why why am i B and now I know the value of YB that's my neighborhood that's no longer uncertain there's no distribution anymore that now that random event now happened right my neighbor sold her house it's done right and now but what I can do now I can take this information and compute the distribution over the remaining you know very random variable for which I don't know the value of it why all right and that changes and so if you compute that and that's the good old you know Spacely the Joint Distribution divided by the marginal distribution of a YP all right this is just good old stats 101 all right turns out that becomes the gouge distribution here's their variable it's basically the mean here plus this term Plus this term and we will get I will explain this in a minute let me show you a little demo so here's the conditioning so here's again our Gaussian distribution and so once again you can see that you know there's been a highlight the fact that we have this structure right this structure here shows that these two are correlated right if x1 goes up we have you see in the 2d if x1 goes up x2 also goes up right so if my neighbor sells the house you know for a large price that means my house is probably also worth a lot more right and now what I can do is I can now condition on something so I can now say well let's say my by the way this is not what these of course these are not house prices right these go from minus 1 to 6 or something but imagine I just say well the house is you just remove the average house in Ithaca or something right so then you could say are you selling below average price are above average price so now I can say ok well she now sells her house at a certain price this is the price right and what happens is the probability mass of everything else gets sucked into this right there disappeared this can no longer happen is this probability zero now right all the probability mass ends up here and essentially all I'm doing is I'm renormalizing and what I'm getting is you know the gouge mr. Bhushan one thing that's really cool as I can now actually say well what happens if she sells a price for a house for a different price right then you know so let's say I'm moving the value of X X X 1 that changes distribution of X 2 right so this is the X distribution of X 2 okay and it happens at a particular value on X 1 so this is conditioned given that my neighbor sold her a price for exactly a house for exactly this value this years now the distribution that I get from my house right and one thing I can now do is I can say well what if she sells a house for a different value how does that affect my my my price right and I can now move around right and what you see is that the gao's shifts right the gal should shift around and if you look at X 2 what's happening right the more valuable her house gets right the more valuable my house gets right so it's her house becomes more expensive this goes up to 6 my house also is more likely than that you know to be worth that much money ok any questions about this demo yeah the berries for this thing yes it does yes yeah and one thing I can do now I can actually show you another example what if we what if my neighbor actually wasn't my neighbor right what if not you know for example Sarah my wife tells me oh by the way my you know our neighbor sold the house and that's gonna affect our house right so what if instead actually steve ballmer right of Bill Gates sells his house right so Bill Gates has a house in Seattle it's worth you know fifty million dollars that's a little more than my house and and essentially what happens when he sells his house there's absolutely zero impact on the price of my house right because totally different people buy these houses right there's there's nothing in common right number you know I probably hear he has more bathrooms and I have rooms in my entire house like you know by factor 10 probably so the gouge distribution would look different right it would look like this right so basically you know there's next one goes up well it's to kind of stays the same right there's still some probability mass that's kind of here the center that says there you know there's still something you know some point that more light is most likely depending on what x one sells for next two sells for because you know Bill Gates house may only sell for certain you know it's probably it's reasonably likely to sell here but it could also sell for that much or for that much it won't affect my house price at all and I can show you this if I now look at the conditional distribution what you see is that basically you know this distribution doesn't change at all right Izzy moves around right so if I look at the the just the view of my house like the fact that Bill Gates changed solar cells has no in fact impact on my house at all right and so the way I encoded this is in the in this matrix here so this here's the covariance matrix so Gaussian two dimensional Gaussian space you have a covariance matrix where the diagonal entries the two and the one show you how much the variance of each particular entry is so this here would be you know what is the uncertainty by out my house was the uncertainty about Bill Gates house and this value C says how correlated are they right how much does my house inform me about the Gates house and vice versa and so the first example I said this two one I believe this constant C was 1 and the second example I said this to zero Bizzy said they have nothing to do with each other right so the gouge distribution is very very neat in that way that you can basically you know connect different random variables with each other in a very obvious way let me stop here and if you get a little time maybe read through these notes it helps a lot to understand what we will be doing on Friday 
","['', 'Gaussian processes', ""SVM's (Support Vector Machines)"", ""Kernel SVM's"", 'Nearest neighbor algorithm', 'K nearest neighbor classification', 'Decision function', 'Convex optimization problem', 'Dual problem', 'Kernel regression', 'High dimensional space', 'Inner product', 'Data set', 'Classification problem', 'Extra credit', 'Machine learning in the wild', 'Cowgirl hosted competition', 'Picture pre-processing', 'Steep learning', 'Discrete class', '']"
"hello everybody oh my my mic my mics alone son okay good okay so last time we talked about the ended with the Gaussian distribution and I told you a few properties of the Gaussian distribution and the beautiful thing about the Gaussian distribution is it appears many times in the world so using the gouge distribution is often a good assumption and that's by the central limit theorem but there's other other great properties about the Gaussian process yeah but the gouge distribution the Gaussian distribution is the black hole off distributions right so if you have a months.you gouge distributed you can a stage our distributive take a gouge distribution adding it up with another Gaussian distribution gives you a Gaussian distribution it's multiplying to gouge distributions give you a Gaussian distribution marginalizing gives you Gauss institution normalizing of course gives you a Gaussian distribution and most importantly conditionally gives you a Gaussian distribution so if you have P of a comma P right then P of a given B is Gaussian P of a is Gaussian which is the integral of P F and of course you know P a plus B etc all gosh so today we will use this right uses the super power and keep in the back of your mind so what I want to do today is go back to linear regression so remember a while ago we talked about linear regression and linear regression we had the following mark right it basically assumed we have data and so we made this an assumption that the data you want to do regression there's no classification yeah but we assumed this data somewhat lies on a line and this lines parameterize with W as you have some function W f of X equals W transpose X that was the idea that's that's the model and you're trying to find this W of course the data is not gonna lie exactly on a line right and so we assume there's some noise and this here's our epsilon and epsilon as Gaussian distributed so what does that mean well this means that this actually is a Gaussian distribution with mean W transpose X and the noise is basically the variance you know epsilon squared yeah okay the people remember this raise the hand if you remember this awesome okay good and then we had two different approaches to learning this parameter W and so by the way what I will talk about today I already mentioned it last time this Gaussian process regression the way it's typically explained right there's some very beautiful side of Gaussian process regression and that's basically that it's a prior over infinite dimensional functions and it's so beautiful that anyone who describes it goes off for pages and pages about internet dimensional functions and priors in that space etc which makes it really hard to understand m's I'm trying to cut all that stuff out I haven't found anywhere else an explanation of Gaussian processes where they don't really manage to resist right the temptation to talk about how beautiful it is to have priors over infinite dimensional functions so make sure you you pay good attention today because it's really really hard if you wanna read the secondary reading it it will always go into that direction just because it's I mean it is very beautiful but it it's hard you haven't heard it for the view if you haven't heard it before okay so we talked about two yet two different approaches to learn W and the first one was maximum like you would estimation does anyone remember what what term be minimized or maximized oh come on guys I know it's beautiful buy that sorry yeah so what does maximum-likelihood about is always the same thing what about to be you're trying to maximize so we have some okay let me get some terminology yes some data x1 y1 xn yn and what are we trying to maximize with Emily yeah that's right the probability of the data given the parameter right this should technically be a semicolon because we're frequencies to you but you know and so basically what does that mean that basically is the following thing we sum of all our data if they take the product of all our data P of Y I give an X I and the parameter dubby they see what we are saying here is if we choose that parameter W how likely is it that each particular X I gives rise to the label Y right and that's the probability of the data because it's already sound would be call it likelihood so we take the product over every single point right and say what's the probability that we if he if the if the data really follows the slope what is the probability that at this value we get exactly this right so if you had something that you know let's say we have the following W here well then you would say at this point here this is X value what is the property that we get this this Y here well it's very unlikely right so by maximizing it be based shifting business this line up okay then they had nap does anyone remember what we did here what do you maximizing here I know it's been a while what do we maximize here and yeah that's right we turn things around say the probability of W given the data okay so here we say which W make some day that most likely explains our data the best and here we say given that we have our data what is the most likely set of parameters right and so the way you do this is you actually use base formula that's where a Bayes rule so we say well that actually equals the same thing it's here the likelyhood times actually a prior or this times of normalization so if you use Bayes rule then basically what this means is you have to put in this prior and this again we choose to be Gaussian now one good question so we be P of Y given X can anyone tell me what this distribution is well I guess I just said it X I W right well that actually is a Gaussian distribution that's our on from some Gaussian distribution you find to you W transpose X and some some noise that's Sigma squared is the variance okay that's that's because for every single X you have a little distribution which comes you know which is basically our noise model so for every single point here the busy stable that's the point by value of your predicting but there could be some Gaussian noise so it's it's somewhere around here so ultimately what we're saying is our labels Y are drawn from a Gaussian distribution where the mean is just W transpose X so the mean can have shift up okay good so this here is the Gaussian distribution that's cows can anyone tell me what this distribution is the Z is distribution over there's the distribution over one single point this here's the distribution over the whole data set what is that distribution and give you 30 seconds discuss it with your neighbor try to figure out the distribution you've heard of [Music] [Music] all right here you want to tell me can anyone tell me anyone is brave it's a product of many oceans so actually it's just a Gaussian distribution right it's a weird Gaussian and we get to this a little bit later on the lecture so let's just go back to Emily and map okay and so what are we doing here right and I mentioned this a little bit a couple lectures back so we have our data D which is drawn from this distribution that we don't know and then we take this data and we fit this line right we fit our model W and then why are we doing this right the only reason we are doing this is to make predictions later on alright so for test point X we would like to predict you know what Y is so in some times the flow is the following right we give we give in our our data right from that data we learn a W right and then from the W we can now compute y equals W transpose X right for some particular test point X now this is so nice but it turns out actually we came to something far more beautiful than that and that's if we can have put on our our Bayesian hats today and so what Bayesian say is the following they say well actually wait a second right you're only using this W to make predictions right at the end of the day you know what now once you've made your predictions no one cares about what W you use as know is correct so why don't we instead of trying to model the probability of W in there make predictions with that why don't we from the start model the prediction of so here's what we do we say we have a test point X and we would like to know what is the label of that text point X we don't know it so whatever you want to do we want to model P of Y given X all right and II have something else we have our data D okay does that make sense so that's ultimately what we want right the beautiful thing about this is this makes no assumption about them all right you didn't you didn't you didn't have to commit to any model right I'm just saying given that this is my test point X you know what's the probability of my what's the distribution of my labor one and let's go through this and think okay well we make this assumption here that you know this is Lydia and so if we had any W right then you could just say you know we can so we have this fault this term here right for a certain W we can make predictions so how do we get here well the trick is very simple we just basically marginalize out the model so let me just show you this so basically what we do is we basically say well in order to make predictions we have to have a model and that's the following W X and W hey and let me know what the probability of W is that's actually you know that's this term here and then we can integrate out all possible values so this here is the same thing does this make sense there's basically probability of you know in some sense this is the probability of I could just write this year as P of Y given X comma D and W and then I integrate out my job alright so clearly that's the same thing right you can always and probably do you can always add another variable and then marginalize it out so it doesn't doesn't mean anything right it's like adding one and then subtracting one all right but if we do this then we can say wait a second the moment I have W Y doesn't actually depend on D anymore Vitus depends on W right but W depends on D any questions at this point yeah um actually yeah I may have always simplify this a little bit you know let me have to be a little careful that I don't make it too simple I just had a big flexor and me maybe not the general case yeah sorry yeah um any questions about this equation raise your hand if this step is clear go from here to here okay good any more questions no all right so this is what you want to do right basically given an X point X we predict the label Y and we have our training data and now I can show you could show you well we can just decompose this and say well you know if we had a model w-with prediction is very simple this is just the gouge mr. bution and we have this thing here this is w given T okay so this here is face this is the term that we already had up here in map and this here's the term that we basically have here that's the final model so now comes the beautiful thing so this year is a gouge mr. beauty and what is this P of W given D well this is P of P of D given W times P of W always M normalization and this is just using Bayes rule all right I can take this and use Bayes rule flip it around now what is this what distribution the answer will always be Gaussian the whole lecture so why does this distribution it's a Gaussian and what is this distribution it's a Gaussian huh and now we take this whole thing might be taking product of two gaussians what is this see joshan good and I'll be normalize it what is it my gosh it good now we multiplied with another gaussian what did you get and how we marginalize our W what do we get a Gaussian this here is a Gaussian and that is amazing it's amazing because that actually worked right it actually worked if he took every possible model in the universe right an average every possible results that could possibly have right and we still get a distribution of you understand and we can solve I'd usually see when Phase II and say they say now ideally we don't care about the model right don't you know integrate out the model we don't need a mock make it disappear right it never works here it actually works right because everything is Gaussian so you can actually make predictions without ever learning the model there is no model we might be marginalised out the set of parameters we never had to find a single set of parameters so essentially let me just explain to you what we're doing here right we have our data set here and we basically say you know I will never like I can make predictions for a certain X point what the Y is without ever committing to a single line you know in which the data supposed to lie what I'm instead doing is I take every possible line that exists and for every possible line I get a different answer but these lines here are much much less likely than these lines right so my result will be heavily influenced by these lines that I actually fit the data and hardly at all by these lines that make no sense that because there here is the P of W given D will be so small that they won't matter right but the question well is this the right fado is this the right in this case but I better see what I'm saying is like well maybe both of them have a point and what I'm essentially doing is I'm averaging the results and the nice thing about this is that what you get is ultimately a Gaussian distribution so it solves another problem it solves a problem that you usually have in regression one second is that when and when you get a a prediction you don't know how sure you are about that prediction right so let's say you know you go to a hospital and the classifier tells you you know you know sorry you have a better terminal illness right you will die you know in a year right we would like to know is that you know how confident are you about that prediction right is this you know are you 1% confident or 99% compo right so this actually gives you a full-on distribution over you of your label so I predict to your label like you know basically just gives you a full distribution and you can estimate exactly an interval and saying well this certain probability you're basically lying here yeah that's right I'm still making that money and something - data lies online still for five more minutes because we can kernel eyes and then it's kind of put everything together and then it doesn't have to line a line but a very good point yes exactly okay good any any other questions about this yeah by PFW yeah the trick is that we that we choose this to be couch so that this is the case this is the trick because this is Gershon under this couch and so this in some sense the choice we have to make you yeah okay good so the question was one of you had a different prior on w right then everything falls apart all right so you better make this Gashi right yes good point any other questions that's why you know people love couch and Pryor's because it's actually called the conjugate prior the conjugate prior is always the prior that basically well it gives the posterior the same distribution it's like yeah any other questions okay good so we could now do this right we could not solve this but actually wait a second right we know that the answer is it's a Gaussian right so if the answer to the Gaussian then you don't have to do any of this stuff right because we know exactly what a girl she looks like here's what I gosh it looks like the Gaussian you just say P of Y given X and our data is drawn from some distribution some mean and some covariance matrix right this is Sigma Sigma just ends here put it comparing speeches so what beginners do is we can just just fit these these parameters right and try to you know make it in some sense he is setting you know to try to estimate with these parameters and then actually we get a distribution so and that's exactly what the Gaussian processes so in Gauss's process regression we basically make the assumption which is not totally crazy right because you just realized they arrive to that assumption making totally reasonable steps and that your data is all Gaussian distributed now let me just explain what I mean by this because it's a little bit confusing right so here's actually he is actually the true distribution that you're making we assume that all data so we have y1 yn and my test points given all my X's X 1 2 X n that's Madeira set D and my test point a drawn from some Gao investigation that's actually the one we use the mute Sigma so this year's mic my gosh distribution okay so that's the assumption I just make up front if I make that assumption I get to the exactly the same result so he may as well just make that assumption and then you know I get that much much quicker that's much much easier don't have any integrals anything so here is the let me just explain what that means so let's assume you have two training points or you have one training point one test point okay what does it mean that they are drawn from a Gaussian but it is important that you understand this part now because that's what always trips everybody up what I mean is that that's a one training point one test point so as I see as x1 that's my training point this is X my test point if they're from the if I say they're Gaussian distributed I'm sorry actually there's not X that's fine sorry I have one Y right if I see the gauchos tributed that means there's some Gaussian distribution the Daisy tells me if I know that label off the training point it informs me about the label of the test okay this is exactly what we did last time that we saw this distribution in this case we say they're highly correlated so if my training point is large then my test point is also large this is likely to be large and so now when you actually tell me the training value and what I'm you know basically and you know this reduces to just a one-dimensional gosh distribution and that's the distribution over the test point okay and that's exactly this thing that basically says giving my training data my test point just becomes a Gaussian distribution any questions let me go over an example so an example would be for example a well let me go over the the old one but the housing right so let's say I want to sell my house now I want to estimate the highest price of my house my training point my one training point that I have is my neighbor's house and my neighbor just sold his house for you know a certain amount of money I know my house is very very similar so what I'm doing is I'm modeling this with a Gaussian distribution I basically say here have a distribution that's highly correlated and if my neighbor sells this house for a large price then I'm probably you know my house is probably worth a lot too if my neighbor ends up not getting a lot of money for his house then I would probably not get a lot of money for my house too because we have very similar houses right this now that's one question right where do I put in that our neighbor actually in my neighbor actually has a similar house than me and that's exactly what we stick into this this value right is the covariance matrix the covariance matrix maybe in this case is a two dimensional matrix where the dot the diagonal entries here basing the variance off the different values so this is basically Sigma 1 squared Sigma 2 squared this basically tells me the variance of my own house how uncertain this basically says how you know how uncertain in my in this direction in this direction but these off diagonal terms they are the interesting ones they tell me if this is large and positive if this is for example it's say you know three or something and you're also three that means that my neighbor is highly correlated with me that means it's my the Gaussian looks like this if this here is zero then the Gaussian would look like this it was like I guess I like this right well basically no matter for what value my my neighbor buy this house sell this house the Gaussian he looks always the same so my point my my house price is not affected by my neighbor's house okay and so the trick is basically if you just assume that your data is couch distributed the mean is not very interesting you can always compute the the mean of your data right so you can just say the mean is zero and just add the mean later on that's not very interesting what's interesting is the signal right so all you need to specify is the covariance of your covariance of your data so base is saying if I have a test point which training points are highly correlated to my test point right and what do you want you want those training points that are very similar to your test point to be correlated to basically have large values here these off diagonals and those that are very different to be 0 so let's say I have two training points let's say this is my house you know my house my neighbor's house and you know Donald Trump's house in Florida right and so actually why don't you fill in that that covariance matrix you and your neighbor just stood it down for a minute and try to figure out what does that look like the first one is me Killian Killians neighbor donut shop like what is it called president of the United States by the way I don't have a huge mansion with 50 in our bedrooms [Music] [Music] okay any suggestions and by the way that variance you obviously you have you know it's just about the scale I which numbers are large which numbers are small right so any suggestions for the diagonal what should be the very you know what to be the diagonal entries yeah just I'm not they're not sure oh I see I see so this yeah okay so basically you know these are PC sometimes so the important thing is face let's say my house is the variance of one right my neighbors are some variance of one for the Trump hobbies the much larger variance right because he sells his house for 50 million or maybe forty million right etc right there could be a lot more variance right let's give this a right so that's in some sense you're busy saying about how uncertain that you know are you about each individual each individual training right now comes the interesting part is the off time right what can you tell me about the off diagonal terms and one more time the off diagonal terms please you tell me how correlated they are right to how much the value of one in some sense tells you about the other so what would you put in here anything with donjon 0 that's right right that's right alright so basically you know me and my neighbor Wright's house it's a very very different than Donald Trump's right O'Donnell chum sell this house for 50 million dollars you know that does not affect my my house price right so we are basically uncorrelated right like the fact that he sells this house for a lot of money will be a little money that makes no difference and my neighbor and I right well we are high pretty correlated so maybe we have a point nine years right so maybe you know so now any questions about this raise your hand that makes sense okay awesome so here comes the last magic trick for this lecture and then today a problem is there gonna be a demo at the end so so what is this basically doing right basically you know once we have the Sigma right this block is into a Gaussian distribution the mean is just the average house price in America who cares this is easy right this is not interesting it's just the a you just get this when your training data and bam what do you have you have a gouge distribution from which you can sample you know anything right you can then just you look at the conditional and for any training point based if any test point I get a distribution over over its name right so all you need to do to make predictions is specify this covariance matrix and what properties is this covariance matrix have to satisfy that's the question any ideas without looking at the notes yeah positive semi-definite exactly and however be aware where did we hear this before I know one place in the survey someone said that was positive definite matrices are the worst part about this course so they keep keep creeping up where have you seen positive semi-definite matrices before kernels that's right this here is a kernel right this matrix here is a kernel it's a little bit more restrictive than a kernel it goes typically well like I mean usually any kind of function actually works typically we try to avoid negative values the reason you want to avoid negative I mean sometimes it's fair to have negative values it's okay but that would be Z means inverse inverse correlative I'd say like you know even neighbors sell something for a large price that reduces your your house price in some settings that's actually that's that's fine actually and often we just use non-negative cards so here comes the beautiful thing right we need a kernel function great what else do we need we need a function such that similar points have large values right that's what we want right so here's our these are these right so me and my neighbor have very similar houses so we want value here to be large right because we call it and points they're very different sort of small eyes right again that's exactly what the kernel functions do it if I just remind you for example the RBF kernel think about the RBF kernel well let me yeah let me buy that here so the RBF kernel says K of X comma Z equals e to the minus X minus C squared over Sigma square okay so that city have two houses here the different dimensions are maybe the features of the house right so baby you know yeah how many how many bedrooms do you have right how many bathrooms do you have I know what was the previous sales price or something like this right well if you don't take me my neighbor we will have very very similar houses there's a very small distance so each and each of them and of something very very small is you know around one right so it's pretty large right that's exactly what we got we got 0.9 that's what we want now we take Donald Trump and me right is Donald Trump I'm of bedrooms I afford bedrooms here's 50 right to four I don't want for him one for his wife I don't know what you hate for but why not and I don't want to ask actually all right so that that's a much much larger distance right this year this distance he is very very large why'd you square it right this sums a massive value right e to the minus something NASA is gonna be very very small so you're gonna capture exactly that's why these points have nothing in common they're very far away and so once again what we're doing here is actually something like nearest neighbors might be basically in the space of houses right and I'm here you know my neighbors here and Donald Trump somewhere over there all right and we basically put these couches around and say if things are close together then they are correlated all right then they influence each other's you know each other's labels and if they're far away then they're you know the label of one does not really effectively well it doesn't tell me anything about and that's actually all we need to do so no more keyboard all right you're basically done if you understand this you're one of the few people understand Gaussian process requests so let me get just go through the steps so in Gaussian process regression but you're basically doing you're making this assumption that P of you know Y given X and your data is star John from some distribution and for now we just say always zero me because you can always add the mean and then some some function Sigma which is really Sigma is just to come right where K IJ equals K of X I comma X J right so it's kind of a funky thing right so what you do is your little bit your feature vector you basically compute a kernel matrix out of it and you stick it into a Gaussian distribution and that's all you need to do and if I now want to know what's the prediction of a test point I need to know this is my test point oh yeah this is now so what do I do with it you know and actually oh sorry so this is x1 y1 yn if I now want to know what the prediction of one particular test point that's now conditioned on on the the labels of all the training points and my test points yeah and this is basically that this year whole thing and what I get is actually something you know very simple I basically get this is again a distribution there's not the conditional so basically what I'm doing here I have let me write this moment the probability of y 1 to y and y tres points given x1 xn and the access point is Gaussian distributed there's basically this this n plus 1 dimensional space now I condition on the first end because these are the training labels that I have so I assume I data is from a huge gigantic Gaussian distribution and now what I'm doing they see I'm saying well for most of these actually I know what the value is right I know my neighbor still has his house right there is now a training point so now I'm conditioning on the data that I which I know the labels and want to know what's the value of the test point and so that is now also a Gaussian with a very specific mean and variance then the means the following K star transpose K inverse times wide and the variance is K star star K inverse times K stop and let me remind you of case what K star is yeah K the covariance matrix is the following k k star star transpose so here we look at my covariance matrix these are basically the entries of the training points Y and this here's my test point so this Y is basically my house the last column here is my house this here's my neighbor's house in this year at Donald Trump's house place right so my training data is basically this block up here I just called K and my test point is there what I've star in K star star is basically the variance of my of the test point does that make sense sorry one more time okay good so let me specify this this this covariance matrix that's over all my data my training data and my test data so I just sort them such a might reindeer comes first May comes my testator and I call the the column that's training data with training data ok whether the matrix up matrix that's Kennedy data x trillion data okay that's please look how that that was exactly how correlated is Donald Trump's house with my neighbor's house and then here's my test data I'm the test date as to how the first one is basically how correlated I with my neighbors how a second one is how correlated my with my Donald Trump's house the third one is how correlated am I with my own house my variance of my so that's the K star is faces the correlation with the training data K star stars men on variance any questions yeah yes exactly right this year is just kernel regression right so it turns out if we do this we get exactly the same thing as kernel regression now I know what you're thinking what the bleed right we just spent a whole lecture through this and we just derive the kernel regression but there's something beautiful that we got in addition to this we got kernel regression and we get variance right so we actually got not just the point prediction not just saying oh the price of your value is if the price of your house is 250 thousand dollars it tells you no no no the price of your house is an expectation three hundred fifty thousand dollars right well it's a good chance that you sell it about four hundred thousand us right and these distributions are worth a lot because if you actually if you want to integrate your predictions that you have a self-driving car or something right you want to know you let's say you know you make a prediction and say oh this is not a pedestrian in front of me right you want to know well the wait a second how certain are you about right if you say well it's not a pedestrian I'm 90% sure it's on a pedestrian ten percent of all yeah maybe that's right and you may still when I stand with the brakes right so these these these uncertainties are actually super super important right and this is why I love Gaussian processes so much a because it's the most elegant algorithm you just computed kernel matrix and then just this year's the prediction in this year's variance right how many lines of Julia is this one one line of Julia writes two lines of Python any more questions yeah I can show you a little demo of the algorithm in action okay any more questions someone had a question Yeah right so he's saying well it's not predicting the same but we just get a distribution around wouldn't you still just go with the same prediction right yeah well so if you just need a prediction then we just waste I just wasted your time right but actually if you want to do something with prediction with that prediction if you want for example want to integrate it right then actually distributions are really really really important let me give you a simple example right no obvious examples for example speech recognition right so speech recognition then you know the system like Siri right hears from the microphone what you say right and let's say you know you could have said multiple things why do you say ice right well did you mean eyes or did you mean eyes right well I have somewhat of an accent so no one else right but if you would just predict the most likely one right then you would just always take the most likely words you just heard but it could be way off but if I give you a distribution then I you would actually see it could be ice or it could also be eyes right eyes a little less likely because the way I speak right but it's not you know it's possible the beautiful thing is now you can actually take the previous verse in the account and say wait wait a second I just said I look you in the right it's pretty unlikely I say ice cream right so if you basically have two different probability distributions you can now integrate them all right if they're both Gaussian you get a Gaussian right and then you actually can predict the most most plausible word right and that's exactly by the way how speech recognition systems work so the actually audio you know the recognition of the audio is actually pretty unenacted but one of the big breakthroughs in the last couple years was that basically you know Google and those companies could scrape the web and guess got you know millions and billions of lines of text and what they could estimate very oculi is which words are likely to fall after other words and if you integrate that that into the speech recognition system right anybody say well you could have said you know one of these five words but only one of them is actually likely given what he or she said before right an interrogation the way these things work is also given what the person said afterwards so these recognition systems are always a little bit behind right they kind of also look into the future you know look back right and say given that I just said cream right I probably meant ice right instead of ice etc yeah [Music] no no kernel is just a well-defined covariance function everything works out and you can define any kernel function you want let me give you a little example where this is really really cool so here's a common is this the right one yes oh no that's the wrong one sorry wrong demo GP demo okay good so here's that here's a common case where this is used all right so here's a scenario that comes up a lot you'd be surprised actually last time I taught this a faculty from from the med school actually sat in in the class and he saw this and he ran out afterwards and started a start-up and and actually he that's what he does now that she is no longer faculty so here's an example of a problem that's actually quite common so imagine I want to minimize a function but this function is really expensive right so you can't just evaluate it everywhere like every time you you know you can query me you can ask me what's the value at X and I give you the value but it cost you a hundred bucks right or it costs you five days right a typical example could be a chemist right trying to set up some some experiments right and doesn't know how much co2 or how much do you want to put enough that's a material science you want to make a new material and you would like to maximize the conductivity or something right but every time you make a new material it takes you five days or 10 days right it cost a lot of money and you want to find the best material so let's cut a minimization problem alright and you know before we be a weighting descendants all right but there's no gradient right what's the gradient of a material right I get doesn't mean right so here's what we do you basically use the gap you basically use Gaussian processes and we assume the data has drawn from a Gaussian distribution and so this here my data is now any X X values and Y values right and so there's some function that I'm trying to minimize I don't know what that function is and so the only thing I'm doing a mooing are using an RBF kernel I say the function is somewhat smooth if I know the value at a certain - - that's probably predictive of - to point one so here's what I'm doing now I give you a single training point what you see here the red line is my prediction of what the function looks like right so right now this is just straight lines as I don't know anything right so now here comes the game I give you one value right I tell you oh I run an experiment and it cost me 10,000 bucks right but here I know it - one this here's the function value okay and now what I can do is I can refit my Gaussian process and say what's given that I have that one value what is the Gaussian distribution and this is what it looks like right let me just understand you where's the gap you know where's the gouge distribution here right the gouge distribution is kind of coming out of this right at any given X slice you have a Gaussian distribution and its center the mean is the red line and this here is one standard deviation up and this is one standard deviation down so kind of you have to kind of think of this is a 3d thing right that kind of goes it goes this way does that make sense and so for any test point now if I this is my test point I get a Gaussian distribution right that tells me what the mean value is and how uncertain I am about this okay does that make sense raise your hand at that finger make sense okay awesome and here comes the beauty right the beauty is the uncertainty is very small around here right because I observe the value but it's really large here right so let's say I'm now trying to find the minimum I want to find the material that minimize a certain property right well I could search here because you know I want to find out new run new experiments I know here the function is pretty low right but I'm also not very very I'm also very certain about the value here right the function my predicted function is pretty large but I'm very uncertain so it could be that actually if I run an experiment here the value is actually you know lower right so basically what I could do is I could actually trade this off and I'll say you know here I know what I get right the value is around minus 3 but over here you know it's probably 1 0 but I don't know I'm really uncertain so maybe I try this out right and let's say I try this out and I get a pretty large map right so this is my function looks like now let's try another value here and so on and so in searching for the minimum I basically now know okay here not very certain right I know this value here is a lot worse than here so there's no point trying here anymore right but here is a lot of uncertainty so maybe I should try it - for some all right so you try minus 4 and you get this value right so now you know oh maybe the minimum is somewhere here okay does that make sense so your basic heap searching and keep searching by the way this is exactly the technique they used to find the Malaysian airliner that got lost in the ocean right the Vaizey shot you know they had submarines there to go down and try to find it's anything like you let me go okay now now you're pretty certain it's not here right that's it sir somewhere else they just busy use Gaussian processes right okay we are over time already I apologize I will show you another demo the beginning of next lecture 
","['', 'Gaussian distribution: A bell-shaped probability distribution with a single mode. [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]', 'Maximum a posteriori (MAP): A method in Bayesian statistics to estimate the parameters of a model. [2,3,4,5,6,7,8,9]', 'Linear regression: A statistical method for modeling the relationship between a dependent variable and one or more independent variables. [1]', 'Noise: In statistics, the difference between an observed value and the expected value or the �true� value. [1,3,8,9,10,13,14,15,16]', 'Prior: A probability distribution that expresses beliefs about an unknown quantity before some evidence is observed. [2,3,8,9]', 'Likelihood: A function that describes the probability of observing a particular outcome given a certain set of parameters. [2,3,8,9]', 'Bayesian rule: A method for updating beliefs based on new evidence. [2,3,8,9]', 'Integral: The limit of a sum of infinitely many infinitely small terms, approaching a definite value as the terms become infinitely small and numerous. [3]', 'Prediction: An educated guess about a future outcome. [2,18,19]', 'Speech recognition: The technology of converting spoken language into text. [18]', 'Uncertainty: Lack of certainty about the outcome of an event. [18,19,20,21]', 'Gaussian process regression: A type of regression that uses Gaussian processes to model the relationship between a dependent variable and one or more independent variables. [18,19,20,21]', 'Kernel: A function that determines the similarity between two data points. [18]', 'RBF kernel: A popular kernel function used in Gaussian process regression. [18]', 'Minimization: The act of finding the smallest value of a function. [19,20,21]', 'Malaysian airliner: Refers to Malaysia Airlines Flight 370, which disappeared in March 2014. [21]', 'Vaizey search strategy: A search strategy used to locate a missing object or person. [21]', '']"
"oh come today we will be finishing up Gaussian processes I'll just show a little demo and then we move on to something completely different also briefly about logistics so the new project is out on kernels and the last homework was do the next homework will be shipped very very soon it's currently being tested one thing that's important about the next homework assignment is you have to do it before the next project I guess we Kunia project six that's the kernels after that comes project seven on trees boosting and bagging for that you it will use something that we don't cover in class that you have to derive in the homework so please keep that in mind so if there's a project you don't know how to do it's probably because you haven't done the part at the homework yet right so it's the question where you have to derive a linear time update for the regression tree yeah there's a lot so there is hopefully eight projects in total so six right now is online there's seven and then comes eight eight is I think it's a 90% chance that you have project eight and and there's also the carrying competition which will go alive tomorrow or on Wednesday and that's all set up now they're just testing it one more time okay any questions about any of sorry no there's one more after this so the homework that would come out now and then there's one more after that on deep learning and then that's it yeah any more questions okay so what we did last time let me just briefly review you know the high-level picture of Gaussian processes one more time if for a show you a little demo so basically what we did is we showed we went back to this linear model and we say well if you want to do a regression right and behind this Lenny a model with y equals W transpose X plus B and then we said some Gaussian noise and in that case actually the question is how do we learn that W and what we've done several lectures ago we fuse introduced two different methods to learn this W that's either Emily or Matt in the last lecture we said well actually there's another alternative it doesn't have to be Emily and map Emily a map what they do they both give us one particular model W and then once you have that model you make predictions Emily Emily gives you the the model that maximize the probability of the data given W Matt gives you the probability of W given the data and so we introduced another alternative which is you know I just called the fully Bayesian or the the true Bayesian approach which is but you said well let's just get rid of the model okay altogether and instead just say well the probability of a Y given X is and the data as here's my test point it's basically you know the distribution it is this the air is the the prediction under every possible model it could possibly have so we say well for any W that I could have I get a prediction and every W has some probability that I would actually get it from my training data and I integrate that out and so if I do this basically I'm giving you the prediction that's really the outcome of every possible model I could learn weight it by the probability that I actually get this model from my training data then it turns out because everything is Gaussian and everything is just product of gaussians and the product of two gaussians the gain Gaussian by the way that's exactly what you had to do in the placement exam that was one of the questions in the placement exam to prove that what we get is this here's a Gaussian distribution and let me say well why might be actually doing all this stuff if you just know it's a Gaussian distribution we can just fit that Gaussian distribution to begin with and so that's the Gershon process these come in you Vaizey say all right well that sounds great all right and anyone know how to move this thing up I don't okay that's so what we then do is we say well actually we can just assume a whole data is Gaussian distributed right and that makes sense because in some sense here the all the assumptions we made a pretty reasonable right maybe arrive at exactly that conclusion so let's just assume that all right then basically what we're saying is all our data all our training data and our test data come from the same Gaussian distribution if some huge distribution in this n plus 1 dimensional space and so in this high dimensional space we now have the scout's distribution we can always assume it as zero mean just because you can just subtract the average label from your labels then you have zero mean so that just makes the math easier and a Gaussian distribution has a covariance function right so it has a mean as a covariance function and so this covariance function is a big matrix that's n plus 1 by n plus 1 and so in some sense what are you doing when you have such a Gaussian distribution you can now draw labels right so what does you know if you now draw a sample from this that's an n plus 1 dimensional vector that gives me a label for every single training point and the test point okay that's what that's what the distribution does and the important part is that these these labels they're interconnected right otherwise you wouldn't be able to do any training otherwise the training data wouldn't tell us anything about it and the covariance matrix the mean just tell us an average they're kind of zero you know that's fine because we subtracted the mean that's exactly what you would get the covariance matrix tells us how they are tied together the first thing you have is the diagonal the diagonal of the covariance matrix tells us for every single dimension here how spread out that is if this entry here is small then that means you will always get something around zero if this is large this entry here the Sigma of one one of one of the first dimension and that means sometimes you get a very positive values on them to get a very negative value in expectation of zero right but sometimes positive sometimes negative you know varies around right this month the Sigma busy tells you how much does each dimension very on its own and you have Frazee one such entry for every single one of these these dimensions and then come the off diagonal terms and the off diagonal terms if you have an I J entry and that's really where the magic is right the off diagonal terms tell us when we saw draw a sample right how correlated are they with each other the different dimensions with each other so what we get here is IJ is a large value here then this means that if you draw weii if you draw a sample then by I my J will always get roughly you know similar values right because they're correlated with each other okay so if you can draw as many samples as you want you right but you know one of them will be these two kind of go together okay so it will be very rare that one is large and the other one is small right because the IJ entry in the covariance matrix is large this is kind of like imagine these that you know this now goes back to the housing price one more time these are all the houses in Ithaca or something or you know in the United States right I and J may not be my house in my my neighbor's house right or two neighboring houses so I can draw a different values here maybe they correspond to different times where houses are different prices but these two roughly always had very similar prices if the IJ entry is smaller that's a zero here then these are not uncorrelated and actually this can be large and this can be small it doesn't really matter all right so they kind of they're independent of each other right and so what you as a data scientist but would you job is you basically the first thing is you make this assumption that the data is Gaussian distributed that's not a crazy assumption the see it proves that you're not crazy that's good right keep that keep that around in case someone accuses you have been crazy you're not and so after you made that assumption you have to make a second assumption and that is basically what points are correlated with what other points and that's where you baked the assumption of the algorithm and remember the very first lecture I told you every machine learning algorithm has to make assumptions otherwise we cannot make predictions you have to assume something about your data this is what you bake it it right and so once it's Assumption for example could be like hey nearest neighbors where you say similar points have similarly then you could for example say here the entry Sigma IJ is e to the minus X I minus XJ squared over some Sigma so basically saying if these two points are similar and features then this year will be you know around one so they're highly correlated if they are very different than this year will be large so this year will be zero so they are not coded right but that's your assumption that's just one assumption you could make another assumption you could make is that you say Sigma IJ is X I transpose XJ alright then you're assuming that they're linearly there's basis I'm lied right so if you you may have a value that's really far away that's okay right that's basically saying there's some kind of line saying if this guy goes out this guy goes out too so there's a linear dependency between those two or you could have a periodic common actually says you know you have a you have a periodic function anyway so you do this most of the time we just use the RBF kernel that's because the simplest at least in this class and the Mangia of this kernel function then you do the magic the magic is that you can actually just say what is why test given all the other ones all the training points and and my my coordinates at that so if you know conditioned on everything else that is this still stays Gaussian and in the like survey see I showed you what the derivation is what the expression is so this years again Gaussian distributed right with some value K star where this here's K star transpose K minus 1 this here's my cake and Y and then you have some signature and the beauty is this is now a one dimensional Gaussian distribution where this here's my mean and this is my variance any questions about Gaussian processes yeah what is the by without any subscripts where is it oh I see I see yeah this is my test point yeah oh I see that's a long vector y1 to yn yeah yeah so essentially all you're doing is you're putting you know you're putting all your data in one bit you busy making a big gouge distribution and now you say well I know the name the values of all the late training data and that now gives me a distribution over the test date so in some sense it's not really an algorithm right as a formula it's a there's nothing elevated about ya Sigma is the so this Sigma yeah this is amazing you have to look it up it's the you know it's the closed form solution for the variance of the test point so it's a star star minus K star transpose came in as 1 K star so I just didn't fit in it so the reason I like Gaussian processes so much is because they give very good estimates for for regression problems and they give you uncertainty for aggression problems and that's usually hard to come by right if I regress something like with classification problems you can kind of say about how far am i off of the hyperplane right if I'm very close and I'm uncertain if I'm far away I'm uncertain yeah I'm uncertain as always I'm very certain if I'm close I'm uncertain with the regression you typically don't have such a measure right so I estimate something but it's not clear how uncertain the algorithm is all right the beautiful thing here is that you actually get these very well calibrated Gaussian distributions and we know you know Gaussian distributions make a lot of sense to us and we know exactly how to interpret these one place where Gaussian process is used a lot is function minimization of unknown expensive functions and I mentioned last time the the Malaysian airline actually someone told me afterwards by the way they never found that planes and so good you know algorithms can't be that great I guess they did find it eventually but I am Not sure the M but one place where machine learning that comes in a lot is actually hyper parameter search so think about the following problem I have a support vector machine and I would like to know and one thing you will see in this project that you're doing now is that you know we've taken a function and you take your C / n parameter and actually support vector machines are very sensitive to high parameters so if you have the wrong parameter for your kernel and the wrong C parameter you get horrible results so how do you find the best results and basically what you're doing is to try a combination of your hyper parameters then you train the system then you evaluate it on your validation set and then you get a number which is the validation error right and so that's essentially what does this is a function right it's a function that takes is import to hyper parameters and it's output gives you the validation error and you would like to minimize that function okay and that's my favorite application of Gaussian processes is called Bayesian optimization or short Bo which is a horrible acronym and I can now show you an example of Bayesian optimization okay and what I will show you okay you now I show you twice because it's going to be anything what I have is I have a function I think it's actually kernel regression that I'm solving repeatedly and I would like to find the best hyper parameters I have two hyper parameters one is regularization the other one is the kernel width so the Sigma and the crown and I want to now find which one gives me the lowest validation error and I have two methods one is to take grid search just try out any combination of these two parameters and see which one gives me the lowest error that's reasonable and that's widely used another one is to use Gaussian process regression and so here's the cool thing that I can do with Gaussian process regression I can actually I can always look at the setting that is you know where I'm still really uncertain and say well I don't really know we know what but I should get here or I can look at something where I know I parameter setting that has worked in the past very well and that's called an exploitation exploitation trade-off so I can either try out new values where I'm sure that may surprise me or I can try something that I think well that's probably going to be pretty good and there's nice ways to trade these two off and I can now show you a little demo where I put these two head-to-head so next to each other okay so here is my demo and what you see here on the left is this here's my grid search on the left so what I'm trying out I'm trying every possible combination of Sigma and lambda this is my regularization value and this is my kernel with all right and one by one I trying these these different values out and what do you see here on the the a this is actually the test error or the validation error right and I'm trying to find the setting that gives me the lowest validation error what I'm doing here instead is I'm fitting a regression function to this right so I'm trying out some random values at the beginning and then what I'm doing is I try look at more and more values that basically could be promising and that's the the red line here and one thing if I show it one more time but what you see here the surface here is actually whether you know any given point Sigma Lambda I can get make a prediction and the Carroll regression tells me what it thinks my validation error will be right that's the surface here okay and that's not always correct but the nice thing is that also shows me it also tells me how uncertain it is so the beautiful thing is that what you will see if I run it one more time is that the Gaussian process regression very quickly and she learns that that area that the surface very quickly very quickly and then I actually have a pretty good idea what the best parameters are so one more time right here understand I start off with some random ones and here already know what the surface looks like and here are the best values right so only after a couple of runs actually Gaussian process regression has already figured out what the best type of parameters are and this guy's still kind of you know slugging away here trying out values here they're always terrible every single one of them has been terrible right it keeps doing it miss Gaussian process regression one thing you see is here we know they are pretty bad right and the uncertainty is not enough to convince us that it's worth exploring here right because we know that the error here is very high and even if the uncertainty is you know reasonable it this is not going to give us anything so very quickly it learns to dig here right and try to find the high parameters and one thing you can see here on the on the bottom graph is basically the number of iterations the number of times I had to train my algorithm and this year is basically the best error that I got and Gaussian process regression very quickly you know arrived at the best algorithm right at the best setting where now after maybe 35 iterations worse you know only after 80 iterations that the grid search get there right so that's that was twice as long any questions about this so keep this in mind when you do and when you do hyper parameter search later on in your industry or something why do people do these quit search you will see people do the script search right it's much much better to use a machine learning algorithm to predict you which high parameter setting will work well yeah question and then grid search yes right oh why do you people use the good question so people see us why do people use yes the use grid search and the reason is because they don't understand Gaussian processes that's right yeah so yeah beige optimization just has a two dimensional regression problem the first type of parameter and the second type of parameter and try to predict the validation error in that setting that's what it is right since sometimes you put a machine learning algorithm the top of another machine learning algorithm and the nice thing is the Gaussian process regression really doesn't really have any parameters right so then that that's why you can actually use it right I give you that again had parameters then you would have to use another algorithm the top of that another one on top of that right it wouldn't work right yes there's a some high parameters but you can this good ways of setting else any other questions yeah that's right good questions in the first few it starts randomly and then after that it actually then it fits the surface and then it basically looks into what's the minimum and taking into account the uncertainty that's the important point right so you may have some areas where things is pretty bad but it's so uncertain that it could surprise you other areas where it's probably pretty promising but it has no uncertainty and then it trades off and basically looks which is the one that's most promising and it evaluates that one then retrains the gaussian process last time actually I gave this lecture there was a some amount that for an American school was actually in the audience who took the class and he had a very interesting problem he was like well actually I'm working something I mean that's very related he works on hearing aids and so what he wants to do is he goes to humans and asked them and makes these audiograms right these are audiograms where basically what you do is you have frequencies here and this here is the volume right the intensity this is a logarithmic both of these are logarithmic right and so what you want to know is at what volume do you do not hear it anymore so blue means lazy you look at this you have a mouse here I don't have a mouse you have these sorry one second here he may see this is the line right these red ones it could not the person could not hear at this frequency he or she could not hear these and then eventually suddenly could hear them right and what do you want to know what he what he was very interested in is how do you you know once you have these curves you can build hearing aids that basically amplify certain regions where you hear badly to kind of make the server somewhat have strange strain like this is actually what you want to get so one more times the bays the up here you know in the very very top like this is local rhythmic right this he is very very loud right this is kind of the volume way you know he would go to a dead mouse concert right that would be the kind of the volume of a dead mouse concert and down here is not very loud at all that's kind of that's the sound that a real dead mouse makes [Laughter] the and and so the what they currently do is they take these these patients and they take a certain frequency and they slowly increase the volume and basically say say something press a button when you hear it right but often is the elderly people so at some point they fall asleep you know and then they they don't say anything or you know it's a very annoying process and they have to actually try every some of these tones multiple times he was actually right down at this threshold you actually hear it half the time actually right when your heart beats you don't hear it and you heart is not beating you do hear it it's like anyway so the what he wanted to do is actually want use gas process regression and so the idea was basically instead of using this grid which is the fda-approved way of doing it in the United States can we just actually always kind of find the optimal tone that is most informative and so we actually started a research project this together and and turns out you can do this very well right so the first thing what it does place this tone you hear it and this here's the current threshold and you keep playing tones and only after 15 tones right usually you subtract them 200 tones after 15 tones you're already much more accurate than anything that was previous technology it could have given you right and one thing here was a realize it's really really coolest with tones you can actually play two tones at the same time and then see if they you know hear something or if they don't hear anything know something and so actually those guys now went off and actually they actually that this person actually now is commercializing this as a hearing aid system yeah that's right that's right that's right the thing with the hyper yeah that's right mm well Passover has passed so it is a it's a slightly different problem it's called an active learning problem but you actually generate your own data right so it's basically you you get a new data point now you get a label for it and you basically try to find the decision boundaries right that's called active learning so it's it's totally fine and in fact actually you can speed up your training a lot especially even labeling points I bet it's very expensive I can show you this one more time here in the and I think I is this the demo yeah so this is basically the demo where you basically you get a bunch of data points and now I fit my function and now I want to say well where is you know that's anyone for now find the maximum instead of the minimum or something well you know this maximum was probably around here but it could also be here so maybe add a new data point to get that label that's maybe here right and so then I get this value etc that's an anti question okay any more questions about this alrighty I have one more warning to you actually the Gaussian processes are awesome and the video regression I would always recommend you do Gaussian processes they're extremely accurate and like they're so accurate that they actually became illegal in North Carolina and I'm not joking the here's actually I this morning I'm proud of found the legislature that actually outlawed scales and processes in North Carolina and it's actually okay that it's a proud moment and it's in the context of global warming of course so the basically people use Gaussian processes that nonlinear kernels to predict the sea water levels and so North Carolina's in the coast and people had to predict you know it's very important to know where the sea water levels are going if you take these nonlinear algorithms not only you know you basically get a very good prediction of where you see the water levels are going but you also get uncertainty so you can actually say well it's at most a pad or it's at most that good etc and unfortunately that was still too bad and people didn't really believe in global warming so they they couldn't really do anything about the data but they could do something about the algorithms and so they actually passed this law is that from 2011 since a while ago but it's still active and actually shows here that so if you do predictions on sea water level rising these rates all only be determined using historical data this is fine and the date should be limited to the time period following the year 1900 that seems fine rates of sea level rise may be extrapolated linearly so you're not allowed to use nonlinear algorithms to estimate future raises of rice but shall not include scenarios of accelerated rate that's the nonlinear thing so no matter what your data it looks like right even if it you know has this very clear elbow structure which it does right so basically the temperature goes up right like this you have to draw a line through it and can tell you voters no problem nothing is happening so please keep that in mind when you use Gaussian processes North Carolina and don't go to the bathroom all right okay and now for something totally different all right so one thing that the Gaussian process essentially is is the nearest neighbor algorithm and it's like so we be it's basically a linear a linear regression that we kernel iced kernelization always reduces bias and then essentially what we arrived there then we used the RBF kernel is something that's very similar to the nearest neighbor algorithm so what I want to do is spend one lecture on the nearest neighbor algorithm and there's a good reason why I will do this you will see it next lecture when we go back to this or when I you know build on top of it and what I will briefly review is KD trees anyone heard of KD trees raise your hand okay very few people so here's the problem and I give you the notes in a few minutes and K nearest neighbors is a very slow algorithm and why is it very slow because what you have to do is you have all your data points and what you need to do is you need to find the nearest neighbor right that's it this is my test point I don't know the label of the sky and in order to find the nearest neighbor what you need to do is you define the distance to every single one of these points and then you find the minimum and that's the nearest name alright so imagine I would like to find out who lives closest to me right in Ithaca I guess most of you probably live in Ithaca well the one thing I could do is I could go the first person say where do you live write compute the distance writing and go the next person whether you live whether you live whether you live where do you live right and eventually basically at the end I get take the minimum and that's my nearest neighbor right seems rather slow the much faster way could be if I've before and say well I live in Belle Sherman I don't know if you guys know the different parts of it huh does anyone else live in Belle Sherman raise your hat well it doesn't think so all right they would have made it too easy who lives on north who lives in South Campus he live in South Campus is exist well so basic people North Campus they're pretty far away from where it's all right so you guys can probably you know I can probably rule you out and so one thing if you do is crazy look in the beginning at the global structure of the problem and say well you know I have a good hunch right people who kind of took me my nearest neighbors they're probably you know roughly close to me or another way they put another way like one way to speed up nearest neighbor search by in fact the two is that before I do this very simple pre-processing step where a Basel II to a hyperplane that divides my data in half okay and I do this at the beginning when I collect my data and now what I do is I have my test point and the first thing I do I look for the nearest neighbor only and I am the half that I'm in and I do this and I find the nearest neighbor it's this point here okay this is my immunise neighbor and I claim I make an outrageous claim I claim that once I found this nearest neighbor which is the nearest neighbor of all the points in here I came I don't even have to look at any one of these points why not who can tell me why not it's not a trick question yeah that's right so the other points are at least that far away and that's already further away than this the point that I already have right so nobody can be closer than the neighbor that I've already found all right so I don't even have to look at any of these points yeah any other questions all right good he got me he said well wait a second you cheated I just shot at the point right here what are the point is right here right and in that case you have to still look all right and so the argument is basically well most of the time this won't be the case and then you're lucky and occasionally you just has to do have to do as much work as you would have to do anyway right so that's not a big deal either right so you either get a speed-up or you don't but yeah so ah all right all right all right that's exactly where we're going that's right any other questions yeah the same question okay good good right so then of course you know comes the same question well if you can do this once right why can't we do this multiple times right so we could for example now say well you split it up like this and then you spit up again like this yeah and then the violence I could spit up like this like this you spit up like this like this and so on right Jimmie's you have little boxes right and what do you do initially you just find the nearest neighbor in your box and then you try to rule out the other boxes okay that's the high-level idea behind KD trees that's exactly what we're doing any questions about the high-level idea okay so here's the first quiz I have for you and your neighbor imagine I have the following and have a test point this is my test point XT I have a previous nearest neighbor there's my previous nearest neighbor and I claim that this point is closer to my test point then this wall now I have another point here but I haven't looked at any more can you prove that this point here must be further away than this point it doesn't make sense so I basically have my text II have my presumptuous presumptive new nearest neighbor now one more candidate over here the only thing I know about this point is that it's on the other side of the wall right the wall divides the entire space can you prove to me because I have to be mathematically rigorous right so if you do this kind of stuff can you prove to me that this point must be further away than that I provided that that the wall is further away than they give you a minute see if you can formally derive it I mean I know it's obvious right but we have to be rigorous because the next steps will be less obvious so [Music] [Music] all right who thinks he or she has proof oh no you can't prove it you can't use it yeah this point yeah that's a projection onto the plane oh okay good I like this this point is closer than that yes yes yeah I think you're going the right direction I mean that's definitely a way to prove that they absolutely right actually and I let me just do it one that's even simpler with the notation that I drew here but you definitely write right the spacing the distance from X T to this point here is C plus D all right now I claim there's this theorem by this guy I don't know if you heard of him Pythagoras because T squared equals B squared a squared plus B squared right so C equals the square root of a squared plus B squared and then we have D well this is certainly you know waiter than zero so we can just one thing we can so vague and that's greater equal than just the square root of a squared plus D right well that's just a and this year so this is a a plus d DS all the positive thing on non-negative thing this is greater equal a okay so C plus D is greater equal a so it's greater than this and a is greater than the distance to to the nearest neighbor and that completes the proof anyone spot a mistake now okay any questions all right awesome so the important thing is we didn't actually use anything particular here one thing we used is abused Euclidean distances right so we made the assumption that the zpd distance okay I can now give you the lecture notes a lot of excitement for this next time I'm I'm glad all right and so the goal is to speed up nearest neighbor search and we basically partition the data repeatedly into half and so the idea is the following at the beginning when we have our data we build a data structure and then hopefully we can use this data structure just a lot later on during test time to find nearest neighbor really really efficiently click thing who did not get a printout sorry and the printer broke during printing so this is all I got and the notes will be online over there or over the our online actually so maybe there was if someone has you know someone in their neighbor have they both have one and you get along with your neighbor maybe one of them you could pass them back to those people in the middle all right so let's say we we have this data set Andy I know I'll tell you how to prado set up this data structure and the data structure is a tree I took all those computer scientists in the room you will now both be relieved right finally something from computer science so you guys all know trees really really well and the idea basically is we build a balanced tree where every single node and every time you split we'd split the data set in half so here's the idea it's quite simple the algorithm is false we just take the first thing we do is we take one dimension of our data and it doesn't really matter which one of this typically what I recommend is take the one with the largest spread so you compute the variance in each time and you think the one with the largest one but that's you just use a heuristic another option is to rotate to the dimensions and typically variance works better so in this case that's if you take this dimension what we do is you find a spreading point I said exactly half the data points on one side half the data points on the other side that's easy to do we just look at this one coordinate we just find the median maybe split along the meetings so now we busy have this here there's my first putting value T and my dimension is 1 right X 1 so then you basically take your data initially all your data is in the first node and then you say we have two subsets at the left and the right one so we basically say it's X 1 greater than T like you know yes or no and now you get two new notes does that make sense raise your hand that's that makes sense ok awesome so I take all my data I find a splitting point which is fine today I a directions or a dimension for example that I mentioned with largest variance I find the threshold it is just the median that's very fast to compute and just say if you're larger than the meaning go to the right if you're less than the median go to the left and then here I just run the algorithm again recursively right I find some other value it could be the same value and again I you know I keep splitting so let's say for example here I split again along this direction like yesterday in this direction this is my first one this here's my second the first bit my second space now you have for example X 2 greater than T you know T - 1 something X 1 greater than T 2 2 and another threshold and then these are my lease so these are the final ball air boxes that I end up with and this is all there is to it so it's a very very simple algorithm to find such a data structure and now how do you use this during test time well I'll do a test time it's any questions about how to create this destructor so the end is a balance tree because you always split everything in half and all your data actually isn't these things at the bottom and the leaves and ADA every data point can only be in exactly one leaf all right so how do you know how to testing and it's quite simple what you do is you say you take a test point and you basically descend into the down the tree why do you say like on the first I mentioned I'm on the right side I'm on this half under this house and if you're on this side you go down here then you say okay my x1 is a garage larger than this threshold or smaller if it's larger than I go here otherwise I go here right if AZ just log into which of these leaf boxes to your phone that escapes me of these four leaf boxes okay so you fall into one of these boxes and then what do you do is I'd say I end up here then they do you need you do a nearest neighbor search in this box well that's very fast because we have very few points in these boxes this tree can be pretty big going down the tree doesn't take very long how long does it take I heard it login it right log n time it's o log 2n right what's log n it's dirty right it's no matter what your n is log 2n is pretty much always less equal 30 right because we never work with numbers that allowed onto to the 30 that's ridiculous right I mean even if you're 31 that which wise is large right or 32 that with four times as large like that you never see that in computer science right so that's basically you know let's you know for all means and purposes are constant okay so you go down here and you end up in one of these these leaves you find the nearest neighbor if you're pretty good about yourself now you're not sure yet if this is your nearest neighbor but you haven't looked at all of these data points yet so what do you do now who knows that first search raise your hand okay this is just steps first search so you go down you know into this you know the one of these let's say I end up here and what you do is you back up one and at this point you know let's say I fall into this this this this bucket here I find my nearest neighbor here that's this guy now I go one level up what does this level up that's exactly this decision boundary and here's where I do the pruning so here say I've looked at all these guys I thought my nearest neighbor let go one up is it worth going down here right and when is it worth going down here only of the distance to the wall to this threshold is less than the distance to make your nearest neighbor if this if these points are all behind the wall right if these points are all further away than my years David I don't even have to go down and I can just prune it then I go one up and again I have the whole subtree that I haven't looked at yet and once again I look into all these points further away than my current nearest neighbor yes or no and if no then I just prune the entire subtree and if yes and they go down here I go down and that busy you know run the algorithm over and over again any questions about the algorithm yeah yeah sorry one more time how can be well you basically bound ready basically say so he's they're saying you're just looking at one dimension when you when you look at this wall right but that's enough right there sometimes you say in that dimension you're already that far away right if you look at the other dimensions you can only get further away right so if you're already too far away in that one dimension then I may as well just you know not look at you okay does that does that make sense okay last question that I have to put for today yeah right we will get into that I show you this next time so it depends a little on your processor so usually what you want is that these are all yeah punk-ass yeah okay see you all in two days 
","['', 'Gaussian Processes: a Bayesian approach to regression', 'Gaussian distribution: a probability distribution', 'Covariance function: a function that defines the covariance between every pair of variables in a joint Gaussian distribution', 'Kernel: a function used to compute similarity between data points', 'Least squares: a method for finding a line of best fit by minimizing the sum of squared errors', 'Maximum a posteriori (MAP): a method to estimate the parameters of a model by finding the mode of the posterior distribution', 'Overfitting: a modeling issue that occurs when a function learns the training data too well and fails to generalize to new data', 'Uncertainty quantification: the ability to quantify the uncertainty associated with a prediction', 'North Carolina outlawed Gaussian processes: a legislation to limit the use of Gaussian processes with nonlinear kernels to predict sea water levels', 'Kernel dependency: a limitation of Gaussian processes where the model heavily relies on the chosen kernel', 'Linear regression: a method for modeling the relationship between a dependent variable and one or more independent variables by fitting a linear function to the data', 'Bayesian linear regression: a probabilistic approach to linear regression where the parameters of the model are treated as random variables', 'Decision tree: a flowchart-like tree structure where each internal node represents a test on an attribute, and each leaf node holds a class label', 'KD-tree: a k-dimensional tree used for nearest neighbor search', 'Ball tree: a tree data structure where each node represents a hypersphere', 'Splitting: dividing a dataset into subsets based on a certain attribute value', 'Median: the middle value in a sorted list of numbers', 'Pruning: removing unnecessary branches from a tree', 'Nearest neighbor search: searching for the data point in a dataset that is closest to a given query point', '']"
"welcome everybody one quick thing I have a few more notes from last lecture anyone want any okay terrific yes you oh sorry you go please pass them behind you okay quick thing about logistics parties underway things seem to be going well the latest homework is out that's the homework on trees and the next project is coming out on Monday one thing that's really really important I said this before but I want to re-emphasize it the project covers something that we do not cover in class so you have to derive it in the homework so the current homework seven I believe that just came out the question on the regression trees that is something you're easily deriving an algorithm that you have to implement in the project so please do it in the right order right if you start the project too late and you know haven't done the homework yet that's the problems we try to kind of make the homework ahead of the project to kind of force you to do it in the right order but don't leave it until the last minute also when you do the project and if something doesn't make sense it's probably because you haven't done the homework yet okay so yesterday our last lecture we talked about KT trees and KD trees are a just to remind you a data structure to speed up nearest neighbor search and the idea is quite simple it is that you imagine we have a dataset like this and you would like to find the nearest neighbor in this data sets you build a tree by you you know repeatedly splitting the data in half in some dimension so good heuristic is to take the data set take the dimensions that has maximum spread so in this case one thing you can do is you can split the data in half here along this axis that say there's a few more points here and essentially what you're doing by doing this you're building a tree so initially you have one node that contains the entire data set and then you split it in half so you have the right subtree and the left subtree and then you recursively call the function again and you stop the moment you only have some number n points left so in this case for example we could split half here and approximately in half here right so this here is basically let's be clear this box here this one this box here is 2 this is 1 and this is 2 and then you split again into two sub trees where this here is 2 1 this is 2 2 and this is 1 1 1 2 1 1 1 2 2 1 2 2 and you stop the moment the number of points in your that you still have is smaller than some threshold that goes down very very quickly because you keep having it right so you only need lock - that's the least reason ever all that deep ok so how do you now do nearest neighbor search let me just walk through the algorithm so there are written to construct this should be clear right I take my dataset I find son dimensional with a lot of spreads I just compute the median in the dimension say everything that's larger than the median goes down the right subtree everything that's smaller than the meeting goes down the left subtree and then I call a function again on those two subsets and they give me new stats there's just a recursive okay so how do you now do and that's say you have a test point how do you find the nearest neighbor assume you have a test point that's here okay so this here's my test point I would like to find the nearest neighbor the first thing I do is I descend the tree so I look at the top node that's this split here and my left or right of that split turns out I'm left so I go down this now now this node looks at this split yes and my brother below this threshold and below so I end up in box 2.2 oh sorry so here you go I end up in this box 2.2 okay so this is where my test point is now I find the nearest neighbor in that set what's the nearest name in the set it's this point here right so I compute the distance to all these guys this guy's my nearest neighbor I conclude this many years neighbor and now here's the crucial thing I store this distance and I draw a circle around it but you're a circle around it here's my circle and say I know one thing right I know now I don't know what my nearest neighbor is yet because I haven't looked at all the data points but I know it's within that radius so this point this I've now exhaustively looked at I haven't looked at these guys yet so I have to make sure that there's no point closer in these sets and these leaves so here's what I do once I've found this nearest neighbor I go one up so going one up is again this split so I'm back at this split and now I lady say is this wall so far away that I can rule it out essentially what I'm saying is that's the circle intersect this wall okay and if it doesn't then I know that the nearest neighbor can't be in here because I know one nearest neighbor has to be in this little ball and that's you know that's on this side of this wall so I can rule out the entire this entire box which is equivalent to ruling out this entire subtree okay good so this is I have to rule them all out or you have to you know have to kind of kick off every one of these leaves then I'm done so now I go one up that's corresponds to this split here and again I say that's the splitting line intersect with my ball right and it does so I can't rule it out so here's an intersection right so I can't rule out there's no point behind kind of hiding behind the wall right I don't see it so I have to descend into this tree so I go down here and now I'm at this node when I look at this node what does that correspond to that corresponds to this this and again I'm looking into and saying well there's two sides of this wall right the upper one does that does that is that you know within the radius and turns out it's not so I can rule it ruin it out that's one point one this one here well actually sorry let me let me let me just this was a little bit too fast that's not how you do in fact so what you really do is the following now you descend this tree so I you go to the right you look at this this but you have to go down here because you can't put it out then you go all the way to the bottom so you say on this split which but where do I lie I lie below the split right so this can imagine this this wall goes all the way here so I'm here I have to go down the left tree I go down to this thing now I say which way do you have to go I'd have to go here do have to go here I have to go down here because I'm below this this wall so I'm a lower side I don't think about this this box here so now I go in here and now I have to find the nearest neighbor here I do a search and I find this man your neighbor so now I update my bomb this is my new ball okay so I look at this guy and I do for look for find the nearest neighbor here also found the nearest neighbor now I descend back up so this is done this guy is the one you have looked at it I go one up this is this wall here this wall here does not touch this new wall so I can rule it out I ruled it out at this point I've now checked off every single leaf and I'm done any questions about this algorithm for those computer scientists in you this is just DFS there's nothing else in depth-first search yeah let's pruning okay good so good questions or what if the question is what if I have to find K nearest neighbors what do I do right and actually nothing changes the only difference is that basically in this case I would you know initially I was in this here right my first church was in this box here let's say I find the two nearest neighbors this is my first nearest neighbor this is my second nearest neighbor what I do is I draw a circle around the second alright so I can prune everything if you know that it's further away than the current best case nearest neighbor you know there can't be anything closer right so that's essentially what it is very good question yes in the worst case scenario you have to go through and search every can't rule out anything and then you have to search every single point and if I could slow up right because you always have to descend the tree it could be the distance can anyone tell me when you reach that worst-case scenario yeah so that's one option you know if I put it right here or something right sneaky little fella right then I have to kind of search for a lot of stuff that seems unlikely when is there some kinda scenario where you will always get the worst case yeah sorry de Gozen okay good and yeah so if you with well k cosine is trivial right so we just returned the entire dataset all right so you don't have to run the algorithm yeah but you're right if K gets large then these balls get larger and you can prune less that's Beth Lee true yeah I think you're getting in the right direction you saying when the points are close to the boundary when a points always close to the boundary and when does it happen when is it that all the points are really far away from each other when D is very high exactly right so in the high dimensional space but this is no longer free accident there's actually always but every point is like this right you're always like in two dimensions it's great right because these two boxes only kind of touch each other here but high dimensional space every box touches every other box and every point is that the boundary of the boxes so you actually you know it would be extremely unlikely to be the interior of a box you never have that action so Katie trees don't work at all in high dimensional spaces they're completely useless all right they really work in very low dimension spaces two or three dimensional space yeah maybe lower than ten yeah and so I said again oh I see I see good so he's raising good point he said like wait a second but you told us back then and we did and I'm paraphrasing what you're saying they told us back then maybe that the curse of dimensionality right that's really only an issue when it's uniformly distributed but if it actually has some intrinsic structure right if it lies on a Riemannian manifold or something whether it's curled up in the space and that's not a big deal right well actually it is if the data lies in some low dimensional manifold on some low dimensional manifold this is my surface right I think this kind of a right this is a two-dimensional surface that's embedded in that thousand dimensional space right but this is low dimensional k-nearest neighbors still works because you know because nearest neighbors are actually truly close because they're confined to this low dimensional space right so the parameters across the dimensionality is that nearest neighbors are just not close anymore they're not closer than other points that's the problem right if you know high dimensional space you your nearest neighbor is almost the same distance as the point that's furthest away from you entire data set right so the nearest neighbor I wouldn't lose its meaning points are no longer similar if they are confined to a low dimensional structure then you actually have locality and K nearest neighbors works but KT trees do not work anymore and the reason is because you have these access aligned splits and this can be curled up in some very high dimensional space and touches every single dimension and so that's a you know you know million dimensional space you actually have to cut on all these million dimensions and you get really bizarre trees that Bay's the only cut off one point at a time what's the spiritual is not the meeting dare that won't help you much so the meeting is not the problem that access the line is the problem yeah that would be great if you figure it out submitted as your PhD thesis and happy had you were granted to you it's a very hard problem I learn in these manifolds so as I spent actually a whole decade trying to do this actually don't be curious to hear thoughts on it if you have some good ideas we can publish it together but yeah so this is very very hard because these these manifolds you know basically there's many many possibilities and freedom how they can be cured up in this high dimensional space and yeah this is very challenging okay good yeah sorry one last question maybe last questions yeah so you could project into a low dimensional space very good idea right so you could just take us very high dimensional data so just take two random directions project everything on to the man I'm at random these two dimensions right and then I search in this two dimensional space now it's really really fast the problem is when you project right and everything all moves close right so you know if something is closed in the original space they will still be closed after the projection because they can't move away right so think about this room that your heads be the data points let's say I'm projecting or to some of them you know the wall here right you will still have two neighboring people will still be neighboring on that wall but what he will also get is people that here in the front row will be very close to the people in the background right so you can't trust you nearest neighbors anymore you have a nearest neighbor you know that you know you know in the original space nearest neighbors will still be close in the projected space but if you find a nearest neighbor in the projected space they could be really far away along some dimension that you projected out and that would actually happen in the real case yes last question but that's right that's right actually you can't have holes that's fine right if you have a hole here and you have no points and here that that's okay right the important thing is because you're constrained to this low dimensional space your data will be dense basically right in a high dimensional space they does never dance because there's so many degrees of freedom you know the number of points you need increases exponentially but here the number of points that you need to have a dense sampling only increases with the dimensionality of this low dimensional manifold and that's that's something you can handle oh okay but very very last question yeah oh I see I see yeah yeah yeah so good questions the question is why don't we just you know save all this point you kind of ought be in both boxes right that's essentially what you're saying yes some spillover people have explored this actually it's called spill trees they are slightly better than a slightly better guarantees but ultimately does not change the asymptotic behavior it's still in the worst case you still have to search everything okay good so KD trees are great for low dimensional space in there particularly you know there's an area of computer science with extremely popular that's computer graphics because computer graphics basically what you do is you have some polygons etc you have some you know whatever's a little does the figure that you're trying to animate and actually also the other thing is computer games and everything is in a three-dimensional space and you always want to know how you know you know which points are close to other points so they use KD trees a lot in high dimension spaces they are less useful but we will make them useful in a few minutes so don't ride them off quite yet and let me just quickly tell you about a fix that you can use in these manifold setting and that's called ball trees so if your data actually is like this so ABC lies on a low dimensional manifold and then there's actually a data structure that is much much more efficient effective than KD trees and that's called ball trees and ball trees are essentially the same thing the only thing that they are relaxing is the constraint that you're only split along axis so KD trees only spit on AK season that makes it very very fast it's very fast to compute the distance to an axis that's just you just take two numbers and take their absolute difference right that's one cycle right that this is in the order of nanoseconds that's super super super super fast so if you're in a low dimensional space that's great if you're in a slightly higher dimensional space or you have some intrinsically no what I mention space but are embedded in the high dimensional space that's terrible idea so what do you do is instead you take your data set and you don't build boxes you build big balls here's what you do you have your data set and what you do is you find a direction in the space that has a lot of spread it doesn't have to be access aligned and the details on the pair in the handout I would just conceptually describe it to you so you want to find a direction that has a lot of spread in this in this in this potentially very high dimensional space all right so what do you do that's a very good heuristic so one thing is you could use principal component analysis or something for those who know this take the leading eigenvector of the covariance matrix etc that's all overkill you want to have something that's really fast so here's a trick you want to find a direction where you know there's a lot of spread so here's what you do you pick a random point let's say this one doesn't matter then you find the point that's farthest away well that's this one right and then you find the point that's farthest away from that one that's this one and now we connect these two and that will almost with a very high probability give you a direction with a lot of spread there's no approximation of PCA essentially right of principal component answers and so it just needs to distance searches with respect to one point that's a vectorized operation is very fast right so one more time you pick a point at random you find the point that's farthest away and now you find the point that's farthest away from that one and what you tend to get you can always say and get two points that are really far away from each other and then now you have a direction and so now in some sense you want to split along that direction and here's what you do you project everything onto that direction that's simple right there is some some vector W is this different so you just compute W transpose X for every single point it gives you the value where you are in this direction and then you split again by the median and now comes the trick so you have all these points on the left and all the points on the right okay raise your hand if you're still with me okay and now here's what you do you want to find some structure that actually contains all of these points some structures contains all these points and what you do is you just make a gigantic she just compute the mean point that's here and you can beat the radius distance to the furthest point and you know now this sphere contains all of these points do the same thing on the right what's the mean point maybe this thing what's the distance to the furthest point and now you get no well there's the horrible oh my gosh there's an easter egg just try to be in season all right and this is now this is now what you get and now you build the same thing you build a tree that Daisy has here some Direction W and you baby say along that direction I cut off at some specialty if you're at the right side you here at left side you're here well trying to be actually one thing you can do is you can basically typically what you stores you just store the mean and the radius of these two trees and now you do the same thing again right so here you basically you know these points the other direction and you have to two sub balls right you know spheres again and what you get is a tree of these balls and then doing test time you have to do do exactly the same thing right beforehand we basically had a point here and we said this is my nearest neighbor so far I want to rule out that I'm lying behind this no they don't have to search behind this wall now it's exactly the same thing except this wall is no longer wall in fact it's a big sphere so you say well what's the distance to the sphere if the distance to the fear is already further than my current best nearest neighbor then I could just ride off everything that's inside that sphere right the nice thing about this is this is completely invariant to how many dimensions the space has right if you take a data set like this and you add a few more million dimensions of noise it doesn't make any difference the data structure is exactly the same right oh you yeah dimensions and then you rotate the whole thing around the data structure is not affected by this right because you just these these these spheres can be easily be sent that anywhere and what you will see with this data structure like this it's initially of direction like here you get a big ball on the left and on the right and then you divide these again now you get a speed like this you get a ball here and then you divide this again when you essentially have is all these little spheres that capture exactly your data structure any questions about ball trees yeah ah good question the balls with the sect yes that's fine so here actually you store the data points in the leaves so every data point is exactly and only one of them okay if you have a test point and you fall here in the middle then Bayes either you go into one arbitrary one typically what we do is we go into the one that has the closer center you just descend the tree into here and then you just do as normal I mean doesn't actually make any difference that you're in here right the only thing is you you can't really rule out that part because you're inside of it right so basically you know if you're inside the ball then you can't set you can never prune it that's all this yeah okay any more questions okay have a little demo actually wasted several months of my life trying to generalize balls into eggs and I just thought oh this because I drew that egg so actually I thought he could be generalized as ellipsoids and I called it egg trees and had these great jokes about eggs and Easter eggs and egg hunt and all this stuff but ultimately it's not faster because the the distance to it ellipsoid actually is computationally so expensive that it's just much faster just to have two more levels of a ball tree all right okay so one thing I want to show you is first of all tree vision visualized okay good yeah this here's my data set oh my gosh there's like a can you see the green dots inside the if you do you have good vision okay so this is my data set actually let me just trace it so some of these dots are pink some of them are green if you only see the green ones you should go see your doctor sir so here's the data set follows the spiral here and the top node is basically a gigantic ball that contains the entire data set right special just take the mean of the data and take the massive you know they're just the radius to the farthest point of a most distant point and that contains the entire dataset now III divide this into right and this is what I get two more balls right and they have huge overlap here right you can see this and then I you know I keep recursing so each one of these balls now I find two more balls right and I keep doing this and keep doing this and keep doing this and what you see now is these little balls right they really trace the data said very very nicely right and now so I keep doing this until I busy have some stopping criteria and this is my final data structure this is really a tree right to the top ball as the top node and then I you know you know you always have like two to stop balls and now if I want to do a nearest neighbor search this here's my test point the blue point is my test point the first thing I do is I look at you which one of these two two spheres do I fall into a pond to the green one so I descend into a you know always descent now so now again have to suppose I fall into this one into this one you know you see what's going on going down the tree you always have two bolts right or left and I choose one of them okay and that's the one where I'm closer to the meat that's the one I'm picking and till then I'm in the file ball and here do a nearest neighbor search i compute the distance to these these guys and I find my nearest neighbor and now I go to the you know go one up and I can actually rule out all these balls right because they're all each one of them is already further away then they're my nearest neighbor so I'm done okay so actually the number of distance computations had to make two data points is three I only have to compute three distances and add the computer few distances to the means but that's you know maybe five or something depends how deep the tree was right so in terms of computation right it's a massive saving let me do one more so here's another one this also actually this is it's a it's very very close so maybe you know and maybe there's nothing I don't know what just happened um wait do you still season now sorry one more time okay here we go oh these are all very close so it's essentially you see what's happening is I'm basically sending then I compute the nearest neighbors and then I go back out and all the red points I could rule out right away without actually any more distance computations right that's a very very effective way to do the nearest neighbor search yeah it solves the problem for high dimensional data if your data is intrinsically low dimensional that that's really so the nice thing about trees is the ambient dimensionality and the space of the ambience so dimensionality of the ambient space does not affect your data structure I can actually I have that plot for you to here's a ball here's a demo where I take a data set of digits and I registered sorry I don't know why this oh here we go so what you see here on the right is what I'm doing here is I'm increasing the name dimensionality of my data and I'm showing you here how many distances I have to compute right and you see a very low dimensional space these ball juices you know I have to compute like hardly any distances at all this is the power of ten to the seventh there's a pretty large data set right and but as my dimensionality increases you can see I have to have to compute more and more distances right so that the savings go down if you look at the time actually here's something funny so here the number of dimensionality busy tells me I it takes me longer and longer and after 40 dimensions I'm actually I become slower than the brute-force attacks so the blue line here is what if I just did a brute-force attack and just compute the distance with everything and so it's not actually all that much affected by by dimensionality and so as I hit 40 dimensions that 40 dimensions I still only compute a fraction like about half the number of distances yet I'm actually slower can anyone tell me why what's happening so one more time did you understand what I'm doing yeah all right so what this graph tells me is how many distances they have to compute two other data points right actually and then to within the poetry structure so if I increase the dimensionality of my data set right if I do a nearest neighbor search I have to compute more and more distances and this is because I can prune fewer treats right that's the curse of dimensionality starting to bite me and the stateís structure I don't get much saving anymore but even at you know sixty dimensions I still only compute roughly half of the distances yet if I look at the wall clock time how long it takes me I'm actually slower all right does this here's how long the redline is how long it takes me and actually I'm actually slower than the blue line the blue line is brute force but if I just compute the distance from every point to every other point so how can the speed but compute the distance from every point every other point are actually faster than if I just compute half the distances any suggestions yeah the back checking takes too much time actually backtracking takes very little time very very little time but that's but it's a good suggestion yeah anybody else yeah that's exactly right so it depends on the you know it's because modern computers are designed for actually just two things cache performance and number two parallelism right so matrix multiplication you can actually compute the distance as you've seen in project one if I compute the distance of a point to many many other points I can actually you know this base is just a matrix vector operations which are highly optimized right and can be done parallel and these vector vector units on your chip and so you can exploit this very effectively whereas if you do one distance at a time you cannot exploit it and so as computers get you know with modern computers this intersection point moves down rapidly actually but GPUs and somewhere here right so GPUs allow you very very fast parallel computation and there's little benefit for from ball tricks what do you effectively do is you just make the the ball size the fleece is much much bigger that's essentially what you're doing right just means now you can search if you have hundreds of millions points and each sleeve actually has maybe a hundred thousand points something like this ok any last questions about ball trees yeah do you want to partition the space and - I mean that's essentially what these are doing though so spheres are nice because the distance to our sphere is very high fast to compute and that's why you want to use spheres or boxes right and that that's really the motivation I mean otherwise we could do you know I'd be known now for my egg trees but it you know it didn't it wasn't possible right because like other data structures is much more expensive the distance to a sphere is really easy right let's compute the distance of this point to the center all right you've tracked the radius right so that's really really fast all right any last questions okay good stuff so well there was a reason actually I want to talk about this and it's the next topic oh shoot does someone have two copies I gave my own copy away wait wait before you pass them on no no no no no I have to intercept okay thank you okay well we're gonna cover now is were the oldest machine learning algorithms but it's actually it was written off as pretty bad and kind of didn't get much of the attention anymore but then actually had a major comeback because it turns out actually so we talked about its decision trees and decision trees are not very good algorithms they really are not but if you understand bias-variance tradeoff then you can make them amazing algorithms and that's what we will talk about now so let me briefly introduce decision trees and whatever do is I will start with KD trees so here's the idea one more time you have this KD tree well let me draw you know what you deserve a new Katie tree here's a new Katie - okay I assume you want to do one yearís neighbor classification you have two classes for now see if I put my in my OHS and my my exes and this is what my data set looks like and now I want to the nearest neighbor search well the first thing that people realize is that even with Katie trees things until too slow right so actually finding the exact nearest neighbor it takes a lot of time and most of the time you backtrack tracking but actually all this backtracking right it's just to make sure that you really really got the exact nearest neighbor right you probably get a pretty good nearest neighbor when you went down here and did your first search and then just make sure it could somewhere here behind a wall hide another nearest neighbor right and when you think about it it doesn't actually really matter who your nearest neighbor is right again nearest neighbor classification you just care about making a prediction right and if it has the same label it doesn't actually make any difference so the first thing people did is they did approximate nearest neighbor search and approximate nearest neighbor search is really simple you build a ball through you build a KD tree you have a test point you'll be down to a leaf you find the nearest neighbor in this in the sleep and that's it you don't do any background by backtracking and that's super fast all right that's ridiculously fast but you just have log and operations to go down to the leaf that's it's nothing right and then you do a few nearest neighbor searches in here right so now your nearest neighbor search is really really really really fast sometimes a little bit less accurate because you didn't actually correct correct maybe you were just on the boundary and you know maybe you were just behind a wall and turns out that you know right behind the wall a lot of points from the other cast but these are kind of paths cases so one more time you just go down that leaf you find the nearest neighbor in that leap you return that nearest name it's let's approximate nearest neighbor search works great for nearest neighbor classification because again there you don't need the new your state the the exact nearest neighbor doesn't matter all right so that's good but you can make it even faster and here's how well imagine to go down this tree right and now you're in this box and now I computing the nearest neighbor well I claim I don't actually have to do it why not any ideas okay one more time the question is you go down you don't on it into these leaves and I claim actually that in three of these four leaves you don't have to find the nearest neighbor I'll give you a minute try to figure it out with your neighbor or is it too obvious that people just shy like okay well if someone who's not Arthur raises hands as the right thing then I yeah sorry you do you know that say you know what the box is and why do you not have to find the nearest name it yeah they're all access that's right right so as you construct the tree right you just say way by the way these are all exits right so if you ever fallen they're the nearest neighbor will be an X right don't bother finding it right this is all XS right there's all sorry there's all alls there's all those in here in this one here there's three o's and two four five x XS okay so if you get near approximate nearest neighbor search to fall in here you just say all right you don't even have to do it and that's even faster and it's even better because you don't have to store the data all right you just throw the tree you just throw this label at the bottom and you don't really have to store anything else right so now suddenly your data set to I agree with them beforehand you needed d x and storage here to store the entire train either set and that data structure on top now just have to store the data structure right and the data structure takes absolutely no stores are tiny and absolutely tiny so that's not a big deal at all okay the last question is what if he falling here right this is the box where you have three o's and two four five X's right here you would have to do a nearest neighbor search but one thing we can just do is we can say well actually they can skip it let's just store the following and say what the probability of an X right is 5 over 8 and the probability of an O is 3 or 8 and we just pick a nearest neighbor at random and if you do this then we can also throw away the data set all we need to store is how many X's how many always they are right so maybe here we save you're dead certain and here we say we uncertain right but we know exactly how we answered let me know that actually three out of eight times it's an it's an O and five out of eight times its next yeah very good question so okay wait before I get into this question any does this make sense raise yeah no that makes sense you wouldn't okay some people seem confused raizy Ana that doesn't make sense that's alright so one more time I build a solution to you to find the nearest Katie tree to find the nearest neighbor what I do doing test time I go down to the leaf you know compute the nearest neighbors the first you know go to one leaf find the nearest neighbors then I backtrack you know and check every single leaf if they are you know the nearest neighbors in there I could prune some the first approximation we made as we said okay well all you do is you just go into the first leaf that you fall into and you find the nearest neighbor there that's a lot faster the second approximation say well we don't even find the nearest neighbor we just output like if these are all the same label that we don't actually have to find the nearest neighbor we just have to say what that label is because all the nearest neighbors have the same label and we know what you're going to use that label for for nearest neighbor classification so doesn't make any difference if you fall in such a leap where the actually is mixed between X's and O's then you would have to do a nearest neighbor search but we say well don't bother all we doing is we just say you know in this case there's 8 points of the training set fall into this three of them are OHS five of them are exits so we just give you that information we say well we are uncertain about the label but there's the 3/8 probability isn't oh and five-eighths poverty's an X now the question that was just asked was well if we know we're doing this couldn't we just keep splitting until this could be for example split this one more time right and you keep splitting and splitting and splitting until every leaf is pure first question is that always possible right can I always take my data set and keep splitting until every single leaf is pure if yes why if not why not or if it's possible under what conditions is it possible I'll give you a minute in your name I try to figure out the proof one way or another either come up with a counter example or prove that it's always possible [Music] all right who has an answer raise your hands can you always keep splitting until every single leaf is pure yes or no to three anybody else okay quickly have a boat who says yes raise your hands who says No raise your hands it's tied all right so first argument sure answer yes or no can you always do it or not yeah that's exactly right so the answer's no because he could have two points that have exactly the same features that have two different labels sike yes that is kind of you know it's a pathological case but it's absolutely right so that's the counter example right you cannot do it but if you rule out that case can you now always do it yes all right the answer is yes right so um and that's exactly the official answer by the way that's absolutely correct so you know as long as the moment you have two points that are exactly identical they will always end up in the same leaf there's nothing you can do about it if you don't have that case then you can always split them what you can do is if you the proof is very simple that'd be easy it's safe you know for back count you basically prove it by contradiction you say assume it's not possible then you build up a tree and you end up with a leaf that has two points that have different labels these two points can't be the same because that'd be the counter example you just mentioned we ruled that out therefore there must be different in one feature at least so what we can do we can just take the median you know it's this difference between these two and spit on the median and now we could split again and now they're different leaves so that's a contradiction right so it must always be possible okay very nice it's a question no yeah so okay good questions and what if that case exists so aids easy to check and basically you have to implement this in your project you have to make sure that doesn't corrupt you that's also a very simple way of making sure this never happens any idea how to make sure they can't happen yeah yeah either one that's exactly right you just add a little bit of Gaussian noise right it's basically a ten to the minus nine right so it doesn't make any difference but it's optimum that the computer can tell the difference between them if you add a little noise to every single training point now two points will never be exactly identical i that's probability zero essentially and then then it will always work out yeah so if you spit on the meeting you always say the median either goes to the viral to the left so you just make that as you just say greater equal to the right or less than to the left yeah and I have some little thing that I want to do but make you know it given us to 15 maybe we start next lecture with my slides 
","['', 'KD trees', 'nearest neighbor search', 'data set', 'split the data in half', 'dimension', 'recursive function', 'threshold', 'nearest neighbor', 'circle', 'ball', 'dimensionality', 'curse of dimensionality', 'brute-force attack', 'distance', 'cache performance', 'parallelism', 'matrix multiplication', 'vector vector units', 'ball trees', '']"
"all right so last time yesterday somebody went is it it's some event today why is it so sparse they're bees sorry oh yeah I got to pet it go on my way here they're goats on the lawn okay well I can't compete with go it's can I so where are the goats okay all right all right so last time we talked about Katie trees and in particular we thought well the first thing we start out with sorry one second where's my chalk so we built this this isn't tree like repeatedly splits the data in half and so we save all of you use this through nearest neighbor classification then one thing you could do is you know could actually it doesn't really matter if you have the exact nearest neighbor as long as you have it approximately because ultimately you're not interested in what exactly is your nearest neighbor you're interested in what is the label of the nearest neighbors so if you get the you know one remove to something it doesn't probably doesn't make much of a difference so we could speed up the algorithm a lot if you just go down the tree and then just only do a nearest neighbor search in that leaf and that was the first approximation that's called approximate nearest neighbors then he said well wait a second if we do this right if I only search for nearest neighbors in here well imagine we have the scenario that all the points in here actually acrosses then I don't have to do a nearest neighbor search because the only thing I'm interested in is what is the label of my nearest neighbor well I know the neighbor of my nearest neighbor inside this box without even finding the nearest neighbor it's a cross is positive and so one thing you just do is instead of actually doing the nearest neighbor search it just without associate a label with each one of these right something like this and that's essentially where decision trees start so I just have three slides you know decision trees and just to get you guys started so I want to go back to the problem that we've seen before it's a very important problem real world it's is your date good or evil psychopath and the Train indeed I said once again our superheroes so we have good superheroes that are all psychopaths and bad superheroes were also psychopaths and the video of my feature set I think I got a right this time I think last time someone fun a mistake but I think so we have the features do you have wear a mask do you have a cape you do you wear tied yours as I remove the feature if you wear your underpants outside your pants ever and so the first three are good that next to your evil and these here are my test cases so these fear I'm not sure about Batgirl the Riddler and this is your date and you would like to know is he or she evil and instead of naivebayes this time what I would like to do is put a decision tree and here is a decision tree so I basically the first thing I say does the person smoke yes or no and if the person smokes then I say do you have years well years I mean pointy years I guess and most people have years and if I say if the person does not smoke then it comes down to this person may a mask if no then while people who don't smoke and don't wear a mask are usually evil but if the person wears a mask that depends was the height greater than one at 75 that's not that's meter a centimeters I'm that's my European upbringing okay and so we can now test this first question is this is a good tree would it misclassify anybody so maybe just take a look and you and your neighbors see if I can if this is consistent that means do I get everybody right and feel free discuss it with your neighbor I give you a minute [Music] all right any complaints yeah I'll Fred that's right poor Alfred Wright's always be the loyal friend and we say is evil let's just test this so we put all the people in there we check who smokes and who doesn't smoke so if we do this turns out only penguin actually smokes then we check next level I think that's just the penguin has he has point there's not a point eight years so he's evil and then we have all these other guys who wears a mask who doesn't wear a mask turns out Joker and Alfred don't wear a mask all right so here we have a Miss classification Alfred actually is as good but evil now we have the remaining ones that I think Batman Robin and the Catwoman and turns out the cap was all smaller so she's evil but we get off at wrong all right so of course what we showed last time is you can actually always get a consistent treat right that's important you could split here again get any idea how we can split again to make this correct any idea tie right for example right that's right so the tie then will be if you wear a tie then it would be good otherwise we'll be evil and then you spit again I told and we derived as last time as long as we don't have two identical points who have different labels you can always keep splitting until you get everything right all right now it turns out actually in practice you know you can do this you can keep splitting and you can make it really really complicated to you that always gives you zero percent resting error but turns out that's actually not typically what you want to do any idea why you may not want to do this yeah it leads to overfitting that's right but so if you basically essentially you know split until basically every single point is right you have millions of data points you know what what you essentially do is you just memorize your data set right so that's the danger and what I will show you in a minute in a few minutes is that decision trees have a very there's a very tight balance between bias and variance so can anyone tell me when do we have high bias when you have high variance as a function of the depth of the tree yeah that's right low depth means high bias that's right so high depth means high variance okay so now I will show you a demo in the in a minute to see this on a real data set so the promise if you think I said is you know if you're if you're a tree is too small then you probably even have a high error on your trained ear said that's a high bias scenario if it's too deep you get you trained SL right but I've seen a test that you will have a large error and so what you want to do in practice is you would like to find the tree the smallest tree that gets your trained ear said all right all right that'd be the perfect you know that's it's kind of the goal of the decision tree algorithm and turns out that's an np-hard problem so who has heard of np-hard okay awesome good so np-hard is really hard takes a long time so it just means basically if you did I said gets larger the amount of extra computation time I have to do to accommodate the extra data grows exponentially and everything that goes exponentially you should you know scare you all right so basically very Sunni whatever data said that you know will force you to Train longer than you know the time we have left in our solar system or something right so that's not a feasibility that's infeasible but turns out there's many good heuristics to find trees that are actually small and and work well so hey one second let me just oh sorry okay so this we did last time so one quick quiz maybe give you a minute you and your neighbor try to come up with the smallest tree that gets every single point right yeah did you know it already okay as it is a question yeah oh yeah you can split on the same feature the question is can you spit on the same feature multiple times and the answer is yes absolutely all right so I'll give you a minute try to figure out what's the smallest possible tree that gets every every data point right [Music] [Music] okay any suggestions who is a small tree yeah how many splits do you have to split that's pretty good yeah that that makes a Batman and Robin good and everybody else is still in limbo yeah and that cuts off Alfred's that's good and everybody else they for you that's right that's that's perfect that's great and I think that's the smallest tree you can actually get even I made the slides I think I didn't find that solution it can be hard all right I don't think you have animation here so that's actually that that was my tree but that's too but actually has three notes instead of two okay let me just show you how to do it in practice how do actually how to actually build that how to build these trees all right in the way it works is it's actually very similar to Katy tree usually we just there's many different variants of decision trees and by the way one thing I just want to say is decision tree is a horrible machine learning algorithms they really suck right they're absolutely terrible however they have such clear bias variance problems that it's very very easy to address those problems and if you address the variance with bagging and if you address the bias with boosting and they become amazing algorithms right so don't be disappointed yet today be doing decision trees very soon will be bagging and boosting and these are busy basically methods to you know compensate for high bias or high variance and then they become very very good algorithm so the fact that you're probably using it every single day so search engine for example I'll just boost the decision trees right that's typically what people use and also random forests which is a bagged trees are one of the most one of my favorite machine learning algorithms they are very very very strong so a single tree is not very powerful but once we correct them and that you know we will do this in the next couple lectures that become amazing classifiers okay so once again we have our data set and we have n data points and just as before what we want to do is a data structure where we split our data set on some specific them dimension if some dimension you know xt xt is greater than some threshold go to the right if it's less than a threshold good left but with katie trees what we try to do is we try to split the data exactly in half to get you know to basically get petitioning get a balanced tree now we have a different objective what we really want is we want to have leaves that are pure right such that every single point in the leaf has the same label and the reason make sure that you know the reasons obvious because once we've done this then we know of a test point goes in there but very high probability that's just that like the same leg okay and so what we need is impurity functions an impurity function let me just write this the purity functions are functions that measure how pure a set is in terms of for example the label okay and so what you want is that basically you know you want to have high purity that means every single point in that data set has the same label the first impurity that you know I just want to mention I think less commonly used but it's still you know quite popular is the Gini impurity and the Jimmy impurity I don't know if you guys have heard of the Jimmy index so the Gini index is something very similar it's an index that people compute to measure for example how uneven as a society how uneven as the income distribution in a certain country that's not the same thing as the Gini index as as always a Gini impurity so this is invented by the same mathematician and his name was Gini and he had the Gini coefficient engineer purely okay the idea is very very simple the idea is you basically say for a set of data points let's call this my set it's might be this could be the set in a certain leaf so I've n data points and I would like to know how pure is the set of data points and what I say is well first a computer probability of getting a certain labels let's say half you know K different labels of capital K different labels then I say well the probability of picking a certain label is like you know I simulate if I just pick one point at random what's the probability that I would pick particularly and so I can estimate that with maximum likelihood estimation and the probability for a label K is just the subset the number of points with that label divided by the total set its where now SK SK is defined it the subset X comma Y s such that y equals cake okay so one more time this is my total data set for which I want to compute the impurity let's say F capital K possible labels so what I do is for each possible label I define a set SK that it's just the contains just the data points with this particular label okay so s is the union of s 1 Union Union s capital K alright does that make sense and these there's no intersection between any two of these subsets and then I say the probability that I pick a point with a particular label is just the number of points off that label divided by the total so this is this should be pretty clear AZ Hana that makes sense okay all right good so in this chaos basically if I just say you know what's the probability of picking undergrads as they have undergrad and grad students so you know I just petition the class I say the probability of picking undergrad is the number of undergrads divided by the total number in the class and then I say the impurity is the off the set F it's very simple it's Tom over all my 1 to capital K or I guess I call it C in this and what I say that PK times 1 minus P so I go over every single class and I multiply the probability of picking someone of that of that class times probability of not picking someone and when you think about this a little bit like say if you have you know the two dimensional case with two classes right Alyssia is exactly the probability of picking class 1 times probability of picking class 2 and that's just the function of you just make that a function of probability of class 1 and and the Gini index looks something like this G of the probability P of 1 so one more time so if I have just two classes this year becomes P times one minus P plus one minus P times P that's obviously two times P times one minus P okay so this is my P that's my one minus P and the other class of it has exactly the opposite this here's my Gini index and if I make that a function of P then of course if P is zero this is zero the Gini index is zero so my member jeanja it takes is very very low if it's one it's very very low and point five is really really high okay so there's a measure of impurities or what I want I want this to be love okay so in P at the highest value basically ever even split between one class and the other class and what I want to get I want to get a case where either have just one class or oh that's the other class so I'm basically starting out with somewhere here and every time I'm splitting out I'm trying to raise the improve my score to go down raise e Anna that makes sense okay any questions about the Gini coefficient yeah sorry D index yeah yeah so I guess he has a function of P but P is defined to s yes yeah yeah so you the reason I didn't draw it from no classes because you can't really draw from glasses so it becomes a little more high dimensional I guess because you have more than one parameter but if you have two classes and I can say the probability of the first class that that's that's P and the probability of the other class is there for the determined if you had three classes then I have the probability of the first class the probability of the second class and the third class is 1 minus the sum of those two right so then becomes a three dimensional plot and not good at drawing up but I mean it looks very similar but it's nak summize when they are all the same any more questions so the important thing is an impurity function all it does it measures you ever set it measures how pure is or how impure is that set okay and so it should have the highest value in basically all the classes are you know equally common and has a low a low value when it's dominated by one class okay another one is the entropy and let me just quickly define to you what the entropy scores has anyone heard of information entropy in the information sense okay awesome oh I bet you've all heard there was in the placement exam there was a trick question all right and the entropy that's the following Daisy says what do we want right actually what do we not want and what we don't want is leaves where every single class is common right that's that that's the case we don't want we want to split such that every single leaf that's just you know just one class now the worst possible distribution within a leaf would be the following where p1 equals p2 equals equals P K equals 1 over okay okay so each one of the classes is equally likely okay does that make sense that'd be the worst possible outcome I do you have a terrible decision tree all right you go down the leaf and then the leave you know all the classes are equally likely right so you really don't know what the class of your test point is any questions about this this here is this is the enemy we don't want this so what we're trying to do is try to find a different distribution that as far as away as possible from this distribution and one way to measure distance and distribution is the K our divergence and this is exactly by a place that's on the placement exams if you just close your eyes for a second and remember the questions on the placement exam there we actually you know defined the KL divergence and the KO divergence is basically tells you how different two distributions are and the KO divergence is defined as follows the KL divergence between the distribution P and Q is the following k equals 1 2 okay we sum over all the different classes and it's PK times log p k / qk actually let me let me call the CQ so the evil one is Q we'd like to find peas that are really far away from Q now think about this for a second okay so these are probabilities Q's the probabilities and for each one of these K events we have a probability then you know that it happens they sum to one obviously so what does the KO divergence do well if PK equals QK this year's one log of 1 is 0 right so then actually they're the same so that's good that's a good sanity check right and now it's a little less obvious that this KL divergence is always non-negative right so this also this thing can become negative right if Q K is large larger than pk but then actually this here's smaller negative values uh I have a small weight and that was actually what you play it proved in the placement exam that this cannot possibly be negative it's always a non-negative sir okay good so we want to actually maximize this term where Q is this year so Q is basically the distribution that we don't want we want to say let's assign P such that has a very large KL divergence from this horrible case Q for every class is equally likely and how do we do this well let's first plug in 1 over C here any questions at this point yeah okay so P so we want to find we build a tree okay and what I'm telling you right now is not yet how to build a tree what I'm telling you right now is how to evaluate if a tree is good or not that's basically a metric that you're trying to optimize that's a machine learning you always have a loss function that you're trying to optimize in that that one once you have such a loss function then you can basically make the tree to optimize this last module so right now just for a a particular leaf node you have you have a tree like this right and I would like to know for a certain leaf notice this basically did we build a good tree that we build a battery and one thing it's a bad tree if this leaf has points of all classes evenly right because then if you didn't achieve anything right that was the case but basically we have the Joker and Alfred here right so if you know your test point goes in here well here it's equally likely that you're good or bad a positive or negative so you haven't haven't actually achieved anything right what you would like to build a tree is that a tree that makes the all the leaves are here all positive these are all negative or something these are all positive and these are all negative right these leaves are all pure then when a test point comes into fours into a particular leaf then you know that's the label that I want to predict okay does that make sense and what I'm giving you now is a function where you stick in a leaf and all the points of their leaves that's the set s and it tells me how pure that leaf is it basically gives me a number of saying how good a job we did and if all the points have the same label then that is zero and if they all have different labels then it's larger let me try to minimize what we will do then it's eventually basically if a leaf raised he has zero loss right let me just stop and if it has large loss then we split again such that the two leaves that we create will each one have lower locks that's that's the algorithm exception any questions about a high-level view Arthur you look like you have a question but no okay all right so now we're doing this and so the first thing is we take this leaf with the sleep this is set s and out of that said s we now define probabilities if a Z say what's the probability if I would draw a data point randomly what's the probability that I get a certain class K so each leaf you give so it gives rise to p1 to pk and that's basically the fraction of points with that label inside this leaf and what I would like to do is that one of them is one and all the others are 0 all right that'd be great that basically means we have a pure leaf okay good and now I'm giving you these are my piece and now I want to get some score that basically tells me how good it is and then why do I need to score I know that I want this kind of thing but one thing I don't know you know if I for example have something where I have a distribution where p1 is this and p2 is this you know I'm p3 is this I don't know how good that is okay if that is better than something else and so that's what I need I need to have some some score that tells me how good is a certain you know partition how good is it certainly that's what this function will give us and the way we define it as we say if it's good if it's far away from this pathological case where every single early aka class is equally likely okay raise your hand if that all makes sense now all right okay good so now let me compute this and we say what this is of P to Q where Q is this pathological case we actually know what Q exactly is Q is 1 over 1 over capital K we can plug this in yeah there's one of a capital K for every single one and one thing we can do now is we can say well we can split this up this equals PK log P K so I'm just splitting this up is it's the sum of two different terms x PK log 1 sorry - so this should be K here right okay okay does that make sense going from here to here all I did is I took a fraction if you have a fractions at a log that becomes a minus and have another fraction that becomes another - so that becomes a plus last lock' raise your hand if you're still with me okay good now this here is a constant all right and to be a summing over all K now this is log K and sum of a PK times lock a lock a here so it's the capital K so this does not depend on my little ki should and so what I can do is I can pull this out just back up the envelope calculation so what I'm getting is that's the same thing as log of K times PK can anyone tell me what this is one okay cuz this one so we just get a lot cake all right perfect good we take this out very nice now the other thing is that we actually want to maximize this term right with respect to our piece this here is independent about these this is the constant we're always adding it doesn't make any difference so when we maximize this the score here that's equivalent to just maximizing this right because this has there's no no matter what what our peas are this actually is always the same you're just lifting the whole function so what we ultimately maximizing is the following the maximizing over P k PK log K and that's one last rule that is maximization is for losers so we minimize instead so we minimize the negative sum over K DK okay all right and that's the negative entropy oh did I'd like sorry this is the entropy sorry what I get it wrong you go to minimizing its oh yeah yeah yeah sorry okay I think there's a negative sign in my notes yeah okay good good good catch yes so entropy is maximized when everything is the same and what we want to do is you want to minimize the entropy right so we want to kind of done sense create order in the universe any questions about this and please please correct us there's a negative sign in front of the h of s that's actually wrong so the H of s actually is negative KPK log okay here we go any questions about entropy he turns out the entropy is also is actually very very important not just in decision trees it's information theory and as anyone taking information theory class very few okay it if you basically go into compression so one one you can view all of machine learning as just compression right so one of the first machine learning algorithms in some sense was a spam filter well you basically imagine you take all your spam email no you're not spam email you put the golden both in a zip file and then the new email comes in and you just either added you add it to both the fights to the spam zip file to the not spam zip file and you see which one increases by the least of a smallest amount and that's probably the class it is right it's because if it's spam it looks a lot like spam so it's much more compressible and so then you can actually use your comprende you spam compressor works better right so machine learning is actually very very similar to compression and also if you have a very good model of your data that actually you know you can use that for compression actually the way compression algorithms work essentially is you have some some data stream and what you're doing is you're saying given the past data I'm trying to predict the next bit of data we've I'm very very good at doing this then you know you basically just oh you have the same model I send some data to you you have the same model so you predict you know the next next bit of information the next byte of information if the prediction is right then I don't have to tell you anything I just have to correct it when it's wrong essentially so there's a little bit more to this but that's essentially how a compression algorithms work okay good so we now know what we want to do right we want to minimize the entropy of a set so you know Asia spaces the entropy of a set and what we would like to do is minimize this and for any given set we can't compute the entropy right it's very very simple this here is the charm okay by the way please keep pay good attention because you have to implement this in the next project which we post in a few days of course you knew you would have okay so for everything is that you just compute these these piece that's trivial right is just the number of point the ratio points of a certain class and then just you just compute this the certain term and okay good damn it what is this okay good so we're almost done the last thing is what is the entropy over a tree right so far we only have entropy over leaf but what if you leave this so that say if you have a data set you can compute there's my data set s I compute the entropy of s that's easy I can do this and now what I want to do is I want to say I want to make a split into s left and s right such that the entropy of these two is actually lower than the original entropy and how do i define as well it's quite simple I've a Z say the you know the entropy over such a split is the following I first took a point randomly of a cc either go left or right randomly and then I compute the entropy of this of this leaf and when I go left or right I basically pick the choice a to choose based on the number of points on the left side on the right side so let me just explain this one more time so PL is the number of points on the Left divided by the total PR is the number of points on the right by the total so this is the fraction of points on the left perfection point on the right and then just say the entropy of this tree H of this tree here it's just PL times the entropy off on the left hand side plus PR times the entropy on the right yep that's exactly right we take a weighted average of the two enterpise right so we take a set we split into two sets you compute the entropy of each one of them and let me take evaded average view where we weigh each set by how many in a proportional to how many points are in it yeah if what is if PK is zero oh and then this year is zero this CHUM's that guy all right let's just you yeah okay any more questions so what you want to do is you want to find you want to find the tree that actually has minimum entropy and here comes the algorithm how to do it so now basically what I did what I did I showed you what entropy is and should be is an impurity function that is lowest when basically a sets you know aspires to what's just one label and all the other label students are not in it its highest and all the labels are equally likely which is what we don't want so we start out with sisters that setting usually and then what we want to do is we want to reduce the entropy and we do this by repeatedly splitting the data did you take the data and we split it into two halves and then B basically now say okay now we compute the entropy on this level all right that's just debated average of these two leaves now here comes the questions okay so this is this way all you to do now comes the question how do you find the optimal split right and that's the empty hard problem but we can do really really well by doing being greedy and it's actually the algorithm is trivially simple so what we want to find is we want to find a particular split on a certain feature now this is maybe my data set let's say here mine my exes heroes alright I want to find the split that gives me that decreases my entropy by the mouse right so right now the entropy is pretty high let's say there's an equal number of X's and O's so right now the entropy is spaced as high as it gets but if I make one split right in that the right split of course is here then actually my entropy here is actually minimized and in this case I just have one little oh here so I would have to do a second split so cut this off okay and how do I find these splits and the answer is very very simple the answer is I try out every single one all right that's so but is what you do if you say okay why is that here right like this but here it's but here but here I could split here could split here you know I could here and so on and I try it for every single one I compute the entropy of the two leaves and then just take the one that is the lowest that's it and so the algorithm is trivial and you do this for everything that I mentioned you can either do it here I can do it in this dimension you could spit here here yeah and so on so for everything that I mentioned you try out every possible split and then you pick the one that's best yeah d squared times T squared no u n times T splits right so for everything that I mention your end splits why do you have n splits can anyone tell me where your friend spits that's right the only you only have to cut once between any two points so you look at all the gaps you cut between them that's right so you try n and splits try this for everything that I mentioned so you have n times D splits that you try oh I see that's a no of the short don't know it's not all of K operations you do it for every class so it's n times D times K that's not bad that's pretty good yeah right good questions those question is here you're assuming that you have continued a discrete classes you do a classification problem what if you have aggression problem right and regression this wouldn't work anymore and that's right very very good point and I didn't quite want to get there yet so then you just used for example the square loss or something III get to this in a minute yes you but you can extend actually in the project when you have primarily after implement is the regression version ok any more questions you have a question sorry I see by ok good question why are the splits always acts as a line why don't you make them diagonal right and people have of course done this and because decision trees were so popular back in the days people that every sing of air in every single single thing you can think of how you could split people have explored in practice axes the line splits work pretty well and they are very very fast to compute so typically does not pay off to have non anta's aligned spits yeah that's good question but but you know there's pathological cases in which you may want to have non access enhancements ok so well who wants to see a demo all right let's do it [Music] all right so here's my good old window I can add some data points so let me add some positives of any of these suggestions for what data said you want to see smiley face smiley face is always so ill-defined I don't know what positive and what's negative all right that's my part isn't oh I just made it past this happy okay so here's my positive and then I guess he gave him freckles or something or her okay so these are the negative points and we can now build a tree this what it does BAM all right and so basically it stops pretty quickly and one thing you see it's very interesting right so it doesn't actually care about these kind of this region here right so it doesn't have any points here so this is why by decision trees have problems if we don't account for bias and variance right so by busy tries to find the minimum number of splits and he can actually cut this whole region here off because it turns out there's all OHS but you guys of course know it was a smiley face so they're actually this year would be actually the excess region right but this is just what this is actually a very small trees only a couple of splits here and any other data sets and diagonal data set so what do you want oh I see okay so he's trying to be really really mean because he knows the algorithm is axis aligned is that I can show your ID three three all right here we go there's actually you me do this too often you get actually repetitive stress injury carpal tunnel or something okay good and here we go ah I told then once again you get a simple treat actually explains the data set very well right but one thing you can see here like different to an SVM or something I would actually cut here right down the middle it basically cuts down these these weirdal you know corners right you would that's clearly not as good as an SVM for example right but turns out actually decision trees have many many nice advantages that make it make it work really really well in practice and and one of those is for example that the features can have any scale right some ski just can be very large-scale or very small scale it doesn't this doesn't matter and in the next few lectures people yes even more maybe I'll show you one last thing is the here's the data set of that I drew from a Gaussian distribution so this is to the Gaussian distributions these are my positive points is my negative points and here's what I drawn out the if I just make a single split right it just allowed a single split and it splits down here and it basically tries to cut off the positive points and then so here what you see you on the top right is the training error on the testing or both pretty high and pretty close together so what kind of problem is this bias or variance its bias right so training and testing very small gap training is high that's a bias problem now we make the tree bigger and bigger right so they allow two splits or three splits in this case four let me keep going and so now you always see like it's kind of just something funky here right I was trying to get one X here right so it splits it down in the middle and in eventually even get every single point right right it's like trees you know they have there's no man left behind right that's the slogan and they keep splitting but one thing you see is here the training error keeps going down right merciless right keep splitting until you get every single point right but it no longer generalizes right and the testing error goes up and the the reason why decision trees are problematic is because this sweet spot right where you actually have the optimal it's really really hard to find like in some sense what you would like to have this reason to be large right but there's very little wiggle room and what we will do later on is kind of try to to make that wider okay so in some sense you can oh they all you can do is kind of try to for it a depth of 4 and that's the best thing you can do and then you're already overfitting all right we continue on Monday 
","['', 'decision tree', 'nearest neighbor', 'approximate nearest neighbor', 'label', 'psychopath', 'superhero', 'feature set', 'mask', 'cape', 'underpants', 'evil', 'good', 'test case', 'smoke', 'pointy ears', 'height', 'centimeter', 'meter', 'bias', '']"
"welcome everybody all right so two quick announcements of one thing is we will the caracal competition is done so we will I guess post this on Piazza now so these are optional for credit points so basically the way it works is you have to you can submit something like you know there's busy a dataset you get a training data set and you get the test points but not the test labels and then you have to train your classifier on the training data set and submits predictions for the test points and then there's a leaderboard and you basically move up on the leaderboard we submitted a bunch of strawman and if you beat these strongman's the more strawman you beat the better you get basically one thing is you're not allowed to infer the labels of the test point so you could actually these actually images I can tell you this right away is the images of handwritten letters are not have written machine written letters so you could just take the test points if you wanted to and look at each one of them and label up a hand I'll be really boring and you would get hundred percent correct but you actually have to submit the code at the end and you have to buy that one page right up if you want to get it for credit describing what you did yeah sorry can use any library yes you can use any library you want so this is an open you don't have to use bouquet or anything right you can use any algorithm you've on etc the only thing is you cannot use humans to infer that you know do not go online and search for the data set or something their data is actually something we created but you may find something else so you have to use that training data set and that tested any other questions yeah Wednesday I believe it's made tense I'm not entirely sure it's on the character competition webpage it's the last day that anything can be do yeah it's teamwork and I believe the team can be up to four people we specified it on the on the webpage yeah all right oh yeah last question yeah what other you have to do you have to scream everybody else please be quiet yes so yeah that's gonna be there's nothing with three people if that's what you're asking yeah I just try to you know increase my coverage okay any more questions all right okay the last time you know maybe next time I do pie you can have pie another people all right so the last time we talked about decision trees so the way we arrived there was we said we have a data set and these Thetas have many of different classes and beforehand the first thing we did is we did Katie trees to speed up nearest neighbor search and then we kind of realized well actually if you build a KD trees would be good if you have leaves that are all pure because in that case you know you don't actually have to do the nearest neighbor search anymore you can just look at we know what's the label in that leaf and then you just return that label now when we build a katie so decision trees are essentially the same thing as KD trees right you only split along one dimension but you have a different spitting criteria so with KD trees what you did is we just looked at each dimension we do pick one dimension at a time and we split such that the dataset was exactly divided in half and that was very easy because you just have to busy look at the feature value along this direction and you just split along the median right so if you find the median that's easy to find you just count the moment you have half your data you split there that's very very simple with decision trees it's not so easy right what you want to do bless you what you want to do is you want to find a splitting such that the two halves afterwards are as pure as possible by the way it doesn't have to be exactly balanced so how do you do this right with Katie trees was easy we just put it on the mean with decision trees it's not so easy and therefore it's a very simple trick we just try out every possible split there is that's that's the solution alright so what you do is you just look at either every dimension and at every single split you could make and you try out every single one all right so there's D dimensions in this case there's two dimensions there's n data points there's D times n splits it's just n minus 1 to be Technium correct because you only split between two points all right so basically what you did say I would like to split along this direction I try that dimension first then I look at every single data point this here's the projection onto this direction it's just that coordinate and I try out a split between any such data points so we need a second ingredient so the first ingredient would how do we find the best list the first thing is well okay we just try out every possible split that's great now we need one more thing we need to be able to evaluate how good a split is and for that we need an impurity function and last lecture we went over two of those Gini coefficient and sorry Gini index and the entropy function and so the way this works is quite simple let's evaluate this split here we want to know how pure is that split and so basically what we're doing is we look at each one of these subsets so we take the entire set and we divide it in two does here's the writes up said this is the left subset and what we compute we compute the following these have this function H which come for for a set and computes how how pure that set is and we had two different once the entropy or the entropy or we had the Ginny Ginny said we sum over k PK 1 minus PK and entropy was we sum over k PK log of P K's which was negative so that's the only difference the CS Ginny let's see has entropy and so both of these function have one thing in common they kind of if you know PK let me just define PK remind you of what PK is and for each one of these subsets that say I have you know this set s then I define s K is the set of all the points X element of LM in the past such that y equals K so all the points with a particular label let us say PK is the fraction of points that have that length all right so when I do a split what do I have to do I look at you know each set individually in each side individually and what I do is for I compute what's the fraction of points with a give a label I compute this for every one of the labels and then I compute the impurity either using this formula entropy or using this formula it's Virginia now I get a impurity for the left-hand side and the purity of the right-hand side and how do I weigh these two just based on how many points are in each side so if this here's my left hand side this here's my right-hand side and I say the impurity of the split is H of s left times the fraction of s left divided by the Hodel total plus h of s right times the fraction right okay any questions at this point yeah that's what this lecture is about good question any other questions yeah well he probably would if you're probably not going to change into another habit approximate nearest neighbor yes yes so in practice there's decision trees there's so much faster if you don't have to store the data points then it's actually well worth it so yes so by the way I'm still standing with this decision trees allows the algorithms right but if you bag them and boost them they become really good algorithms and that's what we will talk about so because they're now so so so cheap we can actually compute many of them and average them and that's essentially banging and boosting okay any more questions okay raise your hand that's clear now okay all right so now comes a little quiz when you consider a split what is the complexity of evaluating this formula complexity what I want to know is this Big O notation right so how many in terms of dimensions in terms of samples how many operations do have to make in order to compute the impurity of a particular state that's a you and your neighbor give you a few minutes try to figure it out you know [Music] all right to figure it out any guesses I may get splits and I want to know so now given that split an hour basically divided the data set it left and right and now I have to compute this impurity to know how good that split is my answer based you have to compute the impurity on the left hand side impurity right-hand side and have to wait by these two terms what fraction of the points go to the left what fraction the points go to the right and I have to do this for every single one of the splits sort of end times these splits and now the question is what is the computation of evaluating one particular spits who has a gas so what is the computation of of what are the computation requirements about the Kapiti to computational complexity of computing this term here for a particular split so if a split as splitted and right the light what does it cost me in terms of the number of classes number of data points and it guesses yeah so yeah it's I guess I called last night called C's okay is number of classes so this function s HFS L is actually you know let's say it's the one to capital K or C PK log PK all right so that's definitely a sum that goes over K you know all classes all right capital K class so we definitely have let's call it capital K so we have capital K classes and then what was your guess times n in yn that's right so you have to go with the entire data set and see which one goes left and which one goes right and you also have to go and B then each subset you have to basically count how many for every class how about the fraction of points of that class right so you have to compute your PK well it's a PK so you know ultimately you go over all the data points so it's n times these splits and each evaluation because K times n so that is the complexity is d times K and N squared okay enik yeah that's that's not a no problem right so we split the data set in two halves and this function here just sums over all your classes there's a K so this is actually as nothing here that's about binary you keep splitting and to the impurity function Kosovo multiple classes that's fine that's right you want what you want to do is you want to find leaves that have only one particular class right but you could have many many different classes not a problem okay raise e Hana this makes sense not many okay who understands Big O notation okay this by the way they was invented at Cornell University Big O notation was a Turing Award yeah oh we got to make this a lot faster don't worry so one more time what this here is basically how much work do we have to do right in order to find a good split and so what do we do we go over all and of all the dimensions and we try out every possible split if you have n data points you can kind of split before each data point in that dimension so if n times the possible splits that we want to evaluate in order to evaluate a split see how good it is you have to do that much computation we have to basically go over all our data points and divide them up into K classes and then we go over each K class and we express this term which is basis as the entropy section negative we just have to sum up of all these K classes and then compute this term okay but estimating all these peas or the fractions for each class cost us order n because we have to go over each data point and see which label it is does that make sense now raise the hand it makes sense now okay all right so it turns out that this is the computation complex that'll be pretty bad because quadratic that means if your data get twice as large it takes four times as long right so that's actually a pretty slow algorithm you couldn't run this over large data sets turns out there's a very good trick and how to do this much much much faster and that's the following imagine I want to evaluate a split but I've already evaluated the split before and please pay attention now because that's exactly what's going to be the question three I think in your next project so if you implement you tree the first thing you can do is you raise the implement exactly exist you chart every single split and for every single split you compute the purity and then you just pick the one that basically you know minimizes your last function that's it that you remember that split yeah I'll put that and then you build the next bit that's so you all build the tree one by one so basically you have any reason alone you find the best weight that gives you two sub nodes and then recursively for this one you find the best fit and for this one we find the best but each one of these requires you to go over the entire data set if you implement it naively but if you're smart and you do the following you go through these splits one by one and you will realize one thing every time you let's say I've already evaluated the split now I move one to the right this would have already evaluated now I want to move evaluate this that the only changes that one cross moved from the right side to the left side okay does that make sense otherwise these splits are exactly the same the only difference is this guy for this bit is on the right-hand side for this bit it's on the left-hand side okay so on the right-hand side my fraction of points base my counter of points with the crosses goes down by one and the left-hand side it goes up by one okay that's a constant operation so if you do the splits one by one that actually evaluating the splits only costs you order K so you can actually get rid of this factor and so you don't need this factor n because you really can reuse all the computation from the previous bit you just count now when you move one to the right you may say okay well exactly one point moved over so you just keep a counter how many points you have of each class on the left hand side and the moment you move one step to the right only one of these counters goes up that's it and then you basically have again all that these probabilities for every single class a class label so that's how you implement the project and that actually now makes this algorithm really really fast now it's linear with the data set size so now if you dataset size doubles the algorithm is only twice as slow right it's not four times as slow that makes a huge difference any questions yeah you do have a log in there for sort of sorting yeah I kind of put that under the rug log n is 30 any other questions yeah why the M was in there the questions initially about was the end there because you have to compute the P case you have to know how many what's the fraction of positive points on the left hand side what's the fraction of negative points on the left hand side so you have to these P K's are computed from these data sets right SK so these counters how many points are positive on the left hand side right so you have this for the left hand side say all right what I'm telling you is if you move the split that can only change by one so you just keep a counter here how many positive points you have on the left hand side how many positive points you have on the right hand side and you just decrease one and increase the other does that make sense OSE you re okay good good well that's that's good okay any more questions all right let me get to Arthur's question then Arthur's question was how do you do this for regression right so imagine you want to do regression and so regression we try to be donut with the classifier you don't have classes but our class actually real numbers so for example you want to learn a function to look like this can you use decision trees for regression right and the answer is yes absolutely it is called regression trees and actually if you seek heart they're often called car trees classification and regression trees so from the beginning they were actually considered for classification and for regression how does this work it's very very simple he do exactly the same algorithm the only thing is you don't use entropy or you don't use the Gini coefficient instead use the squalor so you say my last function for a certain set is the following this is on 1 over 2x why element of s and you just compute y minus mu squared where mu equals the average that the mean point of all the labels and in s okay so here's what you do for a set you say the impurity of that set is how close everything is to the mean so for a set s you compute the mean the average label in that set and then you say on average how far am i off from there from that mean does anyone recognize this formula some my miners knew squared it's a variance that's right it's just the variance on the right on the left that's all there is right so in practice if you do this what happens so you take the every data points here sampled from this this function so what do you do you take this function you split it somewhere and you estimate everything on the right by the mean and everything on the left by the mean you say my function kind of looks like this right just two straight lines that allows the approximation of that function right that's okay because you can keep splitting so you split again you say okay well now I take this mean and this mean right and here's put again take this mean and maybe this mean and then I split again and again and again and at the end if you do this many many times what you will get as a function like this will be approximated and many such you know straight straight lines basically okay so each leaf basically just predicts one value and you you know you basically predict you you split your space repeatedly and then for any given x value that all the points of easily fall into one interval you just predict the same label so it's not a smooth function but actually works surprisingly well any questions what you need to do in homework 7 is you have to show that this update can also be computed incremental e so if you basically move your splitting point one to the right you can also update the mean and you can update this loss with just a very small computation so you don't actually have to always compute the mean on the right the mean on the left because the fact that you know this one point moves to the right you basically subtract a little bit from the mean on the right a little bit on the add a little bit on the mean to the right a left and then you just clean up these factors and the way this works is by just summing out if writing this as Y squared minus 2y mu plus mu squared and then you keep track of each one of these terms that's in some sense that's a big hint for homework seven and so you need to do that first before you implement it in the next project any questions yeah ah very good point so his question is we keep splitting and you can always keep splitting and improve your result right so when do you know when to stop right because this you will never have two points that exactly it would very rarely you know all the leaves with the same value and that's absolutely correct and what you essentially do is you trade off your beta just say you only grow to a certain amount of depth or you only you stop and there's n points in your Leafs that's that's what it is very good question any more questions yeah oh you can use any laughs you want right it's actually you know this is you know because you try out every single split right you can even use glasses that are not differentiable I which is awesome it is really really powerful right you can have a really complicated las if you want to the absolutely right right the mean is just you know square laws just need because you're actually computing the average on each side but you can use any kind of lots very good question okay any other questions all right so decision trees are in some sense a very different way of doing that much machine learning then empirical risk minimization with these linear classifiers but you're also it's someone analogous right you also have a loss function you minimize that loss function you don't minimize it with gradient descent you minimize it by repeatedly splitting your data and changing your predictions and you also have a regularization term and the regularization in the Bay Z says don't make your solution too complex and in empirical risk minimization we said it's lambda times the square norm of W for example that was the one regular visor when SVM's here we typically say you know we just limit the depth of the tree so we busy say you want to find a tree that minimizes our loss subject to the constraint that our tree can ever most depths of 10 or something okay and so this step is you know what you have to balance bias-variance tradeoff and what I showed you last time is this little demo where you could see the training in the test error by the way what people typically do so in our homework project we just say we limit the depth of the tree a better thing to do actually is to limit the number of nodes in a tree to basically say well a tree could look like this where you actually have a split here a split here and you have two more splits here maybe something like this right that's only a complexity of two four six alright um that works a little bit better than just mini my limiting the depth of the tree in practice so if you take the the depth off your tree then the training error as a function of depth will always go down and it will go to zero eventually alright unless you have two identical points they can't be split any more with different labels and the reason is because you can keep splitting until every single leaf is pure okay until you basically means that must mean you have zero training error even in the regression case right as was just pointed out earlier you can keep splitting until you make zero error anymore now that this is the training error the testing error looks like this right and so here is the problem of decision trees that it's very hard to find the sweet spot here and the reason is bigger risk minimization with lambda lambda can take any real number right decision trees this has to be it has to be an integer so you can either have three or four all right but you can't have three point three doesn't really work right because you can only go in these entry in this integral steps alright you made the chip going down and then you go up and then there's not much wiggle room here so it's very you know it's the sweet spot but you know there's some depth that actually gives you the minimum test error but it's not a very good way to balance like it doesn't allow you to have a fine-grained sensitive analysis of what the best trade office between bias and variance and that is in some sense the downfall of decision trees that's why in practice they don't work very well because you know there's basically really only one setting and that may not work all that well right but you would really want to something in between here but it doesn't exist there's no notion so what we'll do the next two lectures actually starting now is take this so decision trees have a bias and a variance problem but here they have high variance here they have high bias and finding the sweet spot is really really hard so what we will do is we will find the sweet spot with a different way not changing the depth of the tree it will either basically make the depth really small and admit that we have a bias problem and then combat the bias another way or we will make the tree really really deep and then we have a variance problem then B of memorizing our data set overfitting until the cows come home right but that's okay because we can actually address the variance problem in another way and that's exactly what we'll do now and that's called bagging [Music] [Music] all right so remember the three first what we will do is he will grow really big trees and just say they have a variance problem and we try to reduce the variance and so actually the way we will I will describe value to you is independent of the classifier and actually bagging is something you can use with any classifier that has a variance problem and please remember it it's something that somehow people have forgotten over the years the algorithm from the 90s early nineties invented by Lee of Ryman perky University and it's really a fantastic algorithm it's ingenious but it works really really well so please keep it in mind and you have a variance problem like when you go out in the real world you use machine learning on your data sets analyze the problem you have if you have a variance problem use bagging it's amazing so here's how bagging works so if you look at the notes who didn't get a note almost everybody sorry that was a printer problem the printer died once again and it's not an excuse I'm not you know it's not that my my dog ate the homework is it really dies apparently quite a bit okay so you remember that YB bias-variance decomposition so the air decomposes to three terms the noise bias and variance in variance is the following it's just a reminder so if you have HD of X can anyone tell me remind me what's H bar watch HD who remembers yeah that's right so HD is if I have a change that said D HD is the classifier I learn those expectations also over D and over X and H bar is the expected classifier if they integrate out d and so if you have a variance problem that means this term here is large so it means that your classifier is quite a ways off from the expected classifier right so if you had a Chinese very different data set you would get a very different answer and that's exactly what you get with decision trees can anyone tell me why why is that happening with decision trees why do they have a variance problem why do you get a different now if you decide to different data set you will get a different classify yes yeah you're right so busy you spit on the data point right and you keep splitting until you make no more mistakes right see if you know at the end you will get everybody right but all these low level splits they're very data specific right so if you had different data you would get very different splits and you would split in different dimensions right and that makes could completely different classifiers all right so decision trees have a variance problem so how could we fix it and he is a bear he is ingenious way of fixing instead of just saying one do you have one dataset D I just asked my as my uncle was a magician and he gives me many different data sets d12 DM okay and a train a decision tree or classifier on each one of these M data sets and then what I do is I say my final classifier H is just the average of those let's call this a DJ alright so you know I asked someone to give me em datasets you know let us not think about how I get those em datasets but if I had em datasets and what I could do I could train a classifier on each one of these em datasets and I could average the results and why is that a good idea well if n gets large and this it becomes exactly H bar okay by the weak law of large numbers that's because these are basically iid drawn these are basely by my random variables here and as my classifier becomes closer and closer to H bar this since this becomes 0 so if I had access to end different data sets and i could make em really really large then i could make the variance arbitrarily slow small and go to 0 ok and because i have a variance problem my error is dominated by this term so I can make my arrow error really really small what's the catch why isn't everybody doing this yeah so buy the food though I guess what you implying is but you know but I take my data set Dean divided into M data sets I actually thought that you know somehow miraculously my uncle just gives me m times more data but you're right so the answer is you don't actually have those data sets right and one thing you could do is you could take your data set and divide it into M small data sets but then you're totally right then you're actually trading that off you know and actually you're making the or you also making variants worse because your data set is small right in fact the kid if you can't make em infinitely large the chaos in the extreme case you only have one training data point right so you can't do this right because you don't have M data sets you only have one one data point one data set right and here comes the ingenious thing that bagging us right bang that's well never mind right let's just keep going this is pretend he had these m data sets and in fact we can make them up and here's the trick is anyone hurt bootstrapping before and bootstrapping comes from the you know a Barone from munchausen who you've probably not heard of but he was a big aristocrat in Germany and he was a big liar and one day he gets stuck in the snow and they asked him how did you get out he said oh I pulled myself up on my own bootstraps and that was the expression obviously that's not possible he was mine and but the name stuck so what we will do is people you know kind of when you this will make a lot of sense in this minute we will actually create these data sets out of the original data set and here's what we do we say okay you take our original data set D that we have and now we create M different data sets with a little magic trick we sample n points with replacement it's a sampling with replacement with replacement and we do this n times let me get d1 okay so basically what we do is we take endpoints they can be filled as theta set up and every time if one point at a time and every time you roll a die and pick one data point here and now the important thing is that we do it with the replacement and because we're doing it with replacement these data sets are not all the same because you can pick something with replacement basically means you can take the same point over and over again right in the extreme case this data set could just take consists of one data point repeated n times right the chances are very small the chances are one over N to the power of n right but it's possible okay so what we will get should we get n different data sets that each one of them consists of data from my original data set D any questions raised e Hannah that makes sense okay awesome you will have to implement it in the next project all right so now if we do this right you can basically now do the same thing we just want each one on these data sets we trained our classifier let me get h1 h2 - hm and we average them now that's one thing as n goes to infinity this does not become the average classifier can anyone tell me why without looking at the notes which property of the weak law of large numbers is violated yeah that's right right so they're actually not independent all right if I tell you this theta said in this theta at this day I said you get a very good idea of what's going to be in the Lexx data set because this is drawn from the same point it's the same point over and over again right there's only so many points that go around right so you have these m data and function that's okay right so this is no longer halts but it holds a proxy and given some assumptions the kind of holds right or at least say this term still goes down so they are no longer independent but one thing we can show is that these datasets are still so do you know that's not another problem that we could have and that's of these datasets are no longer drawn from the original distribution so our distribution D is drawn from some distribution P and you know one thing we need to show is that these distributions that these Thetas has are drawn from the same distribution P what's the time yeah let me let me do it brick turns out he can prove this well it's everything easily and here's the proof so what I want to show is that each dataset di odj is drawn from the original of the distribution P and so here's what I want to say let's let me call the following distribution Q she was the distribution where basically pick a data point from this dataset uniformly at random oh sorry she was a distribution from which this data said DJ is drawn and I want to show that's the same distribution as the original data set P and so here's so whatwhat's this causes of Q while the process of Q is I first saw data pointing to my data set D and then I draw n of them to D into my data set di okay that's basically the process that's the process you know why didn't I just show your demo when I do this next lecture all realest people give me this this look like oh my gosh you're three minutes left come on oh I generate a data set so we do something hard any suggestions what should we do well that's so easy for the regression tree you can just split horizontally and vertically I think a spiral all right okay spiral spiral is actually really really hard for decision tree right there's like this you know decision trees nightmares are made of spirals oh okay this is my spiral it's a good spiral okay good here's my spirals a ying and yang okay so what I will now show you oh yeah you can actually see it okay good those actually whenever I averaged many many decision trees and then and never lets me down all right and what do you see is that here's actually you see the average of many many trees right so on average basically right it actually covers this very very nicely that basically and where there's positive masses right about the axes are actually on average these trees are doing very very well right and although each one of them looks very very different and should let me let me show you one more demo actually I can actually have even random forest variance demo right once okay one more time I do one most oh yeah here we go this is a slightly different data set and what I have here show you different decision trees that I learned in a slightly different data set basically doing doing bootstrapping and what you see in the bottom now is the forest right it's a wonder you can see very very nice and look at these these vertical lines here right but if the decision tree basically make some wacko decisions right just because of splits he's all of this is blue right he makes these long vertical lines because of one point it kind of takes this entire area and makes it makes it blue or red right so that's the high bias that's where the high is or the high variance of decision trees come in because they always make axes the line splits they kind of always cut the entire space in half right but if you look at the forests on average actually this is very well behaved and you see this kind of you know this is gonna spiral Li data set yeah that you actually comes out here very very nicely right and so turns out random forests actually very very so these these been reports to base the trees of forest was a average decision trees are very very good classifiers right so you can take some really bad classifier that it's because you know it has a variance problem that that's a thing that's the key thing I want you to take home from this right we took a crappy classifier but we knew it was a variance problem we could address it with bagging and then you got a really really good classifier right and so please remember this when you you know do data science later on your life right when you analyze you have a variance problem you do bagging begging works really really well and some very nice advantages that we will get to next lecture all right see you guys 
","['', 'decision trees', 'k-d trees', 'impurity function', 'Gini coefficient', 'entropy', 'nearest neighbor search', 'classification', 'machine learning', 'bootstrapping', 'bagging', 'bias-variance tradeoff', 'random forests', 'weak law of large numbers', 'overfitting', 'underfitting', 'in-sample error', 'out-of-sample error', 'handwritten letters', 'character competition', '']"
"few logistical things so the character competition is up and running the homework assignments do but we slip slip days the project on kernels and the next project will come out on Monday that's the current plan or maybe Friday right there Monday then we will so i guess basically you know we had this this you guys filled in the survey how to improve the class so I think the three main things were CMS pushing the great under CMS that's done one thing people ask you know making the homeworks more related to the project or morally to the course so now the homeworks will have a very clear link to the project certainly the next homeworks you will see this and the last thing was recitations and so we now actually do have recitations some some people already went to the last recitation on Colonel's and there will be one on Gaussian processes there's still something unclear about Josh processes please go to the recitation student giving it actually he's riding his PhDs last year was PhD on Gaza processes there's few people in the world who know more about it than he does okay good then last time we talked about bagging and bagging is a very effective way to reduce the variance of a classifier and so we're using this in the you know in particular in the context of decision trees goes to some trees if you grow them fully a very high variance classifiers and so the idea behind bagging is very simple you have some distribution P from which your data is drawn you don't know this distribution P but do you have this finite data set D and so now if you train a classifier this classifier has high variance so what does that mean in May instead in some sense you're worried that you your classifier capture stuff about this specific distribution like this if there's specific data set D that is actually not all that common in the distribution P it's your overfitting so how do you combat that problem it's quite simple what bragging does is the following you take this distribution D that's and you guys already this dataset D there's n data points and from it you draw multiple datasets d1 all the way to DM each one has the same size as the original dataset and you draw them with a replacement uniformly at random with replacement okay so the difference busy if you draw on the replacement EBC draw one point at a time but you can pick the same point multiple times every single point it's kind of drawn you know independently from this data set and then what you do is from each one of these data sets you now learn our classifier H and you train all of these and then you're saying well my pilot classifier is just the average of all of these so now you go back say my final classifier it's just the average J and that's it and the idea basically is that let's say because you're overreacting I think you can kind of view overfitting there's an overreaction to certain that your sync receives in your data set well these will be all different than these different data sets and they essentially average out all right so that's basically the idea and yeah yeah that's right that's right but you know what you do we just buy em computers you do it all in parallel so it's actually true by the way now that we you know computers are so cheap a multi-core every computer now has whatever 16 cores at least something if you buy a desktop but it actually becomes very cheap to run things in parallel right so in some sense it becomes you know more of a more appealing method with modern hardware so the testing is however slower that's that's fair right the testing takes ten times as long but you get another advantage and the you know one advantage is basically reducing variants but there's another advantage and that is that now you actually get a very good calibrated probabilities so one thing you can say is you could actually you know that's say on average you predict a certain label you could actually look into each one of these different classifiers and see how many of these classifiers have predicted that certain label right and that basically tells you how certain you are about your prediction all right so let me backing is really awesome I'll show you a little demo in a few minutes one of the most popular and instances of bagging is called random forests and random forests is awesome it's a really really awesome algorithm it almost was the machine learning was almost forgotten and then it came back big time as what also invented by the Oh Brian leo breiman was the same person who was invented bagging after he was done with bagging he invented random forests and random forests is very very simple algorithm it's basically bad decision trees but there's a very small modification and so here's what you do you basically you take your data points you take your data set this there's a D if you draw the different data sets do you want to DM and now for each one of these you learn a decision tree all the way to the end right so but if you keep splitting it to you we have zero training error or until you basally have two points that identically you can't split them anymore okay so that's your really overfitting in some sense each one of these data sets right you're so confident that bagging is gonna rescue you right just you know driving you know you going all in with the variance and in fact you do going even crazier they make a small modification to your decision tree algorithm and that is whenever you make a split right before you split currently what decision tree does it tries out every single dimension and everything that I mentioned it tries out every single slit what you do in random forests you say instead of doing that trying all D dimensions I randomly subsample K dimensions and I restrict my search to those K dimensions all right so baby I build a tree I take my data set to say no d3 here or something right and then I split it in half right roughly in half and two different subsets as he has now my first node of the tree but here for this split I split ensemble break a feature F enzyme perceived a threshold T so I'm you know we fix F is greater than G go to the right otherwise go to the left here I'm only considering K but K is less than K is less than than D dimensions and the reason I'm doing it doing this is to make sure that these theta these classifiers are all very different right series he's splitting on very different dimensions and we think about it right what's basically happening here is that because they are so different they will make very different mistakes on test time and now if you average them the mistakes averaged out okay and that is an extremely effective either like random force is one of my favorite algorithms so just you know I'm not going to write down the pseudo code it's on the notes under random forests but really them the important thing is that you only split once I can only split on K less than the features and so the important thing is for every single split your sample K features a fresh right so it's not for every single function you use up some features of it every time you split right so here when you do this but again you pick K features at random and you split on those and then you find as you find the best one and then here you pick KP just a random kind of estimate any questions yeah okay actually maybe maybe this is I'm bragging it maybe it's just less not less less okay and so it works for any kind of dimensionality and so one question is how do you set K all right so now we have an additional hyper parameter and it turns out there's a very good justification that I'm not going into just set K to a square root of D and you round up and that turns out to work really really well in practice and I've never really seen that's please D I have a parameter that's that you don't have to chew you just set K to the square root of T okay this people have analyzed this and there's the Stewart theoretical justification why this works so well but for all means and practices he can almost consider it not a hot now he comes the beauty here's the reason I love random forests so much I mean actually I would say oh sorry yeah go on yeah oh then you try out every single one and for each dimension you've tried every single split and you pick the one that minimizes your impurity so there's just the normal decision tree argument right you're just doing that time basically you pretend that you only have these K features right that's all you could as you know if you were implementing this and Python or something you would just take you just just only take out those K features and now just call you're splitting function okay so one reason I love which when enforced so much because it really you know it's one of the very few algorithms that works out of the box it's been practitioner comes to me and it happens a lot people say like oh I have this following data set I collected whatever like you know samples of horses and I did the following strap the following sensors instead of whatever right and I measured the heart rate and these kind of things and they feature vectors they're you know they're totally different you know units etc right one is the gender of the horse one is the age of the horse one is the heart rate etc right if you want to use most machine learning algorithms we'd have to make sure that these actually scale between 0 and 1 etc that you know if you have to pre-process the these features very carefully when the forests you don't have to do any of this right because you're just splitting so actually the scale of the feature doesn't matter at all if you measure your height in centimeters or nanometers or feet doesn't matter right so you can't just most the time you can just take your data out of the box and now comes the beautiful thing what are the hybrid parameters of random forests it's K well you know what I said it's square root of thee you're done right and you have M right how many of these to you how many of these data sets do you subsample and how do you set em well as large as you can write there's no real you know it can't be too large because basically you know you said it until you get bored but you know you keep training and then somebody just stop and that's the only thing so you can't go wrong all right that's the only thing if you go wrong with you too impatient you said M equals 1 or something but you know typically M is a hundred or thousands you know you know gets a little better you know it's em gets larger thank you uses the error kind of a better plateaus right someone there's no benefit anymore and so it's a beautiful algorithm because it's so so easy to use and actually it's really really amazingly good so most of the time like my rule of thumb is it's always the second best algorithm alright so if you basically if you take some other algorithm that actually is tailored towards you know you know and massage it very carefully and makes about assumptions then you can do better you typically can always do better but it takes a lot of tender loving care to get there yeah so how is the size of oh they they're unrelated so you choose em independent of the size of thee oh no no M can be can be 10 million if you want to that can be larger right and it's just how many times do you want to sub sample a dataset right and so the important thing is each one of these datasets has the same size as D you're not taking this data set and splitting it into m partitions what you're doing instead is you take this data set and you sample a dataset it has the same size the same number of data points but with some replacements no they're not the same become because you're sampling with replacement right so let's say I've a data set has now ABC and I want to start from that another data set that has three entries alright so you trust a coin the first one it's a you get B so you sleep being here second one you stick you know it's also coin let's say you get c third one you may get again B alright so then they see the state I said it's quite different from that one and so that that's the key and because you repeat some data points you basically give more emphasis to some data point sometimes and other times you actually don't sample them at all so then you know you get this into different decision boundaries yes that's a lot sure yeah I said yeah if you can afford it yes absolutely you know but you may want to use that electricity for something else you know I know charge a test love with it or something so yeah okay good it's a good question so you're basically saying can you give us some intuitions about how what it means to average decision trees right and so think about the following way right I have a data set now you know some exes years and you know say some some it's hard into two dimensions actually and okay let's do this one right then you could either split this way and you're doing fine or you could play this way and you're doing fine right if you average out you're actually splitting out both ways right so half the time you're splitting this way half the time you staying this way so each the first tree would say all of this is positive and all of this is negative right because you split this way the second tree would say all of this is positive and all of this is negative right but on average what you get is the following this year is positive this year is negative and this year's conneautville I don't we're not really sure about right and that's the neat part actually gives you uncertainty right so now if you have a test point here you say well that on it's positive here this is negative right and here not sure it could go either way and so in higher dimensions there's not exactly 50/50 yeah good question yes any more questions yeah yes yes yes random force actually then one of the most commonly used algorithms for feature selection and maybe what you do is that say so what is feature selection selection feature selection is the following task I give you data set I try to predict something and I want to know which features are most predictive you know which features are most important and why is that why is that something that's interesting so when I worked you know I used to work a lot with neuroscientists and they for example one thing they did is you know one collaboration I had is they implanted for example electrodes into on top of someone's brain so they took the skull open they opened the skull put electrodes at a foil of electrodes onto the brain and then they closed the skull again let the person run around you know for a couple weeks and the and they recorded so they've easily had a the people wore a data glove and they recorded the finger movements and hand movements of these people by the way sorry these are not volunteers these people actually head it's not my path students or something they hack see these are people actually had to get done it had to had the surgery done for other reasons and so we basically use that and they bought you know BB I mean they were a volunteer but it's not that they're in this randomly cutting people's heads open so these are busy people had to get the surgery done they had to get this these electrodes implanted for their brains and so then we asked them to wear this data glove and this data graph basically recorded the position of any given finger and arm at any given time and then the task that we looked into is can be you know busy be recording the brain activity all the time so the motor cortex the motor cortex is the part that actually of your brain that basically you know it commands your muscles essentially and so the question was can you predict you know finger movements just paying based on the brain signal and you get a lot of phases brain signal the whole time and turns out you can actually predict it pretty accurate right so you can predict the hand movements etc and so what we could then do is you could actually then you know just pacing the brain signal actually you know predict whether they are moving boss and then it could also ask these patients to then for example plays on computer games but they just thought of moving a joystick and that actually worked and but the ultimately the reason the neuroscientists really want to do this is because they they were less interested in actually the application of brain computer interaction what they were really really interested is which part of the brain does what right so they ultimately what they really really want to do is they want to figure out which part of the motor cortex controls your index finger for example right and so they then Beasley once we had the model it actually could could make these predictions they came back to me said okay that's that's nice right it's great that we could do this but you know how how does it work right and so that's when you do feature select and body for example use a random forest and now if you have a several thousand of these trees what you look at is basically whenever you split a certain feature how much does the impurity go down all right so that's basically and then you rank them by that amount and and that actually is a very very good signal that a feature is very important right usually the ultimately these random forests because they are so random I did they spit on every feature eventually because eventually they always overfitting right so they spit on the noise but when they over quit they impurity goes down very very little but the you know when they spit on a real signal the impurity goes down a lot okay so if you average that and rank the features then actually you get very very strong signal and then what you usually do is you select like if very few features these are basically a few electrodes and a few frequency bands in that case and you retrain the classifier and you show that it still works and then you know okay please be found the region in the brain that doesn't yeah that's that's random forest work really well and high dimensional database low dimensional data actually works is surprisingly resilient to what the against the course of them a choice of dimensionality and in some sense because you never really compute distances in a high dimensional space different than for example the RBF Nam etc so it gets slower and high dimensional spaces but it still works pretty well yeah okay any more questions all right so one more time random voice are so awesome because they they extremely you know are the most out of the box classifier that I know right they only have two hyper parameters and I can tell you exactly how to set them right k is spirit of V and M is as large as you can afford right so maybe a couple hundred maybe a couple thousand and that's it all right so keep this in mind there's actually there's one additional advantage so that that's one advantage is very easy to run you do have to pre-process the data and there's a last advantage and that's the killer advantage and that actually is an advantage that every bagging algorithm has and it's called the out of bag error so remember when we did model selection before what we did is we took our data and we split it in training and validation okay and so let's say you want to you know develop some system etc and you you know in order to know or your training a test right and in order to know the error about UBC what you do is you train your algorithm on this part let's say 80% of the data and then you evaluate in 20% of the data and that tells you how well you're probably doing on the test dataset you know and we're real live in run time right on the true distribution okay the downside of this approach is that you have to only train on 80% of the data all right so that's kind of a pain set up turns out for random forests or for bad classifiers you don't have to do this you do not have to make a training validation data set split you can actually estimate your test error directly on the training data set all right that seems crazy but you can actually do it any ideas how we can do it code the out of bag estimation yeah that's right that's right you have many median Assessors are all different that's exactly right so what you do is you want to know what is the test error right and you want to estimate it on your training error so here's what you do you go through all your train data points and for every single training for your point I do you have many different classifiers H of x equals H J of X all right each one of these was trained on a different data set do you want to DM all right so here's what we can do for every single training point we can estimate the error right or what's the error the error is just basically you know saying the loss right if you want to compute loss and that's just the average loss you know yeah all right so if you have point x and y then we basically actually compute the loss of x and y which is the loss of the whole classify the whole classifiers earlier some of many different gasifiers so it's just the loss you know the average loss of all these different classifiers and here comes the cool part some of these classifiers will not have had that point X in their dataset right so if I'm one particular nearest point X I Y I that will not be included in all of these data sets all right does that make sense so when I Samba live data set with replacement an expectation I get around sixty percent overlap with the original data set in the other forty percent are just repetitions of data points being chosen multiple times okay right for example here had this let see was my D this here's my D D one that I'm subsampling in this case I didn't subsample point a instead I had point B twice okay does that make sense Louisiana that makes sense okay awesome so what I do is for my point a what I'm what I'm choosing I just compute the error only on those classifiers that were trained on data sets that didn't contain a all right and therefore I basically took it out right therefore it was basically implicitly in the validation dataset and if your M is large enough and doesn't really matter if you just remove a few classifiers right just make em twice as large right and then you're removing you know Basya Sencha li but every every see a data point you remove 40% of the classifiers right that works really really well so maybe I can formalize this just a little bit so busy what I'm saying is my my error rate I closed the out-of-bag error is the following a sum over all my data points to n is the average and then I do for each one of these data points I basically some about all the the J such that X I Y I is not an element of DJ I so I take all the data sets for which X I was not in there so I take all my data point again where everything at the training point for everything a training points I now go over all the classifiers and I say actually only take those classifiers that were not trained on that data point and then basically compute the loss of that particular classifier X I Y and I average that Zi Zi is just the number of data points the sum of number of data points that X I just take a 1 here so Zi is just the number of number of classifiers that were not trained on X I okay there's just a normalization so I just average them that's all that's any questions raise the end of that make sense okay awesome okay so this is extremely powerful all right because this means you train your whole classifier on all your data and for free you're getting an unbiased estimate of how well your classifier is doing on the test set and this is super super super powerful it doesn't get easier than that you don't have to be careful with your train validations setup etc and you know if you have a good implementations of random force you just spit that out right away all right so you know how well you're doing on your testing okay any last questions about random forests yeah so okay good question so saying is there any data which random forests are not good and so one thing for example in computer vision if you take images there are specific algorithms that we will cover next week actually that take into account that vision like pixels are kind of co-located and have a very specific structure and random forests don't really do this they check each dimension independently and so therefore they can probably never do as well as algorithms that take into account the specific structure of of an image there's of course other data sets for you for example I've structured data etc so then then it's also tricky yeah what is the promise of regression problem no problem at all you just take your question trees yeah yeah it's great awesome here's actually another thing reason why it's so good for me question is because random forests give you an uncertainty for regression but you typically don't get but to do you know I kernel eyes regression or something you have to use Gaussian processes together uncertainties all right so again so only Gaussian processes and dinin forests give you an uncertainty estimate for your regression value yeah yeah okay so you may you would stop at some point that's right so you would introduce that habit parameter yeah that's actually one thing you can improve and of course a little bit if you for each tree debate the full tree and then you prune away from the bottom like you take edge split and you see if that split does not improve my out of bag error then I remove it so you busy does that make sense you have the full tree and you look at the last split you know any to know it's a busy one point of one split you say does that split actually improve my out of bag error there's those data points that were not in this in my training data set and if it doesn't then I just remove it so you kind of reduce the size of the tree let help tends to help a little bit because you're busy removing total noise splits yeah all of you basically compute the error across all trees yeah I would probably busier it's not very interesting he's not entirely true right it's zero for all the trees were on which it was included in the training dataset but for 40% of the trees a training point was not included in the training set and for those who will not be zero no no no no because you include it will be lower right because including all the trees that were actually trained on that particular point how does that make sense let's say you treat a trainer random forest on a on a data set and you take one data point that you totally mess up you flip the label makes no sense right only the trees that were actually trained on that particular data point will get that point right everybody else would get it wrong so the out of bag error will be very high because whenever was not in the bag so in that's in that data set it will be wrong but the training error will be right low because 60% of the time it was in the training set and therefore you got to ride sixty percent of the time so training error is not very interesting for random force yeah okay good and so here's one little oh yeah one thing I want to show you the bias-variance demo that you all know I can now do it with random forests and so what you see on the axes is the exact same demo that you've seen before that you may see was on your project and so what you see here is the ensemble size the ensemble size is the number of trees that I'm averaging and what do you see here is the further a few interesting things so now I'm increasing the number of trees and I'm averaging and the first thing one thing you notice is that the bias is very very low like if I have one and it's just normal there is and a decision tree in this case by the way I'm not doing the random feature selection just because the two-dimensional data set so square root of 2 is 1.4 I've rounded up is 2 again so so what do you see but just take a little a single decision tree and I train it you know basically I have basically no web bias right the bias is basically 0 and the variance is very very high so they've been enjoy the variance is black line black line is the variance the Green Line is the noise the pretty noisy data set the same data set that you had it's basically two gaussians I didn't show the data set here but it's two gaussians that basically have some overlap and I try to disambiguate them and this here is the error basically off the whole thing and it's basically essentially bias plus variance plus noise which is exactly what the theory would tell us so here's the beautiful thing right as the ensemble size goes up one thing you can see very very nicely is that you know the the variance goes down drastically right here we have 10% variance right then it goes down a lot right to 4% right and it keeps going down but you also see this diminishing returns right this is my M okay so as my m increases my variance keeps going down an invention eventually basically you know a plot chokes right so doesn't doesn't change anymore right but you get a huge improvement here I'd have drastic improvement and the the amazing thing about dying is it reduces variance but it does not increase bias all right bias is unaffected and it should be very easy to prove that bias term actually is not affected at all by averaging the classifier I can anyone tell me why yeah yeah but actually this is a very obvious reason yeah that's right bias is not a function of H right off you classify that you get it it's a function of the average classifier which is unaffected in this case right so you know this is just having more averages of more averages right like it doesn't you know doesn't make any difference so so the bias stays constant but the variance goes down beautifully right this is just a fluctuation because that small sample size okay any questions about this demo does this bake up they don't make sense so I think it really shows very very nicely that the there's amazing power of random force so so please keep in mind bagging and keeping buys my mind random forests they're really awesome algorithms okay there's no more questions then we can move on last question yeah oh I see good questions they're very good question see saying what if he knew classification and I have apples and bananas right these are my two class labels and what I get is at the end 60% of the trees say it's Apple and 40% say it's banana right what's the output it's something in between what you do is you pick the mode you pick the mode you click the most common one and then the ratio tells you the probability with which you predict it yeah all right good stuff and now comes when the board is pretty awesome there's only one thing that's wore off and that is boosting and today we do boosting question yeah sorry can you actually good question the question is can you apply bagging to any algorithm and the answer is yes you can always bag everything and it really helps when you have a variance problem like bagging with users variance so if you don't have a variance problem if your training error you know on the test area roughly the same essentially it doesn't do anything it just makes you slower okay but yes you can you know people have backed all sorts of things okay good so boosting is I would say even more beautiful than bagging it brings me to tears sometimes but it's also a little bit more complicated so it's not quite as simple but I think you can get it so just you know just please pay attention so in sometimes boosting and bagging are very related so both of these are ensembles of classifiers so in bagging we just said you know H of X is just the average okay let's put the 1 over N in yeah and then boosting you do the same thing you say H of X is J equals 1 to N and you just have an alpha here alpha T alpha J sir okay so it's the same thing are you also averaging many classifiers but boosting goes the other way so boosting says what if we have a very very simple classifier that has really high bias that's right now with bag I gave you basically no zero bias but really high variance so we averaged out the variance now we're doing something else is saying what if we have really we high bias how can you get rid of the bias and turns out something very very simp similar can be used and actually started with a very famous question and this was in 1988 Mike occurrence was a professor at University of Pennsylvania and it's one of the fathers of learning theory he asked a very interesting question he asked the question can if you have weak learners which is very similar to raise the high bias man so big learners are basically classifiers that if you know they can never learn go down to zero training huh can we combine weak learners like if you you know if you have multiple weak learners can we combine them to a strong learner so can you if you have a classifier that is so bad it can never get even to zero training error can we take multiple such classifiers and kind of you know construct like you know a super classifier that that actually can get a serious training error on every single data set that that's the idea so that's called a strong learner so big van B's if you were convinced but you know the quite big question what question was which classifiers can give you zero training errors and which cannot like which can learn something which could not and so he asked that question and it was an open question and Robert Shapiro a few years later he was a postdoc at the time he actually came up with the answer very famously and he came up with the proof and it's really cool so Rob is really nice guys he came up with his proof that actually showed that yes you can and the proof basically constructed some algorithm that you nobody said well yeah we can always make a you know combine bad classifiers to make a very strong classifier but he was a pure theory tition that he didn't even think of it very much it's you know as a he just was the proof right but actually a few years later I you know he busy will actually we can actually make this an algorithm and the other papers called adaboost and this was a huge it right this was published I forgot which conference but the baby said you know anyone of you right have a shitty classifier right I have a way to make it really awesome I never was like yeah I have a shitty classifier I just published one and so it became a tremendous it right like you know suddenly everybody started using boosting and it has amazing theoretical guarantees that people go over in the next I guess two or three lectures they can beezie show that not only if you boost the classifier will you always end up a zero training no matter how bad your classifier is even if you have really bad you know really bad bias is the simplest classifier you can think of you can bring it down to zero training error but not only that you can bring it down to zero training error in a very small number of steps and then step that actually is logarithmic with the training day it's ridiculous xxx right so it's tiny so very very quickly you know you can turn here as an extremely practical algorithm you can turn any kind of loose idea that you have you know you just come up with some kagura algorithms not very good right you boosted and becomes an amazing argument and maybe I what time is it now okay so I set up the the overall system and then we will get dived a little bit more into it in the next lecture and then is the following so let me just call capital H as the classifier that I'm assembling let me call this your capital H yeah capital H is basically an average of many classifiers and these are all have high bias they're not very good but I would like to kind of kind of make it you know combine them into some really strong Frankenstein classifier that's the idea and so what we need to do is we you know the first thing we need in your last function that tells us how well our classifier does we've done this before we just take any of the last functions that we've seen before for example the exponential loss or square loss it doesn't really matter and let me write the last function a little different let me say the last function is now the last function over H I right this is a little bit of abuse ability I'm saying if the following it's basically the sum of all the losses and sum up all the losses about all the data points that's it should be pretty clear right so basically the last busy I'm embedding the loss is the loss of a function so ever you know there have a function and say the loss of that function is if I go over all my training data points and look at the prediction and the real label and I compute the loss an average that's the last of my classifier right and this last function can be the square loss or exponential loss or whatever you like right your favorite last ones give me the hinge loss doesn't matter okay so here's the idea and it's the idea is this H is a sum of classifiers okay and so imagine I've already done a few iterations what I would like to know is which classifier do I have to add right so if I now go from n to n plus 1 which classified after that all right so initially if it's a I just have the zero classifier that predicts zero for everything and so given that I already have some classifier what classifier that and so I can write it as the following question which little function H is this out of the set of my terrible high bias classifiers my weak learners should I add to my H such that I minimize my loss and so which function should be added such that I minimize the following function h plus alpha H okay does that make sense so this here's basically this set here is basically my my set of my sum of classifiers if I add one additional classifier to it that's what I'm interested in okay and I'm trying to come up with a mechanism that tells me when you have a set of classifiers which one should you add to it okay and once I have this mechanism then I can just start with any kind of classifier just a brand you know and then I just keep adding you know use that mechanism to add classifiers to it and that minimizes my last month alright so this is what I'm trying to solve any questions at this point yeah my first thing yeah you just take an arbitrary one actually what you really do is the first guy surprised the old zero classifier so just gives you always zero and then you basically start from there alright so this is kind of the base case yeah but just good point yeah okay good question how about alpha come from for now let's actually not so we will get into how to choose alpha optimally for now I just say alpha it is a small constant okay yeah good point we will get into yeah all right um you know here's what we do we just stop here but you gotta promise me that next time you need a lot of fruit before you come here and you're mentally alert and then we gonna go through it 
","['', 'bagging', 'classifier', 'decision tree', 'Gaussian processes', 'homework', 'kernel', 'machine learning', 'overfitting', 'prediction', 'probability', 'random forest', 'recitations', 'SMS', 'variance', 'classifier ensemble', 'boosting', 'error function', 'loss function', 'weak learner', '']"
"welcome please everybody close your laptops everybody close your laptops thank you all right good stuff so um quick fever blister just takes the support vector machine project is done turns out the competition was a little hard not too much variance on that one however the CAG competition turns out a little bit too easy some people are doing really really well already that's really really cool I won't please remember you have to submit a write-up describing your solution and code at the end okay good so we are talking about boosting so last lecture we still talked about bagging bagging if you remember we basically take classifiers that have a variance problem so if you have a variance problem you know you can identify this your training error is much much lower than your test error typically and what you do in bagging you basically to sample a whole bunch of train dealer sets with replacement and you train a classifier on each one of them on your average and we could see very nicely that reduces the variance does not increase bias so therefore your test error goes down bearing is a very very good solution if you have a variance problem so today if you're doing boosting and boosting is the setting where you have a bias problem and it started out with a question I mentioned this last time whether weak learners this was in the 80s the late 80s people considered the question be differentiated between weak learners and strong learners weak learners are algorithms that cannot learn you know bring your training error down to zero and strong learners can't so that was important distinction back then and the question was can we take weak learners we have many weak learners each one of them cannot get your training error down to zero can you combine them in a way to get zero training and can you do this efficiently and so this was a question post about Michael currents and very famously rupture Peary came up with the answer he proved first just proved theoretically yes that always possible and then he came out with an algorithm that was essentially part of his proof and that actually showed how to do it and that became a huge instant hit so what I wanted to give you today is a little first I want to start with a little intuition how that works and so once again our classifier is the following we have H of X is an ensemble of many different classifiers for each one of these little T's a weak learner so that's not algorithm that's not very good and the way we define weak learner is in the classification setting it does better than a random point Hoss okay so that's that's all that's a pretty blow bar right that's better than guessing and okay so now I'm going to give you some intuition so please pay attention because if you reprehend around this and everything becomes a lot easier so imagine we can write you know and then we should use the following coordinate system that we've seen before in the context of Gaussian processes where I give you an n-dimensional space we have M training points and I can now define an n-dimensional space where each axis corresponds to one training point again baby says the prediction or the label off that training point nice we have enough here x1 to explain what these are this is important these are not the feature values these are the labels right and so in this space let's say we have a three dimensional space let's say I have a data set I know just make this up x1 has the label 1 x2 as the label minus 1 x3 has the label 1 again all right then I can actually these are my wives right this is my y1 y2 y3 so I can make a vector y equals y1 y2 y3 then that is a point in this three dimensional space all right so in this case Y y1 is 1 so I'm here y2 is negative that's actually down here and by 3 is positive that's yeah all right so I basically you know I can go here and then I go down and this here is my vector Y okay ray Z and you still with me ok awesome ok and now I can define my loss function I can just map this basically all in this space okay I can busy say well if I now run a classifier capital H on my data what I get is predictions for every single training point so what my ad h gives me it gives me a vector H of X 1 H of X 2 H of X 3 all right so that's basically what my function does right so I piss thicken it's basically you know I stick in my mind set X 1 X 2 X 3 and outcomes a vector like this we can call this H ok this is my prediction vector never have a strong learner for example in RBF SVM or have a decision tree that I grow all the way to the bottom like without stopping this will be exactly the same as Y right because these algorithms can drive the training error down to zero okay so you know you train the you know you train the algorithm then on the training data set but you will basically get this exactly the same point but a weak learner when it's not very good for example a decision tree that is where you're only allowed to spit twice right so the limited depth will not be able to get every single training point right so what it will give you this is my little age now is again a vector edge age of one H of X 2 H of X 3 but it won't match this alright it's pretty crappy so let's say you know it's here all right let's take in my training data and this here's my prediction right C equals H of X 1 H of X 2 X 3 ok and so the question is if I have an algorithm that gives me these can allow Z predictions right can I somehow combine many of those to actually get to the you know to the true answer all right that's that's the question we have that's the question of boosting and the idea is quite intuitive actually it's quite simple how this works so think of it in terms of a regression problem ok so let's say you want to do regression on these labels so what we do is we start out and we train our classifier H on our training data and because our classifier H is not very good right whatever give us is this point here right and we know this is far off because we have the labels might be computer oh my gosh all right this is not great right so we wanted this this vector here of what we got what's this vector here ok so here's what we do it's a little bit like like you know it's like when you when you would you're drunk right you can do this experiment tonight it's Friday right so go out go to college town get you know not too drunk has you know that's dangerous but you know drunk enough that you can't walk properly anymore and then try to walk home and so what happens is now you know overall you're probably you know gonna be pretty bad at you know pinpointing which direction your home is right but you still gonna make it home right and why because well your average are you know going home having a good time right on average if you sum up all the stats but each one of them is pretty bad right eventually you get there so there is this what we're doing you have this drunk little drunk age guy right it's pretty bad but it's not horrible it's not horrible and sense that it needs to know which direction has to go right it knows you know well roughly South right so we trust it a little bit so instead of going all the way you just go a little step right you know you just take a little step in that direction and we stop here and now we reevaluate again and in some sense basically now we have exactly the same problem we had before could now imagine a new coordinate system that starts here's the origin here's our wise ry vectors slightly different right and we solve the problem again and you get some other H you know as I get again this drunk you know algorithm gives us another H right let's say this kind of points you know whatever points here right so you think it's some tiny step in this direction right need to stop here and then you solve again right and it's essentially what you're doing is very very similar to gradient descent that's what grading descent basically you always get a direction I kind of doesn't really point you to the minimum but it points you in the direction of them anymore and you just take many many such steps and add them up and that's exactly what boosting is boosting is grading descent in function space so you have a function and you just basically performing grading descent on these on these functions and the condition that you have on the weak learner is really really mild so what you want to do is every single step you want to get somehow closer to your goal and that's the case if you know if I take this week learner let's see they say this is my weak learner I'm pointing in this direction right I'm making progress as long as in a product of that weak learner and my label is possible so basically as long as I'm not going completely in the wrong direction right so if I based a point in this direction that means the goal must be somewhere forward of me but it can be over there it can be over there that's okay so this can't have more than a 90 degree angle between the direction that I'm that my migraineurs pointing me and the true label now turns out that's very easy to find I'd imagine you have a weak learner that's really really terrible right and in fact here's the to label instead what it tells you it has to go here that's not a problem why is that not a problem any idea yeah you can just flip it but he's right you can write you can detect this so if it's negative if you just flip it all right so then you're actually pointing in the right direction so the only case where really doesn't work at all is if you're completely orthogonal right then there's nothing you can do anymore all right yes kind of now that's when you add your limit that's actually when the algorithm stops in some sense all right and that's exactly what boosting does so basically whose thing you basically iteratively you know find a new new weak learner you add a tune Sowell that means now you move and now you find a new big learner and you know you add to example etc etc and the smaller steps you know the longer takes you but you know after the you know the more accurate you can add you are at the end any questions about this intuition yeah no no they're just weak learners have a see a little better than toy costing a coin tossing sorry so in terms of classification you get better than se of a balanced data set binary data said you get better than 50% accuracy and in regression essentially you have an inner product divided in a minute like what exactly happened you're not just orthogonal to that so you go yeah that's exactly right so each of this is basically all my little steps right so I basically take alpha is a small value and this here easily gives me each of these weekly owners trying to predict the label all the way right that gives me a vector that points that it thinks points to the label and what I'm saying I multiply this by a small constant just add them up yeah how do you know sorry okay good question so he's raising how do we know that averaging all these classifiers will give us the correct answer right and the answer is very simple the answer is the following if you like I'm here right this here is the Y vector that I'm want to get to I have a big learner it's not very good right let's see points and whatever points in this direction right right because if you take the you know this defines a hyperplane so because the label is on that side of the hyperplane if I take a step in this direction I must get closer to Y and Y is this is Pythagoras again all right so if you take the distance to Y right or the distance to my after a small step this here must be close and the reason is because the distance to Y is a squared plus B squared a squared stays constant and B squared just got smaller okay so provided why there's actually lie on that side of the hyperplane now the positive side of the hyperplane and your step size is small enough if you overshoot it then it doesn't work anymore except size is small enough you must get closer to Y so every single step you are getting closer to Y right and so eventually basically you will get very close to it so that's basically that's kind of how the convergence works so the ya time point-of-view weak learners so weak that basically it can't point it basically the only thing you can do is point orthogonal to Y right like if you're pointing here and the only thing I can do is return back just like this then then I'm done then my me glue and I it's just not good enough anymore yeah are we introducing so the question is are we increasing variants that's the question it's actually this is a funny question so the adaboost seems to surprisingly resistant and increasing variants of your classifier so it reduces bias and it's actually amazing at not increasingly marriages and I will get able to let me get to this a little bit at the end again okay last question maybe yeah yeah you will see that exactly in a minute that's right so we basically have to make a different call to the weak learner right because Snoopy is now our after step that Y has changed right that the angle that we want to bias that some sense we have a new coordinate system and now this years our Y right so that's a very good question that will exactly really exactly address this now okay and this is actually where I stopped last time so if you just if you formalized this we have our H let me just write it down one more times if you have it on this court is the sum Chi equals 1 to capital T alpha HT of X and so for now let's just keep alpha constant later on we will actually optimize alpha as well and this is when you get to add a boost to adaboost stands for adaptive boosting where alpha is adaptive that that's the second then then we get an amazing classifier for now alpha is just a constant and so imagine we Paisley run for a few steps it could be you know could be zero right so initially we just say you know T is zero then we just have the old zero prediction we at the origin and the question is which next classifier should we add and so to formalize this we basically saying you would like to find H which minimizes yeah pothinus classes the set of all the weak learners that we could possibly get we'd find the age that minimizes the loss of h plus alpha times H okay so far so good raise your hand of you with me okay good so the last function space your last function that is defined over my ages for example one example of loss function could be L of H equals the sum I equals 1 to n H of X I minus y I squared but it really doesn't have to be the square roots right it can be any kind of loss can mean the exponential loss or something right so it just has to be differentiable it has to be convex so ok so we take this loss and now we want to add one where we take one more step we want to find which H which direction is that is that step so we do exactly the same thing that we did with gradient descent does anyone remember how do we solve this for gradient descent we did an approximation it begins with J and ends with lil Taylor that's right how did you know so this year is approximately L of h plus the inner product of the loss function with respect to H and little H ok good so we can plug this in now I know some people are scared right now there's an inner product over two functions right you may have not seen this before it just don't worry it won't be as bad as you think so it's now that the argument over this these two terms this here is a constant does it's not affected by H so don't worry about it just drop it and what we get here is the inner product of dld H cos alpha so okay good so we know we want to minimize this now what's the inner product between a function and a derivative of a loss function with respect to another function right well we did this in middle school no I'm kidding actually it's not very complicated and so you hey that's you know you can't define this in multiple ways but one way you can just do this is basically say we just evaluate both of these guys at you know many different points and multiply them so if you basically had and because we're in a coordinate system is just endpoints it's simply the following it's simply just be some I equals 1 to N and it's DL the H of X I times alpha H of X so if you why why does why is this correct or why does this make sense think of your function because we're only interested in these n training points essentially I'll be expressing everything in terms of these a and training points we basically say a function is represented by the values it gives for these n training points so the function in some sense is a point in this in this pace right the two functions could be very different outside and these are endpoints they're the same then you kind of consider them the same for now and then this is just the inner product between two vectors and so that's exactly the definition ok any questions about this okay good so I wrote a little something about the nose but it's really you know the idea at the end of the day is that we just think of these as vectors right so each basically for each one of these these coordinates X 1 X 2 X 3 V base you get a value and that gives us a point in this n-dimensional space and we take the inner product between these two points so that's that's totally doable okay awesome so now we are here ok so let's take a step back we're trying to reduce thing intuitionists basil that we take we add up many of these little weak learners that are not very good to get a strong learner and what we need to do is the following at any given time point if you already have a bunch of them found you want to find the next one and all we need to do for that it needs an algorithm that says if I have the derivative of the last function expect to every single point obviously no training point then I want to find the little H that give me the minimum in a product with this vector ok so I get to a few examples in a minute let me first this years may still seem scary so let me just unscary this in some sense well why don't you just do this I'll give you a minute you and your neighbor figure out what this term is for the square loss it's up here ok so that's how you want to want to minimize the square loss what is this term give one minute please feel free to discuss with your neighbor [Music] [Music] alright who's - who wants another minute who thinks he or she has the answer okay good yeah that's right exactly and so actually a cheese little I made up one half in front of it to get rid of two but then it's actually just H of X I - why all right so that's easy to compute right so you have a function H E evaluated in each one of your training points and it subtract from it the label so in the figure that we just had the figure that we just had here's our shoe vector Y and let's say we are he right after our first you know after bunch of iterations we kind of ended up here this is our H all right what is this this is basically this vector here right and so what you want to do is you want to find an H that is the minimum inner product with this vector what's minimum inner product it's exactly the opposite right that's pointing in this direction that's pointing exactly - why okay does that make sense so if we want to find this vector here the derivative of L respect to H is the vector from by 2h we want to find the function capital H our current current classifier we've won to find the little of H it has the minimum in the product minimum when a product means a very negative value so if the arrow goes this way we go exactly the opposite and we're pointing to Y and that's exactly scare like a compact right it's always pointing towards Y right the labels of our training data points and that's exactly what we want to add right and if the wiggler and I could actually do it you would just have to add one more of you doc but the weak learners pretty lousy right that's the problem with you know drunk classifiers right you can have take a small step you know go like this that's like this but it will always point to Y right no matter where where you are so you will eventually get there so just that you believe me I made a little demo and so what I thought is what the crappiest classifier on the planet but I really had to think about like what's the stupidest thing I could possibly do and I came up with something and that was like the stupidest algorithm in the world takes a dataset of feature vectors x and y totally ignores the axis and returns random labels all right that's pretty stupid now you have to be a little better than that because has to be a weak Liana so here's what we do we just you know actually what I'm predicting is just labels plus one or minus one and I try to deliver different versions either flip them or I don't and as see which one gives me lower loss and that's it alright so busy do this like a point in a random direction and say if that's actually oh that's actually backwards oh oh never mind go in this direction right that's all that's all I'm doing clicking a rat blunder drunk guy giving you directions well you like you know how do you get the end of this race I don't think they can't be right well then it must be this way so all right good and so let me explain to you the image so this here's now exactly what I do so this here is my origin the red point is where I am right now that's the old zero vector so initially I'm predicting the old zero vector there's just a three dimensional case so three training points this is the label of the first data point second data point and third data point so here the label is minus 1 1 and minus 1 ok I'm starting here and I'm running my first classifier and of course a spy gives me random garbage right but the random garbage is such that it kind of always points in in the Paisley it's positively aligned with the Trulie like that's the only thing in guarantees otherwise I would have flipped it and now I keep adding them up and so here's what I'm what's happening so what you see here is basically the red points basically moving and down here you see the errors at the beginning every single you know this is the squared error to the Y vector the root mean squared errors at the beginning it's really high and now I keep moving and adding like these are just the dumbest classifiers on the planet right you know anybody kind of looks like a drunk guy doesn't it like you know it can be good seems like oh that was a good night out right yeah you see it slowly getting it right it's sneaking up there I didn't hit it boom right and now at some point you have basically and this is not the stage where you get home and you can't fit the keyhole you like so we'll never be that exact right that's where I get stuck there's actually how you can win character competitions right so if you carry competitions and the prize money is actually given out on the training data right and what you do is just always upload random noise right and if the error is above chance then you keep it otherwise you just flip the the random bits all right and then you just keep adding them up and you will win the competition a bachelor right that's why people don't that some people have secret test sets otherwise it could has always do this cheating and this is yeah there's also related to why in science many results are not reproducible because we make small decisions that basically lead us to to the outcome that we want okay any questions about this demo no okay good awesome so all right so the the resulting algorithm is called any boost and by the way that was invented ten years after the first boosting algorithm so it's amazing in hindsight but it took ten years for people to realize that boosting was actually gradient descent it's exactly gradient descent in function space just taking the gradient we're adding it it wasn't clear at the time and in part because the notation was very different people thought of it very very differently so and now it's it's it's you know it's very obvious obvious obvious so if you look at you note the any boost algorithm basically summarizes this you compute the gradient for every single data point that's I call this part here call this alright and then you call you algorithm that basically gives you a minimum of the alignment and one thing I have to say have to stress is that this doesn't have to be very good right it doesn't have to be exact minimum it just has to be something that's you know it's pointing somewhere in the right direction even works in this absolutely worst case but you just have this random classifier and then you know you just add them up and that's that's it okay good any questions for any boost then let me move to so there's two very famous cases of those that have special names and I would like to go over them the first one is gradient boosted trees and the second one is adaboost gradient boosted trees has become extremely popular because that's essentially what search engines use so if you use a Google or Yahoo or Microsoft Bing or whatever this is the the way they predict what you're looking at is the output of a grating boosted tree and there's there's very good reasons for this and there's actually something I used to work on when I was at Yahoo research and in some sense they are busy just very specific choices about this this boosting algorithm and then Grady Booch the trees the first choice is that it can be regression or classification so we're not setting you know why eyes can be anything let's say the my eyes are regression and emote I class they move the dimensional regression and the HS the weak learners are trees of limited depth so H is decision tree or card tree and she's sorry every question trees it hard cheese off limited depth typically maybe depth equals four or five any four to six but not more than that these are highly biased classifiers right there I'm only allowed to split four times right four five times so each one of them is pretty lousy they're very nice because they're very fast it can be evaluated very very fast and they don't make any assumptions on the scaling of the data set etcetera alright and then the last assumption making is alpha is a constant small constant okay good so so far there's nothing special about why don't we pick you know age to be no y is just a regression problem that's fine we didn't mean you know but but we just but I just we just talked about you know we actually assumed advise you know just a point in this n-dimensional space that's there's not no problem with this alpha a small constant that's what we assumed anyways that's fine this year it gets tricky H is a card tree right but so card trees we found that it can either minimize you know entropy loss genus John Gini impurity or the key minimizes square loss but we what we want to do here is this alignment between two vectors so we have this vector with all these gradients we would like to find a tree that actually aligns with it alright that's that doesn't really we don't really know how to use car trees that and that's the one thing we have to quickly fix any questions okay good so one more time what we want to find is a card tree that solves this equation here let me just write it down our twin h element of H I think these are now the cards depth for whatever card trees such that the Alliance or I of H of X i where are I here is this the gradient of the gradient of the loss with respect to H of X I and you just derived correctly that for the square loss this is just the residual it's just the direction from my current predictor for my for my label to my current village so I want to minimize this function and I want to find a tree that minimizes this function the question is do I have to derive a totally new card algorithm to minimize such a function right so far we haven't had this this is not a convex law strong so this is like a weird thing right turns out actually we can shoo one that's very very easily into our existing regression tree framework so the first thing you gotta notice let me just do one thing let me just save all this here's the lady let's just define the negative gradient is minus TI so the negative gradient this is my figure this is my Y this is my current H now the negative gradient points exactly to Y so if I want to do some other things more intuitive that I want to go to what's Y okay so if I plug in my TI and I don't want to because this is negative gradient I don't want to minimize anymore I would have to maximize maximize I'm maximizing these producers so what we do instead is we just minimize the negative right as we say argument negative I RI sorry TI h of x i okay so what I did is it's very very simple i justified TI is the negative RI and so I just have to put a negative here I also dropped the Alpha because of the constant it doesn't make any difference raise the hand if you're still with me okay any questions so one more time you're trying to minimize this in a product this here's the function we are learning this is a tree there's a you know a limited depth tree that's not very good but you know it's a function these are given to us those are the gradients of the last function with respect to every single point what I now did I just said okay we have the negative gradients and then I get a minus here and because minimizing this is only half as good as minimizing it twice it just put a factor two in front of it why not right I can do it all right doesn't make any difference alright raise your hand if you still hit me okay good so far so good so now here comes he comes a little bit step you have to believe me the first one is that I say sum over all t I is a constant that's independent of my H that's true all right so the T eyes are given to me sum of all the squared T eyes has nothing do with H so can just add it right at t I squared all right raise your hand if you still with me okay good now comes the the last bit I'm telling that some of our I H of X I squared is also constant now this is not true but it's very convenient so here's why we should make it true because if he want to minimize this you could just take any tree and just multiply it by a large number and you get a smaller value here okay because this is linear there's an inner product so essentially what we're saying is I want to find a tree that points to what's Y right but if I just get a really long era right that actually that seems better than than a shorter right well that that doesn't matter because we're just just interested in the direction okay so what we're saying that H squares should be a constant or be essentially saying each square should be lying on a circle of radius C whatever C is doesn't matter so this way we are just decoupling the the length of the vector with this direction so we're just enforcing that we're just saying H of X I although the the the sum of all the squares must be a constant and we can easily achieve this because whatever tree we have we can just evaluate it on all the training points and then just rescale all the predictions such it's a con if you just know we could just make this one and I make this one it doesn't matter okay so we just normalize the outputs so we changed the card tree algorithm t need a little bit if that's the case then I can also add that to it because now it's a constant right this is I'm just adding this constant so if I do this I get arc min here's my constant I'm just adding a constant okay H of X I squared minus 2t I H of X I plus TI squared constant constant the thing we want to optimize ah what's going on who can see it this is let's get off H of X I minus T I squared which is exactly what the regression tree minimizes all right so we're done we don't have to do anything right so all we need to do is just here's our our label a vector bye this the factory here's what we initially have that's a base the one value one label for every single training point we stick that into our tree it sends us here actually it sends us here but we just take a small step and now what we do we just take the remainder right it stick that into our tree tonight okay well that was a good start one more time all right yeah is that okay you know stick in this vector now we end up here right we stick in this vector right let's say we go here and so on I mean we keep moving around but every time is target you just take the residual which is exactly the the thing that's left the direction that's left between our current prediction and the lake and that's all there is to educating both trees is a beautiful algorithm let me write it down in pseudocode how much time - okay good here's the here's the see I just need a small blackboard here the entire algorithm will fit on this blackboard and now you can go out and build your own search engine so here's the idea just say initially H equals zero all right so initially we don't you know by the way that algorithm here i yeah that was you know please ignore that I rewrote that on the on the notes that online so here's what we do we take T steps and every time it's the following we say T I equals y I minus H of X I so we take our vector we take our labels and B subtract from them what we already have that's that we already have in the bank right that's how close we already are we subtracted from the label and let me say okay the next next H you're trying to find it's just the country that minimizes and the square loss with what's left my TI and then I say my H becomes H plus alpha times H and that's it then you say n you repeat this so it's like it's four lines of code as a loop it's one of the most powerful algorithms and the only hyper parameter you really have it's too it's like one is how deep your trees are and the second one is your step size and that's something you have to tweak a level so usually basically deeper trees give you more except steps more except exact steps and you also you have more variants and if you take smaller trees well you know you have to find something intimate usually four to six is good and the step size here if you make alpha smaller you have to boost for you have to move for more iterations well that's really all there is to any questions about Grady boost the trees yeah so is it with certainly with Grady boosting Bo yes but it's four lines of code what are you talking about I can put this inside of here then it's me yeah okay so questions like why don't we just so here's actually that wasn't um it was actually a big competition yahoo actually fell apart which was a couple years ago like you know actually right when I left the pattern part they had a big competition where they BZ took all their search engine data and made it public they anonymize data made it public so all the features and they try to made a big a big competition you know they offered several thousand dollars of price money for the person can build a best search engine and actually the winner what they did is they boosted for a thousand iterations and they did that several hundred times and then they backed those and the result was like you know I think it was actually it was even more that was like you know the result was I think 10 million trees that you had to evaluate for each search query for each webpage so it takes like you know minutes and minutes for each search query to reach Cannell get any results but it was the most accurate by far so you can you know you're right right so in some sense basically they do serve ions per variance problem and then they reduced it again with bagging and so on typically however the variance doesn't go up too quickly and so if you if you keep your alpha and your T in check right you don't make alpha too small and you don't boost for too long you actually eat should be fine yeah I mean the difference of the questions we are not having are completely combining different weak classifiers but these are very different trees though right every single tree would be very very different it's just not yeah we're not combining trees and SVM's or something right yeah yeah that's right but you've really have one type of algorithm that generates you we classifiers and you basically want to make a strong classify out of it all right thank you everybody see you next week 
","['', 'Boosting', 'Weak learner', 'Strong learner', 'Bias problem', 'Gradient descent', 'Ensemble methods', 'AdaBoost', 'Gradient boosted trees', 'Support Vector Machine (SVM)', 'Classification setting', 'Loss function', 'Hypothesis space', 'Training data', 'Labels', 'Regression problem', 'Feature vectors', 'Random classifier', 'Greedy algorithm', 'Stochastic gradient descent', '']"
"okay we talked about boosting and just to recap last time if it's a boost thing what we do is we say our H of X is a sum of capital T classifiers and in some sense what we're doing we basically say each you know it's gonna shoot me each one of these classifiers is roughly you know it's pretty weak but it's better than a random coin costs or at least the poor shows in the right direction and then what boost what gradient boosting does is we we start out at zero and this year is the this is the N dimensional space for every dimension corresponds to a training point and so this year is my Y vector and I would like to have my predictor point exactly to this Y vector at the moment I do this at zero training error and so what I do is I have my first little classifier and it doesn't predict Y you know predict something here but I take this vector and take a step in that direction and then I I really say okay belong to start here now my Y's have changed right so one thing is either you know update you guys or you just say you know I'm starting from here and now you learn another classifier and the points that say gives me this as answer right because they are pretty noisy you take that back turn you take a small step in that direction right and you repeatedly take small steps into the direction of Y and because each one of these classifiers is a little more right than wrong appoints at least from the right direction you have to get to by exactly right so that's that's the that's the rationale basically and that's called gradient boosting so it means you have a loss function every single iteration you trailers classifier that really points you in the direction of the gradient of your last function ok any questions about this one thing that's important is that there's two last functions actually right the ball first are sponsors this last function around why we try to get close here and the second one so if here a certain point let's say we are here and the gradient points in this direction toward wine and then the function that I'm actually calling my weekly honor with says give me a give me a returner that points in the same direction as Miss greedy so the first gradient comes from the global loss function I'm trying to minimize then every time I take a step I call my weak learner by giving it a new base e new target and that's you know then it solves it's only the last function and then grading boosted trees that's the square loss okay any questions of a gradient boosted trees or gradient boosting in general alright okay today we are going to do the most famous boosting algorithm which was also the original one and it's called adaboost adaboost stands for adaptive boosting and to add a boost was actually what was originally invented and people did not realize that what it was doing was actually gradient descent in function space so the order is the other way around right so I just described this gradient boosting actually is a whole family of algorithms you can take any last function and any big learner but that wasn't clear at the time people just came up with adaboost and just you know really you know like that algorithm allowed and once they analyzed it they realized that this was actually a very specific instance of gradient descent and function space and so that took years for people to discover this more general framework which is called you know gradient boosting and a Debus is a very specific instance where we make a few choices the first one is the loss function the last function and add abuse is just the exponential loss so it's e to the sum of all data points e to the minus y I H of X I that's the exponential loss actually remember this from your empirical risk minimization so in gradient boosting we kept that open and the second thing is the step sighs step size and here we no longer set alpha to be a constant instead the optimized alpha take the ideal step so optimizer we call it actually a line search what does that mean that means we have this last function which is basically this exponential loss around our optimal you know minimum that we try to get to and let's say we are somewhere here and you would like to take a gradient step before we be said okay well you know it said that you know the gradient points in this direction or something it's not a little noisy it's go it goes in this direction right so the gradient goes in this direction the weak learner points in this direction and we just said before when we just take a small step some alpha and we just fix it right maybe 0.05 so we take a small step to here in in inaudible what we will do is we will say actually let's say I'm staying with this direction then I basically look at this whole line and say what's the minimum of the loss and that's right here and I go all the way into my loss versus minimize and that's my next step all right Sam and then I get a new week learner ideally you would like to go in this direction because the weak learner you know let's say I go in this direction and again I take a step and till my last as many months all right so that's the idea so each step is raising of different lengths and but you know when you that the good thing is when you finally find a good week learn and they're pretty noisy right but occasionally you will find really good once you take a really large step and because of that adaboost can converge really really quickly and in fact we will prove that it will always converge very very quickly okay so those are the two designs or the two design choices with one more design choice and that's what we end classification so it only works for classification so the Y il iment of minus 1 plus 1 and the O's assume that the weak learners only return us minus 1 and plus 1 so we learn errs they return H of X I is element of - one plus one for all for all X all right so these are the assumptions of adaboost so first you know so it's gradient boosting you have to choose the last function so we pick the exponential loss you have to pick how we set the step size we say we gonna we will do a line search and then we have to set the setting in this case we say it's a classification setting and the week learners will just be binary classifiers plus 1 or minus 1 so it's a very specific setting of gradient boosting and turns out when you do this you get some very beautiful properties and try as I said earlier it was in history it was discovered the other way around so people basically first invented adaboost and then they say well could you also run this with a different loss function could you also run this with different settings here and so on and then they actually that's how people notice that actually this is a much more general boosting is much more general than adaboost ok any questions before we dive into it ok good that's going to be a little bit of derivations today there will be a demo at the end so please bear with me it's really not very complicated it's actually this class is never really complicated and just stay with me he does like it three four steps right and then then we are there so the first thing is we have to go through the same steps as last time and that was imagine you already have a function H we want to know what is the next week learner that we should should get okay so let's see now for now just assume don't worry about but alpha yet one second okay yeah so and when you remember from so he base you want to have alpha plus alpha times little H you want to find the next you want to find the H that minimizes our loss right so you easily have okay so we did this last lecture two lectures ago right they already have a bunch of weak learners which one should we add now right so that's the that's basically the part of the boosting but you call the weak learners they give me one more functional I'm not going through the derivation again phasing what we have to do is we have to take the gradient of this function with respect to H right and so let's just do this so that was that was a previous lecture one second let me just make sure yeah so let's let me define this so the gradient of the last function with respect to H of X I one particular X I is if you look at the last function where is it up here I'll give you a minute to maybe figure it out yourself there's my last function I would like to take the gradient with respect to H of X I any suggestions all right ok you take you take a minute and discuss it with your neighbor I want everybody to figure out what that last one so here's my last function I would like to take the gradient of L with respect to H of X I [Music] all right who can tell me all is the last function someone who hasn't said all that much what's the gradient yeah e to the negative Y I H of X i but actually is not a son because only of the terms are zero because we only take derivative with respect to H of X I yeah so good so where are we in the boosting process right so we assumed or we taking some steps now we are right here and now we want to know which which direction to go next so what we're doing is we're computing the gradient of the loss with respect to every single dimension and that gives us some vector here right that's basically what that is and every single dimension basically this here is the great ok so FAR's are good and let's just call this this is our RI that's how we call this RI in the past and so here's what I want to do I just call just to make notation later on easier I call this thing here WR it's quite actually w hat all right why not and that's a weight so this is basically what is this this is basically the loss if you just look at there's just a loss of this particular data point ok how much does this particular data point I contribute to the last function that's what it is right you sum over all of those that's the loss okay raise your hand if you still with me all right good good good and we call this thing here we do something else we call Z the loss I know would you vote it now this thing here I just call this Z okay there's the sum of all the spaces just the sum of all the losses right that's the overall US and then I say WI equals WI hat over C now who can tell me what's the sum of all the I WI give you a minute discuss it with your neighbor give you one hint it's not Z [Music] yeah it's one exactly it's one why is it one that's right right so this be some overall iw head over Z and what's the summer oh WI hat that's this if I sum over those that's exactly the definition of Z so Z over Z it's what okay and they're all non-negative so ready in some sense that kind of weights right that's why I call them W actually their weights okay good so far so good so just remember what we're trying to do right will be at somewhere here we have our H so far we're trying to find the direction which to go down right to most increase our loss how are we doing this you're computing the gradient right off the function L with respect to every single dimension right sure why not right and this is what we get right okay good and now we want to find a wiggler on earth at its best aligned with a gradient okay would like to find because ideally kind of points exactly in that direction so this is how we find H of X I so now next H of X I equals arc min and this is exactly from last lecture so within our set of weak learners we try to find the weak learner that minimizes the inner product right between this gradient and the output of the wiggler so what we go is we go for I equals 1 to n H of X I times the gradient while the gradient is this thing here right so that's wi e to the minus y i H of X I have a nice okay raise e hand if you understand what I'm doing here that's not many people okay yeah oh sorry that's why yeah yeah I'll take good good good thank you thank you good okay good wait I mean a mistake okay okay you can also do this you can also do stop your head if I uh why okay there now correct okay good okay so one more time guys get attention so here's what we do we are here it is our H of X those where we currently are there's our H now we try to find a direction the gradient points in this direction this here's my gradient right this gradient what does it look like it's like you know r1 r2 RN right that's my vector I want to get something for each R is exactly this I want to find a function the only thing is the points in the other direction because gradients point in the wrong direction now I want to find a function H that is a line that goes the points down here right so what I want to do is I want to find the function H it has minimum inner product with this is my R I so this brings the inner product between the R vector and my H vector all right so I just go over all the mentions let us compute H of X I times R I and so on tonight and that's exactly the inner product between these two vectors so if this is this is very very large then I'm busy pointing exactly in that they're raising and that makes sense now okay if it doesn't make sense please read last like just notes again because that's exactly what we derived last time and you base you have to minimize this in a product okay let's just go ahead from here one thing we can now do is we can actually now multiply divide by Z right because Y naught is a constant right and then this year becomes my WI here okay so given the previous step does that make sense the next step I just took this and just my peptide bond over Z and now I get instead of a WI had I'll just get WI because WI is WI hat over Z crazy hand if that makes sense okay good we almost there guys we're almost there let me rearrange some terms WI why I what is this age of X items why I anyone tell me what those you know what values is that take on yeah that's right either positive either plus 1 or it's minus 1 okay if if this guy gets this point correct it's past 1 if this guy gets this point incorrect it's minus 1 so this is n plus 1 minus 1 ok cool now last step okay there's you were one step we're there all right here we go we now divide it up actually two more steps I like but you know all right tumor steps so here's what we do these are the past one or minus one so what we do is you just divide out the sum be something for all the points where it's positive and then we sum over all the points is negative right why not so he first sum over all the points that's positive I such that Y I equals H let's do this way H of X I equals 1 and this is always 1 this is WI plus now we sum of all the points negative is minus 1 then this is just minus WI okay raise the end of that statement Saints okay awesome good good good good they're getting there okay now can anyone tell me what this is let's count tricky I sound what all my data points actually let's do this one this is easier to interpret as to this term here that's how all my data points whereby I'd H of X I is minus 1 and as some of their weights so I'm summing over all the points that I got wrong and I sum up their weights what do I get just you want to say yeah yeah this is basically this base is that this is no weighted error right lets me see if I give every data point a weight WI and as sum of all the points that I get wrong that their weights and that's basically the amount of weight that I got wrong this is the weighted error this here's the weighted accuracy all right so let's call that weighted error epsilon let's call this thing Epsilon right why not then what is this all right this is the epsilon and what is this is 1 minus Epsilon exactly right so what we get is the following so we have here minus 1 minus epsilon minus epsilon and that gives me epsilon plus minus some constants that I don't care about ok so what does that mean it's actually a really profound result all right let's just put it back in so what we are minimizing and some of all I equals one to n I'm sorry H of X I not equal Y I WI let me illustrate to you what that means if they have a data set all right I can do this it was really fancy the everyday a set of positive and negative points and each one of these points has a weight associated with it so let's say this here is 0.3 you know whether 0.05 0.025 or something you know 0.025 etc every point has a weight associated with it I can also draw them smaller and bigger over this and this guy is a big weight so it's big right this guy's a small weight it's tiny okay and what I'm trying to minimize is the weighted error that's it all right so I want to find the H that minimizes my weighted error right which is basically here my all data points that I got wrong and I summed the weight of all the points that I only grow that's what I wanted minimize okay that's the idea and the weights that we find a very specific way but it doesn't really matter right so this is my definition of the weight where is it somewhere here where no it's I guess it disappeared well but it used to be there it was beautiful all right so so what do you need for adaboost is an algorithm that takes an X's and Y's and weights and it tries to find you the lowest weighted error I know many if you can do this then they can do boosting any questions yeah oh no oh no oh no so easy initially oh yeah okay good initially right so initially what he's saying is my H of X is just 0 or something in this league right well then each of these wait is 1 right and that's that's great that's what you want initially we get a data set and everything is equal and then what adaboost us this basically the idea via data point everything is equal now we run the first classifier let's say you put it here we say oh this is positive or this is negative these points here all correct these three points are wrong this point as well so what will adaboost do it will now increase the weight of this guy by a lot increase the weight of this guy and this guy I just say like look these guys are much bigger now nice and now it runs another classifier and say you know now run it again try to get this data set right and now I won't miss classify these data points anymore because these are now really really heavily weighted these are very small weight in comparison so now you get a different gasifier maybe it does this right the face says these are negative these are positive right and now this guy is misclassified so what okay now they will give this guy larger weight and so on right so every iteration you reweighed your data points and you give those data points that you're still getting wrong more weight and those that you're getting right lower weight and not only this we will get to this in a minute the weight increases exponentially right so if you get a point wrong repeatedly at some point you will all the weight on that one points like damn it I'm gonna get it right right until you get it right and then it will back off again all right so it's you know but using this that you know by doing this it has a lot of power right it can be enforce that every single data point no matter where it is really eventually get the attention it needs to get right all right that's basically what's going on that's the intuition behind adaboost any questions yeah minimizing the yeah that's the same thing right so minimizing the error is the same thing as maximizing the accuracy yeah at the same thing right because one is one minus the other so the accuracy is one minus the error yeah absolutely yeah the question sorry said again hold on yeah no but the nice thing about adaboost is you can take classifiers that are very very like they're very very simple right and so they would get a lot of points wrong but now the next time you take these points and made them higher right so boosting SVM's doesn't make too much sense actually right because they're already as you saying they're already doing that right but they're specifically very big there's various reasons why occasionally you want to use adaboost right and in part because decision trees that has great algorithms so you can just boost trees but you can actually boost anything right like justin SVM's you won't get much benefit anymore because they don't really have a bias problem right it's really this is a bias issue right so you're the classifier is a bias towards these lines yeah last question yeah so the amazing thing about adaboost and then when I started my PhD that was like when you into a conference this was with everybody like you went for a beer afterwards but some people from the conference everybody was talking about why does Adam was not over fit and nobody knew the answer and it was this open question like everybody machinery was scratching their heads about right it was reducing the error so quickly and it was so powerful yet it didn't over fit right and the curve just didn't look like it like you know if you looked at sorry yeah in the number of iterations there's number of iterations and the training error went down like this very very quickly in the testing error looked like this it didn't go down and nobody knew why like every theory like you know we had no reason like nobody could make you no rhyme or reason about and then actually took a long time and in fact the computers just said get faster it's a bit lazy at some point people actually what they did is they just kept boosting for a really really long time and then when they saw it then it slowly goes back up slowly and then actually and to tell if you plot this on a log scale we just stay here log iterations that actually just looks just like any other code so basically the amount of overfits actually does not you know busy a grows at a local rhythmic scale there's good reasons for this why this is the case so yes they do have a fit but the beauty of adaboost is that it over fits really slowly which gives you a lot of flexibility to find the sweet spot when to stop and that's why it's so so so powerful right it's really hard you know if you have so much time in the world you know like you know it slowly signs of a fit but yet you still keep boosting for another week right well that's really own fault at this point so you really get you know very good heads up right that's very different to decision trees the decision trees they go down a BAM go back up right so they over fit like very very quickly yeah and actually this is what you know Rob superior actually so he invented boosting and then he went to Princeton and for 15 years he basically worked on nothing else in charge of video mister now you wrote a book about it I think there's multiple chapters on that question so it's it is counter-intuitive and that's why and it was so surprising xxe I can explain you I doubt I can yes here's the intuition the intuition is that these alphas get really really tiny right because you basically get closer and closer to the to the loss so at the beginning you take gigantic steps but because you do this line search right you basically there's very little to be gained for each step so you add very very small alphas so actually you're doing a lot less right and as you keep going your office gets smaller and smaller so the more you were adding them less you actually changing is that does that make sense so you take AG at the beginning you take really large steps then you take many many small steps so that's kind of this automatic built in system against overfitting all right I have another derivation for you guys but I feel like people are kind of done with generations when was the demo first and then derivation I was like who's for demo first raise your hand who's for demo second some people are hardcore all right good you know what I think it's it's a tie so I can choose all right good so here is again one thing I want to show you is okay first the good old demo where I play some dots and so let's make some challenging data set this decision trees and boosting some decision trees is a very little decision trees so anything suggestions interleave spires okay you guys are nasty all right all right okay good does I think this is actually against you know some conventions treatment of algorithms but okay alright to do this just make them a little meet them yin and yang that's more positive and okay good oh yeah okay hit shoot so what are you seeing yeah this is a decision boundary and it adds a little classifier right and it's trying to be used to get the points right and it keeps reading them and so this point actually got all the points right already alright so the error is zero the arrow on the right hand side is the black line the red one is the loss so the lost keeps going downstairs but still actually adds classifiers to the ensemble all right and it basically finds the decision that the decision boundary right so keeps adding trees keeps adding trees and what you see here the red thing is where it's very very sure it's positive blue thing it's very sure it's negative and so you know here's kind of the regions where it's not entirely sure right but it gets it right right the way the cuts are nicely around us this the spiral right I mean this is really a really good decision boundary in this case I think makes a lot of sense right and the amazing thing is this just build all these very simple trees right so just by adding them repeatedly you know and then do you see you see you question was like is it going to overfit right do you see now actually like the decision boundary doesn't go crazy right it doesn't do silly things right like decision trees we saw they kind of did really silly things to cut out one one point here you keep boosting for two hundred iterations now I'd and really cut it out very very nicely and this is probably a you know a reasonable decision boundary let me maybe do one more example now that you know what to look at any other data sets centric circles or that's easy but that's too easy actually because decision trees can do that by themselves right that's you know it's like that boosting algorithm will laugh at noise okay oh you nasty okay you got it so here's here's the here's the problem with adaboost Adam who's actually anal retentive right yeah so and there's this little point that's mislabeled it just can't look past it right it's gonna give it all the way it's like like a little piece of jurors right at your grandma's house it's like now I gotta clean it away you know that's that's what you get all right let's all right here like very prominently here actually it's probably to you it's putting right in the center oh no that's so nasty I feel bad all right okay all right okay here we go here you go but I had to do I was forced to do it okay here we go see it's cutting out right see but it's the last is one point it's fighting it's fighting around that piece of dirt it keeps scrubbing your now get it right they got it you know they actually got it right they cut around it but it will not stop until it gets every single point right right it just dumped on it just gives this point all the weight in the world and then yeah you know it always gets a red okay yeah yeah I actually have another demo but it's not very good this is actually yeah I don't like this demo but I show it to you anyway umm right what is this and there's actually so now what you can see is actually the weights so let me let me do the spiral again because it's it's it's interesting it just gets very messy so I tried to make this look cool by making the points large but it didn't work so I apologize this below my demo standard like I only show it because you asked for it all right here we go so that let's do this what do you see a CC that's weights you see some points are really really large right like a lot of points of AZ disappearing right you see these points in that way it's really sure they're disappear you can't eat that below one pixel now alright it doesn't care about them anymore right but it's still fighting around these points in the decision but you know where the decision boundary it's right there just you know it's just obsesses about them right and it keeps getting back and forth between them so there's the adds one that gets a circle ride and then it gets one that gets crossed right and you know it gets them yeah alternates between them right it's at some point you see these in the dry that's quite nice these these crosses on the right it disappear then it gets them right and then doesn't care about them anymore I see now get them wrong again right so then it's like oh wait a second right see little like whack-a-mole right whatever that is all right okay any more questions yeah when in the sub adaboost never stops adaboost will never stop because the loss is always none it's always nonzero ID because each of the something but it can never reach zero only in the limit will it reach to europe so adaboost ball you know we're all dead right adaboost will still be boosting anymore questions okay i can show you it show you one more demo this bias variance bias variance boost so here's the good old data set off I should let me show the data set okay good so this here's the data set one more time so I have positive or negative points and I try to separate them and I believe have very simple decision trees that just have one split a bit so it's a little bit artificially easy data set but it's very noisy right so this is actually this is quite bad for adaboost right so this kind of this is what I can't do all that well and what I'm sure you now is the decision boundary that the bias-variance curve and so there's actually not a great example but what do you see is initially all right the bias is really high right and the first iteration of boosting after three divisions of boosting the bias get you know half right it's now you know from point O to 2.01 right the second thing that's very interesting is as that goes down the variance is the black lines the variance the variance does not increase right this is actually quite noticeable right similar to bagging bagging we'd use the variance but it's not actually increase the bias adaboost is pretty good about not increasing the very variance right eventually you will go up but this takes a long long long time right so here actually maybe the arrow goes down very quickly and and so the bias comes down very quickly here at this point it doesn't you know it goes down very very slowly because it's a very noisy data set so what happens it is just obsesses over the noise right so adaboost is not great when it's been it's too noisy and that's the weakness of adaboost and but you know I mostly want to show you this for this beginning here you see this massive drop in error right this is a ye buddy a huge drop right it was just a few iterations of who's this by the way the test error there's not that the training error the training error has long been zero at this point right any questions yeah yeah with this data said it was hard to find at the NSA that SOPA and I wasn't that so biased it's not not generally true it's just because I have this a simple 2d data set nonsense actually a linear you can except a line is actually the optimal classifier okay good um RC you can do one more there's actually the spiral there's actually this is a pre pre is a can spiral spiral okay I forgot what that demo is say oh yeah okay so this here is actually a there's a more complicated spiral and what do you see here it's basically exponential loss goes down very very nicely the training error is already zero right and and and so here's busy the spiral in some sense it cuts out this fine this is a very sparsely sparsely sampled spirals of decision boundary doesn't doesn't look as cute as it could be but one thing is nice the arrow tendon testing error does not really shoot up right this is really I think something you should know right so as you start boosting you get this initial drop this massive drop of test error and then it doesn't really go up very much right so the boosting is really very forgiving right so you can boost for quite some time and then find the lowest validation errors it's not a problem the other thing that I really want to emphasize is we haven't shown this yet its loss is actually an upper bound of the training error so once the loss is lower than one over n you know you must have zero training error right because it's an upper bound the loss is always larger than the training error and so the last one go down every single iteration so that is why boosting will always give you zero training because you're driving down the loss and the loss is an upper bound all right any more questions all right so maybe we spent the last few minutes looking at the proof of how to find the step size alpha and the most amazing thing about this is and it's going to show you the problem setting and then I want you and your neighbor to kind of look at it independently not independently by yourself and that's the following imagine you now found this classifier H right we now know how to find this classifier H we just get our weights we give the weak learner the weight of the data and it returns us via big learner right so now what we like to find so now we have this H if we would like to do the following this is our new classifier H is done this we have from the previous round alpha we don't know so we would like to know the alpha that minimizes this is the line search that minimizes this expression so what that means in the space here we have our point H we have our last function it gave us a gradient it gave us an H that's eight points in this direction because they're pretty bad and now you would like to find how far do we have to go to minimize this function right and - turns out this is actually a closed form solution so if you do this it turns out alpha is just one half log of 1 minus epsilon over Epsilon that's it where absolute the error this is the accuracy over the error that's basically what it is by the weighted accuracy divided by the weighted error you take the natural log and times one-half that turns out to be always exactly the right step size the proof is here under finding the step size so my Devon and you and your neighbor just you know start reading through this and make sure you understand every single step this by the way by my adaboost is such magic right because this year's trivial this is one line of Juliet right two lines of Python right so this is really really simple to implement Baraboo's to implement our abuses with literally five you know maybe seven lines of code and by the way that's exactly what you will have to do in the next project so that's coming out today maybe tomorrow morning you have to build in decision trees and adaboost and tagging now everything just works out so beautifully because the exponential loss that's right they also have other names as logic boost as logistic loss right so for most people have now analyzed all of these last functions it just turned out that a Debus it was the first one actually has the most beautiful properties so often it does not often it does not have a closed form solution or you you can't we could do a line search yeah you could just try out 30 alphas and pick the lowest one it's not a closed form yeah all right good and time is up why don't we continue this next time please until next lecture please read through this little proof and you come in with a head start 
","['', 'Gradient boosting', 'Weak learner', 'Loss function', 'Step size', 'Gradient descent', 'AdaBoost', 'Exponential loss', 'Classification setting', 'Binary classifiers', 'Line search', 'Weights', 'Error function', 'Accuracy', 'Overfitting', 'Decision boundary', 'Training error', 'Testing error', 'Validation error', 'Upper bound', '']"
"finally I can sit down comfy - please put your laptops away all right thanks - ever put that chair yeah all right a few logistic things so first thing is the carrying competition seems to be going strong people are doing extremely well a little bit - well actually so several people these three times three teams have 100% accuracy on the test set yeah has xkcd toe and monday.a seems very suspicious wanting its 100% accuracy just after one try so I don't know how that is possible that's not how the world works usually so either you extremely lucky that night means I'm tipped I don't know what that means but I can read this part tensorflow or shouldn't click on this okay but yeah so basically please everybody who does hasn't submitted yet you know it seems like there's actually a very easy way to move up your grades so don't don't let this you know don't waste this opportunity I talked to some students what they're doing is very very interesting so that's you know they're really using the tricks they learned here that's really awesome so then there's a new homework assignment out the new project and so someone actually asked on their tumor projects and the answer is yes they're sewer projects and one person is excited so but don't worry so that there's project 7 which is car trees and that is amongst the more challenging ones so please take that very seriously that was pushed out yesterday so what do you have to implement there is I'll just bring it up here no that's the other one here car trees so you actually have to make the implement trees and then you have to you know you visualize just a normal car tree on a data set then you have to implement baggy this year is bagging and you see the error as the error goes down so this here's number of iterations that you bear back for training and testing error and then you have to implement boosting and so here you see the testing error go down as you keep boosting and the training error in this case on this date I said actually boosting outperforms bagging by quite a bit and you can visualize the decision boundary off boosting so you have to put car trees you have to have been bagging and you have to implement boosting adaboost era so getting car treats try to take a little bit of time i actually revamp the whole assignment this last time the feedback was this assignment was too hard so i made it easier this time so it's only regression trees but and in particular if you've done the homework then you should have actually had you know done all the math that you need to do implement these regression trees efficiently so you know it's basically what you did in the homework just implement that so it shouldn't be that hard when you're done with this you know take a well-deserved break of an hour and then switch over to the next project and that's will be what we cover starting very very soon there will be the next topic the next project aid is very easy so I'm not just saying this really is very very easy right it's kind of like remember the perceptron project it's easier than the perceptron project right so it should only take you an hour or two hours so just don't leave it until the last minute in case something goes wrong but you should easily be able to do it in a couple so people only give you a week to do it but you should easily be able to do it it's really not hard alright so then if you might think so you know a few weeks ago we did this midterm evaluation and at the end I asked you what is what do you still want to cover and you know so people put in all sorts of things so here's a list of things you wanted to see as a deep learning deep learning deep learning deep learning deep learning deep learning neural nets new enhanced neural nets and have the feeling you tried to tell me something so I decided to have the last cup lectures is actually all along this deeper and deeper and deeper in deep learning it's um you know finally the end someone says kernels so it's a tie but I decided to go with deep learning because it came first so what I want to do today is actually I want to finish up boosting and in fact what I will do I actually have if you look at the notes there's a lot of stuff that we haven't covered yet but a lot of it is just derivation so it's actually the high level is quite in quite simple and you just read through the derivation so what I did is actually add a lot more description I in the lecture notes and I just trust you that you read through it and if we do this deal that you read through the lecture notes carefully and you make sure you understand the derivations then I don't have to do the derivations in the lecture which is kind of boring anyway okay there's not that much value in going through it in the lecture and you just because you often lose half the audience and then you can just move on today so let me briefly go over the lecture notes and just tell you because you have to do it before you have to know it for the project so remember last time we basically talked about boosting and then B and boosting okay where are we he basically adds you know let me just go to the very first equation here okay he basically have a classifier that basis has some of many different classifiers and we have to every single term we add you know we have our previous classifier capital H and we add a little weak learner little H and then we solve this what we realized is what we have to minimize is the inner product of this little H with the gradient of the last function and so then the events of two different case ad the first one was gradient boosting as two lectures ago and last time we did add a boost and adaboost we did two things so ad performed a line search so it remember is let me just go through this we're trying to minimize this function this here's my function as he has the minimum and I'm at some point here this is my current H now I get a great information which is basically you know pointing downhill for that function and then I get a Lee weak learner that has aligned with this great information so I try to approximate this with a weak learner it's not going to be great so let's say points a little bit off and then what I do in adaboost is I you know I don't just have a small step size I don't just take a small step instead what I'm doing is I try to go as fast you know exactly the the longest step to minimize my function that's if I look at this the slice through here what's the minimum value of the objective function and because the objective function is convex I can actually solve it in closed form and in particular because the exponential loss so I jump right here alright and then I can basically now go take the next step so that's one of the reasons about adaboost converges so quickly and turns out if you solve for this if you basically say what's the best officer here's an optimization problem which i defined the alpha such that if I add my weak learner with that step size I'm minimizing the loss all right so let's face it this equation here and if I go through this math it's relatively straightforward it's just it's just algebra then what I see that actually the end alpha is 1/2 log 1 minus epsilon over epsilon where epsilon is the weighted error alright and that is beautiful because that means we don't really have to do anything right so typically when you try to find the right step size what you do is you try many many different step sizes and pick the minimum in this case we don't have to do this you get this magical equation that tells us exactly this is the perfect step size for this particular weak learner with this error right it's amazing it just takes as input the weighted error and out comes the optimal step size and this gives rise to to the following algorithm so this year's adaboost and this is exactly what you have to implement in the project and you see now you can see why it's became so popular it made everybody go crazy when it came out because it's just a few lines of code right so let's just quickly walk through it so you start out initially you have the zero prediction and the WI is this is the weight of every single points you give every one of your training points the same way it's just one over N and then what you do is you say I'm calling my weak learner that minimizes my error approximately of my data set x1 y1 to x2 and in my end with and this is a typo here there should be WN with with these weights so Bayes they have a weighted version of my data set every single data point has a weight and my algorithm my weak learner algorithm takes as inputs these weights and gives me a weak learner for this weighted data set and then all I need to do is i compute the weighted error so I look at all the points that get wrong and I sum up their weights so that's pleasing the fraction of weight that I got wrong and if that's if that's greater than one half then I stop because that means that actually you can never be greater than 1/2 then I can just flip the predictions right so but it can be exactly one-half up to my seat machine position then I'm toast but as long as less than 1/2 that means on average I'm doing a little better than random that's that's my requirement right so you have to have a weak learner that an average or they did that SOI do that every time you give it a weighted data set a binary classification data set that's weighted it does a little better than random coin testing right it can be arbitrarily you know small and how much bet it does then I'll just get my step size and I just add this classifier to my H and that's it and that's one more thing I have to add recompute my weights if you think about this is very very simple it's just I just you know if you remember the weight was actually just the defined as follows the weight was this term here so it's just e to the minus H times rise is just the the loss of this particular data point so now because we added alpha H to it we also have to add this in the exponent so we just multiplied W with this term okay so my W now gets Paisley my old W times this term and if you this is actually very important what does this mean alpha H times y is either plus one was negative one right because both of these are either plus 1 or minus 1 so if this years if H of X I agrees with my ice so my little my weak learner gets this point correct what happens and this year is just 1 right it's 1 times 1 or minus 1 times minus 1 is also 1 right then what I'm doing is I take this weight that I have and I multiply it with e to the minus alpha what does e to the minus alpha it's pretty small okay it's a small number because alphas my step size into the - something is small so what I'm doing is I make my weight a lot smaller if I get this point wrong H&Y doesn't do not agree in sign then this e is minus 1 minus minus is positive so I get e to the Alpha so this means I take my weight in a multiplier but each of the Alpha so it gets larger alright and so if you do this repeatedly if you get a point wrong a couple of times in a row every time you multiply with e to the Alpha right suspect you know alpha can change but so basically this is an exponential growth and weight and the other one you know has an exponential decrease in weight so what happens if you get a point wrong basically Inc the weight gets so large right that eventually the entire dataset will just consist of this one point right it's just like get this one point right right and the next big owner will get it right eventually and that's the magic of data boost right so basically always focus on the data point that you can still get it wrong and just assigns more weights to it and because it's so aggressive with the weighting it converges very quickly can anyone tell me a downside of this yeah if you have a noisy data that's exactly right so if you have a mislabeled data point right so let's say you know you caste if your data said everything is fine but one point just accidentally flipped the label but adaboost will not be able to look past it right it will just get that it will make the entire decision boundary go all the way into that region right and and get the rescue that one point so that that's that's an inherent limitation of adaboost if you have noisy data you know this is it's not the right algorithm right but if you have view trust your labels are abuse can be amazing Akram that works really really really well alright and down here we developed the divide by this term - episode 1 minus epsilon and so again I'm not going through the derivation here but it's actually very easy to show that's actually the normalization so if you sum over all that's exactly the loss so you sum over all the weights that actually is exactly this term here so you don't even have to do that any questions so the pseudocode is really just you know these these five steps here right you call you waited your algorithm to get me the mated data set you compute the error could be the step size added to your classifier re-weight the data points repeat all right and that's what it is well that's what you're doing for regressions you cannot do that abuse for the regression now there's no such thing then you have to do regression Pusa Jimena other boosting algorithm can do it but the adaptive boost thing does not work yeah good question okay any more questions alright and finally let me just go over one last buta that's this kind of the last thing if you if you're not convinced yet that add up was the most beautiful thing you've ever laid your eyes on here comes the last thing that will convince it's gonna blow you away now can you take it all right wait it's not here yes it oh I haven't where's my proof one second and no problem oh shoot I stashed my it changes how does a good era alright that's that's horrible alright I will get I will recover this and sorry about this you won't see the most beautiful thing you've ever laid your eyes on but I can't that's really bizarre I see okay so I guess I had a conflict with my TAS on get I see that the get sucks sorry so just get this horrible I mean it's great for Co it but it's horrible for lecture notes okay so yes yes let me just let me just tell you let me just tell you why I okay let me just close this I have to tell you this last thing yeah okay it's here huh okay [Music] so in the notes so it's not here right now but the moment I will recover my get mishap I will be back back on the notes it's easy to show two things so the loss of a classifier is some of our I each of the minus y I H of X I okay in the error of a classifier the training error is the following is it's called this error of H equals some of our I equals 1 to N Delta H of X I does not equal Y but this is either one if that's the case and 0 otherwise now I'll give you a minute try to prove to me that this here is an upper bound of that so claim the loss is always larger than your error right so maybe you spend a minute you and your neighbor try to prove it it's a one-line proof half a line and Julia all right who has a suggestion how do we prove it yeah the max on the error can be is n okay that's right so that's true that's true the maximum is larger here but I guess in the so that's placed in the worst case the the loss can be much larger but the question is can the loss also be lower it could it be that the last ever loves so the max that's true you're totally right but I want to show that actually on every possible K it's like even out of the maximum this must be Raja yeah yeah you've you go in the right direction that's right so so yeah so yeah you have exactly the right thing so basically as two cases let me just write down there's two cases for any X&Y first case H of X I equals y I right so they are basically positive then this here is e to the minus in this case they match so it's just e to the minus H of X I I guess well I guess the pot enough what's the value here and on the right hand side if they that's zero right so no matter what this is up here there's always going to equal zero right so if they're correct this Delta term is zero well e to the something is always greater equals zero those will be affine right so for all the points that are that are correctly classified we're fine second case is H of X I does not equal Y right well in this case this here equals one and this year equals e to the something positive right so H of X I well that's actually you know each of the zero is one each other anything positive must positive must be greater than one right so again the left hand side is greater than the right hand side all right so in every case we are always greater than the right hand side so the loss is always greater equal and then the error all right so that's great because we're driving down the last very very quickly and therefore we must be driving down the training error right so we know certainly and here comes the cool thing we know that the moment the loss is less than one we can't have a single training point training error anymore right so if this is less than 1 this must be less than 1 so it must be zero right because the smallest it can be you get one point wrong and so now if you look at these notes which will be online this afternoon and it's very easy to show like five steps that I'm just not going through right now because it's not it's easy but not very exciting that the loss actually the loss is bounded above by n times y minus 4 some terms gamma square to the T over 2 so what does this doesn't matter this here this very small number so this here is less than 1 is a positive number less than 1 to the power of T T is number of iterations you boost so loss goes down exponentially with the number of iterations you boost so it goes to 0 very very quickly right so every time busy comes come slower by that factor at every single time right you multiplied by some number that's less than than 1 so it must be less than than one bear after the final number of steps in fact if you've solved this you say when is that less than 1 right you could solve this and what you get is that after T is you know log n after log n steps so easy after log n steps because the loss is less than some term that goes down exponentially so this term here looks like this it goes down very very quickly they see on the right-hand side the loss is lower than that term in the error is even lower than that term now because this year is very quickly less than 1 the loss must be less than 1 and the error must be less than 1 that means we can't get a single point wrong anymore and that happens after log n steps so log n step is not very much as tiny right so if you burst boost up like after couple of iterations you will have no more training yeah and there's a guarantee all right it will always happen and that's the amazing thing that's that so people kind of blew their minds when they first saw this yeah yes there is best arias because and it's a good question actually it's a very good question but actually what you're doing because you're minimizing the loss and by minimizing the loss what are you doing you are still increasing age like Paisley after final numbers like after you know log n steps you will have that the signs here are correct so everything is in the right side but then what will happen what are you doing next right you're pushing the points that are close to the decision boundary further away right and so that's what you're doing you're building a margin so adaboost actually becomes a large margin classifier it took people years to figure this out actually I came at the beginning people had exactly that's like you stop when you have zero training error because people hadn't didn't hadn't discovered a large margin yes only when when SV ends came along with a large margin then you know people kind of realized oh that's exactly why adaboost keeps lowering the test error even after you have zero training error people thought the moment you've zero training error stopped boosting because you're starting to overfit but actually it's the opposite good luck buddy the margin so you actually get a better generalization yeah the loss will not never be zero the last is each of the something will never be zero only in the limit no no it will keep going down by the last Paisley is e to the think about it e to the e to the minus y I H of X i if this is my decision boundary have a point really close by right this he has the right label so this e to the minus something this is small but if I move this point further ways if I move to the decision boundary further away from the point this will be even larger so the loss will be even smaller oh yeah so age is the sum of many alpha times zero and once yeah yeah very good point very good clarification that's right right each one of the Middle Ages is plus one or minus one but H actually can be can be R is a real number yeah very good point otherwise you're right thanks for asking that yeah any other questions okay last last chance so okay what I handed out is a so people asked for more practice questions so what I give you is a practice question so um I thought instead of doing this in class now you can do it at home and see this preparation for the exam and so what I did here is basically I started out with the data set the Blue Cross classes and red minuses and at the beginning everything has a weight of 1 minus N and then you've read the first classifiers that's making this this how is this vertical line on the left hand side divided and positive and negative now you see some points I'm misclassified this three little circle time is classified and what you're supposed to do when everything around you have to suppose to compute the error so this epsilon one in green below it and he's supposed to compute the step size and he's supposed to compute the weight for every single point and then you can do the next iteration the next iteration then we found the next classifier which is another vertical line that's in the iteration two in the center you the left hand side is the first classifier and the middle one is the second classifier and then again please compute the error and the step size and then comes the third classifier in iteration three that's on the very right and so if you keep computing these then at the end base you get the final classifier and you see every single point is correct right so but if you walk through the motion it should become very very clear in your head what's going on any questions about this problems that I'm what to do yeah yeah that I mean actually because he weighed in by alpha it could actually it could actually happen anytime yeah yeah oh I see once you have a training so actually there it's a very good question he's asking once you reach the point of training zero training error don't you just move around that point right actually peak be careful this here is not the point of zero training error this thing is the point of the minimum loss actually be easy get in this case like Shahadah boo is actually really far away right you don't get there at least the step size gets smaller and smaller alpha we get really really small at some point you just creeping up to this it took me that he already you have zero training error yeah okay any more questions all right if that is the case then we can now finally I can answer your your calls and we get to deep learning and so on I could have apologized today you will not get deep learning you will not get printed notes because I'm actually rewriting that part of the class so the last one thing you don't know is the last nine years everybody always got handwritten notes so you're just only getting at the very last lecture so once because deep learning changed so quickly it's I don't want to use last year's notes for this if you can't read my handwriting you're not alone all right so a lot of you have probably already heard of deep learning and neural networks and actually before I get there I remembered something I want to mention so lunch and last time at lunch of students someone asked me about I posted some demo of some bike demo of the other GP that's - GPS - Gaussian processes the other one is genetic programming and so someone asked you know I'll be covering genetic programming or evolutionary programming and the answer's no absolutely not and I just want to maybe give it with two minutes explanation so genetic programming is something that's very popular and it's probably also something that a lot of you have heard of and it's like you know that random forests kind of always the second best algorithm genetic programming is almost always the worst algorithm that still kind of works and it's kind of like the Bud Light right the bottle either but she nothing out of those and I said analogy actually works pretty well because you know you're wondering like why do people drink about lights terrible beer right but why why is that happening and because it's like because of marketing right I guess right like people everybody knows it so is that all why not right I guess sometimes the situation reasons have no other choice and genetic programming is similar so it should have died out a long time ago right it's really a terrible algorithm like it doesn't make a lot of sense and it's my personal explanation is lazy it was invented by people like machine on Experian intersection of statistics computer science optimization Theory information theory and sometimes people reinvent things that you know because they don't talk to other fields so something but for example fuzzy logic was something like this rather basically people were very familiar with logic tried to incorporate uncertainties and they you know instead of using statistics they asked talking statisticians who had like you know decades of experience with you know dealing with certainties they kind of came up with their own way of doing logic with uncertainties and it didn't really work nowadays nobody really works or very few people were from fuzzy logic and so genetic program is similar right to some people who face it didn't really embrace optimization theory which is really at the own field and has very smart people working on it kind of came up with this idea and the idea is not bad right that's basically inspired by evolution so the idea in some sense is you try to minimize that function but I don't really know what that function is so I can't take gradients or something so what I do is I try out some points and I get a function value try some other points get some function value some points here and then I really say okay well let's mix fix these points up so I know this one does pretty well so let me basically kind of say well these two now you know get married and have offspring you know they're not get married actually it's a little old-fashioned but easier I'm yeah and so amazing you say okay well let's just try out two random points around here basically right and that it's a terrible way of doing it or I'm trying not to judge okay that's a suboptimal way of doing it because because you're not taking any information into account right so basically what you're not doing is you're not taking the count well here's the direction right that's very very interesting right and so maybe if I follow that direction that Phyllis keeps going down right so what joshan processes for example do is they would fit a line around this say here I'm really uncertain right so maybe I should explore something down here right that's very informed that takes a lot of second order information about these points in the account etc so genetic program doesn't do any of this right so because of this it also has no convergence guarantees well has it converges but you know occasionally and now it doesn't mean you know it doesn't work at all right so sometimes all you need is some reasonable you know solution of your function and so it works it's just never the I've never encountered a single setting whether it is the right algorithm to use or the optimal algorithm to use all right and the reason it's still very popular is because these that's my explanation is that it's very intuitive right it has this very cool name like genetic it's a branding is that Bud Light right there's always you know Superbowl commercials right good-looking people on the Beach Street in Bud Light you know yo that sounds pretty good right so it's kind of like this right so so we all know evolution worked really well so why shouldn't that work well on computers right and of course computers is very a punch optimization in computers is completely different than evolution so the the you know in evolution you have trillions of organisms who kind of mutate and it never happens that they all get together and say okay let's do it's now it's kind of look at the gradient descent directions and it's all make an update Oh everybody spread out again all right all right that doesn't work has to be completely distributed but at the moment you have a computer you can always kind of do some centralized update and you get a lot more information right if you incorporate this also you don't have to be it's not that half your organism suddenly die because they're meteoroids you know comes in or something like it's just a very very different setting right so that's that's why I don't really go into it and if I ever find a setting where it's actually the right algorithm to use then I will incorporate it in my class but I've been saying this for many years has never come up with a single one and no one has been able to show me one the demo that I posted by the way it's kind of cute so I may look at it and that's on the web page it's you know little bikes that basically have to write a little track and then the one that's furthest you know has a higher chance of basically getting offspring etc and it looks cute but actually what it really shows us how how slow session optimization problem is like they just watch this twice once or twice you see you know that actually you know what the right configuration would be but actually the algorithm buddy does not discover it and it's still like you demo and it was actually it was in fact written by a high school student right and then in some sense shows it right like it's it's it's it's awesome I that I has costume did a really good job at implementing that genetic programming demo and so it's really also the kind of thing that you can do when you're in high school right so you don't need a stats degree you don't need something to know something about optimization theory but if you do then I think it's not particularly interesting so anyway I just want to say this because I know there's always every year as a lot of people want to know about genetic programs and I and that's kind of all I give it any questions all right good new networks so newer networks also have a branding problem and just quickly in your networks were invented at Cornell University by Frank Rosenblatt in 1963 I believe and he did back then didn't call it in your networks he called it multi-layer perceptron so it's also called multi-layer perceptron or as a few years ago it's called deep learning it's all essentially the same thing so it's actually literally the same thing multi-layer perceptron was against the original name you know perceptron we get a little bit of a it has been it has been renamed a couple of times perceptron got a little bit of a bad rap because of its you know because of the AI winter when it was renamed to newer networks that was really here there was a big brain inspiration and when it first came out people thought that new artificial neural networks are doing something that's similar to the brain and that's really not really the case it's if you squint and look at a very tiny brain maybe that's the case but there's still a lot of ton of things that are different if you you know and then that makes a lot of sense because the hardware is very very different and I think actually a lot of a lot of inspiration that came from the brain but people thought oh this is how the brain does it so maybe we should do this and computers automatically I think there are mostly distractions to be honest like it's rare that I does anything of any of these leads have followed up and neural networks were very popular in the 80s so in the 1980s machine learning was there's basically two approaches to machine learning logic based machine learning that was and newer networks based machine learning and in fact the people that logic based machine learning they did not recognize new network as a proper thing and they did not accept them at their conferences and so the people who did newer networks get mad and they started their own conference it's called Nero information processing systems and that's today actually the number one conference in machine learning and so that actually that conference became very popular and there was a lot of papers and newer networks and you know became very very popular until the 90s late 90s when SVM's came around and people just fell in love with SVM's that it was so beautiful and so then people you know like yes we have so much there were more more SVM's paper svm paper and till actually in some times at some point 2002 or 2003 it happened there was not a single neural network paper anymore I had nips the conference on ulam for me in your networks and in fact every single paper that had you in networks then got started getting rejected so in fact it was a new generation of students and they basically thought in your network so just stupid right that's the old way of doing it the new way of doing it is SVM's and it got so bad that hardly any months that neural networks anymore and there was three people who held onto it very much like Jeff in Jana Cole and Yasha Benjy oh three professors from NYU Toronto and Montreal and at some point they got together one night this was in 1982 2006 and they got together of a glass wine they kind of all chatting it but you know how frustrating it is that all their papers get rejected just because they have the word new and approximate and like students just rejected because like reviewers just rejected just because they mentioned the word new in networks and the model that might be idea well why don't we just stop saying your networks right we just just don't you don't say I think well what about should we say means to say well we just call a deep learning and why deep learning because then everything else is shallow learning and that is literally what happens and so they wrote papers and they just changed you know you know we're you know replace all right I knew and I purchased 2d Blahnik deep network and suddenly the papers got accepted to be like well what is this deep learning oh it's interesting yeah interesting stuff and you know a lot of people are interested and now I see people say that others the new machine learning it's deep learning so um well so but really you know conceptually not much has changed all right let me let me tell you tell you the basics so it's actually very related to kernels that you will see why people fell in love with console it's kind of the way I'm explaining it it's kind of upside down right like history upside down so I'm starting with kernels and then tell you how to making your networks out of it so in cars we had linear classifiers H of X equals W transpose X and we said that's just a linear classifier that's too simple right that's the same problem that Rosenblatt had here at Cornell back when he invented the perceptron so the solution of Carlos was well why don't we just have some function Phi of X we map our X into a very high dimensional space and in that space you know now we have a lot more expressive power and that you know that linear decision boundaries now becomes a lot more powerful and actually becomes a nonlinear decision boundary in the original space and so the way kernels did this is they had a very very well-crafted functions such that the inner products of these functions are computed it can be computed in closed form so we don't actually have to do this napping that's awesome right because you have to do this mapping implicitly and it's very very fast but it's restrictive right because you can only pick functions such that you actually know how to compute that in the product like for example the RBF kernel the polynomial kernel now what deep learning does about the new networks do as I say instead of actually taking in handcraft that function like the RBF kernel or something what we do is we make Phi of X just the primary device functions so we just actually learn that function and so in neural networks what we say is Phi of X equals the following equals Sigma of a of X plus B where a is just a matrix so just take my X multiply our matrix and then add a constant to it okay so basically what are you doing right if you might define our matrix there's an affine transformation so you take your data you can of stretch it all right you stretch arbitrary directions and other directions you shrink right so you can it's like yeah you put your data so this is kind of you know made out of rubber something made your space you pull it and then you have this thing right here and what is that Sigma that Sigma is some nonlinear function and for a long time but people had here is Sigma is the sigmoid function that's why it's still called Sigma often the Sigma is Sigma or do you know that from logistic regression is 1 over 1 plus e to the minus Z so that was at least before Jeff in and used in 19 you know in the 80s and he that caught on and if you remember what that function looks like it looks like this all right so it's plays the if z is very very negative it becomes a 0 and then it goes up to 1/2 and then if it becomes very very positive it goes to 1 and and you apply this function to every C as a vector right so x times matrix is the vector you apply this to every single dimension and so the inspiration in some sense voss this is like the brain right the neuron space the gets an input and the either switched on or switched off so basically what your vector becomes is either ones or zeros essentially right so either you have large value then you have a 1 or you have a small value you have a 0 so you may see take your input you've happened into some different space of zeros and ones but for different inputs they're different that's really easy you switch them on or off and I'm going to use this actually for 20-years sigmoid function and then actually Geoff Hinton who was one of the people who introduced it just a few years ago was like actually don't use it Sigma function its Sigma functions terrible and I remember why we introduced it and I remember having be introduced it and we didn't really have a good reason to use the sigmoid function and actually it has big disadvantages when we optimize it so what people nowadays do is actually something much more simpler so they just use the max with zero that's called a rectified linear unit so that looks like this now it explains to you in a minute why one is much better than the other so it turns out you need some nonlinear function here so power of X just based yet after I transformation you take your import you multiply by our matrix Y at some constant and then you have some some nonlinear function here but you basically in this kit nowadays you just take all the negative values then set them to zero and so if you didn't have this that actually it would just become a linear classifier that's easy to show if you didn't have this then if Sigma what was actually not there then you just have W transpose ax plus B well that just becomes W transpose a X thus W transpose B where this years another vector W hat w hat transpose X plus as here's my PB hat right so if you just have a transformation without the Sigma then actually multiplying by a vector is the same thing as if you would just learn a different vector so you still actually just have a linear classifier so the trick of newer networks is you have some affine transformation which is very very general here right it's just that magic equation for the matrix that's it and then you just have some very simple function that just sets everything that's negative to zero and that's all that's going on and and when you learn it you basically learn W and you learn a and you know B at the same time next lecture we will tell you a lot more about this and we get into all the details and I'll show you a ton of demos actually with pleasure 
","['', 'Adaboost', 'Bagging', 'Boosting', 'Car Trees', 'Deep Learning', 'Error function', 'Gradient boosting', 'Kernel', 'Logistic regression', 'Neural networks', 'Perceptron project', 'Regression trees', 'Semester project', 'Sigmoid function', 'Supervised learning', 'Training error', 'Testing error', 'Weak learner', 'Weighted data points', '']"
"only three lectures left to go you think so a new homework is out and you need this is very simple this is all very simple stuff one second [Music] and I can PvP hear me all right better okay um yeah new homework is out a lot of people are doing well on the Calgary competition I had a look today on the private leader board actually on the private set which will be the one determining the grades it's very similar it's not identical to the public leader board but generally people are doing very well I sincerely apologize for the mistake we had project seven that's Marie sorry we have a lot of testings but sometimes something slips too so for those who didn't catch I didn't see it a Piazza that was a small mistake in the last function that we wrote so basically in the Python homework and the only in the Python version actually there was a mistake in the formula of the splitting function that did not match the lecture notes and did not match the homework so it's now should be corrected in case you still have an old version of the Bukharian thing because once if you checked it out it does we didn't overwrite it just ignore that equation just take the equation from the homework or from the lecture notes basically it was a normalization constant that was wrong okay any questions about this there will also be one more project it's being rolled out tonight that's a very simple project don't be scared it's the easiest project you've ever done in your life so you know just most of the work is just reading through it and understanding what you have to do it's it's just it's basically a deep learning anybody see what you have to do is you have to implement a new network and train it so that's all there is to it that may have sounded complicated if intimidating a few months ago but now you know this is very simple okay so last time we talked about new networks and just a little recap the idea basically is just as before it started crazy this came from you know same person Frank Rosenblatt who invented the perceptron and his problem was you know he invented the linear classifier and that was too restrictive so his idea was how can we make it more complicated or more powerful that's really what he wanted and the idea was we map our X to Phi of X such that our classifier becomes H of X equals Phi of X transpose W plus NP and so previously in the when we talked about kernels we chose this Phi of X implicitly by defining it in a product function that was a good idea here in this case we are learning Phi of X so if a see saying we are learning two things here we are learning the W and the B that's the hyperplane and we're learning the representation of the data okay so if we are mapping the data into some space we're learning that mapping and then in that space or space we also have a classifier that's kind of how you can view it so that's a lot more powerful than you know then actually just learning a linear classifier by itself new networks have been around for a long time and in fact machine learning started out in some sense of the modern machine that have started out with newer networks and they recently had this Renaissance since the Renaissance that's you know Jim as a peon everybody has gone bonkers over it like yeah for multiple reasons and but it's not entirely fair to say that it's exactly the same algorithms like there's some subtle changes and let me just write them down here and and we will go through them today and some of them we already mentioned last time so the first change in that B that's one thing we mentioned last time is that people used rectified linear units instead of sigmoid transition functions so just to remind you PI of X actually has the following form Phi of X is some function Sigma of a of X plus C and this function here people used to think has to be a sigmoidal function that basically switches on dimensions yes or no so what this year does this gives me a vector given ax this gives me a new vector which is a transform vector and then the sigmoidal function would just set things to one or two zero so if it's above a certain threshold you said to one if it's below a certain threshold is the way you have to use in your networks and there may be in part because newer networks is the same word as newer networks in your brain and people this was really brain inspired and the brain does something like this so neurons basically have you know you know inputs and if these are above a certain threshold then there's a high probability of them actually firing themselves so this is kind of how you get Cascades in your brain ultimately this is a really bad way of doing it when your networks for artificial neural networks the reason is because we use gradient descent our brain doesn't use gradient descent if we train them with gradient descent the promise that this function here is completely flat here and completely flat here it has no gradient so my people nowadays uses the rectified linear unit where Sigma of Z equals max Z with zero and by the way the rectified linear unit was invented actually 20 years ago it's just people didn't catch on people didn't realize and there's a good reason why I fact then it wasn't worth it but nowadays everybody you know rethinking the transition functions made a big difference the second thing that made a huge difference and that actually is really what it's all about is GPUs so one thing I will mention today is that you know maybe you next lecture is yeah the new network is actually quite expensive to train and they're very slow so that was one reason people abandoned them and you're a kernel machines came around yes the ends were much much faster and they were better because he couldn't train your networks all that much but new network training has a very specific form and what it does it involves almost only matrix multiplications so it's matrix matrix modification that's all that is so you spend all your time doing matrix matrix multiplications and that used to be really slow all right so that that was basically you know well you have to do all these matrix meetings mummification is a really slow algorithm just use an SVM with GPUs and we really have to thank you guys because you guys are the generation when you were kids right you did not go outside and play instead you played computer games thank you hey all right we have any you know can't can't thank you enough with it because that that drove the development of really high-performance GPUs which then could be used for scientific computing and they're really invented for 3d operations but that's all just matrix of multiplications so what GPUs really do really really well is matrix operations and some AG's operations is really kind of it's actually most it's actually very easy to write code that runs on the GPU it's very hard to write code that runs the GPU is faster than code that runs on CPU and but matrix modifications in some sense this golden thing will actually works really really well on GPU because it's so paralyzed abour and it you know you have enough operations per memory access etcetera so because GPUs came around suddenly could train really big newer networks which beforehand but completely out of the question and turns out then actually if you train them enough they actually became really good the last thing and this is a really important one if SGD which stands for stochastic gradient descent and this is what we will do today it's a variant of grain descent that's it's that was considered a really stupid algorithm and so machine learning has this history of laughing at things and then later on going like oh by the way that's that's awesome so maybe one day genetic programming is kind of rules I have rule everything so SGD really actually made a huge difference that was something that wasn't taken seriously because STD is actually from an optimization point of view a terrible algorithm you can prove it's a terrible algorithm but actually has some properties that people did not think of and that those three things together and oh the last one you know rebranding beep Randy yes so you know so that was that was the final thing right so in some sense people figure these things out but they had to rename it to kind of to make people realize this now actually is different ever now works yeah good question the question is what's the benefit of a deep learning deep learning over kernels and yes one very nice thing about deep learning is that it scales linearly with more data so if you get more and more data let's say you get million yeah give one hundred thousand points two million data points and it just you know it's just ten times slower right whereas kernel it's actually because you have the kernel matrix is actually quadratic in the number of data points and so the actually the interesting thing is the improvement that you get if you draw draw a figure that's a number of theta training data right and the accuracy that you get this is that diminishing at the beginning like every single either put makes a huge difference and then it kind of you know there's a craft that slowly kind of asymptotes right so um in order to get kernel machines to work on more data you have to do approximations yeah the key context you compute the kernel matrix exactly anymore typically you have to do approximations that costs you accuracy the problem is at some point you difference is so big that this improvement here is very little and basically that's the improvement that you get by more data is roughly the same about it this is roughly the same word of magnitude as as much as you have to pay for scaling the algorithm up does that make sense so in some sense you go from here to here right you get an improvement theoretically but because you approximate you actually go down again so kernel machines do not improve anymore right that's people people realize if you added more data you know you didn't get the benefit anymore but it's new or new networks you could actually throw more data at them and do better and better and so people then like companies like Google and Facebook they generate these datasets with you know fifteen million data points that they label they pay people to label them and the better you know the benefits that you got basically only became apparent with deep learning other algorithms could not capitalize on that there's other approaches there's other things too and I will get into this more envision so there's the last thing deep learning actually I you've may have heard a lot about it lately it works really well in two domains right there's really two domains where it's the Killa it's not the answers or everything this is like people miss a misconception it's not the modern way of doing machine learning it's silly it's it's really works really well on object recognition and images and that was something that beforehand was very hard because before we had a long pipeline we had took images he had to extract features he had put them into an SVM and so on yet have made a special kernel deep the nice thing on your networks is they just taken input as a images input and the prediction is output and they are much much much better and there was a actually I'm still going to get to this remember there was a famous competition every year on who has the best results in object recognition that's called the image net competition run by faithfully and this was a competition that's been going on for many years and it's exactly like you know the kaga leaderboard of this in class competition basically and what happens with the every year kind of they got a little bit better right so there are new improvements and so on and so the way it looked Paisley is in 2011 right like the best performance was this slice since this here's my error right and then you know the other submissions were here and then the next year but this was 2009 say 2010 keeping a little better right in the the other ones you know here the other submissions and then 2011 you know we got here right and into the things got more and more sophisticated and then 2012 came around and you know the best thing was here you know and so on that looked just like the same and then suddenly you know Jeff Hinz teams admitted theirs and it was down here right so with one submission they actually have more than half the error right and before I basically looked like you know they would not reach this in like several decades if you would you know continue this graph right and suddenly with a busier than within one year they made as much progress as was to be expected but in several decades and that was busy by switching to deep learning that were trained on GPUs and use these tricks and so then the next year actually the result was here and then you know here and now actually they're better than human performance right so at least on this this task computers outperform humans and so that that was really you know in fact actually cvpr has a computer vision conference at that conference Jeff Fenton explained his algorithm and more people came to that talk then came to the conference actually like you know they and they had to get extra chairs and so on like you know everybody just want to see him explain how the how the hell did that happen right and that was what really everyone was interested and the next tier busy you know now nobody does anything but deep learning these conferences and so these the second thing one is images and I didn't say but the second thing is does anyone know what the second application is were deep learning is really really amazing speech that's right right so on that enabled series so if you have an iPhone or Android phone something you can talk to these things now and that suddenly that that was the your the similar drop right so basically people at these we had been working on speech recognition for decades and suddenly the Bernie came along and then you know within a few years every year they have the error rate until it was good enough that you could actually use it and that started already in 2006 so that was right when the iPhone came out okay so let me I actually want to show you a demo still today I promised you a demo because of the screw-up on project seven alright so here's the idea we we basically want to learn a classifier H of X equals W transpose Phi of X where Phi of X is is Sigma of ax plus C and this here is P and C here's the vector so actually I think I called you here all right let's call this UX the first thing is people asking you how do you learn this right then you learn it just the normal way but you have a loss function l equals the sum of all your data points want to N and then you have some loss of you know H of X I and why I write this can be the square loss or something right if some lasts and because some convex function that tells you how well you're doing all right and then how do you know this thing about you take gradient is send respect to W and respect to human spec to see right so there's really nothing different like you know all the components we've done this now for for several months and let me give you a slide you know kind of an inside view of what these NiO networks are doing right so so one thing I'm not a big fan of is people say like oh these black boxes right they just do whatever nobody understands but I think they're thinking of the human brain they're not thinking of these functions right now it's true that at the end often we don't know what this function looks like that we have that be a training but if I train a function to you know recognize pelicans from dogs or something well you also wouldn't know what that function looks like right I mean what functions separates pelicans from dogs so the fact that you can approximated of course you wouldn't know what what kind of exact function it is so let me give you a slightly different perspective on what exactly is going on inside a new network so let's say I want to do a regression problem so my last function in this case I just you know I just fixed this is one to the N H of X I minus y I squared okay and my H of X is based the following is it's basically this function here right so H of X is I take my let me just write it as once one thing is w transpose Sigma of UX plus C plus P and so these are my parameters okay and my Sigma in this case is a max was zero okay so let me write this down max was 0 this is the rectified linear unit so what does the max with zero do and max with zero this operates element-wise so if it's positive it just keeps it doesn't do anything and if it's negative it set it to zero okay good any questions at this point yeah because I guess ultimately they're all Universal approximator z' so you can approximate any function it's mostly the transition function is mostly a matter of convenience but what's best to optimize actually so let's say might the function of trying to learn let's say I just have a 1 in 1 dimensional input one-dimensional output so I'm trying to learn this function here okay all right this is the function this regression problem and this here's my X this years know why okay raise your hand if you're still with me all right good and now here's what what the new network really is doing right so think of this matrix U as the following think of our matrix U as Paisley this is my matrix and I have many little vectors right so these column vectors u1 u I don't know u H right so have each of these alright and my vector C is just C 1 CH ok all right so then what is this right this is basically if you write this s there's just a sum of you know of the different dimensions yeah WD x max of you D transpose damn it it's chalk exponent on UD UD transpose X plus C D comma 0 plus feet okay does that does it make sense that's the same thing all I'm doing is I'm just dividing out the sum here I guess some over everything that I mentioned and here's I'm aware every single dimension any questions raise your hand if you're with me okay this is just a know what I did is I just decompress the mate you expect a modification right does beezy get multiply u times X right here what is U times X u times X is dammit this chalk is horrible yeah is this Q 1 transpose X u H transpose X this is a vector transpose transpose X ok let's just please raise your hand is that does that make sense okay thank you okay all right so then so but what is this you chose was ecstasy that's a hyperplane right so hyper clean in the two dimension spaces the line okay so these are lines and if they are positive there are none if they're negative theta zero so these things here look like this right so you have is he have some function that looks like like this right and then suddenly hits zero and just disappears okay this could be you one right you want transpose X right and this year is is C okay so when it's a 0 plus C 1 okay this is C 1 does that make sense this is a line linear function and when X is zero this is exactly C 1 okay with me any questions okay another one could be this right copy could look like this right this is YouTube transpose X plus C add enough people here can see this oh and this baby here you know my c2 is actually down here right so at 0 I'm at c2 and then I have this line that goes up okay all right so what am i doing I have many of these functions and I add them up and I give each other the weight okay and at the end of shift the whole thing up or down okay B is just saying I'm taking this whole thing so what if I add up these two functions what do I get right well here in this region this front the second function here is zero so I just get the first reason function right but then here the second function becomes nonzero so what's happening it moves up right so the sum of these two functions look like this okay this year is the first function plus the second function okay if I now had a third function right let's say I have a third function here that starts here and goes this way there's my u3 all right then I would be easy at this point here what are after jaw and so you can't see anything you have no idea what I'm doing right okay so this is my son right this is u1 plus u2 now and their third function comes in here what do I have to do how does this function now continue so yeah Hassler it's like it goes even higher right you can see by changing this function alright I could also I could also have a function like this I could actually you know have a constant function here that I suddenly switch off I could also make it go down again okay does that make sense so you can you can do anything you want and so essentially what you're doing is you take this function that you're trying to approximate and you're basically okay well this is pretty straight years I just take my first function right after this point where it gets pretty bad some jeans what I'm doing a switch on the second function right and he have to switch on the third function fourth function v function six function seventh function okay and that's all that's going on in your network that's obey see what it does the proxy mates functions by piecewise linear components all right so this is actually quite simple and two internally if you look at these you is those bases the first function here is this one right plus some B that shifts the whole thing up then here comes the second one so here you actually have another one that actually goes up right you add these two up and then you get this nope you oh sorry actually this nation- right so you actually have to what do you have to do you have to oh sorry you actually do this one and you can have a negative weight here how do you have a negative weight then you just subtract them all right there's a consensus then you actually go down you can have another one that you're subtracting and so on okay any questions yeah sorry each line corresponds to a row in my you matrix okay and your row matrix you you matrix basically isn't you know isn't a use element of our H by D but D is the input dimension of your data and H is the number of Venus's you know something that you choose so you have a three parameter how many of these linear components you allow to have alright if you just have one what you're gonna get is this right it's just the line if you have two were you gonna get something like this or something I don't know all right and as you switch on more and more you get a more and more complex fitting of the function oh you don't decide this right the general network decides it but it minimizes a square loss right so if you actually imagine you would switch on something else here well you would actually have quite a last year right because you square the difference between what your prediction is and what the true value is right so the square loss penalizes you for being too far off so what it's going to do it's going to shift the you know that function over the second one right to reduce that gap any car step how do you switch on/off so you have these little functions here right and when the moment they are negative they are zero and you can shift them around any time you want right you can you can change the slope by changing you and you can shift them around this way by changing the C value but if I would lower the C value the whole function would go down and it would be switched on here all right it's switched on the moment it becomes positive and then I can always add it or subtract it okay yeah yeah you could have a constant function right that you were just always and actually that's that's true we don't really need it yeah so the question was why do we need that second B right well in some sense she could just model it's by one more function that's always positive and always constant and you're exactly right so the Neo networks don't really need this and actually the project actually I took out the constant so that that's why the project is so easy all right nice are you laughing the people really that the thing one thing they found half was the constant last time okay any any more questions and and so and turns out actually it's very simple proved actually and that within your network so you can approximate any smooth function arbitrarily close all right and how do you prove this well it's quite simple you just say if I make my H here arbitrarily large so I have many many many many functions right well I can you know if I'm allowed if I allow you to have as as many of these linear functions as you you want right then you can make the error as small as possible right so if I say the error should not be larger than Delta or Delta something very small then you can tell me okay I can do this I just need so onto many million linear components right and then I can show that the sum of all the errors is less than Delta why does that make sense so that's essentially you know the way you argue that these are universal approximated so universal approximate us you know used to be a big deal nowadays many algorithms have this you know can be shown at this property but you can approximate any continuous function arbitrarily closely right and so you know your networks that's shoe but this age will come very very large so you basically train a lot of parameters yeah no you select this there's something you put it yeah that's right that's right and the more you have the more powerful is your near Network oh I haven't said anything about layers yet actually layers what are you talking about I don't know there's no layers and no you're right so there's later on do layers and later on five minutes you will do layers and so okay well let me tell you layers you just you know okay good so here calm layers just out of order yeah but that's okay so people have heard of these layers and so basically we have this w transpose X so H of X equals W transpose Phi of X right where v of X is you know what you know well I guess you have it right 5x equals Sigma of a of X plus C well let's call this a why not it's gonna say all right now what are layers well here's the idea well people want to make this more complicated so I call this u let's call this u u and C okay why not so one way to make this more powerful is to make this matrix you really really really rectangular right so increase H a lot so have many many linear components another way to make this mock-up a powerful it's an idea that comes from Russia I don't know if you know these little ladies if you open them there's another lady in there you open them is another lady in there right so here's what you do we see well actually that's not X that's five prime of X and Phi prime of X is Sigma of U prime of X plus C Prime alright so now you busy you have one more lady inside the ladies I don't know they're caught by Musco that what do they call I know what they got right now so now you actually have two layers IGBT is saying I first have a transformation and then I have another transformation and if you want to you can say well actually that's one move right this Phi double prime of X and you can do it as many of these you know as if you want right like you can spend your whole life making layers right and so that's actually one thing that that's new these days is that people have many layers that's where the word deep comes from deep learning and there's a very good reason why people did not do this in the past so beforehand like when I was a student a PhD student everybody just had one layer everybody knew he could have more layers in fact Frank Rosenblatt himself right in 1963 he or what he came up with the idea is that you could have more layers if you want to write but nobody did it and why didn't one do it yeah has two reasons the first one is it's very expensive right because you always have to multiply it with matrices and matrix multiplications are very calm very expensive and the second one is that's the practical reason there was a practical reason a theoretical reason the practical reason is it was very expensive and it took a long time and the theoretical reason is that you could prove there's no function in the world that you can learn with a deep network that you cannot also learn with the shadow network it's just one layer so it's a very famous theorem that says all you ever need is one layer all these other layers don't don't improve your expresses the expressivity of your function and so that was great right because people said well multiple layers this complicates expensive anyway and I can prove to you that you don't need it there's no reason to do it so no one did it and that was in some sense you know that was in some sense all but forgotten right like no one no one worked on multiple layers it was considered a silly thing to do right we could prove theoretically that it's there's no benefit from it and it's really slow so why would you do it and it was actually that really came down to Geoff Hinton and the University of Toronto who basically who realized that they actually there is a benefit like you know that actually in practice that makes quite a difference and he he bought a whole bunch of GPUs at the time they were very expensive so what he did is he raised his own money he started a company and had bought tons of GPUs and trained these networks and that was actually how you won that imagenet competition and then actually that company was bought by Google so but but it took him in some sense you know to show the whole world that actually you know everybody was wrong in that sense and and the funny thing is the theory is not wrong actually all right a theory is right you can go through the proof it's correct and it just says essentially that the number of the size of this matrix and this is something that people didn't really look into is how big would that matrix have to be and so here come let me show you a good analogy and then I will show you a demo I promise you them okay here's here's the analogy when I but I just showed you is the way your networks work is they Beasley have these these piecewise linear functions so now I'll pay attention you have to see this just like it's my prop yeah at this prop this actually does not just my notes as prop so here's what I'm gonna do we just like this so a piecewise linear don't do this on your nose so the piecewise linear functions essentially what they are there's these little hyper planes right so think of it as if I'm folding this paper right so I have my first piecewise linear function so now I have a fold here and now I want the function to go down so I have a slightly different fold here right that goes in a different direction all right great all right so now if you look at this I know if you can see this function it goes up and then goes down okay these are my two piecewise linear function that I use to approximate a nonlinear function now here comes the cool thing if you have layers you can do the following let me fold this there's my first layer becomes my second layer rights now if two layers are the three layers come on right and now in this there I defined a piecewise linear function so I do this fold here and you know what's happening you seen this and now I do another one here hey and out comes a beautiful thing if I now unfold this thing right well it looks horrible [Music] what happens is you have food all over the paper I just fold at Rice and actually I reuse the same folds over and over the entire space right and that is the power of multiple of multiple layers and actually the first layer what it does it does exactly the same thing it learns these piecewise linear functions but what is the second layer do the second layer takes this entire thing and reuses it right the second layer it doesn't take piecewise linear it like doesn't take simple straight lines anymore the second layer takes his entire construct and basically uses that as a building block right and the third layer takes the construct of constructs of piecewise linear functions and builds those does that make sense so you actually get a you know you get a pre exponential explosion and your capacity by having more layers and that is the powerful thing because the theorem that says that NiO networks is a powerful said that this matrix is one layer has to be exponentially deep yet so has to be experientially wide but you get that exponential effect also by actually having multiple layers just having a few more layers ubz multiply you know the number of possible you know lines or hyperplanes that you're putting together right exponentially with the depth and that is what's so powerful because before we'd you would have this massive matrix right that you couldn't afford to ever use whereas now you actually just have a few small matrices and they have the same expressive power does that make sense raise your hand if that makes sense awesome any questions yeah [Music] so what's the trade-off between having a wide network and a deep network and that is a question that people argue about a lot and the answer is your network should not be too wide and not be too deep you're welcome I was consulting that would have cost you a thousand bucks that is kind of where we are right now we're trying to in some sense it took the whole community by storm because now we actually we used to be the safe we had us like this theoretical understanding now are we going like oh actually if you have to rethink our theory right so we actually have to and we're still working on this theory like the theory is not mature yet at this point yeah but it's the first one only has one function then it's also hard right so then the new basically those oh you're right right and so currently the trend is to deeper and deeper deeper that's currently the trend right so people have trained networks in fact that she I myself am guilty so I just had a paper last year that train networks have had 1,300 layers deep right so a few years ago if I take this we would have thrown stones at me right you know so now you can do it like it used to be by the way just to make this clear like seven years ago right if you had three layers that was considered ridiculous right it's like five layers nobody even but if you know it was like why are you wasting energy right global warming you know now people trained thousand layer networks right so it's really it's really changed completely ah good question so why do these not over fit right let me go to the demo I don't know if it's the biggest question right now is why do these new networks not over fit so drastically we have pretty good understanding but it's well beyond this course and actually it comes down well ah okay let me tell you it comes down to stochastic gradient descent which actually I didn't have time to today I will get to a next lecture I promise stochastic gradient descent is actually a variant of Gwaine descent that's so bad that it doesn't optimize the function properly and because of that you not have a feeling and that that is the chip and turn now that was actually crucial people originally just an SGD because it was faster and then it turned out actually you know they just did this - - you know it was fast the kind of engine they knew it wasn't very good but actually turned out suddenly their networks didn't it worth it anymore I will but I now we started to understand this theory pretty well I will hopefully get into it more in next lecture a small demo so um where are we ok 1 D C is the first thing I can now take a network and today I know let me just define a few there's a regression network so I just define a function kind of like the one I just showed you these are my training points there's my x-axis my y-axis not gonna fit a network through it and what do you see it's basically yeah you know there's actually two layers here so as it goes through it basically you know it makes these pieces in the air functions actually I have 40 components here and then actually another 40 in the second layer so this is very very you know you can see basic fits this is absolutely perfectly I can also do this one more time with fewer components actually I think I can actually I think I prepared this one second is the class demo is this correct yeah so oh sorry this is not rectified linear units and what is this sorry let me just do this yeah okay Oh is he just a demo 1d and I just number of layers that just make this one and that's how many layer how many hidden nodes do you want for okay let's do one for no one's at four let's do it so let me do that is the same kind of function but now I only have four piecewise components if you look at it well that looks horrible but this is actually the rectified linear units Oh actually depends those old haven't gotten instance yet so actually depend how you optimize it because you have a lot of zero gradients actually sometimes to get stuck so this actually got stuck this doesn't happen if you have more and more units let me do eight here we go okay okay so here's the first one I have why does it say what is it to us oh sorry I just I just edited the demo which was a huge mistake oh here we go trans is soft plus sorry should never change my demos okay so why is it just Sigma oh sorry guys trucks here we go yeah there you go okay good now to work okay good this just having two different components right 200 components actually be careful if you count carefully if you have very good eyes you can see this is one two three why is it three only of two hidden layers only I would have two two functions yeah that's exactly right right so one of them becomes 0 and the other start so actually what you get is actually you actually get sweet at two turning points all right so here's the first turning point here's the second turning point so he means the old both of them are switched off first one switched on second one switched on okay so you get actually two turning points now they can take the same function and let's go okay here so now we have four right so here's what it does it kind of is seen kind of approximates this better here's the last loss goes down until it's flat this is because they're rectified linear units up one has zero gradient and you - for too many iterations oh I read 4,000 oh my gosh all right anyway you get the picture [Applause] okay so yes ah you know what let me go to bed of them I know this is boring all right I'm much cooler demo so you can actually learn really really complicated stuff with this and so some of you have demos do this demo before I apologize but it'sit's good enough to show again so here's the idea so I tried to come up something really really complicated so you learn these if you have these functions you know it works really well for images and you know the amazing thing about neural networks is you can actually learn concepts that are quite complicated so I thought about what's a really complicated complex a contact and so one is whether people are good looking or not because that's something I I really have no idea so what I did is I took this data set off there's a webpage called Hottentots you can upload an image of yourself and then people who pray to you and say you're good-looking or you're not and I did not upload an image of myself but I I took the images there and actually I raised them all and are they above average good-looking or below average so I just have two labels so 50% man 50% women and the idea was to resist votive on labor wise you're either positive +1 above average good-looking or -1 below average good-looking and I just cropped out the faces so this is arguably a really hard problem right so if you just get your face you know the questions are they above average good-looking or below average people so here's when you run it the training accuracy of course a hundred percent it memorized the train there said but at the testing accuracy is 74 all right there's a balance data set so that's a lot better than a coin toss so you know this is a really hard problem so let me test it for you so here's a you know let me just get a few test points that we all agree on so the first one is Heidi Klum so I love you know Heidi Klum she's from Germany she's a supermodel and she's arguably very very good-looking by the fact that she makes gazillions you know with her looks so I can now take a dko and put that image to that Network and see how you know how good luggage this -1 is bad looking 1 plus 1 is good-looking so you can test this we get two points that's off the charts right so the newer network has the hots for Heidi Klum's not bad so then I thought about what about a guy so I I don't know what guys are good-looking so I asked my wife and she she didn't skip a beat he was like George Clooney so here's George Clooney and I gotta be honest with you bugs me a little there I know I look nothing might rush to me that's a different topic that says you know anyway so we can put rush Clooney through the new network and see what happens and here he is oh wow you know I told you so that not even saving all right then of course that you know I couldn't resist right so here you know here [Applause] their penis maybe there's a second career for me a swimsuit model like you never know alright let's go and this point I was very curious like how does that that's pretty bad actually that's that's pretty close to my house mom so how do I do compared to the ugliest person on the internet and so I found the ugliest person on the Internet which is very easy I typed in ugliest person on the Internet into Google and that's the first image that comes up is this so that alright let's let's try this is the lower bound let's go all right now All Right see y'all on Monday 
","['', 'new homework assignment', 'mistake in the last function of Python homework related to splitting function', 'rectified linear units (ReLU) vs sigmoid transition functions', 'gradient descent', 'GPUs (graphical processing units) and their role in deep learning', 'deep learning for object recognition', 'convolutional neural networks', 'image net competition', 'deep learning for speech recognition', 'loss function', 'classifier', 'perceptron', 'linear classifier', 'kernel machines', '颓废 (tuífèi) - Chinese word for ""decadent"" or ""depraved"" (used jokingly in the video)', 'Xavier Simonyi (mentioned as one of the people who contributed to deep learning)', 'Hottentots webpage (used for data collection on attractiveness)', 'training accuracy vs testing accuracy', 'bias in training data', '']"
"I can't last lecture of machine learning and please everybody remove your laptops unless you in the very last row few logistic things so the project eight is out and I said don't worry it's relatively easy it's a lot easier than project seven which is probably among the harder ones then the oath of the latest homework assignment is out that is optional so you don't have to hand this in the there's two reasons why you should do it anyway and that's a because it's a good preparation for the exam and B because what you're deriving in project in the homework six is exactly what you have to implement the project eight so it helps you a lot if you've done it and if you you know then project eight is really trivial okay also some people asked about practice exams I posted them on Piazza under resources so if you the very top is practice final exams simple so please look at those in preparation finally I believe the early sign up for the conflict exam is now over I believe so there will be I think the only very few people who can take the exam because of a university recognized conflict and we will have an alternate date so just be ready we will post that today evidence okay any questions about just logistics okay good we're talking about deep learning and neuro networks and so as we you know last time basically deep learning you have a loss function of our predictor HR hypothesis and this can just be you know some generic function that goes over all the different samples and compute some loss for example just you know a typical example maybe the Skrillex H of X I minus y I squared this is the regression setting and in new networks are just a simple extension of linear classifiers H of X is w transpose Phi of X and now comes the trick Phi of X itself as a mapping so Phi of x equals some transition functions times U of X and you know let's just remove the bias and that's a one layer new network if you have multiple layers you just stick another function U and then you have v prime of x equals Sigma of u prime or Phi double prime of X it's on you can make as many layers as you feel as necessary u double prime of X right so in this case we have one one two three three layer new network okay so the input comes in here you multiply with a matrix apply our transition function the transition function can just be the max was zero so you said everything that's negative to zero and you take the outcome that's a vector you multiply by the matrix set everything negative to zero take the outcome multiplied by matrix set everything negative to zero and then at the end you have your request so what I want to talk today is how to learn such a new network so if you have a loss function and we've seen before when you have a loss function and you have a classifier like this maybe just do gradient descent and so new networks actually you do exactly the same thing the only thing is you don't just have that one vector W you also have these matrices in between all right so these are your parameters see if that W that's the the last the last layer learning they're all the intermediate layers each have a transition matrix so how do you do how do you degrade in the sand well the first thing is you have to take gradient the gradient with respect to every single one of these matrices and actually it's exactly the same thing as before you just take the gradient and then you just take a small step here just a small update so you just say you know u becomes u minus alpha times the gradient so it's just like good old gradient descent and there's nothing special about it the one thing that's a little tricky is that you have to take these gradients with respect to these these may these weight matrices that are hidden deep inside the network right so you basically have to think about it you kind of have to go through this function for this function and so on until you actually end up here all right and turns out that's a very good trick how to do this and it's called the chain rule who's heard of the chain rule awesome who's heard of chain-smoking all right it's about the same actually okay so here's what you do right you busy say well the first gradient is simple right that's just the gradient that's just let me just write this is DL DW okay so in this case this is just some of our I equals 1 to N and then here W transpose Phi of X I minus y I times Phi of X I all right so that's just the gradient of the square laws where H of X is this thing you okay any questions yeah oh yeah no X is just a sample a training example I don't need some of all the trainings now that's the gradient all right good question yes any other questions so so far there is nothing special at all right this is just that this is just a square loss you implemented this right this was the ERM homework you did exactly this Phi of X was your feature vector right you just minimize the square loss you just compute the gradient and then when we do a gradient update we just say W becomes W minus alpha times this fellow you okay raise your hand of you with me all right awesome now comes this guy here right and now he gets a little tricky how do you get this guy right and well we just use the chain rule so you just want to have the LD u and if you use a chain with one thing you gotta realize this well you have to do all these chains right all the way you know each one gets more and more expensive that's what you would think but turns out actually it's very cheap because it can reuse computation and this is called back propagation so actually a long time for Beck congregation to be invented like long after new networks were invented people actually at the beginning they did not train it with a chain rule and that's there's good reasons for this but so okay you want to have the green in perspective this first matrix it's basically inside the spire of X the first weight matrix that we have and how do we do this well that's DL and it comes a trick we can actually say D Phi and then D Phi to you but one thing we got to do is go to the simple trick we call this year a of X this here a prime of X this yeah a double prime of X okay that's just the notation so a of X base D takes the output of the previous layer and just multiplies with the matrix and then Phi of X is just this transition function or Phi of X ok any questions does that make sense so it's kind of what I do is like you know if you have these Russian women like inside a woman is another woman now what I did is I kind of a half woman that's kind of you know in vitam between two stage I don't know if that makes any sense but you know you see what I'm saying okay raise your hand if that makes sense oh no no no sorry the lake's layer it's just a one and that's just the next layer now I can also call this one and then two does that help this here's my 1 1 and this here is 0 I don't know I'm just trying to go into the layers make sense actually let me go back to the primes I prefer the price yes so basically all I'm saying is inside this layer what hat what is happening I take the previous slot at the output of the previous layer and modified by the matrix I call that a and then I'm pushing through a transition function I call that Phi okay let's just give the first step it is a two-step approach because each step now a name stem of the chain rule I can just say well this is the lastest gradient of the overall last function respect to this U is by the chain rule the gradient of the last function with respect to a and then a with respect to u okay raising ended that makes sense awesome okay good good if this is boring it should be boring right and I'm trying to make it really like mechanical right and because it's going to turn to an algorithm this is going to be a very simple algorithm that computes the gradients and just pops them out one by one every iteration you get a new greedy all right so now let's try the second gradient what do we do all right so sorry this is U u prime it's 1 prime here so now I would like to know the gradient of the last function with respect to this guy all right so I first go through the first representation the first representation is a transformation of the second representation and so on down here so how do I get this gradient right so by chain rule that's the same as before I first go to this a and then from this a I go to u prime a sorry one second da prime D a prime u Prime okay so the trick is that what you do is you always take the gradient with respect to the the function which encapsulates your gradient either your your weight and then the gradient of a with respect to you now can anyone tell me what is the queen of a respect to you so who knows it's a of x equals u times pi of x right what's the a do you that's just 5x right that's just 5x okay so that's really easy to compute you have this right you had to compute it anyway when you when you put when you pushed in an input through this function if you just save all these five X's right and those are actually the gradients that you need here alright so you've already computed this part right so this part is it's done right let's just you know these parts are done it's just the five exits and what about this part well this part you've already computed here so all you need to do is compute this part da da Prime and turns out that's very easy all right it's just actually the derivative of the transition function so I'm not gonna go through it in more detail because it's boring and it's on the homework assignment so now I just admitted that the homework is boring but never mind it's just something if the seediest doesn't help you much if I do it in front of you the best thing is if you just work through it so please you know take the seriously go through the homework assignment and just you know please leave like the whole homework assignment all it is it just says show that the gradient respect to this thing is just the green respect to this thing times some term and so on okay and you just derive all these updates the beautiful thing about it is that ultimately what you get out of this is a very simple algorithm it just says alright here's I want to take the gradient that's back to all of these all of these different matrices what do I do I take we need with the first one and then what I do I just multiply this by a by a vector and then I get the weighted with the second one and then I multiply this again they get a third one and so on it's a very simple photo and in fact I put the pseudocode up in these notes if you look at these notes where it says backward sparse that's actually the pseudocode so in project eight that's what you have to implement that code and when I said it was easy it's like yes it's three lines three lines of pseudocode my handwriting yeah so use this oh okay your question is whether we use really gray in descent uh stochastic go into sin give me two minutes I get it yeah that's right so the key point is that when you go when you compute the gradient with something that's deep down than your network and let's say you have thousand layers right all you need to do is you take the gradient of the layer right just before hand and you take almost all of it you just multiply over one term and now you get the next queen okay so that the computing all these green is actually very very efficient all right and that's the key taking this gradient is less and less important these days so it used to be you know even five years ago that you had to like what PhD students that most of their time is computing gradients and implementing gradients all right there was a huge pain you need to make it fast and all the stuff nowadays actually this is really just a very recent development is that all these languages have automated automatic differentiation so you actually just computer fungus you just put in the function and you say take way to go ahead compute the gradient and it computes the gradient symbolically so this is something that only got very efficient in the last couple years and so nowadays if you do you know tens a flow or use PI torch etcetera all these packages they all take the gradient for you so you don't have to do it anymore but it's important to know what's going on all right any questions all right good so now we know how to take the gradient and so especially once you've done the homework it's going to be crystal clear to you okay good so please let me have our network and what we get is we get all these different gradients and then all we need to do is just gradient descent right so we just say you know W becomes W minus alpha times DL DW u becomes u minus alpha times DL U and so on then you do this for all of these and then you start over alright so there's two more subtleties two more changes to what we've done before so this is this just normal gradient descent and we've done this now and you've implemented it better I have a a subtle a very important difference and this was one of the reasons why people mocked neo networks for years and that is that the function is not convex so all the last function that we've had in the past when we divide gradient ascent and we just had linear classifiers we had a convex function convex function just look like this so we said you can't just start anywhere you want so we just started with a zero vector and then we just take gradient steps and you know and every single time you take a little step you go further down eventually we will get to the minimum right that was creating descent on a convex function we still use convex loss functions for newer networks but it's not convex why's it not convex what's happening so by curly L on the very top left is a convex function square loss as a parabola yet I'm telling you it's not convex why is it not connects yeah because a fire is centrally right because of the transition function that's right so you have a nonlinear function in there so it's convex with respect to W but it's no longer convex expect to u u Prime and u double Prime alright so the function isn't you know because you have these nonlinearities if it's if you take the second derivative U it's very easy to show that it's not a convex function so what you're trying to do is you're minimizing a function it looks like this alright something like this right and so you're trying to find the minimum here the first thing is you may as well forget about finding the global minimum you will never find the global minimum right there's exponentially many local minima like little little valleys like these and you will always get trapped in one of those right so that's out of the window finding the global what you want to do is you find a find a good local minimum the first so the most important thing is the most important change to previous is that suddenly initialization matters where you start is no longer irrelevant before it was always irrelevant right you just you start somewhere just take the old zero vector because it's convenient that's no longer the case right if you start here need a gradient sent what's gonna happen you're gonna take steps down steps down steps down to here Richie and you gonna converge to this minimum right because grading descent converge to the local minimum if you start here you gonna go here if you start you start here right so initialization is a big deal yeah of course if you can if you can afford it right train ten different networks and take the minimum what's even better is average the outputs and ensemble them but yeah you're right okay so initialization is now really important and in fact if you initialize with the old zero matrix you will actually get horrible results because then actually every single dimension is the same remember what we said last time what did the low-level features learn right these low-level features learn very simple functions and then basically the next level learns more complicated functions build up little functions but if they're all initialized the same actually what happens is there every single function is the same so what you want to do is initialize the randomly so people just you know initialize these W meet these new matrices and the W vectors with random noise low order Gaussian noise so that's that's that's a really important difference you know between you and network optimization and and for example SPF and that was one of the reasons people loved SVM so much right because they were global and so they optimize things globally you get the glow a minimum you yet guarantee that everybody chains the same SVM and the same data gets exactly the same answer with newer networks you and I train you and a probe on the same data if you initialize randomly if you get different answers right so that confuses people either people didn't like this okay good any questions about this alright so here comes the second thing and that was something I mentioned last time is stochastic gradient descent was really important so here's the problem that people did so people the first time around new networks were around they use these very aggressive solvers like you know gradient descent they've a kind of approximated Hessian steps and so on and these are great optimization algorithms because they get you to the minimum as quickly as possible right so if you know you can't refer usually can't afford a Hessian a Newton method but people don't approximate mutants methods which are very good very very effective right and so people have known these for a long time or the optimization community has developed these and they converge to the minimum in just a couple of steps and they are much much faster than gradient descent right so people said don't use gradient descent use these much fast optimization algorithms right and so you start here and it turns out yeah you're right right they just with a few steps right you're actually at the minimum that's awesome the promise is until that you know and that worked great but you never could not do very well and so the problem is that you're in a highly non convex space so where you start there's always the terrible local minima right next to you next to you all right not far away from you and now if you these very aggressive optimization algorithms that basically you know find and you know immediately converge to the local minimum right you will go right there right and but often these are not the great minute all right often the minima over here I'd very far away from you so what you would like to get is somewhere like to deepen you on a minimum which may be you know further away and that's where stochastic gradient descent comes so let me tell you what stochastic gradient descent thank you so the gradient of every single every single weight it's basically yeah you right it's basically the sum of all the data points I equals 1 to n you know D and then that little loss D right so basically there we sum over every single data point and we sum up you know every single data point contributes a little bit to the gradient and the computers overall and then that's our final gradient now here's here's the idea behind stochastic we instead so imagine you have a function like this this is my function I'm trying to minimize and I'd say it's non convex give you some other holes around yeah so stochastic you in descent or something very very simple he's saying well instead of computing the gradient over all data points which is the correct thing to do I just approximate it and approximated with a single point so I say no DL do you is roughly just this H of X I it's roughly just the last the gradient of the loss of one single point that's totally wrong right so the gradient is an average like this he has Bayesian average of many little gradients right and what I'm saying is okay I'm just looking at a one training sample at a time I just computed one training sample and that gives me an you know just compute the gradient that that sample gives me I pretend that wasn't our only training example so let's say I'm here for example write the gradient points in this direction because of target all to this function the last function but if I just take a single sample I may take you mean point in this direction right that's terrible but that's something we've seen before where have you seen this before a lot of terrible gradients boosting right that's exactly the same idea so if you have a lot of noisy gradients all you do is you just take a tiny little step size right you take a small step in this direction right and you know on average these gradients are exactly the correct gradient all right so sometimes a little bit to the right a little bit to the left cetera right give me our back to the drunk drunk a gradient descent algorithm right so you take a bunch of steps right on you know an average you kind of go in the right way and the cool thing is this takes you computed the exact gradient takes you and operations because you have to go through your entire trained ear set and compute the gradient that you know the contribution of each one of your training points and sum them up okay so that's that's a greedy right takes your order and time now what you do with stochastic we need a sense in this time a time you can actually take n steps so you take a little step another little step little other step and so on and as you do this right you move along and your gradient changes because if you're here the gradient actually points more in this direction right so after you've kind of passed half your data already you've you've actually made some progress right and now the future they're half the data points on average don't point in this direction anymore of the original gradient they're now point in the direction of the gradient at the very point of a mistake right which actually goes more in this direction so if in the original gradient descent algorithm you basically take some large steps right like this then you take a step like this and like this and so on and stochastic we need to said you take many many teeny little steps right but because you know because you can have adapted direction already as you go into the data set you actually get to the to the minimum faster any questions yeah okay good good he's saying like well wait a second well you know you switch the story like initially you started off with the local minima and now you're telling me something about drunk people pay enough that was not exactly what he said oh yeah so here's let me get to this clear this you know that this one reason ivz people and didn't take stochastic way in descent series the stochastic Gideon said was known but actually people people thought it was terrible I word and I can tell you why because if I try to prove how many steps does it take me to find the exact minimum up to ten to the minus five accuracy right it turns out the stochastic we descent it takes you forever why does it take you forever because initially you get very close to the minimum and then once you're there you take a lot of random steps you just never really hit it right so that's again once again be at the you know the drunk guy walks home right he gets this house then you can't find the damn keyhole right so that's the problem right you're right here and because it's too noisy you just never really converge Hey but that's not a big problem because in machine learning right if we don't actually care about the exact minimum right the whole last function was made up anyway right but here's the even better thing right and that gets us back to the optimization turns out now we're walking home right and turns out there's many little holes here right this function is non convex right and there's many attractors here right there's little holes here there's basically the function the function looks like this okay when you're walking downhill right now if you an exact Radian method what are you going to do is you compute the gradient here and you always kind of find exactly this minimum right but stochastic gradient descent is so noisy what it's going to do it's going to go past here it's just not going to find the stupid minimum it's gonna walk right past it Hey so because it's such a terrible algorithm right it's actually great right because it doesn't doesn't fall into every single hole along the way and you only end up at some of the large holes right where it can't escape from and so people haven't thought about this right but it turns out that's exactly what you want you want these large holes why is this people thought you want to have the really deep hole so you want to minimize loss and get really really deep right that's what gradient descent is really good at at least a second-order method but not you don't actually want us imagine a function looks like this right all right so the minimum is actually here that's very very steep loss right but I claim that's a terrible loss to get into what you really want is you want to end up here can anyone tell me why any ideas huh don't you tell me right this is your function but there's a function right my last function what is my last function my last function is a function of my training set all right Alif age of x i yr okay it's a function of two things my data and my function right by my hypothesis if I fix the data which I do here then I just get a function of my hypothesis and I try to find the minimum but what are we actually going to do later on right later are we gonna throw away the training data and we're gonna switch to test data the moment we put in a different data set here the functions gonna change and we have these really really steep local minima what happens right now we they optimize this thing and now doing test time actually have a different function right maybe it looks like this right well it weeks right I was here now I'm here right suddenly the minimum I found is really really terrible alright does that make sense so if you have these very narrow minima right they tend to be very specific to the data set that you trained on if you take very wide minima right it's unlikely that if you change the data it would actually you know would change a lot so if you what you really want us to get as low as possible in your last function on a really wide minimum and that's the only thing that STD is capable of finding all right and so that was actually that was crucial yeah if you have a very large data set and you can afford to overfit so then you're not overfitting anymore I guess so then these if effects good they did it becomes less dangerous all right that's basically what it means like that if you have a very large data set and the change of function becomes very small that's basically what it is yeah it may have been okay it may have been okay but but it's you know what's a very large data set right so you know there's always a lot more use a very high dimensional spaces but you're always under sampled right so what people found and initially they just found empirically and nobody could really answer it is if you train in your network with SGD it suddenly worked a lot better and it took us a long time to understand where they came from yeah is the number of gradient steps bounded by the number of training points no no you go over the data set over and over again so in green descent one time basically what you do is you just go over all the data points compute the gradient take your gradient step right so now what you do is you take like with any potted random compute that little approximation of the gradient make it create an update and just keep iterating this yeah so you can't really guarantee it it's just that they leave you a very narrow neo Network right then your base is very very narrow minimum right and there's a high dimensional space right the one dimensional space it seems obvious that this thing falls in here right but even a two dimensional space right imagine this here's my function and here's a tiny little hole right if I get too close I actually fall in here I think reading suddenly point in this direction doesn't point in this direction anymore right but because the SGD is so noisy or keys you're gonna point out of it again take a step out of it non non clear right so it seems unlikely that you actually will end up there it's just you're busy shaking it right and so actually the trick what people really do actually to optimize these things and do two tricks in practice the first one is they don't just take one point it's at random they take around 64 128 points and take they call it mini batches and the reason it's just very simple as just because if you propagate a single point through network or 64 is about the same cost that's just because newer computers have these little parallel vectors and so on right so it's actually you have now one cache etc so it's actually it's just efficient to do this and one one second yeah and the second thing is that what you do is initially and this goes back to your question how can we guarantee that you don't fall in these local minima initially you take a pretty large learning rate okay so then you get a really noisy you know oh my god you know um but the the good thing is this prevents you from falling into small local minima and you get close to the actual area where the local minimum is so maybe you do a a box 100 epics err pocus you want to go over your entire data set right hundred air box with a large learning rate and then after that you kind of somewhere here right now you're just jumping around right so this looks a little like this like your function you know now we're in here right and what you're doing is you're jumping back and forth between us right because these steps are too large and then what you do is you lower your learning rate by a factor 10 and then you basically take small steps and that means now I'm converging to the actual local minima let me let me draw it one more time so here's kind of the idea right imagine your function typically looks about something like this but here's the minimum so what you want to do that so you start out here initially you take a lot of steps like random steps right they're very noisy to get you somewhere here now you're just bouncing back and forth so you do this for a couple steps until you basically don't make progress anymore and then once you're here you take a small learning rate and now you basically move down here until you get them okay and when you look at the loss it literally looks like this like if you look at the loss of modern new networks it kind of it looks like this right and then you drop the learning rate and it looks like this and sometimes we'll drop them running okay yeah you can you can absolutely right so his question is you know once you've done STD for a while and now you're pretty confident you're near the local minimum you know why don't you just now use the second-order method and you can and practice this just typically not versa to implement it even because now you you know you did a lot of you know you just take a couple of iterations of the small learning rate and you're good to go but yeah you could absolutely yeah yeah oh yeah yeah yeah this is just it would take me in a month to explain it but you know if you're interested in there they're basically there is that a whole body of literature now that analyzed I'm sorry the question was is there more rigorous answer why STD gives us good local minima and the answer is yes there's multiple you can formalize it in multiple ways if you want if you come after lecture I can give you a few intuitions why that is the case yeah oh yeah yeah so the coast questions like what if whatever function looks like this right that's a bunch of you know why and actually that's what it's gonna look like like you have millions of parameters you're in a million dimensional space right there's not just a few of those right there's billions of minima right and you will end up in one of them one thing that's nice about this though is that if you want to do bearing with newer networks you typically have enough randomness that you don't have to stop sample your data so typically when you do bagging your sub sample n data points with replacement with newer networks because you know if you just initialize your network randomly you've started to a different part of the space you end up in a lo different local minimum without even subsampling your data set differently you actually get quite differently behaving new networks surprisingly they tend to have roughly the same error rates but they make very different mistakes and this makes new networks amazingly well suited for ensemble so if you really want to get the lowest error for example in the Calgary competition right so what you do is you just train five newer networks and you average their results right and that actually makes a huge huge improvement okay any more questions alright so I have I want to show you a few things ok good and two last things so actually before you finish I want to get to one thing what's the neuron and neural networks and so people always talk about new likewise are called neuro networks and so this is all just functions and I try to explain that way because I think that's actually kind of it's really not that different from what we've done so far in the class but let me quickly explain to you why people call in your networks and what's the neural view and I'm not a big fan of it because it leads to weird negative articles in Wired magazine or something but basically what people say is well you can write this as a graph in some sense right so you have your X comes in and like I write like this is kind of these these images that people draw but every single dimension of X is a little ball here a little circle okay so this it isn't a vector X and this is the first the value of the first dimension second dimension you know there's like a five dimensional vector okay make sense it's just a vector notation raise your hand that makes sense okay awesome so now what do I do I stick my accent here I remove the right and what do I get I get yeah my eggs are sticking to the stubble you know into this this first function so I get a new representation it's called as Phi double prime of X right and this has a bunch of dimensions and the transition here is X goes to u times X right u times X space that's what it is right so Phi xx equals or you know it's Sigma u double chyme times X okay make sense raise the end of that make sense okay good and now we can write again as he has my Phi prime of X this is my Phi of X and this here's my H of X and now if I think about this what is this here well this here is a function of every single input so what people do is they draw and draw an arrow I just say this value here the first dimension of this guy isn't its base the first row of this U matrix times X this is so they just call these these connections then they have so everything is basically connected here and then you do this for every single is illegal a and now you have a network right I don't like it I could tell you why I don't like it it's because people that have done these analogies with the brain and people have done analyzed these neurons and said like what what do these neurons do etc and and the problem the reason I don't like it very much is because it's really just a end you know a d-dimensional space well you could just rotate the space and you get totally different representation but it's exactly the same output right so in some sense it seems odd to interpret the different dimensions of the space I don't see too much value in this and but but it also has led to a lot of misinterpretation of misunderstandings between people think hey this is what the brain does which is not true right what they say this is kind of you know these neurons are doing a lot more than they are actually doing it's really just a function Phi of X and you know of course that's a you know some D dimensional vector so each one of these vectors has a value but just you know it's important for you to know when people say oh I take a look at a neuron the value of a neuron but they really mean it's just you know one of the dimensions of this upper vector any questions by the way initially it was inspired by the brain right so the original neuro networks came from that direction yeah okay good in practice do people use anything else than rectified linear unit and the answer is nowadays rectified linear units are very very popular and the reason it's actually quite simple is that these but people use before it so people thought you have to use a continuous function and differentiable function so rectified linear unit if you paid close attention you realize it's not differentiable so you do gradient descent on a non differentiable function right that seemed crazy right and that leads to all sorts of problems turns out if you do STD it's so noisy anyway it doesn't really matter all right and so when people started doing STD you could actually use rectified linear units right and then actually the advantages over these tan age and sigmoid functions became apparent because these tan Jason Sigma forms is a very very flat alright so remember what they look like they kind of they have everything from zero to one essentially and you get you basically what these networks with sigmoid internees functions get is they saturate very quickly so rectified linear units tend to be better at not game trapped in local minima yeah oh is he your ATK was a big deal when you use these sigmoid functions because you want to keep them in the middle weight DK is just another word for l2 regularization there's nothing else right it's just something was just invented parallel and what's called Beatty K so people also use regularization on the weights but yeah so it's less important for rectified linear units but it's still do to avoid overfitting okay so quick okay good those of them I want to show you last time and it didn't really work so so now he here is so this years after this dysfunction that I the black dots are basically my training points and actually on the left hand side is a newer network with rectified linear units and you see here the structure is one input to hidden notes and one output so two hidden notes means my first function maps my one dimension input into two dimensional output and then the next layer takes the two dimensional output maps to a one dimensional album so what you basically map here is the number of dimensions and what you see is the rectified linear unit has two notes so what it does is it basically you know as first you know these two lay as busy to to change points and you see town age is much much much smoother right I can now introduce four notes and one thing you can see here there's the reason people thought tonight is much better right because tan H here almost hits every single training point right the error is much lower where's rectified linear unit is kind of you know they are kind of struggling here but the reason is these problems are rectified linear units have is because they are hard to optimize go away when you have millions of them right then basically there's always some that point in the right direction and so rectified linear units turn to be turn out to be better at complex problem but they're worse at these little demos which was misleading so now you know I can make eight a dimensional hidden representation and and so on so you know if I make it more and more complex you see you know one thing you'd see here the challenge function optimized this very very nicely and here you still have these these piecewise linear functions what you can see here is that in some senses are newson aiding some some thing here right so the function goes up here despite there's no data to support this that's fine all right and one thing I still want to show you real quick is so here's a one thing tensorflow so I don't know if tensorflow is a prepackaged Google I see most people prefer high torch actually that comes from Facebook but I think hands are flow is a little easier to use what you can see here is we can take a little dataset easier my data points positive and negative and I can now construct a newer network you can play with this if you just type in tensorflow playground it's a nice little demo that they provide and so here's my input this is the first I mention of my input that's the second dimension of my input so is that either based is my x coordinates my Y coordinates and then I have a bunch of a 4-dimensional learn a four dimensional representation this is my Phi of X and this is my output let's just make a one-dimensional output Y naught and I can now train this thing and what you see is here this is the output right so it has now trained this and you know gets it all correct right and so one thing I can now look at I can actually look at these you know one layer new network is actually looking at these neurons makes that so here actually what you see is it learned a bunch of functions okay now type in X or make it a little harder optimize this and you can see here right oh it's struggling come on all right so here's what you do you know the struggles you just add more and more neurons right so here we go all right here we go better right so and what you see is these is here's my inputs and I can now look at when i'm hover over this you can see this image here on the right changes so what it shows me is what the activations are of this particular function and so you casey see it learns these individual little functions and the final is now composite of these functions which is exactly in our nails the the X or data set right now I can do something really hard but I can't do this one first this one should be easier let's see if now it should be able to nail this no time oh yeah Oh beautiful right and now the spiral spiral is actually very hard right and so we can now see if this works it probably won't get it actually what is it doing oh yeah so one thing you do now is we can add a lot more neurons see what it does now alrighty I had another layer it's like a lasagna more layers is usually better alright is it doing something it's starting the whole thing is 1 since JavaScript of my laptop's there's a little unfair right I mean there's like alright it's doing something well we could add a lot more I don't know it's just you know okay let's just max it out it's through rectified linear units okay let's see what happens no not yet it will get there eventually it can't do it you can't do it it just takes a while all right let me interrupt this one last thing so just last time I mentioned that this has become extremely powerful with images and so there's actually just to show you here's an example as clarify as a start-up they actually they sell their deep learning community all they've done is trained deep nets to you take input as images and then classifies on the right the classes that it gets put into so there's an image the baby says the sunset water dawn dusk and so on and so one thing you can now do is we can try our own image so I can actually go to Google Image Search and I know what should i search for any elephant Killians right I see have my okay all right so here's an image that's a little small I could try it he's a this how about this this is a picture of when I was younger when I still lived in Germany okay never mind all right so here's the image all right I'm not a robot okay what does it say it says success oh my god all right you gotta leave on a high note so this is the last one see you on next on Wednesday 
","['', 'logistic regression [1,2]', 'neural networks [1,2]', 'deep learning [1,2]', 'gradient descent [2,3,8]', 'chain rule [4,5,6]', 'backpropagation [5,6,7]', 'activation function [3,8,9]', 'loss function [1,2,8]', 'hidden layers [3,8,9]', 'error function [2,8]', 'learning rate [8]', 'cost function [2,8]', '嘰 (Ji) - Chinese character for loss [1]', 'squared error loss [2]', 'rectified linear units (ReLU) [9]', 'TensorFlow Playground [9]', 'Xavier initialization [NOT MENTIONED, but might be related to the lecture on backpropagation]', 'Adam optimizer [NOT MENTIONED, but might be related to the lecture on gradient descent]', 'softmax function [NOT MENTIONED, but might be related to the lecture on loss function]', 'convolutional neural networks (CNNs) [NOT MENTIONED, but might be related to the lecture on deep learning]', 'recurrent neural networks (RNNs) [NOT MENTIONED, but might be related to the lecture on deep learning]', 'bias term [NOT MENTIONED, but might be related to the lecture on neural networks]', '']"
"buddy this is the last lecture of machine learning I'm said to all right so given that this is the last lecture and we actually last once one day short in some sense because of the snow day actually last one lecture and unfortunately I am it worked out that he's ever exactly one lecture short that's okay but that leaves me or leaves us with a choice so ultimately this is your class so I want you guys to decide what we do so we can do there's multiple things we can do today so I know some people have been asking for a review session so the exam is next week and exactly one week I believe yeah so one thing we could do is we could go over all the material briefly kind of to a you know a three minutes mini version of the class one more time and just summarize the important points that's boring but useful and another option is that I could become continued deep learning and I can talk about convolutional neural networks and know a lot of you would like to hear about it that's also how you in the Calgary competition and then some people asked what's going on in research right now a machine learning so what's you know after this class so one thing you could talk about is what a different area is machine learning that we did not cover and what a research topics so so these are some senses you have four choices I believe so one is review session one as we talk more about deep learning one as we talk about you know other other stuff you know other classes in Cornell that Arbor Mich machine learning like what is unsupervised learning was active learning what is reinforcement learning all the things we did not cover and finally what is current research and you know I could show some research that's going on in my lab so these are the four different choices and so why don't you have a quick vote and then I decide to ignore your vote and just do whatever I want all right so okay first one the first one is review session so who was in favor of review session okay oh yes it's gonna be hard actually it's gonna okay wait one what's that piece of raise your hands only one hand okay alright may have to come down to one more vote like in France okay that was review session second thing was deep learning what's the more about deep learning okay we use searches out all right third one is other classes at Cornell nobody okay forget about other classes was alright and finally you current research that's going on oh okay well is that a majority okay one more time deep learning okay how about current research on deep learning how about this this okay have a winner all right okay good so we can do this that's that that works fine so what I need to do though is I need to give you a five minute explanation above components are actually because that's what current research so is on so in the last lecture we've done in the last couple lectures we've talked about deep neural networks and if you remember correctly and I've you know summarized is now many times CBC say you have a function Texas input some data instance and you would like to make a prediction let's W transpose Phi of X where the trick is that Phi actually is another you know functional inside that function that you know it looks something like this and for those people who have implemented the homework you know you know all about at this point and I realize every single lecture I call this something else you all right and this again can be Phi prime of X so you have this nest that these multiple layers of in this case we have just one hidden layer okay so that's great and that's what people call a fully connected Network and a fully connected Network means if I write this as a network structure but take my original input and I write every single dimension as a you know one of these circles there's my X then what I learn is the first identifier of X I call this Phi prime of X then I learned Phi of X and then I learned my H of X and every single dimension of this function here has its input every single dimension of the previous previous representation so this way this notion of network comes from and it's fully connected because everything is connected with everything else and that's the most general form you know of course you can draw this out at the any reason you have one dimensional or multiple dimensional outputs so that's great that's very very general but turns out and you work on images you have a lot more information and you would like to incorporate that information into your classifier and in fact if you take your image and you represented as a vector right you have an image that consists of six million pixels right that's you know whatever the resolution of your iPhone camera and if you vectorize it that every single dimension corresponds to a pixel the problem is you've now thrown away something right you've thrown away the fact that actually it was a two-dimensional image and well you say you know it's not a problem right like this is something that just represent it differently I just represented as a vector but turns out natural images have very specific features or very specific properties they don't apply to data generically and that is let's say you have a little picture of you know some good-looking person like me here we go and if I now you know if you calcify this you say okay this is whatever person X right if I would now move this person and move it a little bit to the right right it would still be the same person I'd if I move a little bit but down something right it wouldn't change but this vectorial representation would change drastically and it would look completely different because the pixels would be completely different than those two images and they call this translation invariance so images are locally translational invariance so if I take any part of the image and move it a little bit the image actually the semantic interpretation of the image does not change and that's crucial and the reason is crucial is because if I train a neural network to recognize objects and image if I were to learn this yearís you know whatever person acts this years you know whatever let's say Donald Trump or something and I'll give you less good-looking enzyme if you know and you know it but always recognize if it's exactly this position but if you actually have him over here right you wouldn't recognize him anymore so what you would need is you would need training training data in where you know he's basically in every single part of the image all right so you have one image like this and what I mentioned like this etc right so you need be millions minutes of images so what convolutional neural networks do is they change this function Phi to incorporate that so basically say if you would change your data input a little bit but just moving things around it won't change the output and it turns out that's that's the big hit like that made all the difference in the world because it reduces the amount of data that you need to learn something exponentially losers what next well I agree let's keep that let's keep that's good I like it all right so what convolutional neural networks do they take an input input image and they respect the fact that's an image and so they actually say the image has multiple channels so actually typically what you have you have three channels so what that means is if you have an image that has natural colors you can divide that into red green and blue right so if you actually you have three images where this one just shows you all the Reds you know how red every single point is that shows you how green every point isn't it's now blue every point is and if you add these two up you get the color image mm-hmm so you take that as an input to your ex to your function v so this is my X my axis basically consists of three different images and what my function Phi now does it takes that input and then it generates new images and this doesn't have to be three could be many could be a hundred or something it doesn't matter and then my next function Phi prime takes these as inputs and generates another whatever one hundred images in size so you never ever you know destroy the image form and only at the very very end do you get a final representation and you know I'll put some some H of X which is actually so maybe here you actually have some some fully connected layer or something okay so far so clear raise your hand if you're still with me okay awesome so how does this work and the trick is the convolution operator so what this function Phi does it actually has a little you know instead of actually taking a weighted average this image here actually is associated with a particular convolution who has some sort of convolution raising hand okay so you basically what you do is you have a little teeny little image of maybe three by three pixels and what you do is you move that image around so you start here and you multiply this pixel with that pixel and this pixel of that picks on this pixel by that pixel and you sum them all up and that gives you a value and that's the value here okay and now what you do is you take this and shift it one to the right so now you take this pixel here Matt you have at this pixel and so on you I sum them all up and you get this value here so you take the same this little image and gets move it around and compute the inner product everywhere and that gives you an image here so what that does is essentially you're kind of blurring the pixels right so the pixel here is an average of the corresponding pixels around the same location in this image so that's what you're doing but it's a weighted average and that's the key so these weights this these these values here actually weights they basically tell you how much do a value now how much do away the pixel here here and here for example right and I imagine you get strong weights here but low weights here what you would be doing is you would get a large value if in the image here you have something similar you also have you know something like this and so what you're essentially doing is you detect edges that look like this right so what's happening is that these images here what they are doing is they are basically detecting local features in an image so some of them detect edge edges like straight edges or you know horizontal edges vertical edges circles etc and you keep doing this so initially you just have very low-level things that you're detecting and so you get these images here that means the highlights highlight certain certain features or highlight edges or highlight circles or highlight something like this and then you know here basically you know you highlight you know more complex things that's correspond of you know out of so for example here you could be highlighting eyes right eyes consist of edges and circles and so our eyes all look pretty similar actually like no matter what you tell your your date right it's actually yeah they're all look pretty similar so so you know what bassy what you can do is you can build an eye classifier or something somewhere here that basically is a kernel that BZ goes over the image and scans for eyes that's essentially what they are doing right then you have these little things that basically check for eyes right but these links here again because this of little you know is there an edge or is there a circle or something does that make sense you're building more more complex pattern detectors that's essentially what it is yeah are you you're right so you do actually have nonlinear radio functions in between yeah so these are actually made of functions in here that's right so you said everything that's negative to zero but you have to have this otherwise you just have one than the operation yeah good point okay does this make sense any questions and the way you train is it's just the same way turns out actually the convolution your network it's just a special case of this right so you could actually view it this year is one pixel in you know in this image here right and what I'm doing is I'm basically you know setting zero weights to almost all of it and just attaching it to you know just to the first 3x3 pixels here so these are the only only connections that actually have nonzero weight everything else has a zero weight so you couldn't take a fully connected network and make it identical to convolution on your network so the fully connected network is always more powerful but the promise is too powerful right by restricting your network to only learn functions that are locate a translation invariant you actually make it easier for the network right you're saying whatever I'm learning it has to be something that if I shift the image around a little bit the content the image around it shouldn't change much right and that's crucial that's very very important to understand natural images and the if I would say the big success in deep learning was essentially in these two domains so one was speech recognition and the other one but the you know by vastly vastly more successful is images so convolution in your neural networks is really the the area or your computer vision is the area but deep learning is much better now than anything else and so it used to be that computer vision used all sorts of different paradigms SVM's etc boosting and convolutional networks have really in the last two or three years actually taken the computer vision conferences by storm and now nowadays it's you know if you go there you don't see a single paper anymore that doesn't have convolution on your network something yeah yeah so there actually owes a little bit or laser irritation Americans not not all that much right but wait a little bit because you're blurring but if you blur you you know if you wrote it a little bit you it still looks the same roughly right it's good point you can explicitly point that a code that M and it gets more complicated the nice part in your networks is that it consists of very very simple building blocks but can you can learn very calm play it complex functions that way and so here this convolution is a really simple building block yeah I will get to this that's exactly what I want to show you actually and so maybe I can show it to you so okay any more questions about company confident so like C well I'm sure I'll show you now is research on confidence so basically trying to improve the state of the art never tried to go over it slowly but please please be free to ask me questions [Music] [Music] you learnin your London so you do background the question is how do you learn these convolutional kernel better they come from and the answers you learn that's exactly what you learn those are the weight of your neural network and the talk I gave very recently at Nvidia actually Nvidia corporation they are very interested in deep learning because they sell GPUs why don't I actually quickly go over there so this is kind of a summary this how I summarized to them deep learning and it starts with the perceptron right so the perceptron invented 1957 at Cornell and the idea you know you know that's very very well you implemented it yourself you just have this hyperplane that separates positive or negative points right and so let me just write on perceptron as a new network right it basically is the following you have your input and it's directly connected to the output node and every single connection is just one weight right so if you look at the new network graph right you encode an inner product W transpose X just as this graph right so the value of the node here is the input this is my different dimensions here x1 don't ever x1 to xn are XD and then you multiply them by the weight that's associated with every single every single edge so I know if you can see this here's my x1 x w1 and if they come together here then that means you add them up ok does that that graph now makes sense raisi on that graph makes sense ok awesome right so when people talk this in some sense the way people did think about neural networks and actually I love these graphs actually it's not that I think they are bad I just the problem I have is when people start relating them to things that's going on in the brain because often that that's that's less helpful and so one second so in deep learning the only difference being what we're doing is we have one additional transformation W where we have our X and we just applied x times W right then you have a nonlinear transition function which in this case it's the rectified linear unit and then we apply our linear classifier right so this years my Phi of X for the longest time if you think about what the kernel is a kernel takes your data and maps into a very high dimensional space before a long time people just thought you just what you need to do is you just need to take your data and map it into a ridiculously high dimensional space and that's very very good and that was in some sense we could show that that theoretically it's just as good as a deep network right but turns out that in practice actually if if my bags on vacation Street because you have GPUs then it's actually better to have just many of such layers and that's the new thing that came out in the last couple years but suddenly GPUs are fast enough to actually train these things so just all the time you know here deep network all we have is like X money up at W let me have a you know rectified linear unit then we multiplied by another matrix and so on right and matrix multiplication here is just a fully connected graph so yeah so deep learning really when he works so here's the interesting thing so yeah so here comes my my research so there's something that my my group and I have been developing last year and basically try to fix a problem and here's the problem so these are kind of this the way the new neural networks look like they are very very deep right so if you look at noon a Rutgers now people have even know 50 100 layers or something and so one thing that's kind of going on in your network is you kind of what you're doing is you're playing a game of telephone right it's a CO telephone yeah telephone where the first layer is that this guy here is the input but it sees this car and then you can have talked to each other until the last layer right and the car stays is like it's a car right and the reason the game telephone is fun it's because people always mess up alright then you see something up so here we have you know she took you know it says bar right and then you know because this person only hears the output of this layer you know the the final output actually is wrong okay so the the in some sense the reason that the gain telephone is set up in a way that basically have every single person is a bottleneck and you're just waiting for someone to mess up right well that's exactly the set up be half an hour neural networks right so we have many many many in many layers then what happens is that each one of them if you know that messes up and extracts the wrong features doesn't really fire properly everybody else afterwards is you know looks like this so the promise during training you can't tell because they're in training actually these layers don't mess up right well if they mess up you just do a gradient step and they don't myself again right but the question is what happens during testing and the longer my linear networks are the more likely it is that eventually I will mess up so what we're trying to do is we're trying to reduce the impact of layers and so one thing that someone traduced is a very informational paper and this was 2016 coming he from China and he had this great idea maybe he said we make additional connection we connect the output here to to this layer so if this guy messes up we still have some kind of we still have the information here around it so intelafone that kind of what be you know you listen to the person before you but yeah but you can also listen to the person before that he said I get what did you say right and that would use the probability of mess ups that kind of works but you still have the same problem like it's still you know it's still the the issue that doing training these layers don't mess up right so what the lab rogue learns is that actually you can kind of ignore these to skip connections so here's what we came up with there's a very very simple trick but it's very very effective that's why you know that it's good to present it today so let's say you train in your network and it's really really deep right there's a really good trick you should try it next time you train a really deep in your network here's what you do you actually introduce a coin right so for every single layer you introduce a coin and you flip that coin every time you do the green update and if it's heads you keep it if its tail's you remove it so just be surgically remove certain layers from your network random and then you just pretend the network is much much smaller when you now do an update right you just you know propagate forward need your gradient update right so you get your original network and the next step you do this again so now you trust again times you remove different different layers so every single time a district a stick you need to send every single time to do an update I remove different layers so I'm really kind of messing up my network whether the worst possible thing you could do right and so the reason you might do this was the one thing that I'm you know it's not actually the continent all the same distributions so you don't wanna move the very first layer you never gonna remove the data right and you have nothing to go let that be an advice so you have a 0% probability of removing the data you have a 50% chance of removing the last layer and you kind of interpolate between them so you know when you train Beasley you know you train this kind of network right so every single time you never see the same network again it's very unlikely when you finally classify you actually take the full full network I can tell you now why this is a really good idea so here's a really good idea here's why it's a really good idea the first one is the final network is one that you that he actually used during testing is where every single layer is there that's something you never actually trained on all right not one story training did you see that network it would be extremely unlikely so what you're actually doing during testing is you take an average of all these networks that you trained during training so you can have half of some ensemble but the other thing is a lot faster because during training you only just take these very very short networks and finally you know you actually have more information slow I didn't go into this but they actually the gradient actually propagating the gradient so many layers at the end you you're badly of anything left stupidly anyway let me show you let me show you the effect of this so here take that this was the current world record on all the tireless datasets image you know there's actually cypher tenseness but it's all the works on image net there's the current era this that this is these are images where we have ten different classes we try to classify them into you know is this a boat this is a house in cetera and if you take this network and the only change you make is to introduce these coin tosses and move networks to layers during training this is the era that we get right so piece of the error job stress this is the this was the world record at the time right so and everything about these nap air datasets every single dataset we tried the error goes down and the reason it is very intuitive right so busy what you're doing is during training like you're trying to lower the gap between training and testing area your training error actually goes up are you testing error goes down because during training you actually simulate the fact something goes wrong and one thing that my students want to do is I say okay well this is cool and now they can exert a really really deep network so beforehand that rotor get 110 layers so they meant 1200 layers which took about as much electricity is all of Ithaca something I'm not not entirely sure but it was a lot so this is what it looks like there's a thousand layers and that gives you a little bit different heads and so they published this and this most like she gave a lot of get a lot of attention and then I felt really bad I was like why did I have what if I'd done all right I've told the whole community to Train networks never 1,000 layers right this is like you know the amount of electricity on co2 I'm generating is like you know this is I the opposite of you know what I believe in so society something is going wrong a Fed responsible for this right I told my students that they were like give all like this is awesome and I was like no it's not it's terrible we've done something wrong we went you know I have I don't want to go in that direction so so we be care with something else and so here's the idea that actually in some sense what a network does is reads the previous state every single layer reads the previous state and rides the next state so you can kind of be the Z on your network is it's kind of a function right there you busy read the state to some manipulations then write it again and what we could show is that if we actually just remove something in the middle that doesn't make much of a difference now that's really weird right that basically means that these these individuals functions these are my functions Phi yeah right they're not doing much all right they can't be doing much because I just removed them randomly right like if is always like you know someone now if you just randomly remove something and there's no difference well that didn't do much right so we have it a ridiculous amount of redundancy that's basically what's happening in these networks so here's an interesting thing well let me just explain this so the problem is in some sense we're still playing the game of telephone just occasionally we remove people right so Evan has to speak louder right if you don't know if the next person is still gonna be there you have to speak louder that's because he what is so and that's that works that's better right but it's somewhat still stupid it's still very telephone so you know there's a reason we don't play a telephone when we have business meetings so why don't we do this why don't we do that setting instead everybody can talk to everybody so believe it or not you know this is something that you know no one had thought up until this year actually we just showing this this is Jessica polished presenting this next month so we've AZ take we have an input layer input image and what we do is we take you know the first function but that actually is the input to every subsequent function so every input every function because it takes its input or previous functions right so this function Phi of X since this function space it takes this function as input but also actually this layers and put in all it's a previous layers let me not go to the details the beautiful thing about this is that actually you have no more redundancy like all of these layers basically in some sense learn everything they learn you can can reuse later on if you do this just a very small network immediately improves your result there's actually currently the state-of-the-art and deep learning so if you wanted a deep learning you wanna win a prize on Carol Hughes dance nets not many people know about it yet we're presenting it next month so you have an edge for one month and then of course my students say they are to make a really really big dance now so so that actually you can't get this there's a 26 mega a million million weights that's it's ridiculous actually it's it's illegal in some states I'm sure but actually if you do this you actually you can pretty much win the you know be the world record and any dataset alright good that's one thing so just keep that in mind because you know it works really really well so that mean let me ask you let me show you another project there's something that I asked my students and we've worked a lot on these deep nets with images and so one question I asked my students oh why is it right the companies work so well on images right like the others translation invariance but it's still like it's so amazing right like if you put in this translation of variants into other project I will insist it doesn't help all that much it's not as good as confidence so why is this and so so had a conjecture and my conjecture was the following that what deep nets are doing and there's very interesting by the way this is this transition you're in machine learning so machine learning used to be completely deductive so that means we have some acts axioms that we think of and something via it was like boosting for example as complete adductors right so we think if you have we Gunners we have the following properties and now we be basically if you assume these axioms we can now derive an algorithm that is following guarantees inductive researches the other way round we do you have something and you try to like knowledge it you write you have the cells made by some I don't know who made the cell right it's alien technology to us and we try to figure out we telex two experiments so in some sense what we're doing is actually something inductive yeah which is very I'm machine learning so we kind of look at these your networks they say well why what are they doing what are they learning and there's busy this you know this very deep you never my conjecture was images lie on manifolds right so that's we know this because otherwise we couldn't do anything with images and also if you were just randomly sample image you would always get noise right you would never actually could just randomly assign pixels to an image look at it it will never be anything interesting right so then we know the lines are very small part a part of the space of all possible images and we know that locally smooth because if you take two very very similar images and average them you still get something that resembles an image a lot so what I was cleaning is but deep nets are doing is that all this convolution your linearizing the manifold and there's not something that's you know I didn't come up with this conjecture I was you know there's something that a lot of people kind of just kind of you know people were kind of talking about this was in the air right how do you how do you figure that how do you prove this stuff so here's you know an experiment so but what I'm claiming is like we have this image of this manifold of images so manifold is busy this curled you know surface in a high dimensional space and I claim that this new network somewhere internally turns it into a subspace and then actually everything becomes easy and so this doesn't come from nowhere because at the end the last layer is still a linear classifier so in some sense what we must be doing is we must be somehow linearizing this and one thing people realize is if you train a new network on some task and then you actually just take that representation and you stick in some images for some other task it still actually works pretty well so somehow they're doing something more than just learning that task they're somehow you know and their conjecture was maybe you know they're just linearizing the whole manifold of the images so here's the here's the idea I came up with so you know let's take a problem that's really easy for deep nets but it's pretty complicated like demands have beards or not right so here we have people these are all men and they have no beard and these have beards and so I came that and on this manifold they look really far away from each other but it's like jig - bearded man and take an average right it's not gonna look like anything pretty right just because they have beers does not mean I can they look alike right so look at all over the manifold the same with men without beards you can't just take you know to like a but do people understand this my averaging analogy if you have like a manifold like this and I take an image here an image here and I taken with the averages here so it's off the manifold that means it's no longer a true image does that make sense the only things on the surface are images are true images okay so that's all you realize if you take two images they're really far apart right take you know Donald Trump Hillary Clinton take the average between the two it's not a natural image anymore right so but that's even true within you know just generic people that appeared without beard but actually a linear classifier works really really well on that space so you can train a deep net very easily to classify you know this person have a beard not so here's what that what I want to do I said okay well if my projectors true then I could take a network that's not trained on beard at all it's trained on classifying dogs and cars and trucks and anything now I stick in an image that has never seen before this person and I move him he doesn't have a beard arguably and I try to move them towards the bearded people they should get a beard that that's my that's my levers my contractor so we tested this so this is a setup so what we did is we took this new a network that was actually trained by not by us right we want to be you just say what a neural networks do that other people train so we took it let's chamber the University of Oxford you know and they put the new network online so we just downloaded it now we take this image that was clearly they didn't have right I know this for sure so I take that image in and I I represent this image now as all the activations so what I'm doing is like all the internal basically values of the neural network make one long long long vector right that's that's how I represent that image okay now I take the space and I claim this is now now here in the space basic things are linear it's no longer manifold is now a subspace so here's I do this I take additional images of people man they kind of look similar to me more or less not George Clooney it's not there and without beard and with beard and I average them okay so this in some sense the average man without abuse is the average man but with a beard and other questions if this describes a vector right is the difference vectors this is the direction of beardless right in this high dimensional space if you go along this direction you should get a beard that's that's my hope if it's a subspace if it's not a subspace you're gonna move off into lala land and it's gonna be look ugly right so now we can take this vector and add it to my image and now hopefully this here will now be the version of me with a beard so how do we find this out as well it's quite simple actually so we now take this go back to our newer network and here's what we do we just take the we just take you know this may see a whole lot of numbers that basic correspond to values in this new your networks now we can just populate the newer network with these values and now the gradient descent not to change the weights but to change the input image and saying which input image would give me rise to these values so which input image would give me this point okay so I'm just reversing things alright does that make sense crazy under that make sense okay awesome all right do you want to do it are you ready alright one two three here yeah how it works and there's nothing special about me so you can take other people's I randomly chosen people I mean some celebrity datasets so I know some of them now personally it just recognize them and so they get us take them an ad at beers to them but like there's nothing special about beards either so we can't just take you know take these images and and you know make them look older make their mouth open right so here we can make the eyes open this looks little weird here but this guy pops out right away you can make them smile right them glasses right all that stuff right and the important thing though is that all of these are just linear interpolation so all I'm doing is I take this image put in the space I'm not training any neural networks right I'm just putting in the space I add a vector to it and we constructed right and I'm busy and it's really about of improving a point right I'm the point is that now in a subspace in a subspace everything becomes easy heroes do something finally we can take people's faces and cut out the middle part so this way we move off the manifold then because they were back into the manifold and then the face comes back all right so these are by the way these are not the right faces these two totally different vision it's just a face but it's better you know it tends to give them in female faces and the man but then this just means there's not that doesn't learn anything right it's just space it means it has not a space in which way if I go back to the subspace I'm you know like it somatically makes sense right so that basically women on some reason and men and the other reason we can even try there some very high resolution so this is now a thousand by I think nine hundred pixels so this this fellow years we can now take this point at this guy in and move him along along the direction of be redness and so this is what we get we can go be back again right and so it's not bad you look at your boyfriend say like you know or maybe maybe stop here right like this is my yeah so of course my Kido can't even believe like it when I went to my lab like every time that my students are hiding some things like oh we just made you into a woman have you just made you a lot of it I I don't I don't want to see it huh nobody wants to but that's a good one this Macy does I think at the very first lecture I showed this image this is kind of they took me and aged me right I like this a lot because it kind of age they oughta make me younger and then venture my wife and ask is that what he used to look like when you met him yeah and she was like yeah that's kind of what he looked like all right so but but you can see basically you know this is kind of the progression of Aging in us they give me puts and wrinkles right and puts in I don't know well that's what I have to look forward to and anyway oh yeah there's also a good one so this is this is a boy so obviously the image that doesn't exist right until you know surprising how much fun you can have like adding beers and the people its indirectly so that works really well actually for men like we've had actually uh some failure cases where does it not work and this was a very interesting where breaks and because it tells us something so this is this person here and we gave him a beard but he also actually changed his race right so he looks a lot darker and what we found is that basically what we did it's like the images of people with be a view try to find similar people so that kind of the people are bald and actually have a beard and to that that's actually more common I guess in the african-american community so somehow we actually had a vector that can point in two directions business and you know a different race basically so we thought that's interesting in some sense busy it's you could actually do two things you could piece it to two changes right first ad Bearden and change trench race and I see turns out women is much much harder than math which is for two reasons a because the manifold like it's more complicated of women and that's because there's a lot more variations going on so man don't you know it's kind of funny man can all look pretty similar actually so there's a women have more variety in terms of lipstick or you know like makeup and then the hair is this much more variety compared to men so what you would need a lot more data but actually we get our data from the internet actually turns out as you get celebrity is actually that a lot of most politicians so there's a huge bias towards man so we actually had few of them in but you would have needed more of them so this is something that I think it shows you buddy how densely sampled their manifold is it's crucial I think that's the last slide I have on that project is any questions so anything that I talked about okay good the next actually you know what I can actually show you one more demo okay so yeah top out chop-chop out I mean o0 there are our-our dropping layers and intense nets so I said some more time chop out you mean I see so drop out what job are does but we're dropping layers so you shrink the length of the network cha POW changes the width of network right so the change in the width of the network is not so it's not such a big surgery in some sense right because you're just removing a few notes if you change the layers you could actually change multiple layers that you're really kind of breaking the network so the effect is very different actually a job part used to be just for the also don't know it was a something introduced as a regularization method you just take different dimensions to set them to zero inside a new network and but it actually has recently like it was very popular for a couple years and has recently gone away because people now use batch normalization as a new thing so the one thing about deep neural networks is the area is moving very very quickly every few months there's completely new papers that come out and so something that's within the three years cannot a lifespan of any technology and then new things new better things come out alright good so well I know who won last time oh yeah raise your hand if you want to see one last demo okay sure okay good so this is there's that's about research this I guess at the yeah so away what I thought I kind of I was ready for any outcome of this of this of this vote so what I want to tell you about the other class so the other class is machine learning for data science so we have this is machine learning for intelligent systems and the other one is with data science and the distinction actually is that intelligent systems in this class what we did is predictions so we tried to busy say let's make a prediction so to predict something and like in a self-driving car or you know to the predict stock market etc for decision-making in the other class machine learning for data science what you do is you learn that you discover structure in data so you have basically imagine you have a data set your biologist you try to make sense of it then they see what you would like to find cluster structure you've terrifying low dimensional low dimensional sub spaces etc so there is something I worked on it so you know you saw just now I talked about manifold so one thing I've been working on a lot of my life is actually mad learning manifolds and so this is in some sense why I'm working in the bucket has presented you like how we're learning manifolds it's really something I've been working on for 15 years actually like my PhD thesis was on how to learn manifolds so I've been thinking way too much of my life actually about this I think the question is given that you data lies in the manifold how can I learn that's this low dimensional representation and this wasn't you know 10 you know maybe 12 years ago this was a very very big topic and I can show you some here you go some of the artists actually algorithm that was my PhD thesis is here you have data that's in a three dimensional manifold it's like two dimensional manifold that's embedded in the three dimensional space so there's an unsupervised this kind of theta analysis it's a kind of stuff you would learn in the other class and the idea is basically you know here's what you know what the algorithm would do and people thought it was very exciting I thought it was extremely exciting at the time and you have this dataset it's it's true it's three dimensional but it's truly two-dimensional how can you discover this and what you see what you do is you build a graph and you can flatten that graph so there's what happens and actually it learns that actually this lies in the two dimensional subspace and it's a very similar to what confidence are doing so you actually now can you take any point here and move to any other point and you just move in a straight line and what you're doing along the straight line actually is a you know it's actually a meaning for distance R as in the original representation if you would be the blue blue point and he moved in a straight line you would always move off the manifold right and you move somewhere where there's no data so once you have this representation you can you can you can do a lot more about the data set so that's the sound work and I know the sound works can I test the sound server I think it's on okay so so this was my PhD thesis I worked a lot on this and then at the end of my PhD thesis I had a workshop that I organized so this was me and this is not computer-generated there's actually me younger and and that's what's basically on applications of dimensionality reduction so if you can basically learn these manifolds right if you can do something like this but data is really truly on a manifold right you can unfold it what are the applications we thought there's gonna be so many application so that's huge so we had this workshop and people came and these are the applications that we found like every single moment was the same stupid Swiss potato said there was completely artificial and this actually this is why I got out of there because there was absolutely no future due to it so so am busy but turns out that actually at the time you could already only unfold very synthetic data sets so now now we can actually do it 15 years later we can actually learn the manifolds but at the time he could really only unfold these very very low remedies these kind of synthetic data sets so I was extremely disappointed and I gave this talk in front of the the entire field and I was like you know basically every all our data sets at synthetic and can't be find a single real-world data set and so I was like well I found one data set that we like and that's the Swiss roll the action cinnamon roll and so I try to apply these algorithms on the cinnamon roll and so here this is my own PhD theses I wear them apply to the cinnamon roll let's see if the sound works I hope the sound works and this was kind of a competitor has local Indian Betty local Indian betting was a different paradigm you take it manifold you cut it up into little pieces they're all linear and he's stitch it together a low dimensional space you can do that too [Applause] [Music] [Applause] [Music] [Music] it's amazing you got a job after this but yeah of course one thing you can always do is you can just instead of doing these nonlinear unfolding methods you can just use a linear method right so and the most famous one is PCA so principal component analysis busy projected data in a low dimensional space we can try this too and the nice thing about many methods is he can reconstruct the data so this is the reconstruction ami ultimately of course the whole point is to do something with this data there's no dimension to say that but turns out it didn't matter my my co-authors just ate them all anyway that's all right all right thank you all this was a wonderful class and I see you all on Wednesday I'm ith Toby to make the exam easier and I will try 
","['', 'convolutional neural networks', 'local translational invariance', 'fully connected network', 'weights', 'image classification', 'kernels', 'non-linear radio functions', 'rectified linear unit', 'deep learning', 'GPU', 'error rate', 'training data', 'manifold', 'dimensionality reduction', 'Swiss roll', 'principal component analysis (PCA)', 'co-authors', 'Ithaka', 'Toby', '']"
"time in my class we had a competition a kegger competition where we gave students tweets from the President Trump and they had to identify if east tweets were treated from a iPhone or an Android phone and the hypothesis was that maybe the president himself uses an Android and his staff members use an Android phone an iPhone and so these are really different people tweeting under the same account then we could train a classifier to differentiate between the two so what we did is we made this leaderboard and I actually submitted my own submission am i down here initially the leaderboard was just me and the game was the following I gave myself five minutes to submit something and the idea was if you can the students can beat me in three weeks what I did in five five minutes then and they would get hundred percent on this project and so a lot of students did turns out it wasn't very hard to beat me and you can see I got 80% accuracy and here at the top there's actually got 90% accuracy it's pretty significant and there's actually the private data board is also the public leaderboard but it's quite different here Slumdog Millionaire thanks and so here 82 percent accuracy versus 87 point 5 mm-hmm and ok so since then though students have challenged me I want to see a screencast video of me actually training the classifier to see if I can really do it in five minutes and so sure I can show you how to do this and what I will do is I will actually use the tool called the household wabbit I'll pull that it is a classifier and we just search for this how so grab it so as you can see it's actually a classifier that's made by John Langford when he was at Yahoo now is at Microsoft Research it's a very very fast very very fast though just aggregation classifier but actually can do many many things square losses many different last functions and it incorporates feature hashing so he can just take words stick them in hashes them into the right bucket so here's kind of a command-line description and so on so installing BW is very easy he just you know pull it with it whatever you brew or whatever your favorite app gets tooless and okay I can now get started so let me just set a time I download this time that hi my apps so I'm five minutes let's go so the first thing I'm doing is I'm downloading the data there's now in download all and I open all of it and I open all of it and sublime as well and good so here's now the training and the test data and what I do in the test data I just need I just just used the tweets I don't use any of the metadata that I gave you guys it turns out the metadata is actually really really good so this is how most people beat me because they had a metadata for example time of day when you tweet that's a very very very good feature and some people say that you know some people may be tweeting on the toilet I don't know if that's true if it is good for him so here we go here's the training data and now I pasted that in so we just have to label on the left and the tweet on the right for the test we don't have to see it so we kind of add a label like this and then I just add a feature than INF for the training data and I have total tab total like tabs tabs of losers so what I do is I make this also a feature domain now one thing I need to do is I need to change I have to remove characters that BW can't swallow for example a colon column would kill it because it would think that's the means the value a dot it's not a good idea exclamation point backslash I mean these are generally you should just remove these okay here we go semi caught how about this area Moses to and now the most important thing I made everything lower case lower case here me goes everything lower case and our case it's good stuff now I have my files as I want them actually I still have to remember remove the front line the head and this should be no Hera okay good and you go islets three minutes left and okay good so now I need to shuffle them in random order sort our trains train I don't see this move why not no no I am not kidding me brew install okay have no sorting all right never mind then and all right so what okay good sort our trainers use trained artists USB right doesn't know why does not so short okay Oh II see I see what's going on I have to pipe it okay good good good good and and now I can run VW so what I need to do is I train r dot CSV my loss function is logistic my learning rate Howard one passes how about five and I have to make a cache file the final model I say this M and I had I think that's pretty good I would see how that initial T equals one that's pretty good in this she'll there's a typo okay good and now I can say BW initial the model I think I call it mes then T is the test that C's V the prediction going into output dot CSV and that didn't work why didn't that work ah okay here we go at TT here we go so open output dot CSV and here are my predictions ladies and gentlemens and now I can paste these into the sample file and I still have a whole minute left baby and what I can now do I can even be fancy it wouldn't have to do this but I'm just showing off that I have so much time left and it was sign this thing and I can now convert them into just draw science yeah and then I can edit paste from the results and now we have these wonderful predictions here and I can now do this where samples use B and I can replace this by comma and here is the file I need to upload and ladies and gentleman I'm done I can now upload this can do a late submission upload files open here we go I can just drag this in sample at CSV and I make a submission and I'm done two seconds left all right that was it and here's the timer how was my submission it was yeah okay it didn't change actually and that's all there is to it so the key thing is I just used Valpo webbot and I use the logistic loss I can now actually and go back here so initially I just once I got the command line right so the key thing is I made the training data in random format I use logistic loss learning rate one you can probably do better than that five passes over the data that's just because my daughter is five so I thought it's a good number normally you would cross validate this but this was really just meant being them you know as a mission that you can beat right this see here just means create as cache file you need this if you have multiple passes over the data - s means save the model in the file they just called em terrible naming convention and initial T just means that the first iteration is T that's just how you said how did learning rate decays and for testing I baby say I start with my model M I test this file and these are my predictions and that's it I hope you enjoyed the class and how do I start this video 
","['', 'in-class Kaggle competition', 'identify tweets from iPhone or Android phone', ""classify between President Trump's tweets from iPhone or Android phone"", 'leaderboard for classification accuracy', 'household wabbit (HW) classifier', 'download data for training and testing', 'features for classification: tweet text, not including metadata', 'remove characters that HW cannot swallow (e.g. colon, dot, exclamation point)', 'convert all text to lowercase', 'shuffle training data', 'logistic loss function', 'learning rate', 'number of passes over the data', 'create a cache file', 'save the model', 'test the model', 'predictions', 'upload results', 'Kaggle competition in 5 minutes', '']"
"- The reason machine
learning is so big these days is because of its massive potential. Essentially, instead of writing a new
program for every single task, you write a program once, and then you teach it
multiple things to do. AI is not yet at a stage where you can use them without understanding anything. It's not that simple. There are many courses that teach AI. A lot of them only teach you how to use code that you download from the internet. And, typically, that's not enough. This course explains, essentially, how these algorithms work, but not only how to use them, but also really in detail, like what the underlying principles are. And so this course really
gives you the foundations. It basically starts with the simplest algorithm, the perceptron, and then goes through all the way to the most, you know, recent state-of-the-art algorithms. Someone who takes that class
will basically, afterwards, have the ability to
apply these algorithms, and also debug them, and make them really
work on real problems. (upbeat music) 
","['', 'machine learning', 'massive potential', 'write a program once', 'teach it multiple things', 'artificial intelligence', 'understand anything', 'not enough', 'underlying principles', 'foundations', 'perceptron', 'state-of-the-art algorithms', 'apply these algorithms', 'debug them', '']"
"all right I'm talking about Transformers and at the heart of that problem is a language you know language modeling problem so Transformers can be used for many many things but you know often they're used for language modeling and so language modeling is the following you know problem you have a sentence you know let's say you know I like to eat right and now I have I want to predict the next word so this here is my input and this here is the label I'm trying to predict how do you do this right and the the simplest thing in some sense that we've already known in this class is you could represent your input as a bag of word Vector so in this case my input Vector is X right is some really high dimensional Vector with many many zeros right and you have somewhere a one right for I and you have somewhere one for like and somewhere I want for it and somewhere one for two and the rest is zero so right and this is a very very high dimensional Vector almost all of it is uh zeros right there's only very very few ones these you know these four ones here that actually correspond to the four words that I have in my my input context and then I train a neural network to predict the last word so what does that look like that I have some function and that basically you know is a classifier and you know this is this is what a Neo Network looks like so here this year you know this year may be a relu function and this year maybe a soft Max and the U here is one big Matrix that basically um Maps Maps our input let's say we have a vocabulary that's you know maybe 10 000 dimensional right and this is ten thousand by maybe some 128 dimensional space or something like this and W is some Matrix that goes from 128 to 10 000 again so let me just explain this right so this is the first hidden layer my X is mapped onto you right so I have my my X Vector here and it's multiplied with u that gives me and the B is just a constant this gives me a new representation 128 dimensional representation and then I multiply this with w and W the way you have to think about it is that each row of w this can be W 2 this is W1 right each row of that is actually a classifier that predicts this particular word so this may be the classification of a right and the CM may be and right and so on and this here down here is zebra right so I have all my possible words in my dictionary right and I have a classifier for each one other than just says is the next word and or is the next word zebra right and somewhere here I have banana right and let's say banana is pretty likely right because I like to eat bananas right and the transition function at the end this the the soft Max here what is the softmax the soft Max basically um let's call this let's call this here let's call this here Z right then the soft Max is essentially just um h of x is e to the oh sorry let me just say the the ith dimension of the of the soft Max h of X is a vector and the eighth dimension of this is e to the W I transpose Z divided by e to the W J transpose C so what does that mean that basically says this here is the strength of the classification for wi that's a w I's banana right this gives me an inner product if this is large that means banana is likely as the next word and if there's a small that means banana is not likely and this has all the other words in the dictionary right and e to the the reason that we exponentiate it is positive or non-negative right so the output here is definitely greater equal one right and all the different dimensions also sum to one so the output of this function here is a probability distribution over all possible words so I take my input Vector let me open in my input context I transform into the bag of word Vector then I stick it into a neural network right this neural network lends me a hidden representation and then in the hidden representation I have apply a classifier w which gives me a classifier for every single word in my dictionary right and so then I picked the you know I have to make a decision right they all give me a different uh strength of How likely a certain word is so I can turn this into probabilities with the soft Max function this here is my soft Max function right so I apply the classifier and then apply the soft Max and so the output then is something like some Vector where I basically say well the word banana maybe that has a 10 chance right and the word cherry hasn't opened off right now the word table has a zero percent chance right I don't like to eat tables right nobody does right and so on so most most things have very very low probability but some words actually have higher probability and then if I want to pick the next word I pick it randomly from this probability distribution and so for example let's say I pick banana right then actually I fill in the word banana right and then if I want to continue the text I say okay well now this here is my new input right and I'm predicting the next word after this maybe I like to eat banana you know bananas sorry but um you know not if they are too old or something like this right or the last word is you know I'm actually predicting a full stop here right so maybe I'm just predicting period right that's also reasonably likely right and the sense is over and then I start a new sentence so this is kind of the simplest language model you can imagine right this is not yet a Transformer um but let us just think about this a little bit so what's going on here right the first thing is I'm converting my text to a bag of word Vector right and then I multiply this with u so there's two things that are happening so if I multiply this bag of word Vector Let Me Maybe copy it and move it over here right um copy and paste it in here here's my bag of word vector and I multiply this with the Matrix U what happens right I'm actually taking the different Columns of you and I multiply them with this you know X Vector so imagine playing the first column with zero right so this guy is Multiplied with zero the second column of 95 is zero right but this column here is Multiplied with one right so in some sense you can think about it as as if I'm pulling out this column of U and then I'm pulling out another column of U that corresponds to to this one and I put another column of U and another column of U and then I add them all up so this here corresponds to the word I right this you like you know eat right so what am I doing I'm basically pulling out columns that correspond to words so this column of U corresponds to I this quarter bonds to like this to eat and this to 2 right so and then I add them all up then I sum up these columns right and that's actually my my word representation right that's my secret my representation Z my hidden representation in my neural network so there's two observations number one the order of that sentence is lost right if I say I like to eat or I say eat like to I right would be exactly the same bag of word representation would be exactly the same hidden representation Z right so that's a little funny and we have to fix this right that's that's a downside of this representation of the algorithm the second thing we're going to observe is that the Matrix U the C is my Matrix U just to make this clear this is the CSU right um my Matrix U um each column of U corresponds to the representation of one particular word and this column is only ever used if that word is in the input otherwise it's not used when I then train this function with back prop right and so I get a loss and then you know I say well the word was actually cherry right so I kind of suffer some loss and I update my weights and my w my U and my my B right I update all of these three guys right then I'm changing the representation of these words and this is this word representation is something that's very very similar or essentially known to his word to back representations or word embeddings so if you take a large Corpus and you take this some very large text like for example you know Wikipedia right and you take any kind of sentence of K words before and then you try to predict the next word right to be honest sometimes people also take the words before and after and predict the word in the middle right that works too um then you learn this Matrix U and you can basically now you pull out these these word embeddings that basically is some representation of the meaning of the word and one thing you observe is that words that are very similar like January February March or something right we'll have very similar representations and words that are you know different you know you know very very different and people actually then take these for example U of Queen right the word Queen and to subtract U of woman and you add you of man right and that is roughly U of King right so the the relationship between these word vectors actually has some meaning right so somehow woman and man in relationship to King and Queen kind of you know stand in a meaningful relationship with each other all right so okay so one more time we have our words that they're encoded in here we pull out the word vectors from you which is also what we're learning with back prop later on so we're learning a representation for these different words we sum them up and then we stick it to a classifier to predict the next word so one thing we already observed is that you know the order of the words here you know is lost which is problematic if you have a long context right it's very very important what the last word is right and another issue is that um you know that each word has a fixed embedding so let me give you another example um I say I may visit in May right um if I ignore capitalization here right so here we have these two words may and may which are actually the same word right but they have very very different meaning right so one is the month May and the other one is May like in wood could right so you would expect this word here should have an embedding that's very very similar to wood or could because you know or might right you could also have Mite here right which is essentially the same thing as May right wouldn't change the meaning very much right and here you know you could have February right which of course you know would change the meaning but but you know it was perfectly perfectly fine to substitute this so in some sense May here it should be very similar to to uh February March the other months of the year right the issue is the following that if I turn this into a vector you know input Vector X right and then apply U you you know U times x what does it look like right my X Vector here in this case has you know a one for I right and one for in right a 2 for May and a 1 for visit right so here essentially what I'm doing I'm just you know treating these two words exactly the same right I'm going to do gradient descent I'm not differentiating between the two different meanings of this word which is very problematic so how can I tell that the second May is different than the uh the first May is different than the second May and the reason is and the way I can tell is from the context right so if I say visit in May right and here I may right that essentially gives it away that these two have two different meanings and so essentially the context changes the meaning and this is essentially what um what Transformers are doing can Transformers learn contextual embedding of the words so very similar to what we had before right where we mapped our inputs to word vectors Transformers do the same thing but they do it such that the inputs can influence each other and I'm going to now explain how this works so the first thing Transformers do not have a bag of word embedding I can remove this right as there's no bag of words because that you know removes the order so that's uh you want to keep the vectors alive so the first thing we're doing is we map each Vector to a predefined mapping right and that's just indexed by the word itself so you know in this case this year becomes U of U1 you you know you you two U three u4 u5 right and this is essentially just the u5 here is just U of May like the the U is some Matrix just like before use term Matrix that has a vector for every single for every single word so the word may has some root here so u5 here equals U1 right this is still you still have the same problem as before this is May this is May it was our YouTube right so you u5 equals U2 because they're both the same word right so this is a problem right because these are not the same words right they're actually quite different words right and we need to change this and this is essentially what what Transformers allow you to do so how do Transformers do this they take these vectors and they embed them into a space so these vectors are now UI at element of sum let's say 128 dimensional space right this here goes from 10K to 128 and in this 128 dimensional space all these different words are basically just now points and we want these words to influence each other we want to want wanted this word in influence is made that the word I influences May and so on right so the first thing is that we need to differentiate um the fact that you know in gets influenced make the second um influence put in because they follow right after each other right the I influences me because they follow right after each other so somehow position is very very important right and how do we do that how do we encode the position into these embeddings and this is relatively simple we basically add a position or something that's called positional embedding tool so we say you you I becomes u i plus P of I and P of I is a vector that only depends on the position so it is a function of of sine sine waves where you essentially have p over you know so what is p p of i p of I is some high dimensional vector where each Dimension that's also 128 dimensional in this case and each dimension corresponds to some sine function that is just a function of I the position I'm in divided by some constant right that's a function of d and so when you look at these position embeddings they're essentially the First Dimension is maybe a high frequency signal so and you know different positions basically have different values here the second dimension is more slow frequency and and so on you get increasingly low frequency or something right and and the the first Vector basically has these values right and the second Vector has these values and the third Vector has these values right so in the First Dimension you have this value second dimension if this value the dimension of this vector and so on and the whole point is that vectors that are close together are very similar whereas vectors that are far away you're going to have different different representations because you have some really low frequency signal down here right where this guy may be very different than this guy down here so this is essentially a trick to make vectors that are close together more similar right so give the neural network some chance to learn which vector is in what position of my context so we basically here get okay plus P of I right that's the that's the new new One Prime and then we have here U2 plus P two right and here we get plus P3 and so on so the first things are doing is they they add these positional embeddings um let me maybe move this up here okay once this is done we now have something that we already achieved something right the word May and may here are no longer the same right why is this because they have different positions this here is in position two and this is in position five right so this is U of May plus P5 and this is U of May plus P2 right so these two map in slightly different places let's say this guy Maps here and this guy Maps here or something right maybe I should maybe I should say key actually these are the vectors right let's go and move C and this guy moves here okay so the first thing we need we've already solved the problem that basically shuffling the words in different order will give us different embeddings right beforehand we had this problem in the back of word representation here we had the problem that it was in you know uh independent of the order in which the words appear right here actually we don't have that anymore right so the order of the words actually is important it gets baked into uh into the representation through this positional embedding okay good so now we have these vectors U1 Prime U2 Prime and so on right U three prime U four Prime and so on so here we have position four and but now what we want to do is we want to allow the word made to be modified by the word in and the word I to be mod you know modify the word may right and maybe visit can also modify me I'm not sure right so how does that work essentially U and this is now where Transformer you know when attention layer comes in this is kind of the heart of a transformer we take each one of these vectors and we map them into this space like they're basically in in the space here in this 128 dimensional space and this is our 128 dimensional and what we're essentially doing is we're saying each Vector we find the nearest Neighbors and we make this Vector become a weighted average of its nearest Neighbors so the way you can imagine this is UI becomes some overall J s i j u j so I'm summing over all other vectors this would be a prime here I'm summing over all other vectors and I weigh them by some score s i j and so one property all the sij s i j sum to one and all srj are non-negative so this is a weighted average of my neighbor so each Vector becomes a weighted average of its neighbors so the neighbors change this vector now this is great right and this is what's called attention right the only issue is that um that you don't actually you know you may want to modify words that are not like you so for example let me give you the following you know in this case the word in in the word may may actually be really far away from each other right the one reason there should be close is because they're in similar positions but the original embedding of in in May is probably quite quite different right the same with I and may right so they may actually not be nearest neighbors right but if I have this nearest neighbor approach where I say I get Modified by my nearest neighbors right I want these two to be close to each other and that's where multi-head attention comes in and this is the following that you essentially saying each Vector UI becomes three vectors it becomes q i is Q times U right k I is K times UI and v i is V times UI and again q k and V are matrices that we learn and so what are these what are these vectors if I want to modify if I want to modify the vector UI right what do I do I find my nearest neighbors by matching kqi with KJ from everybody else and what does that mean so q i is basically stands for query and query is basically what you are looking for key is K is key and means this is how you can find me and V is the value this is the value that I'm passing on let me give you an example so for example I have the word white right the word right can modify the word house right if I have the word house but in front of it I have the word white that really changes the meaning of house right so this is the White House so we're talking about presidents right it can also be chocolate right so if I talk about chocolate right then I'm modifying the word chocolate and suddenly I'm talking about white chocolate rather than dark chocolate I can already also modify the word paint right and the other way around these words can also modify white right so if I have paint around it then I'm talking about a color if I have chocolate I'm talking about sweet and if I have house then I'm talking about you know something presidential right some something political so the word white may be looking for words that are actually quite different um than itself it may also actually look for by the way white you know you can have white blue right you know white blue per shirt right so here actually it's looking for a word that's quite similar right to itself so the way you can view these cues in the case are a little bit like a dating profile right imagine you set up a dating profile on a web page right the K is your dating profile right the K is basically says how you want to be found right so you set up a profile that basically describes you as a person right let's say you are a 23 year old female right who's a college student or something right and so that is basically how you describe yourself that's what you want others to see about yourself to to find you yeah Mary right the query may be different from from UK right because maybe you are you know a female college student and you may be looking for you know a male a college student you know who's who's maybe older or something like this right so the query is what you are searching for and the K is what you want how you want to be discovered by others right and that's the same thing with words right so in this case the word q may be looking for something like like cows right and this is the the vector of house that actually wants to be found by by the word white right so how does the trans the attention layer work I take for let's say I'm modifying this this word here right U2 then I actually take my Q2 and I look at all the other case right and I compute their similarity with each other and the similarity between any word any two words s i j is the inner product between q i and KJ this basically says how similar are these two words and to make sure that we can compare these inner products we again apply the soft Max so we just go over all other primes and we say e to the q i k j Prime and this is now the new similarity between these so this basically says how much word J should influence word I right when I'm modifying I'm transforming word I right so at a take word I and I'm now transforming it by pulling information from the other words right this is my context to modify this word how much should word J modify word I right and for this I map word I into some Q vector and which I'm just learning right this is just a matrix I'm learning and any other word K is J I'm mapping to a k Vector which again is just a linear transformation that I'm learning and I see how good of a match are these two words to influence each other right and if this is a large value it will dominate the sum right and that basically means this will be a strong value right and once I've computed all of these Then I then I go back to my equations that I had here right see here's this equation let me just pull this down right so now my new UI becomes the sum of all the other words I compute the weight how much would J will influence word I and now I'm changing this here I'm actually making this v j so if you haven't talked about V yet what is V V is the value that J sends to I so in this case what does in centimet right to modify me that is v j right and what does I send to me to modify this may right that's the you know V1 here and this here is the V4 right so every word has three vectors Q oops sorry Q is so that I you know what I'm looking for to modify me K is the vector that you know others can discover to modify them and V is what I'm sending to others to modify them that's actually the value I'm passing on and all of these matrices are learned and this is actually all there is to a attention layer so we now go back to our Transformer I clean this here up a little bit let's move this down what you're getting here is the following that I have these U vectors and now I have a Transformer like this here space the positional embedding that's the first thing I'm doing and then I have an attention layer and a tension layer takes as input a bunch of vectors then computes for each one of them Q Maps them into these three different vectors Q K and B and lets them all influence each other so the output is again a u one a U2 u3 and so on u5 right and internally what's happening is this here becomes a q q1 K1 V1 right and let's see becomes Q Q3 K3 V3 right and these different K and B values get matched with the Q values so these these match with the Q value to become s i j and then these V values basically influence this vector and this becomes this new entry by the way it can also be influenced by my own V Vector right because I'm also matching my Q value with my own key value right so this guy here is now a weighted average of all the other vectors right but because the soft Max is such that because if you're exponentiating here right if one value here is a little bit larger than all the others because I'm exponentiating this will be much much larger than all the others so really what's happening is that this is sparse that really this UI Vector will only be influenced by a few words right not many and so probably for example this may Vector will be influenced by May and by you know the word that precedes it right so in May these two basically give me a new Vector here and these two here give me well these two give me a new Vector here right and probably actually the may also May influence the end I don't know okay so Transformer transforms the the vectors right that this the vector corresponds represents the meaning of every single word and then I transform them you know through the context so how do the different words influence each other so that's the attention layer this is the tension let me write this down here attention often actually referred to as self-attention because you know all the vectors kind of attending each other thank you and there's also cross attention which is and you have two sets of vectors um okay good so what do you do once you're done with this then actually in a Transformer you do one more step where each Vector just gets mapped with a with an MLP or feed forward Network into a new vector and this is the same feed forward Network so you basically have some Network where a vector comes in UI comes in and then you have some you know multi-layer perception or free forward network is the same thing right and uh a vector comes out of it and that's really all there is to it so and you apply the same network oh sorry you're playing the same network for every single vector Q5 so this this network you know just has some parameters right if you write this out this basically is you know u i triple I equals you know sum transition function of W Times some transition system functions of of uh you you've it's called not you let's call it something else they haven't used yet uh W Prime Maybe um of you I input right so I take my my input multiplied with the Matrix apply transition function by the power Matrix it transition function and that is just this is my newer Network here time and now I'm done so now the inputs this year essentially is one Transformer layer where I have um a set of set of vectors that come in and a set of vectors that come out and actually the number of vectors is preserved right and actually that position is preserved so the first Vector corresponds to the first Vector all along right the fifth Vector corresponds to the fifth Vector all along but in the process here they can be influenced by each other there's a few more subtleties one subtlety is that actually you have a skipped connection so you're actually adding here the original Vector to it so you actually have something you know this is just so that you can learn the identity very easily so if you basically have a word Vector here that's good right in some sense you don't want to lose that information right this is a little like residual networks resonates right so um there's just a standard trick same thing here you basically add the output here to the input so you can just say this is actually Plus plus u i double Prime right so you just add that input to the output at the end and the other thing is that you have a layer normalization so you can you know either can have it different positions and often you have it here but you actually just normalize uh normalize the representation um to have zero mean and you know this is essentially very similar to batch Norm except you actually go over the different dimensions of the input and so it's called a layer normalization so this is just to make sure that you know you don't get some crazy large values and if you have crazy large values then optimization becomes very hard so you just make sure that you know your output always has a zero mean and a small standard deviation and okay so this is essentially a Transformer and let me add this maybe here we have a layer Norm layer Norm yeah we have one more layer now and and now we can just stick the output again into another Transformer layer and something like gpt4 I think has 96 Transformer layers bird has I think 13 Transformer layers right so you know we can just do this times M right where m is the number of layers that you have um one more subtlety is that at any given layer right so basically let's say you have based this we have you know input comes in you have one layer input another Transformer layer another Transformer layer and you could actually have multiple such layers in parallel so you actually have each word it goes here but each would also goes here and then you combine them again when you're done this is very very common practice right and basically everybody does this um and the way you combine them is by just taking the outputs of each of these layers you concatenate them so you basically get U one of the first of the one layer U1 of the other layer right and so I don't know how to maybe change the color and then I have another you know q1 of the third layer right this is my Green Layer and this here is my red layer and I concatenate them all and then I multiply them with some projection Matrix let's call this l and this actually now gives me the output that's basically combined that's the output to the next layer U1 Prime double Prime so okay one more time so you have the different words to come in here I stick them to a Transformer layer in fact I stick them to many different from transform only each one has their own q k and V matrices right and then I combine the outputs and again I stick them into many such layers Transformer layers and I put them again combine them right and this is very similar to a convolutional Neo Network where you actually also have an input image and then you transform it with one convolution operator get a new image another convolutionally another convolution layer and then you combine them again right in the next layer foreign okay so once you have all these vectors you can now actually do a lot of stuff with it so now we actually have a contextual embedding of our sentence right and it's contextual embedding because each word corresponds to a word Vector but the word Vector is influenced by two things number one what the word is and number two what the other words around it are right and now I can use this to predict the next word in the sentence and the simplest thing is I could just average them right to sum them up and then run it through a classifier right to predict the next word often what people use is they have a decoder and uh then they call the spirit part here this this part here is an encoder that basically just goes from text to you know these word embeddings we call this encoder and the decoder is essentially the same thing right decoder is essentially the same thing except it's um has cross attention so let me just explain what a decoder is so decoder goes to the same steps and decoders usually the stuff so in this case this is the the text that I'm given to me as a context this goes into the encoder and then a decoder what I do is I had I start with some dummy token that basically says beginning of sentence that's just a decoder a first token beginning of sentence and I stick this into my decoder and then I have you know this gives me a mapping it's called this V right V1 then I add position on the bedding for position one that gives me V1 Prime then I have a self-attention layer self attention attention in this case it's trivial because I only have one vector so it become a linear combination of myself right and then comes cross attention versus cross attention cross attention oh let me sorry let me write this cleaner um cross attention and cross attention is very very simple cross attention and the cross attention I take my input vectors and I map them to a q Matrix but then I take these guys here and map them to K and V so cross attention I take these are the output of the encoder and map it with keys and values and I take this guy and map it with Q right so this is in some sense you know a dating app you have two different colleges right and people from one college are only allowed to date people from other college or something right but not within a college so this is basically what it is so everything in the decoder is allowed to find something in the encoder to modify it right but here they don't modify each other right and then I get a new new representation here so this means that the token here is actually a linear combination of the of the stuff from the context right these these all together give me a new Vector which corresponds to this guy right of course I still have skipped connections so I I'm not totally losing the information that I have and uh then again I do you know I do some feed forward new networks and so on and have multiple such layers so the encoder is essentially the same thing as a decoder the only thing that's different is this cross attention that I'm basically applying attention to all my contexts and the reason you're doing this is because sometimes you have a lot of context so let's say I take a lot of text you know that I put in as background and now I want to start you know um generating new text so in some sense I don't want to every time re-encode this stuff right so I basically keep this stuff encoded once you know encode these vectors once and then I just every time I'm generating new new words I just cross a tent over this context foreign so once I'm done here I'm you know I still have some feet forward necklace this is my you know I have some feed forward vectors you know some MLPs here uh feed forward Network this is the same as here right that's going on and at the end I get get some Vector out right I can do this again you know M Prime Times right and the end basically I get some vectors here that's my contacts and my from my encoder and then I run it through a classifier right classifier and predict the next word output let's say here the output now is you know let's go back to my original original example I like to eat right let's go here I like to eat I like to eat right and now I predict bananas right then what I do is I go back to my encoder and put on the word bananas right and now I go through the whole thing again and I predict another word right what we're predicting now maybe now I'm predicting you know but right so then I put in but eight and then I predict not right but not you know when they are too young or something when they're too you know and I like to eat bananas but not when they are too fresh or too not ripe or something like this right so this is the sentence that I'm busy generating here okay so in the actual paper that presented the or introduced the Transformer it's called attention is all you need from 2017. um you can see a layout of the Transformer architecture and you should now be able to understand this so first we have the inputs these are actually the words right I like to eat bananas something and then you have these words I like or something get mapped into vectors so now we have U1 U2 here I've added the word may may I visit in May right you have two words that identical they get mapped to exactly the same Vector right because I'm just looking up the word you know in this in this kind of U Matrix I have a u Matrix and I just pull out the the corresponding vector then I add the positional embedding so this is basically P1 P2 for each token I added Vector that is only you know specified by the position so here is now with the two maze will differ right so one is basically the beginning of the sentence one at the end of the sentence so they change a little bit just based on on their position then we have the multi-hat self-attention so we take all these different vectors and they attend over each other so each Vector becomes an a weighted average of the other vectors right you just sum up the main average and get a new Vector here and here the word may will change because it will be influenced by its previous vectors we're also adding the original Vector to this so This base this in Vector comes in then it gets Modified by the other vectors and here comes out right and then take the input vector and add it to the output Vector this is the skip connection this is this is what we have here this is called add if you also do a normalization so we just make it zero mean all the representation zero mean and you know divide by some constant to have the standard deviation small uh here comes the feed forward and you've also understood this now so we have a vector and we just apply a neural network to get a new Vector right and you just do this for every single Vector in isolation so here they don't interact with each other and again we have a skip connection and we have a normalization and that gives us the output vectors so this kind of architecture has input vectors right then they mix up with each other right so each one becomes a weighted combination of each other and they get you know then I pass them to the new network and then I get the output vectors and then I put them back into the next layer so I do this n times I have n such Transformer layers the output of this is a bunch of vectors that's basically the the text encoded this here is called the encoder right um encoder on the left hand side you could now actually take you could take the the text of the the output here run it through you know average them and run it through a classifier right but this is not actually typically what people do typically what they do they have a decoder as well decoder is the same architecture as an encoder this is exactly the same except there's one small change is this thing here and what is this this is basically after you take your inputs in here and you push them through here they at this point you actually only have the Q Matrix from the inputs and the K and V from the outputs of the encoder these are my output vectors they go in here so what's happening here in the decoder this is basically the encoder encodes my contacts that I'm not going to you know this is this given to me right and now I'm encoding my first words imagine for example if chat GPT you put in a prompt The Prompt goes in the encoder and then I want to decode something so I start with a token and this usually I just start with the start token I have some special basically special word that basically means now I'm beginning and I stick this in this is going to turn to a vector then I do self attention here right this is going to become a linear combination of itself trivity that don't change and now here comes the interesting thing this now becomes a query so I'm looking you know the Q vector and I attend over all the vectors from my from my context and I make the new Vector a linear combination of my context so some I sum up these guys here right I sum them up some baited some of these guys right of their Viva of their V values becomes my new Vector here then I think it's your Neo Network and I do this a couple times over and over again at the end I get a linear vector that I'm through a linear layer just make one vector and I run it through a classifier and this final prediction here is I classify over all possible words so this year maybe you know I like to eat right I stick this all in I get a whole bunch of vectors then I put in start in the decoder and that leads to bananas right the word bananas and where does the word bananas come from because you you cross a tent the start Vector with all of these guys right and these guys basically tell it oh yeah a reasonable word afterwards gonna be bananas and then here you have a classifier that basically translates into probabilities once we have bananas you stick that in as the next token into the decoder you push it through and you get a new word out and you stick that and it's a new word push it through get a new word out stick that in and so on right so you always kind of add the previous output as a new um as a new input this is why it's called outputs shifted right so you always have the previous output here you know as an input one more detail the CSS masked multi-head attention so what does that mean if I have all my outputs that I've generated so let's say I've generated the word banana hey banana I'd like to eat bananas and cherries right this is what I've you know what I've generated so far next I'm going to generate a full stop right so these are now all going to become my inputs when I have these tokens bananas and cherries right start bananas and cherries and I do cross attention then in this case I don't want cherries to modify bananas why Because by the time I wrote bananas I didn't actually know about cherries yet right so I can't actually have information from the future right so when I'm generating these these vectors they're always just a linear combination of the previous lectures vectors that actually came beforehand in time right so in the decoder time is actually a component because you're generating the words one by one right this this word here can and cannot be modified by cherries because when I generate end I don't actually know about cherries yet I only know it up to here right I'm generating these words one by one and this is important because if I do this then I can just keep these vectors in memory right and now I add a new word and I just add one more Vector right and it can be a linear combination of everything in the left so don't actually have to recompute the representation of the previous words if I add a new word right does that make sense so this way it's very very fast to do actually to do the decoding I don't actually always have to re put put everything else into the encoder one more time a decoder one more time this encoder text is bi-directional that's encoded once and every word in the decoder is just decoded one by one by one by one um we can now so we can now try this here's gpd3 and I start the sentence I like to eat and now NZ basically says here's where dip defeat should start and I press submit and it's now generating text and it will insert it in a minute and you can see I like to eat boiled eggs boiled potatoes in some greens I usually have some fruit blah blah blah dessert right and so essentially what it does it just starts you know generating one word after another and um you know each each one of you now run it again you will see that actually it generates different text um because it has a it samples different words right so if I do it one more time I like to eat all kinds of food but my favorite type of food is Mexican and so on you can also uh if you're here you can actually um give it something else I can actually say you know um quicksort and implementation in Python I can say death quick swords or my quicksort or something my quicksort um taking a variable unsorted list and then I say well I'll continue this text now right and say submit and this is actually give me python code right and so this is pretty amazing that actually it has learned code so well from just basically seeing code on a stranger data set that actually keeps the variable names right so here unsorted list that I introduced here right actually it uses this throughout um and basically says well if you know if you look at this code it's probably correct right um I haven't checked you know I haven't checked the code but as you um it seems pretty reasonable um and so these are what this is what people call emerging abilities that you actually by predicting the next word what actually helps is to understand something about the world and so here actually you know gpd3 has actually understood something about coding right and maybe this is a very common example so it's probably seen something very similar on the internet but you may have seen these examples where you can type and put in some code and ask it what is the bug and actually finds the bug right and there seems to be some bizarre understanding of of the world that it had to learn in order to predict the next word um accurately and this is what people are so excited about these days 
","['', 'language modeling', 'bag of word vector', 'neural network', 'hidden layer', 'softmax function', 'classifier', 'word embedding', 'positional embedding', 'attention layer', 'Transformer encoder', 'Transformer decoder', 'masked multi-head attention', 'gpt3', 'code generation', 'emerging abilities', 'quicksort implementation in Python', '']"
"hello as promised a short video on DB scan DB scan stands for density based spatial clustering with noise DB scan and it was invented by estal in 1996 DB scan has become my personal my favorite clustering algorithm it is really really cool and has many nice advantages over for example cin or gaan mixture models DB scan has two parameters Epsilon and M and the way works is as follows so the first thing you look at Epsilon Epsilon is some small constant and let's say this here on the left is my data set so for now just look at the you know these points right ignore the graph and the coloring and all this stuff so this is my data set and the very first thing you do is you take a data set and you convert it into a nearest neighbor graph and the way you do this is you take any single data point you draw an Epsilon ball around it distance Epsilon and you look at all the points that are inside of it it and these become your nearest neighbors and points that are outside of it do not become nearest neighbors so the nearest neighbor you know when your dense region of the graph you have a lot of nearest neighbors and if you sparse region you know then in your data data set then you actually have no nearest neighbors for example this guy here has no nearest neighbors because there's nothing inside of an Epsilon ball okay so you do this for example this guy right these are you know these points are all within its nearest within Epsilon distance so these are all the nearest neighbors this guy also then this guy here has only two nearest neighbors this guy here has you know and so on multiple nearest neighbors and so on and so you build this whole graph and that's the very first thing you do to do DP scan then the second thing you do is you take the second constant m equals 3 and you define core points and the definition is very simple any data point that is a degree which is also the same thing as number number of nearest neighbors greater than M greater equal m is a core point in this case I drew them all as red points so for example this guy here has 1 two three four five nearest neighbors five is more than three so it is a core point this guy has one two three nearest neighbors three is great equal three so it's yes it's also core point this guy one two three core point this guy one two three core point this guy has only one years neighbor not a core point right not cool enough enough this guy not a core Point only two nearest neighbors so again you compute this for every single data point and you get all your core points these are here these red you know red points now you've done all the pre-processing now we can start with the clustering and the clustering essentially um follows a simple principle the first thing you do is you pick a random core point for example this guy here on the left this is now my random core point and I assign it to Cluster one so this here let's call this cluster one so right now I have cluster one and only consists of one point this core Point here and here comes the clustering rule any data point that is connected with a core point it's important that it's core that's already in a cluster will also become part of the cluster so all of these guys also become part of the cluster right because they're all connected to this guy here and because it's a core Point all its neighbors can come in so little like it's a party and only the core points are allowed to bring their friends with them to the party now you know this this core Point LED in these these these different points but this guy here also came in with it and this is again a core point so this point here let you know can let its strengths in so we have these two points that also come in uh because they're friends with this core point now we have this core point in the cluster and now again all of its friends can come in right so now we have these points as well part of the cluster again two more core points they let their friends in so they are also part of the cluster and this core Point lets this point in so this is now the last Point that's led into the cluster so if you draw the cluster line you'll notice that this here is exactly the cluster line all of these points inside the green circle are part of the cluster it's important that this point here is not part of the cluster why it has a friend that's in the cluster but this here not a core Point only core points can bring their friends to the cluster all right so at this point we have run out of neighbors that we can let in and what we do is we just you know repeat the process we start we pick a new uh core point at random for example this guy down here okay we say all right this guy down here is part of cluster number two cluster two yeah this is cluster two and we do the same thing this guy has you know a whole bunch of nearest neighbors can bring them all with them to the party now this core Point has a bunch of neighbors can let them all in this core Point can let this guy in and this core Point can let this guy in and before you notice it you realize all of these points belong to Cluster number two now we would again pick a random clust uh core point but there aren't any core points left so what we do now is we take all the points at this stage that have not been assigned to any cluster and call them noise so these are just noise points basically they are not part of the cluster structure we just they're kind of in their own category one thing that's also interesting to see is this point here in the middle could really be in either one of the clusters but because cluster one was assigned first this point basically joined the party of this core point and couldn't join the party anymore here because it was already assigned somewhere else every Point can only be in one cluster that's sometimes why the first cluster is maybe a little bit bigger than subsequent clusters I should draw a little circle around this guy and that's all there is to DB scan I can now show you this a little bit on so if you have a noisy smiley face and Min points here is four Epson is one let's get started and the first thing he does picks a random core point and lets all its neighbors into the into the cluster then you know once they're all done it picks a second core point and defines a a new cluster this is the blue one now second cluster and again lets in all its neighbors you can see how this bottom up cre creeps around and um captures the circular chuster very very well which is something K means could never do right because K means basically assumes that the whole circle inside of it must be a cluster it can't just be a circle it must be filled with points and now it assigns a third cluster and a fourth cluster and now it's done with all the core points and everything that's here with the black circle is a noise point and look how beautifully DB scan nailed this pretty challenging cluster structure if you do this take the same data set and run K means on it you can now do this add a centroid one two three four let tell it how many classes there are there are four clusters we give K means in some set an advantage right we tell it how many clusters there are and we start assigning centroids um you can see here what does it do it just basically divides the space up into five clusters is completely oblivious to the true cluster structure of course you know we're beating up K means here and K means actually you know does work very nicely when its assumptions are satisfied but one thing I like about DB scan so much is that it doesn't make as many assumptions so in practice it can sometimes be more robust and you know bring out cluster structure even in challenging data sets 
","['', 'DBSCAN', 'density-based spatial clustering of applications with noise', 'invented by Ester et al. in 1996', 'clustering algorithm', 'Epsilon (ε)', 'MinPts (M)', 'nearest neighbor graph', 'core point', 'degree', 'cluster', 'random core point', 'connected', 'friends', 'bring their friends with them', 'noise point', 'noisy smiley face', 'K-means', '']"
,[]
