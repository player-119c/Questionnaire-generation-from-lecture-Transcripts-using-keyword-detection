- So welcome everyone to CS231n. I'm super excited to
offer this class again for the third time. It seems that every
time we offer this class it's growing exponentially
unlike most things in the world. This is the third time
we're teaching this class. The first time we had 150 students. Last year, we had 350
students, so it doubled. This year we've doubled
again to about 730 students when I checked this morning. So anyone who was not able
to fit into the lecture hall I apologize. But, the videos will be
up on the SCPD website within about two hours. So if you weren't able to come today, then you can still check it
out within a couple hours. So this class CS231n is
really about computer vision. And, what is computer vision? Computer vision is really
the study of visual data. Since there's so many people
enrolled in this class, I think I probably don't
need to convince you that this is an important problem, but I'm still going to
try to do that anyway. The amount of visual data in our world has really exploded to a ridiculous degree in the last couple of years. And, this is largely a
result of the large number of sensors in the world. Probably most of us in this room are carrying around smartphones, and each smartphone has one, two, or maybe even three cameras on it. So I think on average
there's even more cameras in the world than there are people. And, as a result of all of these sensors, there's just a crazy large, massive amount of visual data being produced
out there in the world each day. So one statistic that I
really like to kind of put this in perspective is a 2015 study from CISCO that estimated that by 2017 which is where we are now that roughly 80% of all traffic on the
internet would be video. This is not even counting all the images and other types of visual data on the web. But, just from a pure
number of bits perspective, the majority of bits
flying around the internet are actually visual data. So it's really critical
that we develop algorithms that can utilize and understand this data. However, there's a
problem with visual data, and that's that it's
really hard to understand. Sometimes we call visual
data the dark matter of the internet in analogy
with dark matter in physics. So for those of you who have
heard of this in physics before, dark matter accounts
for some astonishingly large fraction of the mass in the universe, and we know about it due to the existence of gravitational pulls on
various celestial bodies and what not, but we
can't directly observe it. And, visual data on the
internet is much the same where it comprises the majority of bits flying around the internet,
but it's very difficult for algorithms to actually
go in and understand and see what exactly is
comprising all the visual data on the web. Another statistic that I
like is that of Youtube. So roughly every second of clock time that happens in the world,
there's something like five hours of video being uploaded to Youtube. So if we just sit here and count, one, two, three, now there's 15 more hours of video on Youtube. Google has a lot of
employees, but there's no way that they could ever
have an employee sit down and watch and understand
and annotate every video. So if they want to catalog and serve you relevant videos and maybe
monetize by putting ads on those videos, it's really
crucial that we develop technologies that can dive in
and automatically understand the content of visual data. So this field of computer vision is truly an interdisciplinary
field, and it touches on many different areas of science and engineering and technology. So obviously, computer vision's
the center of the universe, but sort of as a constellation of fields around computer vision, we
touch on areas like physics because we need to understand
optics and image formation and how images are
actually physically formed. We need to understand
biology and psychology to understand how animal
brains physically see and process visual information. We of course draw a lot
on computer science, mathematics, and engineering
as we actually strive to build computer systems that implement our computer vision algorithms. So a little bit more about
where I'm coming from and about where the teaching
staff of this course is coming from. Me and my co-instructor
Serena are both PHD students in the Stanford Vision Lab which is headed by professor Fei-Fei Li,
and our lab really focuses on machine learning and
the computer science side of things. I work a little bit more
on language and vision. I've done some projects in that. And, other folks in our group have worked a little bit on the neuroscience
and cognitive science side of things. So as a bit of introduction,
you might be curious about how this course relates
to other courses at Stanford. So we kind of assume a basic
introductory understanding of computer vision. So if you're kind of an undergrad, and you've never seen
computer vision before, maybe you should've taken
CS131 which was offered earlier this year by Fei-Fei
and Juan Carlos Niebles. There was a course taught last quarter by Professor Chris
Manning and Richard Socher about the intersection of deep learning and natural language processing. And, I imagine a number of
you may have taken that course last quarter. There'll be some overlap
between this course and that, but we're really focusing
on the computer vision side of thing, and really
focusing all of our motivation in computer vision. Also concurrently taught this quarter is CS231a taught by
Professor Silvio Savarese. And, CS231a really focuses
is a more all encompassing computer vision course. It's focusing on things
like 3D reconstruction, on matching and robotic vision, and it's a bit more all encompassing with regards to vision than our course. And, this course, CS231n, really focuses on a particular class
of algorithms revolving around neural networks and
especially convolutional neural networks and their applications to various visual recognition tasks. Of course, there's also a number of seminar courses that are taught, and you'll have to check the syllabus and course schedule for
more details on those 'cause they vary a bit each year. So this lecture is normally given by Professor Fei-Fei Li. Unfortunately, she wasn't
able to be here today, so instead for the majority of the lecture we're going to tag team a little bit. She actually recorded a
bit of pre-recorded audio describing to you the
history of computer vision because this class is a
computer vision course, and it's very critical and
important that you understand the history and the context
of all the existing work that led us to these developments of convolutional neural
networks as we know them today. I'll let virtual Fei-Fei take over [laughing] and give you a brief
introduction to the history of computer vision. Okay let's start with today's agenda. 
So we have two topics to cover one is a brief history of computer vision and the
other one is the overview of our course CS 231 so we'll start with a very
brief history of where vision comes from when did computer vision start and
where we are today. The history the history of vision can go back many many
years ago in fact about 543 million years ago. What was life like during that
time? Well the earth was mostly water there were a few species of animals
floating around in the ocean and life was very chill. Animals didn't move around
much there they don't have eyes or anything when food swims by they grab
them if the food didn't swim by they just float around but something really
remarkable happened around 540 million years ago. From fossil studies zoologists
found out within a very short period of time —  ten million years — the number of
animal species just exploded. It went from a few of them to hundreds of
thousands and that was strange — what caused this? There were many theories but for many
years it was a mystery evolutionary biologists call this evolution's Big Bang.
A few years ago an Australian zoologist called Andrew Parker proposed one of the
most convincing theory from the studies of fossils
he discovered around 540 million years ago the first animals developed eyes and
the onset of vision started this explosive speciation phase. Animals can
suddenly see; once you can see life becomes much more proactive. Some
predators went after prey and prey have to escape from predators so the
evolution or onset of vision started a evolutionary arms race and animals had
to evolve quickly in order to survive as a species so that was the beginning of
vision in animals after 540 million years vision has developed into the
biggest sensory system of almost all animals especially intelligent animals
in humans we have almost 50% of the neurons in our cortex involved in visual
processing it is the biggest sensory system that enables us to survive, work,
move around, manipulate things, communicate, entertain, and many things.
The vision is really important for animals and especially intelligent
animals. So that was a quick story of biological vision. What about humans, the
history of humans making mechanical vision or cameras? Well one of the early
cameras that we know today is from the 1600s, the Renaissance period of time,
camera obscura and this is a camera based on pinhole camera theories. It's
very similar to, it's very similar to the to the early eyes that animals developed
with a hole that collects lights and then a plane in the back of the
camera that collects the information and project the imagery. So
as cameras evolved, today we have cameras everywhere this is one of the most
popular sensors people use from smartphones to to other sensors. In the
mean time biologists started studying the mechanism of vision. One of
the most influential work in both human vision where animal vision as well as
that inspired computer vision is the work done by Hubel and Wiesel in the 50s
and 60s using electrophysiology. What they were asking, the question is "what was the visual processing mechanism like in primates, in mammals" so they chose
to study cat brain which is more or less similar to human brain from a visual
processing point of view. What they did is to stick some electrodes in the back
of the cat brain which is where the primary visual cortex area is and then
look at what stimuli makes the neurons in the in the back in the primary visual
cortex of cat brain respond excitedly what they learned is that there are many
types of cells in the, in the primary visual cortex part of the the cat brain
but one of the most important cell is the simple cells they respond to
oriented edges when they move in certain directions. Of course there are also more
complex cells but by and large what they discovered is visual processing starts
with simple structure of the visual world, oriented edges and as information
moves along the visual processing pathway the brain builds up the
complexity of the visual information until it can recognize the complex
visual world. So the history of computer vision also starts around early
60s. Block World is a set of work published by Larry Roberts which is
widely known as one of the first, probably the first PhD thesis of
computer vision where the visual world was simplified into simple geometric
shapes and the goal is to be able to recognize them and reconstruct what
these shapes are. In 1966 there was a now famous MIT summer project called "The
Summer Vision Project." The goal of this Summer Vision Project, I read: "is an
attempt to use our summer workers effectively in a construction of a
significant part of a visual system." So the goal is in one summer we're gonna work
out the bulk of the visual system. That was
an ambitious goal. Fifty years have passed; the field of computer vision has
blossomed from one summer project into a field of thousands of researchers
worldwide still working on some of the most fundamental problems of vision. We
still have not yet solved vision but it has grown into one of the most important
and fastest growing areas of artificial intelligence. Another
person that we should pay tribute to is David Marr. David Marr was a MIT vision
scientist and he has written an influential book in the late 70s about
what he thinks vision is and how we should go about computer vision
and developing algorithms that can enable computers to recognize the visual
world. The thought process in his, in David Mars book is
that in order to take an image and arrive at a final holistic full 3d
representation of the visual world we have to go through several process. The
first process is what he calls "primal sketch;" this is where mostly the edges,
the bars, the ends, the virtual lines, the curves, the boundaries, are represented
and this is very much inspired by what neuroscientists have seen: Hubel and
Wiesel told us the early stage of visual processing has a lot to do with simple
structures like edges. Then the next step after the edges and the curves is what David Marr calls "two-and-a-half d sketch;" this is where we
start to piece together the surfaces, the depth information, the layers, or the
discontinuities of the visual scene, and then eventually we put everything
together and have a 3d model hierarchically organized in terms of
surface and volumetric primitives and so on. So that was a very idealized thought
process of what vision is and this way of thinking actually has dominated
computer vision for several decades and is also a very intuitive way for
students to enter the field of vision and think about how we can deconstruct
the visual information. Another very important seminal group of
work happened in the 70s where people began to ask the question "how can we
move beyond the simple block world and start recognizing or representing real
world objects?" Think about the 70s, it's the time that there's very little
data available; computers are extremely slow, PCs are not even around,
but computer scientists are starting to think about how we can recognize and
represent objects. So in Palo Alto both at Stanford as well as SRI, two
groups of scientists that propose similar ideas: one is called "generalized
cylinder," one is called "pictorial structure." The basic idea is that every
object is composed of simple geometric primitives; for example a person can be
pieced together by generalized cylindrical shapes or a person can be
pieced together by critical part in their elastic distance between
these parts so either representation is a way to
reduce the complex structure of the object into a collection of
simpler shapes and their geometric configuration. These work have been
influential for quite a few, quite a few years and then in the 80s David Lowe, here
is another example of thinking how to reconstruct or recognize the visual
world from simple world structures, this work is by David Lowe which he tries to
recognize razors by constructing lines and edges and and mostly
straight lines and their combination. So there was a lot of effort in trying to
think what what is the tasks in computer vision in the 60s 70s and 80s and frankly
it was very hard to solve the problem of object recognition; everything I've shown
you so far are very audacious ambitious attempts but they remain at the level of
toy examples or just a few examples. Not a lot of
progress have been made in terms of delivering something that can work in
real world. So as people think about what are the problems to solving vision one
important question came around is: if object recognition is too hard,
maybe we should first do object segmentation, that is the task of taking
an image and group the pixels into meaningful areas. We might not know the
pixels that group together is called a person, but we can extract out all the
pixels that belong to the person from its background; that is called image
segmentation. So here's one very early seminal work by Jitendra Malik and his
student Jianbo Shi from Berkeley from using a graph theory algorithm for the
problem of image segmentation. Here's another problem that made some headway
ahead of many other problems in computer vision, which is face detection.
Faces one of the most important objects to humans, probably the most important
objects to humans, around the time of 1999 to 2000 machine learning techniques,
especially statistical machine learning techniques start to gain
momentum. These are techniques such as support vector machines, boosting,
graphical models, including the first wave of neural networks. One particular
work that made a lot of contribution was using AdaBoost algorithm to do
real-time face detection by Paul Viola and Michael Jones and there's a lot to
admire in this work. It was done in 2001 when computer chips are still very very
slow but they're able to do face detection in
images in near-real-time and after the publication of this paper in five years
time, 2006, Fujifilm rolled out the first digital camera that has a real-time
face detector in the in the camera so it was a very rapid transfer from basic
science research to real world application. So as a field we continue to
explore how we can do object recognition better so one of the very influential
way of thinking in the late 90s til the first 10 years of 2000 is feature based
object recognition and here is a seminal work by David Lowe called SIFT feature. 
The idea is that to match and the entire object for example here is a stop sign to
another stop sight is very difficult because there might be all kinds of
changes due to camera angles, occlusion, viewpoint, lighting, and just the
intrinsic variation of the object itself but it's inspired to observe that there
are some parts of the object, some features, that tend to remain diagnostic
and invariant to changes so the task of object recognition began with identifying
these critical features on the object and then match the features to a similar
object, that's a easier task than pattern matching the entire object. So here is a
figure from his paper where it shows that a handful, several dozen SIFT
features from one stop sign are identified and matched to the SIFT
features of another stop sign. Using the same building block which is
features, diagnostic features in images, we have as a field has made another step
forward and start to recognizing holistic scenes. Here is an example
algorithm called Spatial Pyramid Matching; the idea is that there are
features in the images that can give us clues about which type of scene it is,
whether it's a landscape or a kitchen or a highway and so on and this particular
work takes these features from different parts of the image and in different
resolutions and put them together in a feature descriptor and then we do
support vector machine algorithm on top of that. Similarly a very similar work
has gained momentum in human recognition so putting together these features well
we have a number of work that looks at how we can compose human bodies in more
realistic images and recognize them. So one work is called the "histogram of
gradients," another work is called "deformable part models," so as you
can see as we move from the 60s 70s 80s towards the first decade of the 21st
century one thing is changing and that's the quality of the pictures were no
longer, with the Internet the the the growth of the Internet the digital
cameras were having better and better data to study computer vision. So one of
the outcome in the early 2000s is that the field of computer vision has defined
a very important building block problem to solve. It's not the only problem to solve but in terms of recognition this is a very
important problem to solve which is object recognition. I talked about object
recognition all along but in the early 2000s we began to have benchmark data
set that can enable us to measure the progress of object recognition. One of
the most influential benchmark data set is called PASCAL Visual Object Challenge,
and it's a data set composed of 20 object classes, three of them are shown
here: train, airplane, person; I think it also has cows, bottles, cats, and so on; and
the data set is composed of several thousand to ten thousand images per
category and then the field different groups develop algorithm to test
against the testing set and see how we have made progress. So here is a figure
that shows from year 2007 to year 2012. The performance on detecting objects the
20 object in this image in a in a benchmark data set has steadily
increased. So there was a lot of progress made. Around that time a group of us from
Princeton to Stanford also began to ask a harder question to ourselves as well
as our field which is: are we ready to recognize every object or most of the
object in the world. It's also motivated by an observation that is rooted in
machine learning which is that most of the machine learning algorithms it
doesn't matter if it's graphical model, or support vector machine, or AdaBoost,
is very likely to overfit in the training process and part of the
problem is visual data is very complex because it's complex our models tend to
have a high dimension a high dimension of input and have to have a lot of
parameters to fit and when we don't have enough training data overfitting happens
very fast and then we cannot generalize very well. So motivated by this dual
reason, one is just want to recognize the world of all the objects, the other
one is to come back the machine learning overcome the the machine learning
bottleneck of overfitting, we began this project called ImageNet. We wanted to
put together the largest possible dataset of all the pictures we can find, the
world of objects, and use that for training as well as for benchmarking. So
it was a project that took us about three years, lots of hard work; it
basically began with downloading billions of images from the internet
organized by the dictionary we called WordNet which is tens of thousands of
object classes and then we have to use some clever crowd engineering trick a
method using Amazon Mechanical Turk platform to sort, clean, label each of the
images. The end result is a ImageNet of almost 15 million or 40 million plus
images organized in twenty-two thousand categories of objects and scenes and
this is the gigantic, probably the biggest dataset produced in the field of
AI at that time and it began to push forward the algorithm development of
object recognition into another phase. Especially important is how to benchmark
the progress so starting 2009 the ImageNet team rolled
out an international challenge called ImageNet Large-Scale Visual Recognition
Challenge and for this challenge we put together a more stringent test set of
1.4 million objects across 1,000 object classes and this is to test the image
classification recognition results for the computer vision algorithms. So here's
the example picture and if an algorithm can output 5 labels and and top five
labels includes the correct object in this picture then we call this a success.
So here is a result summary of the ImageNet Challenge, of the image
classification result from 2010 to 2015 so on x axis you see the
years and the y axis you see the error rate. So the good news is the error rate
is steadily decreasing to the point by 2012 the error rate is so low is on par
with what humans can do and here a human I mean a single Stanford PhD student who
spend weeks doing this task as if he were a computer participating in the
ImageNet Challenge. So that's a lot of progress made even though we have not
solved all the problems of object recognition which you'll learn about in
this class but to go from an error rate that's
unacceptable for real-world application all the way to on par being on par with
humans in ImageNet challenge, the field took only a few years. And one particular
moment you should notice on this graph is the the year 2012. In the first two
years our error rate hovered around 25 percent but in 2012 the error rate was
dropped more almost 10 percent to 16 percent even though now it's better but
that drop was very significant and the winning algorithm of that year is a
convolutional neural network model that beat all other algorithms around that
time to win the ImageNet challenge and this is the focus of our whole course
this quarter is to look at to have a deep dive into what convolutional neural
network models are and another name for this is deep learning by by popular popular name now it's called deep
learning and to look at what these models are what are the principles what
are the good practices what are the recent progress of this model, but
here is where the history was made is that we, around 2012 convolutional
neural network model or deep learning models showed the tremendous capacity
and ability in making a good progress in the field of computer vision along with
several other sister fields like natural language processing and speech
recognition. So without further ado I'm going to hand the rest of the lecture to
to Justin to talk about the overview of CS 231n.  Alright, thanks so much Fei-Fei. I'll take it over from here. So now I want to shift gears a little bit and talk a little bit more
about this class CS231n. So this class focuses
on one of these most, so the primary focus of this class is this image classification problem which we previewed a
little bit in the contex of the ImageNet Challenge. So in image classification, again, the setup is that your
algorithm looks at an image and then picks from among
some fixed set of categories to classify that image. And, this might seem like
somewhat of a restrictive or artificial setup, but
it's actual quite general. And, this problem can be applied
in many different settings both in industry and academia
and many different places. So for example, you could
apply this to recognizing food or recognizing calories
in food or recognizing different artworks, different
product out in the world. So this relatively basic
tool of image classification is super useful on its
own and could be applied all over the place for many
different applications. But, in this course,
we're also going to talk about several other visual
recognition problems that build upon many of
the tools that we develop for the purpose of image classification. We'll talk about other problems such as object detection
or image captioning. So the setup in object detection is a little bit different. Rather than classifying an entire image as a cat or a dog or a horse or whatnot, instead we want to go in
and draw bounding boxes and say that there is a
dog here, and a cat here, and a car over in the background, and draw these boxes describing where objects are in the image. We'll also talk about image captioning where given an image the system now needs to produce a
natural language sentence describing the image. It sounds like a really hard, complicated, and different problem, but we'll see that many of the tools that we develop in service of image classification will be reused in these
other problems as well. So we mentioned this before in the context of the ImageNet Challenge,
but one of the things that's really driven the
progress of the field in recent years has been this adoption of convolutional neural networks or CNNs or sometimes called convnets. So if we look at the
algorithms that have won the ImageNet Challenge for
the last several years, in 2011 we see this method from Lin et al which is still hierarchical. It consists of multiple layers. So first we compute some features, next we compute some local invariances, some pooling, and go
through several layers of processing, and then finally feed this resulting descriptor to a linear SVN. What you'll notice here is that
this is still hierarchical. We're still detecting edges. We're still having notions of invariance. And, many of these
intuitions will carry over into convnets. But, the breakthrough
moment was really in 2012 when Jeff Hinton's group in Toronto together with Alex
Krizhevsky and Ilya Sutskever who were his PHD student at that time created this seven layer
convolutional neural network now known as AlexNet,
then called Supervision which just did very, very well
in the ImageNet competition in 2012. And, since then every year
the winner of ImageNet has been a neural network. And, the trend has been
that these networks are getting deeper and deeper each year. So AlexNet was a seven or
eight layer neural network depending on how exactly you count things. In 2015 we had these much deeper networks. GoogleNet from Google
and VGG, the VGG network from Oxford which was about
19 layers at that time. And, then in 2015 it got really crazy and this paper came out
from Microsoft Research Asia called Residual Networks which
were 152 layers at that time. And, since then it turns out you can get a little bit better if you go up to 200, but you run our of memory on your GPUs. We'll get into all of that later, but the main takeaway here
is that convolutional neural networks really had
this breakthrough moment in 2012, and since then there's been a lot of effort focused
in tuning and tweaking these algorithms to make them
perform better and better on this problem of image classification. And, throughout the rest of the quarter, we're going to really dive in deep, and you'll understand exactly
how these different models work. But, one point that's really important, it's true that the breakthrough moment for convolutional neural
networks was in 2012 when these networks performed very well on the ImageNet Challenge,
but they certainly weren't invented in 2012. These algorithms had actually been around for quite a long time before that. So one of the sort of foundational works in this area of
convolutional neural networks was actually in the '90s from
Jan LeCun and collaborators who at that time were at Bell Labs. So in 1998 they build this
convolutional neural network for recognizing digits. They wanted to deploy
this and wanted to be able to automatically recognize
handwritten checks or addresses for the post office. And, they built this
convolutional neural network which could take in the pixels of an image and then classify either what digit it was or what letter it was or whatnot. And, the structure of this network actually look pretty
similar to the AlexNet architecture that was used in 2012. Here we see that, you know, we're taking in these raw pixels. We have many layers of
convolution and sub-sampling, together with the so called
fully connected layers. All of which will be
explained in much more detail later in the course. But, if you just kind of
look at these two pictures, they look pretty similar. And, this architecture in 2012 has a lot of these architectural similarities that are shared with this
network going back to the '90s. So then the question you might ask is if these algorithms
were around since the '90s, why have they only suddenly become popular in the last couple of years? And, there's a couple
really key innovations that happened that have
changed since the '90s. One is computation. Thanks to Moore's law, we've gotten faster and faster computers every year. And, this is kind of a coarse measure, but if you just look at
the number of transistors that are on chips, then that has grown by several orders of magnitude
between the '90s and today. We've also had this advent
of graphics processing units or GPUs which are super parallelizable and ended up being a perfect tool for really crunching these
computationally intensive convolutional neural network models. So just by having more compute available, it allowed researchers to
explore with larger architectures and larger models, and in some cases, just increasing the model
size, but still using these kind of classical approaches
and classical algorithms tends to work quite well. So this idea of increasing computation is super important in the
history of deep learning. I think the second key
innovation that changed between now and the '90s was data. So these algorithms are
very hungry for data. You need to feed them
a lot of labeled images and labeled pixels for them
to eventually work quite well. And, in the '90s there just wasn't that much labeled data available. This was, again, before
tools like Mechanical Turk, before the internet was
super, super widely used. And, it was very difficult to collect large, varied datasets. But, now in the 2010s
with datasets like PASCAL and ImageNet, there existed
these relatively large, high quality labeled
datasets that were, again, orders and orders magnitude bigger than the dataset available in the '90s. And, these much large datasets, again, allowed us to work with
higher capacity models and train these models to
actually work quite well on real world problems. But, the critical takeaway here is that convolutional neural networks although they seem like this
sort of fancy, new thing that's only popped up in
the last couple of years, that's really not the case. And, these class of
algorithms have existed for quite a long time in
their own right as well. Another thing I'd like to point out in computer vision we're in the business of trying to build machines
that can see like people. And, people can actually
do a lot of amazing things with their visual systems. When you go around the world, you do a lot more than just drawing boxes around the objects and classifying
things as cats or dogs. Your visual system is much
more powerful than that. And, as we move forward in the field, I think there's still a
ton of open challenges and open problems that we need to address. And, we need to continue
to develop our algorithms to do even better and tackle
even more ambitious problems. Some examples of this are
going back to these older ideas in fact. Things like semantic segmentation
or perceptual grouping where rather than
labeling the entire image, we want to understand for
every pixel in the image what is it doing, what does it mean. And, we'll revisit that
idea a little bit later in the course. There's definitely work going back to this idea of 3D understanding, of reconstructing the entire world, and that's still an
unsolved problem I think. There're just tons and tons of other tasks that you can imagine. For example activity recognition, if I'm given a video of some person doing some activity, what's the best way to recognize that activity? That's quite a challenging
problem as well. And, then as we move forward with things like augmented reality
and virtual reality, and as new technologies
and new types of sensors become available, I think we'll come up with a lot of new, interesting
hard and challenging problems to tackle as a field. So this is an example
from some of my own work in the vision lab on this
dataset called Visual Genome. So here the idea is that
we're trying to capture some of these intricacies
in the real world. Rather than maybe describing just boxes, maybe we should be describing images as these whole large graphs
of semantically related concepts that encompass
not just object identities but also object relationships,
object attributes, actions that are occurring in the scene, and this type of
representation might allow us to capture some of this
richness of the visual world that's left on the table when we're using simple classification. This is by no means a standard
approach at this point, but just kind of giving you this sense that there's so much more
that your visual system can do that is maybe not captured in this vanilla image classification setup. I think another really interesting work that kind of points in this direction actually comes from
Fei-Fei's grad school days when she was doing her PHD at Cal Tech with her advisors there. In this setup, they had
people, they stuck people, and they showed people this
image for just half a second. So they flashed this
image in front of them for just a very short period of time, and even in this very, very rapid exposure to an image, people were able to write these long descriptive paragraphs giving a whole story of the image. And, this is quite remarkable
if you think about it that after just half a second
of looking at this image, a person was able to say that this is some kind of a game or
fight, two groups of men. The man on the left is throwing something. Outdoors because it seem like
I have an impression of grass, and so on and so on. And, you can imagine that if a person were to look even longer at this image, they could write probably a whole novel about who these people
are, and why are they in this field playing this game. They could go on and on and on roping in things from
their external knowledge and their prior experience. This is in some sense the
holy grail of computer vision. To sort of understand
the story of an image in a very rich and deep way. And, I think that despite
the massive progress in the field that we've had
over the past several years, we're still quite a long way
from achieving this holy grail. Another image that I
think really exemplifies this idea actually comes, again,
from Andrej Karpathy's blog is this amazing image. Many of you smiled, many of you laughed. I think this is a pretty funny image. But, why is it a funny image? Well we've got a man standing on a scale, and we know that people
are kind of self conscious about their weight sometimes,
and scales measure weight. Then we've got this other guy behind him pushing his foot down on the scale, and we know that because
of the way scales work that will cause him to
have an inflated reading on the scale. But, there's more. We know that this person
is not just any person. This is actually Barack
Obama who was at the time President of the United States, and we know that Presidents
of the United States are supposed to be respectable
politicians that are [laughing] probably not supposed to be playing jokes on their compatriots in this way. We know that there's these people in the background that
are laughing and smiling, and we know that that means that they're understanding something about the scene. We have some understanding that they know that President Obama
is this respectable guy who's looking at this other guy. Like, this is crazy. There's so much going on in this image. And, our computer vision algorithms today are actually a long way
I think from this true, deep understanding of images. So I think that sort of
despite the massive progress in the field, we really
have a long way to go. To me, that's really
exciting as a researcher 'cause I think that we'll have just a lot of really
exciting, cool problems to tackle moving forward. So I hope at this point I've
done a relatively good job to convince you that computer
vision is really interesting. It's really exciting. It can be very useful. It can go out and make
the world a better place in various ways. Computer vision could be applied in places like medical
diagnosis and self-driving cars and robotics and all
these different places. In addition to sort of tying
back to sort of this core idea of understanding human intelligence. So to me, I think that computer vision is this fantastically
amazing, interesting field, and I'm really glad that over the course of the quarter, we'll
get to really dive in and dig into all these different details about how these algorithms
are working these days. That's sort of my pitch
about computer vision and about the history of computer vision. I don't know if there's
any questions about this at this time. Okay. So then I want to talk a little bit more about the logistics of this class for the rest of the quarter. So you might ask who are we? So this class is taught by Fei-Fei Li who is a professor of computer
science here at Standford who's my advisor and director
of the Stanford Vision Lab and also the Stanford AI Lab. The other two instructors
are me, Justin Johnson, and Serena Yeung who is
up here in the front. We're both PHD students
working under Fei-Fei on various computer vision problems. We have an amazing
teaching staff this year of 18 TAs so far. Many of whom are sitting
over here in the front. These guys are really the unsung heroes behind the scenes making
the course run smoothly, making sure everything happens well. So be nice to them. [laughing] I think I also should mention
this is the third time we've taught this course,
and it's the first time that Andrej Karpathy has
not been an instructor in this course. He was a very close friend of mine. He's still alive. He's okay, don't worry. [laughing] But, he graduated, so he's actually here I think hanging around
in the lecture hall. A lot of the development and
the history of this course is really due to him working on it with me over the last couple of years. So I think you should be aware of that. Also about logistics,
probably the best way for keeping in touch with the course staff is through Piazza. You should all go and signup right now. Piazza is really our preferred
method of communication with the class with the teaching staff. If you have questions that you're afraid of being embarrassed about asking in front of your classmates, go ahead and ask anonymously even
post private questions directly to the teaching staff. So basically anything that you need should ideally go through Piazza. We also have a staff mailing list, but we ask that this is mostly for sort of personal, confidential things that you don't want going on Piazza, or if you have something
that's super confidential, super personal, then feel free to directly email me or
Fei-Fei or Serena about that. But, for the most part,
most of your communication with the staff should be through Piazza. We also have an optional
textbook this year. This is by no means required. You can go through the course
totally fine without it. Everything will be self contained. This is sort of exciting
because it's maybe the first textbook about deep
learning that got published earlier this year by E.N. Goodfellow, Yoshua Bengio, and Aaron Courville. I put the Amazon link here in the slides. You can get it if you want to, but also the whole content of the book is free online, so you
don't even have to buy it if you don't want to. So again, this is totally optional, but we'll probably be
posting some readings throughout the quarter
that give you an additional perspective on some of the material. So our philosophy about this class is that you should really
understand the deep mechanics of all of these algorithms. You should understand at a very deep level exactly how these algorithms are working like what exactly is going on when you're stitching together these neural networks, how do these architectural decisions influence how the network is trained and tested and whatnot and all that. And, throughout the course
through the assignments, you'll be implementing
your own convolutional neural networks from scratch in Python. You'll be implementing the
full forward and backward passes through these
things, and by the end, you'll have implemented a whole
convolutional neural network totally on your own. I think that's really cool. But, we also kind of
practical, and we know that in most cases people
are not writing these things from scratch, so we also want to give you a good introduction to some
of the state of the art software tools that are used
in practice for these things. So we're going to talk about
some of the state of the art software packages like Tensor
Flow, Torch, [Py]Torch, all these other things. And, I think you'll get some exposure to those on the homeworks
and definitely through the course project as well. Another note about this course is that it's very state of the art. I think it's super exciting. This is a very fast moving field. As you saw, even these plots
in the imaging challenge basically there's been a ton of progress since 2012, and like while
I've been in grad school, the whole field is sort
of transforming ever year. And, that's super exciting
and super encouraging. But, what that means is that
there's probably content that we'll cover this
year that did not exist the last time that this
course was taught last year. I think that's super
exciting, and that's one of my favorite parts
about teaching this course is just roping in all
these new scientific, hot off the presses stuff and being able to present it to you guys. We're also sort of about fun. So we're going to talk
about some interesting maybe not so serious
topics as well this quarter including image captioning is pretty fun where we can write
descriptions about images. But, we'll also cover some
of these more artistic things like DeepDream here on the left where we can use neural
networks to hallucinate these crazy, psychedelic images. And, by the end of the course, you'll know how that works. Or on the right, this
idea of style transfer where we can take an image and render it in the style of famous artists
like Picasso or Van Gogh or what not. And again, by the end of the quarter, you'll see how this stuff works. So the way the course works
is we're going to have three problem sets. The first problem set
will hopefully be out by the end of the week. We'll have an in class,
written midterm exam. And, a large portion of your grade will be the final course
project where you'll work in teams of one to three and produce some amazing project that
will blow everyone's minds. We have a late policy, so
you have seven late days that you're free to allocate
among your different homeworks. These are meant to cover
things like minor illnesses or traveling or conferences
or anything like that. If you come to us at
the end of the quarter and say that, "I suddenly
have to give a presentation "at this conference." That's not going to be okay. That's what your late days are for. That being said, if you have some very extenuating circumstances,
then do feel free to email the course staff
if you have some extreme circumstances about that. Finally, I want to make a note about the collaboration policy. As Stanford students,
you should all be aware of the honor code that governs the way that you should be collaborating
and working together, and we take this very seriously. We encourage you to think very carefully about how you're
collaborating and making sure it's within the bounds of the honor code. So in terms of prerequisites,
I think the most important is probably a deep familiarity with Python because all of the programming assignments will be in Python. Some familiarity with C
or C++ would be useful. You will probably not
be writing any C or C++ in this course, but as you're
browsing through the source code of these various software packages, being able to read C++ code at least is very useful for understanding
how these packages work. We also assume that you
know what calculus is, you know how to take derivatives
all that sort of stuff. We assume some linear algebra. That you know what matrices are and how to multiply them
and stuff like that. We can't be teaching you how to take like derivatives and stuff. We also assume a little bit of knowledge coming in of computer
vision maybe at the level of CS131 or 231a. If you have taken those courses before, you'll be fine. If you haven't, I think
you'll be okay in this class, but you might have a tiny
bit of catching up to do. But, I think you'll probably be okay. Those are not super strict prerequisites. We also assume a little
bit of background knowledge about machine learning
maybe at the level of CS229. But again, I think really
important, key fundamental machine learning concepts
we'll reintroduce as they come up and become important. But, that being said, a
familiarity with these things will be helpful going forward. So we have a course website. Go check it out. There's a lot of information and links and syllabus and all that. I think that's all that I
really want to cover today. And, then later this week on Thursday, we'll really dive into our
first learning algorithm and start diving into the
details of these things. 

Okay, so welcome to lecture two of CS231N. On Tuesday we, just recall,
we, sort of, gave you the big picture view of
what is computer vision, what is the history, and a little bit of the
overview of the class. And today, we're really going
to dive in, for the first time, into the details. And we'll start to see,
in much more depth, exactly how some of
these learning algorithms actually work in practice. So, the first lecture of the class is probably, sort of, the
largest big picture vision. And the majority of the
lectures in this class will be much more detail orientated, much more focused on
the specific mechanics, of these different algorithms. So, today we'll see our
first learning algorithm and that'll be really exciting, I think. But, before we get to that, I wanted to talk about a couple
of administrative issues. One, is Piazza. So, I saw it when I checked yesterday, it seemed like we had maybe 500 students signed up on Piazza. Which means that there
are several hundred of you who are not yet there. So, we really want Piazza
to be the main source of communication between the
students and the core staff. So, we've gotten a lot of
questions to the staff list about project ideas or questions
about midterm attendance or poster session attendance. And, any, sort of, questions like that should really go to Piazza. You'll probably get answers
to your questions faster on Piazza, because all the
TAs are knowing to check that. And it's, sort of, easy
for emails to get lost in the shuffle if you just
send to the course list. It's also come to my attention
that some SCPD students are having a bit of a hard
time signing up for Piazza. SCPD students are supposed to receive a @stanford.edu email address. So, once you get that email address, then you can use the Stanford
email to sign into Piazza. Probably that doesn't
affect those of you who are sitting in the room right now, but, for those students listening on SCPD. The next administrative issue
is about assignment one. Assignment one will be up later today, probably sometime this afternoon, but I promise, before
I go to sleep tonight, it'll be up. But, if you're getting a little bit antsy and really want to start
working on it right now, then you can look at last year's version of assignment one. It'll be pretty much the same content. We're just reshuffling it
a little bit to make it, like, for example, upgrading
to work with Python 3, rather than Python 2.7. And some of these minor cosmetic changes, but the content of the
assignment will still be the same as last year. So, in this assignment you'll
be implementing your own k-nearest neighbor classifier, which we're going to talk
about in this lecture. You'll also implement several
different linear classifiers, including the SVM and Softmax, as well as a simple
two-layer neural network. And we'll cover all this content over the next couple of lectures. So, all of our assignments
are using Python and NumPy. If you aren't familiar
with Python or NumPy, then we have written a
tutorial that you can find on the course website to
try and get you up to speed. But, this is, actually, pretty important. NumPy lets you write these
very efficient vectorized operations that let you do
quite a lot of computation in just a couple lines of code. So this is super important for pretty much all aspects of numerical
computing and machine learning and everything like that, is efficiently implementing
these vectorized operations. And you'll get a lot of practice with this on the first assignment. So, for those of you who
don't have a lot of experience with Matlab or NumPy or
other types of vectorized tensor computation, I recommend
that you start looking at this assignment pretty early and also, read carefully
through the tutorial. The other thing I wanted to talk about is that we're happy to announce that we're officially supported
through Google Cloud for this class. So, Google Cloud is somewhat
similar to Amazon AWS. You can go and start virtual
machines up in the cloud. These virtual machines can have GPUs. We're working on the tutorial
for exactly how to use Google Cloud and get it to
work for the assignments. But our intention is that
you'll be able to just download some image, and it'll be very seamless for you to work on the assignments on one of these instances on the cloud. And because Google has, very generously, supported this course, we'll be able to distribute to each of you coupons that let you use
Google Cloud credits for free for the class. So you can feel free to use
these for the assignments and also for the course projects when you want to start using
GPUs and larger machines and whatnot. So, we'll post more details about that, probably, on Piazza later today. But, I just wanted to mention, because I know there had
been a couple of questions about, can I use my laptop? Do I have to run on corn? Do I have to, whatever? And the answer is that,
you'll be able to run on Google Cloud and we'll provide
you some coupons for that. Yeah, so, those are, kind of, the
major administrative issues I wanted to talk about today. And then, let's dive into the content. So, the last lecture
we talked a little bit about this task of image classification, which is really a core
task in computer vision. And this is something
that we'll really focus on throughout the course of the class. Is, exactly, how do we work on this
image classification task? So, a little bit more concretely, when you're doing image classification, your system receives some input image, which is this cute cat in this example, and the system is aware
of some predetermined set of categories or labels. So, these might be, like,
a dog or a cat or a truck or a plane, and there's some
fixed set of category labels, and the job of the computer
is to look at the picture and assign it one of these
fixed category labels. This seems like a really easy problem, because so much of your own
visual system in your brain is hardwired to doing these, sort of, visual recognition tasks. But this is actually a
really, really hard problem for a machine. So, if you dig in and
think about, actually, what does a computer see
when it looks at this image, it definitely doesn't get
this holistic idea of a cat that you see when you look at it. And the computer really
is representing the image as this gigantic grid of numbers. So, the image might be something
like 800 by 600 pixels. And each pixel is
represented by three numbers, giving the red, green, and
blue values for that pixel. So, to the computer, this is just a gigantic grid of numbers. And it's very difficult
to distill the cat-ness out of this, like, giant array
of thousands, or whatever, very many different numbers. So, we refer to this
problem as the semantic gap. This idea of a cat, or
this label of a cat, is a semantic label that
we're assigning to this image, and there's this huge gap between
the semantic idea of a cat and these pixel values that the
computer is actually seeing. And this is a really hard problem because you can change the picture
in very small, subtle ways that will cause this pixel
grid to change entirely. So, for example, if we took this same cat, and if the cat happened to sit still and not even twitch, not move a muscle, which is never going to happen, but we moved the camera to the other side, then every single grid,
every single pixel, in this giant grid of numbers would be completely different. But, somehow, it's still
representing the same cat. And our algorithms need
to be robust to this. But, not only viewpoint is one problem, another is illumination. There can be different
lighting conditions going on in the scene. Whether the cat is appearing
in this very dark, moody scene, or like is this very bright,
sunlit scene, it's still a cat, and our algorithms need
to be robust to that. Objects can also deform. I think cats are, maybe,
among the more deformable of animals that you might see out there. And cats can really assume a
lot of different, varied poses and positions. And our algorithms should
be robust to these different kinds of transforms. There can also be problems of occlusion, where you might only see part
of a cat, like, just the face, or in this extreme example,
just a tail peeking out from under the couch cushion. But, in these cases, it's pretty
easy for you, as a person, to realize that this is probably a cat, and you still recognize
these images as cats. And this is something that our algorithms also must be robust to, which is quite difficult, I think. There can also be problems
of background clutter, where maybe the foreground
object of the cat, could actually look quite
similar in appearance to the background. And this is another thing
that we need to handle. There's also this problem
of intraclass variation, that this one notion of
cat-ness, actually spans a lot of different visual appearances. And cats can come in
different shapes and sizes and colors and ages. And our algorithm, again, needs to work and handle all these different variations. So, this is actually a really,
really challenging problem. And it's sort of easy to
forget how easy this is because so much of your
brain is specifically tuned for dealing with these things. But now if we want our computer programs to deal with all of these
problems, all simultaneously, and not just for cats, by the way, but for just about any object
category you can imagine, this is a fantastically
challenging problem. And it's, actually, somewhat miraculous that this works at all, in my opinion. But, actually, not only does it work, but these things work very
close to human accuracy in some limited situations. And take only hundreds
of milliseconds to do so. So, this is some pretty
amazing, incredible technology, in my opinion, and over the
course of the rest of the class we will really see what
kinds of advancements have made this possible. So now, if you, kind of, think about what is the API for writing
an image classifier, you might sit down and try
to write a method in Python like this. Where you want to take in an image and then do some crazy magic and then, eventually,
spit out this class label to say cat or dog or whatnot. And there's really no obvious
way to do this, right? If you're taking an algorithms class and your task is to sort numbers or compute a convex hull or, even, do something
like RSA encryption, you, sort of, can write down an algorithm and enumerate all the
steps that need to happen in order for this things to work. But, when we're trying
to recognize objects, or recognize cats or images, there's no really clear,
explicit algorithm that makes intuitive sense, for how you might go about
recognizing these objects. So, this is, again, quite challenging, if you think about, if it was your first day programming and you had to sit down
and write this function, I think most people would be in trouble. That being said, people have definitely
made explicit attempts to try to write, sort
of, high-end coded rules for recognizing different animals. So, we touched on this a
little bit in the last lecture, but maybe one idea for cats is that, we know that cats have ears
and eyes and mouths and noses. And we know that edges,
from Hubel and Wiesel, we know that edges are pretty important when it comes to visual recognition. So one thing we might try to do is compute the edges of this image and then go in and try to
categorize all the different corners and boundaries, and
say that, if we have maybe three lines meeting this way,
then it might be a corner, and an ear has one corner
here and one corner there and one corner there, and then, kind of, write down
this explicit set of rules for recognizing cats. But this turns out not to work very well. One, it's super brittle. And, two, say, if you want
to start over for another object category, and maybe
not worry about cats, but talk about trucks or dogs
or fishes or something else, then you need to start all over again. So, this is really not a
very scalable approach. We want to come up with some
algorithm, or some method, for these recognition tasks which scales much more
naturally to all the variety of objects in the world. So, the insight that, sort
of, makes this all work is this idea of the data-driven approach. Rather than sitting down and
writing these hand-specified rules to try to craft exactly
what is a cat or a fish or what have you, instead, we'll go out onto the internet and collect a large
dataset of many, many cats and many, many airplanes
and many, many deer and different things like this. And we can actually use tools
like Google Image Search, or something like that, to go out and collect a very
large number of examples of these different categories. By the way, this actually
takes quite a lot of effort to go out and actually
collect these datasets but, luckily, there's a lot
of really good, high quality datasets out there already for you to use. Then once we get this dataset, we train this machine learning classifier that is going to ingest all of the data, summarize it in some way, and then spit out a model that summarizes the
knowledge of how to recognize these different object categories. Then finally, we'll
use this training model and apply it on new images that will then be able to recognize cats and dogs and whatnot. So here our API has changed a little bit. Rather than a single function that just inputs an image
and recognizes a cat, we have these two functions. One, called, train, that's
going to input images and labels and then output a model, and then, separately, another
function called, predict, which will input the model
and than make predictions for images. And this is, kind of, the key insight that allowed all these things
to start working really well over the last 10, 20 years or so. So, this class is primarily
about neural networks and convolutional neural networks and deep learning and all that, but this idea of a data-driven
approach is much more general than just deep learning. And I think it's useful to, sort of, step through this process for a very simple classifier first, before we get to these big, complex ones. So, probably, the simplest
classifier you can imagine is something we call nearest neighbor. The algorithm is pretty dumb, honestly. So, during the training
step we won't do anything, we'll just memorize all
of the training data. So this is very simple. And now, during the prediction step, we're going to take some new image and go and try to find
the most similar image in the training data to that new image, and now predict the label
of that most similar image. A very simple algorithm. But it, sort of, has a lot
of these nice properties with respect to
data-drivenness and whatnot. So, to be a little bit more concrete, you might imagine working on
this dataset called CIFAR-10, which is very commonly
used in machine learning, as kind of a small test case. And you'll be working with
this dataset on your homework. So, the CIFAR-10 dataset gives
you 10 different classes, airplanes and automobiles and
birds and cats and different things like that. And for each of those 10 categories it provides 50,000 training images, roughly evenly distributed
across these 10 categories. And then 10,000 additional testing images that you're supposed to
test your algorithm on. So here's an example
of applying this simple nearest neighbor classifier
to some of these test images on CIFAR-10. So, on this grid on the right, for the left most column, gives a test image in
the CIFAR-10 dataset. And now on the right, we've
sorted the training images and show the most similar training images to each of these test examples. And you can see that they
look kind of visually similar to the training images, although they are not
always correct, right? So, maybe on the second row,
we see that the testing, this is kind of hard to see, because these images are 32 by 32 pixels, you need to really dive in there and try to make your best guess. But, this image is a dog and
it's nearest neighbor is also a dog, but this next one,
I think is actually a deer or a horse or something else. But, you can see that it
looks quite visually similar, because there's kind of a
white blob in the middle and whatnot. So, if we're applying the
nearest neighbor algorithm to this image, we'll find the closest
example in the training set. And now, the closest
example, we know it's label, because it comes from the training set. And now, we'll simply say that
this testing image is also a dog. You can see from these
examples that is probably not going to work very well, but it's still kind of a
nice example to work through. But then, one detail
that we need to know is, given a pair of images, how can we actually compare them? Because, if we're going to take
our test image and compare it to all the training images, we actually have many different choices for exactly what that comparison
function should look like. So, in the example in the previous slide, we've used what's called the L1 distance, also sometimes called
the Manhattan distance. So, this is a really
sort of simple, easy idea for comparing images. And that's that we're going to
just compare individual pixels in these images. So, supposing that our test
image is maybe just a tiny four by four image of pixel values, then we're take this upper-left hand pixel of the test image, subtract off the value
in the training image, take the absolute value, and get the difference in that
pixel between the two images. And then, sum all these
up across all the pixels in the image. So, this is kind of a stupid
way to compare images, but it does some reasonable
things sometimes. But, this gives us a very concrete way to measure the difference
between two images. And in this case, we have
this difference of 456 between these two images. So, here's some full Python code for implementing this
nearest neighbor classifier and you can see it's pretty
short and pretty concise because we've made use of
many of these vectorized operations offered by NumPy. So, here we can see that
this training function, that we talked about earlier, is, again, very simple, in
the case of nearest neighbor, you just memorize the training data, there's not really much to do here. And now, at test time, we're
going to take in our image and then go in and compare
using this L1 distance function, our test image to each of
these training examples and find the most similar
example in the training set. And you can see that, we're
actually able to do this in just one or two lines of Python code by utilizing these vectorized
operations in NumPy. So, this is something that
you'll get practice with on the first assignment. So now, a couple questions
about this simple classifier. First, if we have N examples
in our training set, then how fast can we expect
training and testing to be? Well, training is probably constant because we don't really
need to do anything, we just need to memorize the data. And if you're just copying a pointer, that's going to be constant time no matter how big your dataset is. But now, at test time we need
to do this comparison stop and compare our test image to each of the N training
examples in the dataset. And this is actually quite slow. So, this is actually somewhat backwards, if you think about it. Because, in practice, we want our classifiers to
be slow at training time and then fast at testing time. Because, you might imagine,
that a classifier might go and be trained in a data center somewhere and you can afford to
spend a lot of computation at training time to make
the classifier really good. But then, when you go and deploy the
classifier at test time, you want it to run on your mobile phone or in a browser or some
other low power device, and you really want the
testing time performance of your classifier to be quite fast. So, from this perspective, this
nearest neighbor algorithm, is, actually, a little bit backwards. And we'll see that once we move to convolutional neural networks, and other types of parametric models, they'll be the reverse of this. Where you'll spend a lot of
compute at training time, but then they'll be quite
fast at testing time. So then, the question is, what exactly does this
nearest neighbor algorithm look like when you apply it in practice? So, here we've drawn, what
we call the decision regions of a nearest neighbor classifier. So, here our training set
consists of these points in the two dimensional plane, where the color of the point
represents the category, or the class label, of that point. So, here we see we have five classes and some blue ones up in the corner here, some purple ones in the
upper-right hand corner. And now for each pixel
in this entire plane, we've gone and computed
what is the nearest example in these training data, and then colored the
point of the background corresponding to what is the class label. So, you can see that this
nearest neighbor classifier is just sort of carving up the space and coloring the space
according to the nearby points. But this classifier is maybe not so great. And by looking at this picture we can start to see some of the
problems that might come out with a nearest neighbor classifier. For one, this central
region actually contains mostly green points, but one little yellow point in the middle. But because we're just looking
at the nearest neighbor, this causes a little
yellow island to appear in this middle of this green cluster. And that's, maybe, not so great. Maybe those points actually
should have been green. And then, similarly we also
see these, sort of, fingers, like the green region
pushing into the blue region, again, due to the presence of one point, which may have been noisy or spurious. So, this kind of motivates
a slight generalization of this algorithm called
k-nearest neighbors. So rather than just looking for
the single nearest neighbor, instead we'll do something
a little bit fancier and find K of our nearest neighbors, according to our distance metric, and then take a vote among
each of our neighbors. And then predict the majority vote among our neighbors. You can imagine slightly more
complex ways of doing this. Maybe you'd vote weighted on the distance, or something like that, but the simplest thing that
tends to work pretty well is just taking a majority vote. So here we've shown the
exact same set of points using this K=1 nearest
neighbor classifier, as well as K=3 and K=5 in
the middle and on the right. And once we move to K=3, you
can see that that spurious yellow point in the middle
of the green cluster is no longer causing the
points near that region to be classified as yellow. Now this entire green
portion in the middle is all being classified as green. You can also see that these fingers of the red and blue regions are starting to get smoothed out due to this majority voting. And then, once we move to the K=5 case, then these decision boundaries between the blue and red regions have become quite smooth and quite nice. So, generally when you're
using nearest neighbors classifiers, you almost always want
to use some value of K, which is larger than one because this tends to
smooth out your decision boundaries and lead to better results. Question? [student asking a question] Yes, so the question is, what is the deal with these white regions? The white regions are
where there was no majority among the k-nearest neighbors. You could imagine maybe doing
something slightly fancier and maybe taking a guess
or randomly selecting among the majority winners, but for this simple example
we're just coloring it white to indicate there was no nearest neighbor in those points. Whenever we're thinking
about computer vision I think it's really useful to kind of flip back and forth between
several different viewpoints. One, is this idea of high
dimensional points in the plane, and then the other is actually
looking at concrete images. Because the pixels of the image actually allow us to think of these
images as high dimensional vectors. And it's sort of useful to
ping pong back and forth between these two different viewpoints. So then, sort of taking
this k-nearest neighbor and going back to the images you can see that it's
actually not very good. Here I've colored in red and green which images would actually
be classified correctly or incorrectly according
to their nearest neighbor. And you can see that it's
really not very good. But maybe if we used a larger value of K then this would involve
actually voting among maybe the top three or the top five or maybe even the whole row. And you could imagine that
that would end up being a lot more robust to some
of this noise that we see when retrieving neighbors in this way. So another choice we
have when we're working with the k-nearest neighbor algorithm is determining exactly
how we should be comparing our different points. For the examples so far we've just shown we've talked about this L1 distance which takes the sum of the absolute values between the pixels. But another common choice is
the L2 or Euclidean distance where you take the square
root of the sum of the squares and take this as your distance. Choosing different
distance metrics actually is a pretty interesting topic because different distance metrics make different assumptions
about the underlying geometry or topology that
you'd expect in the space. So, this L1 distance, underneath
this, this is actually a circle according to the L1 distance and it forms this square shape thing around the origin. Where each of the points
on this, on the square, is equidistant from the
origin according to L1, whereas with the L2 or Euclidean distance then this circle is a familiar circle, it looks like what you'd expect. So one interesting thing to
point out between these two metrics in particular, is that the L1 distance
depends on your choice of coordinates system. So if you were to rotate
the coordinate frame that would actually change the L1 distance between the points. Whereas changing the coordinate
frame in the L2 distance doesn't matter, it's the
same thing no matter what your coordinate frame is. Maybe if your input features,
if the individual entries in your vector have some important meaning for your task, then maybe somehow L1 might
be a more natural fit. But if it's just a generic
vector in some space and you don't know which
of the different elements, you don't know what they actually mean, then maybe L2 is slightly more natural. And another point here is that by using different distance metrics we can actually generalize
the k-nearest neighbor classifier to many, many
different types of data, not just vectors, not just images. So, for example, imagine you
wanted to classify pieces of text, then the only
thing you need to do to use k-nearest neighbors is to specify some distance function that can measure distances
between maybe two paragraphs or two sentences or something like that. So, simply by specifying
different distance metrics we can actually apply this
algorithm very generally to basically any type of data. Even though it's a kind
of simple algorithm, in general, it's a very
good thing to try first when you're looking at a new problem. So then, it's also kind of
interesting to think about what is actually happening geometrically if we choose different distance metrics. So here we see the same
set of points on the left using the L1, or Manhattan distance, and then, on the right,
using the familiar L2, or Euclidean distance. And you can see that the
shapes of these decision boundaries actually change quite a bit between the two metrics. So when you're looking at
L1 these decision boundaries tend to follow the coordinate axes. And this is again because
the L1 depends on our choice of coordinate system. Where the L2 sort of doesn't
really care about the coordinate axis, it
just puts the boundaries where they should fall naturally. My confession is that
each of these examples that I've shown you is
actually from this interactive web demo that I built, where you can go and play
with this k-nearest neighbor classifier on your own. And this is really hard to
work on a projector screen. So maybe we'll do that on your own time. So, let's just go back to here. Man, this is kind of embarrassing. Okay, that was way more
trouble than it was worth. So, let's skip this, but I encourage you to go play with this in your browser. It's actually pretty fun and kind of nice to build intuition about how the decision boundary changes as you change the K and change your distance metric and all those sorts of things. Okay, so then the question is once you're actually trying
to use this algorithm in practice, there's several choices you need to make. We talked about choosing
different values of K. We talked about choosing
different distance metrics. And the question becomes how do you actually make
these choices for your problem and for your data? So, these choices, of things
like K and the distance metric, we call hyperparameters, because they are not necessarily
learned from the training data, instead these are choices about
your algorithm that you make ahead of time and there's no way to learn
them directly from the data. So, the question is how
do you set these things in practice? And they turn out to be
very problem-dependent. And the simple thing that
most people do is simply try different values of
hyperparameters for your data and for your problem, and
figure out which one works best. There's a question? [student asking a question] So, the question is, where L1
distance might be preferable to using L2 distance? I think it's mainly problem-dependent, it's sort of difficult to say in which cases you think
one might be better than the other. but I think that because L1
has this sort of coordinate dependency, it actually depends
on the coordinate system of your data, if you know that you have a vector, and maybe the individual
elements of the vector have meaning. Like maybe you're classifying
employees for some reason and then the different elements
of that vector correspond to different features or
aspects of an employee. Like their salary or the
number of years they've been working at the company
or something like that. So I think when your
individual elements actually have some meaning, is where I think maybe using
L1 might make a little bit more sense. But in general, again,
this is a hyperparameter and it really depends on
your problem and your data so the best answer is
just to try them both and see what works better. Even this idea of trying
out different values of hyperparameters and
seeing what works best, there are many different choices here. What exactly does it mean
to try hyperparameters and see what works best? Well, the first idea you might think of is simply choosing the
hyperparameters that give you the best accuracy or best performance on your training data. This is actually a really terrible idea. You should never do this. In the concrete case
of the nearest neighbor classifier, for example, if we set K=1, we will always
classify the training data perfectly. So if we use this strategy
we'll always pick K=1, but, as we saw from the examples earlier, in practice it seems that
setting K equals to larger values might cause us to misclassify
some of the training data, but, in fact, lead to better performance on points that were not
in the training data. And ultimately in machine learning we don't care about
fitting the training data, we really care about how our classifier, or how our method, will perform on unseen
data after training. So, this is a terrible
idea, don't do this. So, another idea that you might think of, is maybe we'll take our full dataset and we'll split it into some training data and some test data. And now I'll try training
my algorithm with different choices of hyperparameters
on the training data and then I'll go and apply
that trained classifier on the test data and now I will pick the set of hyperparameters
that cause me to perform best on the test data. This seems like maybe a
more reasonable strategy, but, in fact, this is also a terrible idea and you should never do this. Because, again, the point
of machine learning systems is that we want to know how
our algorithm will perform. So, the point of the test set is to give us some estimate
of how our method will do on unseen data that's
coming out from the wild. And if we use this strategy
of training many different algorithms with different hyperparameters, and then, selecting the
one which does the best on the test data, then, it's possible, that
we may have just picked the right set of hyperparameters that caused our algorithm
to work quite well on this testing set, but now our performance on this test set will no longer be representative of our performance of new, unseen data. So, again, you should not
do this, this is a bad idea, you'll get in trouble if you do this. What is much more common, is
to actually split your data into three different sets. You'll partition most of
your data into a training set and then you'll create a validation set and a test set. And now what we typically do
is go and train our algorithm with many different
choices of hyperparameters on the training set, evaluate on the validation set, and now pick the set of hyperparameters which performs best on the validation set. And now, after you've
done all your development, you've done all your debugging, after you've dome everything, then you'd take that best
performing classifier on the validation set and run it once on the test set. And now that's the number
that goes into your paper, that's the number that
goes into your report, that's the number that
actually is telling you how your algorithm is doing on unseen data. And this is actually
really, really important that you keep a very
strict separation between the validation data and the test data. So, for example, when we're
working on research papers, we typically only touch the test set at the very last minute. So, when I'm writing papers, I tend to only touch the
test set for my problem in maybe the week before
the deadline or so to really insure that we're not being dishonest here and
we're not reporting a number which is unfair. So, this is actually super important and you want to make sure
to keep your test data quite under control. So another strategy for
setting hyperparameters is called cross validation. And this is used a
little bit more commonly for small data sets, not used
so much in deep learning. So here the idea is we're
going to take our test data, or we're going to take our dataset, as usual, hold out some test
set to use at the very end, and now, for the rest of the data, rather than splitting it
into a single training and validation partition, instead, we can split our training data into many different folds. And now, in this way, we've
cycled through choosing which fold is going to be the validation set. So now, in this example, we're using five fold cross validation, so you would train your
algorithm with one set of hyperparameters on the first four folds, evaluate the performance on fold four, and now go and retrain
your algorithm on folds one, two, three, and five, evaluate on fold four, and cycle through all the different folds. And, when you do it this way, you get much higher confidence about which hyperparameters are going to perform more robustly. So this is kind of the
gold standard to use, but, in practice in deep learning when we're training large models and training is very
computationally expensive, these doesn't get used
too much in practice. Question? [student asking a question] Yeah, so the question is, a little bit more concretely, what's the difference
between the training and the validation set? So, if you think about the
k-nearest neighbor classifier then the training set is this
set of images with labels where we memorize the labels. And now, to classify an image, we're going to take the image
and compare it to each element in the training data, and then transfer the label
from the nearest training point. So now our algorithm
will memorize everything in the training set, and now we'll take each
element of the validation set and compare it to each
element in the training data and then use this to
determine what is the accuracy of our classifier when it's
applied on the validation set. So this is the distinction
between training and validation. Where your algorithm is
able to see the labels of the training set, but for the validation set, your algorithm doesn't have
direct access to the labels. We only use the labels
of the validation set to check how well our algorithm is doing. A question? [student asking a question] The question is, whether the test set, is it possible that the
test set might not be representative of data
out there in the wild? This definitely can be
a problem in practice, the underlying statistical
assumption here is that your data are all independently
and identically distributed, so that all of your data points should be drawn from the same underlying
probability distribution. Of course, in practice, this
might not always be the case, and you definitely can run into cases where the test set might
not be super representative of what you see in the wild. So this is kind of a problem
that dataset creators and dataset curators need to think about. But when I'm creating
datasets, for example, one thing I do, is I'll go and collect a whole
bunch of data all at once, using the exact same methodology
for collecting the data, and then afterwards you go
and partition it randomly between train and test. One thing that can screw you up here is maybe if you're collecting data over time and you make the earlier
data, that you collect first, be the training data, and the later data that you
collect be the test data, then you actually might
run into this shift that could cause problems. But as long as this partition is random among your entire set of data points, then that's how we try
to alleviate this problem in practice. So then, once you've gone through this cross validation procedure, then you end up with graphs
that look something like this. So here, on the X axis, we
are showing the value of K for a k-nearest neighbor
classifier on some problem, and now on the Y axis, we are
showing what is the accuracy of our classifier on some dataset for different values of K. And you can see that, in this case, we've done five fold cross
validation over the data, so, for each value of K we
have five different examples of how well this algorithm is doing. And, actually, going back
to the question about having some test sets
that are better or worse for your algorithm, using K fold cross validation is maybe one way to help
quantify that a little bit. And, in that, we can see the
variance of how this algorithm performs on different
of the validation folds. And that gives you some sense of, not just what is the best, but, also, what is the
distribution of that performance. So, whenever you're training
machine learning models you end up making plots like this, where they show you what is your accuracy, or your performance as a
function of your hyperparameters, and then you want to
go and pick the model, or the set of hyperparameters, at the end of the day, that performs the best
on the validation set. So, here we see that maybe
about K=7 probably works about best for this problem. So, k-nearest neighbor
classifiers on images are actually almost
never used in practice. Because, with all of these
problems that we've talked about. So, one problem is that
it's very slow at test time, which is the reverse of what we want, which we talked about earlier. Another problem is that these things like Euclidean
distance, or L1 distance, are really not a very good way to measure distances between images. These, sort of, vectorial
distance functions do not correspond very well
to perceptual similarity between images. How you perceive
differences between images. So, in this example, we've constructed, there's this image on the left of a girl, and then three different
distorted images on the right where we've blocked out her mouth, we've actually shifted
down by a couple pixels, or tinted the entire image blue. And, actually, if you compute
the Euclidean distance between the original and the boxed, the original and the shuffled, and original in the tinted, they all have the same L2 distance. Which is, maybe, not so good because it sort of
gives you the sense that the L2 distance is really
not doing a very good job at capturing these perceptional
distances between images. Another, sort of, problem
with the k-nearest neighbor classifier has to do with
something we call the curse of dimensionality. So, if you recall back this
viewpoint we had of the k-nearest neighbor classifier, it's sort of dropping paint
around each of the training data points and using that to
sort of partition the space. So that means that if we
expect the k-nearest neighbor classifier to work well, we kind of need our training
examples to cover the space quite densely. Otherwise our nearest neighbors
could actually be quite far away and might not actually
be very similar to our testing points. And the problem is, that actually densely covering the space, means that we need a number
of training examples, which is exponential in the
dimension of the problem. So this is very bad, exponential
growth is always bad, basically, you're never
going to get enough images to densely cover this space of pixels in this high dimensional space. So that's maybe another
thing to keep in mind when you're thinking about
using k-nearest neighbor. So, kind of the summary
is that we're using k-nearest neighbor to introduce this idea of image classification. We have a training set
of images and labels and then we use that to predict these labels on the test set. Question? [student asking a question] Oh, sorry, the question is, what was going on with this picture? What are the green and the blue dots? So here, we have some training samples which are represented by points, and the color of the dot
maybe represents the category of the point, of this training sample. So, if we're in one dimension, then you maybe only need
four training samples to densely cover the space, but if we move to two dimensions, then, we now need, four times
four is 16 training examples to densely cover this space. And if we move to three, four,
five, many more dimensions, the number of training
examples that we need to densely cover the space, grows exponentially with the dimension. So, this is kind of giving you the sense, that maybe in two dimensions we might have this kind
of funny curved shape, or you might have sort of
arbitrary manifolds of labels in different dimensional spaces. Because the k-nearest neighbor algorithm doesn't really make any
assumptions about these underlying manifolds, the only way it can perform properly is if it has quite a dense
sample of training points to work with. So, this is kind of the
overview of k-nearest neighbors and you'll get a chance
to actually implement this and try it out on images
in the first assignment. So, if there's any last minute
questions about K and N, I'm going to move on to the next topic. Question? [student is asking a question] Sorry, say that again. [student is asking a question] Yeah, so the question is, why do these images have
the same L2 distance? And the answer is that, I
carefully constructed them to have the same L2 distance. [laughing] But it's just giving you the
sense that the L2 distance is not a very good measure
of similarity between images. And these images are
actually all different from each other in quite disparate ways. If you're using K and N, then the only thing you
have to measure distance between images, is this single distance metric. And this kind of gives
you an example where that distance metric is
actually not capturing the full description of
distance or difference between images. So, if this case, I just sort
of carefully constructed these translations and these
offsets to match exactly. Question? [student asking a question] So, the question is, maybe this is actually good, because all of these things are actually having the
same distance to the image. That's maybe true for this example, but I think you could also
construct examples where maybe we have two original images and then by putting the
boxes in the right places or tinting them, we could cause it to be
nearer to pretty much anything that you want, right? Because in this example, we
can kind of like do arbitrary shifting and tinting to kind of change these
distances nearly arbitrarily without changing the perceptional
nature of these images. So, I think that this
can actually screw you up if you have many
different original images. Question? [student is asking a question] The question is, whether or not it's
common in real-world cases to go back and retrain the entire dataset once you've found those
best hyperparameters? So, people do sometimes
do this in practice, but it's somewhat a matter of taste. If you're really rushing for that deadline and you've really got to
get this model out the door then, if it takes a long
time to retrain the model on the whole dataset, then maybe you won't do it. But if you have a little
bit more time to spare and a little bit more compute to spare, and you want to squeeze out
that maybe that extra 1% of performance, then that
is a trick you can use. So we kind of saw that
the k-nearest neighbor has a lot of the nice properties of machine learning algorithms, but in practice it's not so great, and really not used very much in images. So the next thing I'd
like to talk about is linear classification. And linear classification is,
again, quite a simple learning algorithm, but this will
become super important and help us build up to
whole neural networks and whole convolutional networks. So, one analogy people often talk about when working with neural networks is we think of them as being
kind of like Lego blocks. That you can have different
kinds of components of neural networks and you
can stick these components together to build these
large different towers of convolutional networks. One of the most basic
building blocks that we'll see in different types of
deep learning applications is this linear classifier. So, I think it's actually
really important to have a good understanding
of what's happening with linear classification. Because these will end up
generalizing quite nicely to whole neural networks. So another example of kind
of this modular nature of neural networks comes from some research in our
own lab on image captioning, just as a little bit of a preview. So here the setup is that
we want to input an image and then output a descriptive sentence describing the image. And the way this kind of works is that we have one convolutional
neural network that's looking at the image, and a recurrent neural network that knows about language. And we can kind of just stick
these two pieces together like Lego blocks and train
the whole thing together and end up with a pretty cool system that can do some non-trivial things. And we'll work through the
details of this model as we go forward in the class, but this just gives you the sense that, these deep neural networks
are kind of like Legos and this linear classifier is kind of like the most
basic building blocks of these giant networks. But that's a little bit too
exciting for lecture two, so we have to go back to
CIFAR-10 for the moment. [laughing] So, recall that CIFAR-10 has
these 50,000 training examples, each image is 32 by 32 pixels
and three color channels. In linear classification,
we're going to take a bit of a different approach
from k-nearest neighbor. So, the linear classifier is
one of the simplest examples of what we call a parametric model. So now, our parametric model
actually has two different components. It's going to take in this image,
maybe, of a cat on the left, and this, that we usually write
as X for our input data, and also a set of parameters, or weights, which is usually called
W, also sometimes theta, depending on the literature. And now we're going to
write down some function which takes in both the data,
X, and the parameters, W, and this'll spit out now
10 numbers describing what are the scores
corresponding to each of those 10 categories in CIFAR-10. With the interpretation that,
like the larger score for cat, indicates a larger probability
of that input X being cat. And now, a question? [student asking a question] Sorry, can you repeat that? [student asking a question] Oh, so the question is what is the three? The three, in this example,
corresponds to the three color channels, red, green, and blue. Because we typically work on color images, that's nice information that
you don't want to throw away. So, in the k-nearest neighbor setup there was no parameters, instead, we just kind of keep around
the whole training data, the whole training set, and use that at test time. But now, in a parametric approach, we're going to summarize our
knowledge of the training data and stick all that knowledge
into these parameters, W. And now, at test time, we
no longer need the actual training data, we can throw it away. We only need these
parameters, W, at test time. So this allows our models
to now be more efficient and actually run on maybe
small devices like phones. So, kind of, the whole
story in deep learning is coming up with the
right structure for this function, F. You can imagine writing down
different functional forms for how to combine weights
and data in different complex ways, and these
could correspond to different network architectures. But the simplest possible example of combining these two things is just, maybe, to multiply them. And this is a linear classifier. So here our F of X, W is
just equal to the W times X. Probably the simplest
equation you can imagine. So here, if you kind of unpack the
dimensions of these things, we recall that our image was
maybe 32 by 32 by 3 values. So then, we're going to take
those values and then stretch them out into a long column vector that has 3,072 by one entries. And now we want to end
up with 10 class scores. We want to end up with
10 numbers for this image giving us the scores for
each of the 10 categories. Which means that now our matrix, W, needs to be ten by 3072. So that once we multiply
these two things out then we'll end up with
a single column vector 10 by one, giving us our 10 class scores. Also sometimes, you'll typically see this, we'll often add a bias term which will be a constant
vector of 10 elements that does not interact
with the training data, and instead just gives us
some sort of data independent preferences for some classes over another. So you might imagine that
if you're dataset was unbalanced and had many
more cats than dogs, for example, then the bias
elements corresponding to cat would be higher
than the other ones. So if you kind of think about pictorially what this function is doing, in this figure we have
an example on the left of a simple image with
just a two by two image, so it has four pixels total. So the way that the
linear classifier works is that we take this two by two image, we stretch it out into a column vector with four elements, and now, in this example,
we are just restricting to three classes, cat, dog, and ship, because you can't fit 10 on a slide, and now our weight matrix is
going to be four by three, so we have four pixels and three classes. And now, again, we have a
three element bias vector that gives us data independent bias terms for each category. Now we see that the cat score
is going to be the enter product between the pixels of our image and this row in the weight matrix added together with this bias term. So, when you look at it this way you can kind of understand
linear classification as almost a template matching approach. Where each of the rows in this matrix correspond to some template of the image. And now the enter product or dot product between the row of the
matrix and the column giving the pixels of the image, computing this dot
product kind of gives us a similarity between this
template for the class and the pixels of our image. And then bias just,
again, gives you this data independence scaling offset
to each of the classes. If we think about linear classification from this viewpoint of template matching we can actually take the
rows of that weight matrix and unravel them back into images and actually visualize
those templates as images. And this gives us some
sense of what a linear classifier might actually be doing to try to understand our data. So, in this example, we've
gone ahead and trained a linear classifier on our images. And now on the bottom we're visualizing what are those rows in
that learned weight matrix corresponding to each of the 10 categories in CIFAR-10. And in this way we kind
of get a sense for what's going on in these images. So, for example, in the
left, on the bottom left, we see the template for the plane class, kind of consists of this like blue blob, this kind of blobby thing in the middle and maybe blue in the background, which gives you the sense
that this linear classifier for plane is maybe looking for blue stuff and blobby stuff, and those
features are going to cause the classifier to like planes more. Or if we look at this car example, we kind of see that
there's a red blobby thing through the middle and a
blue blobby thing at the top that maybe is kind of a blurry windshield. But this is a little bit weird, this doesn't really look like a car. No individual car
actually looks like this. So the problem is that
the linear classifier is only learning one
template for each class. So if there's sort of
variations in how that class might appear, it's trying to average out all
those different variations, all those different appearances, and use just one single template to recognize each of those categories. We can also see this pretty
explicitly in the horse classifier. So in the horse classifier we
see green stuff on the bottom because horses are usually on grass. And then, if you look
carefully, the horse actually seems to have maybe two
heads, one head on each side. And I've never seen a
horse with two heads. But the linear classifier
is just doing the best that it can, because it's
only allowed to learn one template per category. And as we move forward
into neural networks and more complex models, we'll be able to achieve
much better accuracy because they no longer
have this restriction of just learning a single
template per category. Another viewpoint of the linear classifier is to go back to this idea of images as points and high dimensional space. And you can imagine
that each of our images is something like a point in
this high dimensional space. And now the linear classifier
is putting in these linear decision boundaries
to try to draw linear separation between one category and the rest of the categories. So maybe up on the upper-left hand side we see these training
examples of airplanes and throughout the process of training the linear classier will
go and try to draw this blue line to separate
out with a single line the airplane class from all
the rest of the classes. And it's actually kind of
fun if you watch during the training process these
lines will start out randomly and then go and snap into
place to try to separate the data properly. But when you think about
linear classification in this way, from this high
dimensional point of view, you can start to see again
what are some of the problems that might come up with
linear classification. And it's not too hard
to construct examples of datasets where a linear
classifier will totally fail. So, one example, on the left here, is that, suppose we have a
dataset of two categories, and these are all maybe
somewhat artificial, but maybe our dataset has two categories, blue and red. And the blue categories
are the number of pixels in the image, which are
greater than zero, is odd. And anything where the
number of pixels greater than zero is even, we want to
classify as the red category. So if you actually go and
draw what these different decisions regions look like in the plane, you can see that our blue class
with an odd number of pixels is going to be these two
quadrants in the plane, and even will be the
opposite two quadrants. So now, there's no way that we
can draw a single linear line to separate the blue from the red. So this would be an example
where a linear classifier would really struggle. And this is maybe not such an
artificial thing after all. Instead of counting pixels, maybe we're actually trying
to count whether the number of animals or people in
an image is odd or even. So this kind of a parity problem of separating odds from evens is something that linear classification really struggles with traditionally. Other situations where a linear
classifier really struggles are multimodal situations. So here on the right, maybe our blue category has
these three different islands of where the blue category lives, and then everything else
is some other category. So, for something like horses, we saw on the previous example, is something where this
actually might be happening in practice. Where there's maybe one
island in the pixel space of horses looking to the left, and another island of
horses looking to the right. And now there's no good
way to draw a single linear boundary between these two
isolated islands of data. So anytime where you have multimodal data, like one class that can appear in
different regions of space, is another place where linear
classifiers might struggle. So there's kind of a lot of problems with linear classifiers, but it
is a super simple algorithm, super nice and easy to interpret
and easy to understand. So you'll actually be
implementing these things on your first homework assignment. At this point, we kind of talked about what is the functional
form corresponding to a linear classifier. And we've seen that this functional form of matrix vector multiply corresponds this idea of template matching and learning a single
template for each category in your data. And then once we have this trained matrix you can use it to actually
go and get your scores for any new training example. But what we have not told you is how do you actually go
about choosing the right W for your dataset. We've just talked about
what is the functional form and what is going on with this thing. So that's something we'll
really focus on next time. And next lecture we'll talk about what are the strategies and algorithms for choosing the right W. And this will lead us to questions of loss functions and optimization and eventually ConvNets. So, that's a bit of the
preview for next week. And that's all we have for today. 

- Okay so welcome to
CS 231N Lecture three. Today we're going to talk about
loss functions and optimization but as usual, before we
get to the main content of the lecture, there's a
couple administrative things to talk about. So the first thing is that
assignment one has been released. You can find the link up on the website. And since we were a little bit late in getting this assignment
out to you guys, we've decided to change
the due date to Thursday, April 20th at 11:59 p.m., this will give you a full
two weeks from the assignment release date to go and
actually finish and work on it, so we'll update the syllabus
for this new due date in a little bit later today. And as a reminder, when you
complete the assignment, you should go turn in the
final zip file on Canvas so we can grade it and get
your grades back as quickly as possible. So the next thing is always
check out Piazza for interesting administrative stuff. So this week I wanted to
highlight that we have several example project ideas as
a pinned post on Piazza. So we went out and solicited
example of project ideas from various people in the
Stanford community or affiliated to Stanford, and they came
up with some interesting suggestions for projects
that they might want students in the class to work on. So check out this pinned post
on Piazza and if you want to work on any of these projects,
then feel free to contact the project mentors
directly about these things. Aditionally we posted office
hours on the course website, this is a Google calendar, so
this is something that people have been asking about
and now it's up there. The final administrative
note is about Google Cloud, as a reminder, because we're
supported by Google Cloud in this class, we're able to
give each of you an additional $100 credit for Google Cloud
to work on your assignments and projects, and the exact
details of how to redeem that credit will go out later
today, most likely on Piazza. So if there's, I guess if
there's no questions about administrative stuff then we'll
move on to course content. Okay cool. So recall from last time in lecture two, we were really talking about
the challenges of recognition and trying to hone in on this idea of a data-driven approach. We talked about this idea
of image classification, talked about why it's hard,
there's this semantic gap between the giant grid of
numbers that the computer sees and the actual image that you see. We talked about various
challenges regarding this around illumination,
deformation, et cetera, and why this is actually a
really, really hard problem even though it's super
easy for people to do with their human eyes
and human visual system. Then also recall last time
we talked about the k-nearest neighbor classifier as kind
of a simple introduction to this whole data-driven mindset. We talked about the CIFAR-10
data set where you can see an example of these images
on the upper left here, where CIFAR-10 gives you
these 10 different categories, airplane, automobile, whatnot, and we talked about how the
k-nearest neighbor classifier can be used to learn decision boundaries to separate these data points into classes based on the training data. This also led us to a
discussion of the idea of cross validation and setting
hyper parameters by dividing your data into train,
validation and test sets. Then also recall last time
we talked about linear classification as the first
sort of building block as we move toward neural networks. Recall that the linear
classifier is an example of a parametric classifier
where all of our knowledge about the training data gets summarized into this parameter matrix W that is set during the process of training. And this linear classifier
recall is super simple, where we're going to take
the image and stretch it out into a long vector. So here the image is x and
then we take that image which might be 32 by 32 by
3 pixels, stretch it out into a long column vector of 32 times 32 times 3 entries, where the 32 and 32 are
the height and width, and the 3 give you
the three color channels, red, green, blue. Then there exists some parameter matrix, W which will take this long column vector representing the image
pixels, and convert this and give you 10 numbers giving scores for each of the 10 classes
in the case of CIFAR-10. Where we kind of had this interpretation where larger values of those scores, so a larger value for the cat
class means the classifier thinks that the cat is
more likely for that image, and lower values for
maybe the dog or car class indicate lower probabilities
of those classes being present in the image. Also, so I think this point
was a little bit unclear last time that linear classification
has this interpretation as learning templates per class, where if you look at the
diagram on the lower left, you think that, so for
every pixel in the image, and for every one of our 10 classes, there exists some entry in this matrix W, telling us how much does that
pixel influence that class. So that means that each of
these rows in the matrix W ends up corresponding to
a template for the class. And if we take those rows and unravel, so each of those rows again corresponds to a weighting between the values of, between the pixel values of
the image and that class, so if we take that row and
unravel it back into an image, then we can visualize the
learned template for each of these classes. We also had this interpretation
of linear classification as learning linear decision
boundaries between pixels in some high dimensional
space where the dimensions of the space correspond
to the values of the pixel intensity values of the image. So this is kind of where
we left off last time. And so where we kind of
stopped, where we ended up last time is we got this idea
of a linear classifier, and we didn't talk about how
to actually choose the W. How to actually use the training data to determine which value
of W should be best. So kind of where we stopped off at is that for some setting
of W, we can use this W to come up with 10 with our
class scores for any image. So and some of these class
scores might be better or worse. So here in this simple example, we've shown maybe just a
training data set of three images along with the 10 class scores
predicted for some value of W for those images. And you can see that some
of these scores are better or worse than others. So for example in the image
on the left, if you look up, it's actually a cat because you're a human and you can tell these things, but if we look at the
assigned probabilities, cat, well not probabilities but scores, then the classifier maybe
for this setting of W gave the cat class a score
of 2.9 for this image, whereas the frog class gave 3.78. So maybe the classifier
is not doing not so good on this image, that's bad,
we wanted the true class to be actually the highest class score, whereas for some of these
other examples, like the car for example, you see
that the automobile class has a score of six which is much higher than any of the others, so that's good. And the frog, the predicted
scores are maybe negative four, which is much lower
than all the other ones, so that's actually bad. So this is kind of a hand wavy approach, just kind of looking at
the scores and eyeballing which ones are good
and which ones are bad. But to actually write
algorithms about these things and to actually to determine
automatically which W will be best, we need some
way to quantify the badness of any particular W. And that's this function
that takes in a W, looks at the scores and then
tells us how bad quantitatively is that W, is something that
we'll call a loss function. And in this lecture we'll
see a couple examples of different loss functions
that you can use for this image classification problem. So then once we've got this
idea of a loss function, this allows us to quantify
for any given value of W, how good or bad is it? But then we actually need to find and come up with an efficient procedure for searching through the
space of all possible Ws and actually come up with
what is the correct value of W that is the least bad, and this process will be
an optimization procedure and we'll talk more about
that in this lecture. So I'm going to shrink
this example a little bit because 10 classes is
a little bit unwieldy. So we'll kind of work with
this tiny toy data set of three examples and
three classes going forward in this lecture. So again, in this example, the
cat is maybe not so correctly classified, the car is correctly
classified, and the frog, this setting of W got this
frog image totally wrong, because the frog score is
much lower than others. So to formalize this a little
bit, usually when we talk about a loss function, we imagine that we have some training
data set of xs and ys, usually N examples of these
where the xs are the inputs to the algorithm in the
image classification case, the xs would be the actually
pixel values of your images, and the ys will be the things
you want your algorithm to predict, we usually call
these the labels or the targets. So in the case of image classification, remember we're trying
to categorize each image for CIFAR-10 to one of 10 categories, so the label y here will be an integer between one and 10 or
maybe between zero and nine depending on what programming
language you're using, but it'll be an integer telling you what is the correct category
for each one of those images x. And now our loss function
will denote L_i to denote the, so then we have this prediction function x which takes in our example
x and our weight matrix W and makes some prediction for y, in the case of image classification these will be our 10 numbers. Then we'll define some loss function L_i which will take in the predicted scores coming out of the function f together with the true target or label Y and give us some quantitative
value for how bad those predictions are for
that training example. And now the final loss L will
be the average of these losses summed over the entire data
set over each of the N examples in our data set. So this is actually a
very general formulation, and actually extends even
beyond image classification. Kind of as we move forward
and see other tasks, other examples of tasks and deep learning, the kind of generic setup
is that for any task you have some xs and ys
and you want to write down some loss function that
quantifies exactly how happy you are with your particular
parameter settings W and then you'll eventually
search over the space of W to find the W that minimizes
the loss on your training data. So as a first example of
a concrete loss function that is a nice thing to work
with in image classification, we'll talk about the multi-class SVM loss. You may have seen the binary
SVM, our support vector machine in CS 229 and the multiclass SVM is a generalization of that
to handle multiple classes. In the binary SVM case as
you may have seen in 229, you only had two classes, each example x was going to be classified
as either positive or negative example, but now we have 10 categories,
so we need to generalize this notion to handle multiple classes. So this loss function has kind
of a funny functional form, so we'll walk through it in a bit more, in quite a bit of detail over
the next couple of slides. But what this is saying
is that the loss L_i for any individual example,
the way we'll compute it is we're going to perform a sum
over all of the categories, Y, except for the true category, Y_i, so we're going to sum over
all the incorrect categories, and then we're going to compare the score of the correct category, and the score of the incorrect category, and now if the score
for the correct category is greater than the score
of the incorrect category, greater than the incorrect
score by some safety margin that we set to one, if that's
the case that means that the true score is much, or the
score for the true category is if it's much larger than
any of the false categories, then we'll get a loss of zero. And we'll sum this up over all
of the incorrect categories for our image and this
will give us our final loss for this one example in the data set. And again we'll take
the average of this loss over the whole training data set. So this kind of like if then
statement, like if the true class score is much
larger than the others, this kind of if then
formulation we often compactify into this single max of zero
S_j minus S_Yi plus one thing, but I always find that notation
a little bit confusing, and it always helps me to write it out in this
sort of case based notation to figure out exactly
what the two cases are and what's going on. And by the way, this
style of loss function where we take max of zero
and some other quantity is often referred to as
some type of a hinge loss, and this name comes from
the shape of the graph when you go and plot it, so here the x axis corresponds to the S_Yi, that is the score of the
true class for some training example, and now the y axis is the loss, and you can see that as the
score for the true category for this example increases, then the loss will go down linearly until we get to above this safety margin, after which the loss will be zero because we've already correctly
classified this example. So let's, oh, question? - [Student] Sorry, in terms of notation what is S underscore Yi? Is that your right score? - Yeah, so the question is in terms of notation,
what is S and what is SYI in particular, so the Ss
are the predicted scores for the classes that are
coming out of the classifier. So if one is the cat class and
two is the dog class then S1 and S2 would be the cat and
dog scores respectively. And remember we said that Yi
was the category of the ground truth label for the example
which is some integer. So then S sub Y sub i, sorry
for the double subscript, that corresponds to the
score of the true class for the i-th example in the training set. Question? - [Student] So what
exactly is this computing? - Yeah the question is what
exactly is this computing here? It's a little bit funny, I
think it will become more clear when we walk through an explicit
example, but in some sense what this loss is saying is
that we are happy if the true score is much higher than
all the other scores. It needs to be higher
than all the other scores by some safety margin,
and if the true score is not high enough, greater
than any of the other scores, then we will incur some
loss and that would be bad. So this might make a little bit more sense if we walk through an explicit example for this tiny three example data set. So here remember I've sort
of removed the case space notation and just switching
back to the zero one notation, and now if we look at, if we think about computing
this multi-class SVM loss for just this first training example on the left, then remember
we're going to loop over all of the incorrect
classes, so for this example, cat is the correct class, so
we're going to loop over the car and frog classes, and now for
car, we're going to compare the, we're going to look at the car
score, 5.1, minus the cat score, 3.2 plus one, when we're
comparing cat and car we expect to incur some loss here because
the car score is greater than the cat score which is bad. So for this one class,
for this one example, we'll incur a loss of 2.9, and then when we go and
compare the cat score and the frog score we see that cat is 3.2, frog is minus 1.7, so cat is more than one greater than frog, which means that between these two classes we incur zero loss. So then the multiclass SVM
loss for this training example will be the sum of the losses
across each of these pairs of classes, which will be
2.9 plus zero which is 2.9. Which is sort of saying that
2.9 is a quantitative measure of how much our classifier screwed up on this one training example. And then if we repeat this procedure for this next car image, then
again the true class is car, so we're going to iterate
over all the other categories when we compare the car and the cat score, we see that car is more
than one greater than cat so we get no loss here. When we compare car and frog, we again see that the car score is more
than one greater than frog, so we get again no loss
here, and our total loss for this training example is zero. And now I think you hopefully
get the picture by now, but, if you go look at frog, now
frog, we again compare frog and cat, incur quite a lot of
loss because the frog score is very low, compare frog
and car, incur a lot of loss because the score is very low,
and then our loss for this example is 12.9. And then our final loss
for the entire data set is the average of these losses across the different examples, so when you sum those out
it comes to about 5.3. So then it's sort of, this
is our quantitative measure that our classifier is
5.3 bad on this data set. Is there a question? - [Student] How do you
choose the plus one? - Yeah, the question is how
do you choose the plus one? That's actually a really great question, it seems like kind of an
arbitrary choice here, it's the only constant that
appears in the loss function and that seems to offend
your aesthetic sensibilities a bit maybe. But it turns out that this is somewhat of an arbitrary choice,
because we don't actually care about the absolute values of the scores in this loss function, we only care about the relative differences
between the scores. We only care that the correct score is much greater than the incorrect scores. So in fact if you imagine
scaling up your whole W up or down, then it kind
of rescales all the scores correspondingly and if you kind
of work through the details and there's a detailed derivation
of this in the course notes online, you find this choice
of one actually doesn't matter. That this free parameter
of one kind of washes out and is canceled with this scale, like the overall setting
of the scale in W. And again, check the course
notes for a bit more detail on that. So then I think it's
kind of useful to think about a couple different
questions to try to understand intuitively what this loss is doing. So the first question is what's
going to happen to the loss if we change the scores of the
car image just a little bit? Any ideas? Everyone's too scared to ask a question? Answer? [student speaking faintly] - Yeah, so the answer is
that if we jiggle the scores for this car image a little
bit, the loss will not change. So the SVM loss, remember,
the only thing it cares about is getting the correct
score to be greater than one more than the incorrect
scores, but in this case, the car score is already quite
a bit larger than the others, so if the scores for this
class changed for this example changed just a little
bit, this margin of one will still be retained and
the loss will not change, we'll still get zero loss. The next question, what's
the min and max possible loss for SVM? [student speaking faintly] Oh I hear some murmurs. So the minimum loss is zero,
because if you can imagine that across all the classes, if
our correct score was much larger then we'll incur zero
loss across all the classes and it will be zero, and if you think back to this
hinge loss plot that we had, then you can see that if the correct score goes very, very negative,
then we could incur potentially infinite loss. So the min is zero and
the max is infinity. Another question, sort of when
you initialize these things and start training from scratch, usually you kind of initialize W with some small random values,
as a result your scores tend to be sort of small
uniform random values at the beginning of training. And then the question is
that if all of your Ss, if all of the scores
are approximately zero and approximately equal, then what kind of loss do you expect when you're using multiclass SVM? - [Student] Number of classes minus one. - Yeah, so the answer is
number of classes minus one, because remember that
if we're looping over all of the incorrect classes,
so we're looping over C minus one classes, within
each of those classes the two Ss will be about the same, so we'll get a loss of one because of the margin and
we'll get C minus one. So this is actually kind
of useful because when you, this is a useful debugging strategy when you're using these things, that when you start off training, you should think about what
you expect your loss to be, and if the loss you actually
see at the start of training at that first iteration is
not equal to C minus one in this case, that means you probably have
a bug and you should go check your code, so this is actually
kind of a useful thing to be checking in practice. Another question, what happens
if, so I said we're summing an SVM over the incorrect
classes, what happens if the sum is also over the correct class if we just go over everything? - [Student] The loss increases by one. - Yeah, so the answer is that
the loss increases by one. And I think the reason
that we do this in practice is because normally loss of
zero is kind of, has this nice interpretation that
you're not losing at all, so that's nice, so I think your answers wouldn't really change, you would end up finding
the same classifier if you actually looped
over all the categories, but if just by conventions
we omit the correct class so that our minimum loss is zero. So another question, what if we used mean instead of sum here? - [Student] Doesn't change. - Yeah, the answer is
that it doesn't change. So the number of classes is
going to be fixed ahead of time when we select our data set,
so that's just rescaling the whole loss function by a constant, so it doesn't really matter,
it'll sort of wash out with all the other scale things because we don't actually
care about the true values of the scores, or the
true value of the loss for that matter. So now here's another
example, what if we change this loss formulation and we
actually added a square term on top of this max? Would this end up being the same problem or would this be a different
classification algorithm? - [Student] Different. - Yes, this would be different. So here the idea is that
we're kind of changing the trade-offs between good and badness in kind of a nonlinear way, so this would end up actually computing a different loss function. This idea of a squared hinge
loss actually does get used sometimes in practice, so
that's kind of another trick to have in your bag when you're making up your own loss functions
for your own problems. So now you'll end up,
oh, was there a question? - [Student] Why would
you use a squared loss instead of a non-squared loss? - Yeah, so the question is
why would you ever consider using a squared loss instead
of a non-squared loss? And the whole point of a loss function is to kind of quantify how
bad are different mistakes. And if the classifier is making
different sorts of mistakes, how do we weigh off the
different trade-offs between different types
of mistakes the classifier might make? So if you're using a squared loss, that sort of says that things
that are very, very bad are now going to be squared bad so that's like really, really bad, like we don't want anything that's totally
catastrophically misclassified, whereas if you're using this hinge loss, we don't actually care between
being a little bit wrong and being a lot wrong, being
a lot wrong kind of like, if an example is a lot
wrong, and we increase it and make it a little bit less wrong, that's kind of the same
goodness as an example which was only a little bit
wrong and then increasing it to be a little bit more right. So that's a little bit hand wavy, but this idea of using
a linear versus a square is a way to quantify how much we care about different categories of errors. And this is definitely something
that you should think about when you're actually applying
these things in practice, because the loss function is the way that you tell your algorithm
what types of errors you care about and what types of errors it should trade off against. So that's actually super
important in practice depending on your application. So here's just a little snippet
of sort of vectorized code in numpy, and you'll end up implementing something like this for
the first assignment, but this kind of gives you
the sense that this sum is actually like pretty easy to implement in numpy, it
only takes a couple lines of vectorized code. And you can see in practice,
like one nice trick is that we can actually go in
here and zero out the margins corresponding to the correct class, and that makes it easy to then just, that's sort of one nice
vectorized trick to skip, iterate over all but one class. You just kind of zero out
the one you want to skip and then compute the sum
anyway, so that's a nice trick you might consider
using on the assignment. So now, another question
about this loss function. Suppose that you were
lucky enough to find a W that has loss of zero,
you're not losing at all, you're totally winning, this loss function is crushing it, but then there's a
question, is this W unique or were there other Ws that could also have achieved zero loss? - [Student] There are other Ws. - Answer, yeah, so there
are definitely other Ws. And in particular, because
we talked a little bit about this thing of scaling
the whole problem up or down depending on W, so you
could actually take W multiplied by two and this
doubled W (Is it quad U now? I don't know.) [laughing] This would also achieve zero loss. So as a concrete example of this, you can go back to your favorite example and maybe work through the numbers a little bit later, but if you're taking W and we double W, then the margins between the
correct and incorrect scores will also double. So that means that if all these margins were already greater than
one, and we doubled them, they're still going to
be greater than one, so you'll still have zero loss. And this is kind of interesting, because if our loss function is the way that we tell our
classifier which W we want and which W we care about, this is a little bit weird, now there's this inconsistency and how is the classifier to choose between these different versions of W that all achieve zero loss? And that's because what we've done here is written down only a
loss in terms of the data, and we've only told our classifier that it should try to find the W that fits the training data. But really in practice,
we don't actually care that much about fitting the training data, the whole point of machine learning is that we use the training
data to find some classifier and then we'll apply
that thing on test data. So we don't really care about
the training data performance, we really care about the performance of this classifier on test data. So as a result, if the only thing we're telling our classifier to do is fit the training data, then we can lead ourselves into some of these weird
situations sometimes, where the classifier might
have unintuitive behavior. So a concrete, canonical example
of this sort of thing, by the way, this is not
linear classification anymore, this is a little bit of a more general machine learning concept, is that suppose we have this
data set of blue points, and we're going to fit some
curve to the training data, the blue points, then if the only thing we've
told our classifier to do is to try and fit the training data, it might go in and have very wiggly curves to try to perfectly classify all of the training data points. But this is bad, because
we don't actually care about this performance, we care about the
performance on the test data. So now if we have some new data come in that sort of follows the same trend, then this very wiggly blue line is going to be totally wrong. And in fact, what we
probably would have preferred the classifier to do was maybe predict this straight green line, rather than this very complex wiggly line to perfectly fit all the training data. And this is a core fundamental problem in machine learning, and the way we usually solve it, is this concept of regularization. So here we're going to
add an additional term to the loss function. In addition to the data loss, which will tell our
classifier that it should fit the training data,
we'll also typically add another term to the loss function called a regularization term, which encourages the model
to somehow pick a simpler W, where the concept of simple kind of depends on the task and the model. There's this whole idea of Occam's Razor, which is this fundamental
idea in scientific discovery more broadly, which is that
if you have many different competing hypotheses, that could explain your observations, you should generally
prefer the simpler one, because that's the explanation
that is more likely to generalize to new
observations in the future. And the way we
operationalize this intuition in machine learning is
typically through some explicit regularization penalty that's often written down as R. So then your standard loss function usually has these two terms, a data loss and a regularization loss, and there's some
hyper-parameter here, lambda, that trades off between the two. And we talked about hyper-parameters and cross-validation in the last lecture, so this regularization
hyper-parameter lambda will be one of the more important ones that you'll need to tune when training these models in practice. Question? - [Student] What does that lambda R W term have to do with [speaking faintly]. - Yeah, so the question is, what's the connection
between this lambda R W term and actually forcing this wiggly line to become a straight green line? I didn't want to go through
the derivation on this because I thought it would
lead us too far astray, but you can imagine, maybe you're doing a regression problem, in terms of different
polynomial basis functions, and if you're adding
this regression penalty, maybe the model has access to polynomials of very high degree, but
through this regression term you could encourage the
model to prefer polynomials of lower degree, if they
fit the data properly, or if they fit the data relatively well. So you could imagine
there's two ways to do this, either you can constrain your model class to just not contain the more powerful, more complex models, or you
can add this soft penalty where the model still has
access to more complex models, maybe high degree
polynomials in this case, but you add this soft constraint saying that if you want to
use these more complex models, you need to overcome this penalty for using their complexity. So that's the connection here, that is not quite linear classification, this is the picture that
many people have in mind when they think about
regularization at least. So there's actually a
lot of different types of regularization that
get used in practice. The most common one is
probably L2 regularization, or weight decay. But there's a lot of other
ones that you might see. This L2 regularization is
just the euclidean norm of this weight vector W, or sometimes the squared norm. Or sometimes half the squared norm because it makes your derivatives work out a little bit nicer. But the idea of L2 regularization is you're just penalizing
the euclidean norm of this weight vector. You might also sometimes
see L1 regularization, where we're penalizing the
L1 norm of the weight vector, and the L1 regularization
has some nice properties like encouraging sparsity
in this matrix W. Some other things you might see would be this elastic net regularization, which is some combination of L1 and L2. You sometimes see max norm regularization, penalizing the max norm
rather than the L1 or L2 norm. But these sorts of regularizations are things that you see
not just in deep learning, but across many areas of machine learning and even optimization more broadly. In some later lectures, we'll also see some types of regularization
that are more specific to deep learning. For example dropout, we'll
see in a couple lectures, or batch normalization, stochastic depth, these things get kind of
crazy in recent years. But the whole idea of regularization is just any thing that
you do to your model, that sort of penalizes somehow
the complexity of the model, rather than explicitly trying
to fit the training data. Question? [student speaking faintly] Yeah, so the question is, how does the L2 regularization
measure the complexity of the model? Thankfully we have an
example of that right here, maybe we can walk through. So here we maybe have
some training example, x, and there's two different
Ws that we're considering. So x is just this vector of four ones, and we're considering these
two different possibilities for W. One is a one in the
first, one is a single one and three zeros, and the other has this 0.25 spread across the four different entries. And now, when we're doing
linear classification, we're really taking dot products between our x and our W. So in terms of linear classification, these two Ws are the same, because they give the same result when dot producted with x. But now the question is, if you look at these two examples, which one would L2 regression prefer? Yeah, so L2 regression would prefer W2, because it has a smaller norm. So the answer is that the L2 regression measures complexity of the classifier in this relatively coarse way, where the idea is that, remember the Ws in linear classification had this interpretation of how much does this value of the vector x correspond to this output class? So L2 regularization is saying that it prefers to spread that influence across all the different values in x. Maybe this might be more robust, in case you come up with xs that vary, then our decisions are spread out and depend on the entire x vector, rather than depending
only on certain elements of the x vector. And by the way, L1 regularization has this opposite interpretation. So actually if we were
using L1 regularization, then we would actually prefer W1 over W2, because L1 regularization
has this different notion of complexity, saying that
maybe the model is less complex, maybe we measure model
complexity by the number of zeros in the weight vector, so the question of how
do we measure complexity and how does L2 measure complexity? They're kind of problem dependent. And you have to think about
for your particular setup, for your particular model and data, how do you think that
complexity should be measured on this task? Question? - [Student] So why would L1 prefer W1? Don't they sum to the same one? - Oh yes, you're right. So in this case, L1 is actually the same between these two. But you could construct
a similar example to this where W1 would be preferred
by L1 regularization. I guess the general intuition behind L1 is that it generally
prefers sparse solutions, that it drives all your
entries of W to zero for most of the entries,
except for a couple where it's allowed to deviate from zero. The way of measuring complexity for L1 is maybe the number of non-zero entries, and then for L2, it thinks
that things that spread the W across all the values are less complex. So it depends on your data,
depends on your problem. Oh and by the way, if
you're a hardcore Bayesian, then using L2 regularization
has this nice interpretation of MAP inference under a Gaussian prior on the parameter vector. I think there was a
homework problem about that in 229, but we won't talk about that for the rest of the quarter. That's sort of my long, deep dive into the multi-class SVM loss. Question? - [Student] Yeah, so I'm still confused about what the kind of stuff I need to do when the linear versus polynomial thing, because the use of this loss function isn't going to change the
fact that you're just doing, you're looking at a
linear classifier, right? - Yeah, so the question is that, adding a regularization is not going to change
the hypothesis class. This is not going to change us
away from a linear classifier. The idea is that maybe this example of this polynomial regression is definitely not linear regression. That could be seen as linear regression on top of a polynomial
expansion of the input, and in which case, this
regression sort of says that you're not allowed
to use as many polynomial coefficients as maybe you should have. Right, so you can imagine this is like, when you're doing polynomial regression, you can write out a polynomial as f of x equals A zero plus A one
x plus A two x squared plus A three x whatever, in that case your parameters, your Ws, would be these As, in which case, penalizing the W could force it towards lower degree polynomials. Except in the case of
polynomial regression, you don't actually want to parameterize in terms of As, there's
some other paramterization that you want to use, but that's the general idea, that you're sort of penalizing
the parameters of the model to force it towards the simpler hypotheses within your hypothesis class. And maybe we can take this offline if that's still a bit confusing. So then we've sort of seen
this multi-class SVM loss, and just by the way as a side note, this is one extension or
generalization of the SVM loss to multiple classes, there's actually a couple
different formulations that you can see around in literature, but I mean, my intuition is
that they all tend to work similarly in practice, at least in the context of deep learning. So we'll stick with this
one particular formulation of the multi-class SVM loss in this class. But of course there's many
different loss functions you might imagine. And another really popular choice, in addition to the multi-class SVM loss, another really popular
choice in deep learning is this multinomial logistic regression, or a softmax loss. And this one is probably
actually a bit more common in the context of deep learning, but I decided to present
this second for some reason. So remember in the context
of the multi-class SVM loss, we didn't actually have an interpretation for those scores. Remember, when we're
doing some classification, our model F, spits our these 10 numbers, which are our scores for the classes, and for the multi-class SVM, we didn't actually give much
interpretation to those scores. We just said that we want the true score, the score of the correct class to be greater than the incorrect classes, and beyond that we don't really
say what those scores mean. But now, for the multinomial
logistic regression loss function, we actually
will endow those scores with some additional meaning. And in particular we're
going to use those scores to compute a probability distribution over our classes. So we use this so-called softmax function where we take all of our scores, we exponentiate them so that
now they become positive, then we re-normalize them by
the sum of those exponents so now after we send our scores through this softmax function, now we end up with this
probability distribution, where now we have
probabilities over our classes, where each probability
is between zero and one, and the sum of probabilities
across all classes sum to one. And now the interpretation
is that we want, there's this computed
probability distribution that's implied by our scores, and we want to compare
this with the target or true probability distribution. So if we know that the thing is a cat, then the target probability distribution would put all of the
probability mass on cat, so we would have probability
of cat equals one, and zero probability for
all the other classes. So now what we want to do is encourage our computed probability distribution that's coming out of this softmax function to match this target
probability distribution that has all the mass
on the correct class. And the way that we do this, I mean, you can do this
equation in many ways, you can do this as a KL divergence between the target and the computed probability distribution, you can do this as a
maximum likelihood estimate, but at the end of the day, what we really want is
that the probability of the true class is
high and as close to one. So then our loss will
now be the negative log of the probability of the true class. This is confusing 'cause
we're putting this through multiple different things, but remember we wanted the probability to be close to one, so now log is a monotonic
function, it goes like this, and it turns out mathematically, it's easier to maximize log than it is to maximize
the raw probability, so we stick with log. And now log is monotonic, so if we maximize log P of correct class, that means we want that to be high, but loss functions measure
badness not goodness so we need to put in the minus one to make it go the right way. So now our loss function for SVM is going to be the minus
log of the probability of the true class. Yeah, so that's the summary here, is that we take our scores,
we run through the softmax, and now our loss is this
minus log of the probability of the true class. Okay, so then if you look
at what this looks like on a concrete example, then we go back to our
favorite beautiful cat with our three examples and
we've got these three scores that are coming out of
our linear classifier, and these scores are exactly
the way that they were in the context of the SVM loss. But now, rather than taking these scores and putting them directly
into our loss function, we're going to take them
all and exponentiate them so that they're all positive, and then we'll normalize them to make sure that they all sum to one. And now our loss will be the minus log of the true class score. So that's the softmax loss, also called multinomial
logistic regression. So now we asked several questions to try to gain intuition about
the multi-class SVM loss, and it's useful to think about
some of the same questions to contrast with the softmax loss. So then the question is, what's the min and max
value of the softmax loss? Okay, maybe not so sure, there's too many logs and sums and stuff going on in here. So the answer is that the min loss is zero and the max loss is infinity. And the way that you can see this, the probability distribution that we want is one on the correct class,
zero on the incorrect classes, the way that we do that is, so if that were the case, then this thing inside the
log would end up being one, because it's the log
probability of the true class, then log of one is zero, minus
log of one is still zero. So that means that if we
got the thing totally right, then our loss would be zero. But by the way, in order to
get the thing totally right, what would our scores have to look like? Murmuring, murmuring. So the scores would actually
have to go quite extreme, like towards infinity. So because we actually
have this exponentiation, this normalization, the only way we can actually get a
probability distribution of one and zero, is actually
putting an infinite score for the correct class,
and minus infinity score for all the incorrect classes. And computers don't do
so well with infinities, so in practice, you'll
never get to zero loss on this thing with finite precision. But you still have this interpretation that zero is the theoretical
minimum loss here. And the maximum loss is unbounded. So suppose that if we
had zero probability mass on the correct class, then
you would have minus log of zero, log of zero is minus infinity, so minus log of zero
would be plus infinity, so that's really bad. But again, you'll never really get here because the only way you can
actually get this probability to be zero, is if e to the
correct class score is zero, and that can only happen
if that correct class score is minus infinity. So again, you'll never
actually get to these minimum, maximum values with finite precision. So then, remember we had this debugging, sanity check question in the
context of the multi-class SVM, and we can ask the same for the softmax. If all the Ss are small and about zero, then what is the loss here? Yeah, answer? - [Student] Minus log one over C. - So minus log of one over C? I think that's, yeah, so then it'd be minus log of one over C, because log can flip the thing so then it's just log of C. Yeah, so it's just log of C. And again, this is a nice debugging thing, if you're training a model
with this softmax loss, you should check at the first iteration. If it's not log C, then
something's gone wrong. So then we can compare and
contrast these two loss functions a bit. In terms of linear classification, this setup looks the same. We've got this W matrix
that gets multiplied against our input to produce
this specter of scores, and now the difference
between the two loss functions is how we choose to interpret those scores to quantitatively measure
the badness afterwards. So for SVM, we were going to
go in and look at the margins between the scores of the correct class and the scores of the incorrect class, whereas for this softmax
or cross-entropy loss, we're going to go and compute
a probability distribution and then look at the minus log probability of the correct class. So sometimes if you look at, in terms of, nevermind,
I'll skip that point. [laughing] So another question that's interesting when contrasting these two
loss functions is thinking, suppose that I've got this example point, and if you change its scores, so assume that we've got
three scores for this, ignore the part on the bottom. But remember if we go back to this example where in the multi-class SVM loss, when we had the car, and the
car score was much better than all the incorrect classes, then jiggling the scores
for that car image didn't change the
multi-class SVM loss at all, because the only thing that the SVM loss cared about was getting that correct score to be greater than a margin
above the incorrect scores. But now the softmax loss
is actually quite different in this respect. The softmax loss actually
always wants to drive that probability mass all the way to one. So even if you're giving very high score to the correct class, and very low score to all the incorrect classes, softmax will want you to pile
more and more probability mass on the correct class, and
continue to push the score of that correct class up towards infinity, and the score of the incorrect classes down towards minus infinity. So that's the interesting difference between these two loss
functions in practice. That SVM, it'll get this
data point over the bar to be correctly classified
and then just give up, it doesn't care about
that data point any more. Whereas softmax will just always
try to continually improve every single data point
to get better and better and better and better. So that's an interesting difference between these two functions. In practice, I think it tends
not to make a huge difference which one you choose, they tend to perform pretty similarly across, at least a lot of deep
learning applications. But it is very useful to keep
some of these differences in mind. Yeah, so to recap where
we've come to from here, is that we've got some
data set of xs and ys, we use our linear classifier
to get some score function, to compute our scores
S, from our inputs, x, and then we'll use a loss function, maybe softmax or SVM or
some other loss function to compute how quantitatively
bad were our predictions compared to this ground true targets, y. And then we'll often
augment this loss function with a regularization term, that tries to trade off between
fitting the training data and preferring simpler models. So this is a pretty generic overview of a lot of what we call
supervised learning, and what we'll see in deep
learning as we move forward, is that generally you'll want
to specify some function, f, that could be very complex in structure, specify some loss function that determines how well your algorithm is doing, given any value of the parameters, some regularization term for how to penalize model complexity and then you combine these things together and try to find the W that minimizes this final loss function. But then the question is, how do we actually go about doing that? How do we actually find this
W that minimizes the loss? And that leads us to the
topic of optimization. So when we're doing optimization, I usually think of things
in terms of walking around some large valley. So the idea is that you're
walking around this large valley with different mountains
and valleys and streams and stuff, and every
point on this landscape corresponds to some setting
of the parameters W. And you're this little guy who's
walking around this valley, and you're trying to find, and the height of each of these points, sorry, is equal to the loss
incurred by that setting of W. And now your job as this little man walking around this landscape, you need to somehow find
the bottom of this valley. And this is kind of a
hard problem in general. You might think, maybe I'm really smart and I can think really hard
about the analytic properties of my loss function, my
regularization all that, maybe I can just write down the minimizer, and that would sort of correspond
to magically teleporting all the way to the bottom of this valley. But in practice, once your
prediction function, f, and your loss function
and your regularizer, once these things get big and complex and using neural networks, there's really not much
hope in trying to write down an explicit analytic solution that takes you directly to the minima. So in practice we tend to use various
types of iterative methods where we start with some solution and then gradually improve it over time. So the very first, stupidest thing that you might imagine is random search, that will just take a bunch of Ws, sampled randomly, and throw
them into our loss function and see how well they do. So spoiler alert, this is
a really bad algorithm, you probably shouldn't use this, but at least it's one thing
you might imagine trying. And we can actually do this, we can actually try to
train a linear classifier via random search, for CIFAR-10 and for this there's 10 classes, so random chance is 10%, and if we did some
number of random trials, we eventually found just
through sheer dumb luck, some setting of W that
got maybe 15% accuracy. So it's better than random, but state of the art is maybe 95% so we've got a little
bit of gap to close here. So again, probably don't
use this in practice, but you might imagine
that this is something you could potentially do. So in practice, maybe a better strategy is actually using some
of the local geometry of this landscape. So if you're this little guy who's walking around this landscape, maybe you can't see directly the path down to the bottom of the valley, but what you can do is feel with your foot and figure out what is the local geometry, if I'm standing right here, which way will take me
a little bit downhill? So you can feel with your feet and feel where is the slope of the ground taking me down a little
bit in this direction? And you can take a step in that direction, and then you'll go down a little bit, feel again with your feet to
figure out which way is down, and then repeat over and over again and hope that you'll end up at the bottom of the valley eventually. So this also seems like a
relatively simple algorithm, but actually this one
tends to work really well in practice if you get
all the details right. So this is generally the
strategy that we'll follow when training these large neural networks and linear classifiers and other things. So then, that was a little hand wavy, so what is slope? If you remember back
to your calculus class, then at least in one dimension, the slope is the derivative
of this function. So if we've got some
one-dimensional function, f, that takes in a scalar x,
and then outputs the height of some curve, then we
can compute the slope or derivative at any point by imagining, if we take a small step,
h, in any direction, take a small step, h, and
compare the difference in the function value over that step and then drag the step size to zero, that will give us the
slope of that function at that point. And this generalizes quite naturally to multi-variable functions as well. So in practice, our x
is maybe not a scalar but a whole vector, 'cause remember, x
might be a whole vector, so we need to generalize this notion to multi-variable things. And the generalization that
we use of the derivative in the multi-variable
setting is the gradient, so the gradient is a vector
of partial derivatives. So the gradient will
have the same shape as x, and each element of the
gradient will tell us what is the slope of the function f, if we move in that coordinate direction. And the gradient turns out to have these very nice properties, so the gradient is now a
vector of partial derivatives, but it points in the
direction of greatest increase of the function and correspondingly, if you look at the negative
gradient direction, that gives you the direction
of greatest decrease of the function. And more generally, if you want to know, what is the slope of my
landscape in any direction? Then that's equal to the
dot product of the gradient with the unit vector
describing that direction. So this gradient is super important, because it gives you this
linear, first-order approximation to your function at your current point. So in practice, a lot of deep learning is about computing
gradients of your functions and then using those gradients
to iteratively update your parameter vector. So one naive way that you might imagine actually evaluating this
gradient on a computer, is using the method of finite differences, going back to the limit
definition of gradient. So here on the left, we
imagine that our current W is this parameter vector that maybe gives us some
current loss of maybe 1.25 and our goal is to
compute the gradient, dW, which will be a vector
of the same shape as W, and each slot in that
gradient will tell us how much will the loss
change is we move a tiny, infinitesimal amount in
that coordinate direction. So one thing you might imagine is just computing these
finite differences, that we have our W, we
might try to increment the first element of
W by a small value, h, and then re-compute the
loss using our loss function and our classifier and all that. And maybe in this setting,
if we move a little bit in the first dimension,
then our loss will decrease a little bit from 1.2534 to 1.25322. And then we can use this limit definition to come up with this finite
differences approximation to the gradient in this first dimension. And now you can imagine
repeating this procedure in the second dimension, where now we take the first dimension, set it back to the original value, and now increment the second
direction by a small step. And again, we compute the loss and use this finite
differences approximation to compute an approximation
to the gradient in the second slot. And now repeat this again for the third, and on and on and on. So this is actually a terrible idea because it's super slow. So you might imagine that
computing this function, f, might actually be super
slow if it's a large, convolutional neural network. And this parameter vector, W, probably will not have 10
entries like it does here, it might have tens of millions or even hundreds of millions
for some of these large, complex deep learning models. So in practice, you'll
never want to compute your gradients for your
finite differences, 'cause you'd have to wait
for hundreds of millions potentially of function evaluations to get a single gradient,
and that would be super slow and super bad. But thankfully we don't have to do that. Hopefully you took a calculus course at some point in your lives, so you know that thanks to these guys, we can just write down the
expression for our loss and then use the magical
hammer of calculus to just write down an expression for what this gradient should be. And this'll be way more efficient than trying to compute it analytically via finite differences. One, it'll be exact, and two, it'll be much faster
since we just need to compute this single expression. So what this would look like is now, if we go back to this
picture of our current W, rather than iterating over
all the dimensions of W, we'll figure out ahead of time what is the analytic
expression for the gradient, and then just write it down
and go directly from the W and compute the dW or
the gradient in one step. And that will be much better in practice. So in summary, this numerical gradient is something that's
simple and makes sense, but you won't really use it in practice. In practice, you'll always
take an analytic gradient and use that when actually performing
these gradient computations. However, one interesting note is that these numeric gradients
are actually a very useful debugging tool. Say you've written some code, and you wrote some code
that computes the loss and the gradient of the loss, then how do you debug this thing? How do you make sure that
this analytic expression that you derived and wrote down in code is actually correct? So a really common debugging
strategy for these things is to use the numeric gradient as a way, as sort of a unit test to make sure that your analytic gradient was correct. Again, because this is
super slow and inexact, then when doing this
numeric gradient checking, as it's called, you'll tend
to scale down the parameter of the problem so that it actually runs in a reasonable amount of time. But this ends up being a super
useful debugging strategy when you're writing your
own gradient computations. So this is actually very
commonly used in practice, and you'll do this on
your assignments as well. So then once we know how
to compute the gradient, then it leads us to this
super simple algorithm that's like three lines, but
turns out to be at the heart of how we train even these very biggest, most complex deep learning algorithms, and that's gradient descent. So gradient descent is
first we initialize our W as some random thing, then while true, we'll compute our loss and our gradient and then we'll update our weights in the opposite of the gradient direction, 'cause remember that the gradient was pointing in the direction
of greatest increase of the function, so minus gradient points in the direction
of greatest decrease, so we'll take a small
step in the direction of minus gradient, and
just repeat this forever and eventually your network will converge and you'll be very happy, hopefully. But this step size is
actually a hyper-parameter, and this tells us that every
time we compute the gradient, how far do we step in that direction. And this step size, also
sometimes called a learning rate, is probably one of the
single most important hyper-parameters that you need to set when you're actually training
these things in practice. Actually for me when I'm
training these things, trying to figure out this step size or this learning rate, is
the first hyper-parameter that I always check. Things like model size or
regularization strength I leave until a little bit later, and getting the learning
rate or the step size correct is the first thing that I
try to set at the beginning. So pictorially what this looks like here's a simple example in two dimensions. So here we've got maybe this bowl that's showing our loss function where this red region in the center is this region of low
loss we want to get to and these blue and green
regions towards the edge are higher loss that we want to avoid. So now we're going to start of our W at some random point in the space, and then we'll compute the
negative gradient direction, which will hopefully
point us in the direction of the minima eventually. And if we repeat this over and over again, we'll hopefully eventually
get to the exact minima. And what this looks like in practice is, oh man, we've got this
mouse problem again. So what this looks like in practice is that if we repeat this
thing over and over again, then we will start off at some point and eventually, taking tiny
gradient steps each time, you'll see that the parameter
will arc in toward the center, this region of minima, and that's really what you want, because you want to get to low loss. And by the way, as a bit of a teaser, we saw in the previous slide, this example of very
simple gradient descent, where at every step, we're
just stepping in the direction of the gradient. But in practice, over the
next couple of lectures, we'll see that there are
slightly fancier step, what they call these update rules, where you can take slightly fancier things to incorporate gradients
across multiple time steps and stuff like that, that tend
to work a little bit better in practice and are
used much more commonly than this vanilla gradient descent when training these things in practice. And then, as a bit of a preview, we can look at some of these
slightly fancier methods on optimizing the same problem. So again, the black will be
this same gradient computation, and these, I forgot which color they are, but these two other curves are using slightly fancier update rules to decide exactly how to
use the gradient information to make our next step. So one of these is gradient
descent with momentum, the other is this Adam optimizer, and we'll see more details about those later in the course. But the idea is that we have
this very basic algorithm called gradient descent, where we use the gradient
at every time step to determine where to step next, and there exist different
update rules which tell us how exactly do we use
that gradient information. But it's all the same basic algorithm of trying to go downhill
at every time step. But there's actually
one more little wrinkle that we should talk about. So remember that we
defined our loss function, we defined a loss that computes how bad is our classifier doing at
any single training example, and then we said that our
full loss over the data set was going to be the average loss across the entire training set. But in practice, this N
could be very very large. If we're using the image
net data set for example, that we talked about in the first lecture, then N could be like 1.3 million, so actually computing this loss could be actually very expensive and require computing perhaps
millions of evaluations of this function. So that could be really slow. And actually, because the
gradient is a linear operator, when you actually try
to compute the gradient of this expression, you see
that the gradient of our loss is now the sum of the
gradient of the losses for each of the individual terms. So now if we want to
compute the gradient again, it sort of requires us to iterate over the entire training data set all N of these examples. So if our N was like a million, this would be super super slow, and we would have to wait
a very very long time before we make any individual update to W. So in practice, we tend to use what is called stochastic
gradient descent, where rather than computing
the loss and gradient over the entire training set, instead at every iteration,
we sample some small set of training examples, called a minibatch. Usually this is a power
of two by convention, like 32, 64, 128 are common numbers, and then we'll use this small minibatch to compute an estimate of the full sum, and an estimate of the true gradient. And now this is stochastic
because you can view this as maybe a Monte Carlo
estimate of some expectation of the true value. So now this makes our
algorithm slightly fancier, but it's still only four lines. So now it's well true, sample
some random minibatch of data, evaluate your loss and
gradient on the minibatch, and now make an update on your parameters based on this estimate of the loss, and this estimate of the gradient. And again, we'll see
slightly fancier update rules of exactly how to integrate
multiple gradients over time, but this is the
basic training algorithm that we use for pretty much
all deep neural networks in practice. So we have another interactive web demo actually playing around
with linear classifiers, and training these things via
stochastic gradient descent, but given how miserable
the web demo was last time, I'm not actually going to open the link. Instead, I'll just play this video. [laughing] But I encourage you to go check this out and play with it online, because it actually helps
to build some intuition about linear classifiers and training them via gradient descent. So here you can see on the left, we've got this problem
where we're categorizing three different classes, and we've got these
green, blue and red points that are our training samples
from these three classes. And now we've drawn
the decision boundaries for these classes, which are
the colored background regions, as well as these directions, giving you the direction of
increase for the class scores for each of these three classes. And now if you see, if
you actually go and play with this thing online, you can see that we can
go in and adjust the Ws and changing the values of the Ws will cause these decision
boundaries to rotate. If you change the biases,
then the decision boundaries will not rotate, but will
instead move side to side or up and down. Then we can actually make steps that are trying to update this loss, or you can change the step
size with this slider. You can hit this button
to actually run the thing. So now with a big step size, we're running gradient descent right now, and these decision boundaries
are flipping around and trying to fit the data. So it's doing okay now, but we can actually change
the loss function in real time between these different SVM formulations and the different softmax. And you can see that as you flip between these different
formulations of loss functions, it's generally doing the same thing. Our decision regions are
mostly in the same place, but exactly how they end
up relative to each other and exactly what the trade-offs are between categorizing
these different things changes a little bit. So I really encourage you to go online and play with this thing to
try to get some intuition for what it actually looks like to try to train these linear classifiers via gradient descent. Now as an aside, I'd like
to talk about another idea, which is that of image features. So so far we've talked
about linear classifiers, which is just maybe taking
our raw image pixels and then feeding the raw pixels themselves into our linear classifier. But as we talked about
in the last lecture, this is maybe not such
a great thing to do, because of things like
multi-modality and whatnot. So in practice, actually
feeding raw pixel values into linear classifiers
tends to not work so well. So it was actually common
before the dominance of deep neural networks, was instead to have
this two-stage approach, where first, you would take your image and then compute various
feature representations of that image, that are maybe computing different kinds of quantities
relating to the appearance of the image, and then concatenate these
different feature vectors to give you some feature
representation of the image, and now this feature
representation of the image would be fed into a linear classifier, rather than feeding the
raw pixels themselves into the classifier. And the motivation here is that, so imagine we have a
training data set on the left of these red points, and
red points in the middle and blue points around that. And for this kind of data set, there's no way that we can
draw a linear decision boundary to separate the red points
from the blue points. And we saw more examples of
this in the last lecture. But if we use a clever feature transform, in this case transforming
to polar coordinates, then now after we do
the feature transform, then this complex data
set actually might become linearly separable, and actually could be classified correctly by a linear classifier. And the whole trick here
now is to figure out what is the right feature transform that is computing the right quantities for the problem that you care about. So for images, maybe
converting your pixels to polar coordinates, doesn't make sense, but you actually can try to write down feature representations of images that might make sense, and actually might help you out and might do better than
putting in raw pixels into the classifier. So one example of this kind
of feature representation that's super simple, is this
idea of a color histogram. So you'll take maybe each pixel, you'll take this hue color spectrum and divide it into buckets
and then for every pixel, you'll map it into one
of those color buckets and then count up how many pixels fall into each of these different buckets. So this tells you globally
what colors are in the image. Maybe if this example of a frog, this feature vector would tell us there's a lot of green stuff, and maybe not a lot of
purple or red stuff. And this is kind of a simple feature
vector that you might see in practice. Another common feature vector that we saw before the rise of neural networks, or before the dominance of neural networks was this histogram of oriented gradients. So remember from the first lecture, that Hubel and Wiesel
found these oriented edges are really important in
the human visual system, and this histogram of oriented gradients feature representation tries to capture the same intuition and
measure the local orientation of edges on the image. So what this thing is going to do, is take our image and then divide it into these little eight
by eight pixel regions. And then within each of those
eight by eight pixel regions, we'll compute what is the
dominant edge direction of each pixel, quantize
those edge directions into several buckets and then
within each of those regions, compute a histogram over these
different edge orientations. And now your full-feature vector will be these different
bucketed histograms of edge orientations across all the different
eight by eight regions in the image. So this is in some sense dual to the color histogram
classifier that we saw before. So color histogram is
saying, globally, what colors exist in the image, and this is saying, overall,
what types of edge information exist in the image. And even localized to
different parts of the image, what types of edges exist
in different regions. So maybe for this frog on the left, you can see he's sitting on a leaf, and these leaves have these
dominant diagonal edges, and if you visualize the
histogram of oriented gradient features, then you can
see that in this region, we've got a lot of diagonal edges, that this histogram of oriented gradient feature representation's capturing. So this was a super common
feature representation and was used a lot for object recognition actually not too long ago. Another feature representation
that you might see out there is this idea of bag of words. So this is taking inspiration from natural language processing. So if you've got a paragraph, then a way that you might
represent a paragraph by a feature vector is
counting up the occurrences of different words in that paragraph. So we want to take that
intuition and apply it to images in some way. But the problem is that
there's no really simple, straightforward analogy
of words to images, so we need to define our own vocabulary of visual words. So we take this two-stage approach, where first we'll get a bunch of images, sample a whole bunch of tiny random crops from those images and then cluster them using something like K means to come up with these
different cluster centers that are maybe representing
different types of visual words in the images. So if you look at this
example on the right here, this is a real example of clustering actually different image
patches from images, and you can see that after
this clustering step, our visual words capture
these different colors, like red and blue and yellow, as well as these different
types of oriented edges in different directions, which is interesting that
now we're starting to see these oriented edges
come out from the data in a data-driven way. And now, once we've got
these set of visual words, also called a codebook, then we can encode our
image by trying to say, for each of these visual words, how much does this visual
word occur in the image? And now this gives us, again, some slightly different information about what is the visual
appearance of this image. And actually this is a type
of feature representation that Fei-Fei worked on when
she was a grad student, so this is something
that you saw in practice not too long ago. So then as a bit of teaser, tying this all back together, the way that this image
classification pipeline might have looked like, maybe about five to 10 years ago, would be that you would take your image, and then compute these different
feature representations of your image, things like bag of words, or histogram of orientated gradients, concatenate a whole bunch
of features together, and then feed these feature extractors down into some linear classifier. I'm simplifying a little bit, the pipelines were a little
bit more complex than that, but this is the general intuition. And then the idea here was
that after you extracted these features, this feature extractor would be a fixed block
that would not be updated during training. And during training, you would only update
the linear classifier if it's working on top of features. And actually, I would
argue that once we move to convolutional neural networks, and these deep neural networks, it actually doesn't look that different. The only difference is that
rather than writing down the features ahead of time, we're going to learn the
features directly from the data. So we'll take our raw pixels and feed them into this to convolutional network, which will end up computing
through many different layers some type of feature representation driven by the data, and
then we'll actually train this entire weights for
this entire network, rather than just the
weights of linear classifier on top. So, next time we'll really
start diving into this idea in more detail, and we'll
introduce some neural networks, and start talking about
backpropagation as well. 

[students murmuring] - Okay, so good afternoon
everyone, let's get started. So hi, so for those of
you who I haven't met yet, my name is Serena Yeung and I'm the third and final instructor for this class, and I'm also a PhD student
in Fei-Fei's group. Okay, so today we're going
to talk about backpropagation and neural networks, and so
now we're really starting to get to some of the core
material in this class. Before we begin, let's see, oh. So a few administrative details, so assignment one is due
Thursday, April 20th, so a reminder, we shifted
the date back by a little bit and it's going to be due
11:59 p.m. on Canvas. So you should start thinking
about your projects, there are TA specialties
listed on the Piazza website so if you have questions
about a specific project topic you're thinking about, you
can go and try and find the TAs that might be most relevant. And then also for Google Cloud,
so all students are going to get $100 in credits
to use for Google Cloud for their assignments and project, so you should be receiving an email for that this week, I think. A lot of you may have already, and then for those of you who haven't,
they're going to come, should be by the end of this week. Okay so where we are, so
far we've talked about how to define a classifier
using a function f, parameterized by weights
W, and this function f is going to take data x as input,
and output a vector of scores for each of the classes
that you want to classify. And so from here we can also define a loss function, so for
example, the SVM loss function that we've talked about
which basically quantifies how happy or unhappy we are with the scores that we've produced, right, and then we can use that to
define a total loss term. So L here, which is a
combination of this data term, combined with a regularization
term that expresses how simple our model is,
and we have a preference for simpler models, for
better generalization. And so now we want to
find the parameters W that correspond to our lowest loss, right? We want to minimize the loss function, and so to do that we want to find the gradient of L with respect to W. So last lecture we talked
about how we can do this using optimization, and we're going to iteratively take steps in the direction of steepest descent, which is
the negative of the gradient, in order to walk down this loss landscape and get to the point
of lowest loss, right? And we saw how this gradient
descent can basically take this trajectory, looking
like this image on the right, getting to the bottom
of your loss landscape. Oh! Okay, and so we also
talked about different ways for computing a gradient, right? We can compute this numerically using finite difference approximation which is slow and approximate,
but at the same time it's really easy to write out, you know you can always
get the gradient this way. We also talked about how to
use the analytic gradient and computing this is, it's fast and exact once you've
gotten the expression for the analytic gradient, but
at the same time you have to do all the math and the
calculus to derive this, so it's also, you know, easy
to make mistakes, right? So in practice what we want
to do is we want to derive the analytic gradient and use this, but at the same time check
our implementation using the numerical gradient to make sure that we've gotten all of our math right. So today we're going to
talk about how to compute the analytic gradient for
arbitrarily complex functions, using a framework that I'm going
to call computational graphs. And so basically what a
computational graph is, is that we can use this
kind of graph in order to represent any function,
where the nodes of the graph are steps of computation
that we go through. So for example, in this example, the linear classifier
that we've talked about, the inputs here are x and W, right, and then this multiplication
node represents the matrix multiplier,
the multiplication of the parameters W with
our data x that we have, outputting our vector of scores. And then we have another
computational node which represents our hinge loss, right, computing our data loss term, Li. And we also have this
regularization term at the bottom right, so this node which computes our regularization term, and then our total loss
here at the end, L, is the sum of the regularization
term and the data term. And the advantage is
that once we can express a function using a computational graph, then we can use a technique
that we call backpropagation which is going to recursively
use the chain rule in order to compute the gradient with respect to every variable
in the computational graph, and so we're going to
see how this is done. And this becomes very
useful when we start working with really complex functions, so for example,
convolutional neural networks that we're going to talk
about later in this class. We have here the input image at the top, we have our loss at the bottom, and the input has to
go through many layers of transformations in order to get all the way down to the loss function. And this can get even
crazier with things like, the, you know, like a
neural turing machine, which is another kind
of deep learning model, and in this case you can see
that the computational graph for this is really insane, and especially, we end up, you know,
unrolling this over time. It's basically completely impractical if you want to compute the gradients for any of these intermediate variables. Okay, so how does backpropagation work? So we're going to start
off with a simple example, where again, our goal is
that we have a function. So in this case, f of x, y, z equals x plus y times z, and we want to find the
gradients of the output of the function with respect
to any of the variables. So the first step, always, is we want to take our function f, and we want to represent it using
a computational graph. Right, so here our computational
graph is on the right, and you can see that we have our, first we have the plus node, so x plus y, and then we have this
multiplication node, right, for the second computation
that we're doing. And then, now we're going
to do a forward pass of this network, so given the values of the variables that we have, so here, x equals negative two, y equals five and z equals negative four,
I'm going to fill these all in in our computational graph,
and then here we can compute an intermediate value,
so x plus y gives three, and then finally we pass it through again, through the last node, the multiplication, to get our final node
of f equals negative 12. So here we want to give every
intermediate variable a name. So here I've called this
intermediate variable after the plus node q, and we
have q equals x plus y, and then f equals q times z,
using this intermediate node. And I've also written
out here, the gradients of q with respect to x
and y, which are just one because of the addition,
and then the gradients of f with respect to q and z,
which is z and q respectively because of the multiplication rule. And so what we want to
find, is we want to find the gradients of f with
respect to x, y and z. So what backprop is, it's
a recursive application of the chain rule, so we're
going to start at the back, the very end of the computational graph, and then we're going to
work our way backwards and compute all the
gradients along the way. So here if we start at
the very end, right, we want to compute the
gradient of the output with respect to the last
variable, which is just f. And so this gradient is
just one, it's trivial. So now, moving backwards,
we want the gradient with respect to z, right, and we know that df over dz is equal to q. So the value of q is just three, and so we have here, df
over dz equals three. And so next if we want to do df over dq, what is the value of that? What is df over dq? So we have here, df over
dq is equal to z, right, and the value of z is negative four. So here we have df over dq
is equal to negative four. Okay, so now continuing to
move backwards to the graph, we want to find df over dy, right, but here in this case, the
gradient with respect to y, y is not connected directly to f, right? It's connected through an
intermediate node of z, and so the way we're going to do this is we can leverage the
chain rule which says that df over dy can be
written as df over dq, times dq over dy, and
so the intuition of this is that in order to get to
find the effect of y on f, this is actually equivalent to if we take the effect of q times q on f,
which we already know, right? df over dq is equal to negative four, and we compound it with the
effect of y on q, dq over dy. So what's dq over dy
equal to in this case? - [Student] One. - One, right. Exactly. So dq over dy is equal to
one, which means, you know, if we change y by a little bit, q is going to change by approximately the same amount right, this is the effect, and so what this is
doing is this is saying, well if I change y by a little bit, the effect of y on q is going to be one, and then the effect of q on f
is going to be approximately a factor of negative four, right? So then we multiply these together and we get that the effect of y on f is going to be negative four. Okay, so now if we want
to do the same thing for the gradient with respect to x, right, we can do the, we can
follow the same procedure, and so what is this going to be? [students speaking away from microphone] - I heard the same. Yeah exactly, so in this
case we want to, again, apply the chain rule, right? We know the effect of q on
f is negative four, and here again, since we have
also the same addition node, dq over dx is equal to one, again, we have negative four times
one, right, and the gradient with respect to x is
going to be negative four. Okay, so what we're doing is, in backprop, is we basically have all of these nodes in our computational graph, but each node is only aware of its
immediate surroundings, right? So we have, at each node,
we have the local inputs that are connected to this node, the values that are flowing into the node, and then we also have the output that is directly outputted from this node. So here our local inputs are
x and y, and the output is z. And at this node we also know
the local gradient, right, we can compute the gradient
of z with respect to x, and the gradient of z with respect to y, and these are usually really
simple operations, right? Each node is going to be something like the addition or the multiplication that we had in that earlier example, which is something where
we can just write down the gradient, and we
don't have to, you know, go through very complex
calculus in order to find this. - [Student] Can you go
back and explain why more in the last slide was
different than planning the first part of it using
just normal calculus? - Yeah, so basically if we go back, hold on, let me... So if we go back here, we
could exactly write out, find all of these using just calculus, so we could say, you know,
we want df over dx, right, and we can probably
expand out this expression and see that it's just going to be z, but we can do this for, in this case, because it's simple, but
we'll see examples later on where once this becomes a
really complicated expression, you don't want to have to use calculus to derive, right, the
gradient for something, for a super-complicated expression, and instead, if you use this formalism and you break it down into
these computational nodes, then you can only ever work with gradients of very simple computations, right, at the level of, you know,
additions, multiplications, exponentials, things as
simple as you want them, and then you just use the chain rule to multiply all these together, and get your, the value of your gradient without having to ever
derive the entire expression. Does that make sense? [student murmuring] Okay, so we'll see an
example of this later. And so, was there another question, yeah? [student speaking away from microphone] - [Student] What's the negative four next to the z representing? - Negative, okay yeah,
so the negative four, these were the, the green values on top were all the values of
the function as we passed it forward through the
computational graph, right? So we said up here that x
is equal to negative two, y is equal to five, and
z equals negative four, so we filled in all of these
values, and then we just wanted to compute the value of this function. Right, so we said this value
of q is going to be x plus y, it's going to be negative
two plus five, it is going to be three, and we have z
is equal to negative four so we fill that in here,
and then we multiplied q and z together, negative four times three in order to get the
final value of f, right? And then the red values underneath were as we were filling in the gradients as we were working backwards. Okay. Okay, so right, so we said that, you know, we have these local, these nodes, and each node basically gets
its local inputs coming in and the output that it
sees directly passing on to the next node, and we also
have these local gradients that we computed, right, the gradient of the immediate output of the node with respect to the inputs coming in. And so what happens during
backprop is we have these, we'll start from the
back of the graph, right, and then we work our way from the end all the way back to the beginning, and when we reach each
node, at each node we have the upstream gradients coming back, right, with respect to the
immediate output of the node. So by the time we reach
this node in backprop, we've already computed the gradient of our final loss l,
with respect to z, right? And so now what we want to find next is we want to find the
gradients with respect to just before the node,
to the values of x and y. And so as we saw earlier, we
do this using the chain rule, right, we have from the chain rule, that the gradient of this loss function with respect to x is going to be the gradient with respect
to z times, compounded by this gradient, local gradient
of z with respect to x. Right, so in the chain rule we always take this upstream gradient coming down, and we multiply it by the local gradient in order to get the gradient
with respect to the input. - [Student] So, sorry, is it, it's different because
this would never work to get a general formula into the, or general symbolic
formula for the gradient. It only works with instantaneous values, where you like. [student coughing] Or passing a little constant
value as a symbolic. - So the question is
whether this only works because we're working
with the current values of the function, and so it works, right, given the current values of
the function that we plug in, but we can write an expression for this, still in terms of the variables, right? So we'll see that gradient
of L with respect to z is going to be some
expression, and gradient of z with respect to x is going to
be another expression, right? But we plug in these,
we plug in the values of these numbers at the
time in order to get the value of the gradient
with respect to x. So what you could do is you
could recursively plug in all of these expressions, right? Gradient with respect, z with respect to x is going to be a simple,
simple expression, right? So in this case, if we
have a multiplication node, gradient of z with
respect to x is just going to be y, right, we know that, but the gradient of L with respect to z, this is probably a complex part of the graph in itself, right, so
here's where we want to just, in this case, have this numerical, right? So as you said, basically
this is going to be just a number coming down, right, a value, and then we just multiply it with the expression that we have
for the local gradient. And I think this will be
more clear when we go through a more complicated
example in a few slides. Okay, so now the gradient
of L with respect to y, we have exactly the
same idea, where again, we use the chain rule,
we have gradient of L with respect to z, times the gradient of z with respect to y, right,
we use the chain rule, multiply these together
and get our gradient. And then once we have these,
we'll pass these on to the node directly before,
or connected to this node. And so the main thing
to take away from this is that at each node we just
want to have our local gradient that we compute, just keep track of this, and then during backprop as
we're receiving, you know, numerical values of gradients
coming from upstream, we just take what that is, multiply it by the local gradient, and then this is what we then send back
to the connected nodes, the next nodes going backwards,
without having to care about anything else besides
these immediate surroundings. So now we're going to go
through another example, this time a little bit more complex, so we can see more why
backprop is so useful. So in this case, our
function is f of w and x, which is equal to one over one plus e to the negative of w-zero times x-zero plus w-one x-one, plus w-two, right? So again, the first step always is we want to write this out as
a computational graph. So in this case we can see
that in this graph, right, first we multiply together the
w and x terms that we have, w-zero with x-zero, w-one with x-one, and w-two, then we add all
of these together, right? Then we do, scale it by negative one, we take the exponential, we add one, and then finally we do
one over this whole term. And then here I've also
filled in values of these, so let's say given values that we have for the ws and xs, right,
we can make a forward pass and basically compute what the value is at every stage of the computation. And here I've also written
down here at the bottom the values, the expressions
for some derivatives that are going to be helpful later on, so same as we did before
with the simple example. Okay, so now then we're going
to do backprop through here, right, so again, we're going to start at the very end of the
graph, and so here again the gradient of the output with
respect to the last variable is just one, it's just trivial, and so now moving
backwards one step, right? So what's the gradient with respect to the input just before one over x? Well, so in this case, we know
that the upstream gradient that we have coming down,
right, is this red one, right? This is the upstream gradient
that we have flowing down, and then now we need to find
the local gradient, right, and the local gradient of this node, this node is one over x, right, so we have f of x equals
one over x here in red, and the local gradient of this df over dx is equal to negative one
over x-squared, right? So here we're going to take
negative one over x-squared, and plug in the value
of x that we had during this forward pass, 1.37,
and so our final gradient with respect to this variable is going to be negative one over
1.37 squared times one equals negative 0.53. So moving back to the next node, we're going to go through the
exact same process, right? So here, the gradient
flowing from upstream is going to be negative 0.53, right, and here the local gradient,
the node here is a plus one, and so now looking at our
reference of derivatives at the bottom, we have that
for a constant plus x, the local gradient is just one, right? So what's the gradient with respect to this variable using the chain rule? So it's going to be the upstream gradient of negative 0.53 times
our local gradient of one, which is equal to negative 0.53. So let's keep moving
backwards one more step. So here we have the exponential, right? So what's the upstream
gradient coming down? [student speaking away from microphone] Right, so the upstream
gradient is negative 0.53, what's the local gradient here? It's going to be the local
gradient of e to the x, right? This is an exponential
node, and so our chain rule is going to tell us that our gradient is going to be negative 0.53
times e to the power of x, which in this case is negative one, from our forward pass, and
this is going to give us our final gradient of negative 0.2. Okay, so now one more node here, the next node is, that
we reach, is going to be a multiplication with negative one, right? So here, what's the upstream
gradient coming down? - [Student] Negative 0.2? - [Serena] Negative 0.2,
right, and what's going to be the local gradient, can
look at the reference sheet. It's going to be, what was it? I think I heard it. - [Student] That's minus one? - It's going to be minus
one, exactly, yeah, because our local gradient
says it's going to be, df over dx is a, right, and the value of a that we scaled x by is negative one here. So we have here that the gradient is negative one times negative 0.2, and so our gradient is 0.2. Okay, so now we've
reached an addition node, and so in this case we
have these two branches both connected to it, right? So what's the upstream gradient here? It's going to be 0.2, right,
just as everything else, and here now the gradient with respect to each of these branches,
it's an addition, right, and we saw from before
in our simple example that when we have an addition node, the gradient with respect
to each of the inputs to the addition is just
going to be one, right? So here, our local gradient
for looking at our top stream is going to be one times
the upstream gradient of 0.2, which is going to give
a total gradient of 0.2, right? And then we, for our bottom branch we'd do the same thing, right, our
upstream gradient is 0.2, our local gradient is one again, and the total gradient is 0.2. So is everything clear about this? Okay. So we have a few more
gradients to fill out, so moving back now we've
reached w-zero and x-zero, and so here we have a
multiplication node, right, so we saw the multiplication
node from before, it just, the gradient with respect to one of the inputs just is
the value of the other input. And so in this case, what's the gradient with respect to w-zero? - [Student] Minus 0.2. - Minus, I'm hearing minus 0.2, exactly. Yeah, so with respect to w-zero, we have our upstream gradient, 0.2, right, times our, this is the bottom one, times our value of x,
which is negative one, we get negative 0.2 and
we can do the same thing for our gradient with respect to x-zero. It's going to be 0.2
times the value of w-zero which is two, and we get 0.4. Okay, so here we've filled
out most of these gradients, and so there was the question earlier about why this is simpler
than just computing, deriving the analytic gradient,
the expression with respect to any of these variables, right? And so you can see here,
all we ever dealt with was expressions for local gradients that we had to write out, so
once we had these expressions for local gradients, all we did was plug in the values for
each of these that we have, and use the chain rule to
numerically multiply this all the way backwards and get the gradients with respect to all of the variables. And so, you know, we can also fill out the gradients with respect
to w-one and x-one here in exactly the same way, and so one thing that I want to note is that
right when we're creating these computational graphs, we can define the computational nodes at any
granularity that we want to. So in this case, we broke it down into the absolute simplest
that we could, right, we broke it down into
additions and multiplications, you know, it basically can't
get any simpler than that, but in practice, right,
we can group some of these nodes together into
more complex nodes if we want. As long as we're able to write down the local gradient for that node, right? And so as an example, if we
look at a sigmoid function, so I've defined the sigmoid function in the upper-right here, of a sigmoid of x is equal to one over one
plus e to the negative x, and this is something that's
a really common function that you'll see a lot in
the rest of this class, and we can compute the gradient for this, we can write it out, and if
we do actually go through the math of doing this analytically, we can get a nice expression at the end. So in this case it's equal
to one minus sigma of x, so the output of this function
times sigma of x, right? And so in cases where we
have something like this, we could just take all the computations that we had in our graph
that made up this sigmoid, and we could just replace it with one big node that's a sigmoid, right, because we do know the local
gradient for this gate, it's this expression, d of the
sigmoid of x over dx, right? So basically the important thing here is that you can, group any nodes that you want to make any sorts of a little
bit more complex nodes, as long as you can write down
the local gradient for this. And so all this is is
basically a trade-off between, you know, how much math
that you want to do in order to get a more, kind
of concise and simpler graph, right, versus how simple you want each of your gradients to be, right? And then you can write out as complex of a computational graph that you want. Yeah, question? - [Student] This is a
question on the graph itself, is there a reason that the
first two multiplication nodes and the weights are not connected
to a single addition node? - So they could also be connected into a single addition node,
so the question was, is there a reason why w-zero and x-zero are not connected with w-two? All of these additions
just connected together, and yeah, so the reason, the answer is that you can do that if you want, and in practice, maybe you
would actually want to do that because this is still a
very simple node, right? So in this case I just wrote
this out into as simple as possible, where each node
only had up to two inputs, but yeah, you could definitely do that. Any other questions about this? Okay, so the one thing that I really like about thinking about this
like a computational graph is that I feel very comforted, right, like anytime I have to take a gradient, find gradients of something,
even if the expression that I want to compute
gradients of is really hairy, and really scary, you know,
whether it's something like this sigmoid or something worse, I know that, you know, I could
derive this if I want to, but really, if I just
sit down and write it out in terms of a computational graph, I can go as simple as I need to to always be able to apply
backprop and the chain rule, and be able to compute all
the gradients that I need. And so this is something that
you guys should think about when you're doing your homeworks,
as basically, you know, anytime you're having trouble
finding gradients of something just think about it as
a computational graph, break it down into all of these parts, and then use the chain rule. Okay, and so, you know, so we talked about how we could group these
set of nodes together into a sigmoid gate, and
just to confirm, like, that this is actually exactly equivalent, we can plug this in, right? So we have that our input
here to the sigmoid gate is going to be one, in
green, and then we have that the output is going
to be here, 0.73, right, and this'll work out if you plug it in to the sigmoid function. And so now if we want to
do, if we want to take the gradient, and we want
to treat this entire sigmoid as one node, now what we should do is we need to use this local gradient that we've derived up here, right? One minus sigmoid of x
times the sigmoid of x. So if we plug this in, and here we know that the value of sigmoid of x was 0.73, so if we plug this value
in we'll see that this, the value of this gradient
is equal to 0.2, right, and so the value of this
local gradient is 0.2, we multiply it by the x
upstream gradient which is one, and we're going to get
out exactly the same value of the gradient with respect
to before the sigmoid gate, as if we broke it down into all
of the smaller computations. Okay, and so as we're looking
at what's happening, right, as we're taking these
gradients going backwards through our computational graph, there's some patterns that you'll notice where there's some
intuitive interpretation that we can give these, right? So we saw that the add gate is
a gradient distributor right, when we passed through
this addition gate here, which had two branches coming out of it, it took the gradient,
the upstream gradient and it just distributed it,
passed the exact same thing to both of the branches
that were connected. So here's a couple more
that we can think about. So what's a max gate look like? So we have a max gate
here at the bottom, right, where the input's coming in are z and w, z has a value of two, w has
a value of negative one, and then we took the max of
this, which is two, right, and so we pass this
down into the remainder of our computational graph. So now if we're taking the
gradients with respect to this, the upstream gradient is, let's
say two coming back, right, and what does this local
gradient look like? So anyone, yes? - [Student] It'll be zero for
one, and one for the other? - Right. [student speaking away from microphone] Exactly, so the answer that was given is that z will have a gradient of two, w will have a value, a gradient of zero, and so one of these is going to get the full value of the
gradient just passed back, and routed to that variable,
and then the other one will have a gradient of zero, and so, so we can think of this as kind
of a gradient router, right, so, whereas the addition node passed back the same gradient to
both branches coming in, the max gate will just take the gradient and route it to one of the branches, and this makes sense because
if we look at our forward pass, what's happening is that only the value that was the maximum got passed down to the rest of the
computational graph, right? So it's the only value
that actually affected our function computation at
the end, and so it makes sense that when we're passing
our gradients back, we just want to adjust what, you know, flow it through that
branch of the computation. Okay, and so another one,
what's a multiplication gate, which we saw earlier, is there
any interpretation of this? [student speaking away from microphone] Okay, so the answer that was given is that the local
gradient is basically just the value of the other variable. Yeah, so that's exactly right. So we can think of this as
a gradient switcher, right? A switcher, and I guess
a scaler, where we take the upstream gradient and we scale it by the value of the other branch. Okay, and so one other thing to note is that when we have a place where one node is connected to multiple nodes, the gradients add up at this node, right? So at these branches, using
the multivariate chain rule, we're just going to take the value of the upstream gradient coming
back from each of these nodes, and we'll add these
together to get the total upstream gradient that's
flowing back into this node, and you can see this from
the multivariate chain rule and also thinking about this,
you can think about this that if you're going to
change this node a little bit, it's going to affect both
of these connected nodes in the forward pass,
right, when you're making your forward pass through the graph. And so then when you're
doing backprop, right, then now the, both of
these gradients coming back are going to affect this node, right, and so that's how we're
going to sum these up to be the total upstream gradient
flowing back into this node. Okay, so any questions about backprop, going through these forward
and backward passes? - [Student] So we haven't did anything to actually update the weights. [speaking away from microphone] - Right, so the question is,
we haven't done anything yet to update the values of these weights, we've only found the
gradients with respect to the variables, that's exactly right. So what we've talked about
so far in this lecture is how to compute gradients with
respect to any variables in our function, right,
and then once we have these we can just apply everything we learned in the optimization lecture,
last lecture, right? So given the gradient,
we now take a step in the direction of the gradient in order to update our weight,
our parameters, right? So you can just take this entire framework that we learned about last
lecture for optimization, and what we've done here is
just learn how to compute the gradients we need for
arbitrarily complex functions, right, and so this is going
to be useful when we talk about complex functions like
neural networks later on. Yeah? - [Student] Do you mind writing out the, all the variate, so you could help explain this slide a little better? - Yeah, so I can write
this maybe on the board. Right, so basically if we're
going to have, let's see, if we're going to have the gradient of f with respect to some variable x, right, and let's say it's
connected through variables, let's see, i, we can basically... Right, so this is basically saying that if x is connected to
these multiple elements, right, which in this case, different q-is, then the chain rule is taking all, it's going to take the effect of each of these intermediate variables, right, on our final output f, and
then compound each one with the local effect of our variable x on that intermediate value, right? So yeah, it's basically just
summing all these up together. Okay, so now that we've, you
know, done all these examples in the scalar case, we're going to look at what happens when we have vectors, right? So now if our variables x, y and z, instead of just being numbers,
we have vectors for these. And so everything stays exactly
the same, the entire flow, the only difference is
that now our gradients are going to be Jacobian matrices, right, so these are now going
to be matrices containing the derivative of each
element of, for example z with respect to each element of x. Okay, and so to, you
know, so give an example of something where this is
happening, right, let's say that we have our input is
going to now be a vector, so let's say we have a
4096-dimensional input vector, and this is kind of a common
size that you might see in convolutional neural networks later on, and our node is going to be an
element-wise maximum, right? So we have f of x is equal to the maximum of x compared with zero
element-wise, and then our output is going to be also a
4096-dimensional vector. Okay, so in this case, what's the size of our Jacobian matrix? Remember I said earlier,
the Jacobian matrix is going to be, like each row is, it's going to be partial derivatives, a matrix of partial derivatives
of each dimension of the output with respect to
each dimension of the input. Okay, so the answer I
heard was 4,096 squared, and that's, yeah, that's correct. So this is pretty large,
right, 4,096 by 4,096 and in practice this is
going to be even larger because we're going to
work with many batches of, you know, of, for example, 100 inputs at the same time, right,
and we'll put all of these through our node at the same
time to be more efficient, and so this is going to scale this by 100, and in practice our Jacobian's
actually going to turn out to be something like
409,000 by 409,000 right, so this is really huge, and basically completely impractical to work with. So in practice though,
we don't actually need to compute this huge
Jacobian most of the time, and so why is that, like, what does this Jacobian matrix look like? If we think about what's happening here, where we're taking this
element-wise maximum, and we think about what are each of the partial derivatives, right, which dimension of the inputs affect which dimensions of the output? What sort of structure can we
see in our Jacobian matrix? [student speaking away from microphone] Okay, so I heard that it's
diagonal, right, exactly. So because this is element-wise,
right, each element of the input, say the first
dimension, only affects that corresponding element
in the output, right? And so because of that
our Jacobian matrix, which is just going to
be a diagonal matrix. And so in practice then,
we don't actually have to write out and formulate
this entire Jacobian, we can just know the effect
of x on the output, right, and then we can just
use these values, right, and fill it in as we're
computing the gradient. Okay, so now we're going to go through a more concrete vectorized
example of a computational graph. Right, so let's look at a case where we have the function f of x and W is equal to, basically the
L-two of W multiplied by x, and so in this case we're going to say x is n-dimensional and W is n by n. Right, so again our first step, writing out the
computational graph, right? We have W multiplied by
x, and then followed by, I'm just going to call this L-two. And so now let's also fill
out some values for this, so we can see that, you
know, let's say have W be this two by two matrix, and x is going to be this
two-dimensional vector, right? And so we can say, label
again our intermediate nodes. So our intermediate node
after the multiplication it's going to be q, we
have q equals W times x, which we can write out
element-wise this way, where the first element is
just W-one-one times x-one plus W-one-two times x-two and so on, and then we can now express
f in relation to q, right? So looking at the second
node we have f of q is equal to the L-two norm of q, which is equal to q-one
squared plus q-two squared. Okay, so we filled this in, right, we get q and then we get our final output. Okay, so now let's do
backprop through this, right? So again, this is always the first step, we have the gradient with respect
to our output is just one. Okay, so now let's move back one node, so now we want to find the
gradient with respect to q, right, our intermediate
variable before the L-two. And so q is a two-dimensional vector, and what we want to do is we want to find how each element of q
affects our final value of f, right, and so if we
look at this expression that we've written out
for f here at the bottom, we can see that the gradient of f with respect to a specific
q-i, let's say q-one, is just going to be two times q-i, right? This is just taking this derivative here, and so we have this expression for, with respect to each element of q-i, we could also, you know, write this out in vector form if we want to, it's just going to be two
times our vector of q, right, if we want to write
this out in vector form, and so what we get is
that our gradient is 0.44, and 0.52, this vector, right? And so you can see that it just took q and it scaled it by two, right? Each element is just multiplied by two. So the gradient of a vector
is always going to be the same size as the original vector, and each element of this
gradient is going to, it means how much of
this particular element affects our final output of the function. Okay, so now let's move
one step backwards, right, what's the gradient with respect to W? And so here again we want
to use the same concept of trying to apply the chain rule, right, so we want to compute our local gradient of q with respect to W, and so let's look at this again element-wise,
and if we do that, let's see what's the
effect of each q, right, each element of q with
respect to each element of W, and so this is going to be the Jacobian that we talked about earlier,
and if we look at this in this multiplication, q is equal to W times x, right,
what's the derivative, or the gradient of the first element of q, so our first element up top,
with respect to W-one-one? So q-one with respect to W-one-one? What's that value? X-one, exactly. Yeah, so we know that this is x-one, and we can write this
out more generally of the gradient of q-k with respect
to W-i,j is equal to X-j. And then now if we want
to find the gradient with respect to, of f,
with respect to each W-i,j. So looking at these derivatives now, we can use this chain rule
that we talked earlier where we basically compound df over dq-k for each element of q with dq-k over W-i,j for each element of W-i,j, right? So we find the effect of each element of W on each element of q, and
sum this across all q. And so if you write this
out, this is going to give this expression of two
times q-i times x-j. Okay, and so filling this out then we get this gradient with respect to W, and so again we can compute
this each element-wise, or we can also look at this
expression that we've derived and write it out in
vectorized form, right? So okay, and remember, the important thing is always to check the gradient
with respect to a variable should have the same shape as
the variable, and something, so this is something
really useful in practice to sanity check, right,
like once you've computed what your gradient should
be, check that this is the same shape as your variable, because again, the element,
each element of your gradient is quantifying how much that element is contributing to your, is
affecting your final output. Yeah? [student speaking away from microphone] The both sides, oh the both sides one is an indicator function,
so this is saying that it's just one if k equals i. Okay, so let's see, so we've done that, and so now just see, one more example. Now our last thing we need to find is the gradient with respect to q-I. So here if we compute the
partial derivatives we can see that dq-k over dx-i is
equal to W-k,i, right, using the same way as we did it for W, and then again we can
just use the chain rule and get the total
expression for that, right? And so this is going to be the gradient with respect to x, again,
of the same shape as x, and we can also write this out in vectorized form if we want. Okay, so any questions about this, yeah? [student speaking away from microphone] So we are computing the Jacobian, so let me go back here, right, so if we're doing, so right, so we have these partial derivatives of q-k with respect to x-i, right, and these are forming your, the entries
of your Jacobian, right? And so in practice what we're going to do is we basically take that,
and you're going to see it up there in the chain rule,
so the vectorized expression of gradient with respect to x, right, this is going to have the Jacobian here which is this transposed value here, so you can write it
out in vectorized form. [student speaking away from microphone] So well, so in this case the matrix is going to be the same size as W right, so it's not actually a large
matrix in this case, right? Okay, so the way that we've
been thinking about this is like a really modularized
implementation, right, where in our computational graph, right, we look at each node
locally and we compute the local gradients and chain them with upstream gradients coming down, and so you can think of this as basically a forward and a backwards API, right? In the forward pass we
implement the, you know, a function computing
the output of this node, and then in the backwards
pass we compute the gradient. And so when we actually
implement this in code, we're going to do this
in exactly the same way. So we can basically think
about, for each gate, right, if we implement a forward
function and a backward function, where the backward function
is computing the chain rule, then if we have our entire
graph, we can just make a forward pass through the
entire graph by iterating through all the nodes in the graph, all the gates. Here I'm going to use
the word gate and node, kind of interchangeably,
we can iterate through all of these gates and just call forward on each of the gates, right? And we just want to do this
in topologically sorted order, so we process all of
the inputs coming in to a node before we process that node. And then going backwards,
we're just going to then go through all of the gates
in this reverse sorted order, and then call backwards
on each of these gates. Okay, and so if we look at then the implementation for
our particular gates, so for example, this MultiplyGate here, we want to implement
the forward pass, right, so it gets x and y as inputs,
and returns the value of z, and then when we go backwards, right, we get as input dz, which
is our upstream gradient, and we want to output the gradients on the input's x and
y to pass down, right? So we're going to output dx and dy, and so in this case, in this example, everything is back to
the scalar case here, and so if we look at
this in the forward pass, one thing that's important
is that we need to, we should cache the values
of the forward pass, right, because we end up using this in the backward pass a lot of the time. So here in the forward pass,
we want to cache the values of x and y, right, and
in the backward pass, using the chain rule,
we're going to, remember, take the value of the upstream gradient and scale it by the value
of the other branch, right, and so we'll keep, for
dx we'll take our value of self.y that we kept, and multiply it by dz coming down, and same for dy. Okay, so if you look at a lot
of deep-learning frameworks and libraries you'll see
that they exactly follow this kind of modularization, right? So for example, Caffe is a
popular deep learning framework, and you'll see, if you go look
through the Caffe source code you'll get to some
directory that says layers, and in layers, which are
basically computational nodes, usually layers might be
slightly more, you know, some of these more complex
computational nodes like the sigmoid that
we talked about earlier, you'll see, basically just a whole list of all different kinds of
computational nodes, right? So you might have the sigmoid, and I know there might be here, there's
like a convolution is one, there's an Argmax is another layer, you'll have all of these
layers and if you dig in to each of them, they're
just exactly implementing a forward pass and a backward pass, and then all of these are called when we do forward and backward pass through the entire network that we formed, and so our network is just basically going to be stacking up all of these, the different layers that we
choose to use in the network. So for example, if we
look at a specific one, in this case a sigmoid layer, you'll see that in the sigmoid layer, right, we've talked about the sigmoid function, you'll see that there's a forward pass which basically computes
exactly the sigmoid expression, and then a backward pass, right, where it is taking as input
something, basically a top_diff, which is our upstream
gradient in this case, and multiplying it by a local
gradient that we compute. So in assignment one you'll get practice with this kind of, this
computational graph way of thinking where, you know, you're
going to be writing your SVM and Softmax classes, and taking the gradients of these. And so again, remember always
you want to first step, represent it as a
computational graph, right? Figure out what are all the computations that you did leading up to the output, and then when you, when it's time to do your backward pass,
just take the gradient with respect to each of
these intermediate variables that you've defined in
your computational graph, and use the chain rule to
link them all together. Okay, so summary of what
we've talked about so far. When we get down to, you know,
working with neural networks, these are going to be
really large and complex, so it's going to be
impractical to write down the gradient formula by hand
for all your parameters. So in order to get these gradients, right, we talked about how, what we
should use is backpropagation, right, and this is kind of
one of the core techniques of, you know, neural
networks, is basically using backpropagation to
get your gradients, right? And so this is a recursive application of the chain rule where we have
this computational graph, and we start at the back and
we go backwards through it to compute the gradients with respect to all of the intermediate variables, which are your inputs, your parameters, and everything else in the middle. And we've also talked about how really this implementation and
this graph structure, each of these nodes is
really, you can see this as implementing a forward
and backwards API, right? And so in the forward
pass we want to compute the results of the operation, and we want to save any intermediate values that we might want to use later
in our gradient computation, and then in the backwards
pass we apply this chain rule and we take this upstream gradient, we chain it, multiply it
with our local gradient to compute the gradient with respect to the inputs of the node,
and we pass this down to the nodes that are connected next. Okay, so now finally we're going to talk about neural networks. All right, so really, you
know, neural networks, people draw a lot of analogies
between neural networks and the brain, and different types of biological inspirations,
and we'll get to that in a little bit, but first let's
talk about it, you know, just looking at it as a function, as a class of functions
without all of the brain stuff. So, so far we've talked about, you know, we've worked a lot with this
linear score function, right? f equals W times x, and
so we've been using this as a running example of a
function that we want to optimize. So instead of using the
single in your transformation, if we want a neural network where we can just, as the simplest form, just stack two of these together, right? Just a linear transformation
on top of another one in order to get a two-layer
neural network, right? And so what this looks like is
first we have our, you know, a matrix multiply of W-one with x, and then we get this intermediate variable and we have this non-linear
function of a max of zero with W, max with this
output of this linear layer, and it's really important to
have these non-linearities in place, which we'll
talk about more later, because otherwise if you just
stack linear layers on top of each other, they're
just going to collapse to, like a single linear function. Okay, so we have our first linear layer and then we have this
non-linearity, right, and then on top of this we'll
add another linear layer. And then from here, finally
we can get our score function, our output vector of scores. So basically, like, more broadly speaking, neural networks are a class of functions where we have simpler functions, right, that are stacked on top of each other, and we stack them in a hierarchical way in order to make up a more
complex non-linear function, and so this is the idea of
having, basically multiple stages of hierarchical computation, right? And so, you know, so this is kind of the main way that we do this is by taking something like this matrix
multiply, this linear layer, and we just stack multiple
of these on top of each other with non-linear functions
in-between, right? And so one thing that this
can help solve is if we look, if we remember back to
this linear score function that we were talking about, right, remember we discussed earlier how each row of our weight matrix W was
something like a template. It was a template that sort
of expressed, you know, what we're looking for in the input for a specific class, right,
so for example, you know, the car template looks something like this kind of fuzzy red car,
and we were looking for this in the input to compute the
score for the car class. And we talked about one
of the problems with this is that there's only one template, right? There's this red car, whereas in practice, we actually have multiple modes, right? We might want, we're looking
for, you know, a red car, there's also a yellow
car, like all of these are different kinds of cars, and so what this kind of multiple
layer network lets you do is now, you know, each of
this intermediate variable h, right, W-one can still be
these kinds of templates, but now you have all of these scores for these templates in h,
and we can have another layer on top that's combining
these together, right? So we can say that actually
my car class should be, you know, connected to, we're looking for both red cars as well
as yellow cars, right, because we have this matrix W-two which is now a weighting
of all of our vector in h. Okay, any questions about this? Yeah? [student speaking away from microphone] Yeah, so there's a lot of ways, so there's a lot of different
non-linear functions that you can choose from,
and we'll talk later on in a later lecture about
all the different kinds of non-linearities that
you might want to use. - [Student] For the pictures in the slide, so, on the bottom row you have images of your vector W-one weight, and so maybe you would have images
of another vector W-two? - So W-one, because it's
directly connected to the input x, this is what's
like, really interpretable, because you can formulate
all of these templates. W-two, so h is going to be a score of how much of each template
you solve, for example, all right, so it might be
like you have a, you know, like a, I don't know, two for the red car, and like, one for the yellow
car or something like that. - [Student] Oh, okay, so
instead of W-one being just 10, like, you would have a left-facing horse and a right-facing horse,
and they'd both be included-- - Exactly, so the question
is basically whether in W-one you could have
both left-facing horse and right-facing horse,
right, and so yeah, exactly. So now W-one can be many different
kinds of templates right? They're not, and then W-two,
now we can, like basically it's a weighted sum of
all of these templates. So now it allows you to weight
together multiple templates in order to get the final
score for a particular class. - [Student] So if you're
processing an image then it's actually left-facing horse. It'll get a really high score with the left-facing horse template, and a lower score with the
right-facing horse template, and then this will take
the maximum of the two? - Right, so okay, so the question is, if our image x is like a left-facing horse and in W-one we have a template of a left-facing horse and
a right-facing horse, then what's happening, right? So what happens is yeah,
so in h you might have a really high score for
your left-facing horse, kind of a lower score for
your right-facing horse, and W-two is, it's a weighted
sum, so it's not a maximum. It's a weighted sum of these templates, but if you have either a really high score for one of these templates,
or let's say you have, kind of a lower and medium score
for both of these templates, all of these kinds of combinations are going to give high scores, right? And so in the end what you're going to get is something that generally scores high when you have a horse of any kind. So let's say you had a front-facing horse, you might have medium values for both the left and the right templates. Yeah, question? - [Student] So is W-two
doing the weighting, or is h doing the weighting? - W-two is doing the
weighting, so the question is, "Is W-two doing the weighting
or is h doing the weighting?" h is the value, like in this example, h is the value of scores
for each of your templates that you have in W-one, right? So h is like the score function, right, it's how much of each
template in W-one is present, and then W-two is going
to weight all of these, weight all of these intermediate scores to get your final score for the class. - [Student] And which
is the non-linear thing? - So the question is, "which
is the non-linear thing?" So the non-linearity usually
happens right before h, so h is the value right
after the non-linearity. So we're talking about
this, like, you know, intuitively as this example of like, W-one is looking for, you know, has these same templates as before, and W-two is a weighting for these. In practice it's not
exactly like this, right, because as you said, there's all these non-linearities thrown in and so on, but it has this approximate
type of interpretation to it. - [Student] So h is just W-one-x then? - Yeah, yeah, so the
question is h just W-one-x? So h is just W-one times x,
with the max function on top. Oh, let me just, okay so, so we've talked about
this as an example of a two-layer neural network,
and we can stack more layers of these to get deeper networks
of arbitrary depth, right? So we can just do this one more time at another non-linearity and
matrix multiply now by W-three, and now we have a three-layer
neural network, right? And so this is where the
term deep neural networks is basically coming from, right? This idea that you can stack
multiple of these layers, you know, for very deep networks. And so in homework you'll get a practice of writing and you know, training one of these neural networks, I
think in assignment two, but basically a full
implementation of this using this idea of forward pass, right, and backward passes, and using chain rule to compute gradients
that we've already seen. The entire implementation of
a two-layer neural network is actually really simple, it
can just be done in 20 lines, and so you'll get some practice
with this in assignment two, writing out all of these parts. And okay, so now that we've sort of seen what neural networks are
as a function, right, like, you know, we hear
people talking a lot about how there's biological
inspirations for neural networks, and so even though it's
important that to emphasize that these analogies are really loose, it's really just very loose ties, but it's still interesting to understand where some of these connections
and inspirations come from. And so now I'm going to
talk briefly about that. So if we think about a neuron, in kind of a very simple way, this neuron is, here's a diagram of a neuron. We have the impulses
that are carried towards each neuron, right, so we have a lot of neurons connected together
and each neuron has dendrites, right, and these are sort
of, these are what receives the impulses that come into the neuron. And then we have a cell body, right, that basically integrates
these signals coming in and then there's a kind
of, then it takes this, and after integrating all
these signals, it passes on, you know, the impulse carries away from the cell body to downstream
neurons that it's connected to, right, and it carries
this away through axons. So now if we look at what
we've been doing so far, right, with each computational node, you can see that this actually has, you can see it in kind of a similar way, right? Where nodes are connected to each other in the computational graph,
and we have inputs, or signals, x, x right,
coming into a neuron, and then all of these x,
right, x-zero, x-one, x-two, these are combined and
integrated together, right, using, for example our weights, W. So we do some sort of computation, right, and in some of the computations
we've been doing so far, something like W times x plus b, right, integrating all these together, and then we have an activation function that we apply on top, we get
this value of this output, and we pass it down to
the connecting neurons. So if you look at that
this, this is actually, you can think about this in
a very similar way, right? Like, you know, these are
what's the signals coming in are kind of the, connected
at synapses, right? The synapse connecting
the multiple neurons, the dendrites are
integrating all of these, they're integrating all of
this information together in the cell body, and then we have the output carried on the output later on. And so this is kind of the analogy that you can draw between them, and if you look at these
activation functions, right? This is what basically takes
all the inputs coming in and outputs one number
that's going out later on, and we've talked about examples like sigmoid activation function, right, and different kinds of non-linearities, and so sort of one kind of
loose analogy that you can draw is that these
non-linearities can represent something sort of like the firing, or spiking rate of the neurons, right? Where our neurons transmit
signals to connecting neurons using kind of these
discrete spikes, right? And so we can think of, you know, if they're spiking very
fast then there's kind of a strong signal that's passed later on, and so we can think of this value after our activation function
as sort of, in a sense, sort of this firing rate
that we're going to pass on. And you know in practice,
I think neuroscientists who are actually studying this say that kind of one of the non-linearities that are most similar to the way that neurons are actually behaving is a ReLU function, which
is a ReLU non-linearity, which is something that we're going to look at more later
on, but it's a function that's at zero for all
negative values of input, and then it's a linear
function for everything that's in kind of a positive regime. And so, you know, we'll talk more about this activation function later on, but that's kind of, in practice, maybe the one that's most similar to how neurons are actually behaving. But it's really important
to be extremely careful with making any of these
sorts of brain analogies, because in practice biological neurons are way more complex than this. There's many different
kinds of biological neurons, the dendrites can perform really complex non-linear computations. Our synapses, right, the
W-zeros that we had earlier where we drew this analogy,
are not single weights like we had, they're actually
really complex, you know, non-linear dynamical systems in practice, and also this idea of interpreting
our activation function as a sort of rate code
or firing rate is also, is insufficient in practice, you know. It's just this kind of
firing rate is probably not a sufficient model of how neurons will actually communicate to
downstream neurons, right, like even as a very simple
way, there's a very, the neurons will fire at a variable rate, and this variability probably
should be taken into account. And so there's all of these, you know, it's kind of a much more complex thing than what we're dealing with. There's references, for example
this dendritic computation that you can look at if you're
interested in this topic, but yeah, so that in practice, you know, we can sort of see how
it may resemble a neuron at this very high level, but neurons are, in practice, much more
complicated than that. Okay, so we talked about how
there's many different kinds of activation functions
that could be used, there's the ReLU that I mentioned earlier, and we'll talk about all
of these different kinds of activation functions in
much more detail later on, choices of these activation functions that you might want to use. And so we'll also talk
about different kinds of neural network architectures. So we gave the example
of these fully connected neural networks, right,
where each layer is this matrix multiply, and
so the way we actually want to call these is, we said
two-layer neural network before, and that corresponded to
the fact that we have two of these linear layers,
right, where we're doing a matrix multiply, two
fully connected layers is what we call these. We could also call this a
one-hidden-layer neural network, so instead of counting the number of matrix multiplies we're doing, counting the number of
hidden layers that we have. I think it's, you can use either, I think maybe two-layer neural network is something that's a
little more commonly used. And then also here, for our
three-layer neural network that we have, this can also be called a two-hidden-layer neural network. And so we saw that, you
know, when we're doing this type of feed forward, right, forward pass through a neural network, each of these nodes in this network is basically doing the
kind of operation of the neuron that I showed earlier, right? And so what's actually happening is is basically each hidden
layer you can think of as a whole vector, right,
a set of these neurons, and so by writing it out this way with these matrix multiplies
to compute our neuron values, it's a way that we can
efficiently evaluate this entire layer of neurons, right? So with one matrix multiply
we get output values of, you know, of a layer of let's say 10, or 50 or 100 of neurons. All right, and so looking at
this again, writing this out, all out in matrix form,
matrix-vector form, we have our, you know, non-linearity here. F that we're using, in this
case a sigmoid function, right, and we can take our data
x, some input vector or our values, and we can apply
our first matrix multiply, W-one on top of this,
then our non-linearity, then a second matrix multiply to get a second hidden layer, h-two, and then we have our final output, right? And so, you know, this
is basically all you need to be able to write a neural network, and as we saw earlier, the backward pass. You then just use backprop
to compute all of those, and so that's basically all there is to kind of the main idea
of what's a neural network. Okay, so just to
summarize, we talked about how we could arrange neurons
into these computations, right, of fully-connected or linear layers. This abstraction of a
layer has a nice property that we can use very
efficient vectorized code to compute all of these. We also talked about how it's important to keep in mind that neural networks do have some, you know,
analogy and loose inspiration from biology, but they're
not really neural. I mean, this is a pretty loose analogy that we're making, and
next time we'll talk about convolutional neural networks. Okay, thanks. 

- Okay, let's get started. Alright, so welcome to lecture five. Today we're going to be getting
to the title of the class, Convolutional Neural Networks. Okay, so a couple of
administrative details before we get started. Assignment one is due Thursday, April 20, 11:59 p.m. on Canvas. We're also going to be releasing
assignment two on Thursday. Okay, so a quick review of last time. We talked about neural
networks, and how we had the running example of
the linear score function that we talked about through
the first few lectures. And then we turned this
into a neural network by stacking these linear
layers on top of each other with non-linearities in between. And we also saw that
this could help address the mode problem where
we are able to learn intermediate templates
that are looking for, for example, different
types of cars, right. A red car versus a yellow car and so on. And to combine these
together to come up with the final score function for a class. Okay, so today we're going to talk about convolutional neural networks, which is basically the same sort of idea, but now we're going to
learn convolutional layers that reason on top of basically explicitly trying to maintain spatial structure. So, let's first talk a little bit about the history of neural
networks, and then also how convolutional neural
networks were developed. So we can go all the way back
to 1957 with Frank Rosenblatt, who developed the Mark
I Perceptron machine, which was the first
implementation of an algorithm called the perceptron, which
had sort of the similar idea of getting score functions,
right, using some, you know, W times X plus a bias. But here the outputs are going
to be either one or a zero. And then in this case
we have an update rule, so an update rule for our weights, W, which also look kind of similar
to the type of update rule that we're also seeing in
backprop, but in this case there was no principled
backpropagation technique yet, we just sort of took the
weights and adjusted them in the direction towards
the target that we wanted. So in 1960, we had Widrow and Hoff, who developed Adaline and
Madaline, which was the first time that we were able to
get, to start to stack these linear layers into
multilayer perceptron networks. And so this is starting to now
look kind of like this idea of neural network layers, but
we still didn't have backprop or any sort of principled
way to train this. And so the first time
backprop was really introduced was in 1986 with Rumelhart. And so here we can start
seeing, you know, these kinds of equations with the chain
rule and the update rules that we're starting to
get familiar with, right, and so this is the first time we started to have a principled way to train these kinds of network architectures. And so after that, you know,
it still wasn't able to scale to very large neural networks,
and so there was sort of a period in which there wasn't a whole lot of new things happening
here, or a lot of popular use of these kinds of networks. And so this really started
being reinvigorated around the 2000s, so in
2006, there was this paper by Geoff Hinton and Ruslan Salakhutdinov, which basically showed that we could train a deep neural network, and show that we could
do this effectively. But it was still not quite the sort of modern iteration
of neural networks. It required really careful initialization in order to be able to do backprop, and so what they had
here was they would have this first pre-training
stage, where you model each hidden layer through this kind of, through a restricted Boltzmann machine, and so you're going to get
some initialized weights by training each of
these layers iteratively. And so once you get all
of these hidden layers you then use that to
initialize your, you know, your full neural network,
and then from there you do backprop and fine tuning of that. And so when we really started
to get the first really strong results using neural networks,
and what sort of really sparked the whole craze
of starting to use these kinds of networks really
widely was at around 2012, where we had first the strongest results using for speech recognition, and so this is work out
of Geoff Hinton's lab for acoustic modeling
and speech recognition. And then for image recognition,
2012 was the landmark paper from Alex Krizhevsky
in Geoff Hinton's lab, which introduced the first
convolutional neural network architecture that was able to do, get really strong results
on ImageNet classification. And so it took the ImageNet,
image classification benchmark, and was able to dramatically reduce the error on that benchmark. And so since then, you
know, ConvNets have gotten really widely used in all
kinds of applications. So now let's step back and
take a look at what gave rise to convolutional neural
networks specifically. And so we can go back to the 1950s, where Hubel and Wiesel did
a series of experiments trying to understand how neurons in the visual cortex worked, and they studied this
specifically for cats. And so we talked a little bit
about this in lecture one, but basically in these
experiments they put electrodes in the cat, into the cat brain, and they gave the cat
different visual stimulus. Right, and so, things like, you know, different kinds of edges, oriented edges, different sorts of
shapes, and they measured the response of the
neurons to these stimuli. And so there were a couple
of important conclusions that they were able to
make, and observations. And so the first thing
found that, you know, there's sort of this topographical
mapping in the cortex. So nearby cells in the
cortex also represent nearby regions in the visual field. And so you can see for
example, on the right here where if you take kind
of the spatial mapping and map this onto a visual cortex there's more peripheral
regions are these blue areas, you know, farther away from the center. And so they also discovered
that these neurons had a hierarchical organization. And so if you look at different
types of visual stimuli they were able to find
that at the earliest layers retinal ganglion cells
were responsive to things that looked kind of like
circular regions of spots. And then on top of that
there are simple cells, and these simple cells are
responsive to oriented edges, so different orientation
of the light stimulus. And then going further,
they discover that these were then connected to more complex cells, which were responsive to
both light orientation as well as movement, and so on. And you get, you know,
increasing complexity, for example, hypercomplex
cells are now responsive to movement with kind
of an endpoint, right, and so now you're starting
to get the idea of corners and then blobs and so on. And so then in 1980, the neocognitron
was the first example of a network architecture, a model, that had this idea of
simple and complex cells that Hubel and Wiesel had discovered. And in this case Fukushima put these into these alternating layers of
simple and complex cells, where you had these simple cells that had modifiable parameters,
and then complex cells on top of these that
performed a sort of pooling so that it was invariant to, you know, different minor modifications
from the simple cells. And so this is work that
was in the 1980s, right, and so by 1998 Yann LeCun basically showed the first example of applying backpropagation
and gradient-based learning to train convolutional neural networks that did really well on
document recognition. And specifically they
were able to do a good job of recognizing digits of zip codes. And so these were then used pretty widely for zip code recognition
in the postal service. But beyond that it
wasn't able to scale yet to more challenging and
complex data, right, digits are still fairly simple and a limited set to recognize. And so this is where
Alex Krizhevsky, in 2012, gave the modern incarnation of
convolutional neural networks and his network we sort of
colloquially call AlexNet. But this network really
didn't look so much different than the convolutional neural networks that Yann LeCun was dealing with. They're now, you know,
they were scaled now to be larger and deeper and able to, the most important parts
were that they were now able to take advantage of
the large amount of data that's now available, in web
images, in ImageNet data set. As well as take advantage of the parallel computing power in GPUs. And so we'll talk more about that later. But fast forwarding
today, so now, you know, ConvNets are used everywhere. And so we have the initial
classification results on ImageNet from Alex Krizhevsky. This is able to do a really
good job of image retrieval. You can see that when we're
trying to retrieve a flower for example, the features that are learned are really powerful for
doing similarity matching. We also have ConvNets that
are used for detection. So we're able to do a really
good job of localizing where in an image is, for
example, a bus, or a boat, and so on, and draw precise
bounding boxes around that. We're able to go even deeper
beyond that to do segmentation, right, and so these are now richer tasks where we're not looking
for just the bounding box but we're actually going
to label every pixel in the outline of, you know,
trees, and people, and so on. And these kind of algorithms are used in, for example, self-driving cars, and a lot of this is powered
by GPUs as I mentioned earlier, that's able to do parallel processing and able to efficiently
train and run these ConvNets. And so we have modern
powerful GPUs as well as ones that work in embedded
systems, for example, that you would use in a self-driving car. So we can also look at some
of the other applications that ConvNets are used for. So, face-recognition, right,
we can put an input image of a face and get out a
likelihood of who this person is. ConvNets are applied to video,
and so this is an example of a video network that
looks at both images as well as temporal information, and from there is able to classify videos. We're also able to do pose recognition. Being able to recognize, you know, shoulders, elbows, and different joints. And so here are some images
of our fabulous TA, Lane, in various kinds of pretty
non-standard human poses. But ConvNets are able
to do a pretty good job of pose recognition these days. They're also used in game playing. So some of the work in
reinforcement learning, deeper enforcement learning
that you may have seen, playing Atari games, and Go, and so on, and ConvNets are an important
part of all of these. Some other applications,
so they're being used for interpretation and
diagnosis of medical images, for classification of galaxies,
for street sign recognition. There's also whale recognition, this is from a recent Kaggle Challenge. We also have examples of
looking at aerial maps and being able to draw
out where are the streets on these maps, where are buildings, and being able to segment all of these. And then beyond recognition
of classification detection, these types of tasks, we also have tasks like image captioning,
where given an image, we want to write a sentence description about what's in the image. And so this is something
that we'll go into a little bit later in the class. And we also have, you know,
really, really fancy and cool kind of artwork that we can
do using neural networks. And so on the left is an
example of a deep dream, where we're able to take
images and kind of hallucinate different kinds of objects
and concepts in the image. There's also neural style type
work, where we take an image and we're able to re-render this image using a style of a particular
artist and artwork, right. And so here we can take, for
example, Van Gogh on the right, Starry Night, and use that to redraw our original image using that style. And Justin has done a lot of work in this and so if you guys are interested, these are images produced
by some of his code and you guys should talk
to him more about it. Okay, so basically, you know,
this is just a small sample of where ConvNets are being used today. But there's really a huge amount
that can be done with this, right, and so, you know,
for you guys' projects, sort of, you know, let
your imagination go wild and we're excited to see
what sorts of applications you can come up with. So today we're going to talk about how convolutional neural networks work. And again, same as with neural
networks, we're going to first talk about how they work
from a functional perspective without any of the brain analogies. And then we'll talk briefly
about some of these connections. Okay, so, last lecture, we talked about this idea of a fully connected layer. And how, you know, for
a fully connected layer what we're doing is we operate
on top of these vectors, right, and so let's say we
have, you know, an image, a 3D image, 32 by 32 by three, so some of the images that we
were looking at previously. We'll take that, we'll stretch
all of the pixels out, right, and then we have this
3072 dimensional vector, for example in this case. And then we have these weights, right, so we're going to multiply
this by a weight matrix. And so here for example our W
we're going to say is 10 by 3072. And then we're going
to get the activations, the output of this layer,
right, and so in this case, we take each of our 10 rows
and we do this dot product with 3072 dimensional input. And from there we get this one number that's kind of the value of that neuron. And so in this case we're going to have 10 of these neuron outputs. And so a convolutional
layer, so the main difference between this and the fully connected layer that we've been talking about is that here we want to
preserve spatial structure. And so taking this 32 by 32 by three image that we had earlier, instead
of stretching this all out into one long vector, we're
now going to keep the structure of this image, right, this
three dimensional input. And then what we're going to do is our weights are going to
be these small filters, so in this case for example, a
five by five by three filter, and we're going to take this filter and we're going to slide
it over the image spatially and compute dot products
at every spatial location. And so we're going to go into
detail of exactly how this works. So, our filters, first of all, always extend the full
depth of the input volume. And so they're going to be
just a smaller spatial area, so in this case five by five, right, instead of our full 32
by 32 spatial input, but they're always going to go
through the full depth, right, so here we're going to
take five by five by three. And then we're going to take this filter and at a given spatial location we're going to do a dot product between this filter and
then a chunk of a image. So we're just going to overlay this filter on top of a spatial location in the image, right, and then do the dot product, the multiplication of each
element of that filter with each corresponding element
in that spatial location that we've just plopped it on top of. And then this is going
to give us a dot product. So in this case, we have
five times five times three, this is the number of multiplications that we're going to do,
right, plus the bias term. And so this is basically
taking our filter W and basically doing W transpose
times X and plus bias. So is that clear how this works? Yeah, question. [faint speaking] Yeah, so the question is,
when we do the dot product do we turn the five by five
by three into one vector? Yeah, in essence that's what you're doing. You can, I mean, you
can think of it as just plopping it on and doing the
element-wise multiplication at each location, but this is
going to give you the same result as if you stretched out
the filter at that point, stretched out the input
volume that it's laid over, and then took the dot product, and that's what's written
here, yeah, question. [faint speaking] Oh, this is, so the question is, any intuition for why
this is a W transpose? And this was just, not really, this is just the notation
that we have here to make the math work
out as a dot product. So it just depends on whether,
how you're representing W and whether in this case
if we look at the W matrix this happens to be each column
and so we're just taking the transpose to get a row out of it. But there's no intuition here, we're just taking the filters of W and we're stretching it
out into a one D vector, and in order for it to be a dot product it has to be like a one
by, one by N vector. [faint speaking] Okay, so the question is, is W here not five by five
by three, it's one by 75. So that's the case, right, if we're going to do this dot product
of W transpose times X, we have to stretch it out first before we do the dot product. So we take the five by five by three, and we just take all these values and stretch it out into a long vector. And so again, similar
to the other question, the actual operation that we're doing here is plopping our filter on top of a spatial location in the image and multiplying all of the
corresponding values together, but in order just to make it
kind of an easy expression similar to what we've seen before we can also just stretch
each of these out, make sure that dimensions
are transposed correctly so that it works out as a dot product. Yeah, question. [faint speaking] Okay, the question is, how do we slide the filter over the image. We'll go into that next, yes. [faint speaking] Okay, so the question is,
should we rotate the kernel by 180 degrees to better
match the convolution, the definition of a convolution. And so the answer is that
we'll also show the equation for this later, but
we're using convolution as kind of a looser definition
of what's happening. So for people from signal processing, what we are actually technically doing, if you want to call this a convolution, is we're convolving with the
flipped version of the filter. But for the most part, we
just don't worry about this and we just, yeah, do this operation and it's like a convolution in spirit. Okay, so... Okay, so we had a question
earlier, how do we, you know, slide this over all the spatial locations. Right, so what we're going to do is we're going to take this
filter, we're going to start at the upper left-hand
corner and basically center our filter on top of every
pixel in this input volume. And at every position, we're
going to do this dot product and this will produce one value in our output activation map. And so then we're going
to just slide this around. The simplest version
is just at every pixel we're going to do this
operation and fill in the corresponding point
in our output activation. You can see here that the
dimensions are not exactly what would happen, right,
if you're going to do this. I had 32 by 32 in the input and I'm having 28 by 28 in the output, and so we'll go into
examples later of the math of exactly how this is going
to work out dimension-wise, but basically you have a choice of how you're going to slide this, whether you go at every
pixel or whether you slide, let's say, you know, two
input values over at a time, two pixels over at a time, and so you can get different size outputs depending on how you choose to slide. But you're basically doing this
operation in a grid fashion. Okay, so what we just saw earlier, this is taking one filter, sliding it over all of the spatial locations in the image and then we're going to get
this activation map out, right, which is the value of that
filter at every spatial location. And so when we're dealing
with a convolutional layer, we want to work with
multiple filters, right, because each filter is kind
of looking for a specific type of template or concept
in the input volume. And so we're going to have
a set of multiple filters, and so here I'm going
to take a second filter, this green filter, which is
again five by five by three, I'm going to slide this over
all of the spatial locations in my input volume, and
then I'm going to get out this second green activation
map also of the same size. And we can do this for as many filters as we want to have in this layer. So for example, if we have six filters, six of these five by five filters, then we're going to get in
total six activation maps out. All of, so we're going
to get this output volume that's going to be
basically six by 28 by 28. Right, and so a preview
of how we're going to use these convolutional layers
in our convolutional network is that our ConvNet is
basically going to be a sequence of these convolutional layers stacked on top of each other,
same way as what we had with the simple linear layers
in their neural network. And then we're going to intersperse these with activation functions, so for example, a ReLU
activation function. Right, and so you're going to
get something like Conv, ReLU, and usually also some pooling layers, and then you're just going
to get a sequence of these each creating an output
that's now going to be the input to the next convolutional layer. Okay, and so each of these
layers, as I said earlier, has multiple filters, right, many filters. And each of the filter is
producing an activation map. And so when you look at
multiple of these layers stacked together in a ConvNet,
what ends up happening is you end up learning this
hierarching of filters, where the filters at the
earlier layers usually represent low-level features that
you're looking for. So things kind of like edges, right. And then at the mid-level, you're going to get more
complex kinds of features, so maybe it's looking more for things like corners and blobs and so on. And then at higher-level features, you're going to get
things that are starting to more resemble concepts than blobs. And we'll go into more
detail later in the class in how you can actually
visualize all these features and try and interpret what your network, what kinds of features
your network is learning. But the important thing for
now is just to understand that what these features end up being when you have a whole stack of these, is these types of simple
to more complex features. [faint speaking] Yeah. Oh, okay. Oh, okay, so the question
is, what's the intuition for increasing the depth each time. So here I had three filters
in the original layer and then six filters in the next layer. Right, and so this is
mostly a design choice. You know, people in practice have found certain types of these
configurations to work better. And so later on we'll go into
case studies of different kinds of convolutional
neural network architectures and design choices for these and why certain ones
work better than others. But yeah, basically the choice of, you're going to have many design choices in a convolutional neural network, the size of your filter, the stride, how many filters you have, and so we'll talk about
this all more later. Question. [faint speaking] Yeah, so the question is,
as we're sliding this filter over the image spatially it
looks like we're sampling the edges and corners less
than the other locations. Yeah, that's a really good point, and we'll talk I think in a few slides about how we try and compensate for that. Okay, so each of these
convolutional layers that we have stacked together,
we saw how we're starting with more simpler features
and then aggregating these into more complex features later on. And so in practice this is compatible with what Hubel and Wiesel
noticed in their experiments, right, that we had these simple cells at the earlier stages of processing, followed by more complex cells later on. And so even though we didn't explicitly force our ConvNet to learn
these kinds of features, in practice when you give it this type of hierarchical structure and
train it using backpropagation, these are the kinds of filters
that end up being learned. [faint speaking] Okay, so yeah, so the question is, what are we seeing in
these visualizations. And so, alright so, in
these visualizations, like, if we look at this Conv1, the
first convolutional layer, each of these grid, each part
of this grid is a one neuron. And so what we've visualized here is what the input looks
like that maximizes the activation of that particular neuron. So what sort of image you would get that would give you the largest value, make that neuron fire and
have the largest value. And so the way we do this is basically by doing backpropagation from
a particular neuron activation and seeing what in the input will trigger, will give you the highest
values of this neuron. And this is something
that we'll talk about in much more depth in a later lecture about how we create all
of these visualizations. But basically each element of these grids is showing what in the
input would look like that basically maximizes the
activation of the neuron. So in a sense, what is
the neuron looking for? Okay, so here is an example
of some of the activation maps produced by each filter, right. So we can visualize up here on the top we have this whole row of
example five by five filters, and so this is basically a real
case from a trained ConvNet where each of these is
what a five by five filter looks like, and then as we
convolve this over an image, so in this case this I think
it's like a corner of a car, the car light, what the
activation looks like. Right, and so here for example, if we look at this first
one, this red filter, filter like with a red box around it, we'll see that it's looking for, the template looks like an
edge, right, an oriented edge. And so if you slide it over the image, it'll have a high value,
a more white value where there are edges in
this type of orientation. And so each of these activation
maps is kind of the output of sliding one of these filters over and where these filters
are causing, you know, where this sort of template
is more present in the image. And so the reason we call
these convolutional is because this is related to the
convolution of two signals, and so someone pointed out earlier that this is basically this
convolution equation over here, for people who have
seen convolutions before in signal processing, and in practice it's actually more like a correlation where we're convolving
with the flipped version of the filter, but this
is kind of a subtlety, it's not really important for
the purposes of this class. But basically if you're
writing out what you're doing, it has an expression that
looks something like this, which is the standard
definition of a convolution. But this is basically
just taking a filter, sliding it spatially over the image and computing the dot
product at every location. Okay, so you know, as I
had mentioned earlier, like what our total
convolutional neural network is going to look like is we're
going to have an input image, and then we're going to pass it through this sequence of layers, right, where we're going to have a
convolutional layer first. We usually have our
non-linear layer after that. So ReLU is something
that's very commonly used that we're going to talk about more later. And then we have these Conv,
ReLU, Conv, ReLU layers, and then once in a while
we'll use a pooling layer that we'll talk about later as well that basically downsamples the
size of our activation maps. And then finally at the end
of this we'll take our last convolutional layer output
and then we're going to use a fully connected layer
that we've seen before, connected to all of these
convolutional outputs, and use that to get a final score function basically like what we've
already been working with. Okay, so now let's work out some examples of how the spatial dimensions work out. So let's take our 32 by 32
by three image as before, right, and we have our five
by five by three filter that we're going to slide over this image. And we're going to see how
we're going to use that to produce exactly this
28 by 28 activation map. So let's assume that we actually
have a seven by seven input just to be simpler, and let's assume we have a three by three filter. So what we're going to do is we're going to take this filter, plop it down in our
upper left-hand corner, right, and we're going to
multiply, do the dot product, multiply all these values
together to get our first value, and this is going to go into
the upper left-hand value of our activation map. Right, and then what
we're going to do next is we're just going to take this filter, slide it one position to the right, and then we're going to get
another value out from here. And so we can continue with
this to have another value, another, and in the end
what we're going to get is a five by five output, right, because what fit was
basically sliding this filter a total of five spatial
locations horizontally and five spatial locations vertically. Okay, so as I said before there's different kinds of
design choices that we can make. Right, so previously I
slid it at every single spatial location and the
interval at which I slide I'm going to call the stride. And so previously we
used the stride of one. And so now let's see what happens if we have a stride of two. Right, so now we're going
to take our first location the same as before, and
then we're going to skip this time two pixels over
and we're going to get our next value centered at this location. Right, and so now if
we use a stride of two, we have in total three
of these that can fit, and so we're going to get
a three by three output. Okay, and so what happens when
we have a stride of three, what's the output size of this? And so in this case, right, we have three, we slide it over by three again, and the problem is that here
it actually doesn't fit. Right, so we slide it over by three and now it doesn't fit
nicely within the image. And so what we in practice we
just, it just doesn't work. We don't do convolutions like this because it's going to lead to
asymmetric outputs happening. Right, and so just kind
of looking at the way that we computed how many, what
the output size is going to be, this actually can work into a nice formula where we take our
dimension of our input N, we have our filter size
F, we have our stride at which we're sliding along,
and our final output size, the spatial dimension of each output size is going to be N minus F
divided by the stride plus one, right, and you can kind of
see this as a, you know, if I'm going to take my
filter, let's say I fill it in at the very last possible
position that it can be in and then take all the pixels before that, how many instances of moving
by this stride can I fit in. Right, and so that's how this
equation kind of works out. And so as we saw before,
right, if we have N equal seven and F equals three, if
we want a stride of one we plug it into this
formula, we get five by five as we had before, and the
same thing we had for two. And with a stride of three,
this doesn't really work out. And so in practice it's actually common to zero pad the borders in order to make the size work out to what we want it to. And so this is kind of
related to a question earlier, which is what do we do,
right, at the corners. And so what in practice happens is we're going to actually pad
our input image with zeros and so now you're going to
be able to place a filter centered at the upper
right-hand pixel location of your actual input image. Okay, so here's a question,
so who can tell me if I have my same input, seven by seven, three by three filter, stride one, but now I pad with a one pixel border, what's the size of my output going to be? [faint speaking] So, I heard some sixes, heard some sev, so remember we have this
formula that we had before. So if we plug in N is equal
to seven, F is equal to three, right, and then our
stride is equal to one. So what we actually get, so
actually this is giving us seven, four, so seven
minus three is four, divided by one plus one is five. And so this is what we had before. So we actually need to adjust
this formula a little bit, right, so this was actually,
this formula is the case where we don't have zero padded pixels. But if we do pad it, then if
you now take your new output and you slide it along, you'll see that actually
seven of the filters fit, so you get a seven by seven output. And plugging in our
original formula, right, so our N now is not seven, it's nine, so if we go back here
we have N equals nine minus a filter size of
three, which gives six. Right, divided by our
stride, which is one, and so still six, and then
plus one we get seven. Right, and so once you've padded it you want to incorporate this
padding into your formula. Yes, question. [faint speaking] Seven, okay, so the question is, what's the actual output of the size, is it seven by seven or
seven by seven by three? The output is going to be seven by seven by the number of filters that you have. So remember each filter is
going to do a dot product through the entire depth
of your input volume. But then that's going to
produce one number, right, so each filter is, let's
see if we can go back here. Each filter is producing
a one by seven by seven in this case activation map
output, and so the depth is going to be the number
of filters that we have. [faint speaking] Sorry, let me just, one second go back. Okay, can you repeat your question again? [muffled speaking] Okay, so the question is, how
does this connect to before when we had a 32 by 32
by three input, right. So our input had depth
and here in this example I'm showing a 2D example with no depth. And so yeah, I'm showing
this for simplicity but in practice you're going to take your, you're going to multiply
throughout the entire depth as we had before, so you're going to, your filter is going to be
in this case a three be three spatial filter by whatever
input depth that you had. So three by three by three in this case. Yeah, everything else stays the same. Yes, question. [muffled speaking] Yeah, so the question
is, does the zero padding add some sort of extraneous
features at the corners? And yeah, so I mean, we're
doing our best to still, get some value and do, like, process that region of the image, and so zero padding is
kind of one way to do this, where I guess we can, we are detecting part of this template in this region. There's also other ways
to do this that, you know, you can try and like,
mirror the values here or extend them, and so it
doesn't have to be zero padding, but in practice this is one
thing that works reasonably. And so, yeah, so there is a
little bit of kind of artifacts at the edge and we sort of just, you do your best to deal with it. And in practice this works reasonably. I think there was another question. Yeah, question. [faint speaking] So if we have non-square
images, do we ever use a stride that's different
horizontally and vertically? So, I mean, there's nothing
stopping you from doing that, you could, but in practice we just usually take the same stride, we
usually operate square regions and we just, yeah we usually just take the same stride everywhere
and it's sort of like, in a sense it's a little bit like, it's a little bit like the
resolution at which you're, you know, looking at this image, and so usually there's kind
of, you might want to match sort of your horizontal
and vertical resolutions. But, yeah, so in practice you could but really people don't do that. Okay, another question. [faint speaking] So the question is, why
do we do zero padding? So the way we do zero padding is to maintain the same
input size as we had before. Right, so we started with seven by seven, and if we looked at just
starting your filter from the upper left-hand
corner, filling everything in, right, then we get a smaller size output, but we would like to maintain
our full size output. Okay, so, yeah, so we saw how padding
can basically help you maintain the size of the
output that you want, as well as apply your filter at these, like, corner regions and edge regions. And so in general in terms of choosing, you know, your stride, your
filter, your filter size, your stride size, zero
padding, what's common to see is filters of size three
by three, five by five, seven by seven, these are
pretty common filter sizes. And so each of these, for three by three you will want to zero pad with one in order to maintain
the same spatial size. If you're going to do five by five, you can work out the math,
but it's going to come out to you want to zero pad by two. And then for seven you
want to zero pad by three. Okay, and so again you
know, the motivation for doing this type of zero padding and trying to maintain
the input size, right, so we kind of alluded to this before, but if you have multiple of
these layers stacked together... So if you have multiple of
these layers stacked together you'll see that, you know,
if we don't do this kind of zero padding, or any kind of padding, we're going to really
quickly shrink the size of the outputs that we have. Right, and so this is not
something that we want. Like, you can imagine if you
have a pretty deep network then very quickly your, the
size of your activation maps is going to shrink to
something very small. And this is bad both because
we're kind of losing out on some of this information, right, now you're using a much
smaller number of values in order to represent your original image, so you don't want that. And then at the same time also as we talked about this earlier, your also kind of losing sort of some of
this edge information, corner information that each time we're losing out and
shrinking that further. Okay, so let's go through
a couple more examples of computing some of these sizes. So let's say that we have an input volume which is 32 by 32 by three. And here we have 10 five by five filters. Let's use stride one and pad two. And so who can tell me what's the output volume size of this? So you can think about
the formula earlier. Sorry, what was it? [faint speaking] 32 by 32 by 10, yes that's correct. And so the way we can see this, right, is so we have our input size, F is 32. Then in this case we want to augment it by the padding that we added onto this. So we padded it two in
each dimension, right, so we're actually going to get,
total width and total height's going to be 32 plus four on each side. And then minus our filter size five, divided by one plus one and we get 32. So our output is going to
be 32 by 32 for each filter. And then we have 10 filters total, so we have 10 of these activation maps, and our total output volume
is going to be 32 by 32 by 10. Okay, next question, so what's the number of
parameters in this layer? So remember we have 10
five by five filters. [faint speaking] I kind of heard something,
but it was quiet. Can you guys speak up? 250, okay so I heard 250, which is close, but remember that we're
also, our input volume, each of these filters
goes through by depth. So maybe this wasn't clearly written here because each of the filters
is five by five spatially, but implicitly we also have
the depth in here, right. It's going to go through the whole volume. So I heard, yeah, 750 I heard. Almost there, this is
kind of a trick question 'cause also remember
we usually always have a bias term, right, so
in practice each filter has five by five by three
weights, plus our one bias term, we have 76 parameters per filter, and then we have 10 of these total, and so there's 760 total parameters. Okay, and so here's just a summary of the convolutional layer
that you guys can read a little bit more carefully later on. But we have our input volume
of a certain dimension, we have all of these choice,
we have our filters, right, where we have number of
filters, the filter size, the stride of the size,
the amount of zero padding, and you basically can use all of these, go through the computations
that we talked about earlier in order to find out what
your output volume is actually going to be and how many total
parameters that you have. And so some common settings of this. You know, we talked earlier
about common filter sizes of three by three, five by five. Stride is usually one
and two is pretty common. And then your padding P is
going to be whatever fits, like, whatever will
preserve your spatial extent is what's common. And then the total number of filters K, usually we use powers of two
just to be nice, so, you know, 32, 64, 128 and so on, 512, these are pretty common
numbers that you'll see. And just as an aside, we can also do a one by one convolution, this still makes perfect sense where given a one by one convolution we still slide it over
each spatial extent, but now, you know, the spatial region is not really five by five it's just kind of the
trivial case of one by one, but we are still having this filter go through the entire depth. Right, so this is going
to be a dot product through the entire depth
of your input volume. And so the output here, right,
if we have an input volume of 56 by 56 by 64 depth and
we're going to do one by one convolution with 32 filters,
then our output is going to be 56 by 56 by our number of filters, 32. Okay, and so here's an example
of a convolutional layer in TORCH, a deep learning framework. And so you'll see that,
you know, last lecture we talked about how you can go into these deep learning frameworks,
you can see these definitions of each layer, right,
where they have kind of the forward pass and the backward pass implemented for each layer. And so you'll see convolutions, spatial convolution is going
to be just one of these, and then the arguments
that it's going to take are going to be all of these
design choices of, you know, I mean, I guess your
input and output sizes, but also your choices of
like your kernel width, your kernel size, padding,
and these kinds of things. Right, and so if we look at
another framework, Caffe, you'll see something very similar, where again now when you're
defining your network you define networks in Caffe
using this kind of, you know, proto text file where you're specifying each of your design choices for your layer and you can see for a convolutional layer will say things like, you
know, the number of outputs that we have, this is going
to be the number of filters for Caffe, as well as the kernel
size and stride and so on. Okay, and so I guess before I go on, any questions about convolution, how the convolution operation works? Yes, question. [faint speaking] Yeah, so the question is, what's the intuition behind
how you choose your stride. And so at one sense it's
kind of the resolution at which you slide it on, and
usually the reason behind this is because when we have a larger stride what we end up getting as the output is a down sampled image, right, and so what this downsampled
image lets us have is both, it's a way, it's kind of
like pooling in a sense but it's just a different
and sometimes works better way of doing pooling is one
of the intuitions behind this, 'cause you get the same effect
of downsampling your image, and then also as you're doing
this you're reducing the size of the activation maps
that you're dealing with at each layer, right, and so
this also affects later on the total number of
parameters that you have because for example at the
end of all your Conv layers, now you might put on fully
connected layers on top, for example, and now the
fully connected layer's going to be connected to every value of your convolutional output, right, and so a smaller one will
give you smaller number of parameters, and so now
you can get into, like, basically thinking about
trade offs of, you know, number of parameters you
have, the size of your model, overfitting, things
like that, and so yeah, these are kind of some of the things that you want to think about
with choosing your stride. Okay, so now if we look a
little bit at kind of the, you know, brain neuron view
of a convolutional layer, similar to what we
looked at for the neurons in the last lecture. So what we have is that
at every spatial location, we take a dot product between a filter and a specific part of the image, right, and we get one number out from here. And so this is the same idea of doing these types
of dot products, right, taking your input, weighting
it by these Ws, right, values of your filter, these
weights that are the synapses, and getting a value out. But the main difference
here is just that now your neuron has local connectivity. So instead of being connected
to the entire input, it's just looking at a local
region spatially of your image. And so this looks at a local region and then now you're going
to get kind of, you know, this, how much this
neuron is being triggered at every spatial location in your image. Right, so now you preserve
the spatial structure and you can say, you
know, be able to reason on top of these kinds of
activation maps in later layers. And just a little bit of terminology, again for, you know, we have
this five by five filter, we can also call this a
five by five receptive field for the neuron, because this is, the receptive field is
basically the, you know, input field that this field of vision that this neuron is receiving, right, and so that's just another common term that you'll hear for this. And then again remember each
of these five by five filters we're sliding them over
the spatial locations but they're the same set of weights, they share the same parameters. Okay, and so, you know, as we talked about what we're going to get at this output is going to be this volume, right, where spatially we have,
you know, let's say 28 by 28 and then our number of
filters is the depth. And so for example with five filters, what we're going to
get out is this 3D grid that's 28 by 28 by five. And so if you look at the filters across in one spatial location
of the activation volume and going through depth
these five neurons, all of these neurons, basically the way you can interpret this is they're all looking at the same region in the input volume, but they're just looking
for different things, right. So they're different filters applied to the same spatial
location in the image. And so just a reminder
again kind of comparing with the fully connected layer
that we talked about earlier. In that case, right, if we
look at each of the neurons in our activation or
output, each of the neurons was connected to the
entire stretched out input, so it looked at the
entire full input volume, compared to now where each one just looks at this local spatial region. Question. [muffled talking] Okay, so the question
is, within a given layer, are the filters completely symmetric? So what do you mean by
symmetric exactly, I guess? Right, so okay, so the
filters, are the filters doing, they're doing the same dimension,
the same calculation, yes. Okay, so is there anything different other than they have the
same parameter values? No, so you're exactly right, we're just taking a filter
with a given set of, you know, five by five by three parameter values, and we just slide this
in exactly the same way over the entire input volume
to get an activation map. Okay, so you know, we've
gone into a lot of detail in what these convolutional
layers look like, and so now I'm just going to go briefly through the other layers that we have that form this entire
convolutional network. Right, so remember again,
we have convolutional layers interspersed with pooling
layers once in a while as well as these non-linearities. Okay, so what the pooling layers do is that they make the representations smaller and more manageable, right, so we talked about this earlier with someone asked a question of
why we would want to make the representation smaller. And so this is again for it to have fewer, it effects the number of
parameters that you have at the end as well as basically does some, you know, invariance over a given region. And so what the pooling layer does is it does exactly just downsamples, and it takes your input
volume, so for example, 224 by 224 by 64, and
spatially downsamples this. So in the end you'll get out 112 by 112. And it's important to note
this doesn't do anything in the depth, right, we're
only pooling spatially. So the number of, your input depth is going to be the same
as your output depth. And so, for example, a common
way to do this is max pooling. So in this case our pooling
layer also has a filter size and this filter size is
going to be the region at which we pool over,
right, so in this case if we have two by two filters,
we're going to slide this, and so, here, we also have
stride two in this case, so we're going to take this filter and we're going to slide
it along our input volume in exactly the same way
as we did for convolution. But here instead of
doing these dot products, we just take the maximum value of the input volume in that region. Right, so here if we
look at the red values, the value of that will
be six is the largest. If we look at the greens
it's going to give an eight, and then we have a three and a four. Yes, question. [muffled speaking] Yeah, so the question is, is
it typical to set up the stride so that there isn't an overlap? And yeah, so for the pooling layers it is, I think the more common thing to do is to have them not have any overlap, and I guess the way you
can think about this is basically we just want to downsample and so it makes sense to
kind of look at this region and just get one value
to represent this region and then just look at the
next region and so on. Yeah, question. [faint speaking] Okay, so the question
is, why is max pooling better than just taking the, doing something like average pooling? Yes, that's a good point,
like, average pooling is also something that you can do, and intuition behind why
max pooling is commonly used is that it can have
this interpretation of, you know, if this is, these
are activations of my neurons, right, and so each value is kind of how much this neuron
fired in this location, how much this filter
fired in this location. And so you can think of
max pooling as saying, you know, giving a signal of
how much did this filter fire at any location in this image. Right, and if we're
thinking about detecting, you know, doing recognition, this might make some intuitive
sense where you're saying, well, you know, whether a
light or whether some aspect of your image that you're looking for, whether it happens anywhere in this region we want to fire at with a high value. Question. [muffled speaking] Yeah, so the question is,
since pooling and stride both have the same effect of downsampling, can you just use stride
instead of pooling and so on? Yeah, and so in practice I think looking at more recent
neural network architectures people have begun to use stride more in order to do the downsampling
instead of just pooling. And I think this gets into
things like, you know, also like fractional strides
and things that you can do. But in practice this in a
sense maybe has a little bit better way to get better
results using that, so. Yeah, so I think using
stride is definitely, you can do it and people are doing it. Okay, so let's see, where were we. Okay, so yeah, so with
these pooling layers, so again, there's right, some
design choices that you make, you take this input volume of W by H by D, and then you're going to
set your hyperparameters for design choices of your filter size or the spatial extent over
which you are pooling, as well as your stride, and
then you can again compute your output volume using the
same equation that you used earlier for convolution, it
still applies here, right, so we still have our W total extent minus filter size divided
by stride plus one. Okay, and so just one other thing to note, it's also, typically people
don't really use zero padding for the pooling layers
because you're just trying to do a direct downsampling, right, so there isn't this problem of like, applying a filter at the corner and having some part of the
filter go off your input volume. And so for pooling we don't
usually have to worry about this and we just directly downsample. And so some common settings
for the pooling layer is a filter size of two by
two or three by three strides. Two by two, you know, you can have, also you can still have
pooling of two by two even with a filter size of three by three, I think someone asked that earlier, but in practice it's pretty
common just to have two by two. Okay, so now we've talked about
these convolutional layers, the ReLU layers were the
same as what we had before with the, you know, just
the base neural network that we talked about last lecture. So we intersperse these and
then we have a pooling layer every once in a while when we
feel like downsampling, right. And then the last thing is that at the end we want to have a fully connected layer. And so this will be just exactly the same as the fully connected layers
that you've seen before. So in this case now what we do is we take the convolutional
network output, at the last layer we have some volume, so we're going to have width
by height by some depth, and we just take all of these and we essentially just
stretch these out, right. And so now we're going
to get the same kind of, you know, basically 1D
input that we're used to for a vanilla neural network,
and then we're going to apply this fully connected layer on top, so now we're going to have connections to every one of these
convolutional map outputs. And so what you can think
of this is basically, now instead of preserving, you know, before we were preserving
spatial structure, right, and so but at the
last layer at the end, we want to aggregate all of this together and we want to reason basically on top of all of this as we had before. And so what you get from that is just our score outputs as we had earlier. Okay, so-- - [Student] This is
sort of a silly question about this visual. Like what are the 16 pixels
that are on the far right, like what should be interpreting those as? - Okay, so the question
is, what are the 16 pixels that are on the far
right, do you mean the-- - [Student] Like that column of-- - [Instructor] Oh, each column. - [Student] The column
on the far right, yeah. - [Instructor] The green
ones or the black ones? - [Student] The ones labeled pool. - The one with hold on, pool. Oh, okay, yeah, so the question is how do we interpret this column,
right, for example at pool. And so what we're showing
here is each of these columns is the output activation maps, right, the output from one of these layers. And so starting from the
beginning, we have our car, after the convolutional layer we now have these activation
maps of each of the filters slid spatially over the input image. Then we pass that through a ReLU, so you can see the values
coming out from there. And then going all the way over, and so what you get for the pooling layer is that it's really just taking the output of the ReLU layer that came just before it
and then it's pooling it. So it's going to downsample it, right, and then it's going to take the max value in each filter location. And so now if you look at
this pool layer output, like, for example, the last
one that you were mentioning, it looks the same as this ReLU output except that it's downsampled
and that it has this kind of max value at every spatial location and so that's the minor difference that you'll see between those two. [distant speaking] So the question is, now this looks like just a very small amount
of information, right, so how can it know to
classify it from here? And so the way that you
should think about this is that each of these values inside one of these pool
outputs is actually, it's the accumulation of all
the processing that you've done throughout this entire network, right. So it's at the very top of your hierarchy, and so each actually represents kind of a higher level concept. So we saw before, you know,
for example, Hubel and Wiesel and building up these
hierarchical filters, where at the bottom level
we're looking for edges, right, or things like very simple
structures, like edges. And so after your convolutional layer the outputs that you see
here in this first column is basically how much do
specific, for example, edges, fire at different locations in the image. But then as you go through
you're going to get more complex, it's looking for more
complex things, right, and so the next convolutional layer is going to fire at how much, you know, let's say certain kinds of
corners show up in the image, right, because it's reasoning. Its input is not the original image, its input is the output, it's
already the edge maps, right, so it's reasoning on top of edge maps, and so that allows it to get more complex, detect more complex things. And so by the time you get all the way up to this last pooling layer,
each value is representing how much a relatively complex
sort of template is firing. Right, and so because of
that now you can just have a fully connected layer,
you're just aggregating all of this information together to get, you know, a score for your class. So each of these values is how much a pretty complicated
complex concept is firing. Question. [faint speaking] So the question is, when
do you know you've done enough pooling to do the classification? And the answer is you just try and see. So in practice, you know,
these are all design choices and you can think about this
a little bit intuitively, right, like you want to pool
but if you pool too much you're going to have very few values representing your entire image and so on, so it's just kind of a trade off. Something reasonable
versus people have tried a lot of different configurations so you'll probably cross validate, right, and try over different pooling sizes, different filter sizes,
different number of layers, and see what works best for
your problem because yeah, like every problem with
different data is going to, you know, different set of these sorts of hyperparameters might work best. Okay, so last thing, just
wanted to point you guys to this demo of training a ConvNet, which was created by Andre Karpathy, the originator of this class. And so he wrote up this demo where you can basically
train a ConvNet on CIFAR-10, the dataset that we've seen
before, right, with 10 classes. And what's nice about
this demo is you can, it basically plots for you
what each of these filters look like, what the
activation maps look like. So some of the images I showed earlier were taken from this demo. And so you can go try it
out, play around with it, and you know, just go through
and try and get a sense for what these activation maps look like. And just one thing to note, usually the first layer
activation maps are, you can interpret them, right, because they're operating
directly on the input image so you can see what these templates mean. As you get to higher level layers it starts getting really hard, like how do you actually
interpret what do these mean. So for the most part it's
just hard to interpret so you shouldn't, you know, don't worry if you can't really make
sense of what's going on. But it's still nice just
to see the entire flow and what outputs are coming out. Okay, so in summary, so
today we talked about how convolutional neural networks work, how they're basically stacks of these convolutional and pooling layers followed by fully connected
layers at the end. There's been a trend towards
having smaller filters and deeper architectures,
so we'll talk more about case studies for
some of these later on. There's also been a trend
towards getting rid of these pooling and fully
connected layers entirely. So just keeping these, just
having, you know, Conv layers, very deep networks of Conv layers, so again we'll discuss
all of this later on. And then typical architectures
again look like this, you know, as we had earlier. Conv, ReLU for some N number of steps followed by a pool every once in a while, this whole thing repeated
some number of times, and then followed by fully
connected ReLU layers that we saw earlier, you know, one or two or just a few of these, and then a softmax at the
end for your class scores. And so, you know, some typical values you might have N up to five of these. You're going to have pretty deep layers of Conv, ReLU, pool
sequences, and then usually just a couple of these fully
connected layers at the end. But we'll also go into
some newer architectures like ResNet and GoogLeNet,
which challenge this and will give pretty different
types of architectures. Okay, thank you and
see you guys next time. 

- Okay, let's get started. Okay, so today we're going to
get into some of the details about how we train neural networks. So, some administrative details first. Assignment 1 is due today, Thursday, so 11:59 p.m. tonight on Canvas. We're also going to be
releasing Assignment 2 today, and then your project proposals are due Tuesday, April 25th. So you should be really starting to think about your projects now
if you haven't already. How many people have decided what they want to do for
their project so far? Okay, so some, some people, so yeah, everyone else, you
can go to TA office hours if you want suggestions and bounce ideas off of TAs. We also have a list of projects that other people have proposed. Some people usually
affiliated with Stanford, so on Piazza, so you
can take a look at those for additional ideas. And we also have some notes on backprop for a linear layer and a
vector and tensor derivatives that Justin's written up, so that should help with understanding how exactly backprop works and for vectors and matrices. So these are linked to
lecture four on the syllabus and you can go and take a look at those. Okay, so where we are now. We've talked about how
to express a function in terms of a computational graph, that we can represent any function in terms of a computational graph. And we've talked more explicitly
about neural networks, which is a type of graph where we have these linear layers that we stack on top of each other with nonlinearities in between. And we've also talked last lecture about convolutional neural networks, which are a particular type of network that uses convolutional layers to preserve the spatial structure throughout all the the
hierarchy of the network. And so we saw exactly how
a convolution layer looked, where each activation map in the convolutional layer output is produced by sliding a filter of weights over all of the spatial
locations in the input. And we also saw that usually we can have many filters per layer, each of which produces a
separate activation map. And so what we can get
is from an input right, with a certain depth, we'll
get an activation map output, which has some spatial
dimension that's preserved, as well as the depth is
the total number of filters that we have in that layer. And so what we want to do is we want to learn the values of all of these weights or parameters, and we saw that we can learn our network parameters
through optimization, which we talked about little bit earlier in the course, right? And so we want to get to a
point in the loss landscape that produces a low loss, and we can do this by taking steps in the direction of the negative gradient. And so the whole process we actually call a Mini-batch Stochastic Gradient Descent where the steps are that we continuously, we sample a batch of data. We forward prop it through our computational graph
or our neural network. We get the loss at the end. We backprop through our network to calculate the gradients. And then we update the parameters or the weights in our
network using this gradient. Okay, so now for the
next couple of lectures we're going to talk
about some of the details involved in training neural networks. And so this involves things like how do we set up our neural
network at the beginning, which activation functions that we choose, how do we preprocess the data, weight initialization,
regularization, gradient checking. We'll also talk about training dynamics. So, how do we babysit
the learning process? How do we choose how we
do parameter updates, specific perimeter update rules, and how do we do
hyperparameter optimization to choose the best hyperparameters? And then we'll also talk about evaluation and model ensembles. So today in the first part, I will talk about activation
functions, data preprocessing, weight initialization,
batch normalization, babysitting the learning process, and hyperparameter optimization. Okay, so first activation functions. So, we saw earlier how out
of any particular layer, we have the data coming in. We multiply by our weight in you know, fully connected or a convolutional layer. And then we'll pass this through an activation function or nonlinearity. And we saw some examples of this. We used sigmoid previously
in some of our examples. We also saw the ReLU nonlinearity. And so today we'll talk
more about different choices for these different nonlinearities and trade-offs between them. So first, the sigmoid,
which we've seen before, and probably the one we're
most comfortable with, right? So the sigmoid function
is as we have up here, one over one plus e to the negative x. And what this does is it takes each number that's input into the sigmoid
nonlinearity, so each element, and the elementwise squashes these into this range [0,1] right,
using this function here. And so, if you get very
high values as input, then output is going to
be something near one. If you get very low values, or, I'm sorry, very negative values, it's going to be near zero. And then we have this regime near zero that it's in a linear regime. It looks a bit like a linear function. And so this is been historically popular, because sigmoids, in a sense, you can interpret them as a kind of a saturating firing
rate of a neuron, right? So if it's something between zero and one, you could think of it as a firing rate. And we'll talk later about
other nonlinearities, like ReLUs that, in practice,
actually turned out to be more biologically plausible, but this does have a
kind of interpretation that you could make. So if we look at this
nonlinearity more carefully, there's several problems that
there actually are with this. So the first is that saturated neurons can kill off the gradient. And so what exactly does this mean? So if we look at a sigmoid gate right, a node in our computational graph, and we have our data X as input into it, and then we have the output of the sigmoid gate coming out of it, what does the gradient flow look like as we're coming back? We have dL over d sigma right? The upstream gradient coming down, and then we're going to
multiply this by dSigma over dX. This will be the gradient
of a local sigmoid function. And we're going to chain these together for our downstream
gradient that we pass back. So who can tell me what happens when X is equal to -10? It's very negative. What does is gradient look like? Zero, yeah, so that's right. So the gradient become zero and that's because in this negative, very negative region of the sigmoid, it's essentially flat,
so the gradient is zero, and we chain any upstream
gradient coming down. We multiply by basically
something near zero, and we're going to get
a very small gradient that's flowing back downwards, right? So, in a sense, after the chain rule, this kills the gradient flow
and you're going to have a zero gradient passed
down to downstream nodes. And so what happens
when X is equal to zero? So there it's, yeah,
it's fine in this regime. So, in this regime near zero, you're going to get a
reasonable gradient here, and then it'll be fine for backprop. And then what about X equals 10? Zero, right. So again, so when X is
equal to a very negative or X is equal to large positive numbers, then these are all regions where the sigmoid function is flat, and it's going to kill off the gradient and you're not going to get
a gradient flow coming back. Okay, so a second problem is that the sigmoid outputs are not zero centered. And so let's take a look
at why this is a problem. So, consider what happens when the input to a neuron is always positive. So in this case, all of our Xs
we're going to say is positive. It's going to be multiplied
by some weight, W, and then we're going to run it through our activation function. So what can we say about
the gradients on W? So think about what the local
gradient is going to be, right, for this linear layer. We have DL over whatever
the activation function, the loss coming down, and then we have our local gradient, which is going to be basically X, right? And so what does this mean,
if all of X is positive? Okay, so I heard it's
always going to be positive. So that's almost right. It's always going to be either positive, or all positive or all negative, right? So, our upstream gradient coming down is DL over our loss. L is going to be DL over DF. and this is going to be
either positive or negative. It's some arbitrary gradient coming down. And then our local gradient
that we multiply this by is, if we're going to find the gradients on W, is going to be DF over DW,
which is going to be X. And if X is always positive
then the gradients on W, which is multiplying these two together, are going to always be the sign of the upstream
gradient coming down. And so what this means is
that all the gradients of W, since they're always either
positive or negative, they're always going to
move in the same direction. You're either going to
increase all of the, when you do a parameter update, you're going to either
increase all of the values of W by a positive amount, or
differing positive amounts, or you will decrease them all. And so the problem with this is that, this gives very inefficient
gradient updates. So, if you look at on the right here, we have an example of a case where, let's say W is two-dimensional, so we have our two axes for W, and if we say that we can only have all positive or all negative updates, then we have these two quadrants, and, are the two places where the axis are either all positive or negative, and these are the only directions in which we're allowed to make a gradient update. And so in the case where, let's say our hypothetical optimal W is actually this blue vector here, right, and we're starting off
at you know some point, or at the top of the the the
beginning of the red arrows, we can't just directly take a gradient update in this direction, because this is not in one of those two allowed gradient directions. And so what we're going to have to do, is we'll have to take a
sequence of gradient updates. For example, in these red arrow directions that are each in allowed directions, in order to finally get to this optimal W. And so this is why also, in general, we want a zero mean data. So, we want our input X to be zero meaned, so that we actually have
positive and negative values and we don't get into this problem of the gradient updates. They'll be all moving
in the same direction. So is this clear? Any questions on this point? Okay. Okay, so we've talked about these two main problems of the sigmoid. The saturated neurons
can kill the gradients if we're too positive or
too negative of an input. They're also not zero-centered and so we get these, this inefficient kind of gradient update. And then a third problem, we have an exponential function in here, so this is a little bit
computationally expensive. In the grand scheme of your network, this is usually not the main problem, because we have all these convolutions and dot products that
are a lot more expensive, but this is just a minor
point also to observe. So now we can look at a second activation function here at tanh. And so this looks very
similar to the sigmoid, but the difference is that now it's squashing to the range [-1, 1]. So here, the main difference is that it's now zero-centered, so we've gotten rid of the
second problem that we had. It still kills the gradients,
however, when it's saturated. So, you still have these regimes where the gradient is essentially flat and you're going to
kill the gradient flow. So this is a bit better than the sigmoid, but it still has some problems. Okay, so now let's look at
the ReLU activation function. And this is one that we saw
in our examples last lecture when we were talking about the convolutional neural network. And we saw that we interspersed
ReLU nonlinearities between many of the convolutional layers. And so, this function is f of
x equals max of zero and x. So it takes an elementwise
operation on your input and basically if your input is negative, it's going to put it to zero. And then if it's positive, it's going to be just passed through. It's the identity. And so this is one that's
pretty commonly used, and if we look at this one and look at and think about the problems that we saw earlier with
the sigmoid and the tanh, we can see that it doesn't saturate in the positive region. So there's whole half of our input space where it's not going to saturate, so this is a big advantage. So this is also
computationally very efficient. We saw earlier that the sigmoid has this E exponential in it. And so the ReLU is just this simple max and there's, it's extremely fast. And in practice, using this ReLU, it converges much faster than
the sigmoid and the tanh, so about six times faster. And it's also turned out to be more biologically plausible than the sigmoid. So if you look at a neuron and you look at what the inputs look like, and you look at what
the outputs look like, and you try to measure this
in neuroscience experiments, you'll see that this one is actually a closer approximation to what's happening than sigmoids. And so ReLUs were starting to be used a lot around 2012 when we had AlexNet, the first major
convolutional neural network that was able to do well on ImageNet and large-scale data. They used the ReLU in their experiments. So a problem however, with the ReLU, is that it's still, it's not
not zero-centered anymore. So we saw that the sigmoid
was not zero-centered. Tanh fixed this and now
ReLU has this problem again. And so that's one of
the issues of the ReLU. And then we also have
this further annoyance of, again we saw that in the
positive half of the inputs, we don't have saturation, but this is not the case
of the negative half. Right, so just thinking about this a little bit more precisely. So what's happening here
when X equals negative 10? So zero gradient, that's right. What happens when X is
equal to positive 10? It's good, right. So, we're in the linear regime. And then what happens
when X is equal to zero? Yes, it undefined here, but in practice, we'll
say, you know, zero, right. And so basically, it's
killing the gradient in half of the regime. And so we can get this phenomenon of basically dead ReLUs, when we're in this bad part of the regime. And so there's, you can look at this in, as coming from several potential reasons. And so if we look at our data cloud here, this is all of our training data, then if we look at where
the ReLUs can fall, so the ReLUs can be,
each of these is basically the half of the plane where
it's going to activate. And so each of these is the plane that defines each of these ReLUs, and we can see that you
can have these dead ReLUs that are basically off of the data cloud. And in this case, it will never
activate and never update, as compared to an active ReLU where some of the data is going to be positive and passed through and some won't be. And so there's several reasons for this. The first is that it can happen when you have bad initialization. So if you have weights
that happen to be unlucky and they happen to be off the data cloud, so they happen to specify
this bad ReLU over here. Then they're never going to get a data input that causes it to activate, and so they're never going to get good gradient flow coming back. And so it'll just never
update and never activate. What's the more common case is when your learning rate is too high. And so this case you started
off with an okay ReLU, but because you're making
these huge updates, the weights jump around and then your ReLU unit in a sense, gets knocked off of the data manifold. And so this happens through training. So it was fine at the beginning and then at some point,
it became bad and it died. And so if in practice, if you freeze a network
that you've trained and you pass the data through, you can see it actually
is much as 10 to 20% of the network is these dead ReLUs. And so you know that's a problem, but also most networks do have this type of problem when you use ReLUs. Some of them will be dead, and in practice, people look into this, and it's a research problem, but it's still doing okay
for training networks. Yeah, is there a question? [student speaking off mic] Right. So the question is, yeah, so the data cloud is
just your training data. [student speaking off mic] Okay, so the question is when, how do you tell when the ReLU
is going to be dead or not, with respect to the data cloud? And so if you look at, this is an example of like a
simple two-dimensional case. And so our ReLU, we're going
to get our input to the ReLU, which is going to be a basically you know, W1 X1 plus W2 X2, and it we apply this, so that that defines this this
separating hyperplane here, and then we're going to take half of it that's going to be positive, and half of it's going to be killed off, and so yes, so you, you know you just, it's whatever the weights happened to be, and where the data happens
to be is where these, where these hyperplanes fall, and so, so yeah so just throughout
the course of training, some of your ReLUs will
be in different places, with respect to the data cloud. Oh, question. [student speaking off mic] Yeah. So okay, so the question is for the sigmoid we talked
about two drawbacks, and one of them was that the
neurons can get saturated, so let's go back to the sigmoid here, and the question was this is not the case, when all of your inputs are positive. So when all of your inputs are positive, they're all going to be coming in in this zero plus region here, and so you can still
get a saturating neuron, because you see up in
this positive region, it also plateaus at one, and so when it's when you
have large positive values as input you're also going
to get the zero gradient, because you have you
have a flat slope here. Okay. Okay, so in practice people
also like to initialize ReLUs with slightly positive biases, in order to increase the
likelihood of it being active at initialization
and to get some updates. Right and so this basically
just biases towards more ReLUs firing at the beginning, and in practice some say that it helps. Some say that it doesn't. Generally people don't always use this. It's yeah, a lot of times
people just initialize it with zero biases still. Okay, so now we can look
at some modifications on the ReLU that have come out since then, and so one example is this leaky ReLU. And so this looks very
similar to the original ReLU, and the only difference is
that now instead of being flat in the negative regime, we're going to give a
slight negative slope here And so this solves a lot of the problems that we mentioned earlier. Right here we don't have
any saturating regime, even in the negative space. It's still very computationally efficient. It still converges faster
than sigmoid and tanh, very similar to a ReLU. And it doesn't have this dying problem. And there's also another example is the parametric rectifier, so PReLU. And so in this case it's
just like a leaky ReLU where we again have this sloped region in the negative space, but now this slope in the negative regime is determined through
this alpha parameter, so we don't specify,
we don't hard-code it. but we treat it as now a parameter that we can backprop into and learn. And so this gives it a
little bit more flexibility. And we also have something called an Exponential Linear Unit, an ELU, so we have all these
different LUs, basically. and this one again, you know, it has all the benefits of the ReLu, but now you're, it is also
closer to zero mean outputs. So, that's actually an
advantage that the leaky ReLU, parametric ReLU, a lot
of these they allow you to have your mean closer to zero, but compared with the leaky ReLU, instead of it being sloped
in the negative regime, here you actually are building back in a negative saturation regime, and there's arguments that
basically this allows you to have some more robustness to noise, and you basically get
these deactivation states that can be more robust. And you can look at this paper for, there's a lot of kind
of more justification for why this is the case. And in a sense this is kind of something in between the ReLUs and the leaky ReLUs, where has some of this shape, which the Leaky ReLU does, which gives it closer to zero mean output, but then it also still has some of this more saturating behavior that ReLUs have. A question? [student speaking off mic] So, whether this parameter alpha is going to be specific for each neuron. So, I believe it is often specified, but I actually can't remember exactly, so you can look in the paper for exactly, yeah, how this is defined, but yeah, so I believe
this function is basically very carefully designed in order to have nice desirable properties. Okay, so there's basically all of these kinds of variants on the ReLU. And so you can see that,
all of these it's kind of, you can argue that each one
may have certain benefits, certain drawbacks in practice. People just want to run
experiments all of them, and see empirically what works better, try and justify it, and
come up with new ones, but they're all different things that are being experimented with. And so let's just mention one more. This is Maxout Neuron. So, this one looks a little bit different in that it doesn't have the
same form as the others did of taking your basic dot product, and then putting this element-wise nonlinearity in front of it. Instead, it looks like this, this max of W dot product of X plus B, and a second set of weights, W2 dot product with X plus B2. And so what does this,
is this is taking the max of these two functions in a sense. And so what it does is
it generalizes the ReLU and the leaky ReLu, because you're just you're
taking the max over these two, two linear functions. And so what this give us, it's again you're operating
in a linear regime. It doesn't saturate and it doesn't die. The problem is that here, you are doubling the number
of parameters per neuron. So, each neuron now has this
original set of weights, W, but it now has W1 and W2,
so you have twice these. So in practice, when we look at all of
these activation functions, kind of a good general
rule of thumb is use ReLU. This is the most standard one that generally just works well. And you know you do want
to be careful in general with your learning rates
to adjust them based, see how things do. We'll talk more about
adjusting learning rates later in this lecture, but you can also try out some of these fancier activation functions, the leaky ReLU, Maxout, ELU, but these are generally, they're still kind of more experimental. So, you can see how they
work for your problem. You can also try out tanh, but probably some of these ReLU and ReLU variants are going to be better. And in general don't use sigmoid. This is one of the earliest
original activation functions, and ReLU and these other variants have generally worked better since then. Okay, so now let's talk a little bit about data preprocessing. Right, so the activation function, we design this is part of our network. Now we want to train the network, and we have our input data that we want to start training from. So, generally we want to
always preprocess the data, and this is something that
you've probably seen before in machine learning
classes if you taken those. And some standard types
of preprocessing are, you take your original data and you want to zero mean them, and then you probably want
to also normalize that, so normalized by the standard deviation, And so why do we want to do this? For zero centering, you
can remember earlier that we talked about when all the inputs are positive, for example, then we get all of our gradients on the weights to be positive, and we get this basically
suboptimal optimization. And in general even if it's
not all zero or all negative, any sort of bias will still
cause this type of problem. And so then in terms of
normalizing the data, this is basically you
want to normalize data typically in the machine
learning problems, so that all features
are in the same range, and so that they contribute equally. In practice, since for
images, which is what we're dealing with in this
course here for the most part, we do do the zero centering, but in practice we
don't actually normalize the pixel value so much,
because generally for images right at each location you already have relatively comparable
scale and distribution, and so we don't really
need to normalize so much, compared to more general
machine learning problems, where you might have different features that are very different and
of very different scales. And in machine learning, you might also see a
more complicated things, like PCA or whitening,
but again with images, we typically just stick
with the zero mean, and we don't do the normalization, and we also don't do some of these more complicated pre-processing. And one reason for this
is generally with images we don't really want to
take all of our input, let's say pixel values and project this onto a lower dimensional space of new kinds of features
that we're dealing with. We typically just want to apply convolutional networks spatially and have our spatial structure
over the original image. Yeah, question. [student speaking off mic] So the question is we
do this pre-processing in a training phase, do we also do the same kind
of thing in the test phase, and the answer is yes. So, let me just move
to the next slide here. So, in general on the training phase is where we determine our let's say, mean, and then we apply this exact
same mean to the test data. So, we'll normalize by the same empirical mean from the training data. Okay, so to summarize
basically for images, we typically just do the
zero mean pre-processing and we can subtract either
the entire mean image. So, from the training data, you compute the mean image, which will be the same size
as your, as each image. So, for example 32 by 32 by three, you'll get this array of numbers, and then you subtract that from each image that you're about to
pass through the network, and you'll do the same thing at test time for this array that you
determined at training time. In practice, we can
also for some networks, we also do this by just of subtracting a per-channel mean, and so instead of having
an entire mean image that were going to zero-center by, we just take the mean by channel, and this is just because it turns out that it was similar enough
across the whole image, it didn't make such a big difference to subtract the mean image versus just a per-channel value. And this is easier to just
pass around and deal with. So, you'll see this as well
for example, in a VGG Network, which is a network that
came after AlexNet, and we'll talk about that later. Question. [student speaking off mic] Okay, so there are two questions. The first is what's a
channel, in this case, when we are subtracting
a per-channel mean? And this is RGB, so our array, our images are typically for
example, 32 by 32 by three. So, width, height, each are 32, and our depth, we have three channels RGB, and so we'll have one
mean for the red channel, one mean for a green, one for blue. And then the second, what
was your second question? [student speaking off mic] Oh. Okay, so the question is when we're subtracting the mean image, what is the mean taken over? And the mean is taking over
all of your training images. So, you'll take all of
your training images and just compute the mean of all of those. Does that make sense? [student speaking off mic] Yeah the question is, we do this for the entire training set, once before we start training. We don't do this per batch, and yeah, that's exactly correct. So we just want to have a good sample, an empirical mean that we have. And so if you take it per batch, if you're sampling reasonable batches, it should be basically, you should be getting the same values anyways for the mean, and so it's more efficient and easier just do this once at the beginning. You might not even have to really take it over the entire training data. You could also just sample
enough training images to get a good estimate of your mean. Okay, so any other questions
about data preprocessing? Yes. [student speaking off mic] So, the question is does
the data preprocessing solve the sigmoid problem? So the data preprocessing
is doing zero mean right? And we talked about how sigmoid, we want to have zero mean. And so it does solve
this for the first layer that we pass it through. So, now our inputs to the first layer of our network is going to be zero mean, but we'll see later on that
we're actually going to have this problem come up in
much worse and greater form, as we have deep networks. You're going to get a lot of nonzero mean problems later on. And so in this case, this is
not going to be sufficient. So this only helps at the
first layer of your network. Okay, so now let's talk
about how do we want to initialize the weights of our network? So, we have let's say our standard two layer neural network and we have all of these
weights that we want to learn, but we have to start them
with some value, right? And then we're going to update them using our gradient updates from there. So first question. What happens when we use an initialization of W equals zero? We just set all of the
parameters to be zero. What's the problem with this? [student speaking off mic] So sorry, say that again. So I heard all the neurons
are going to be dead. No updates ever. So not exactly. So, part of that is correct in that all the neurons will do the same thing. So, they might not all be dead. Depending on your input value, I mean, you could be in any
regime of your neurons, so they might not be dead, but the key thing is that they
will all do the same thing. So, since your weights are zero, given an input, every
neuron is going to be, have the same operation
basically on top of your inputs. And so, since they're all
going to output the same thing, they're also all going
to get the same gradient. And so, because of that, they're all going to
update in the same way. And now you're just going to get all neurons that are exactly the same, which is not what you want. You want the neurons to
learn different things. And so, that's the problem when you initialize everything equally and there's basically no
symmetry breaking here. So, what's the first, yeah question? [student speaking off mic] So the question is, because that, because the gradient
also depends on our loss, won't one backprop differently
compared to the other? So in the last layer, like yes, you do have basically some of this, the gradients will get the same, sorry, will get different
loss for each specific neuron based on which class it was connected to, but if you look at all the neurons generally throughout your
network, like you're going to, you basically have a lot of these neurons that are connected in
exactly the same way. They had the same updates and it's basically
going to be the problem. Okay, so the first idea that we can have to try and improve upon this is to set all of the weights
to be small random numbers that we can sample from a distribution. So, in this case, we're
going to sample from basically a standard gaussian, but we're going to scale it so that the standard deviation is actually one E negative two, 0.01. And so, just give this
many small random weights. And so, this does work
okay for small networks, now we've broken the symmetry, but there's going to be
problems with deeper networks. And so, let's take a look
at why this is the case. So, here this is basically
an experiment that we can do where let's take a deeper network. So in this case, let's initialize
a 10 layer neural network to have 500 neurons in
each of these 10 layers. Okay, we'll use tanh
nonlinearities in this case and we'll initialize it
with small random numbers as we described in the last slide. So here, we're going to basically just initialize this network. We have random data
that we're going to take, and now let's just pass it
through the entire network, and at each layer, look at the statistics of the activations that
come out of that layer. And so, what we'll see this is probably a little bit hard to read up top, but if we compute the mean and the standard deviations at each layer, well see that at the first layer this is, the means are always around zero. There's a funny sound in here. Interesting, okay well that was fixed. So, if we look at, if we look
at the outputs from here, the mean is always
going to be around zero, which makes sense. So, if we look here, let's see, if we take this, we looked at
the dot product of X with W, and then we took the tanh on linearity, and then we store these values and so, because it tanh is centered around zero, this will make sense, and then the standard
deviation however shrinks, and it quickly collapses to zero. So, if we're plotting this, here this second row of plots here is showing the mean
and standard deviations over time per layer
and then in the bottom, the sequence of plots is
showing for each of our layers. What's the distribution of
the activations that we have? And so, we can see that
at the first layer, we still have a reasonable
gaussian looking thing. It's a nice distribution. But the problem is that as we multiply by this W, these small numbers at each layer, this quickly shrinks and
collapses all of these values, as we multiply this over and over again. And so, by the end, we
get all of these zeros, which is not what we want. So we get all the activations become zero. And so now let's think
about the backwards pass. So, if we do a backward pass, now assuming this was our forward pass and now we want to compute our gradients. So first, what does the gradients look like on the weights? Does anyone have a guess? So, if we think about this, we have our input values are very small at each layer right, because they've all
collapsed at this near zero, and then now each layer, we have our upstream
gradient flowing down, and then in order to get
the gradient on the weights remember it's our upstream gradient
times our local gradient, which for this this dot
product were doing W times X. It's just basically going to
be X, which is our inputs. So, it's again a similar kind of problem that we saw earlier, where now since, so
here because X is small, our weights are getting
a very small gradient, and they're basically not updating. So, this is a way that you can
basically try and think about the effect of gradient
flows through your networks. You can always think about
what the forward pass is doing, and then think about what's happening as you have gradient flows coming down, and different types of inputs, what the effect of this
actually is on our weights and the gradients on them. And so also, if now if we think about what's the gradient that's
going to be flowing back from each layer as we're
chaining all these gradients. Alright, so this is going to be the flip thing where we have now the gradient flowing back
is our upstream gradient times in this case the local
gradient is W on our input X. And so again, because
this is the dot product, and so now, actually going
backwards at each layer, we're basically doing a multiplication of the upstream gradient by our weights in order to get the next
gradient flowing downwards. And so because here, we're multiplying by
W over and over again. You're getting basically
the same phenomenon as we had in the forward pass where everything is getting
smaller and smaller. And now the gradient, upstream gradients are collapsing to zero as well. Question? [student speaking off mic] Yes, I guess upstream and downstream is, can be interpreted differently, depending on if you're
going forward and backward, but in this case we're going, we're doing, we're going backwards, right? We're doing back propagation. And so upstream is the gradient flowing, you can think of a flow from your loss, all the way back to your input. And so upstream is what came from what you've already done, flowing you know, down
into your current node. Right, so we're for flowing downwards, and what we get coming into
the node through backprop is coming from upstream. Okay, so now let's think
about what happens when, you know we saw that this was a problem when our weights were pretty small, right? So, we can think about well, what if we just try and solve this by making our weights big? So, let's sample from
this standard gaussian, now with standard deviation one instead of 0.01. So what's the problem here? Does anyone have a guess? If our weights are now all
big, and we're passing them, and we're taking these
outputs of W times X, and passing them through
tanh nonlinearities, remember we were talking
about what happens at different values of inputs to tanh, so what's the problem? Okay, so yeah I heard that
it's going to be saturated, so that's right. Basically now, because our
weights are going to be big, we're going to always
be at saturated regimes of either very negative or
very positive of the tanh. And so in practice, what
you're going to get here is now if we look at the
distribution of the activations at each of the layers here on the bottom, they're going to be all basically
negative one or plus one. Right, and so this will have the problem that we talked about with the tanh earlier,
when they're saturated, that all the gradients will be zero, and our weights are not updating. So basically, it's really hard to get your weight initialization right. When it's too small they all collapse. When it's too large they saturate. So, there's been some work
in trying to figure out well, what's the proper way
to initialize these weights. And so, one kind of good rule
of thumb that you can use is the Xavier initialization. And so this is from this
paper by Glorot in 2010. And so what this formula is, is if we look at W up here, we can see that we want to
initialize them to these, we sample from our standard gaussian, and then we're going to scale by the number of inputs that we have. And you can go through the math, and you can see in the lecture notes as well as in this paper of exactly how this works out, but basically the way
we do it is we specify that we want the variance of the input to be the same as a
variance of the output, and then if you derive
what the weight should be you'll get this formula, and intuitively with this
kind of means is that if you have a small
number of inputs right, then we're going to divide
by the smaller number and get larger weights,
and we need larger weights, because with small inputs, and you're multiplying
each of these by weight, you need a larger weights to get the same larger variance at output, and kind of vice versa for
if we have many inputs, then we want smaller
weights in order to get the same spread at the output. So, you can look at the notes
for more details about this. And so basically now, if we
want to have a unit gaussian, right as input to each layer, we can use this kind of initialization to at training time, to be
able to initialize this, so that there is approximately a unit gaussian at each layer. Okay, and so one thing
is does assume though is that it is assumed that
there's linear activations. and so it assumes that
we are in the activation, in the active region of
the tanh, for example. And so again, you can look at the notes to really try and
understand its derivation, but the problem is that this breaks when now you use something like a ReLU. Right, and so with the
ReLU what happens is that, because it's killing half of your units, it's setting approximately half of them to zero at each time, it's actually halving the
variance that you get out of this. And so now, if you just
make the same assumptions as your derivation
earlier you won't actually get the right variance coming out, it's going to be too small. And so what you see is again
this kind of phenomenon, as the distributions starts collapsing. In this case you get more
and more peaked toward zero, and more units deactivated. And the way to address this with something that has been
pointed out in some papers, which is that you can you
can try to account for this with an extra, divided by two. So, now you're basically
adjusting for the fact that half the neurons get killed. And so you're kind of equivalent input has actually half this number of input, and so you just add this
divided by two factor in, this works much better, and you can see that the
distributions are pretty good throughout all layers of the network. And so in practice this is
been really important actually, for training these types of little things, to a really pay attention
to how your weights are, make a big difference. And so for example,
you'll see in some papers that this actually is the
difference between the network even training at all and performing well versus nothing happening. So, proper initialization is still an active area of research. And so if you're interested in this, you can look at a lot of
these papers and resources. A good general rule of thumb is basically use the Xavier
Initialization to start with, and then you can also think about some of these other kinds of methods. And so now we're going to talk
about a related idea to this, so this idea of wanting
to keep activations in a gaussian range that we want. Right, and so this idea behind what we're going to call
batch normalization is, okay we want unit gaussian activations. Let's just make them that way. Let's just force them to be that way. And so how does this work? So, let's consider a batch
of activations at some layer. And so now we have all of
our activations coming out. If we want to make this unit gaussian, we actually can just do
this empirically, right. We can take the mean of the
batch that we have so far of the current batch, and we
can just and the variance, and we can just normalize by this. Right, and so basically, instead of with weight initialization, we're setting this at
the start of training so that we try and get it into a good spot that we can have unit
gaussians at every layer, and hopefully during training
this will preserve this. Now we're going to
explicitly make that happen on every forward pass through the network. We're going to make this
happen functionally, and basically by normalizing by the mean and the variance of each neuron, we look at all of the
inputs coming into it and calculate the mean and
variance for that batch and normalize it by it. And the thing is that this is a, this is just a differentiable
function right? If we have our mean and
our variance as constants, this is just a sequence of
computational operations that we can differentiate and
do back prop through this. Okay, so just as I was
saying earlier right, if we look at our input
data, and we think of this as we have N training examples
in our current batch, and then each batch has dimension D, we're going to the
compute the empirical mean and variance independently
for each dimension, so each basically feature element, and we compute this across our batch, our current mini-batch that we have and we normalize by this. And so this is usually
inserted after fully connected or convolutional layers. We saw that would we were multiplying by W in these layers, which
we do over and over again, then we can have this bad
scaling effect with each one. And so this basically is
able to undo this effect. Right, and since we're basically
just scaling by the inputs connected to each neuron, each activation, we can apply this the same
way to fully connected convolutional layers, and
the only difference is that, with convolutional layers,
we want to normalize not just across all the training examples, and independently for each
each feature dimension, but we actually want to normalize jointly across both all the feature dimensions, all the spatial locations that we have in our activation map, as well as all of the training examples. And we do this, because we want to obey
the convolutional property, and we want nearby locations to be normalized the same way, right? And so with a convolutional layer, we're basically going to have a one mean and one standard deviation, per activation map that that we have, and we're going to normalize by this across all of the examples in the batch. And so this is something that you guys are going to implement
in your next homework. And so, all of these details
are explained very clearly in this paper from 2015. And so on this is a very useful, useful technique that you
want to use a lot in practice. You want to have these
batch normalization layers. And so you should read this paper. Go through all of the derivations, and then also go through the derivations of how to compute the
gradients with given these, this normalization operation. Okay, so one thing that I just
want to point out is that, it's not clear that, you know, we're doing this batch normalization after every fully connected layer, and it's not clear that
we necessarily want a unit gaussian input to
these tanh nonlinearities, because what this is doing
is this is constraining you to the linear regime of this nonlinearity, and we're not actually, you're
trying to basically say, let's not have any of this saturation, but maybe a little bit
of this is good, right? You you want to be able to control what's, how much saturation that you want to have. And so what, the way that we address this when we're doing batch normalization is that we have our normalization operation, but then after that we
have this additional squashing and scaling operation. So, we do our normalization. Then we're going to scale
by some constant gamma, and then shift by another factor of beta. Right, and so what this
actually does is that this allows you to be able to
recover the identity function if you wanted to. So, if the network wanted to, it could learn your scaling factor gamma to be just your variance. It could learn your beta to be your mean, and in this case you can
recover the identity mapping, as if you didn't have batch normalization. And so now you have the flexibility of doing kind of everything in between and making your the network learning how to make your tanh
more or less saturated, and how much to do so in order to have, to have good training. Okay, so just to sort of summarize the batch normalization idea. Right, so given our inputs, we're going to compute
our mini-batch mean. So, we do this for every
mini-batch that's coming in. We compute our variance. We normalize by the mean and variance, and we have this additional
scaling and shifting factor. And so this improves gradient
flow through the network. it's also more robust as a result. It works for more range of learning rates, and different kinds of initialization, so people have seen that once you put batch normalization in, and it's just easier to train, and so that's why you should do this. And then also when one thing
that I just want to point out is that you can also
think of this as in a way also doing some regularization. Right and so, because now
at the output of each layer, each of these activations,
each of these outputs, is an output of both your input X, as well as the other examples in the batch that it happens to be sampled with, right, because you're going to
normalize each input data by the empirical mean over that batch. So because of that,
it's no longer producing deterministic values for
a given training example, and it's tying all of these
inputs in a batch together. And so this basically, because
it's no longer deterministic, kind of jitters your
representation of X a little bit, and in a sense, gives some
sort of regularization effect. Yeah, question? [student speaking off camera] The question is gamma and
beta are learned parameters, and yes that's the case. [student speaking off mic] Yeah, so the question is
why do we want to learn this gamma and beta to be able to learn the identity function back, and the reason is because you want to give it the flexibility. Right, so what batch
normalization is doing, is it's forcing our data to
become this unit gaussian, our inputs to be unit gaussian, but even though in general
this is a good idea, it's not always that this is
exactly the best thing to do. And we saw in particular
for something like a tanh, you might want to control some degree of saturation that you have. And so what this does is it
gives you the flexibility of doing this exact like
unit gaussian normalization, if it wants to, but
also learning that maybe in this particular part of the network, maybe that's not the best thing to do. Maybe we want something
still in this general idea, but slightly different right,
slightly scaled or shifted. And so these parameters just
give it that extra flexibility to learn that if it wants to. And then yeah, if the the best thing to do is just batch normalization
then it'll learn the right parameters for that. Yeah? [student speaking off mic] Yeah, so basically each neuron output. So, we have output of a
fully connected layer. We have W times X. and so we have the values
of each of these outputs, and then we're going to apply batch normalization separately
to each of these neurons. Question? [student speaking off mic] Yeah, so the question is that for things like reinforcement learning, you might have a really small batch size. How do you deal with this? So in practice, I guess batch
normalization has been used a lot for like for standard
convolutional neural networks, and there's actually papers
on how do we want to do normalization for different
kinds of recurrent networks, or you know some of these networks that might also be in
reinforcement learning. And so there's different considerations that you might want to think of there. And this is still an
active area of research. There's papers on this and we might also talk about some of this more later, but for a typical
convolutional neural network this generally works fine. And then if you have a smaller batch size, maybe this becomes a
little bit less accurate, but you still get kind of the same effect. And you know it's possible also that you could design
your mean and variance to be computed maybe over
more examples, right, and I think in practice
usually it's just okay, so you don't see this too much, but this is something
that maybe could help if that was a problem. Yeah, question? [student speaking off mic] So the question, so the question is, if we force
the inputs to be gaussian, do we lose the structure? So, no in a sense that
you can think of like, if you had all your features distributed as a gaussian for example, even if you were just
doing data pre-processing, this gaussian is not
losing you any structure. All the, it's just shifting and scaling your data into a regime, that works well for the operations that you're going to perform on it. In convolutional layers,
you do have some structure, that you want to preserve
spatially, right. You want, like if you look
at your activation maps, you want them to relatively
all make sense to each other. So, in this case you do want to take that into consideration. And so now, we're going to normalize, find one mean for the
entire activation map, so we only find the
empirical mean and variance over training examples. And so that's something that you'll be doing in your homework, and also explained in the paper as well. So, you should refer to that. Yes. [student speaking off mic] So the question is, are
we normalizing the weight so that they become gaussian. So, if I understand
your question correctly, then the answer is, we're normalizing the
inputs to each layer, so we're not changing the
weights in this process. [student speaking off mic] Yeah, so the question is,
once we subtract by the mean and divide by the standard deviation, does this become gaussian,
and the answer is yes. So, if you think about the
operations that are happening, basically you're shifting
by the mean, right. And so this shift up to be zero-centered, and then you're scaling
by the standard deviation. This now transforms this
into a unit gaussian. And so if you want to look more into that, I think you can look at, there's a lot of machine
learning explanations that go into exactly what this, visualizing with this operation is doing, but yeah this basically takes your data and turns it into a gaussian distribution. Okay, so yeah question? [student speaking off mic] Uh-huh. So the question is, if we're going to be
doing the shift and scale, and learning these is the
batch normalization redundant, because you could recover
the identity mapping? So in the case that the network learns that identity mapping is always the best, and it learns these parameters, the yeah, there would be no
point for batch normalization, but in practice this doesn't happen. So in practice, we will
learn this gamma and beta. That's not the same as a identity mapping. So, it will shift and
scale by some amount, but not the amount that's going to give
you an identity mapping. And so what you get is you still get this batch
normalization effect. Right, so having this
identity mapping there, I'm only putting this here
to say that in the extreme, it could learn the identity mapping, but in practice it doesn't. Yeah, question. [student speaking off mic] Yeah. [student speaking off mic] Oh, right, right. Yeah, yeah sorry, I was
not clear about this, but yeah I think this is related to the other question earlier, that yeah when we're doing this we're actually getting zero
mean and unit gaussian, which put this into a nice shape, but it doesn't have to
actually be a gaussian. So yeah, I mean ideally, if we're looking at like
inputs coming in, as you know, sort of approximately gaussian, we would like it to have
this kind of effect, but yeah, in practice
it doesn't have to be. Okay, so ... Okay, so the last thing I just
want to mention about this is that, so at test time, the
batch normalization layer, we now take the empirical mean and variance from the training data. So, we don't re-compute this at test time. We just estimate this at training time, for example using running averages, and then we're going to
use this as at test time. So, we're just going to scale by that. Okay, so now I'm going to move on to babysitting the learning process. Right, so now we've defined
our network architecture, and we'll talk about how
do we monitor training, and how do we adjust
hyperparameters as we go, to get good learning results? So as always, so the
first step we want to do, is we want to pre-process the data. Right, so we want to zero mean the data as we talked about earlier. Then we want to choose the architecture, and so here we are starting
with one hidden layer of 50 neurons, for example, but we've basically we
can pick any architecture that we want to start with. And then the first
thing that we want to do is we initialize our network. We do a forward pass through it, and we want to make sure
that our loss is reasonable. So, we talked about this
several lectures ago, where we have a basically a, let's say we have a Softmax
classifier that we have here. We know what our loss should be, when our weights are small, and we have generally
a diffuse distribution. Then we want it to be, the
Softmax classifier loss is going to be your
negative log likelihood, which if we have 10 classes, it'll be something like
negative log of one over 10, which here is around 2.3,
and so we want to make sure that our loss is what we expect it to be. So, this is a good
sanity check that we want to always, always do. So, now once we've seen that
our original loss is good, now we want to, so first we want to do this
having zero regularization, right. So, when we disable the regularization, now our only loss term is this data loss, which is going to give 2.3 here. And so here, now we want to
crank up the regularization, and when we do that, we want
to see that our loss goes up, because we've added this
additional regularization term. So, this is a good next step that you can do for your sanity check. And then, now we can start training. So, now we start trying to train. What we do is, a good way to do this is to start up with a
very small amount of data, because if you have just
a very small training set, you should be able to
over fit this very well and get very good training loss on here. And so in this case we want to turn off our regularization again, and just see if we can make
the loss go down to zero. And so we can see how
our loss is changing, as we have all these epochs. We compute our loss at each epoch, and we want to see this go
all the way down to zero. Right, and here we can see that also our training accuracy is going all the way up to one,
and this makes sense right. If you have a very small number of data, you should be able to
over fit this perfectly. Okay, so now once you've done that, these are all sanity checks. Now you can start really trying to train. So, now you can take
your full training data, and now start with a small
amount of regularization, and let's first figure out
what's a good learning rate. So, learning rate is one of the most important hyperparameters, and it's something that
you want to adjust first. So, you want to try some
value of learning rate. and here I've tried one E negative six, and you can see that the
loss is barely changing. Right, and so the reason
this is barely changing is usually because your
learning rate is too small. So when it's too small, your gradient updates are not big enough, and your cost is basically about the same. Okay, so, one thing that I want to point out here, is that we can notice that even though our loss with barely changing, the training and the validation accuracy jumped up to 20% very quickly. And so does anyone have any idea for why this might be the case? Why, so remember we
have a Softmax function, and our loss didn't really change, but our accuracy improved a lot. Okay, so the reason for this is that here the probabilities
are still pretty diffuse, so our loss term is still pretty similar, but when we shift all
of these probabilities slightly in the right direction, because we're learning right? Our weights are changing
the right direction. Now the accuracy all of a sudden can jump, because we're taking the
maximum correct value, and so we're going to get
a big jump in accuracy, even though our loss is
still relatively diffuse. Okay, so now if we try
another learning rate, now here I'm jumping in the other extreme, picking a very big learning
rate, one E negative six. What's happening is that our
cost is now giving us NaNs. And, when you have NaNs,
what this usually means is that basically your cost exploded. And so, the reason for
that is typically that your learning rate was too high. So, then you can adjust your
learning rate down again. Here I can see that we're trying three E to the negative three. The cost is still exploding. So, usually this, the rough
range for learning rates that we want to look at is between one E negative
three, and one E negative five. And, this is the rough
range that we want to be cross-validating in between. So, you want to try out
values in this range, and depending on whether
your loss is too slow, or too small, or whether it's too large, adjust it based on this. And so how do we exactly
pick these hyperparameters? Do hyperparameter optimization, and pick the best values of
all of these hyperparameters? So, the strategy that we're going to use is for any hyperparameter
for example learning rate, is to do cross-validation. So, cross-validation is
training on your training set, and then evaluating on a validation set. How well do this hyperparameter do? Something that you guys have already done in your assignment. And so typically we want
to do this in stages. And so, we can do first of course stage, where we pick values
pretty spread out apart, and then we learn for only a few epochs. And with only a few epochs. you can already get a pretty good sense of which hyperparameters, which values are good or not, right. You can quickly see that it's a NaN, or you can see that nothing is happening, and you can adjust accordingly. So, typically once you do that, then you can see what's
sort of a pretty good range, and the range that you want to now do finer sampling of values in. And so, this is the second stage, where now you might want to
run this for a longer time, and do a finer search over that region. And one tip for detecting
explosions like NaNs, you can have in your training loop, right sample some hyperparameter, start training, and then look at your cost at every iteration or every epoch. And if you ever get a cost that's much larger than
your original cost, so for example, something like
three times original cost, then you know that this is not heading in the right direction. Right, it's getting
very big, very quickly, and you can just break out of your loop, stop this this hyperparameter choice and pick something else. Alright, so an example of this, let's say here we want to run now course search for five epochs. This is a similar network that we were talking about earlier, and what we can do is
we can see all of these validation accuracy that we're getting. And I've put in, highlighted in red the ones that gives better values. And so these are going to be regions that we're going to look
into in more detail. And one thing to note is that it's usually better to
optimize in log space. And so here instead of
sampling, I'd say uniformly between you know one E to
the negative 0.01 and 100, you're going to actually do
10 to the power of some range. Right, and this is because the learning rate is multiplying
your gradient update. And so it has these
multiplicative effects, and so it makes more sense to consider a range of learning
rates that are multiplied or divided by some value,
rather than uniformly sampled. So, you want to be dealing with orders of some magnitude here. Okay, so once you find that, you can then adjust your range. Right get in this case, we
have a range of you know, maybe of 10 to the negative four, right, to 10 to the zero power. This is a good range that
we want to narrow down into. And so we can do this again, and here we can see that we're getting a relatively good accuracy of 53%. And so this means we're
headed in the right direction. The one thing that I
want to point out is that here we actually have a problem. And so the problem is that we can see that our best accuracy here has a learning rate that's about, you know, all of our good learning rates are in this E to the negative four range. Right, and since the learning
rate that we specified was going from 10 to the
negative four to 10 to the zero, that means that all the
good learning rates, were at the edge of the
range that we were sampling. And so this is bad, because this means that
we might not have explored our space sufficiently, right. We might actually want to go
to 10 to the negative five, or 10 to the negative six. There might be still better ranges if we continue shifting down. So, you want to make sure that your range kind of has the good values
somewhere in the middle, or somewhere where you get
a sense that you've hit, you've explored your range fully. Okay, and so another thing is that we can sample all of our
different hyperparameters, using a kind of grid search, right. We can sample for a fixed
set of combinations, a fixed set of values
for each hyperparameter. Sample in a grid manner
over all of these values, but in practice it's
actually better to sample from a random layout,
so sampling random value of each hyperparameter in a range. And so what you'll get instead is we'll have these two
hyper parameters here that we want to sample from. You'll get samples that look
like this right side instead. And the reason for this is
that if a function is really sort of more of a function
of one variable than another, which is usually true. Usually we have little bit more, a lower effective dimensionality
than we actually have. Then you're going to get many more samples of the important variable that you have. You're going to be able to see this shape in this green function
that I've drawn on top, showing where the good values are, compared to if you just did a grid layout where we were only able to
sample three values here, and you've missed where
were the good regions. Right, and so basically
we'll get much more useful signal overall
since we have more samples of different values of
the important variable. And so, hyperparameters to play with, we've talked about learning rate, things like different
types of decay schedules, update types, regularization, also your network architecture, so the number of hidden units, the depth, all of these are hyperparameters
that you can optimize over. And we've talked about some of these, but we'll keep talking about more of these in the next lecture. And so you can think of
this as kind of, you know, if you're basically tuning
all the knobs right, of some turntable where you're, you're a neural networks practitioner. You can think of the music that's output is the loss function that you want, and you want to adjust
everything appropriately to get the kind of output that you want. Alright, so it's really kind
of an art that you're doing. And in practice, you're going to do a lot of hyperparameter optimization, a lot of cross validation. And so you know, in order to get numbers, people will run cross validation over tons of hyperparameters,
monitor all of them, see which ones are doing better, which ones are doing worse. Here we have all these loss curves. Pick the right ones, readjust, and keep going through this process. And so as I mentioned earlier, as you're monitoring each
of these loss curves, learning rate is an important one, but you'll get a sense for
how different learning rates, which learning rates are good and bad. So you'll see that if you have
a very high exploding one, right, this is your loss explodes, then your learning rate is too high. If it's too kind of linear and too flat, you'll see that it's too low,
it's not changing enough. And if you get something that looks like there's a steep
change, but then a plateau, this is also an indicator
of it being maybe too high, because in this case, you're
taking too large jumps, and you're not able to settle
well into your local optimum. And so a good learning rate usually ends up looking
something like this, where you have a relatively steep curve, but then it's continuing to go down, and then you might keep adjusting your learning rate from there. And so this is something that
you'll see through practice. Okay and just, I think
we're very close to the end, so just one last thing
that I want to point out is than in case you ever see
learning rate loss curves, where it's ... So if you ever see loss curves
where it's flat for a while, and then starts training all of a sudden, a potential reason could
be bad initialization. So in this case, your gradients
are not really flowing too well the beginning, so
nothing's really learning, and then at some point, it just happens to
adjust in the right way, such that it tips over and
things just start training right? And so there's a lot of
experience at looking at these and see what's wrong that
you'll get over time. And so you'll usually want to monitor and visualize your accuracy. If you have a big gap between
your training accuracy and your validation accuracy, it usually means that you
might have overfitting and you might want to increase your regularization strength. If you have no gap, you might want to increase
your model capacity, because you haven't overfit yet. You could potentially increase it more. And in general, we also
want to track the updates, the ratio of our weight updates
to our weight magnitudes. We can just take the norm of our parameters that we have to get a sense for how large they are, and when we have our update size, we can also take the norm of that, get a sense for how large that is, and we want this ratio to
be somewhere around 0.001. There's a lot of variance in this range, so you don't have to be exactly on this, but it's just this sense of you don't want your updates to be too
large compared to your value or too small, right? You don't want to dominate
or to have no effect. And so this is just
something that can help debug what might be a problem. Okay, so in summary, today we've looked at
activation functions, data preprocessing, weight initialization, batch norm, babysitting
the learning process, and hyperparameter optimization. These are the kind of
the takeaways for each that you guys should keep in mind. Use ReLUs, subtract the mean,
use Xavier Initialization, use batch norm, and sample
hyperparameters randomly. And next time we'll continue to talk about the training neural networks with all these different topics. Thanks. 

- Okay, it's after 12, so I think we should get started. Today we're going to kind of pick up where we left off last time. Last time we talked about a lot of sort of tips and tricks involved in the nitty gritty details of training neural networks. Today we'll pick up where we left off, and talk about a lot more of these sort of nitty gritty details about training these things. As usual, a couple administrative notes before we get into the material. As you all know, assignment
one is already due. Hopefully you all turned it in. Did it go okay? Was it not okay? Rough sentiment? Mostly okay. Okay, that's good. Awesome. [laughs] We're in the process of grading those, so stay turned. We're hoping to get grades back for those before A two is due. Another reminder, that
your project proposals are due tomorrow. Actually, no, today at 11:59. Make sure you send those in. Details are on the website and on Piazza. Also a reminder, assignment
two is already out. That'll be due a week from Thursday. Historically, assignment two has been the longest one in the class, so if you haven't started already on assignment two, I'd recommend you take a look at that pretty soon. Another reminder is
that for assignment two, I think of a lot of you will be using Google Cloud. Big reminder, make sure
to stop your instances when you're not using them because whenever your instance
is on, you get charged, and we only have so many coupons to distribute to you guys. Anytime your instance is on, even if you're not SSH to it, even if you're not running things immediately
in your Jupyter Notebook, any time that instance is on,
you're going to be charged. Just make sure that you explicitly stop your instances when you're not using them. In this example, I've
got a little screenshot of my dashboard on Google Cloud. I need to go in there and explicitly go to the dropdown and click stop. Just make sure that you do this when you're done working each day. Another thing to remember is it's kind of up to you guys to keep
track of your spending on Google Cloud. In particular, instances that use GPUs are a lot more expensive
than those with CPUs. Rough order of magnitude,
those GPU instances are around 90 cents to a dollar an hour. Those are actually quite pricey. The CPU instances are much cheaper. The general strategy is that you probably want to make two instances, one with a GPU and one without, and then only use that GPU instance when you really need the GPU. For example, on assignment two, most of the assignment,
you should only need the CPU, so you should only use your CPU instance for that. But then the final
question, about TensorFlow or PyTorch, that will need a GPU. This'll give you a little bit of practice with switching between multiple instances and only using that GPU
when it's really necessary. Again, just kind of watch your spending. Try not to go too crazy on these things. Any questions on the administrative stuff before we move on? Question. - [Student] How much RAM should we use? - Question is how much RAM should we use? I think eight or 16 gigs is probably good for everything that
you need in this class. As you scale up the number of CPUs and the number of RAM, you also end up spending more money. If you stick with two or four CPUs and eight or 16 gigs of RAM, that should be plenty for all the
homework-related stuff that you need to do. As a quick recap, last
time we talked about activation functions. We talked about this whole zoo of different activation functions and some of their different properties. We saw that the sigmoid, which used to be quite popular when
training neural networks maybe 10 years ago or so, has this problem with vanishing gradients near the two ends of the activation function. tanh has this similar sort of problem. Kind of the general recommendation is that you probably want to stick with ReLU for most cases as sort of a default choice 'cause it tends to work well for a lot of different architectures. We also talked about
weight initialization. Remember that up on the
top, we have this idea that when you initialize your weights at the start of training, if those weights are initialized to be
too small, then if you look at, then the activations will vanish as you go through the network because as you multiply by these small numbers over and over again, they'll all sort of decay to zero. Then everything will be
zero, learning won't happen, you'll be sad. On the other hand, if you initialize your weights too big,
then as you go through the network and multiply
by your weight matrix over and over again,
eventually they'll explode. You'll be unhappy,
there'll be no learning, it will be very bad. But if you get that
initialization just right, for example, using the
Xavier initialization or the MSRA initialization,
then you kind of keep a nice distribution of activations as you go through the network. Remember that this kind of gets more and more important and
more and more critical as your networks get deeper and deeper because as your network gets deeper, you're multiplying by
those weight matrices over and over again with these
more multiplicative terms. We also talked last time
about data preprocessing. We talked about how it's pretty typical in conv nets to zero center and normalize your data so it has zero
mean and unit variance. I wanted to provide a little
bit of extra intuition about why you might
actually want to do this. Imagine a simple setup where we have a binary classification problem where we want to draw a line to
separate these red points from these blue points. On the left, you have this idea where if those data points are
kind of not normalized and not centered and far
away from the origin, then we can still use a
line to separate them, but now if that line
wiggles just a little bit, then our classification is going to get totally destroyed. That kind of means that in the example on the left, the loss function is now extremely sensitive to small perturbations in that linear classifier
in our weight matrix. We can still represent the same functions, but that might make
learning quite difficult because, again, their
loss is very sensitive to our parameter vector,
whereas in the situation on the right, if you take that data cloud and you move it into the
origin and you make it unit variance, then
now, again, we can still classify that data quite well, but now as we wiggle that line a little bit, then our loss function is less sensitive to small perturbations
in the parameter values. That maybe makes optimization
a little bit easier, as we'll see a little bit going forward. By the way, this situation is not only in the linear classification case. Inside a neural network,
remember we kind of have these interleavings of these linear matrix multiplies, or
convolutions, followed by non-linear activation functions. If the input to some layer in your neural network is not centered or not zero mean, not unit variance, then again, small perturbations in the weight matrix of that layer of the network could cause large perturbations in
the output of that layer, which, again, might
make learning difficult. This is kind of a little bit of extra intuition about why normalization might be important. Because we have this
intuition that normalization is so important, we talked
about batch normalization, which is where we just
add this additional layer inside our networks to just force all of the intermediate
activations to be zero mean and unit variance. I've sort of resummarized the batch normalization equations here with the shapes a little
bit more explicitly. Hopefully this can help you out when you're implementing this thing on assignment two. But again, in batch normalization, we have this idea that in the forward pass, we use the statistics of the mini batch to compute a mean and
a standard deviation, and then use those estimates to normalize our data on the forward pass. Then we also reintroduce the scale and shift parameters to increase the expressivity of the layer. You might want to refer back to this when working on assignment two. We also talked last
time a little bit about babysitting the learning process, how you should probably be looking
at your loss curves during training. Here's an example of some networks I was actually training over the weekend. This is usually my setup when I'm working on these things. On the left, I have some plot showing the training loss over time. You can see it's kind of going down, which means my network
is reducing the loss. It's doing well. On the right, there's this plot where the X axis is, again, time,
or the iteration number, and the Y axis is my performance measure both on my training set
and on my validation set. You can see that as we go over time, then my training set performance goes up and up and up and up and
up as my loss function goes down, but at some
point, my validation set performance kind of plateaus. This kind of suggests that maybe I'm overfitting in this situation. Maybe I should have been trying to add additional regularization. We also talked a bit last time about hyperparameter search. All these networks have
sort of a large zoo of hyperparameters. It's pretty important
to set them correctly. We talked a little bit about grid search versus random search,
and how random search is maybe a little bit nicer in theory because in the situation
where your performance might be more sensitive, with respect to one hyperparameter than
other, and random search lets you cover that space
a little bit better. We also talked about the idea of coarse to fine search, where when you're doing this hyperparameter
optimization, probably you want to start with very wide ranges for your hyperparameters, only train for a couple iterations, and then based on those results, you kind of narrow in on the range of
hyperparameters that are good. Now, again, redo your
search in a smaller range for more iterations. You can kind of iterate this process to kind of hone in on the right region for hyperparameters. But again, it's really important to, at the start, have a very coarse range to start with, where
you want very, very wide ranges for all your hyperparameters. Ideally, those ranges should be so wide that your network is kind of blowing up at either end of the
range so that you know that you've searched a wide enough range for those things. Question? - [Student] How many
[speaks too low to hear] optimize at once? [speaks too low to hear] - The question is how many hyperparameters do we typically search at a time? Here is two, but there's
a lot more than two in these typical things. It kind of depends on the exact model and the exact architecture, but because the number of possibilities is exponential in the number of hyperparameters, you can't really test too many at a time. It also kind of depends
on how many machines you have available. It kind of varies from person to person and from experiment to experiment. But generally, I try
not to do this over more than maybe two or three or four at a time at most because, again,
this exponential search just gets out of control. Typically, learning rate
is the really important one that you need to nail first. Then other things, like regularization, like learning rate decay, model size, these other types of things tend to be a little bit less sensitive
than learning rate. Sometimes you might do kind of a block coordinate descent, where you go and find the good learning rate,
then you go back and try to look at different model sizes. This can help you cut down on the exponential search a little bit, but it's a little bit problem dependent on exactly which ones you
should be searching over in which order. More questions? - [Student] [speaks too low to hear] Another parameter, but then changing that other parameter, two or
three other parameters, makes it so that your
learning rate or the ideal learning rate is still
[speaks too low to hear]. - Question is how often
does it happen where when you change one hyperparameter, then the other, the
optimal values of the other hyperparameters change? That does happen sometimes,
although for learning rates, that's typically less of a problem. For learning rates,
typically you want to get in a good range, and
then set it maybe even a little bit lower than
optimal, and let it go for a long time. Then if you do that, combined with some of the fancier optimization
strategies that we'll talk about today,
then a lot of models tend to be a little bit less sensitive to learning rate once you
get them in a good range. Sorry, did you have a
question in front, as well? - [Student] [speaks too low to hear] - The question is what's wrong with having a small learning rate and increasing the number of epochs? The answer is that it might take a very long time. [laughs] - [Student] [speaks too low to hear] - Intuitively, if you
set the learning rate very low and let it go
for a very long time, then this should, in theory, always work. But in practice, those
factors of 10 or 100 actually matter a lot when you're training these things. Maybe if you got the right learning rate, you could train it in six hours, 12 hours or a day, but then if
you just were super safe and dropped it by a factor of 10 or by a factor of 100,
now that one-day training becomes 100 days of training. That's three months. That's not going to be good. When you're taking these intro computer science classes,
they always kind of sweep the constants under the rug, but when you're actually thinking
about training things, those constants end up mattering a lot. Another question? - [Student] If you have
a low learning rate, [speaks too low to hear]. - Question is for a low learning rate, are you more likely to
be stuck in local optima? I think that makes some intuitive sense, but in practice, that seems not to be much of a problem. I think we'll talk a bit
more about that later today. Today I wanted to talk about a couple other really interesting
and important topics when we're training neural networks. In particular, I wanted to talk, we've kind of alluded to this fact of fancier, more powerful
optimization algorithms a couple times. I wanted to spend some
time today and really dig into those and talk about what are the actual optimization
algorithms that most people are using these days. We also touched on regularization in earlier lectures. This concept of making your network do additional things to reduce the gap between train and test error. I wanted to talk about
some more strategies that people are using in practice of regularization, with
respect to neural networks. Finally, I also wanted to talk a bit about transfer learning, where you can sometimes get away with using less data than you think by transferring from one problem to another. If you recall from a few lectures ago, the kind of core strategy in training neural networks is an optimization problem where we write down some loss function, which defines, for each
value of the network weights, the loss function tells us how good or bad is that value of the weights
doing on our problem. Then we imagine that this loss function gives us some nice
landscape over the weights, where on the right, I've shown this maybe small, two-dimensional
problem, where the X and Y axes are two values of the weights. Then the color of the
plot kind of represents the value of the loss. In this kind of cartoon picture of a two-dimensional problem, we're only optimizing over these
two values, W one, W two. The goal is to find the most red region in this case, which
corresponds to the setting of the weights with the lowest loss. Remember, we've been working so far with this extremely simple
optimization algorithm, stochastic gradient descent, where it's super simple, it's three lines. While true, we first evaluate the loss in the gradient on some
mini batch of data. Then we step, updating
our parameter vector in the negative direction of the gradient because this gives, again, the direction of greatest decrease of the loss function. Then we repeat this over and over again, and hopefully we converge
to the red region and we get great errors
and we're very happy. But unfortunately, this relatively simple optimization algorithm has
quite a lot of problems that actually could come up in practice. One problem with stochastic
gradient descent, imagine what happens if
our objective function looks something like this, where, again, we're plotting two
values, W one and W two. As we change one of those values, the loss function changes very slowly. As we change the horizontal
value, then our loss changes slowly. As we go up and down in this landscape, now our loss is very sensitive to changes in the vertical direction. By the way, this is
referred to as the loss having a bad condition
number at this point, which is the ratio between
the largest and smallest singular values of the Hessian matrix at that point. But the intuitive idea is
that the loss landscape kind of looks like a taco shell. It's sort of very
sensitive in one direction, not sensitive in the other direction. The question is what might SGD, stochastic gradient
descent, do on a function that looks like this? If you run stochastic gradient descent on this type of function, you might get this characteristic zigzagging behavior, where because for this
type of objective function, the direction of the
gradient does not align with the direction towards the minima. When you compute the
gradient and take a step, you might step sort of over this line and sort of zigzag back and forth. In effect, you get very
slow progress along the horizontal dimension, which is the less sensitive dimension, and you get this zigzagging, nasty, nasty
zigzagging behavior across the fast-changing dimension. This is undesirable behavior. By the way, this problem actually becomes much more common in high dimensions. In this kind of cartoon
picture, we're only showing a two-dimensional
optimization landscape, but in practice, our
neural networks might have millions, tens of millions,
hundreds of millions of parameters. That's hundreds of millions of directions along which this thing can move. Now among those hundreds of millions of different directions to move, if the ratio between the largest one and the smallest one is bad, then SGD will not perform so nicely. You can imagine that if we
have 100 million parameters, probably the maximum
ratio between those two will be quite large. I think this is actually
quite a big problem in practice for many
high-dimensional problems. Another problem with SGD
has to do with this idea of local minima or saddle points. Here I've sort of swapped
the graph a little bit. Now the X axis is showing the value of one parameter, and then the Y axis is showing the value of the loss. In this top example, we have kind of this curvy objective function, where there's a valley in the middle. What happens to SGD in this situation? - [Student] [speaks too low to hear] - In this situation, SGD will get stuck because at this local minima, the gradient is zero because it's locally flat. Now remember with SGD,
we compute the gradient and step in the direction
of opposite gradient, so if at our current point,
the opposite gradient is zero, then we're not
going to make any progress, and we'll get stuck at this point. There's another problem with this idea of saddle points. Rather than being a local minima, you can imagine a point
where in one direction we go up, and in the other
direction we go down. Then at our current point,
the gradient is zero. Again, in this situation, the function will get stuck at the saddle point because the gradient is zero. Although one thing I'd like to point out is that in one dimension,
in a one-dimensional problem like this, local minima
seem like a big problem and saddle points seem like kind of not something to worry about, but in fact, it's the opposite once you move to very high-dimensional problems because, again, if you think about you're in this 100 million dimensional space, what does a saddle point mean? That means that at my current point, some directions the loss goes up, and some directions the loss goes down. If you have 100 million dimensions, that's probably going to
happen more frequently than, that's probably going to happen
almost everywhere, basically. Whereas a local minima
says that of all those 100 million directions that I can move, every one of them causes
the loss to go up. In fact, that seems pretty rare when you're thinking about, again, these very high-dimensional problems. Really, the idea that has come to light in the last few years is that when you're training these very
large neural networks, the problem is more about saddle points and less about local minima. By the way, this also is a problem not just exactly at the saddle point, but also near the saddle point. If you look at the example on the bottom, you see that in the regions around the saddle point, the gradient isn't zero, but the slope is very small. That means that if we're,
again, just stepping in the direction of the gradient, and that gradient is very
small, we're going to make very, very slow progress whenever our current parameter value
is near a saddle point in the objective landscape. This is actually a big problem. Another problem with SGD comes from the S. Remember that SGD is
stochastic gradient descent. Recall that our loss function is typically defined by computing the loss over many, many different examples. In this case, if N is
your whole training set, then that could be
something like a million. Each time computing
the loss would be very, very expensive. In practice, remember
that we often estimate the loss and estimate the gradient using a small mini batch of examples. What this means is that we're not actually getting the true information
about the gradient at every time step. Instead, we're just
getting some noisy estimate of the gradient at our current point. Here on the right, I've kind of faked this plot a little bit. I've just added random uniform noise to the gradient at every point, and then run SGD with these noisy,
messed up gradients. This is maybe not exactly what happens with the SGD process,
but it still give you the sense that if there's noise in your gradient estimates, then vanilla SGD kind of meanders around the space and might actually take a long time to get towards the minima. Now that we've talked about a lot of these problems. Sorry, was there a question? - [Student] [speaks too low to hear] - The question is do all of these just go away if we use
normal gradient descent? Let's see. I think that the taco shell problem of high condition numbers
is still a problem with full batch gradient descent. The noise. As we'll see, we might sometimes introduce additional noise into the network, not only due to sampling mini batches, but also due to explicit
stochasticity in the network, so we'll see that later. That can still be a problem. Saddle points, that's still a problem for full batch gradient descent because there can still be saddle points in the full objective landscape. Basically, even if we go to full batch gradient descent, it doesn't really solve these problems. We kind of need to think
about a slightly fancier optimization algorithm that can try to address these concerns. Thankfully, there's a really,
really simple strategy that works pretty well at addressing many of these problems. That's this idea of adding a momentum term to our stochastic gradient descent. Here on the left, we have our classic old friend, SGD, where we just always step in the direction of the gradient. But now on the right, we have this minor, minor variance called SGD plus momentum, which is now two equations
and five lines of code, so it's twice as complicated. But it's very simple. The idea is that we maintain a velocity over time, and we add
our gradient estimates to the velocity. Then we step in the
direction of the velocity, rather than stepping in the
direction of the gradient. This is very, very simple. We also have this hyperparameter rho now which corresponds to friction. Now at every time step, we take our current velocity, we
decay the current velocity by the friction constant,
rho, which is often something high, like
.9 is a common choice. We take our current velocity, we decay it by friction and we add in our gradient. Now we step in the direction
of our velocity vector, rather than the direction of our raw gradient vector. This super, super simple strategy actually helps for all of these problems that we just talked about. If you think about what
happens at local minima or saddle points, then if we're imagining velocity in this system,
then you kind of have this physical interpretation of this ball kind of rolling down the
hill, picking up speed as it comes down. Now once we have velocity,
then even when we pass that point of local minima, the point will still have velocity, even if it doesn't have gradient. Then we can hopefully get
over this local minima and continue downward. There's this similar
intuition near saddle points, where even though the gradient around the saddle point is very small, we have this velocity vector that we've built up as we roll downhill. That can hopefully carry us through the saddle point and
let us continue rolling all the way down. If you think about what happens in poor conditioning, now if we were to have these kind of zigzagging approximations to the gradient, then those zigzags will hopefully cancel
each other out pretty fast once we're using momentum. This will effectively reduce the amount by which we step in the
sensitive direction, whereas in the horizontal direction, our velocity will just keep building up, and will actually accelerate our descent across that less sensitive dimension. Adding momentum here can actually help us with this high condition
number problem, as well. Finally, on the right, we've repeated the same visualization of
gradient descent with noise. Here, the black is this vanilla SGD, which is sort of zigzagging
all over the place, where the blue line is showing now SGD with momentum. You can see that because we're adding it, we're building up this velocity over time, the noise kind of gets averaged out in our gradient estimates. Now SGD ends up taking
a much smoother path towards the minima, compared with the SGD, which is kind of meandering due to noise. Question? - [Student] [speaks too low to hear] - The question is how does SGD momentum help with the poorly
conditioned coordinate? The idea is that if you go back and look at this velocity estimate and look at the velocity
computation, we're adding in the gradient at every time step. It kind of depends on your setting of rho, that hyperparameter, but you can imagine that if the gradient is relatively small, and if rho is well
behaved in this situation, then our velocity could
actually monotonically increase up to a point where the velocity could now be larger than the actual gradient. Then we might actually
make faster progress along the poorly conditioned dimension. Kind of one picture that
you can have in mind when we're doing SGD plus momentum is that the red here is our current point. At our current point,
we have some red vector, which is the direction of the gradient, or rather our estimate of the gradient at the current point. Green is now the direction
of our velocity vector. Now when we do the momentum update, we're actually stepping according to a weighted average of these two. This helps overcome some noise in our gradient estimate. There's a slight variation of momentum that you sometimes see, called Nesterov accelerated gradient, also sometimes called Nesterov momentum. That switches up this order of things a little bit. In sort of normal SGD momentum, we imagine that we estimate the gradient
at our current point, and then take a mix of our velocity and our gradient. With Nesterov accelerated gradient, you do something a little bit different. Here, you start at the red point. You step in the direction
of where the velocity would take you. You evaluate the gradient at that point. Then you go back to your original point and kind of mix together those two. This is kind of a funny interpretation, but you can imagine that you're kind of mixing together information
a little bit more. If your velocity direction was actually a little bit wrong, it
lets you incorporate gradient information
from a little bit larger parts of the objective landscape. This also has some really
nice theoretical properties when it comes to convex optimization, but those guarantees go a little bit out the window once it
comes to non-convex problems like neural networks. Writing it down in
equations, Nesterov momentum looks something like this, where now to update our velocity, we take a step, according to our previous
velocity, and evaluate that gradient there. Now when we take our next step, we actually step in the
direction of our velocity that's incorporating information from these multiple points. Question? - [Student] [speaks too low to hear] - Oh, sorry. The question is what's
a good initialization for the velocity? This is almost always zero. It's not even a hyperparameter. Just set it to zero and don't worry. Another question? - [Student] [speaks too low to hear] - Intuitively, the velocity
is kind of a weighted sum of your gradients that
you've seen over time. - [Student] [speaks too low to hear] - With more recent gradients
being weighted heavier. At every time step, we
take our old velocity, we decay by friction and we add in our current gradient. You can kind of think of this as a smooth moving average
of your recent gradients with kind of a exponentially
decaying weight on your gradients going back in time. This Nesterov formulation
is a little bit annoying 'cause if you look at
this, normally when you have your loss function,
you want to evaluate your loss and your
gradient at the same point. Nesterov breaks this a little bit. It's a little bit annoying to work with. Thankfully, there's a
cute change of variables you can do. If you do the change of variables and reshuffle a little bit, then you can write Nesterov momentum in
a slightly different way that now, again, lets
you evaluate the loss and the gradient at the same point always. Once you make this change of variables, you get kind of a nice
interpretation of Nesterov, which is that here in the first step, this looks exactly like
updating the velocity in the vanilla SGD momentum case, where we have our current velocity,
we evaluate gradient at the current point and
mix these two together in a decaying way. Now in the second update,
now when we're actually updating our parameter vector, if you look at the second equation,
we have our current point plus our current velocity plus a weighted difference
between our current velocity and our previous velocity. Here, Nesterov momentum
is kind of incorporating some kind of error-correcting term between your current velocity and
your previous velocity. If we look at SGD, SGD momentum and Nesterov momentum
on this kind of simple problem, compared with SGD, we notice that SGD kind of takes this,
SGD is in the black, kind of taking this slow
progress toward the minima. The blue and the green show momentum and Nesterov. These have this behavior
of kind of overshooting the minimum 'cause they're
building up velocity going past the minimum, and then kind of correcting themselves and coming back towards the minima. Question? - [Student] [speaks too low to hear] - The question is this picture looks good, but what happens if your minima call lies in this very narrow basin? Will the velocity just cause you to skip right over that minima? That's actually a really
interesting point, and the subject of some
recent theoretical work, but the idea is that maybe those really sharp minima are actually bad minima. We don't want to even land in those 'cause the idea is that maybe if you have a very sharp minima, that actually could be a minima that overfits more. If you imagine that we
doubled our training set, the whole optimization
landscape would change, and maybe that very sensitive minima would actually disappear
if we were to collect more training data. We kind of have this intuition that we maybe want to land in very flat minima because those very flat
minima are probably more robust as we change
the training data. Those flat minima might
actually generalize better to testing data. This is again, sort of very recent theoretical work, but that's actually a really good point that you bring it up. In some sense, it's actually a feature and not a bug that SGD momentum actually skips over those very sharp minima. That's actually a good
thing, believe it or not. Another thing you can see is if you look at the difference between
momentum and Nesterov here, you can see that because of the correction factor in
Nesterov, maybe it's not overshooting quite as drastically, compared to vanilla momentum. There's another kind
of common optimization strategy is this algorithm called AdaGrad, which John Duchi, who's
now a professor here, worked on during his Ph.D. The idea with AdaGrad is that as you, during the course of the optimization, you're going to keep a running estimate or a running sum of all
the squared gradients that you see during training. Now rather than having a velocity term, instead we have this grad squared term. During training, we're
going to just keep adding the squared gradients to
this grad squared term. Now when we update our parameter vector, we'll divide by this grad squared term when we're making our update step. The question is what
does this kind of scaling do in this situation where we have a very high condition number? - [Student] [speaks too low to hear] - The idea is that if
we have two coordinates, one that always has a very high gradient and one that always has
a very small gradient, then as we add the sum of the squares of the small gradient,
we're going to be dividing by a small number, so
we'll accelerate movement along the slow dimension, along the one dimension. Then along the other
dimension, where the gradients tend to be very large,
then we'll be dividing by a large number, so
we'll kind of slow down our progress along the wiggling dimension. But there's kind of a problem here. That's the question of
what happens with AdaGrad over the course of training, as t gets larger and larger and larger? - [Student] [speaks too low to hear] - With AdaGrad, the steps
actually get smaller and smaller and smaller because we just continue updating this estimate of the squared gradients over
time, so this estimate just grows and grows
and grows monotonically over the course of training. Now this causes our
step size to get smaller and smaller and smaller over time. Again, in the convex case, there's some really nice theory showing
that this is actually really good 'cause in the convex case, as you approach a
minimum, you kind of want to slow down so you actually converge. That's actually kind of a feature in the convex case. But in the non-convex
case, that's a little bit problematic because as you come towards a saddle point, you might
get stuck with AdaGrad, and then you kind of no
longer make any progress. There's a slight variation of AdaGrad, called RMSProp, that actually addresses this concern a little bit. Now with RMSProp, we
still keep this estimate of the squared gradients, but instead of just letting that squared estimate continually accumulate over training, instead, we let that squared
estimate actually decay. This ends up looking kind
of like a momentum update, except we're having kind of momentum over the squared gradients,
rather than momentum over the actual gradients. Now with RMSProp, after
we compute our gradient, we take our current estimate
of the grad squared, we multiply it by this decay rate, which is commonly
something like .9 or .99. Then we add in this one
minus the decay rate of our current squared gradient. Now over time, you can imagine that. Then again, when we
make our step, the step looks exactly the same as AdaGrad, where we divide by the squared gradient in the step to again
have this nice property of accelerating movement
along the one dimension, and slowing down movement
along the other dimension. But now with RMSProp,
because these estimates are leaky, then it kind
of addresses the problem of maybe always slowing down where you might not want to. Here again, we're kind
of showing our favorite toy problem with SGD
in black, SGD momentum in blue and RMSProp in red. You can see that RMSProp and SGD momentum are both doing much better than SGD, but their qualitative behavior
is a little bit different. With SGD momentum, it kind of overshoots the minimum and comes back, whereas with RMSProp, it's kind of adjusting its trajectory such that we're making approximately equal progress among all the dimensions. By the way, you can't actually tell, but this plot is also
showing AdaGrad in green with the same learning rate, but it just gets stuck due to this
problem of continually decaying learning rates. In practice, AdaGrad
is maybe not so common for many of these things. That's a little bit of
an unfair comparison of AdaGrad. Probably you need to
increase the learning rate with AdaGrad, and then
it would end up looking kind of like RMSProp in this case. But in general, we tend not to use AdaGrad so much when training neural networks. Question? - [Student] [speaks too low to hear] - The answer is yes,
this problem is convex, but in this case, it's a little bit of an unfair comparison because the learning rates
are not so comparable among the methods. I've been a little bit unfair to AdaGrad in this visualization by showing the same learning rate between
the different algorithms, when probably you should have separately turned the learning rates per algorithm. We saw in momentum, we had this idea of velocity, where we're
building up velocity by adding in the gradients,
and then stepping in the direction of the velocity. We saw with AdaGrad and RMSProp that we had this other idea, of
building up an estimate of the squared gradients,
and then dividing by the squared gradients. Then these both seem like good ideas on their own. Why don't we just stick 'em together and use them both? Maybe that would be even better. That brings us to this
algorithm called Adam, or rather brings us very close to Adam. We'll see in a couple
slides that there's a slight correction we need to make here. Here with Adam, we maintain an estimate of the first moment and the second moment. Now in the red, we make this estimate of the first moment as a weighed sum of our gradients. We have this moving estimate
of the second moment, like AdaGrad and like RMSProp, which is a moving estimate of our squared gradients. Now when we make our update step, we step using both the first
moment, which is kind of our velocity, and also divide
by the second moment, or rather the square root
of the second moment, which is this squared gradient term. This idea of Adam ends
up looking a little bit like RMSProp plus momentum, or ends up looking like momentum plus
second squared gradients. It kind of incorporates the
nice properties of both. But there's a little
bit of a problem here. That's the question of what happens at the very first time step? At the very first time step, you can see that at the beginning, we've initialized our second moment with zero. Now after one update of the second moment, typically this beta two, second moment decay rate, is something like .9 or .99, something very close to one. After one update, our
second moment is still very, very close to zero. Now when we're making our update step here and we divide by our second moment, now we're dividing by a very small number. We're making a very, very large step at the beginning. This very, very large
step at the beginning is not really due to the
geometry of the problem. It's kind of an artifact
of the fact that we initialized our second
moment estimate was zero. Question? - [Student] [speaks too low to hear] - That's true. The comment is that if your first moment is also very small,
then you're multiplying by small and you're
dividing by square root of small squared, so
what's going to happen? They might cancel each other
out, you might be okay. That's true. Sometimes these cancel each other out and you're okay, but
sometimes this ends up in taking very large steps
right at the beginning. That can be quite bad. Maybe you initialize a little bit poorly. You take a very large step. Now your initialization
is completely messed up, and then you're in a very bad part of the objective landscape
and you just can't converge from there. Question? - [Student] [speaks too low to hear] - The idea is what is this 10 to the minus seven term
in the last equation? That's actually appeared in AdaGrad, RMSProp and Adam. The idea is that we're
dividing by something. We want to make sure we're
not dividing by zero, so we always add a small positive constant to the denominator, just to make sure we're not dividing by zero. That's technically a hyperparameter, but it tends not to matter too much, so just setting 10 to minus seven, 10 to minus eight, something like that, tends to work well. With Adam, remember we just talked about this idea of at the first couple steps, it gets very large, and we might take very large steps and mess ourselves up. Adam also adds this bias correction term to avoid this problem of
taking very large steps at the beginning. You can see that after we update our first and second moments, we
create an unbiased estimate of those first and second
moments by incorporating the current time step, t. Now we actually make our step using these unbiased estimates,
rather than the original first and second moment estimates. This gives us our full form of Adam. By the way, Adam is a
really, [laughs] really good optimization algorithm,
and it works really well for a lot of different
problems, so that's kind of my default optimization
algorithm for just about any new problem that I'm tackling. In particular, if you
set beta one equals .9, beta two equals .999, learning rate one e minus three or five e minus four, that's a great staring
point for just about all the architectures
I've ever worked with. Try that. That's a really good place
to start, in general. [laughs] If we actually plot these things out and look at SGD, SGD momentum, RMSProp and Adam on the same problem, you can see that Adam, in the purple here, kind of combines elements
of both SGD momentum and RMSProp. Adam kind of overshoots the minimum a little bit like SGD
momentum, but it doesn't overshoot quite as much as momentum. Adam also has this similar behavior of RMSProp of kind of trying to curve to make equal progress
along all dimensions. Maybe in this small
two-dimensional example, Adam converged about
similarly to other ones, but you can see qualitatively that it's kind of combining
the behaviors of both momentum and RMSProp. Any questions about
optimization algorithms? - [Student] [speaks too low to hear] They still take a very long time to train. [speaks too low to hear] - The question is what does Adam not fix? Would these neural
networks are still large, they still take a long time to train. There can still be a problem. In this picture where
we have this landscape of things looking like
ovals, if you imagine that we're kind of making estimates along each dimension independently to allow us to speed up or slow down along different coordinate axes, but one problem is that if that taco shell is kind of tilted and is not axis aligned, then we're still only making estimates
along the individual axes independently. That corresponds to taking your rotated taco shell and squishing it horizontally and vertically, but you
can't actually unrotate it. In cases where you have this kind of rotated picture of poor conditioning, then Adam or any of these other algorithms really can't address that, that concern. Another thing that we've seen in all these optimization
algorithms is learning rate as a hyperparameter. We've seen this picture
before a couple times, that as you use different learning rates, sometimes if it's too
high, it might explode in the yellow. If it's a very low
learning rate, in the blue, it might take a very
long time to converge. It's kind of tricky to pick the right learning rate. This is a little bit of a trick question because we don't actually have to stick with one learning rate
throughout the course of training. Sometimes you'll see people
decay the learning rates over time, where we can kind of combine the effects of these different curves on the left, and get the
nice properties of each. Sometimes you'll start
with a higher learning rate near the start of training, and then decay the learning rate and make it smaller and smaller throughout
the course of training. A couple strategies for
these would be a step decay, where at 100,000th
iteration, you just decay by some factor and you keep going. You might see an exponential decay, where you continually
decay during training. You might see different variations of continually decaying the learning rate during training. If you look at papers,
especially the resonate paper, you often see plots that
look kind of like this, where the loss is kind of going down, then dropping, then flattening again, then dropping again. What's going on in these plots is that they're using a step decay learning rate, where at these parts where it plateaus and then suddenly drops
again, those are the iterations where they
dropped the learning rate by some factor. This idea of dropping the learning rate, you might imagine that it got near some good region, but now
the gradients got smaller, it's kind of bouncing around too much. Then if we drop the learning rate, it lets it slow down and continue to make progress down the landscape. This tends to help in practice sometimes. Although one thing to point out is that learning rate decay is
a little bit more common with SGD momentum, and
a little bit less common with something like Adam. Another thing I'd like
to point out is that learning rate decay is kind of a second-order hyperparameter. You typically should not optimize over this thing from the start. Usually when you're
kind of getting networks to work at the beginning, you want to pick a good learning rate with
no learning rate decay from the start. Trying to cross-validate jointly over learning rate decay and
initial learning rate and other things, you'll
just get confused. What you do for setting
learning rate decay is try with no decay, see what happens. Then kind of eyeball
the loss curve and see where you think you might need decay. Another thing I wanted to mention briefly is this idea of all these algorithms that we've talked about are first-order optimization algorithms. In this picture, in this
one-dimensional picture, we have this kind of
curvy objective function at our current point in red. What we're basically doing is computing the gradient at that point. We're using the gradient
information to compute some linear approximation to our function, which is kind of a first-order
Taylor approximation to our function. Now we pretend that the
first-order approximation is our actual function, and we make a step to try to minimize the approximation. But this approximation doesn't hold for very large regions, so we can't step too far in that direction. But really, the idea
here is that we're only incorporating information about the first derivative of the function. You can actually go a little bit fancier. There's this idea of
second-order approximation, where we take into account
both first derivative and second derivative information. Now we make a second-order
Taylor approximation to our function and kind
of locally approximate our function with a quadratic. Now with a quadratic, you can step right to the minimum, and you're really happy. That's this idea of
second-order optimization. When you generalize this
to multiple dimensions, you get something called the Newton step, where you compute this Hessian matrix, which is a matrix of second derivatives, and you end up inverting
this Hessian matrix in order to step directly to the minimum of this quadratic
approximation to your function. Does anyone spot something
that's quite different about this update rule,
compared to the other ones that we've seen? - [Student] [speaks too low to hear] - This doesn't have a learning rate. That's kind of cool. We're making this quadratic approximation and we're stepping right to the minimum of the quadratic. At least in this vanilla
version of Newton's method, you don't actually need a learning rate. You just always step to the minimum at every time step. However, in practice, you might end up, have a learning rate
anyway because, again, that quadratic approximation
might not be perfect, so you might only want
to step in the direction towards the minimum, rather than actually stepping to the minimum, but at least in this vanilla version, it doesn't have a learning rate. But unfortunately, this is maybe a little bit impractical for deep learning because this Hessian matrix is N by N, where N is
the number of parameters in your network. If N is 100 million,
then 100 million squared is way too big. You definitely can't store that in memory, and you definitely can't invert it. In practice, people sometimes use these quasi-Newton methods
that, rather than working with the full Hessian and inverting the full Hessian, they
work with approximations. Low-rank approximations are common. You'll sometimes see
these for some problems. L-BFGS is one particular
second-order optimizer that has this approximate second, keeps this approximation of the Hessian that you'll sometimes
see, but in practice, it doesn't work too well for many deep learning problems
because these approximations, these second-order
approximations, don't really handle the stochastic case very much, very nicely. They also tend not to work so well with non-convex problems. I don't want to get into
that right now too much. In practice, what you should really do is probably Adam is a really good choice for many different neural network things, but if you're in a situation where you can afford to do full batch updates, and you know that your
problem doesn't have really any stochasticity, then L-BFGS is kind of a good choice. L-BFGS doesn't really
get used for training neural networks too much, but as we'll see in a couple of lectures, it does sometimes get used for things like style transfer, where you actually have less stochasticity and fewer parameters, but you still want to solve an optimization problem. All of these strategies we've talked about so far are about reducing training error. All these optimization
algorithms are really about driving down your training error and minimizing your objective function, but we don't really care about training error that much. Instead, we really care
about our performance on unseen data. We really care about reducing this gap between train and test error. The question is once we're already good at optimizing our objective function, what can we do to try to reduce this gap and make our model perform better on unseen data? One really quick and dirty, easy thing to try is this idea of model ensembles that sometimes works
across many different areas in machine learning. The idea is pretty simple. Rather than having just one model, we'll train 10 different
models independently from different initial random restarts. Now at test time, we'll run our data through all of the 10 models and average the predictions of those 10 models. Adding these multiple models together tends to reduce overfitting a little bit and tend to improve
performance a little bit, typically by a couple percent. This is generally not
a drastic improvement, but it is a consistent improvement. You'll see that in competitions, like ImageNet and other things like that, using model ensembles is very common to get maximal performance. You can actually get a little
bit creative with this. Sometimes rather than
training separate models independently, you can just keep multiple snapshots of your model during the course of training, and then use these as your ensembles. Then you still, at test
time, need to average the predictions of these
multiple snapshots, but you can collect the snapshots during the course of training. There's actually a very
nice paper being presented at ICLR this week that kind of has a fancy version of this idea, where we use a crazy learning rate schedule, where our learning rate goes very slow, then very fast, then very
slow, then very fast. The idea is that with this crazy learning rate schedule,
then over the course of training, the model
might be able to converge to different regions in
the objective landscape that all are reasonably good. If you do an ensemble over these different snapshots, then you can improve your performance quite nicely, even though you're only
training the model once. Questions? - [Student] [speaks too low to hear] - The question is, it's bad when there's a large gap between error 'cause that means you're overfitting, but if there's no gap, then
is that also maybe bad? Do we actually want
some small, optimal gap between the two? We don't really care about the gap. What we really care about is maximizing the performance on the validation set. What tends to happen is that if you don't see a gap, then
you could have improved your absolute performance, in many cases, by overfitting a little bit more. There's this weird correlation between the absolute performance
on the validation set and the size of that gap. We only care about absolute performance. Question in the back? - [Student] Are hyperparameters the same for the ensemble? - Are the hyperparameters the same for the ensembles? That's a good question. Sometimes they're not. You might want to try
different sizes of the model, different learning rates, different regularization strategies
and ensemble across these different things. That actually does happen sometimes. Another little trick you can do sometimes is that during training,
you might actually keep an exponentially decaying average of your parameter vector
itself to kind of have a smooth ensemble of your own network during training. Then use this smoothly decaying average of your parameter vector, rather than the actual checkpoints themselves. This is called Polyak averaging, and it sometimes helps a little bit. It's just another one
of these small tricks you can sometimes add, but it's not maybe too common in practice. Another question you might have is that how can we actually
improve the performance of single models? When we have ensembles,
we still need to run, like, 10 models at test time. That's not so great. We really want some strategies to improve the performance of our single models. That's really this idea of regularization, where we add something to our model to prevent it from
fitting the training data too well in the attempts
to make it perform better on unseen data. We've seen a couple
ideas, a couple methods for regularization already, where we add some explicit extra term to the loss. Where we have this one
term telling the model to fit the data, and another term that's a regularization term. You saw this in homework
one, where we used L2 regularization. As we talked about in lecture a couple lectures ago, this L2
regularization doesn't really make maybe a lot
of sense in the context of neural networks. Sometimes we use other
things for neural networks. One regularization strategy that's super, super common for neural networks is this idea of dropout. Dropout is super simple. Every time we do a forward pass through the network, at every
layer, we're going to randomly set some neurons to zero. Every time we do a forward pass, we'll set a different random subset of the neurons to zero. This kind of proceeds one layer at a time. We run through one layer, we compute the value of the layer, we randomly set some of them to zero,
and then we continue up through the network. Now if you look at this
fully connected network on the left versus a dropout version of the same network on
the right, you can see that after we do dropout, it kind of looks like a smaller version
of the same network, where we're only using
some subset of the neurons. This subset that we use
varies at each iteration, at each forward pass. Question? - [Student] [speaks too low to hear] - The question is what
are we setting to zero? It's the activations. Each layer is computing
previous activation times the weight matrix gives you our next activation. Then you just take that activation, set some of them to zero, and then your next layer will be
partially zeroed activations times another matrix give
you your next activations. Question? - [Student] [speaks too low to hear] - Question is which
layers do you do this on? It's more common in
fully connected layers, but you sometimes see this in
convolutional layers, as well. When you're working in
convolutional layers, sometimes instead of dropping each activation randomly,
instead you sometimes might drop entire feature maps randomly. In convolutions, you have
this channel dimension, and you might drop out entire channels, rather than random elements. Dropout is kind of super
simple in practice. It only requires adding two lines, one line per dropout call. Here we have a three-layer neural network, and we've added dropout. You can see that all we needed to do was add this extra line where we randomly set some things to zero. This is super easy to implement. But the question is why
is this even a good idea? We're seriously messing with the network at training time by setting a bunch of its values to zero. How can this possibly make sense? One sort of slightly hand wavy idea that people have is that
dropout helps prevent co-adaptation of features. Maybe if you imagine that we're trying to classify cats, maybe in some universe, the network might learn one neuron for having an ear, one
neuron for having a tail, one neuron for the input being furry. Then it kind of combines
these things together to decide whether or not it's a cat. But now if we have dropout, then in making the final decision about
catness, the network cannot depend too much on any of these one features. Instead, it kind of needs to distribute its idea of catness across
many different features. This might help prevent
overfitting somehow. Another interpretation of dropout that's come out a little bit more recently is that it's kind of like
doing model ensembling within a single model. If you look at the picture on the left, after you apply dropout to the network, we're kind of computing this subnetwork using some subset of the neurons. Now every different potential dropout mask leads to a different potential subnetwork. Now dropout is kind of
learning a whole ensemble of networks all at the same time that all share parameters. By the way, because of
the number of potential dropout masks grows
exponentially in the number of neurons, you're never going to sample all of these things. This is really a gigantic,
gigantic ensemble of networks that are all
being trained simultaneously. Then the question is what
happens at test time? Once we move to dropout,
we've kind of fundamentally changed the operation
of our neural network. Previously, we've had
our neural network, f, be a function of the weights, w, and the inputs, x, and then produce the output, y. But now, our network is also taking this additional input, z, which is some random dropout mask. That z is random. Having randomness at
test time is maybe bad. Imagine that you're working at Facebook, and you want to classify the images that people are uploading. Then today, your image
gets classified as a cat, and tomorrow it doesn't. That would be really weird and really bad. You'd probably want to eliminate this stochasticity at test
time once the network is already trained. Then we kind of want to average out this randomness. If you write this out, you can imagine actually marginalizing out this randomness with some integral, but in practice, this integral is totally intractable. We don't know how to evaluate this thing. You're in bad shape. One thing you might imagine doing is approximating this
integral via sampling, where you draw multiple samples of z and then average them out at test time, but this still would
introduce some randomness, which is little bit bad. Thankfully, in the case of dropout, we can actually approximate this integral in kind of a cheap way locally. If we consider a single
neuron, the output is a, the inputs are x and y, with two weights, w one, w two. Then at test time, our value a is just w one x plus w two y. Now imagine that we
trained to this network. During training, we used
dropout with probability 1/2 of dropping our neurons. Now the expected value
of a during training, we can kind of compute analytically for this small case. There's four possible dropout masks, and we're going to average out the values across these four masks. We can see that the expected value of a during training is 1/2
w one x plus w two y. There's this disconnect between this average value of w one x plus w two y at test time, and at training time, the average value is only 1/2 as much. One cheap thing we can do is that at test time, we don't
have any stochasticity. Instead, we just multiply this output by the dropout probability. Now these expected values are the same. This is kind of like a
local cheap approximation to this complex integral. This is what people really commonly do in practice with dropout. At dropout, we have this predict function, and we just multiply
our outputs of the layer by the dropout probability. The summary of dropout is
that it's really simple on the forward pass. You're just adding two
lines to your implementation to randomly zero out some nodes. Then at the test time prediction function, you just added one little multiplication by your probability. Dropout is super simple. It tends to work well sometimes for regularizing neural networks. By the way, one common
trick you see sometimes is this idea of inverted dropout. Maybe at test time, you
care more about efficiency, so you want to eliminate
that extra multiplication by p at test time. Then what you can do is, at test time, you use the entire weight matrix, but now at training time, instead you divide by p because training is
probably happening on a GPU. You don't really care if you do one extra multiply at training time, but then at test time, you kind of want this thing to be as efficient as possible. Question? - [Student] [speaks too low to hear] Now the gradient [speaks too low to hear]. - The question is what
happens to the gradient during training with dropout? You're right. We only end up propagating the gradients through the nodes that were not dropped. This has the consequence that when you're training with dropout, typically training takes longer because at each step, you're only updating some subparts of the network. When you're using dropout, it typically takes longer to train, but you might have a better generalization
after it's converged. Dropout, we kind of saw is like this one concrete instantiation. There's a little bit more general strategy for regularization where during training we add some kind of
randomness to the network to prevent it from fitting
the training data too well. To kind of mess it up and prevent it from fitting the training data perfectly. Now at test time, we want to average out all that randomness to hopefully improve our generalization. Dropout is probably
the most common example of this type of strategy, but actually batch normalization kind
of fits this idea, as well. Remember in batch
normalization, during training, one data point might appear
in different mini batches with different other data points. There's a bit of
stochasticity with respect to a single data point with how exactly that point gets normalized
during training. But now at test time,
we kind of average out this stochasticity by using some global estimates to normalize, rather than the per mini batch estimates. Actually batch normalization tends to have kind of a similar regularizing effect as dropout because they both introduce some kind of stochasticity or noise at training time, but then average it out at test time. Actually, when you train networks with batch normalization,
sometimes you don't use dropout at all, and just
the batch normalization adds enough of a regularizing effect to your network. Dropout is somewhat nice because you can actually tune the regularization strength by varying that parameter
p, and there's no such control in batch normalization. Another kind of strategy that fits in this paradigm is this
idea of data augmentation. During training, in a vanilla version for training, we have our
data, we have our label. We use it to update our
CNN at each time step. But instead, what we can do is randomly transform the image in
some way during training such that the label is preserved. Now we train on these
random transformations of the image rather than
the original images. Sometimes you might see
random horizontal flips 'cause if you take a cat and flip it horizontally, it's still a cat. You'll randomly sample
crops of different sizes from the image because the random crop of the cat is still a cat. Then during testing,
you kind of average out this stochasticity by evaluating with some fixed set of crops, often the four corners and the middle and their flips. What's very common is that when you read, for example, papers on
ImageNet, they'll report a single crop performance of their model, which is just like the whole image, and a 10 crop performance of their model, which are these five standard crops plus their flips. Also with data augmentation,
you'll sometimes use color jittering,
where you might randomly vary the contrast or
brightness of your image during training. You can get a little bit more complex with color jittering,
as well, where you try to make color jitters that are maybe in the PCA directions of your
data space or whatever, where you do some color jittering in some data-dependent way, but that's a little bit less common. In general, data
augmentation is this really general thing that you can apply to just about any problem. Whatever problem you're trying to solve, you kind of think about what are the ways that I can transform my data without changing the label? Now during training, you just apply these random transformations
to your input data. This sort of has a regularizing effect on the network because
you're, again, adding some kind of stochasticity
during training, and then marginalizing
it out at test time. Now we've seen three
examples of this pattern, dropout, batch normalization,
data augmentation, but there's many other examples, as well. Once you have this pattern in your mind, you'll kind of recognize this thing as you read other papers sometimes. There's another kind of
related idea to dropout called DropConnect. With DropConnect, it's the same idea, but rather than zeroing
out the activations at every forward pass, instead we randomly zero out some of the values
of the weight matrix instead. Again, it kind of has this similar flavor. Another kind of cool idea that I like, this one's not so commonly used, but I just think it's a really cool idea, is this idea of fractional max pooling. Normally when you do
two-by-two max pooling, you have these fixed two-by-two regions over which you pool over
in the forward pass, but now with fractional max pooling, every time we have our pooling layer, we're going to randomize exactly the pool that the regions over which we pool. Here in the example on the right, I've shown three different sets of random pooling regions
that you might see during training. Now during test time, you kind of average the stochasticity out by
trying many different, by either sticking to some
fixed set of pooling regions. or drawing many samples
and averaging over them. That's kind of a cool idea, even though it's not so commonly used. Another really kind of surprising paper in this paradigm that actually came out in the last year, so this is new since the last time we taught
the class, is this idea of stochastic depth. Here we have a network on the left. The idea is that we have
a very deep network. We're going to randomly
drop layers from the network during training. During training, we're going to eliminate some layers and only use some subset of the layers during training. Now during test time, we'll
use the whole network. This is kind of crazy. It's kind of amazing that this works, but this tends to have kind of a similar regularizing effect as dropout and these other strategies. But again, this is super,
super cutting-edge research. This is not super
commonly used in practice, but it is a cool idea. Any last minute questions
about regularization? No? Use it. It's a good idea. Yeah? - [Student] [speaks too low to hear] - The question is do you usually use more than one regularization method? You should generally be
using batch normalization as kind of a good thing to have in most networks nowadays because it helps you converge, especially
for very deep things. In many cases, batch normalization alone tends to be enough, but then sometimes if batch normalization
alone is not enough, then you can consider adding dropout or other thing once you see
your network overfitting. You generally don't do
a blind cross-validation over these things. Instead, you add them in in a targeted way once you see your network is overfitting. One quick thing, it's this
idea of transfer learning. We've kind of seen with regularization, we can help reduce the gap between train and test error by
adding these different regularization strategies. One problem with overfitting is sometimes you overfit 'cause you
don't have enough data. You want to use a big, powerful model, but that big, powerful
model just is going to overfit too much on your small dataset. Regularization is one way to combat that, but another way is through
using transfer learning. Transfer learning kind of busts this myth that you don't need a huge amount of data in order to train a CNN. The idea is really simple. You'll maybe first take some CNN. Here is kind of a VGG style architecture. You'll take your CNN, you'll train it in a very large dataset, like ImageNet, where you actually have enough data to train the whole network. Now the idea is that you want to apply the features from this dataset to some small dataset that you care about. Maybe instead of classifying the 1,000 ImageNet categories,
now you want to classify 10 dog breeds or something like that. You only have a small dataset. Here, our small dataset
only has C classes. Then what you'll typically
do is for this last fully connected layer that is going from the last layer features
to the final class scores, this now, you need to
reinitialize that matrix randomly. For ImageNet, it was a 4,096-by-1,000 dimensional matrix. Now for your new classes, it might be 4,096-by-C or by 10 or whatever. You reinitialize this
last matrix randomly, freeze the weights of
all the previous layers and now just basically
train a linear classifier, and only train the
parameters of this last layer and let it converge on your data. This tends to work pretty well if you only have a very small dataset to work with. Now if you have a little bit more data, another thing you can try is actually fine tuning the whole network. After that top layer converges and after you learn that last layer for your data, then you can consider
actually trying to update the whole network, as well. If you have more data,
then you might consider updating larger parts of the network. A general strategy here is that when you're updating the
network, you want to drop the learning rate from
its initial learning rate because probably the original parameters in this network that converged on ImageNet probably worked pretty well generally, and you just want to change
them a very small amount to tune performance for your dataset. Then when you're working
with transfer learning, you kind of imagine this two-by-two grid of scenarios where on
the one side, you have maybe very small amounts of data for your dataset, or very large amount of data for your dataset. Then maybe your data is
very similar to images. Like, ImageNet has a lot
of pictures of animals and plants and stuff like that. If you want to just classify other types of animals and plants
and other types of images like that, then you're
in pretty good shape. Then generally what you do is if your data is very similar to
something like ImageNet, if you have a very small amount of data, you can just basically
train a linear classifier on top of features, extracted using an ImageNet model. If you have a little bit
more data to work with, then you might imagine
fine tuning your data. However, you sometimes get in trouble if your data looks very
different from ImageNet. Maybe if you're working with maybe medical images that
are X-rays or CAT scans or something that looks very different from images in ImageNet, in that case, you maybe need to get a
little bit more creative. Sometimes it still works well here, but those last layer features might not be so informative. You might consider
reinitializing larger parts of the network and getting a little bit more creative and trying
more experiments here. This is somewhat mitigated if you have a large amount of data in
your very different dataset 'cause then you can actually fine tune larger parts of the network. Another point I'd like
to make is this idea of transfer learning is super pervasive. It's actually the norm,
rather than the exception. As you read computer vision papers, you'll often see system diagrams like this for different tasks. On the left, we're working
with object detection. On the right, we're working
with image captioning. Both of these models have a CNN that's kind of processing the image. In almost all applications
of computer vision these days, most people are not training these things from scratch. Almost always, that CNN will be pretrained on ImageNet, and then
potentially fine tuned for the task at hand. Also, in the captioning
sense, sometimes you can actually pretrain some word vectors relating to the language, as well. You maybe pretrain the CNN on ImageNet, pretrain some word vectors on a large text corpus, and then
fine tune the whole thing for your dataset. Although in the case
of captioning, I think this pretraining with word vectors tends to be a little bit less common and a little bit less critical. The takeaway for your projects, and more generally as you
work on different models, is that whenever you
have some large dataset, whenever you have some problem that you want to tackle, but you
don't have a large dataset, then what you should
generally do is download some pretrained model
that's relatively close to the task you care
about, and then either reinitialize parts of
that model or fine tune that model for your data. That tends to work pretty
well, even if you have only a modest amount of training data to work with. Because this is such a common strategy, all of the different deep learning software packages out there provide a model zoo where you can just download pretrained versions of various models. In summary today, we
talked about optimization, which is about how to
improve the training loss. We talked about regularization,
which is improving your performance on the test data. Model ensembling kind of fit into there. We also talked about transfer learning, which is how you can actually do better with less data. These are all super useful strategies. You should use them in
your projects and beyond. Next time, we'll talk
more concretely about some of the different deep learning software packages out there. 

- Hello? Okay, it's after 12, so
I want to get started. So today, lecture eight,
we're going to talk about deep learning software. This is a super exciting
topic because it changes a lot every year. But also means it's a lot
of work to give this lecture 'cause it changes a lot every year. But as usual, a couple
administrative notes before we dive into the material. So as a reminder the
project proposals for your course projects were due on Tuesday. So hopefully you all turned that in, and hopefully you all
have a somewhat good idea of what kind of projects
you want to work on for the class. So we're in the process of
assigning TA's to projects based on what the project area is and the expertise of the TA's. So we'll have some more
information about that in the next couple days I think. We're also in the process
of grading assignment one, so stay tuned and we'll get
those grades back to you as soon as we can. Another reminder is that
assignment two has been out for a while. That's going to be due next week,
a week from today, Thursday. And again, when working on assignment two, remember to stop your
Google Cloud instances when you're not working to
try to preserve your credits. And another bit of
confusion, I just wanted to re-emphasize is that for
assignment two you really only need to use GPU instances
for the last notebook. For all of the several
notebooks it's just in Python and Numpy so you don't need
any GPUs for those questions. So again, conserve your credits, only use GPUs when you need them. And the final reminder is
that the midterm is coming up. It's kind of hard to
believe we're there already, but the midterm will be in
class on Tuesday, five nine. So the midterm will be more theoretical. It'll be sort of pen and paper
working through different kinds of, slightly more
theoretical questions to check your understanding
of the material that we've covered so far. And I think we'll probably
post at least a short sort of sample of the types of
questions to expect. Question? [student's words obscured
due to lack of microphone] Oh yeah, question is
whether it's open-book, so we're going to say
closed note, closed book. So just, Yeah, yeah, so that's what
we've done in the past is just closed note,
closed book, relatively just like want to check
that you understand the intuition behind most of
the stuff we've presented. So, a quick recap as a reminder
of what we were talking about last time. Last time we talked about
fancier optimization algorithms for deep learning models
including SGD Momentum, Nesterov, RMSProp and Adam. And we saw that these
relatively small tweaks on top of vanilla SGD, are
relatively easy to implement but can make your networks
converge a bit faster. We also talked about regularization, especially dropout. So remember dropout, you're
kind of randomly setting parts of the network to zero
during the forward pass, and then you kind of
marginalize out over that noise in the back at test time. And we saw that this was
kind of a general pattern across many different
types of regularization in deep learning, where
you might add some kind of noise during training,
but then marginalize out that noise at test time
so it's not stochastic at test time. We also talked about
transfer learning where you can maybe download big
networks that were pre-trained on some dataset and then
fine tune them for your own problem. And this is one way that you
can attack a lot of problems in deep learning, even
if you don't have a huge dataset of your own. So today we're going to
shift gears a little bit and talk about some of the nuts and bolts about writing software and
how the hardware works. And a little bit, diving
into a lot of details about what the software
looks like that you actually use to train these things in practice. So we'll talk a little
bit about CPUs and GPUs and then we'll talk about
several of the major deep learning frameworks
that are out there in use these days. So first, we've sort of
mentioned this off hand a bunch of different times, that computers have CPUs,
computers have GPUs. Deep learning uses GPUs,
but we weren't really too explicit up to this
point about what exactly these things are and
why one might be better than another for different tasks. So, who's built a computer before? Just kind of show of hands. So, maybe about a third
of you, half of you, somewhere around that ballpark. So this is a shot of my computer at home that I built. And you can see that there's
a lot of stuff going on inside the computer,
maybe, hopefully you know what most of these parts are. And the CPU is the
Central Processing Unit. That's this little chip
hidden under this cooling fan right here near the top of the case. And the CPU is actually
relatively small piece. It's a relatively small
thing inside the case. It's not taking up a lot of space. And the GPUs are these
two big monster things that are taking up a
gigantic amount of space in the case. They have their own cooling, they're taking a lot of power. They're quite large. So, just in terms of how
much power they're using, in terms of how big they
are, the GPUs are kind of physically imposing and
taking up a lot of space in the case. So the question is what are these things and why are they so
important for deep learning? Well, the GPU is called a graphics card, or Graphics Processing Unit. And these were really developed,
originally for rendering computer graphics, and
especially around games and that sort of thing. So another show of hands,
who plays video games at home sometimes, from time to
time on their computer? Yeah, so again, maybe
about half, good fraction. So for those of you who've
played video games before and who've built your own computers, you probably have your own
opinions on this debate. [laughs] So this is one of those big
debates in computer science. You know, there's like Intel versus AMD, NVIDIA versus AMD for graphics cards. It's up there with Vim
versus Emacs for text editor. And pretty much any gamer
has their own opinions on which of these two sides they prefer for their own cards. And in deep learning we
kind of have mostly picked one side of this fight, and that's NVIDIA. So if you guys have AMD cards, you might be in a little
bit more trouble if you want to use those for deep learning. And really, NVIDIA's been
pushing a lot for deep learning in the last several years. It's been kind of a large focus
of some of their strategy. And they put in a lot
effort into engineering sort of good solutions
to make their hardware better suited for deep learning. So most people in deep learning
when we talk about GPUs, we're pretty much exclusively
talking about NVIDIA GPUs. Maybe in the future this'll
change a little bit, and there might be new players coming up, but at least for now
NVIDIA is pretty dominant. So to give you an idea of
like what is the difference between a CPU and a GPU,
I've kind of made a little spread sheet here. On the top we have two of
the kind of top end Intel consumer CPUs, and on
the bottom we have two of NVIDIA's sort of current
top end consumer GPUs. And there's a couple general
trends to notice here. Both GPUs and CPUs are
kind of a general purpose computing machine where
they can execute programs and do sort of arbitrary instructions, but they're qualitatively
pretty different. So CPUs tend to have just a few cores, for consumer desktop CPUs these days, they might have something like four or six or maybe up to 10 cores. With hyperthreading technology
that means they can run, the hardware can physically
run, like maybe eight or up to 20 threads concurrently. So the CPU can maybe do 20
things in parallel at once. So that's just not a gigantic number, but those threads for a
CPU are pretty powerful. They can actually do a lot of things, they're very fast. Every CPU instruction can
actually do quite a lot of stuff. And they can all work
pretty independently. For GPUs it's a little bit different. So for GPUs we see that
these sort of common top end consumer GPUs have thousands of cores. So the NVIDIA Titan XP
which is the current top of the line consumer
GPU has 3840 cores. So that's a crazy number. That's like way more than
the 10 cores that you'll get for a similarly priced CPU. The downside of a GPU is
that each of those cores, one, it runs at a much slower clock speed. And two they really
can't do quite as much. You can't really compare
CPU cores and GPU cores apples to apples. The GPU cores can't really
operate very independently. They all kind of need to work together and sort of paralyze one
task across many cores rather than each core
totally doing its own thing. So you can't really compare
these numbers directly. But it should give you the sense that due to the large number of
cores GPUs can sort of, are really good for
parallel things where you need to do a lot of things
all at the same time, but those things are all
pretty much the same flavor. Another thing to point
out between CPUs and GPUs is this idea of memory. Right, so CPUs have some cache on the CPU, but that's relatively
small and the majority of the memory for your
CPU is pulling from your system memory, the RAM,
which will maybe be like eight, 12, 16, 32 gigabytes
of RAM on a typical consumer desktop these days. Whereas GPUs actually
have their own RAM built into the chip. There's a pretty large
bottleneck communicating between the RAM in your
system and the GPU, so the GPUs typically have their own relatively large block of
memory within the card itself. And for the Titan XP, which
again is maybe the current top of the line consumer card, this thing has 12 gigabytes
of memory local to the GPU. GPUs also have their own caching system where there are sort of
multiple hierarchies of caching between the 12 gigabytes of GPU memory and the actual GPU cores. And that's somewhat similar
to the caching hierarchy that you might see in a CPU. So, CPUs are kind of good for
general purpose processing. They can do a lot of different things. And GPUs are maybe more
specialized for these highly paralyzable algorithms. So the prototypical algorithm
of something that works really really well and
is like perfectly suited to a GPU is matrix multiplication. So remember in matrix
multiplication on the left we've got like a matrix
composed of a bunch of rows. We multiply that on the right
by another matrix composed of a bunch of columns
and then this produces another, a final matrix
where each element in the output matrix is a dot product
between one of the rows and one of the columns of
the two input matrices. And these dot products
are all independent. Like you could imagine,
for this output matrix you could split it up completely and have each of those different elements of the output matrix all
being computed in parallel and they all sort of are
running the same computation which is taking a dot
product of these two vectors. But exactly where they're
reading that data from is from different places
in the two input matrices. So you could imagine that
for a GPU you can just like blast this out and
have all of this elements of the output matrix
all computed in parallel and that could make this thing
computer super super fast on GPU. So that's kind of the
prototypical type of problem that like where a GPU
is really well suited, where a CPU might have
to go in and step through sequentially and compute
each of these elements one by one. That picture is a little
bit of a caricature because CPUs these days have multiple cores, they can do vectorized
instructions as well, but still, for these like
massively parallel problems GPUs tend to have much better throughput. Especially when these matrices
get really really big. And by the way, convolution
is kind of the same kind of story. Where you know in convolution
we have this input tensor, we have this weight tensor
and then every point in the output tensor after a
convolution is again some inner product between some part of the weights and some part of the input. And you can imagine that a
GPU could really paralyze this computation, split it
all up across the many cores and compute it very quickly. So that's kind of the
general flavor of the types of problems where GPUs give
you a huge speed advantage over CPUs. So you can actually write
programs that run directly on GPUs. So NVIDIA has this CUDA
abstraction that lets you write code that kind of looks like C, but executes directly on the GPUs. But CUDA code is really really tricky. It's actually really tough
to write CUDA code that's performant and actually
squeezes all the juice out of these GPUs. You have to be very careful
managing the memory hierarchy and making sure you
don't have cache misses and branch mispredictions
and all that sort of stuff. So it's actually really really
hard to write performant CUDA code on your own. So as a result NVIDIA has
released a lot of libraries that implement common
computational primitives that are very very highly
optimized for GPUs. So for example NVIDIA has a
cuBLAS library that implements different kinds of matrix multiplications and different matrix operations
that are super optimized, run really well on GPU,
get very close to sort of theoretical peak hardware utilization. Similarly they have a cuDNN
library which implements things like convolution,
forward and backward passes, batch normalization, recurrent networks, all these kinds of
computational primitives that we need in deep learning. NVIDIA has gone in there and
released their own binaries that compute these
primitives very efficiently on NVIDIA hardware. So in practice, you tend not
to end up writing your own CUDA code for deep learning. You typically are just
mostly calling into existing code that other people have written. Much of which is the stuff
which has been heavily optimized by NVIDIA already. There's another sort of
language called OpenCL which is a bit more general. Runs on more than just NVIDIA GPUs, can run on AMD hardware, can run on CPUs, but OpenCL, nobody's really
spent a really large amount of effort and energy trying
to get optimized deep learning primitives for OpenCL, so
it tends to be a lot less performant the super
optimized versions in CUDA. So maybe in the future we
might see a bit of a more open standard and we might see
this across many different more types of platforms,
but at least for now, NVIDIA's kind of the main game
in town for deep learning. So you can check, there's a
lot of different resources for learning about how you can
do GPU programming yourself. It's kind of fun. It's sort of a different
paradigm of writing code because it's this massively
parallel architecture, but that's a bit beyond
the scope of this course. And again, you don't really
need to write your own CUDA code much in practice
for deep learning. And in fact, I've never
written my own CUDA code for any research project, so, but it is kind of useful
to know like how it works and what are the basic
ideas even if you're not writing it yourself. So if you want to look at
kind of CPU GPU performance in practice, I did some
benchmarks last summer comparing a decent Intel CPU against a bunch of different
GPUs that were sort of near top of the line at that time. And these were my own
benchmarks that you can find more details on GitHub,
but my findings were that for things like VGG 16 and
19, ResNets, various ResNets, then you typically see
something like a 65 to 75 times speed up when running the
exact same computation on a top of the line GPU, in
this case a Pascal Titan X, versus a top of the line,
well, not quite top of the line CPU, which in this case
was an Intel E5 processor. Although, I'd like to make
one sort of caveat here is that you always need
to be super careful whenever you're reading
any kind of benchmarks about deep learning, because
it's super easy to be unfair between different things. And you kind of need to know
a lot of the details about what exactly is being
benchmarked in order to know whether or not the comparison is fair. So in this case I'll come
right out and tell you that probably this comparison
is a little bit unfair to CPU because I didn't
spend a lot of effort trying to squeeze the maximal performance out of CPUs. I probably could have tuned
the blast libraries better for the CPU performance. And I probably could
have gotten these numbers a bit better. This was sort of out
of the box performance between just installing
Torch, running it on a CPU, just installing Torch running it on a GPU. So this is kind of out
of the box performance, but it's not really like
peak, possible, theoretical throughput on the CPU. But that being said, I
think there are still pretty substantial speed ups to be had here. Another kind of interesting
outcome from this benchmarking was comparing these
optimized cuDNN libraries from NVIDIA for convolution
and whatnot versus sort of more naive CUDA
that had been hand written out in the open source community. And you can see that if you
compare the same networks on the same hardware with
the same deep learning framework and the only
difference is swapping out these cuDNN versus sort of
hand written, less optimized CUDA you can see something
like nearly a three X speed up across the board when you
switch from the relatively simple CUDA to these like
super optimized cuDNN implementations. So in general, whenever
you're writing code on GPU, you should probably almost
always like just make sure you're using cuDNN because
you're leaving probably a three X performance boost
on the table if you're not calling into cuDNN for your stuff. So another problem that
comes up in practice, when you're training these things is that you know, your model is
maybe sitting on the GPU, the weights of the model
are in that 12 gigabytes of local storage on the
GPU, but your big dataset is sitting over on the
right on a hard drive or an SSD or something like that. So if you're not careful
you can actually bottleneck your training by just
trying to read the data off the disk. 'Cause the GPU is super
fast, it can compute forward and backward quite
fast, but if you're reading sequentially off a spinning
disk, you can actually bottleneck your training quite, and that can be really
bad and slow you down. So some solutions here
are that like you know if your dataset's really
small, sometimes you might just read the whole dataset into RAM. Or even if your dataset isn't so small, but you have a giant
server with a ton of RAM, you might do that anyway. You can also make sure
you're using an SSD instead of a hard drive, that can help
a lot with read throughput. Another common strategy
is to use multiple threads on the CPU that are
pre-fetching data off RAM or off disk, buffering it
in memory, in RAM so that then you can continue
feeding that buffer data down to the GPU with good performance. This is a little bit painful to set up, but again like, these
GPU's are so fast that if you're not really
careful with trying to feed them data as quickly as possible, just reading the data
can sometimes bottleneck the whole training process. So that's something to be aware of. So that's kind of the
brief introduction to like sort of GPU CPU hardware
in practice when it comes to deep learning. And then I wanted to
switch gears a little bit and talk about the
software side of things. The various deep learning
frameworks that people are using in practice. But I guess before I move on, is there any sort of
questions about CPU GPU? Yeah, question? [student's words obscured
due to lack of microphone] Yeah, so the question
is what can you sort of, what can you do mechanically
when you're coding to avoid these problems? Probably the biggest thing
you can do in software is set up sort of pre-fetching on the CPU. Like you couldn't like,
sort of a naive thing would be you have this
sequential process where you first read data off
disk, wait for the data, wait for the minibatch to be read, then feed the minibatch to the GPU, then go forward and backward on the GPU, then read another minibatch
and sort of do this all in sequence. And if you actually have multiple, like instead you might have
CPU threads running in the background that are
fetching data off the disk such that while the, you can sort of interleave
all of these things. Like the GPU is computing, the CPU background threads
are feeding data off disk and your main thread is kind
of waiting for these things to, just doing a bit of synchronization
between these things so they're all happening in parallel. And thankfully if you're using
some of these deep learning frameworks that we're about to talk about, then some of this work has
already been done for you 'cause it's a little bit painful. So the landscape of
deep learning frameworks is super fast moving. So last year when I gave
this lecture I talked mostly about Caffe, Torch, Theano and TensorFlow. And when I last gave this talk,
again more than a year ago, TensorFlow was relatively new. It had not seen super widespread
adoption yet at that time. But now I think in the
last year TensorFlow has gotten much more popular. It's probably the main framework
of choice for many people. So that's a big change. We've also seen a ton of new frameworks sort of popping up like
mushrooms in the last year. So in particular Caffe2 and
PyTorch are new frameworks from Facebook that I think
are pretty interesting. There's also a ton of other frameworks. Paddle, Baidu has Paddle,
Microsoft has CNTK, Amazon is mostly using
MXNet and there's a ton of other frameworks as well,
but I'm less familiar with, and really don't have time to get into. But one interesting thing to
point out from this picture is that kind of the first
generation of deep learning frameworks that really saw wide adoption were built in academia. So Caffe was from Berkeley,
Torch was developed originally NYU and also in
collaboration with Facebook. And Theana was mostly build
at the University of Montreal. But these kind of next
generation deep learning frameworks all originated in industry. So Caffe2 is from Facebook,
PyTorch is from Facebook. TensorFlow is from Google. So it's kind of an interesting
shift that we've seen in the landscape over
the last couple of years is that these ideas
have really moved a lot from academia into industry. And now industry is kind of
giving us these big powerful nice frameworks to work with. So today I wanted to
mostly talk about PyTorch and TensorFlow 'cause I
personally think that those are probably the ones you
should be focusing on for a lot of research type
problems these days. I'll also talk a bit
about Caffe and Caffe2. But probably a little bit
less emphasis on those. And before we move any farther,
I thought I should make my own biases a little bit more explicit. So I have mostly, I've
worked with Torch mostly for the last several years. And I've used it quite
a lot, I like it a lot. And then in the last year I've
mostly switched to PyTorch as my main research framework. So I have a little bit
less experience with some of these others, especially TensorFlow, but I'll still try to do
my best to give you a fair picture and a decent
overview of these things. So, remember that in the
last several lectures we've hammered this idea
of computational graphs in sort of over and over. That whenever you're doing deep learning, you want to think about building
some computational graph that computes whatever function
that you want to compute. So in the case of a linear
classifier you'll combine your data X and your weights
W with a matrix multiply. You'll do some kind of
hinge loss to maybe have, compute your loss. You'll have some regularization term and you imagine stitching
together all these different operations into some graph structure. Remember that these graph
structures can get pretty complex in the case of a big neural net, now there's many different layers, many different activations. Many different weights
spread all around in a pretty complex graph. And as you move to things
like neural turing machines then you can get these really
crazy computational graphs that you can't even really
draw because they're so big and messy. So the point of deep learning
frameworks is really, there's really kind of three
main reasons why you might want to use one of these
deep learning frameworks rather than just writing your own code. So the first would be that
these frameworks enable you to easily build and
work with these big hairy computational graphs
without kind of worrying about a lot of those
bookkeeping details yourself. Another major idea is that, whenever we're working in deep learning we always need to compute gradients. We're always computing some loss, we're always computer
gradient of our weight with respect to the loss. And we'd like to make this
automatically computing gradient, you don't want to have to
write that code yourself. You want that framework to
handle all these back propagation details for you so you
can just think about writing down the forward
pass of your network and have the backward pass
sort of come out for free without any additional work. And finally you want all
this stuff to run efficiently on GPUs so you don't have to
worry too much about these low level hardware details
about cuBLAS and cuDNN and CUDA and moving data
between the CPU and GPU memory. You kind of want all those messy
details to be taken care of for you. So those are kind of
some of the major reasons why you might choose to
use frameworks rather than writing your own stuff from scratch. So as kind of a concrete
example of a computational graph we can maybe write down
this super simple thing. Where we have three inputs, X, Y, and Z. We're going to combine
X and Y to produce A. Then we're going to combine
A and Z to produce B and then finally we're going
to do some maybe summing out operation on B to give
some scaler final result C. So you've probably written
enough Numpy code at this point to realize that it's
super easy to write down, to implement this computational graph, or rather to implement this
bit of computation in Numpy, right? You can just kind of write
down in Numpy that you want to generate some random data, you
want to multiply two things, you want to add two things, you
want to sum out a couple things. And it's really easy to do this in Numpy. But then the question is
like suppose that we want to compute the gradient of C
with respect to X, Y, and Z. So, if you're working in Numpy,
you kind of need to write out this backward pass yourself. And you've gotten a lot of
practice with this on the homeworks, but it can be kind of a pain and a little bit annoying
and messy once you get to really big complicated things. The other problem with
Numpy is that it doesn't run on the GPU. So Numpy is definitely CPU only. And you're never going
to be able to experience or take advantage of these
GPU accelerated speedups if you're stuck working in Numpy. And it's, again, it's a
pain to have to compute your own gradients in
all these situations. So, kind of the goal of most
deep learning frameworks these days is to let you
write code in the forward pass that looks very similar to Numpy, but lets you run it on the GPU and lets you automatically
compute gradients. And that's kind of the big
picture goal of most of these frameworks. So if you imagine looking
at, if we look at an example in TensorFlow of the exact
same computational graph, we now see that in this forward pass, you write this code that ends
up looking very very similar to the Numpy forward pass
where you're kind of doing these multiplication and
these addition operations. But now TensorFlow has
this magic line that just computes all the gradients for you. So now you don't have go in and
write your own backward pass and that's much more convenient. The other nice thing about
TensorFlow is you can really just, like with one line you
can switch all this computation between CPU and GPU. So here, if you just
add this with statement before you're doing this forward pass, you just can explicitly
tell the framework, hey I want to run this code on the CPU. But now if we just change that
with statement a little bit with just with a one
character change in this case, changing that C to a G,
now the code runs on GPU. And now in this little code snippet, we've solved these two problems. We're running our code on the GPU and we're having the framework
compute all the gradients for us, so that's really nice. And PyTorch kind looks
almost exactly the same. So again, in PyTorch
you kind of write down, you define some variables, you have some forward pass
and the forward pass again looks very similar to like,
in this case identical to the Numpy code. And then again, you can
just use PyTorch to compute gradients, all your
gradients with just one line. And now in PyTorch again,
it's really easy to switch to GPU, you just need to
cast all your stuff to the CUDA data type before
you rung your computation and now everything runs
transparently on the GPU for you. So if you kind of just look
at these three examples, these three snippets of code side by side, the Numpy, the TensorFlow and the PyTorch you see that the TensorFlow
and the PyTorch code in the forward pass looks
almost exactly like Numpy which is great 'cause
Numpy has a beautiful API, it's really easy to work with. But we can compute gradients automatically and we can run the GPU automatically. So after that kind of introduction, I wanted to dive in and
talk in a little bit more detail about kind of
what's going on inside this TensorFlow example. So as a running example throughout
the rest of the lecture, I'm going to use the training
a two-layer fully connected ReLU network on random data
as kind of a running example throughout the rest of the examples here. And we're going to train this
thing with an L2 Euclidean loss on random data. So this is kind of a silly
network, it's not really doing anything useful, but it does give you the, it's relatively small, self contained, the code fits on the slide
without being too small, and it lets you demonstrate
kind of a lot of the useful ideas inside these frameworks. So here on the right, oh,
and then another note, I'm kind of assuming
that Numpy and TensorFlow have already been imported
in all these code snippets. So in TensorFlow you would
typically divide your computation into two major stages. First, we're going to write
some code that defines our computational graph,
and that's this red code up in the top half. And then after you define your graph, you're going to run the
graph over and over again and actually feed data into the graph to perform whatever computation
you want it to perform. So this is the really,
this is kind of the big common pattern in TensorFlow. You'll first have a bunch of
code that builds the graph and then you'll go and
run the graph and reuse it many many times. So if you kind of dive
into the code of building the graph in this case. Up at the top you see that
we're defining this X, Y, w1 and w2, and we're creating
these tf.placeholder objects. So these are going to be
input nodes to the graph. These are going to be sort
of entry points to the graph where when we run the graph,
we're going to feed in data and put them in through
these input slots in our computational graph. So this is not actually
like allocating any memory right now. We're just sort of setting
up these input slots to the graph. Then we're going to use those
input slots which are now kind of like these symbolic variables and we're going to perform
different TensorFlow operations on these symbolic variables
in order to set up what computation we want
to run on those variables. So in this case we're doing
a matrix multiplication between X and w1, we're
doing some tf.maximum to do a ReLU nonlinearity and
then we're doing another matrix multiplication to
compute our output predictions. And then we're again using
a sort of basic Tensor operations to compute
our Euclidean distance, our L2 loss between our
prediction and the target Y. Another thing to point out here is that these lines of code are not
actually computing anything. There's no data in the system right now. We're just building up this
computational graph data structure telling
TensorFlow which operations we want to eventually run
once we put in real data. So this is just building the graph, this is not actually doing anything. Then we have this magical line
where after we've computed our loss with these symbolic operations, then we can just ask TensorFlow to compute the gradient of the loss
with respect to w1 and w2 in this one magical, beautiful line. And this avoids you writing
all your own backprop code that you had to do in the assignments. But again there's no actual
computation happening here. This is just sort of
adding extra operations to the computational graph
where now the computational graph has these additional
operations which will end up computing these gradients for you. So now at this point we've
computed our computational graph, we have this big graph
in this graph data structure in memory that knows what
operations we want to perform to compute the loss in gradients. And now we enter a TensorFlow
session to actually run this graph and feed it with data. So then, once we've entered the session, then we actually need to
construct some concrete values that will be fed to the graph. So TensorFlow just expects
to receive data from Numpy arrays in most cases. So here we're just creating
concrete actual values for X, Y, w1 and w2 using
Numpy and then storing these in some dictionary. And now here is where we're
actually running the graph. So you can see that we're
calling a session.run to actually execute
some part of the graph. The first argument loss, tells
us which part of the graph do we actually want as output. And that, so we actually want the graph, in this case we need to
tell it that we actually want to compute loss and grad1 and grad w2 and we need to pass in with
this feed dict parameter the actual concrete values
that will be fed to the graph. And then after, in this one line, it's going and running the
graph and then computing those values for loss grad1 to grad w2 and then returning the
actual concrete values for those in Numpy arrays again. So now after you unpack this
output in the second line, you get Numpy arrays, or you
get Numpy arrays with the loss and the gradients. So then you can go and
do whatever you want with these values. So then, this has only run sort
of one forward and backward pass through our graph, and it only takes a couple
extra lines if we actually want to train the network. So here we're, now we're
running the graph many times in a loop so we're doing a four loop and in each iteration of the loop, we're calling session.run
asking it to compute the loss and the gradients. And now we're doing a
manual gradient discent step using those computed gradients
to now update our current values of the weights. So if you actually run this
code and plot the losses, then you'll see that the loss goes down and the network is training and
this is working pretty well. So this is kind of like a
super bare bones example of training a fully connected
network in TensorFlow. But there's a problem here. So here, remember that
on the forward pass, every time we execute this graph, we're actually feeding in the weights. We have the weights as Numpy arrays and we're explicitly
feeding them into the graph. And now when the graph finishes executing it's going to give us these gradients. And remember the gradients
are the same size as the weights. So this means that every time
we're running the graph here, we're copying the weights
from Numpy arrays into TensorFlow then getting the gradients and then copying the
gradients from TensorFlow back out to Numpy arrays. So if you're just running on CPU, this is maybe not a huge deal, but remember we talked
about CPU GPU bottleneck and how it's very expensive
actually to copy data between CPU memory and GPU memory. So if your network is very
large and your weights and gradients were very big, then doing something like
this would be super expensive and super slow because we'd
be copying all kinds of data back and forth between the
CPU and the GPU at every time step. So that's bad, we don't want to do that. We need to fix that. So, obviously TensorFlow
has some solution to this. And the idea is that
now we want our weights, w1 and w2, rather than being
placeholders where we're going to, where we expect to
feed them in to the network on every forward pass, instead
we define them as variables. So a variable is something
is a value that lives inside the computational graph
and it's going to persist inside the computational
graph across different times when you run the same graph. So now instead of declaring
these w1 and w2 as placeholders, instead we just construct
them as variables. But now since they live inside the graph, we also need to tell
TensorFlow how they should be initialized, right? Because in the previous
case we were feeding in their values from outside the graph, so we initialized them in Numpy, but now because these things
live inside the graph, TensorFlow is responsible
for initializing them. So we need to pass in a
tf.randomnormal operation, which again is not
actually initializing them when we run this line, this
is just telling TensorFlow how we want them to be initialized. So it's a little bit of
confusing misdirection going on here. And now, remember in the previous example we were actually updating
the weights outside of the computational graph. We, in the previous example,
we were computing the gradients and then using them to update
the weights as Numpy arrays and then feeding in the
updated weights at the next time step. But now because we want
these weights to live inside the graph, this operation
of updating the weights needs to also be an operation inside the computational graph. So now we used this assign
function which mutates these variables inside
the computational graph and now the mutated value will
persist across multiple runs of the same graph. So now when we run this graph and when we train the network, now we need to run the graph
once with a little bit of special incantation to tell
TensorFlow to set up these variables that are going
to live inside the graph. And then once we've done
that initialization, now we can run the graph
over and over again. And here, we're now only
feeding in the data and labels X and Y and the weights are
living inside the graph. And here we've asked the network to, we've asked TensorFlow to
compute the loss for us. And then you might think that
this would train the network, but there's actually a bug here. So, if you actually run this code, and you plot the loss, it doesn't train. So that's bad, it's confusing,
like what's going on? We wrote this assign
code, we ran the thing, like we computed the
loss and the gradients and our loss is flat, what's going on? Any ideas? [student's words obscured
due to lack of microphone] Yeah so one hypothesis is
that maybe we're accidentally re-initializing the w's
every time we call the graph. That's a good hypothesis,
that's actually not the problem in this case. [student's words obscured
due to lack of microphone] Yeah, so the answer is that
we actually need to explicitly tell TensorFlow that we
want to run these new w1 and new w2 operations. So we've built up this big
computational graph data structure in memory and
now when we call run, we only told TensorFlow that
we wanted to compute loss. And if you look at the
dependencies among these different operations inside the graph, you see that in order to compute loss we don't actually need to
perform this update operation. So TensorFlow is smart and
it only computes the parts of the graph that are necessary
for computing the output that you asked it to compute. So that's kind of a nice thing
because it means it's only doing as much work as it needs to, but in situations like this it
can be a little bit confusing and lead to behavior
that you didn't expect. So the solution in this case
is that we actually need to explicitly tell TensorFlow
to perform those update operations. So one thing we could do,
which is what was suggested is we could add new w1
and new w2 as outputs and just tell TensorFlow
that we want to produce these values as outputs. But that's a problem
too because the values, those new w1, new w2 values
are again these big tensors. So now if we tell TensorFlow
we want those as output, we're going to again get
this copying behavior between CPU and GPU at ever iteration. So that's bad, we don't want that. So there's a little
trick you can do instead. Which is that we add kind of
a dummy node to the graph. With these fake data dependencies and we just say that
this dummy node updates, has these data dependencies
of new w1 and new w2. And now when we actually run the graph, we tell it to compute both
the loss and this dummy node. And this dummy node
doesn't actually return any value it just returns
none, but because of this dependency that we've put
into the node it ensures that when we run the updates value, we actually also run
these update operations. So, question? [student's words obscured
due to lack of microphone] Is there a reason why we didn't
put X and Y into the graph? And that it stayed as Numpy. So in this example we're
reusing X and Y on every, we're reusing the same X
and Y on every iteration. So you're right, we could
have just also stuck those in the graph, but in a
more realistic scenario, X and Y will be minibatches
of data so those will actually change at every iteration
and we will want to feed different values for
those at every iteration. So in this case, they could
have stayed in the graph, but in most cases they will change, so we don't want them
to live in the graph. Oh, another question? [student's words obscured
due to lack of microphone] Yeah, so we've told it,
we had put into TensorFlow that the outputs we want
are loss and updates. Updates is not actually a real value. So when updates evaluates
it just returns none. But because of this dependency
we've told it that updates depends on these assign operations. But these assign operations live inside the computational graph and
all live inside GPU memory. So then we're doing
these update operations entirely on the GPU and
we're no longer copying the updated values back out of the graph. [student's words obscured
due to lack of microphone] So the question is does
tf.group return none? So this gets into the
trickiness of TensorFlow. So tf.group returns some
crazy TensorFlow value. It sort of returns some like
internal TensorFlow node operation that we need to
continue building the graph. But when you execute the graph, and when you tell, inside the session.run, when we told it we want it
to compute the concrete value from updates, then that returns none. So whenever you're working with TensorFlow you have this funny indirection
between building the graph and the actual output values
during building the graph is some funny weird object,
and then you actually get a concrete value when you run the graph. So here after you run updates,
then the output is none. Does that clear it up a little bit? [student's words obscured
due to lack of microphone] So the question is why is loss a value and why is updates none? That's just the way that updates works. So loss is a value when we compute, when we tell TensorFlow
we want to run a tensor, then we get the concrete value. Updates is this kind of
special other data type that does not return a value,
it instead returns none. So it's kind of some TensorFlow
magic that's going on there. Maybe we can talk offline
if you're still confused. [student's words obscured
due to lack of microphone] Yeah, yeah, that behavior is
coming from the group method. So now, we kind of have
this weird pattern where we wanted to do these
different assign operations, we have to use this funny tf.group thing. That's kind of a pain, so
thankfully TensorFlow gives you some convenience
operations that kind of do that kind of stuff for you. And that's called an optimizer. So here we're using a
tf.train.GradientDescentOptimizer and we're telling it what
learning rate we want to use. And you can imagine that
there's, there's RMSprop, there's all kinds of different
optimization algorithms here. And now we call optimizer.minimize of loss and now this is a pretty magical, this is a pretty magical thing, because now this call is
aware that these variables w1 and w2 are marked as
trainable by default, so then internally, inside
this optimizer.minimize it's going in and adding
nodes to the graph which will compute gradient
of loss with respect to w1 and w2 and then it's
also performing that update operation for you and it's
doing the grouping operation for you and it's doing the assigns. It's like doing a lot of
magical stuff inside there. But then it ends up giving
you this magical updates value which, if you dig through the
code they're actually using tf.group so it looks very
similar internally to what we saw before. And now when we run the
graph inside our loop we do the same pattern of
telling it to compute loss and updates. And every time we tell the
graph to compute updates, then it'll actually go
and update the graph. Question? [student's words obscured
due to lack of microphone] Yeah, so what is the
tf.GlobalVariablesInitializer? So that's initializing w1
and w2 because these are variables which live inside the graph. So we need to, when we
saw this, when we create the tf.variable we have
this tf.randomnormal which is this initialization so the tf.GlobalVariablesInitializer
is causing the tf.randomnormal to actually run
and generate concrete values to initialize those variables. [student's words obscured
due to lack of microphone] Sorry, what was the question? [student's words obscured
due to lack of microphone] So it knows that a
placeholder is going to be fed outside of the graph and a
variable is something that lives inside the graph. So I don't know all the
details about how it decides, what exactly it decides
to run with that call. I think you'd need to dig
through the code to figure that out, or maybe it's
documented somewhere. So but now we've kind of got this, again we've got this full
example of training a network in TensorFlow
and we're kind of adding bells and whistles to make it
a little bit more convenient. So we can also here,
in the previous example we were computing the loss
explicitly using our own tensor operations, TensorFlow
you can always do that, you can use basic tensor
operations to compute just about anything you want. But TensorFlow also gives
you a bunch of convenience functions that compute these
common neural network things for you. So in this case we can use
tf.losses.mean_squared_error and it just does the L2
loss for us so we don't have to compute it ourself in terms
of basic tensor operations. So another kind of weirdness
here is that it was kind of annoying that we had to
explicitly define our inputs and define our weights and
then like chain them together in the forward pass
using a matrix multiply. And in this example we've
actually not put biases in the layer because that
would be kind of an extra, then we'd have to initialize biases, we'd have to get them in the right shape, we'd have to broadcast the
biases against the output of the matrix multiply
and you can see that that would kind of be a lot of code. It would be kind of annoying write. And once you get to like convolutions and batch normalizations
and other types of layers this kind of basic way of working, of having these variables,
having these inputs and outputs and combining them all together with basic computational graph operations
could be a little bit unwieldy and it could
be really annoying to make sure you initialize
the weights with the right shapes and all that sort of stuff. So as a result, there's a
bunch of sort of higher level libraries that wrap around TensorFlow and handle some of these details for you. So one example that ships with TensorFlow, is this tf.layers inside. So now in this code example
you can see that our code is only explicitly
declaring the X and the Y which are the placeholders
for the data and the labels. And now we say that H=tf.layers.dense, we give it the input X
and we tell it units=H. This is again kind of a magical line because inside this line,
it's kind of setting up w1 and b1, the bias, it's
setting up variables for those with the right shapes that
are kind of inside the graph but a little bit hidden from us. And it's using this
xavier initializer object to set up an initialization
strategy for those. So before we were doing
that explicitly ourselves with the tf.randomnormal business, but now here it's kind of
handling some of those details for us and it's just spitting out an H, which is again the same
sort of H that we saw in the previous layer, it's
just doing some of those details for us. And you can see here,
we're also passing an activation=tf.nn.relu so it's
even doing the activation, the relu activation function
inside this layer for us. So it's taking care of a
lot of these architectural details for us. Question? [student's words obscured
due to lack of microphone] Question is does the
xavier initializer default to particular distribution? I'm sure it has some default,
I'm not sure what it is. I think you'll have to
look at the documentation. But it seems to be a
reasonable strategy, I guess. And in fact if you run this code, it converges much faster
than the previous one because the initialization is better. And you can see that
we're using two calls to tf.layers and this lets us build our model without doing all these
explicit bookkeeping details ourself. So this is maybe a little
bit more convenient. But tf.contrib.layer is really
not the only game in town. There's like a lot of different
higher level libraries that people build on top of TensorFlow. And it's kind of due to this
basic impotence mis-match where the computational graph
is relatively low level thing, but when we're working
with neural networks we have this concept of layers and weights and some layers have weights
associated with them, and we typically think at
a slightly higher level of abstraction than this
raw computational graph. So that's what these various
packages are trying to help you out and let you
work at this higher layer of abstraction. So another very popular
package that you may have seen before is Keras. Keras is a very beautiful,
nice API that sits on top of TensorFlow and handles
sort of building up these computational graph for
you up in the back end. By the way, Keras also
supports Theano as a back end, so that's also kind of nice. And in this example you
can see we build the model as a sequence of layers. We build some optimizer object and we call model.compile
and this does a lot of magic in the back end to build the graph. And now we can call model.fit
and that does the whole training procedure for us magically. So I don't know all the
details of how this works, but I know Keras is very popular, so you might consider using
it if you're talking about TensorFlow. Question? [student's words obscured
due to lack of microphone] Yeah, so the question is
like why there's no explicit CPU, GPU going on here. So I've kind of left that
out to keep the code clean. But you saw at the beginning examples it was pretty easy to
flop all these things between CPU and GPU and there
was either some global flag or some different data type or some with statement and
it's usually relatively simple and just about one line
to swap in each case. But exactly what that line looks like differs a bit depending on the situation. So there's actually like
this whole large set of higher level TensorFlow
wrappers that you might see out there in the wild. And it seems that like
even people within Google can't really agree on which
one is the right one to use. So Keras and TFLearn are
third party libraries that are out there on the
internet by other people. But there's these three different ones, tf.layers, TF-Slim and tf.contrib.learn that all ship with TensorFlow,
that are all kind of doing a slightly different version of this higher level wrapper thing. There's another framework
also from Google, but not shipping with
TensorFlow called Pretty Tensor that does the same sort of thing. And I guess none of these
were good enough for DeepMind, because they went ahead a couple weeks ago and wrote and released
their very own high level TensorFlow wrapper called Sonnet. So I wouldn't begrudge you
if you were kind of confused by all these things. There's a lot of different choices. They don't always play
nicely with each other. But you have a lot of
options, so that's good. TensorFlow has pretrained models. There's some examples in
TF-Slim, and in Keras. 'Cause remember retrained
models are super important when you're training your own things. There's also this idea of Tensorboard where you can load up your, I don't want to get into details, but Tensorboard you can
add sort of instrumentation to your code and then
plot losses and things as you go through the training process. TensorFlow also let's you run distributed where you can break up
a computational graph run on different machines. That's super cool but I
think probably not anyone outside of Google is really
using that to great success these days, but if you do
want to run distributed stuff probably TensorFlow is the
main game in town for that. A side note is that a lot
of the design of TensorFlow is kind of spiritually inspired
by this earlier framework called Theano from Montreal. I don't want to go
through the details here, just if you go through
these slides on your own, you can see that the code
for Theano ends up looking very similar to TensorFlow. Where we define some variables, we do some forward pass,
we compute some gradients, and we compile some function,
then we run the function over and over to train the network. So it kind of looks a lot like TensorFlow. So we still have a lot to get through, so I'm going to move on to PyTorch and maybe take questions at the end. So, PyTorch from Facebook
is kind of different from TensorFlow in that we have
sort of three explicit different layers of
abstraction inside PyTorch. So PyTorch has this tensor
object which is just like a Numpy array. It's just an imperative array,
it doesn't know anything about deep learning,
but it can run with GPU. We have this variable
object which is a node in a computational graph which
builds up computational graphs, lets you compute gradients,
that sort of thing. And we have a module object
which is a neural network layer that you can compose
together these modules to build big networks. So if you kind of want to
think about rough equivalents between PyTorch and TensorFlow
you can think of the PyTorch tensor as fulfilling the same role as the Numpy array in TensorFlow. The PyTorch variable is similar
to the TensorFlow tensor or variable or placeholder,
which are all sort of nodes in a computational graph. And now the PyTorch module
is kind of equivalent to these higher level things
from tf.slim or tf.layers or sonnet or these other
higher level frameworks. So right away one thing
to notice about PyTorch is that because it ships with
this high level abstraction and like one really nice
higher level abstraction called modules on its own,
there's sort of less choice involved. Just stick with nnmodules
and you'll be good to go. You don't need to worry about
which higher level wrapper to use. So PyTorch tensors, as I said,
are just like Numpy arrays so here on the right we've done
an entire two layer network using entirely PyTorch tensors. One thing to note is that
we're not importing Numpy here at all anymore. We're just doing all these
operations using PyTorch tensors. And this code looks exactly
like the two layer net code that you wrote in Numpy
on the first homework. So you set up some random
data, you use some operations to compute the forward pass. And then we're explicitly
viewing the backward pass ourself. Just sort of backhopping
through the network, through the operations, just
as you did on homework one. And now we're doing a
manual update of the weights using a learning rate and
using our computed gradients. But the major difference
between the PyTorch tensor and Numpy arrays is that they run on GPU so all you have to do
to make this code run on GPU is use a different data type. Rather than using torch.FloatTensor, you do torch.cuda.FloatTensor,
cast all of your tensors to this new datatype and
everything runs magically on the GPU. You should think of PyTorch
tensors as just Numpy plus GPU. That's exactly what it
is, nothing specific to deep learning. So the next layer of abstraction
in PyTorch is the variable. So this is, once we moved
from tensors to variables now we're building computational graphs and we're able to take
gradients automatically and everything like that. So here, if X is a variable,
then x.data is a tensor and x.grad is another variable
containing the gradients of the loss with respect to that tensor. So x.grad.data is an
actual tensor containing those gradients. And PyTorch tensors and variables
have the exact same API. So any code that worked on
PyTorch tensors you can just make them variables instead
and run the same code, except now you're building
up a computational graph rather than just doing
these imperative operations. So here when we create these variables each call to the variable
constructor wraps a PyTorch tensor and then also gives
a flag whether or not we want to compute gradients
with respect to this variable. And now in the forward
pass it looks exactly like it did before in the variable
in the case with tensors because they have the same API. So now we're computing our predictions, we're computing our loss
in kind of this imperative kind of way. And then we call loss.backwards
and now all these gradients come out for us. And then we can make
a gradient update step on our weights using the
gradients that are now present in the w1.grad.data. So this ends up looking
quite like the Numpy case, except all the gradients come for free. One thing to note that's
kind of different between PyTorch and TensorFlow is
that in a TensorFlow case we were building up this explicit graph, then running the graph many times. Here in PyTorch, instead
we're building up a new graph every time we do a forward pass. And this makes the code
look a bit cleaner. And it has some other
implications that we'll get to in a bit. So in PyTorch you can define
your own new autograd functions by defining the forward and
backward in terms of tensors. This ends up looking kind
of like the module layers code that you write for homework two. Where you can implement
forward and backward using tensor operations and then
stick these things inside computational graph. So here we're defining our own relu and then we can actually
go in and use our own relu operation and now stick it
inside our computational graph and define our own operations this way. But most of the time you
will probably not need to define your own autograd operations. Most of the times the
operations you need will mostly be already implemented for you. So in TensorFlow we saw, if we can move to something
like Keras or TF.Learn and this gives us a higher
level API to work with, rather than this raw computational graphs. The equivalent in PyTorch
is the nn package. Where it provides these high
level wrappers for working with these things. But unlike TensorFlow
there's only one of them. And it works pretty well,
so just use that if you're using PyTorch. So here, this ends up
kind of looking like Keras where we define our model
as some sequence of layers. Our linear and relu operations. And we use some loss function
defined in the nn package that's our mean squared error loss. And now inside each iteration of our loop we can run data forward
through the model to get our predictions. We can run the predictions
forward through the loss function to get our scale or loss, then we can call loss.backward,
get all our gradients for free and then loop over
the parameters of the models and do our explicit gradient
descent step to update the models. And again we see that we're
sort of building up this new computational graph every
time we do a forward pass. And just like we saw in TensorFlow, PyTorch provides these
optimizer operations that kind of abstract
away this updating logic and implement fancier
update rules like Adam and whatnot. So here we're constructing
an optimizer object telling it that we want
it to optimize over the parameters of the model. Giving it some learning rate
under the hyper parameters. And now after we compute our gradients we can just call
optimizer.step and it updates all the parameters of the
model for us right here. So another common thing
you'll do in PyTorch a lot is define your own nn modules. So typically you'll write your own class which defines you entire model as a single new nn module class. And a module is just kind
of a neural network layer that can contain either
other other modules or trainable weights or
other other kinds of state. So in this case we can redo
the two layer net example by defining our own nn module class. So now here in the
initializer of the class we're assigning this linear1 and linear2. We're constructing
these new module objects and then store them
inside of our own class. And now in the forward pass
we can use both our own internal modules as well as
arbitrary autograd operations on variables to compute
the output of our network. So here we receive the, inside
this forward method here, the input acts as a variable, then we pass the variable
to our self.linear1 for the first layer. We use an autograd op
clamp to complete the relu, we pass the output of
that to the second linear and then that gives us our output. And now the rest of this
code for training this thing looks pretty much the same. Where we build an optimizer and loop over and on ever iteration
feed data to the model, compute the gradients with loss.backwards, call optimizer.step. So this is like relatively characteristic of what you might see
in a lot of PyTorch type training scenarios. Where you define your own class, defining your own model
that contains other modules and whatnot and then you
have some explicit training loop like this that
runs it and updates it. One kind of nice quality
of life thing that you have in PyTorch is a dataloader. So a dataloader can handle
building minibatches for you. It can handle some of the
multi-threading that we talked about for you, where it can
actually use multiple threads in the background to
build many batches for you and stream off disk. So here a dataloader wraps
a dataset and provides some of these abstractions for you. And in practice when you
want to run your own data, you typically will write
your own dataset class which knows how to read
your particular type of data off whatever source you
want and then wrap it in a data loader and train with that. So, here we can see that
now we're iterating over the dataloader object
and at every iteration this is yielding minibatches of data. And it's internally handling
the shuffling of the data and multithreaded dataloading
and all this sort of stuff for you. So this is kind of a
completely PyTorch example and a lot of PyTorch
training code ends up looking something like this. PyTorch provides pretrained models. And this is probably the
slickest pretrained model experience I've ever seen. You just say torchvision.models.alexnet
pretained=true. That'll go down in the background,
download the pretrained weights for you if you
don't already have them, and then it's right
there, you're good to go. So this is super easy to use. PyTorch also has, there's
also a package called Visdom that lets you visualize some
of these loss statistics somewhat similar to Tensorboard. So that's kind of nice,
I haven't actually gotten a chance to play around with
this myself so I can't really speak to how useful it is, but one of the major
differences between Tensorboard and Visdom is that Tensorboard
actually lets you visualize the structure of the computational graph. Which is really cool, a really
useful debugging strategy. And Visdom does not have
that functionality yet. But I've never really used
this myself so I can't really speak to its utility. As a bit of an aside, PyTorch
is kind of an evolution of, kind of a newer updated
version of an older framework called Torch which I worked
with a lot in the last couple of years. And I don't want to go
through the details here, but PyTorch is pretty much
better in a lot of ways than the old Lua Torch, but
they actually share a lot of the same back end C code
for computing with tensors and GPU operations on tensors and whatnot. So if you look through this Torch example, some of it ends up looking
kind of similar to PyTorch, some of it's a bit different. Maybe you can step through this offline. But kind of the high
level differences between Torch and PyTorch are that
Torch is actually in Lua, not Python, unlike these other things. So learning Lua is a bit of
a turn off for some people. Torch doesn't have autograd. Torch is also older, so it's more stable, less susceptible to bugs,
there's maybe more example code for Torch. They're about the same speeds,
that's not really a concern. But in PyTorch it's in
Python which is great, you've got autograd which
makes it a lot simpler to write complex models. In Lua Torch you end up
writing a lot of your own back prop code sometimes, so
that's a little bit annoying. But PyTorch is newer,
there's less existing code, it's still subject to change. So it's a little bit more of an adventure. But at least for me, I kind of prefer, I don't really see much reason for myself to use Torch over PyTorch
anymore at this time. So I'm pretty much using
PyTorch exclusively for all my work these days. We talked about this a
little bit about this idea of static versus dynamic graphs. And this is one of the main
distinguishing features between PyTorch and TensorFlow. So we saw in TensorFlow
you have these two stages of operation where first you build up this computational graph, then you
run the computational graph over and over again many
many times reusing that same graph. That's called a static
computational graph 'cause there's only one of them. And we saw PyTorch is quite
different where we're actually building up this new computational graph, this new fresh thing
on every forward pass. That's called a dynamic
computational graph. For kind of simple cases,
with kind of feed forward neural networks, it doesn't
really make a huge difference, the code ends up kind of similarly and they work kind of similarly, but I do want to talk a bit
about some of the implications of static versus dynamic. And what are the tradeoffs of those two. So one kind of nice
idea with static graphs is that because we're
kind of building up one computational graph once, and
then reusing it many times, the framework might have
the opportunity to go in and do optimizations on that graph. And kind of fuse some operations,
reorder some operations, figure out the most
efficient way to operate that graph so it can be really efficient. And because we're going
to reuse that graph many times, maybe that
optimization process is expensive up front, but we can amortize that
cost with the speedups that we've gotten when we run
the graph many many times. So as kind of a concrete example, maybe if you write some
graph which has convolution and relu operations kind
of one after another, you might imagine that
some fancy graph optimizer could go in and actually
output, like emit custom code which has fused operations,
fusing the convolution and the relu so now it's
computing the same thing as the code you wrote, but
now might be able to be executed more efficiently. So I'm not too sure on exactly
what the state in practice of TensorFlow graph
optimization is right now, but at least in principle,
this is one place where static graph really, you
can have the potential for doing this optimization in static graphs where maybe it would be not so
tractable for dynamic graphs. Another kind of subtle point
about static versus dynamic is this idea of serialization. So with a static graph you
can imagine that you write this code that builds up the graph and then once you've built the graph, you have this data structure
in memory that represents the entire structure of your network. And now you could take that data structure and just serialize it to disk. And now you've got the whole
structure of your network saved in some file. And then you could later
rear load that thing and then run that computational
graph without access to the original code that built it. So this would be kind of nice
in a deployment scenario. You might imagine that you
might want to train your network in Python because it's
maybe easier to work with, but then after you serialize that network and then you could deploy
it now in maybe a C++ environment where you don't
need to use the original code that built the graph. So that's kind of a nice
advantage of static graphs. Whereas with a dynamic graph,
because we're interleaving these processes of graph
building and graph execution, you kind of need the
original code at all times if you want to reuse
that model in the future. On the other hand, some
advantages for dynamic graphs are that it kind of makes,
it just makes your code a lot cleaner and a lot
easier in a lot of scenarios. So for example, suppose
that we want to do some conditional operation where
depending on the value of some variable Z, we want
to do different operations to compute Y. Where if Z is positive, we
want to use one weight matrix, if Z is negative we want to
use a different weight matrix. And we just want to switch off
between these two alternatives. In PyTorch because we're
using dynamic graphs, it's super simple. Your code kind of looks
exactly like you would expect, exactly what you would do in Numpy. You can just use normal
Python control flow to handle this thing. And now because we're building
up the graph each time, each time we perform this
operation will take one of the two paths and build
up maybe a different graph on each forward pass, but
for any graph that we do end up building up, we can
back propagate through it just fine. And the code is very
clean, easy to work with. Now in TensorFlow the
situations is a little bit more complicated because we
build the graph once, this control flow operator
kind of needs to be an explicit operator in
the TensorFlow graph. And now, so them you can
see that we have this tf.cond call which is kind
of like a TensorFlow version of an if statement,
but now it's baked into the computational graph
rather than using sort of Python control flow. And the problem is that
because we only build the graph once, all the potential
paths of control flow that our program might flow
through need to be baked into the graph at the time we
construct it before we ever run it. So that means that any kind
of control flow operators that you want to have need
to be not Python control flow operators, you need to
use some kind of magic, special tensor flow
operations to do control flow. In this case this tf.cond. Another kind of similar
situation happens if you want to have loops. So suppose that we want to
compute some kind of recurrent relationships where maybe Y
T is equal to Y T minus one plus X T times some weight
matrix W and depending on each time we do this,
every time we compute this, we might have a different
sized sequence of data. And no matter the length
of our sequence of data, we just want to compute this
same recurrence relation no matter the size of the input sequence. So in PyTorch this is super easy. We can just kind of use a
normal for loop in Python to just loop over the number
of times that we want to unroll and now depending on
the size of the input data, our computational graph will
end up as different sizes, but that's fine, we can
just back propagate through each one, one at a time. Now in TensorFlow this
becomes a little bit uglier. And again, because we need
to construct the graph all at once up front, this
control flow looping construct again needs to be an explicit
node in the TensorFlow graph. So I hope you remember
your functional programming because you'll have to use
those kinds of operators to implement looping
constructs in TensorFlow. So in this case, for this
particular recurrence relationship you can use a foldl operation and pass in, sort of implement this particular
loop in terms of a foldl. But what this basically means
is that you have this sense that TensorFlow is almost
building its own entire programming language,
using the language of computational graphs. And any kind of control flow operator, or any kind of data
structure needs to be rolled into the computational graph
so you can't really utilize all your favorite paradigms
for working imperatively in Python. You kind of need to relearn
a whole separate set of control flow operators. And if you want to do
any kinds of control flow inside your computational
graph using TensorFlow. So at least for me, I find
that kind of confusing, a little bit hard to wrap
my head around sometimes, and I kind of like that
using PyTorch dynamic graphs, you can just use your favorite
imperative programming constructs and it all works just fine. By the way, there actually
is some very new library called TensorFlow Fold which
is another one of these layers on top of TensorFlow
that lets you implement dynamic graphs, you kind
of write your own code using TensorFlow Fold that
looks kind of like a dynamic graph operation and then
TensorFlow Fold does some magic for you and somehow implements
that in terms of the static TensorFlow graphs. This is a super new paper
that's being presented at ICLR this week in France. So I haven't had the chance
to like dive in and play with this yet. But my initial impression
was that it does add some amount of dynamic graphs to
TensorFlow but it is still a bit more awkward to work
with than the sort of native dynamic graphs you have in PyTorch. So then, I thought it
might be nice to motivate like why would we care about
dynamic graphs in general? So one option is recurrent networks. So you can see that for
something like image captioning we use a recurrent network
which operates over sequences of different lengths. In this case, the sentence
that we want to generate as a caption is a sequence
and that sequence can vary depending on our input data. So now you can see that we
have this dynamism in the thing where depending on the
size of the sentence, our computational graph
might need to have more or fewer elements. So that's one kind of common
application of dynamic graphs. For those of you who
took CS224N last quarter, you saw this idea of recursive networks where sometimes in natural
language processing you might, for example,
compute a parsed tree of a sentence and then
you want to have a neural network kind of operate
recursively up this parse tree. So having a neural network
that kind of works, it's not just a sequential
sequence of layers, but instead it's kind of
working over some graph or tree structure instead
where now each data point might have a different
graph or tree structure so the structure of
the computational graph then kind of mirrors the
structure of the input data. And it could vary from
data point to data point. So this type of thing seems
kind of complicated and hairy to implement using TensorFlow, but in PyTorch you can just kind of use like normal Python control
flow and it'll work out just fine. Another bit of more researchy
application is this really cool idea that I like
called neuromodule networks for visual question answering. So here the idea is that we
want to ask some questions about images where we
maybe input this image of cats and dogs, there's some question, what color is the cat, and
then internally the system can read the question and
that has these different specialized neural network
modules for performing operations like asking for
colors and finding cats. And then depending on
the text of the question, it can compile this custom
architecture for answering the question. And now if we asked a different question, like are there more cats than dogs? Now we have maybe the
same basic set of modules for doing things like finding
cats and dogs and counting, but they're arranged in a different order. So we get this dynamism again
where different data points might give rise to different
computational graphs. But this is a bit more
of a researchy thing and maybe not so main stream right now. But as kind of a bigger
point, I think that there's a lot of cool, creative
applications that people could do with dynamic computational graphs and maybe there aren't so many right now, just because it's been so
painful to work with them. So I think that there's
a lot of opportunity for doing cool, creative things with dynamic computational graphs. And maybe if you come up with cool ideas, we'll feature it in lecture next year. So I wanted to talk
very briefly about Caffe which is this framework from Berkeley. Which Caffe is somewhat
different from the other deep learning frameworks
where you in many cases you can actually train
networks without writing any code yourself. You kind of just call into
these pre-existing binaries, set up some configuration
files and in many cases you can train on data without
writing any of your own code. So, you may be first,
you convert your data into some format like HDF5
or LMDB and there exists some scripts inside Caffe
that can just convert like folders of images and text files
into these formats for you. You need to define, now
instead of writing code to define the structure of
your computational graph, instead you edit some text
file called a prototxt which sets up the structure
of the computational graph. Here the structure is that
we read from some input HDF5 file, we perform some inner product, we compute some loss
and the whole structure of the graph is set up in this text file. One kind of downside
here is that these files can get really ugly for
very large networks. So for something like the
152 layer ResNet model, which by the way was
trained in Caffe originally, then this prototxt file ends
up almost 7000 lines long. So people are not writing these by hand. People will sometimes will
like write python scripts to generate these prototxt files. [laughter] Then you're kind in the
realm of rolling your own computational graph abstraction. That's probably not a good
idea, but I've seen that before. Then, rather than having
some optimizer object, instead there's some solver,
you define some solver things inside another prototxt. This defines your learning rate, your optimization algorithm and whatnot. And then once you do all these things, you can just run the Caffe
binary with the train command and it all happens magically. Cafee has a model zoo with a
bunch of pretrained models, that's pretty useful. Caffe has a Python
interface but it's not super well documented. You kind of need to read the
source code of the python interface to see what it can do, so that's kind of annoying. But it does work. So, kind of my general thing
about Caffe is that it's maybe good for feed forward models, it's maybe good for production scenarios, because it doesn't depend on Python. But probably for research
these days, I've seen Caffe being used maybe a little bit less. Although I think it is
still pretty commonly used in industry again for production. I promise one slide, one
or two slides on Caffe 2. So Caffe 2 is the successor to
Caffe which is from Facebook. It's super new, it was
only released a week ago. [laughter] So I really haven't had
the time to form a super educated opinion about Caffe 2 yet, but it uses static graphs
kind of similar to TensorFlow. Kind of like Caffe one
the core is written in C++ and they have some Python interface. The difference is that
now you no longer need to write your own Python scripts
to generate prototxt files. You can kind of define your
computational graph structure all in Python, kind of
looking with an API that looks kind of like TensorFlow. But then you can spit out,
you can serialize this computational graph
structure to a prototxt file. And then once your model
is trained and whatnot, then we get this benefit that
we talked about of static graphs where you can, you
don't need the original training code now in order
to deploy a trained model. So one interesting thing
is that you've seen Google maybe has one major
deep running framework, which is TensorFlow, where
Facebook has these two, PyTorch and Caffe 2. So these are kind of
different philosophies. Google's kind of trying to
build one framework to rule them all that maybe works
for every possible scenario for deep learning. This is kind of nice because
it consolidates all efforts onto one framework. It means you only need to learn one thing and it'll work across
many different scenarios including like distributed
systems, production, deployment, mobile, research, everything. Only need to learn one framework
to do all these things. Whereas Facebook is taking a
bit of a different approach. Where PyTorch is really more specialized, more geared towards research
so in terms of writing research code and quickly
iterating on your ideas, that's super easy in
PyTorch, but for things like running in production,
running on mobile devices, PyTorch doesn't have a
lot of great support. Instead, Caffe 2 is kind
of geared toward those more production oriented use cases. So my kind of general study,
my general, overall advice about like which framework
to use for which problems is kind of that both, I think TensorFlow is a
pretty safe bet for just about any project that you
want to start new, right? Because it is sort of one
framework to rule them all, it can be used for just
about any circumstance. However, you probably
need to pair it with a higher level wrapper and
if you want dynamic graphs, you're maybe out of luck. Some of the code ends up
looking a little bit uglier in my opinion, but maybe that's
kind of a cosmetic detail and it doesn't really matter that much. I personally think PyTorch
is really great for research. If you're focused on just
writing research code, I think PyTorch is a great choice. But it's a bit newer, has
less community support, less code out there, so it
could be a bit of an adventure. If you want more of a well
trodden path, TensorFlow might be a better choice. If you're interested in
production deployment, you should probably look at
Caffe, Caffe 2 or TensorFlow. And if you're really focused
on mobile deployment, I think TensorFlow and Caffe
2 both have some built in support for that. So it's kind of unfortunately,
there's not just like one global best framework,
it kind of depends on what you're actually trying to do, what applications you anticipate
but theses are kind of my general advice on those things. So next time we'll talk
about some case studies about various CNN architectures. 

- All right welcome to lecture nine. So today we will be talking
about CNN Architectures. And just a few administrative points before we get started,
assignment two is due Thursday. The mid term will be in
class on Tuesday May ninth, so next week and it will
cover material through Tuesday through this coming Thursday May fourth. So everything up to
recurrent neural networks are going to be fair game. The poster session
we've decided on a time, it's going to be Tuesday June sixth from twelve to three p.m. So this is the last week of classes. So we have our our poster
session a little bit early during the last week so that after that, once you guys get feedback
you still have some time to work for your final report
which will be due finals week. Okay, so just a quick review of last time. Last time we talked
about different kinds of deep learning frameworks. We talked about you know
PyTorch, TensorFlow, Caffe2 and we saw that using
these kinds of frameworks we were able to easily build
big computational graphs, for example very large neural
networks and comm nets, and be able to really
easily compute gradients in these graphs. So to compute all of the
gradients for all the intermediate variables weights inputs and
use that to train our models and to run all this efficiently on GPUs And we saw that for a
lot of these frameworks the way this works is by
working with these modularized layers that you guys have
been working writing with, in your home works as well
where we have a forward pass, we have a backward pass, and then in our final model architecture, all we need to do then is to just define all of these sequence of layers together. So using that we're able
to very easily be able to build up very complex
network architectures. So today we're going to talk
about some specific kinds of CNN Architectures that are
used today in cutting edge applications and research. And so we'll go into depth
in some of the most commonly used architectures for
these that are winners of ImageNet classification benchmarks. So in chronological
order AlexNet, VGG net, GoogLeNet, and ResNet. And so these will go into a lot of depth. And then I'll also after
that, briefly go through some other architectures that are not as prominently used these
days, but are interesting either from a historical perspective, or as recent areas of research. Okay, so just a quick review. We talked a long time ago about LeNet, which was one of the first
instantiations of a comNet that was successfully used in practice. And so this was the comNet
that took an input image, used com filters five by five filters applied at stride one and
had a couple of conv layers, a few pooling layers and then
some fully connected layers at the end. And this fairly simple comNet
was very successfully applied to digit recognition. So AlexNet from 2012 which
you guys have also heard already before in previous classes, was the first large scale
convolutional neural network that was able to do well on
the ImageNet classification task so in 2012 AlexNet was
entered in the competition, and was able to outperform
all previous non deep learning based models
by a significant margin, and so this was the comNet
that started the spree of comNet research and usage afterwards. And so the basic comNet
AlexNet architecture is a conv layer followed by pooling layer, normalization, com pool norm, and then a few more conv
layers, a pooling layer, and then several fully
connected layers afterwards. So this actually looks very
similar to the LeNet network that we just saw. There's just more layers in total. There is five of these conv layers, and two fully connected layers before the final fully connected
layer going to the output classes. So let's first get a sense
of the sizes involved in the AlexNet. So if we look at the input to the AlexNet this was trained on ImageNet, with inputs at a size 227 by 227 by 3 images. And if we look at this first
layer which is a conv layer for the AlexNet, it's 11 by 11 filters, 96 of these applied at stride 4. So let's just think
about this for a moment. What's the output volume
size of this first layer? And there's a hint. So remember we have our input size, we have our convolutional filters, ray. And we have this formula,
which is the hint over here that gives you the size
of the output dimensions after applying com right? So remember it was the full
image, minus the filter size, divided by the stride, plus one. So given that that's
written up here for you 55, does anyone have a guess at
what's the final output size after this conv layer? [student speaks off mic] - So I had 55 by 55 by 96, yep. That's correct. Right so our spatial
dimensions at the output are going to be 55 in each
dimension and then we have 96 total filters so the
depth after our conv layer is going to be 96. So that's the output volume. And what's the total number
of parameters in this layer? So remember we have 96 11 by 11 filters. [student speaks off mic] - [Lecturer] 96 by 11 by 11, almost. So yes, so I had another by three, yes that's correct. So each of the filters is going to see through a local region
of 11 by 11 by three, right because the input depth was three. And so, that's each filter
size, times we have 96 of these total. And so there's 35K parameters
in this first layer. Okay, so now if we look
at the second layer this is a pooling layer
right and in this case we have three three by three
filters applied at stride two. So what's the output volume
of this layer after pooling? And again we have a hint, very
similar to the last question. Okay, 27 by 27 by 96. Yes that's correct. Right so the pooling layer
is basically going to use this formula that we had here. Again because these are pooling
applied at a stride of two so we're going to use the
same formula to determine the spatial dimensions and
so the spatial dimensions are going to be 27 by
27, and pooling preserves the depth. So we had 96 as depth as input, and it's still going to be 96 depth at output. And next question. What's the number of
parameters in this layer? I hear some muttering. [student answers off mic] - Nothing. Okay. Yes, so pooling layer
has no parameters, so, kind of a trick question. Okay, so we can basically, yes, question? [student speaks off mic] - The question is, why are
there no parameters in the pooling layer? The parameters are the weights right, that we're trying to learn. And so convolutional layers
have weights that we learn but pooling all we do is have a rule, we look at the pooling region, and we take the max. So there's no parameters that are learned. So we can keep on doing
this and you can just repeat the process and it's kind of
a good exercise to go through this and figure out the
sizes, the parameters, at every layer. And so if you do this all the way, you can look at this is
the final architecture that you can work with. There's 11 by 11 filters at the beginning, then five by five and some
three by three filters. And so these are generally
pretty familiar looking sizes that you've seen before
and then at the end we have a couple of fully connected layers of size 4096 and finally the last layer, is FC8 going to the soft max, which is going to the
1000 ImageNet classes. And just a couple of details about this, it was the first use of
the ReLu non-linearity that we've talked about
that's the most commonly used non-linearity. They used local response
normalization layers basically trying to
normalize the response across neighboring channels but this
is something that's not really used anymore. It turned out not to, other people showed that it didn't have so much of an effect. There's a lot of heavy data augmentation, and so you can look in the
paper for more details, but things like flipping,
jittering, cropping, color normalization all of these things which you'll probably
find useful for you when you're working on your
projects for example, so a lot of data augmentation here. They also use dropout batch size of 128, and learned with SGD with
momentum which we talked about in an earlier lecture,
and basically just started with a base learning
rate of 1e negative 2. Every time it plateaus,
reduce by a factor of 10 and then just keep going. Until they finish training and a little bit of weight
decay and in the end, in order to get the best
numbers they also did an ensembling of models and
so training multiple of these, averaging them together and
this also gives an improvement in performance. And so one other thing I want to point out is that if you look at this
AlexNet diagram up here, it looks kind of like the
normal comNet diagrams that we've been seeing,
except for one difference, which is that it's, you
can see it's kind of split in these two different rows
or columns going across. And so the reason for this
is mostly historical note, so AlexNet was trained
on GTX580 GPUs older GPUs that only had three gigs of memory. So it couldn't actually fit
this entire network on here, and so what they ended up doing, was they spread the
network across two GPUs. So on each GPU you would
have half of the neurons, or half of the feature maps. And so for example if you
look at this first conv layer, we have 55 by 55 by 96 output, but if you look at this diagram carefully, you can zoom in later in the actual paper, you can see that, it's actually only 48 depth-wise, on each GPU, and so they just spread
it, the feature maps, directly in half. And so what happens is that
for most of these layers, for example com one, two, four and five, the connections are only with feature maps on the same GPU, so you
would take as input, half of the feature maps
that were on the the same GPU as before and you don't
look at the full 96 feature maps for example. You just take as input the
48 in that first layer. And then there's a few
layers so com three, as well as FC six, seven and eight, where here are the GPUs
do talk to each other and so there's connections
with all feature maps in the preceding layer. so there's communication across the GPUs, and each of these neurons
are then connected to the full depth of the
previous input layer. Question. - [Student] It says the
full simplified AlexNetwork architecture. [mumbles] - Oh okay, so the question
is why does it say full simplified AlexNet architecture here? It just says that because I
didn't put all the details on here, so for example this
is the full set of layers in the architecture, and
the strides and so on, but for example the normalization
layer, there's other, these details are not written on here. And then just one little note, if you look at the paper
and try and write out the math and architectures and so on, there's a little bit of
an issue on the very first layer they'll say if
you'll look in the figure they'll say 224 by 224 , but there's actually some
kind of funny pattern going on and so the
numbers actually work out if you look at it as 227. AlexNet was the winner of
the ImageNet classification benchmark in 2012, you can see that it cut the error rate
by quite a large margin. It was the first CNN base
winner, and it was widely used as a base to our architecture almost ubiquitously from then
until a couple years ago. It's still used quite a bit. It's used in transfer learning
for lots of different tasks and so it was used for
basically a long time, and it was very famous and
now though there's been some more recent architectures
that have generally just had better performance
and so we'll talk about these next and these are going to be
the more common architectures that you'll be wanting to use in practice. So just quickly first in
2013 the ImageNet challenge was won by something called a ZFNet. Yes, question. [student speaks off mic] - So the question is intuition why AlexNet was so much better than
the ones that came before, DefLearning comNets [mumbles] this is just a very different kind of
approach in architecture. So this was the first deep
learning based approach first comNet that was used. So in 2013 the challenge
was won by something called a ZFNet [Zeller Fergus Net]
named after the creators. And so this mostly was
improving hyper parameters over the AlexNet. It had the same number of layers, the same general structure
and they made a few changes things like
changing the stride size, different numbers of filters
and after playing around with these hyper parameters more, they were able to improve the error rate. But it's still basically the same idea. So in 2014 there are a
couple of architectures that were now more significantly different and made another jump in performance, and the main difference with
these networks first of all was much deeper networks. So from the eight layer
network that was in 2012 and 2013, now in 2014 we
had two very close winners that were around 19 layers and 22 layers. So significantly deeper. And the winner of this
was GoogleNet, from Google but very close behind was
something called VGGNet from Oxford, and on actually
the localization challenge VGG got first place in
some of the other tracks. So these were both very,
very strong networks. So let's first look at VGG
in a little bit more detail. And so the VGG network is the
idea of much deeper networks and with much smaller filters. So they increased the number of layers from eight layers in AlexNet
right to now they had models with 16 to 19 layers in VGGNet. And one key thing that they
did was they kept very small filter so only three by
three conv all the way, which is basically the
smallest com filter size that is looking at a little
bit of the neighboring pixels. And they just kept this
very simple structure of three by three convs
with the periodic pooling all the way through the network. And it's very simple elegant
network architecture, was able to get 7.3% top five error on the ImageNet challenge. So first the question of
why use smaller filters. So when we take these
small filters now we have fewer parameters and we
try and stack more of them instead of having larger filters, have smaller filters
with more depth instead, have more of these filters instead, what happens is that you end
up having the same effective receptive field as if you
only have one seven by seven convolutional layer. So here's a question, what is
the effective receptive field of three of these three
by three conv layers with stride one? So if you were to stack three
three by three conv layers with Stride one what's the
effective receptive field, the total area of the input,
spatial area of the input that enure at the top
layer of the three layers is looking at. So I heard fifteen pixels,
why fifteen pixels? - [Student] Okay, so the
reason given was because they overlap-- - Okay, so the reason given
was because they overlap. So it's on the right track. What actually is happening
though is you have to see, at the first layer, the
receptive field is going to be three by three right? And then at the second layer, each of these neurons in the second layer is going to look at three
by three other first layer filters, but the corners
of these three by three have an additional pixel on each side, that is looking at in
the original input layer. So the second layer is actually
looking at five by five receptive field and then
if you do this again, the third layer is
looking at three by three in the second layer but this is going to, if you just draw out this
pyramid is looking at seven by seven in the input layer. So the effective receptive field here is going to be seven by seven. Which is the same as one
seven by seven conv layer. So what happens is that
this has the same effective receptive field as a
seven by seven conv layer but it's deeper. It's able to have more
non-linearities in there, and it's also fewer parameters. So if you look at the
total number of parameters, each of these conv filters
for the three by threes is going to have nine parameters
in each conv [mumbles] three times three, and
then times the input depth, so three times three times
C, times this total number of output feature maps, which is again C is we're going to preserve the total number of channels. So you get three times three, times C times C for each of these layers, and we have three layers
so it's going to be three times this number, compared to if you had a
single seven by seven layer then you get, by the same reasoning, seven squared times C squared. So you're going to have
fewer parameters total, which is nice. So now if we look at
this full network here there's a lot of numbers up
here that you can go back and look at more carefully
but if we look at all of the sizes and number
of parameters the same way that we calculated the
example for AlexNet, this is a good exercise to go through, we can see that you
know going the same way we have a couple of these conv
layers and a pooling layer a couple more conv layers,
pooling layer, several more conv layers and so on. And so this just keeps going up. And if you counted the total
number of convolutional and fully connected layers,
we're going to have 16 in this case for VGG 16, and then VGG 19, it's just a very similar architecture, but with a few
more conv layers in there. And so the total memory
usage of this network, so just making a forward
pass through counting up all of these numbers so
in the memory numbers here written in terms of the total numbers, like we calculated earlier, and if you look at four bytes per number, this is going to be
about 100 megs per image, and so this is the scale
of the memory usage that's happening and this is
only for a forward pass right, when you do a backward pass
you're going to have to store more and so this is
pretty heavy memory wise. 100 megs per image, if
you have on five gigs of total memory, then
you're only going to be able to store about 50 of these. And so also the total number
of parameters here we have is 138 million parameters in this network, and this compares with
60 million for AlexNet. Question? [student speaks off mic] - So the question is what
do we mean by deeper, is it the number of
filters, number of layers? So deeper in this case is
always referring to layers. So there are two usages of the word depth which is confusing one is
the depth rate per channel, width by height by depth, you can use the word depth here, but in general we talk about
the depth of a network, this is going to be the
total number of layers in the network, and usually in particular we're counting the total
number of weight layers. So the total number of
layers with trainable weight, so convolutional layers
and fully connected layers. [student mumbles off mic] - Okay, so the question
is, within each layer what do different filters need? And so we talked about this
back in the comNet lecture, so you can also go back and refer to that, but each filter is a set of
let's say three by three convs, so each filter is looking at a, is a set of weight looking at
a three by three value input input depth, and this
produces one feature map, one activation map of
all the responses of the different spatial locations. And then we have we can have
as many filters as we want right so for example 96 and each of these is going to produce a feature map. And so it's just like
each filter corresponds to a different pattern
that we're looking for in the input that we
convolve around and we see the responses everywhere in the input, we create a map of these
and then another filter will we convolve over the
image and create another map. Question. [student speaks off mic] - So question is, is
there intuition behind, as you go deeper into the network
we have more channel depth so more number of filters
right and so you can have any design that you want so
you don't have to do this. In practice you will see this
happen a lot of the times and one of the reasons is
people try and maintain kind of a relatively
constant level of compute, so as you go higher up or
deeper into your network, you're usually also using
basically down sampling and having smaller total
spatial area and then so then they also increase now you
increase by depth a little bit, it's not as expensive
now to increase by depth because it's spatially smaller and so, yeah that's just a reason. Question. [student speaks off mic] - So performance-wise is
there any reason to use SBN [mumbles] instead
of SouthMax [mumbles], so no, for a classifier
you can use either one, and you did that earlier
in the class as well, but in general SouthMax losses, have generally worked
well and been standard use for classification here. Okay yeah one more question. [student mumbles off mic] - Yes, so the question
is, we don't have to store all of the memory like we
can throw away the parts that we don't need and so on? And yes this is true. Some of this you don't need to keep, but you're also going to
be doing a backwards pass through ware for the most part, when you were doing the chain rule and so on you needed
a lot of these activations as part of it and so in
large part a lot of this does need to be kept. So if we look at the distribution
of where memory is used and where parameters are,
you can see that a lot of memories in these early
layers right where you still have spatial dimensions you're
going to have more memory usage and then a lot of the
parameters are actually in the last layers, the
fully connected layers have a huge number of parameters right, because we have all of
these dense connections. And so that's something
just to know and then keep in mind so later on we'll
see some networks actually get rid of these fully
connected layers and be able to save a lot on the number of parameters. And then just one last thing to point out, you'll also see different ways of calling all of these layers right. So here I've written out
exactly what the layers are. conv3-64 means three by three convs with 64 total filters. But for VGGNet on this
diagram on the right here there's also common ways
that people will look at each group of filters, so each orange block here, as in conv1 part one, so conv1-1, conv1-2, and so on. So just something to keep in mind. So VGGNet ended up getting
second place in the ImageNet 2014 classification challenge, first in localization. They followed a very
similar training procedure as Alex Krizhevsky for the AlexNet. They didn't use local
response normalization, so as I mentioned earlier, they found out this
didn't really help them, and so they took it out. You'll see VGG 16 and VGG
19 are common variants of the cycle here, and this is just the number of layers, 19
is slightly deeper than 16. In practice VGG 19 works
very little bit better, and there's a little
bit more memory usage, so you can use either but
16 is very commonly used. For best results, like
AlexNet, they did ensembling in order to average several models, and you get better results. And they also showed in their work that the FC7 features of the last
fully connected layer before going to the 1000 ImageNet classes. The 4096 size layer just before that, is a good feature representation, that can even just be used as is, to extract these features from other data, and generalized these other tasks as well. And so FC7 is a good
feature representation. Yeah question. [student speaks off mic] - Sorry what was the question? Okay, so the question is
what is localization here? And so this is a task,
and we'll talk about it a little bit more in a later lecture on detection and localization
so I don't want to go into detail here but
it's basically an image, not just classifying What's
the class of the image, but also drawing a bounding
box around where that object is in the image. And the difference with detection, which is a very related
task is that detection there can be multiple instances
of this object in the image localization we're
assuming there's just one, this classification but we just how this additional bounding box. So we looked at VGG which
was one of the deep networks from 2014 and then now
we'll talk about GoogleNet which was the other one that won the classification challenge. So GoogleNet again was
a much deeper network with 22 layers but one
of the main insights and special things about
GoogleNet is that it really looked at this problem of
computational efficiency and it tried to design a
network architecture that was very efficient in the amount of compute. And so they did this using
this inception module which we'll go into more
detail and basically stacking a lot of these inception
modules on top of each other. There's also no fully connected
layers in this network, so they got rid of that
were able to save a lot of parameters and so in total
there's only five million parameters which is twelve
times less than AlexNet, which had 60 million even
though it's much deeper now. It got 6.7% top five error. So what's the inception module? So the idea behind the inception module is that they wanted to design
a good local network typology and it has this idea
of this local topology that's you know you can
think of it as a network within a network and
then stack a lot of these local typologies one on top of each other. And so in this local
network that they're calling an inception module what they're
doing is they're basically applying several different
kinds of filter operations in parallel on top of the
same input coming into this same layer. So we have our input coming
in from the previous layer and then we're going to do
different kinds of convolutions. So a one by one conv, right
a three by three conv, five by five conv, and then they also have a pooling operation
in this case three by three pooling, and so you get
all of these different outputs from these different layers, and then what they do is
they concatenate all these filter outputs together depth wise, and so then this creates one
tenser output at the end that is going tom pass
on to the next layer. So if we look at just a
naive way of doing this we just do exactly that we
have all of these different operations we get the outputs
we concatenate them together. So what's the problem with this? And it turns out that
computational complexity is going to be a problem here. So if we look more
carefully at an example, so here just for as an example
I've put one by one conv, 128 filter so three by
three conv 192 filters, five by five convs and 96 filters. Assume everything has basically the stride that's going to maintain
the spatial dimensions, and that we have this input coming in. So what is the output size
of the one by one filter with 128 , one by one
conv with 128 filters? Who has a guess? OK so I heard 28 by 28,
by 128 which is correct. So right by one by one conv
we're going to maintain spatial dimensions and
then on top of that, each conv filter is going to look through the entire 256 depth of the input, but then the output is going to be, we have a 28 by 28 feature map for each of the 128 filters that we have in this conv layer. So we get 28 by 28 by 128. OK and then now if we do the same thing and we look at the filter
sizes of the output sizes sorry of all of the different
filters here, after the three by three conv we're
going to have this volume of 28 by 28 by 192 right
after five by five conv we have 96 filters here. So 28 by 28 by 96, and then out pooling layer is just going to keep the same spatial
dimension here, so pooling layer will preserve it in depth, and here because of our stride, we're also going to preserve
our spatial dimensions. And so now if we look at
the output size after filter concatenation what we're
going to get is 28 by 28, these are all 28 by 28, and
we concatenating depth wise. So we get 28 by 28 times
all of these added together, and the total output size is going to be 28 by 28 by 672. So the input to our
inception module was 28 by 28 by 256, then the output
from this module is 28 by 28 by 672. So we kept the same spatial dimensions, and we blew up the depth. Question. [student speaks off mic] OK So in this case, yeah, the question is, how are we getting 28
by 28 for everything? So here we're doing all the zero padding in order to maintain
the spatial dimensions, and that way we can do this filter concatenation depth-wise. Question in the back. [student speaks off mic] - OK The question is what's
the 256 deep at the input, and so this is not the
input to the network, this is the input just
to this local module that I'm looking at. So in this case 256 is
the depth of the previous inception module that
came just before this. And so now coming out
we have 28 by 28 by 672, and that's going to be
the input to the next inception module. Question. [student speaks off mic] - Okay the question is, how
did we get 28 by 28 by 128 for the first one, the first conv, and this is basically it's a
one by one convolution right, so we're going to take
this one by one convolution slide it across our 28 by
28 by 256 input spatially where it's at each location,
it's going to multiply, it's going to do a [mumbles] through the entire 256
depth, and so we do this one by one conv slide it over spatially and we get a feature map
out that's 28 by 28 by one. There's one number at each
spatial location coming out, and each filter produces
one of these 28 by 28 by one maps, and we have
here a total 128 filters, and that's going to
produce 28 by 28, by 128. OK so if you look at
the number of operations that are happening in
the convolutional layer, let's look at the first one for
example this one by one conv as I was just saying at each
each location we're doing a one by one by 256 dot product. So there's 256 multiply
operations happening here and then for each filter
map we have 28 by 28 spatial locations, so
that's the first 28 times 28 first two numbers that
are multiplied here. These are the spatial
locations for each filter map, and so we have to do this
to 25 60 multiplication each one of these then
we have 128 total filters at this layer, or we're
producing 128 total feature maps. And so the total number
of these operations here is going to be 28 times 28 times 128 times 256. And so this is going to be the same for, you can think about this
for the three by three conv, and the five by five conv,
that's exactly the same principle. And in total we're going to
get 854 million operations that are happening here. - [Student] And the 128,
192, and 96 are just values [mumbles] - Question the 128, 192 and
256 are values that I picked. Yes, these are not values
that I just came up with. They are similar to the
ones that you will see in like a particular
layer of inception net, so in GoogleNet basically,
each module has a different set of these kinds of
parameters, and I picked one that was similar to one of these. And so this is very expensive
computationally right, these these operations. And then the other thing
that I also want to note is that the pooling layer also
adds to this problem because it preserves the whole feature depth. So at every layer your total
depth can only grow right, you're going to take
the full featured depth from your pooling layer, as
well as all the additional feature maps from the conv
layers and add these up together. So here our input was 256
depth and our output is 672 depth and you're just
going to keep increasing this as you go up. So how do we deal with this
and how do we keep this more manageable? And so one of the key
insights that GoogleNet used was that well we can we
can address this by using bottleneck layers and try and
project these feature maps to lower dimension before our
our convolutional operations, so before our expensive layers. And so what exactly does that mean? So reminder one by one
convolution, I guess we were just going through
this but it's taking your input volume, it's performing a
dot product at each spatial location and what it does is
it preserves spatial dimension but it reduces the depth and
it reduces that by projecting your input depth to a lower dimension. It just takes it's basically
like a linear combination of your input feature maps. And so this main idea is
that it's projecting your depth down and so the inception module takes these one by one convs
and adds these at a bunch of places in these modules
where there's going to be, in order to alleviate
this expensive compute. So before the three by three
and five by five conv layers, it puts in one of these
one by one convolutions. And then after the
pooling layer it also puts an additional one by one convolution. Right so these are the one
by one bottleneck layers that are added in. And so how does this change the math that we were looking at earlier? So now basically what's
happening is that we still have the same input here 28 by 28 by 256, but these one by one convs
are going to reduce the depth dimension and so you can see
before the three by three convs, if I put a one by
one conv with 64 filters, my output from that is going to be, 28 by 28 by 64. So instead of now going into
the three by three convs afterwards instead of 28
by 28 by 256 coming in, we only have a 28 by 28,
by 64 block coming in. And so this is now
reducing the smaller input going into these conv
layers, the same thing for the five by five conv, and
then for the pooling layer, after the pooling comes
out, we're going to reduce the depth after this. And so, if you work out
the math the same way for all of the convolutional ops here, adding in now all these one by one convs on top of the three by
threes and five by fives, the total number of operations
is 358 million operations, so it's much less than the
854 million that we had in the naive version, and
so you can see how you can use this one by one
conv, and the filter size for that to control your computation. Yes, question in the back. [student speaks off mic] - Yes, so the question
is, have you looked into what information might be
lost by doing this one by one conv at the beginning. And so there might be
some information loss, but at the same time if
you're doing these projections you're taking a linear
combination of these input feature maps which has redundancy in them, you're taking combinations of them, and you're also introducing
an additional non-linearity after the one by one
conv, so it also actually helps in that way with
adding a little bit more depth and so, I don't think
there's a rigorous analysis of this, but basically in
general this works better and there's reasons why it helps as well. OK so here we have, we're
basically using these one by one convs to help manage our
computational complexity, and then what GooleNet
does is it takes these inception modules and it's going to stack all these together. So this is a full inception architecture. And if we look at this a
little bit more detail, so here I've flipped it, because it's so big, it's not going to fit vertically any more on the slide. So what we start with is
we first have this stem network, so this is more
the kind of vanilla plain conv net that we've seen earlier [mumbles] six sequence of layers. So conv pool a couple
of convs in another pool just to get started and then after that we have all of our different
our multiple inception modules all stacked on top of each other, and then on top we have
our classifier output. And notice here that
they've really removed the expensive fully connected layers it turns out that the model
works great without them, even and you reduce a lot of parameters. And then what they also have here is, you can see these couple
of extra stems coming out and these are auxiliary
classification outputs and so these are also you know
just a little mini networks with an average pooling,
a one by one conv, a couple of fully connected
layers here going to the soft Max and also a 1000 way SoftMax with the ImageNet classes. And so you're actually
using your ImageNet training classification loss in
three separate places here. The standard end of the
network, as well as in these two places earlier on in
the network, and the reason they do that is just
this is a deep network and they found that having
these additional auxiliary classification outputs,
you get more gradient training injected at the earlier layers, and so more just helpful signal flowing in because these intermediate
layers should also be helpful. You should be able to do classification based off some of these as well. And so this is the full architecture, there's 22 total layers
with weights and so within each of these modules
each of those one by one, three by three, five by
five is a weight layer, just including all of
these parallel layers, and in general it's a relatively
more carefully designed architecture and part of this
is based on some of these intuitions that we're talking
about and part of them also is just you know
Google the authors they had huge clusters and they're
cross validating across all kinds of design
choices and this is what ended up working well. Question? [student speaks off mic] - Yeah so the question is,
are the auxiliary outputs actually useful for the
final classification, to use these as well? I think when they're training them they do average all these
for the losses coming out. I think they are helpful. I can't remember if in
the final architecture, whether they average all
of these or just take one, it seems very possible that
they would use all of them, but you'll need to check on that. [student speaks off mic] - So the question is for
the bottleneck layers, is it possible to use some
other types of dimensionality reduction and yes you can use
other kinds of dimensionality reduction. The benefits here of
this one by one conv is, you're getting this effect,
but it's all, you know it's a conv layer just like any other. You have the soul network of these, you just train it this full network back [mumbles] through everything, and it's learning how to combine the previous feature maps. Okay yeah, question in the back. [student speaks off mic] - Yes so, question is
are any weights shared or all they all separate and yeah, all of these layers have separate weights. Question. [student speaks off mic] - Yes so the question is why do we have to inject gradients at earlier layers? So our classification
output at the very end, where we get a gradient on this, it's passed all the way back
through the chain roll but the problem is when
you have very deep networks and you're going all the
way back through these, some of this gradient
signal can become minimized and lost closer to the beginning,
and so that's why having these additional ones in earlier parts can help provide some additional signal. [student mumbles off mic] - So the question is are you
doing back prop all the times for each output. No it's just one back
prop all the way through, and you can think of these three, you can think of there being kind of like an addition at the end
of these if you were to draw up your computational
graph, and so you get your final signal and you can
just take all of these gradients and just back plot
them all the way through. So it's as if they were
added together at the end in a computational graph. OK so in the interest of
time because we still have a lot to get through, can
take other questions offline. Okay so GoogleNet basically 22 layers. It has an efficient inception module, there's no fully connected layers. 12 times fewer parameters than AlexNet, and it's the ILSVRC 2014
classification winner. And so now let's look at the 2015 winner, which is the ResNet network and so here this idea is really, this
revolution of depth net right. We were starting to increase
depth in 2014, and here we've just had this hugely
deeper model at 152 layers was the ResNet architecture. And so now let's look at that
in a little bit more detail. So the ResNet architecture,
is getting extremely deep networks, much deeper
than any other networks before and it's doing this using this idea of residual connections
which we'll talk about. And so, they had 152
layer model for ImageNet. They were able to get 3.5
of 7% top 5 error with this and the really special
thing is that they swept all classification and
detection contests in the ImageNet mart benchmark
and this other benchmark called COCO. It just basically won everything. So it was just clearly
better than everything else. And so now let's go into a
little bit of the motivation behind ResNet and residual connections that we'll talk about. And the question that they
started off by trying to answer is what happens when we try
and stack deeper and deeper layers on a plain
convolutional neural network? So if we take something like VGG or some normal network that's
just stacks of conv and pool layers on top of each
other can we just continuously extend these, get deeper
layers and just do better? And and the answer is no. So if you so if you look at what happens when you get deeper, so here
I'm comparing a 20 layer network and a 56 layer network
and so this is just a plain kind of network you'll see
that in the test error here on the right the 56 layer
network is doing worse than the 28 layer network. So the deeper network was
not able to do better. But then the really weird thing is now if you look at the training error right we here have again the 20 layer network and a 56 layer network. The 56 layer network, one of
the obvious problems you think, I have a really deep network,
I have tons of parameters maybe it's probably starting
to over fit at some point. But what actually happens is
that when you're over fitting you would expect to have very good, very low training error rate,
and just bad test error, but what's happening here is
that in the training error the 56 layer network is
also doing worse than the 20 layer network. And so even though the
deeper model performs worse, this is not caused by over-fitting. And so the hypothesis
of the ResNet creators is that the problem is actually
an optimization problem. Deeper models are just harder to optimize, than more shallow networks. And the reasoning was that well, a deeper model should be
able to perform at least as well as a shallower model. You can have actually a
solution by construction where you just take the learned layers from your shallower model, you just copy these over and then
for the remaining additional deeper layers you just
add identity mappings. So by construction this
should be working just as well as the shallower layer. And your model that weren't
able to learn properly, it should be able to learn at least this. And so motivated by
this their solution was well how can we make it
easier for our architecture, our model to learn these
kinds of solutions, or at least something like this? And so their idea is well
instead of just stacking all these layers on top
of each other and having every layer try and learn
some underlying mapping of a desired function, lets
instead have these blocks, where we try and fit a residual mapping, instead of a direct mapping. And so what this looks
like is here on this right where the input to these block
is just the input coming in and here we are going to
use our, here on the side, we're going to use our
layers to try and fit some residual of our desire to H of X, minus X instead of the desired
function H of X directly. And so basically at the
end of this block we take the step connection on
this right here, this loop, where we just take our input,
we just use pass it through as an identity, and so if
we had no weight layers in between it was just
going to be the identity it would be the same thing
as the output, but now we use our additional weight
layers to learn some delta, for some residual from our X. And so now the output
of this is going to be just our original R X plus some residual that we're going to call it. It's basically a delta
and so the idea is that now the output it should
be easy for example, in the case where identity is ideal, to just squash all of
these weights of F of X from our weight layers
just set it to all zero for example, then we're
just going to get identity as the output, and we can get something, for example, close to this
solution by construction that we had earlier. Right, so this is just
a network architecture that says okay, let's try and fit this, learn how our weight layers
residual, and be something close, that way it'll more
likely be something close to X, it's just modifying X,
than to learn exactly this full mapping of what it should be. Okay, any questions about this? [student speaks off mic] - Question is is there the same dimension? So yes these two paths
are the same dimension. In general either it's the same dimension, or what they actually
do is they have these projections and shortcuts
and they have different ways of padding to make things work
out to be the same dimension. Depth wise. Yes - [Student] When you use the word residual you were talking about [mumbles off mic] - So the question is what
exactly do we mean by residual this output
of this transformation is a residual? So we can think of our output
here right as this F of X plus X, where F of X is the
output of our transformation and then X is our input,
just passed through by the identity. So we'd like to using a plain layer, what we're trying to do is learn something like H of X, but what we saw
earlier is that it's hard to learn H of X. It's a good H of X as we
get very deep networks. And so here the idea is
let's try and break it down instead of as H of X is
equal to F of X plus, and let's just try and learn F of X. And so instead of learning
directly this H of X we just want to learn what
is it that we need to add or subtract to our input as
we move on to the next layer. So you can think of it as
kind of modifying this input, in place in a sense. We have-- [interrupted by student mumbling off mic] - The question is, when we're
saying the word residual are we talking about F of X? Yeah. So F of X is what we're
calling the residual. And it just has that meaning. Yes another question. [student mumbles off mic] - So the question is in
practice do we just sum F of X and X together, or
do we learn some weighted combination and you just do a direct sum. Because when you do a direct sum, this is the idea of let
me just learn what is it I have to add or subtract onto X. Is this clear to everybody,
the main intuition? Question. [student speaks off mic] - Yeah, so the question
is not clear why is it that learning the
residual should be easier than learning the direct mapping? And so this is just their hypotheses, and a hypotheses is that if
we're learning the residual you just have to learn
what's the delta to X right? And if our hypotheses is that generally even something like our
solution by construction, where we had some number
of these shallow layers that were learned and we had
all these identity mappings at the top this was a
solution that should have been good, and so that implies that
maybe a lot of these layers, actually something just close to identity, would be a good layer And so because of that,
now we formulate this as being able to learn the identity plus just a little delta. And if really the identity
is best we just make F of X squashes transformation
to just be zero, which is something that's relatively, might seem easier to learn, also we're able to get
things that are close to identity mappings. And so again this is not
something that's necessarily proven or anything it's just
the intuition and hypothesis, and then we'll also see
later some works where people are actually trying to
challenge this and say oh maybe it's not actually the residuals
that are so necessary, but at least this is the
hypothesis for this paper, and in practice using this model, it was able to do very well. Question. [student speaks off mic] - Yes so the question is
have people tried other ways of combining the inputs
from previous layers and yes so this is basically a very
active area of research on and how we formulate
all these connections, and what's connected to what
in all of these structures. So we'll see a few more
examples of different network architectures briefly later
but this is an active area of research. OK so we basically have all
of these residual blocks that are stacked on top of each other. We can see the full resident architecture. Each of these residual blocks
has two three by three conv layers as part of this block
and there's also been work just saying that this happens
to be a good configuration that works well. We stack all these blocks
together very deeply. Another thing like with
this very deep architecture it's basically also
enabling up to 150 layers deep of this, and then
what we do is we stack all these and periodically we also double the number of filters
and down sample spatially using stride two when we do that. And then we have this additional [mumbles] at the very beginning of our network and at the end we also hear, don't have any fully connected layers and we just have a global
average pooling layer that's going to average
over everything spatially, and then be input into the
last 1000 way classification. So this is the full ResNet architecture and it's very simple and
elegant just stacking up all of these ResNet blocks
on top of each other, and they have total depths
of up to 34, 50, 100, and they tried up to 152 for ImageNet. OK so one additional
thing just to know is that for a very deep network,
so the ones that are more than 50 layers deep, they
also use bottleneck layers similar to what GoogleNet did
in order to improve efficiency and so within each block
now you're going to, what they did is, have this
one by one conv filter, that first projects it
down to a smaller depth. So again if we are looking
at let's say 28 by 28 by 256 implant, we do
this one by one conv, it's taking it's
projecting the depth down. We get 28 by 28 by 64. Now your convolution
your three by three conv, in here they only have
one, is operating over this reduced step so it's going
to be less expensive, and then afterwards they have another one by one conv that
projects the depth back up to 256, and so, this is
the actual block that you'll see in deeper networks. So in practice the ResNet
also uses batch normalization after every conv layer, they
use Xavier initialization with an extra scaling factor
that they helped introduce to improve the initialization
trained with SGD + momentum. Their learning rate they
use a similar learning rate type of schedule where you
decay your learning rate when your validation error plateaus. Mini batch size 256, a
little bit of weight decay and no drop out. And so experimentally they
were able to show that they were able to train these
very deep networks, without degrading. They were able to have
basically good gradient flow coming all the way back
down through the network. They tried up to 152 layers on ImageNet, 1200 on Cifar, which is a,
you have played with it, but a smaller data set
and they also saw that now you're deeper networks are
able to achieve lower training errors as expected. So you don't have the same strange plots that we saw earlier where the behavior was in the wrong direction. And so from here they were
able to sweep first place at all of the ILSVRC competitions, and all of the COCO competitions in 2015 by a significant margins. Their total top five error
was 3.6 % for a classification and this is actually better
than human performance in the ImageNet paper. There was also a human
metric that came from actually [mumbles] our
lab Andre Kapathy spent like a week training
himself and then basically did all of, did this task himself and was I think somewhere around 5-ish %, and so I was basically able to do better than the then that human at least. Okay, so these are kind
of the main networks that have been used recently. We had AlexNet starting off with first, VGG and GoogleNet are still very popular, but ResNet is the most
recent best performing model that if you're looking for
something training a new network ResNet is available, you should try working with it. So just quickly looking at
some of this getting a better sense of the complexity involved. So here we have some
plots that are sorted by performance so this is
top one accuracy here, and higher is better. And so you'll see a lot
of these models that we talked about, as well as
some different versions of them so, this
GoogleNet inception thing, I think there's like V2,
V3 and the best one here is V4, which is actually
a ResNet plus inception combination, so these are just kind of more incremental, smaller
changes that they've built on top of them,
and so that's the best performing model here. And if we look on the
right, these plots of their computational complexity here it's sorted. The Y axis is your top one accuracy so higher is better. The X axis is your operations
and so the more to the right, the more ops you're doing,
the more computationally expensive and then the bigger the circle, your circle is your memory usage, so the gray circles are referenced here, but the bigger the circle
the more memory usage and so here we can see
that VGG these green ones are kind of the least efficient. They have the biggest memory, the most operations, but they they do pretty well. GoogleNet is the most efficient here. It's way down on the operation side, as well as a small little
circle for memory usage. AlexNet, our earlier
model, has lowest accuracy. It's relatively smaller compute, because it's a smaller network, but
it's also not particularly memory efficient. And then ResNet here, we
have moderate efficiency. It's kind of in the middle,
both in terms of memory and operations, and it
has the highest accuracy. And so here also are
some additional plots. You can look at these
more on your own time, but this plot on the left is
showing the forward pass time and so this is in milliseconds
and you can up at the top VGG forward passes about 200
milliseconds you can get about five frames per second with this, and this is sorted in order. There's also this plot on
the right looking at power consumption and if you look
more at this paper here, there's further analysis of
these kinds of computational comparisons. So these were the main
architectures that you should really know in-depth and be familiar with, and be thinking about actively using. But now I'm going just
to go briefly through some other architectures
that are just good to know either historical inspirations or more recent areas of research. So the first one Network in Network, this is from 2014, and
the idea behind this is that we have these
vanilla convolutional layers but we also have these,
this introduces the idea of MLP conv layers they call
it, which are micro networks or basically network within networth, the name of the paper. Where within each conv
layer trying to stack an MLP with a couple of fully
connected layers on top of just the standard conv
and be able to compute more abstract features for these local patches right. So instead of sliding
just a conv filter around, it's sliding a slightly
more complex hierarchical set of filters around
and using that to get the activation maps. And so, it uses these fully connected, or basically one by one
conv kind of layers. It's going to stack them all up like the bottom diagram here where
we just have these networks within networks stacked
in each of the layers. And the main reason to know this is just it was kind of a precursor
to GoogleNet and ResNet in 2014 with this idea
of bottleneck layers that you saw used very heavily in there. And it also had a little bit
of philosophical inspiration for GoogleNet for this idea
of a local network typology network in network that they also used, with a different kind of structure. Now I'm going to talk
about a series of works, on, or works since ResNet
that are mostly geared towards improving resNet
and so this is more recent research has been done since then. I'm going to go over these pretty fast, and so just at a very high level. If you're interested in
any of these you should look at the papers, to have more details. So the authors of ResNet
a little bit later on in 2016 also had this paper
where they improved the ResNet block design. And so they basically
adjusted what were the layers that were in the ResNet block path, and showed this new
structure was able to have a more direct path in order
for propagating information throughout the network,
and you want to have a good path to propagate
information all the way up, and then back up all the way down again. And so they showed that this
new block was better for that and was able to give better performance. There's also a Wide Residual
networks which this paper argued that while ResNets
made networks much deeper as well as added these
residual connections and their argument was
that residuals are really the important factor. Having this residual construction, and not necessarily having
extremely deep networks. And so what they did was they
used wider residual blocks, and so what this means is
just more filters in every conv layer. So before we might have
F filters per layer and they use these factors
of K and said well, every layer it's going to be
F times K filters instead. And so, using these
wider layers they showed that their 50 layer wide
ResNet was able to out-perform the 152 layer original ResNet, and it also had the
additional advantages of increasing with this,
even with the same amount of parameters, tit's more
computationally efficient because you can parallelize
these with operations more easily. Right just convolutions with more neurons just spread across more kernels as opposed to depth
that's more sequential, so it's more computationally
efficient to increase your width. So here you can see
this work is starting to trying to understand the
contributions of width and depth and residual connections, and making some arguments
for one way versus the other. And this other paper around the same time, I think maybe a little
bit later, is ResNeXt, and so this is again,
the creators of ResNet continuing to work on
pushing the architecture. And here they also had
this idea of okay, let's indeed tackle this width
thing more but instead of just increasing the width
of this residual block through more filters they have structure. And so within each residual
block, multiple parallel pathways and they're going to call the total number of these
pathways the cardinality. And so it's basically
taking the one ResNet block with the bottlenecks and having
it be relatively thinner, but having multiple of
these done in parallel. And so here you can also
see that this both have some relation to this idea of wide networks, as well as to has some connection
to the inception module as well right where we
have these parallel, these layers operating in parallel. And so now this ResNeXt has
some flavor of that as well. So another approach
towards improving ResNets was this idea called Stochastic
Depth and in this work the motivation is well let's look more at this depth problem. Once you get deeper and
deeper the typical problems that you're going to have
vanishing gradients right. You're not able to, your
gradients will get smaller and eventually vanish as
you're trying to back propagate them over very long layers,
or a large number of layers. And so what their motivation
is well let's try to have short networks during training
and they use this idea of dropping out a subset of
the layers during training. And so for a subset of the
layers they just drop out the weights and they just set
it to identity connection, and now what you get is you
have these shorter networks during training, you can pass back your gradients better. It's also a little more
efficient, and then it's kind of like the drop out right. It has this sort of flavor
that you've seen before. And then at test time you want
to use the full deep network that you've trained. So these are some of the
works that looking at the resident architecture, trying
to understand different aspects of it and trying
to improve ResNet training. And so there's also some
works now that are going beyond ResNet that are
saying well what are some non ResNet architectures that
maybe can also work better, or comparable or better to ResNets. And so one idea is
FractalNet, which came out pretty recently, and the
argument in FractalNet is that while residual
representations maybe are not actually necessary,
so this goes back to what we were talking about earlier. What's the motivation of
residual networks and it seems to make sense and there's, you know, good reasons for why this
should help but in this paper they're saying that well here
is a different architecture that we're introducing, there's
no residual representations. We think that the key is
more about transitioning effectively from shallow to deep networks, and so they have this fractal architecture which has if you look on the right here, these layers where they compose
it in this fractal fashion. And so there's both
shallow and deep pathways to your output. And so they have these
different length pathways, they train them with
dropping out sub paths, and so again it has this
dropout kind of flavor, and then at test time they'll
use the entire fractal network and they show that this was able to get very good performance. There's another idea
called Densely Connected convolutional Networks,
DenseNet, and this idea is now we have these
blocks that are called dense blocks. And within each block
each layer is going to be connected to every other layer after it, in this feed forward fashion. So within this block,
your input to the block is also the input to
every other conv layer, and as you compute each conv output, those outputs are now connected to every layer after and then,
these are all concatenated as input to the conv
layer, and they do some they have some other
processes for reducing the dimensions and keeping efficient. And so their main takeaway from this, is that they argue that
this is alleviating a vanishing gradient problem
because you have all of these very dense connections. It strengthens feature propagation
and then also encourages future use right because
there are so many of these connections each feature
map that you're learning is input in multiple
later layers and being used multiple times. So these are just a
couple of ideas that are you know alternatives or
what can we do that's not ResNets and yet is still performing either comparably or better to
ResNets and so this is another very active area
of current research. You can see that a lot of this is looking at the way how different layers
are connected to each other and how depth is managed
in these networks. And so one last thing
that I wanted to mention quickly, is just efficient networks. So this idea of efficiency
and you saw that GoogleNet was a work that was
looking into this direction of how can we have efficient
networks which are important for you know a lot of
practical usage both training as well as especially
deployment and so this is another recent network
that's called SqueezeNet which is looking at
very efficient networks. They have these things
called fire modules, which consists of a
squeeze layer with a lot of one by one filters and
then this feeds then into an expand layer with one by
one and three by three filters, and they're showing that with
this kind of architecture they're able to get AlexNet
level accuracy on ImageNet, but with 50 times fewer parameters, and then you can further do
network compression on this to get up to 500 times
smaller than AlexNet and just have the whole
network just be 0.5 megs. And so this is a direction
of how do we have efficient networks model compression that we'll cover more in a lecture later, but just giving you a hint of that. OK so today in summary we've
talked about different kinds of CNN Architectures. We looked in-depth at four
of the main architectures that you'll see in wide usage. AlexNet, one of the early,
very popular networks. VGG and GoogleNet which
are still widely used. But ResNet is kind of
taking over as the thing that you should be
looking most when you can. We also looked at these other networks in a little bit more depth at a brief level overview. And so the takeaway that these
models that are available they're in a lot of
[mumbles] so you can use them when you need them. There's a trend toward
extremely deep networks, but there's also significant
research now around the design of how do we connect layers, skip connections, what
is connected to what, and also using these to
design your architecture to improve gradient flow. There's an even more recent
trend towards examining what's the necessity
of depth versus width, residual connections. Trade offs, what's
actually helping matters, and so there's a lot of these recent works in this direction that you can look into some of the ones I pointed
out if you are interested. And next time we'll talk about
Recurrent neural networks. Thanks. 

- Okay. Can everyone hear me? Okay. Sorry for the delay. I had a bit of technical difficulty. Today was the first time
I was trying to use my new touch bar Mac book pro for presenting, and none of the adapters are working. So, I had to switch
laptops at the last minute. So, thanks. Sorry about that. So, today is lecture 10. We're talking about
recurrent neural networks. So, as of, as usual, a
couple administrative notes. So, We're working hard on assignment one grading. Those grades will probably be out sometime later today. Hopefully, they can get out before the A2 deadline. That's what I'm hoping for. On a related note, Assignment
two is due today at 11:59 p.m. so, who's done with that already? About half you guys. So, you remember, I did warn you when the assignment went out that it was quite long, to start early. So, you were warned about that. But, hopefully, you guys
have some late days left. Also, as another reminder, the midterm will be in class on Tuesday. If you kind of look
around the lecture hall, there are not enough seats in this room to seat all the enrolled
students in the class. So, we'll actually be having the midterm in several other lecture
halls across campus. And we'll be sending out some more details on exactly where to go in
the next couple of days. So a bit of a, another
bit of announcement. We've been working on this sort of fun bit of extra credit
thing for you to play with that we're calling the training game. This is this cool
browser based experience, where you can go in and interactively train neural networks and tweak the hyper
parameters during training. And this should be a
really cool interactive way for you to practice some of these hyper
parameter tuning skills that we've been talking about
the last couple of lectures. So this is not required, but this, I think, will be
a really useful experience to gain a little bit more intuition into how some of these
hyper parameters work for different types of
data sets in practice. So we're still working on getting all the bugs worked out of this setup, and we'll probably send out some more instructions on exactly how this will work in the next couple of days. But again, not required. But please do check it out. I think it'll be really fun and a really cool thing
for you to play with. And will give you a bit of extra credit if you do some, if you end up working with this and doing a couple of runs with it. So, we'll again send out
some more details about this soon once we get all the bugs worked out. As a reminder, last time we were talking
about CNN Architectures. We kind of walked through the time line of some of the various winners of the image net classification challenge, kind of the breakthrough result. As we saw was the AlexNet
architecture in 2012, which was a nine layer
convolutional network. It did amazingly well, and it sort of kick started this whole deep learning
revolution in computer vision, and kind of brought a lot of these models into the mainstream. Then we skipped ahead a couple years, and saw that in 2014 image net challenge, we had these two really
interesting models, VGG and GoogLeNet, which were much deeper. So VGG was, they had a 16 and a 19 layer model, and GoogLeNet was, I believe, a 22 layer model. Although one thing that
is kind of interesting about these models is that the 2014 image net challenge was right before batch
normalization was invented. So at this time, before the invention
of batch normalization, training these relatively deep models of roughly twenty layers
was very challenging. So, in fact, both of these two models had to resort to a little bit of hackery in order to get their
deep models to converge. So for VGG, they had the
16 and 19 layer models, but actually they first
trained an 11 layer model, because that was what they
could get to converge. And then added some extra
random layers in the middle and then continued training, actually training the
16 and 19 layer models. So, managing this training process was very challenging in 2014 before the invention
of batch normalization. Similarly, for GoogLeNet, we saw that GoogLeNet has
these auxiliary classifiers that were stuck into lower
layers of the network. And these were not really
needed for the class to, to get good classification performance. This was just sort of a way to cause extra gradient to be injected directly into the lower
layers of the network. And this sort of, this again was before the
invention of batch normalization and now once you have these networks with batch normalization, then you no longer need
these slightly ugly hacks in order to get these
deeper models to converge. Then we also saw in the
2015 image net challenge was this really cool model called ResNet, these residual networks that now have these shortcut connections that actually have these
little residual blocks where we're going to take our input, pass it through the residual blocks, and then add that output of the, then add our input to the block, to the output from these
convolutional layers. This is kind of a funny architecture, but it actually has two
really nice properties. One is that if we just set all the weights in this residual block to zero, then this block is competing the identity. So in some way, it's relatively easy for this model to learn not to use the
layers that it doesn't need. In addition, it kind of adds this interpretation to L2 regularization in the context of these neural networks, cause once you put L2 regularization, remember, on your, on the weights of your network, that's going to drive all
the parameters towards zero. And maybe your standard
convolutional architecture is driving towards zero. Maybe it doesn't make sense. But in the context of a residual network, if you drive all the
parameters towards zero, that's kind of encouraging the model to not use layers that it doesn't need, because it will just drive those, the residual blocks towards the identity, whether or not needed for classification. The other really useful property
of these residual networks has to do with the gradient
flow in the backward paths. If you remember what happens
at these addition gates in the backward pass, when upstream gradient is coming in through an addition gate, then it will split and fork along these two different paths. So then, when upstream gradient comes in, it'll take one path through
these convolutional blocks, but it will also have a direct
connection of the gradient through this residual connection. So then when you look at, when you imagine stacking many of these residual blocks on top of each other, and our network ends up with hundreds of, potentially hundreds of layers. Then, these residual connections give a sort of gradient super highway for gradients to flow backward through the entire network. And this allows it to train much easier and much faster. And actually allows
these things to converge reasonably well, even when the model is potentially
hundreds of layers deep. And this idea of managing
gradient flow in your models is actually super important
everywhere in machine learning. And super prevalent in
recurrent networks as well. So we'll definitely revisit
this idea of gradient flow later in today's lecture. So then, we kind of also saw
a couple other more exotic, more recent CNN architectures last time, including DenseNet and FractalNet, and once you think about
these architectures in terms of gradient flow, they make a little bit more sense. These things like DenseNet and FractalNet are adding these additional shortcut or identity connections inside the model. And if you think about what happens in the backwards pass for these models, these additional funny topologies are basically providing direct paths for gradients to flow from the loss at the end of the network more easily into all the
different layers of the network. So I think that, again, this idea of managing
gradient flow properly in your CNN Architectures is something that we've really seen a lot more in the last couple of years. And will probably see more moving forward as more exotic architectures are invented. We also saw this kind of nice plot, plotting performance of the number of flops versus
the number of parameters versus the run time of
these various models. And there's some
interesting characteristics that you can dive in
and see from this plot. One idea is that VGG and AlexNet have a huge number of parameters, and these parameters actually come almost entirely from the
fully connected layers of the models. So AlexNet has something like
roughly 62 million parameters, and if you look at that
last fully connected layer, the final fully connected layer in AlexNet is going from an activation
volume of six by six by 256 into this fully connected vector of 496. So if you imagine what the weight matrix needs to look like at that layer, the weight matrix is gigantic. It's number of entries is six by six, six times six times 256 times 496. And if you multiply that out, you see that that single layer has 38 million parameters. So more than half of the parameters of the entire AlexNet model are just sitting in that
last fully connected layer. And if you add up all the parameters in just the fully connected
layers of AlexNet, including these other
fully connected layers, you see something like
59 of the 62 million parameters in AlexNet are sitting in these
fully connected layers. So then when we move other architectures, like GoogLeNet and ResNet, they do away with a lot of these large fully connected layers in favor of global average pooling at the end of the network. And this allows these
networks to really cut, these nicer architectures, to really cut down the parameter count in these architectures. So that was kind of our brief recap of the CNN architectures
that we saw last lecture, and then today, we're going to move to one of my favorite topics to talk about, which is recurrent neural networks. So, so far in this class, we've seen, what I like to think of as kind of a vanilla feed forward network, all of our network
architectures have this flavor, where we receive some input and that input is a fixed size object, like an image or vector. That input is fed through
some set of hidden layers and produces a single output, like a classifications, like a set of classifications scores over a set of categories. But in some context in machine learning, we want to have more flexibility in the types of data that
our models can process. So once we move to this idea
of recurrent neural networks, we have a lot more opportunities to play around with the types
of input and output data that our networks can handle. So once we have recurrent neural networks, we can do what we call
these one to many models. Or where maybe our input is
some object of fixed size, like an image, but now our output is a
sequence of variable length, such as a caption. Where different captions might have different numbers of words, so our output needs to
be variable in length. We also might have many to one models, where our input could be variably sized. This might be something
like a piece of text, and we want to say what is
the sentiment of that text, whether it's positive or
negative in sentiment. Or in a computer vision context, you might imagine taking as input a video, and that video might have a
variable number of frames. And now we want to read this entire video of potentially variable length. And then at the end, make a classification decision about maybe what kind
of activity or action is going on in that video. We also have a, we
might also have problems where we want both the inputs and the output to be variable in length. We might see something like this in machine translation, where our input is some,
maybe, sentence in English, which could have a variable length, and our output is maybe
some sentence in French, which also could have a variable length. And crucially, the length
of the English sentence might be different from the
length of the French sentence. So we need some models
that have the capacity to accept both variable length sequences on the input and on the output. Finally, we might also
consider problems where our input is variably length, like something like a video sequence with a variable number of frames. And now we want to make a decision for each element of that input sequence. So in the context of videos, that might be making some
classification decision along every frame of the video. And recurrent neural networks are this kind of general paradigm for handling variable sized sequence data that allow us to pretty naturally capture all of these different types
of setups in our models. So recurring neural networks
are actually important, even for some problems that
have a fixed size input and a fixed size output. Recurrent neural networks
can still be pretty useful. So in this example, we might want to do, for example, sequential processing of our input. So here, we're receiving
a fixed size input like an image, and we want to make a
classification decision about, like, what number is
being shown in this image? But now, rather than just doing
a single feed forward pass and making the decision all at once, this network is actually
looking around the image and taking various glimpses of
different parts of the image. And then after making
some series of glimpses, then it makes its final decision as to what kind of number is present. So here, we had one, So here, even though
our input and outputs, our input was an image, and our output was a
classification decision, even this context, this idea of being able to handle variably length processing
with recurrent neural networks can lead to some really
interesting types of models. There's a really cool paper that I like that applied this same type of idea to generating new images. Where now, we want the model
to synthesize brand new images that look kind of like the
images it saw in training, and we can use a recurrent
neural network architecture to actually paint these output images sort of one piece at a time in the output. You can see that, even though our output
is this fixed size image, we can have these models
that are working over time to compute parts of the output
one at a time sequentially. And we can use recurrent neural networds for that type of setup as well. So after this sort of cool pitch about all these cool
things that RNNs can do, you might wonder, like what
exactly are these things? So in general, a recurrent neural network is this little, has this
little recurrent core cell and it will take some input x, feed that input into the RNN, and that RNN has some
internal hidden state, and that internal hidden
state will be updated every time that the RNN reads a new input. And that internal hidden state will be then fed back to the model the next time it reads an input. And frequently, we will want our RNN"s to also produce some
output at every time step, so we'll have this pattern
where it will read an input, update its hidden state, and then produce an output. So then the question is what is the functional form
of this recurrence relation that we're computing? So inside this little green RNN block, we're computing some recurrence relation, with a function f. So this function f will
depend on some weights, w. It will accept the previous
hidden state, h t - 1, as well as the input at
the current state, x t, and this will output the next hidden state, or
the updated hidden state, that we call h t. And now, then as we read the next input, this hidden state, this new hidden state, h t, will then just be passed
into the same function as we read the next input, x t plus one. And now, if we wanted
to produce some output at every time step of this network, we might attach some additional
fully connected layers that read in this h t at every time step. And make that decision based on the hidden state at every time step. And one thing to note is that we use the same function, f w, and the same weights, w, at every time step of the computation. So then kind of the simplest function form that you can imagine is what we call this vanilla
recurrent neural network. So here, we have this same functional form from the previous slide, where we're taking in
our previous hidden state and our current input and we need to produce
the next hidden state. And the kind of simplest
thing you might imagine is that we have some weight matrix, w x h, that we multiply against the input, x t, as well as another weight matrix, w h h, that we multiply against
the previous hidden state. So we make these two multiplications against our two states, add them together, and squash them through a tanh, so we get some kind of non
linearity in the system. You might be wondering
why we use a tanh here and not some other type of non-linearity? After all that we've said negative about tanh's in previous lectures, and I think we'll return
a little bit to that later on when we talk about more advanced architectures, like lstm. So then, this, So then, in addition in this architecture, if we wanted to produce
some y t at every time step, you might have another weight matrix, w, you might have another weight matrix that accepts this hidden state and then transforms it to some y to produce maybe some
class score predictions at every time step. And when I think about
recurrent neural networks, I kind of think about, you can also, you can kind of think of
recurrent neural networks in two ways. One is this concept of
having a hidden state that feeds back at itself, recurrently. But I find that picture
a little bit confusing. And sometimes, I find it clearer to think about unrolling
this computational graph for multiple time steps. And this makes the data
flow of the hidden states and the inputs and the
outputs and the weights maybe a little bit more clear. So then at the first time step, we'll have some initial
hidden state h zero. This is usually initialized
to zeros for most context, in most contexts, an then
we'll have some input, x t. This initial hidden state, h zero, and our current input, x t, will go into our f w function. This will produce our
next hidden state, h one. And then, we'll repeat this process when we receive the next input. So now our current h one and our x one, will go into that same f w, to produce our next output, h two. And this process will
repeat over and over again, as we consume all of the input, x ts, in our sequence of inputs. And now, one thing to note, is that we can actually make
this even more explicit and write the w matrix in
our computational graph. And here you can see that we're re-using the same w matrix at every time step of the computation. So now every time that we
have this little f w block, it's receiving a unique h and a unique x, but all of these blocks
are taking the same w. And if you remember, we talked about how gradient
flows in back propagation, when you re-use the same, when you re-use the
same node multiple times in a computational graph, then remember during the backward pass, you end up summing the gradients into the w matrix when you're computing a d los d w. So, if you kind of think about the back propagation for this model, then you'll have a separate gradient for w flowing from each of those time steps, and then the final gradient for w will be the sum of all of those individual per time step gradiants. We can also write to this y t explicitly in this computational graph. So then, this output,
h t, at every time step might feed into some other
little neural network that can produce a y t, which might be some class
scores, or something like that, at every time step. We can also make the loss more explicit. So in many cases, you
might imagine producing, you might imagine that you
have some ground truth label at every time step of your sequence, and then you'll compute some loss, some individual loss, at every time step of these outputs, y t's. And this loss might, it will frequently be
something like soft max loss, in the case where you have, maybe, a ground truth label at every
time step of the sequence. And now the final loss for the entire, for this entire training stop, will be the sum of
these individual losses. So now, we had a scaler
loss at every time step? And we just summed them
up to get our final scaler loss at the top of the network. And now, if you think about, again, back propagation
through this thing, we need, in order to train the model, we need to compute the gradient of the loss with respect to w. So, we'll have loss flowing
from that final loss into each of these time steps. And then each of those time steps will compute a local
gradient on the weights, w, which will all then be
summed to give us our final gradient for the weights, w. Now if we have a, sort of,
this many to one situation, where maybe we want to do
something like sentiment analysis, then we would typically make that decision based on the final hidden
state of this network. Because this final hidden state kind of summarizes all of the context from the entire sequence. Also, if we have a kind of
a one to many situation, where we want to receive a fix sized input and then produce a variably sized output. Then you'll commonly use
that fixed size input to initialize, somehow, the initial hidden state of the model, and now the recurrent network will tick for each cell in the output. And now, as you produce
your variably sized output, you'll unroll the graph for
each element in the output. So this, when we talk about
the sequence to sequence models where you might do something
like machine translation, where you take a variably sized input and a variably sized output. You can think of this as a combination of the many to one, plus a one to many. So, we'll kind of proceed in two stages, what we call an encoder and a decoder. So if you're the encoder, we'll receive the variably sized input, which might be your sentence in English, and then summarize that entire sentence using the final hidden state
of the encoder network. And now we're in this
many to one situation where we've summarized this
entire variably sized input in this single vector, and now, we have a second decoder network, which is a one to many situation, which will input that single vector summarizing the input sentence and now produce this
variably sized output, which might be your sentence
in another language. And now in this variably sized output, we might make some predictions
at every time step, maybe about what word to use. And you can imagine kind of
training this entire thing by unrolling this computational graph summing the losses at the output sequence and just performing back
propagation, as usual. So as a bit of a concrete example, one thing that we frequently use recurrent neural networks for, is this problem called language modeling. So in the language modeling problem, we want to read some sequence of, we want to have our
network, sort of, understand how to produce natural language. So in the, so this, this might
happen at the character level where our model will produce
characters one at a time. This might also happen at the word level where our model will
produce words one at a time. But in a very simple example, you can imagine this
character level language model where we want, where the network will read
some sequence of characters and then it needs to predict, what will the next character
be in this stream of text? So in this example, we have this very small
vocabulary of four letters, h, e, l, and o, and we have
this example training sequence of the word hello, h, e, l, l, o. So during training, when we're training this language model, we will feed the characters
of this training sequence as inputs, as x ts, to out input of our, we'll feed the characters
of our training sequence, these will be the x ts that
we feed in as the inputs to our recurrent neural network. And then, each of these inputs, it's a letter, and we need to figure out a way to represent letters in our network. So what we'll typically do is figure out what is our total vocabulary. In this case, our vocabulary
has four elements. And each letter will be
represented by a vector that has zeros in every slot but one, and a one for the slot in the vocabulary corresponding to that letter. In this little example, since our vocab has the
four letters, h, e, l, o, then our input sequence, the h is represented by
a four element vector with a one in the first slot and zero's in the other three slots. And we use the same sort of pattern to represent all the different letters in the input sequence. Now, during this forward pass of what this network is doing, at the first time step, it will receive the input letter h. That will go into the first RNN, to the RNN cell, and then we'll produce this output, y t, which is the network making predictions about for each letter in the vocabulary, which letter does it think is most likely going to come next. In this example, the correct output letter was e because our training sequence was hello, but the model is actually predicting, I think it's actually predicting o as the most likely letter. So in this case, this prediction was wrong and we would use softmaxt loss to quantify our unhappiness
with these predictions. The next time step, we would feed in the second letter in the training sequence, e, and this process will repeat. We'll now represent e as a vector. Use that input vector together with the previous hidden state to produce a new hidden state and now use the second hidden state to, again, make predictions over every letter in the vocabulary. In this case, because our
training sequence was hello, after the letter e, we want our model to predict l. In this case, our model may have very low predictions for the letter l, so we
would incur high loss. And you kind of repeat
this process over and over, and if you train this model
with many different sequences, then eventually it should learn how to predict the next
character in a sequence based on the context of
all the previous characters that it's seen before. And now, if you think about
what happens at test time, after we train this model, one thing that we might want to do with it is a sample from the model, and actually use this
trained neural network model to synthesize new text that kind of looks similar in spirit to the text that it was trained on. The way that this will work is we'll typically see the model with some input prefix of text. In this case, the prefix is
just the single letter h, and now we'll feed that letter h through the first time step of
our recurrent neural network. It will product this
distribution of scores over all the characters in the vocabulary. Now, at training time,
we'll use these scores to actually sample from it. So we'll use a softmaxt function to convert those scores into
a probability distribution and then we will sample from
that probability distribution to actually synthesize the
second letter in the sequence. And in this case, even though
the scores were pretty bad, maybe we got lucky and
sampled the letter e from this probability distribution. And now, we'll take this letter e that was sampled from this distribution and feed it back as input into the network at the next time step. Now, we'll take this e,
pull it down from the top, feed it back into the network as one of these, sort of, one
hot vectorial representations, and then repeat the process in order to synthesize the
second letter in the output. And we can repeat this
process over and over again to synthesize a new sequence
using this trained model, where we're synthesizing the sequence one character at a time using these predicted
probability distributions at each time step. Question? Yeah, that's a great question. So the question is why might we sample instead of just taking the character with the largest score? In this case, because of the probability
distribution that we had, it was impossible to
get the right character, so we had the sample so
the example could work out, and it would make sense. But in practice,
sometimes you'll see both. So sometimes you'll just
take the argmax probability, and that will sometimes be
a little bit more stable, but one advantage of sampling, in general, is that it lets you get
diversity from your models. Sometimes you might have the same input, maybe the same prefix, or in the case of image captioning, maybe the same image. But then if you sample rather
than taking the argmax, then you'll see that
sometimes these trained models are actually able to produce
multiple different types of reasonable output sequences, depending on the kind, depending on which samples they take at the first time steps. It's actually kind of a benefit cause we can get now more
diversity in our outputs. Another question? Could we feed in the softmax vector instead of the one element vector? You mean at test time? Yeah yeah, so the
question is, at test time, could we feed in this whole softmax vector rather than a one hot vector? There's kind of two problems with that. One is that that's very different from the data that it
saw at training time. In general, if you ask your model to do something at test time, which is different from training time, then it'll usually blow up. It'll usually give you garbage and you'll usually be sad. The other problem is that in practice, our vocabularies might be very large. So maybe, in this simple example, our vocabulary is only four elements, so it's not a big problem. But if you're thinking about
generating words one at a time, now your vocabulary is every
word in the English language, which could be something like
tens of thousands of elements. So in practice, this first element, this first operation that's
taking in this one hot vector, is often performed using
sparse vector operations rather than dense factors. It would be, sort of,
computationally really bad if you wanted to have this load of 10,000 elements softmax vector. So that's usually why we
use a one hot instead, even at test time. This idea that we have a sequence and we produce an output at
every time step of the sequence and then finally compute some loss, this is sometimes called
backpropagation through time because you're imagining
that in the forward pass, you're kind of stepping
forward through time and then during the backward pass, you're sort of going
backwards through time to compute all your gradients. This can actually be kind of problematic if you want to train the sequences
that are very, very long. So if you imagine that we
were kind of trying to train a neural network language model on maybe the entire text of Wikipedia, which is, by the way, something that people
do pretty frequently, this would be super slow, and every time we made a gradient step, we would have to make a forward pass through the entire text
of all of wikipedia, and then make a backward pass
through all of wikipedia, and then make a single gradient update. And that would be super slow. Your model would never converge. It would also take a
ridiculous amount of memory so this would be just really bad. In practice, what people
do is this, sort of, approximation called truncated
backpropagation through time. Here, the idea is that, even though our input
sequence is very, very long, and even potentially infinite, what we'll do is that during, when we're training the model, we'll step forward for
some number of steps, maybe like a hundred is
kind of a ballpark number that people frequently use, and we'll step forward
for maybe a hundred steps, compute a loss only over this
sub sequence of the data, and then back propagate
through this sub sequence, and now make a gradient step. And now, when we repeat, well, we still have these hidden states that we computed from the first batch, and now, when we compute
this next batch of data, we will carry those hidden
states forward in time, so the forward pass will
be exactly the same. But now when we compute a gradient step for this next batch of data, we will only backpropagate
again through this second batch. Now, we'll make a gradient step based on this truncated
backpropagation through time. This process will continue, where now when we make the next batch, we'll again copy these
hidden states forward, but then step forward
and then step backward, but only for some small
number of time steps. So this is, you can kind of think of this as being an alegist who's
the cast at gradient descent in the case of sequences. Remember, when we talked
about training our models on large data sets, then these data sets, it would be super expensive
to compute the gradients over every element in the data set. So instead, we kind of take small samples, small mini batches instead, and use mini batches of data
to compute gradient stops in any kind of image classification case. Question? Is this kind of, the question is, is this kind of making
the Mark Hobb assumption? No, not really. Because we're carrying
this hidden state forward in time forever. It's making a Marcovian assumption in the sense that, conditioned
on the hidden state, but the hidden state is all that we need to predict the entire future of the sequence. But that assumption is kind of built into the recurrent neural network formula from the start. And that's not really particular to back propagation through time. Back propagation through time, or sorry, truncated back prop though time is just the way to
approximate these gradients without going making a backwards pass through your potentially
very large sequence of data. This all sounds very
complicated and confusing and it sounds like a lot of code to write, but in fact, this can
acutally be pretty concise. Andrea has this example of
what he calls min-char-rnn, that does all of this stuff in just like a 112 lines of Python. It handles building the vocabulary. It trains the model with truncated back
propagation through time. And then, it can actually
sample from that model in actually not too much code. So even though this sounds like kind of a big, scary process, it's actually not too difficult. I'd encourage you, if you're confused, to maybe go check this out and step through the
code on your own time, and see, kind of, all
of these concrete steps happening in code. So this is all in just a single file, all using numpy with no dependencies. This was relatively easy to read. So then, once we have this idea of training a recurrent
neural network language model, we can actually have a
lot of fun with this. And we can take in, sort
of, any text that we want. Take in, like, whatever
random text you can think of from the internet, train our recurrent neural
network language model on this text, and then generate new text. So in this example, we
took this entire text of all of Shakespeare's works, and then used that to train a recurrent neural network language model on all of Shakespeare. And you can see that the
beginning of training, it's kind of producing maybe
random gibberish garbage, but throughout the course of training, it ends up producing things
that seem relatively reasonable. And after you've, after this model has
been trained pretty well, then it produces text that seems, kind of, Shakespeare-esque to me. "Why do what that day," replied, whatever, right, you can read this. Like, it kind of looks
kind of like Shakespeare. And if you actually train
this model even more, and let it converge even further, and then sample these
even longer sequences, you can see that it learns
all kinds of crazy cool stuff that really looks like a Shakespeare play. It knows that it uses,
maybe, these headings to say who's speaking. Then it produces these bits of text that have crazy dialogue that sounds kind of Shakespeare-esque. It knows to put line breaks in between these different things. And this is all, like, really cool, all just sort of learned from
the structure of the data. We can actually get
even crazier than this. This was one of my favorite examples. I found online, there's this. Is anyone a mathematician in this room? Has anyone taken an algebraic
topology course by any chance? Wow, a couple, that's impressive. So you probably know more
algebraic topology than me, but I found this open source algebraic topology textbook online. It's just a whole bunch of tech files that are like this
super dense mathematics. And LaTac, cause LaTac is sort of this, let's you write equations and diagrams and everything just using plain text. We can actually train our recurrent neural network language model on the raw Latac source code of this algebraic topology textbook. And if we do that, then after
we sample from the model, then we get something that seems like, kind of like algebraic topology. So it knows to like put equations. It puts all kinds of crazy stuff. It's like, to prove study, we see that F sub U is
a covering of x prime, blah, blah, blah, blah, blah. It knows where to put unions. It knows to put squares
at the end of proofs. It makes lemmas. It makes references to previous lemmas. Right, like we hear, like. It's namely a bi-lemma question. We see that R is geometrically something. So it's actually pretty crazy. It also sometimes tries to make diagrams. For those of you that have
taken algebraic topology, you know that these commutative diagrams are kind of a thing
that you work with a lot So it kind of got the general gist of how to make those diagrams, but they actually don't make any sense. And actually, one of my favorite examples here is that it sometimes omits proofs. So it'll sometimes say, it'll sometimes say something like theorem, blah, blah, blah,
blah, blah, proof omitted. This thing kind of has gotten the gist of how some of these
math textbooks look like. We can have a lot of fun with this. So we also tried training
one of these models on the entire source
code of the Linux kernel. 'Cause again, this character level stuff that we can train on, And then, when we sample this, it acutally again looks
like C source code. It knows how to write if statements. It has, like, pretty good
code formatting skills. It knows to indent after
these if statements. It knows to put curly braces. It actually even makes
comments about some things that are usually nonsense. One problem with this model is that it knows how to declare variables. But it doesn't always use the
variables that it declares. And sometimes it tries to use variables that
haven't been declared. This wouldn't compile. I would not recommend sending this as a pull request to Linux. This thing also figures
out how to recite the GNU, this GNU license character by character. It kind of knows that you
need to recite the GNU license and after the license comes some includes, then some other includes,
then source code. This thing has actually
learned quite a lot about the general structure of the data. Where, again, during training, all we asked this model to do was try to predict the next
character in the sequence. We didn't tell it any of this structure, but somehow, just through the course of this training process, it learned a lot about
the latent structure in the sequential data. Yeah, so it knows how to write code. It does a lot of cool stuff. I had this paper with
Andre a couple years ago where we trained a bunch of these models and then we wanted to try
to poke into the brains of these models and figure out like what are they doing and why are they working. So we saw, in our, these recurring neural networks has this hidden vector which is, maybe, some vector that's
updated over every time step. And then what we wanted
to try to figure out is could we find some elements of this vector that have some Symantec
interpretable meaning. So what we did is we trained a neural
network language model, one of these character level models on one of these data sets, and then we picked one of the
elements in that hidden vector and now we look at what is the
value of that hidden vector over the course of a sequence to try to get some sense of maybe what these different hidden
states are looking for. When you do this, a lot
of them end up looking kind of like random gibberish garbage. So here again, what we've done, is we've picked one
element of that vector, and now we run the sequence forward through the trained model, and now the color of each character corresponds to the
magnitude of that single scaler element of the hidden
vector at every time step when it's reading the sequence. So you can see that a lot of the vectors in these hidden states are kind of not very interpretable. It seems like they're
kind of doing some of this low level language modeling to figure out what
character should come next. But some of them end up quite nice. So here we found this vector
that is looking for quotes. You can see that there's
this one hidden element, this one element in the vector, that is off, off, off, off, off blue and then once it hits a quote, it turns on and remains
on for the duration of this quote. And now when we hit the
second quotation mark, then that cell turns off. So somehow, even though
this model was only trained to predict the next
character in a sequence, it somehow learned that a useful thing, in order to do this, might be to have some cell
that's trying to detect quotes. We also found this other cell that is, looks like it's
counting the number of characters since a line break. So you can see that at the
beginning of each line, this element starts off at zero. Throughout the course of the line, it's gradually more red, so that value increases. And then after the new line character, it resets to zero. So you can imagine that maybe this cell is letting the network keep track of when it needs to write to produce these new line characters. We also found some that, when we trained on the linux source code, we found some examples that are turning on inside the conditions of if statements. So this maybe allows the network to differentiate whether
it's outside an if statement or inside that condition, which might help it model
these sequences better. We also found some that
turn on in comments, or some that seem like they're counting the number of indentation levels. This is all just really cool stuff because it's saying that even though we are only
trying to train this model to predict next characters, it somehow ends up learning a lot of useful structure about the input data. One kind of thing that we often use, so this is not really been
computer vision so far, and we need to pull this
back to computer vision since this is a vision class. We've alluded many times to this image captioning model where we want to build
models that can input an image and then output a
caption in natural language. There were a bunch of
papers a couple years ago that all had relatively
similar approaches. But I'm showing the figure
from the paper from our lab in a totally un-biased way. But, the idea here is that the caption is this variably length
sequence that we might, the sequence might have different numbers of words for different captions. So this is a totally natural fit for a recurrent neural
network language model. So then what this model looks like is we have some convolutional network which will input the, which will take as input the image, and we've seen a lot about how convolution networks work at this point, and that convolutional
network will produce a summary vector of the image which will then feed
into the first time step of one of these recurrent
neural network language models which will then produce words
of the caption one at a time. So the way that this kind
of works at test time after the model is trained looks almost exactly the same as these character level language models that we saw a little bit ago. We'll take our input image, feed it through our convolutional network. But now instead of
taking the softmax scores from an image net model, we'll instead take this
4,096 dimensional vector from the end of the model, and we'll take that vector and use it to summarize the whole
content of the image. Now, remember when we talked
about RNN language models, we said that we need to
see the language model with that first initial input to tell it to start generating text. So in this case, we'll give
it some special start token, which is just saying, hey, this
is the start of a sentence. Please start generating some text conditioned on this image information. So now previously, we saw that
in this RNN language model, we had these matrices that
were taking the previous, the input at the current time step and the hidden state of
the previous time step and combining those to
get the next hidden state. Well now, we also need to add
in this image information. So one way, people play
around with exactly different ways to incorporate
this image information, but one simple way is just to add a third weight matrix that is adding in this image
information at every time step to compute the next hidden state. So now, we'll compute this distribution over all scores in our vocabulary and here, our vocabulary is something like all English words, so it could be pretty large. We'll sample from that distribution and now pass that word back as
input at the next time step. And that will then feed that word in, again get a distribution
over all words in the vocab, and again sample to produce the next word. So then, after that thing is all done, we'll maybe generate, we'll generate this complete sentence. We stop generation once we
sample the special ends token, which kind of corresponds to the period at the end of the sentence. Then once the network
samples this ends token, we stop generation and we're done and we've gotten our
caption for this image. And now, during training, we trained this thing to generate, like we put an end token at the end of every caption during training so that the network kind
of learned during training that end tokens come at
the end of sequences. So then, during test time, it tends to sample these end tokens once it's done generating. So we trained this model in kind of a completely supervised way. You can find data sets that have images together with
natural language captions. Microsoft COCO is probably the biggest and most widely used for this task. But you can just train this model in a purely supervised way. And then backpropagate
through to jointly train both this recurrent neural
network language model and then also pass gradients back into this final layer of this the CNN and additionally update the weights of the CNN to jointly tune
all parts of the model to perform this task. Once you train these models, they actually do some
pretty reasonable things. These are some real results from a model, from one of these trained models, and it says things like a cat sitting on a suitcase on the floor, which is pretty impressive. It knows about cats
sitting on a tree branch, which is also pretty cool. It knows about two people walking on the beach with surfboards. So these models are
actually pretty powerful and can produce relatively
complex captions to describe the image. But that being said, these models are really not perfect. They're not magical. Just like any machine learning model, if you try to run them on data that was very different
from the training data, they don't work very well. So for example, this example, it says a woman is
holding a cat in her hand. There's clearly no cat in the image. But she is wearing a fur coat, and maybe the texture of that coat kind of looked like a cat to the model. Over here, we see a
woman standing on a beach holding a surfboard. Well, she's definitely
not holding a surfboard and she's doing a handstand, which is maybe the interesting
part of that image, and the model totally missed that. Also, over here, we see this example where there's this picture of a spider web in the tree branch, and it totally, and it says something like a bird sitting on a tree branch. So it totally missed the spider, but during training, it never really saw examples of spiders. It just knows that birds sit on tree branches during training. So it kind of makes these
reasonable mistakes. Or here at the bottom, it can't really tell the difference between this guy throwing
and catching the ball, but it does know that
it's a baseball player and there's balls and things involved. So again, just want to
say that these models are not perfect. They work pretty well when
you ask them to caption images that were similar to the training data, but they definitely have a hard time generalizing far beyond that. So another thing you'll sometimes see is this slightly more advanced
model called Attention, where now when we're generating
the words of this caption, we can allow the model
to steer it's attention to different parts of the image. And I don't want to spend
too much time on this. But the general way
that this works is that now our convolutional network, rather than producing a single vector summarizing the entire image, now it produces some grid of vectors that summarize the, that give maybe one vector
for each spatial location in the image. And now, when we, when this model runs forward, in addition to sampling the
vocabulary at every time step, it also produces a distribution over the locations in the image where it wants to look. And now this distribution
over image locations can be seen as a kind of a tension of where the model should
look during training. So now that first hidden state computes this distribution
over image locations, which then goes back to the set of vectors to give a single summary vector that maybe focuses the attention
on one part of that image. And now that summary vector gets fed, as an additional input, at the next time step
of the neural network. And now again, it will
produce two outputs. One is our distribution
over vocabulary words. And the other is a distribution
over image locations. This whole process will continue, and it will sort of do
these two different things at every time step. And after you train the model, then you can see that it kind of will shift it's attention around the image for every word that it
generates in the caption. Here you can see that it produced the caption,
a bird is flying over, I can't see that far. But you can see that its attention is shifting around
different parts of the image for each word in the
caption that it generates. There's this notion of hard attention versus soft attention, which I don't really want
to get into too much, but with this idea of soft attention, we're kind of taking
a weighted combination of all features from all image locations, whereas in the hard attention case, we're forcing the model to
select exactly one location to look at in the image at each time step. So the hard attention case where we're selecting
exactly one image location is a little bit tricky because that is not really
a differentiable function, so you need to do
something slightly fancier than vanilla backpropagation in order to just train the
model in that scenario. And I think we'll talk about
that a little bit later in the lecture on reinforcement learning. Now, when you look at after you train one of these attention models and then run it on to generate captions, you can see that it tends
to focus it's attention on maybe the salient or
semanticly meaningful part of the image when generating captions. You can see that the caption was a woman is throwing a frisbee in a park and you can see that this attention mask, when it generated the word, when the model generated the word frisbee, at the same time, it was focusing it's
attention on this image region that actually contains the frisbee. This is actually really cool. We did not tell the model
where it should be looking at every time step. It sort of figured all that out for itself during the training process. Because somehow, it
figured out that looking at that image region was
the right thing to do for this image. And because everything in
this model is differentiable, because we can backpropagate through all these soft attention steps, all of this soft attention stuff just comes out through
the training process. So that's really, really cool. By the way, this idea of
recurrent neural networks and attention actually
gets used in other tasks beyond image captioning. One recent example is this idea of visual question answering. So here, our model is going
to take two things as input. It's going to take an image and it will also take a
natural language question that's asking some
question about the image. Here, we might see this image on the left and we might ask the question, what endangered animal
is featured on the truck? And now the model needs to select from one of these four natural language answers about which of these answers
correctly answers that question in the context of the image. So you can imagine kind of
stitching this model together using CNNs and RNNs in
kind of a natural way. Now, we're in this many to one scenario, where now our model needs to take as input this natural language sequence, so we can imagine running
a recurrent neural network over each element of that input question, to now summarize the input
question in a single vector. And then we can have a CNN
to again summarize the image, and now combine both
the vector from the CNN and the vector from the
question and coding RNN to then predict a
distribution over answers. We also sometimes, you'll also sometimes see this idea of soft spacial attention
being incorporated into things like visual
question answering. So you can see that here, this model is also having
the spatial attention over the image when it's trying to determine answers to the questions. Just to, yeah, question? So the question is How are the different inputs combined? Do you mean like the
encoded question vector and the encoded image vector? Yeah, so the question is how are the encoded image and the encoded question vector combined? Kind of the simplest thing to do is just to concatenate them and stick them into
fully connected layers. That's probably the most common and that's probably
the first thing to try. Sometimes people do
slightly fancier things where they might try to have
multiplicative interactions between those two vectors to allow a more powerful function. But generally, concatenation
is kind of a good first thing to try. Okay, so now we've talked
about a bunch of scenarios where RNNs are used for
different kinds of problems. And I think it's super cool because it allows you to start tackling really complicated problems combining images and computer vision with natural language processing. And you can see that we
can kind of stith together these models like Lego blocks and attack really complicated things, Like image captioning or
visual question answering just by stitching together
these relatively simple types of neural network modules. But I'd also like to mention that so far, we've talked about this idea of a single recurrent network layer, where we have sort of one hidden state, and another thing that
you'll see pretty commonly is this idea of a multilayer
recurrent neural network. Here, this is a three layer
recurrent neural network, so now our input goes in, goes into, goes in and produces
a sequence of hidden states from the first recurrent
neural network layer. And now, after we run kind of one recurrent neural network layer, then we have this whole
sequence of hidden states. And now, we can use the
sequence of hidden states as an input sequence to another recurrent neural network layer. And then you can just imagine, which will then produce another
sequence of hidden states from the second RNN layer. And then you can just imagine stacking these things
on top of each other, cause we know that we've
seen in other contexts that deeper models tend to perform better for various problems. And the same kind of
holds in RNNs as well. For many problems, you'll see maybe a two or three layer recurrent
neural network model is pretty commonly used. You typically don't see
super deep models in RNNs. So generally, like two,
three, four layer RNNs is maybe as deep as you'll typically go. Then, I think it's also really
interesting and important to think about, now we've seen kind of what kinds of problems
these RNNs can be used for, but then you need to think
a little bit more carefully about exactly what happens to these models when we try to train them. So here, I've drawn this
little vanilla RNN cell that we've talked about so far. So here, we're taking
our current input, x t, and our previous hidden
state, h t minus one, and then we stack, those are two vectors. So we can just stack them together. And then perform this
matrix multiplication with our weight matrix, to give our, and then squash that
output through a tanh, and that will give us
our next hidden state. And that's kind of the
basic functional form of this vanilla recurrent neural network. But then, we need to think about what happens in this architecture during the backward pass when
we try to compute gradients? So then if we think
about trying to compute, so then during the backwards pass, we'll receive the derivative of our h t, we'll receive derivative of loss with respect to h t. And during the backward
pass through the cell, we'll need to compute derivative of loss to the respect of h t minus one. Then, when we compute this backward pass, we see that the gradient flows backward through this red path. So first, that gradient
will flow backwards through this tanh gate, and then it will flow backwards through this matrix multiplication gate. And then, as we've seen in the homework and when implementing these
matrix multiplication layers, when you backpropagate through this matrix multiplication gate, you end up mulitplying by the transpose of that weight matrix. So that means that every
time we backpropagate through one of these vanilla RNN cells, we end up multiplying by some
part of the weight matrix. So now if you imagine
that we are sticking many of these recurrent neural
network cells in sequence, because again this is an RNN. We want a model sequences. Now if you imagine what
happens to the gradient flow through a sequence of these layers, then something kind of
fishy starts to happen. Because now, when we want to compute the gradient of the loss
with respect to h zero, we need to backpropagate through every one of these RNN cells. And every time you
backpropagate through one cell, you'll pick up one of
these w transpose factors. So that means that the final expression for the gradient on h zero will involve many, many factors of this weight matrix, which could be kind of bad. Maybe don't think about the weight, the matrix case, but imagine a scaler case. If we end up, if we have some scaler and we multiply by that
same number over and over and over again, maybe not for four examples, but for something like a hundred or several hundred time steps, then multiplying by the same number over and over again is really bad. In the scaler case, it's either going to explode in the case that that
number is greater than one or it's going to vanish towards zero in the case that number is less than one in absolute value. And the only way in which
this will not happen is if that number is exactly one, which is actually very
rare to happen in practice. That leaves us to, that same intuition
extends to the matrix case, but now, rather than the absolute
value of a scaler number, you instead need to look at the largest, the largest singular value
of this weight matrix. Now if that largest singular
value is greater than one, then during this backward pass, when we multiply by the
weight matrix over and over, that gradient on h w, on h zero, sorry, will become very, very large, when that matrix is too large. And that's something we call
the exploding gradient problem. Where now this gradient will
explode exponentially in depth with the number of time steps that we backpropagate through. And if the largest singular
value is less than one, then we get the opposite problem, where now our gradients will shrink and shrink and shrink exponentially, as we backpropagate and pick
up more and more factors of this weight matrix. That's called the
vanishing gradient problem. THere's a bit of a hack
that people sometimes do to fix the exploding gradient problem called gradient clipping, which is just this simple heuristic saying that after we compute our gradient, if that gradient, if it's L2 norm is above some threshold, then just clamp it down and divide, just clamp it down so it
has this maximum threshold. This is kind of a nasty hack, but it actually gets used
in practice quite a lot when training recurrent neural networks. And it's a relatively useful tool for attacking this
exploding gradient problem. But now for the vanishing
gradient problem, what we typically do is we might need to move to a more complicated RNN architecture. So that motivates this idea of an LSTM. An LSTM, which stands for
Long Short Term Memory, is this slightly fancier
recurrence relation for these recurrent neural networks. It's really designed to help alleviate this problem of vanishing
and exploding gradients. So that rather than kind
of hacking on top of it, we just kind of design the architecture to have better gradient flow properties. Kind of an analogy to those
fancier CNN architectures that we saw at the top of the lecture. Another thing to point out is that the LSTM cell
actually comes from 1997. So this idea of an LSTM has been around for quite a while, and these folks were
working on these ideas way back in the 90s, were definitely ahead of the curve. Because these models are
kind of used everywhere now 20 years later. And LSTMs kind of have
this funny functional form. So remember when we had this vanilla recurrent neural network, it had this hidden state. And we used this recurrence relation to update the hidden
state at every time step. Well, now in an LSTM, we actually have two, we maintain two hidden
states at every time step. One is this h t, which is called the hidden state, which is kind of an
analogy to the hidden state that we had in the vanilla RNN. But an LSTM also maintains
the second vector, c t, called the cell state. And the cell state is this
vector which is kind of internal, kept inside the LSTM, and it does not really get
exposed to the outside world. And we'll see, and you can kind of see that
through this update equation, where you can see that when we, first when we compute these, we take our two inputs, we use them to compute these four gates called i, f, o, n, g. We use those gates to
update our cell states, c t, and then we expose part of our cell state as the hidden state at the next time step. This is kind of a funny functional form, and I want to walk through
for a couple slides exactly why do we use this architecture and why does it make sense, especially in the context of vanishing or exploding gradients. This first thing that we do in an LSTM is that we're given this
previous hidden state, h t, and we're given our
current input vector, x t, and just like the vanilla RNN. In the vanilla RNN, remember, we took those two input vectors. We concatenated them. Then we did a matrix multiply to directly compute the next
hidden state in the RNN. Now, the LSTM does something
a little bit different. We're going to take our
previous hidden state and our current input, stack them, and now multiply by a
very big weight matrix, w, to compute four different gates, Which all have the same
size as the hidden state. Sometimes, you'll see this
written in different ways. Some authors will write
a different weight matrix for each gate. Some authors will combine them all into one big weight matrix. But it's all really the same thing. The ideas is that we
take our hidden state, our current input, and then we use those to
compute these four gates. These four gates are the, you often see this written
as i, f, o, g, ifog, which makes it pretty easy
to remember what they are. I is the input gate. It says how much do we want
to input into our cell. F is the forget gate. How much do we want to
forget the cell memory at the previous, from
the previous time step. O is the output gate, which is how much do we
want to reveal ourself to the outside world. And G really doesn't have a nice name, so I usually call it the gate gate. G, it tells us how much
do we want to write into our input cell. And then you notice that
each of these four gates are using a different non linearity. The input, forget and output gate are all using sigmoids, which means that their values
will be between zero and one. Whereas the gate gate uses a tanh, which means it's output will
be between minus one and one. So, these are kind of weird, but it makes a little bit more sense if you imagine them all as binary values. Right, like what happens at the extremes of these two values? It's kind of what happens, if you look after we compute these gates if you look at this next equation, you can see that our cell state is being multiplied element
wise by the forget gate. Sorry, our cell state from
the previous time step is being multiplied element
wise by this forget gate. And now if this forget gate, you can think of it as being
a vector of zeros and ones, that's telling us for each
element in the cell state, do we want to forget
that element of the cell in the case if the forget gate was zero? Or do we want to remember
that element of the cell in the case if the forget gate was one. Now, once we've used the forget gate to gate off the part of the cell state, then we have the second term, which is the element
wise product of i and g. So now, i is this vector
of zeros and ones, cause it's coming through a sigmoid, telling us for each
element of the cell state, do we want to write to that
element of the cell state in the case that i is one, or do we not want to write to
that element of the cell state at this time step in the case that i is zero. And now the gate gate, because it's coming through a tanh, will be either one or minus one. So that is the value that we want, the candidate value that
we might consider writing to each element of the cell
state at this time step. Then if you look at the
cell state equation, you can see that at every time step, the cell state has these kind of these different,
independent scaler values, and they're all being incremented
or decremented by one. So there's kind of like, inside the cell state,
we can either remember or forget our previous state, and then we can either
increment or decrement each element of that cell state by up to one at each time step. So you can kind of think of
these elements of the cell state as being little scaler integer counters that can be incremented and decremented at each time step. And now, after we've
computed our cell state, then we use our now updated cell state to compute a hidden state, which we will reveal to the outside world. So because this cell state
has this interpretation of being counters, and sort of counting up by one or minus one at each time step, we want to squash that counter value into a nice zero to
one range using a tanh. And now, we multiply element wise, by this output gate. And the output gate is again
coming through a sigmoid, so you can think of it as
being mostly zeros and ones, and the output gate tells us for each element of our cell state, do we want to reveal or not reveal that element of our cell state when we're computing the
external hidden state for this time step. And then, I think there's
kind of a tradition in people trying to explain LSTMs, that everyone needs to come up with their own potentially
confusing LSTM diagram. So here's my attempt. Here, we can see what's going
on inside this LSTM cell, is that we take our, we're taking as input on the
left our previous cell state and the previous hidden state, as well as our current input, x t. Now we're going to take our current, our previous hidden state, as well as our current input, stack them, and then multiply with
this weight matrix, w, to produce our four gates. And here, I've left
out the non linearities because we saw those on a previous slide. And now the forget gate
multiplies element wise with the cell state. The input and gate gate
are multiplied element wise and added to the cell state. And that gives us our next cell. The next cell gets
squashed through a tanh, and multiplied element
wise with this output gate to produce our next hidden state. Question? No, So they're coming through this, they're coming from different
parts of this weight matrix. So if our hidden, if our x and our h all
have this dimension h, then after we stack them, they'll be a vector size two h, and now our weight matrix
will be this matrix of size four h times two h. So you can think of that as sort of having four chunks of this weight matrix. And each of these four
chunks of the weight matrix is going to compute a
different one of these gates. You'll often see this written for clarity, kind of combining all
four of those different weight matrices into a
single large matrix, w, just for notational convenience. But they're all computed using different parts
of the weight matrix. But you're correct in
that they're all computed using the same functional form of just stacking the two things and taking the matrix multiplication. Now that we have this picture, we can think about what
happens to an LSTM cell during the backwards pass? We saw, in the context of vanilla recurrent neural network, that some bad things happened
during the backwards pass, where we were continually multiplying by that weight matrix, w. But now, the situation looks much, quite a bit different in the LSTM. If you imagine this path backwards of computing the gradients
of the cell state, we get quite a nice picture. Now, when we have our upstream gradient from the cell coming in, then once we backpropagate backwards through this addition operation, remember that this addition just copies that upstream gradient
into the two branches, so our upstream gradient
gets copied directly and passed directly to backpropagating through this element wise multiply. So then our upstream
gradient ends up getting multiplied element wise
by the forget gate. As we backpropagate backwards
through this cell state, the only thing that happens to our upstream cell state gradient is that it ends up getting
multiplied element wise by the forget gate. This is really a lot nicer than the vanilla RNN for two reasons. One is that this forget gate is now an element wise multiplication rather than a full matrix multiplication. So element wise multiplication is going to be a little bit nicer than full matrix multiplication. Second is that element wise multiplication will potentially be
multiplying by a different forget gate at every time step. So remember, in the vanilla RNN, we were continually multiplying
by that same weight matrix over and over again, which led very explicitly to these exploding or vanishing gradients. But now in the LSTM case, this forget gate can
vary from each time step. Now, it's much easier for the model to avoid these problems of exploding and vanishing gradients. Finally, because this forget gate is coming out from a sigmoid, this element wise multiply is guaranteed to be between zero and one, which again, leads to sort
of nicer numerical properties if you imagine multiplying by these things over and over again. Another thing to notice
is that in the context of the vanilla recurrent neural network, we saw that during the backward pass, our gradients were flowing
through also a tanh at every time step. But now, in an LSTM, our outputs are, in an LSTM, our hidden state is used to compute those outputs, y t, so now, each hidden state, if you imagine backpropagating
from the final hidden state back to the first cell state, then through that backward path, we only backpropagate through
a single tanh non linearity rather than through a separate
tanh at every time step. So kind of when you put
all these things together, you can see this backwards pass backpropagating through the cell state is kind of a gradient super highway that lets gradients pass
relatively unimpeded from the loss at the very end of the model all the way back to the initial cell state at the beginning of the model. Was there a question? Yeah, what about the
gradient in respect to w? 'Cause that's ultimately the
thing that we care about. So, the gradient with respect to w will come through, at every time step, will take our current cell state as well as our current hidden state and that will give us an element, that will give us our local gradient on w for that time step. So because our cell state, and just in the vanilla RNN case, we'll end up adding those
first time step w gradients to compute our final gradient on w. But now, if you imagine the situation where we have a very long sequence, and we're only getting
gradients to the very end of the sequence. Now, as you backpropagate through, we'll get a local gradient on w for each time step, and that local gradient on w will be coming through
these gradients on c and h. So because we're maintaining
the gradients on c much more nicely in the LSTM case, those local gradients
on w at each time step will also be carried forward and backward through time much more cleanly. Another question? Yeah, so the question is due to the non linearities, could this still be susceptible
to vanishing gradients? And that could be the case. Actually, so one problem you might imagine is that maybe if these forget gates are always less than zero, or always less than one, you might get vanishing gradients as you continually go
through these forget gates. Well, one sort of trick
that people do in practice is that they will, sometimes, initialize the biases of the forget gate to be somewhat positive. So that at the beginning of training, those forget gates are
always very close to one. So that at least at the
beginning of training, then we have not so,
relatively clean gradient flow through these forget gates, since they're all
initialized to be near one. And then throughout
the course of training, then the model can learn those biases and kind of learn to
forget where it needs to. You're right that there
still could be some potential for vanishing gradients here. But it's much less extreme than the vanilla RNN case, both because those fs can
vary at each time step, and also because we're doing this element wise multiplication rather than a full matrix multiplication. So you can see that this LSTM actually looks quite similar to ResNet. In this residual network, we had this path of identity connections going backward through the network and that gave, sort of
a gradient super highway for gradients to flow backward in ResNet. And now it's kind of the
same intuition in LSTM where these additive and element wise multiplicative interactions
of the cell state can give a similar gradient super highway for gradients to flow backwards
through the cell state in an LSTM. And by the way, there's this
other kind of nice paper called highway networks, which is kind of in between this idea of this LSTM cell and these residual networks. So these highway networks actually came before residual networks, and they had this idea where at every layer of the highway network, we're going to compute sort of a candidate activation, as well as a gating function that tells us that interprelates between our previous input at that layer, and that candidate activation that came through our
convolutions or what not. So there's actually a lot of
architectural similarities between these things, and people take a lot of inspiration from training very deep CNNs and very deep RNNs and there's a lot of crossover here. Very briefly, you'll see a
lot of other types of variance of recurrent neural network
architectures out there in the wild. Probably the most common,
apart from the LSTM, is this GRU, called the
gated recurrent unit. And you can see those
update equations here, and it kind of has this
similar flavor of the LSTM, where it uses these
multiplicative element wise gates together with these additive interactions to avoid this vanishing gradient problem. There's also this cool paper called LSTM: a search based oddysey, very inventive title, where they tried to play
around with the LSTM equations and swap out the non
linearities at one point, like do we really need that tanh for exposing the output gate, and they tried to answer a lot
of these different questions about each of those non linearities, each of those pieces of
the LSTM update equations. What happens if we change the model and tweak those LSTM
equations a little bit. And kind of the conclusion is that they all work about the same Some of them work a little
bit better than others for one problem or another. But generally, none of the things, none of the tweaks of LSTM that they tried were significantly better
that the original LSTM for all problems. So that gives you a little bit more faith that the LSTM update
equations seem kind of magical but they're useful anyway. You should probably consider
them for your problem. There's also this cool paper
from Google a couple years ago where they tried to use, where they did kind of
an evolutionary search and did a search over many, over a very large number of
random RNN architectures, they kind of randomly premute
these update equations and try putting the additions
and the multiplications and the gates and the non linearities in different kinds of combinations. They blasted this out over
their huge Google cluster and just tried a whole bunch of these different weigh
updates in various flavors. And again, it was the same story that they didn't really find anything that was significantly better than these existing GRU or LSTM styles. Although there were some
variations that worked maybe slightly better or
worse for certain problems. But kind of the take away is that probably and using an LSTM or GRU is not so much magic in those equations, but this idea of managing
gradient flow properly through these additive connections and these multiplicative gates is super useful. So yeah, the summary is
that RNNs are super cool. They can allow you to attack
tons of new types of problems. They sometimes are
susceptible to vanishing or exploding gradients. But we can address that
with weight clipping and with fancier architectures. And there's a lot of cool overlap between CNN architectures
and RNN architectures. So next time, you'll
be taking the midterm. But after that, we'll
have a, sorry, a question? Midterm is after this lecture so anything up to this point is fair game. And so you guys, good luck
on the midterm on Tuesday. 

- Hello, hi. So I want to get started. Welcome to CS 231N Lecture 11. We're going to talk about
today detection segmentation and a whole bunch of other
really exciting topics around core computer vision tasks. But as usual, a couple
administrative notes. So last time you obviously
took the midterm, we didn't have lecture,
hopefully that went okay for all of you but so we're
going to work on grading the midterm this week, but as a reminder please don't make any public discussions about the midterm questions
or answers or whatever until at least tomorrow
because there are still some people taking makeup midterms today and throughout the rest of the week so we just ask you that
you refrain from talking publicly about midterm questions. Why don't you wait until Monday? [laughing] Okay, great. So we're also starting to
work on midterm grading. We'll get those back to
you as soon as you can, as soon as we can. We're also starting to work
on grading assignment two so there's a lot of grading
being done this week. The TA's are pretty busy. Also a reminder for you guys,
hopefully you've been working hard on your projects now that most of you are done with the midterm
so your project milestones will be due on Tuesday so
any sort of last minute changes that you had in your projects, I know some people
decided to switch projects after the proposal, some
teams reshuffled a little bit, that's fine but your
milestone should reflect the project that you're actually doing for the rest of the quarter. So hopefully that's going out well. I know there's been a
lot of worry and stress on Piazza, wondering
about assignment three. So we're working on that as hard as we can but that's actually a
bit of a new assignment, it's changing a bit from last year so it will be out as soon as possible, hopefully today or tomorrow. Although we promise that
whenever it comes out you'll have two weeks to finish it so try not to stress
out about that too much. But I'm pretty excited,
I think assignment three will be really cool, has a lot of cool, it'll cover a lot of really cool material. So another thing, last time in lecture we mentioned this thing
called the Train Game which is this really cool
thing we've been working on sort of as a side project a little bit. So this is an interactive
tool that you guys can go on and use to explore a
little bit the process of tuning hyperparameters
in practice so we hope that, so this is again totally
not required for the course. Totally optional, but
if you do we will offer a small amount of extra
credit for those of you who want to do well and
participate on this. And we'll send out
exactly some more details later this afternoon on Piazza. But just a bit of a demo for
what exactly is this thing. So you'll get to go in
and we've changed the name from Train Game to HyperQuest
because you're questing to solve, to find the best
hyperparameters for your model so this is really cool,
it'll be an interactive tool that you can use to explore
the training of hyperparameters interactively in your browser. So you'll login with
your student ID and name. You'll fill out a little survey with some of your experience on deep learning then you'll read some instructions. So in this game you'll be
shown some random data set on every trial. This data set might be
images or it might be vectors and your goal is to
train a model by picking the right hyperparameters
interactively to perform as well as you can on the validation set of this random data set. And it'll sort of keep
track of your performance over time and there'll be a leaderboard, it'll be really cool. So every time you play the game, you'll get some statistics
about your data set. In this case we're doing
a classification problem with 10 classes. You can see down at the bottom
you have these statistics about random data set, we have 10 classes. The input data size is three by 32 by 32 so this is some image
data set and we can see that in this case we have 8500 examples in the training set and 1500
examples in the validation set. These are all random, they'll change a little bit every time. Based on these data set statistics
you'll make some choices on your initial learning rate,
your initial network size, and your initial dropout rate. Then you'll see a screen
like this where it'll run one epoch with those
chosen hyperparameters, show you on the right
here you'll see two plots. One is your training and validation loss for that first epoch. Then you'll see your training
and validation accuracy for that first epoch and
based on the gaps that you see in these two graphs you can
make choices interactively to change the learning
rates and hyperparameters for the next epoch. So then you can either
choose to continue training with the current or
changed hyperparameters, you can also stop training,
or you can revert to go back to the previous checkpoint in case things got really messed up. So then you'll get to make some choice, so here we'll decide to continue training and in this case you could
go and set new learning rates and new hyperparameters for
the next epoch of training. You can also, kind of interesting here, you can actually grow
the network interactively during training in this demo. There's this cool trick
from a couple recent papers where you can either take existing layers and make them wider or add
new layers to the network in the middle of training
while still maintaining the same function in the
network so you can do that to increase the size of
your network in the middle of training here which is kind of cool. So then you'll make
choices over several epochs and eventually your
final validation accuracy will be recorded and we'll
have some leaderboard that compares your score on that data set to some simple baseline models. And depending on how well
you do on this leaderboard we'll again offer some small
amounts of extra credit for those of you who
choose to participate. So this is again, totally
optional, but I think it can be a really cool
learning experience for you guys to play around with and
explore how hyperparameters affect the learning process. Also, it's really useful for us. You'll help science out by
participating in this experiment. We're pretty interested in
seeing how people behave when they train neural networks
so you'll be helping us out as well if you decide to play this. But again, totally optional, up to you. Any questions on that? Hopefully at some point but it's. So the question was will this be a paper or whatever eventually? Hopefully but it's really
early stages of this project so I can't make any
promises but I hope so. But I think it'll be really cool. [laughing] Yeah, so the question is
how can you add layers during training? I don't really want to
get into that right now but the paper to read is
Net2Net by Ian Goodfellow's one of the authors and
there's another paper from Microsoft called Network Morphism. So if you read those two papers
you can see how this works. Okay, so last time, a bit of a reminder before we had the midterm
last time we talked about recurrent neural networks. We saw that recurrent
neural networks can be used for different types of problems. In addition to one to one
we can do one to many, many to one, many to many. We saw how this can apply
to language modeling and we saw some cool examples
of applying neural networks to model different sorts of
languages at the character level and we sampled these
artificial math and Shakespeare and C source code. We also saw how similar
things could be applied to image captioning by connecting
a CNN feature extractor together with an RNN language model. And we saw some really
cool examples of that. We also talked about the
different types of RNN's. We talked about this Vanilla RNN. I also want to mention that
this is sometimes called a Simple RNN or an Elman RNN so you'll see all of these different
terms in literature. We also talked about the Long
Short Term Memory or LSTM. And we talked about how the gradient, the LSTM has this crazy set of equations but it makes sense because it
helps improve gradient flow during back propagation
and helps this thing model more longer term dependencies
in our sequences. So today we're going to
switch gears and talk about a whole bunch of different exciting tasks. We're going to talk about, so
so far we've been talking about mostly the image classification problem. Today we're going to talk
about various types of other computer vision tasks where
you actually want to go in and say things about the spatial
pixels inside your images so we'll see segmentation,
localization, detection, a couple other different
computer vision tasks and how you can approach these with convolutional neural networks. So as a bit of refresher,
so far the main thing we've been talking about in this class is image classification so
here we're going to have some input image come in. That input image will go through some deep convolutional network, that network will give
us some feature vector of maybe 4096 dimensions
in the case of AlexNet RGB and then from that final feature vector we'll have some fully-connected, some final fully-connected layer that gives us 1000 numbers
for the different class scores that we care about where
1000 is maybe the number of classes in ImageNet in this example. And then at the end of the day what the network does is we input an image and then we output a single category label saying what is the content of
this entire image as a whole. But this is maybe the
most basic possible task in computer vision and
there's a whole bunch of other interesting types of tasks that we might want to
solve using deep learning. So today we're going to talk about several of these different tasks and
step through each of these and see how they all
work with deep learning. So we'll talk about these more in detail about what each problem is as we get to it but this is kind of a summary slide that we'll talk first about
semantic segmentation. We'll talk about classification
and localization, then we'll talk about object detection, and finally a couple brief words about instance segmentation. So first is the problem
of semantic segmentation. In the problem of semantic segmentation, we want to input an image
and then output a decision of a category for every
pixel in that image so for every pixel in this, so
this input image for example is this cat walking through
the field, he's very cute. And in the output we want to say for every pixel is that pixel
a cat or grass or sky or trees or background or some
other set of categories. So we're going to have
some set of categories just like we did in the
image classification case but now rather than
assigning a single category labeled to the entire
image, we want to produce a category label for each
pixel of the input image. And this is called semantic segmentation. So one interesting thing
about semantic segmentation is that it does not
differentiate instances so in this example on the
right we have this image with two cows where
they're standing right next to each other and when
we're talking about semantic segmentation we're just
labeling all the pixels independently for what is
the category of that pixel. So in the case like this
where we have two cows right next to each other
the output does not make any distinguishing, does not distinguish between these two cows. Instead we just get a whole mass of pixels that are all labeled as cow. So this is a bit of a shortcoming
of semantic segmentation and we'll see how we can fix this later when we move to instance segmentation. But at least for now we'll just talk about semantic segmentation first. So you can imagine maybe using a class, so one potential approach for attacking semantic segmentation might
be through classification. So there's this, you could use this idea of a sliding window approach
to semantic segmentation. So you might imagine that
we take our input image and we break it up into many
many small, tiny local crops of the image so in this
example we've taken maybe three crops from
around the head of this cow and then you could imagine
taking each of those crops and now treating this as
a classification problem. Saying for this crop, what is the category of the central pixel of the crop? And then we could use
all the same machinery that we've developed for
classifying entire images but now just apply it on crops rather than on the entire image. And this would probably
work to some extent but it's probably not a very good idea. So this would end up being super super computationally expensive
because we want to label every pixel in the image,
we would need a separate crop for every pixel in
that image and this would be super super expensive to
run forward and backward passes through. And moreover, we're actually,
if you think about this we can actually share
computation between different patches so if you're trying
to classify two patches that are right next to each
other and actually overlap then the convolutional
features of those patches will end up going through
the same convolutional layers and we can actually share
a lot of the computation when applying this to separate passes or when applying this type of approach to separate patches in the image. So this is actually a terrible
idea and nobody does this and you should probably not do this but it's at least the first
thing you might think of if you were trying to think
about semantic segmentation. Then the next idea that works a bit better is this idea of a fully
convolutional network right. So rather than extracting
individual patches from the image and classifying these
patches independently, we can imagine just having
our network be a whole giant stack of convolutional layers
with no fully connected layers or anything so in this
case we just have a bunch of convolutional layers that
are all maybe three by three with zero padding or something like that so that each convolutional
layer preserves the spatial size of the input and now if we pass our image through a whole stack of
these convolutional layers, then the final convolutional
layer could just output a tensor of something by C by H by W where C is the number of
categories that we care about and you could see this
tensor as just giving our classification scores for every pixel in the input image at every
location in the input image. And we could compute this all at once with just some giant stack
of convolutional layers. And then you could imagine
training this thing by putting a classification
loss at every pixel of this output, taking an
average over those pixels in space, and just training
this kind of network through normal, regular back propagation. Question? Oh, the question is how do you develop training data for this? It's very expensive right. So the training data for this would be we need to label every
pixel in those input images so there's tools that
people sometimes have online where you can go in and
sort of draw contours around the objects and
then fill in regions but in general getting
this kind of training data is very expensive. Yeah, the question is
what is the loss function? So here since we're making
a classification decision per pixel then we put a cross entropy loss on every pixel of the output. So we have the ground truth category label for every pixel in the output, then we compute across entropy loss between every pixel in the output and the ground truth pixels and then take either a sum or an average over space and then sum or average
over the mini-batch. Question? Yeah, yeah. Yeah, the question is do we assume that we know the categories? So yes, we do assume that we
know the categories up front so this is just like the
image classification case. So an image classification we
know at the start of training based on our data set that
maybe there's 10 or 20 or 100 or 1000 classes that we care about for this data set and
then here we are fixed to that set of classes that
are fixed for the data set. So this model is relatively simple and you can imagine this
working reasonably well assuming that you tuned all
the hyperparameters right but it's kind of a problem right. So in this setup, since
we're applying a bunch of convolutions that
are all keeping the same spatial size of the input image, this would be super super expensive right. If you wanted to do
convolutions that maybe have 64 or 128 or 256 channels for
those convolutional filters which is pretty common in
a lot of these networks, then running those convolutions
on this high resolution input image over a
sequence of layers would be extremely computationally expensive and would take a ton of memory. So in practice, you don't
usually see networks with this architecture. Instead you tend to see
networks that look something like this where we have some downsampling and then some upsampling
of the feature map inside the image. So rather than doing all the convolutions of the full spatial
resolution of the image, we'll maybe go through a small number of convolutional layers
at the original resolution then downsample that
feature map using something like max pooling or strided convolutions and sort of downsample, downsample, so we have convolutions in downsampling and convolutions in downsampling that look much like a lot of
the classification networks that you see but now
the difference is that rather than transitioning
to a fully connected layer like you might do in an
image classification setup, instead we want to increase
the spatial resolution of our predictions in the
second half of the network so that our output image
can now be the same size as our input image and this ends up being much more computationally efficient because you can make the network very deep and work at a lower spatial resolution for many of the layers at
the inside of the network. So we've already seen
examples of downsampling when it comes to convolutional networks. We've seen that you can
do strided convolutions or various types of pooling
to reduce the spatial size of the image inside a
network but we haven't really talked about
upsampling and the question you might be wondering is
what are these upsampling layers actually look
like inside the network? And what are our strategies
for increasing the size of a feature map inside the network? Sorry, was there a question in the back? Yeah, so the question
is how do we upsample? And the answer is that's the topic of the next couple slides. [laughing] So one strategy for
upsampling is something like unpooling so we have
this notion of pooling to downsample so we talked
about average pooling or max pooling so when we
talked about average pooling we're kind of taking a spatial average within a receptive field
of each pooling region. One kind of analog for
upsampling is this idea of nearest neighbor unpooling. So here on the left we see this example of nearest neighbor
unpooling where our input is maybe some two by
two grid and our output is a four by four grid
and now in our output we've done a two by two
stride two nearest neighbor unpooling or upsampling
where we've just duplicated that element for every
point in our two by two receptive field of the unpooling region. Another thing you might see
is this bed of nails unpooling or bed of nails upsampling
where you'll just take, again we have a two by two receptive field for our unpooling regions
and then you'll take the, in this case you make it all
zeros except for one element of the unpooling region so
in this case we've taken all of our inputs and
always put them in the upper left hand corner of this unpooling region and everything else is zeros. And this is kind of like a bed of nails because the zeros are very flat, then you've got these things poking up for the values at these
various non-zero regions. Another thing that you see
sometimes which was alluded to by the question a minute ago
is this idea of max unpooling so in a lot of these networks
they tend to be symmetrical where we have a downsampling
portion of the network and then an upsampling
portion of the network with a symmetry between those
two portions of the network. So sometimes what you'll see
is this idea of max unpooling where for each unpooling,
for each upsampling layer, it is associated with
one of the pooling layers in the first half of the network
and now in the first half, in the downsampling when we do max pooling we'll actually remember which
element of the receptive field during max pooling was
used to do the max pooling and now when we go through
the rest of the network then we'll do something that
looks like this bed of nails upsampling except rather than
always putting the elements in the same position,
instead we'll stick it into the position that was
used in the corresponding max pooling step earlier in the network. I'm not sure if that explanation was clear but hopefully the picture makes sense. Yeah, so then you just end up
filling the rest with zeros. So then you fill the rest with zeros and then you stick the elements
from the low resolution patch up into the high resolution patch at the points where the
max pooling took place at the corresponding max pooling there. Okay, so that's kind
of an interesting idea. Sorry, question? Oh yeah, so the question
is why is this a good idea? Why might this matter? So the idea is that when we're
doing semantic segmentation we want our predictions
to be pixel perfect right. We kind of want to get
those sharp boundaries and those tiny details in
our predictive segmentation so now if you're doing this max pooling, there's this sort of
heterogeneity that's happening inside the feature map
due to the max pooling where from the low resolution
image you don't know, you're sort of losing spatial
information in some sense by you don't know where that
feature vector came from in the local receptive
field after max pooling. So if you actually unpool
by putting the vector in the same slot you might
think that that might help us handle these fine details
a little bit better and help us preserve some
of that spatial information that was lost during max pooling. Question? The question is does this make
things easier for back prop? Yeah, I guess, I don't think
it changes the back prop dynamics too much because
storing these indices is not a huge computational overhead. They're pretty small in
comparison to everything else. So another thing that you'll see sometimes is this idea of transpose convolution. So transpose convolution,
so for these various types of unpooling that we just talked about, these bed of nails, this nearest neighbor, this max unpooling, all
of these are kind of a fixed function, they're
not really learning exactly how to do the upsampling so
if you think about something like strided convolution,
strided convolution is kind of like a learnable
layer that learns the way that the network wants
to perform downsampling at that layer. And by analogy with that
there's this type of layer called a transpose
convolution that lets us do kind of learnable upsampling. So it will both upsample the feature map and learn some weights about how it wants to do that upsampling. And this is really just
another type of convolution so to see how this works
remember how a normal three by three stride one pad
one convolution would work. That for this kind of normal convolution that we've seen many
times now in this class, our input might by four by four, our output might be four by four, and now we'll have this
three by three kernel and we'll take an inner product between, we'll plop down that kernel
at the corner of the image, take an inner product,
and that inner product will give us the value and the activation in the upper left hand
corner of our output. And we'll repeat this
for every receptive field in the image. Now if we talk about strided convolution then strided convolution ends
up looking pretty similar. However, our input is
maybe a four by four region and our output is a two by two region. But we still have this idea of taking, of there being some three
by three filter or kernel that we plop down in
the corner of the image, take an inner product
and use that to compute a value of the activation and the output. But now with strided
convolution the idea is that we're moving that, rather
than popping down that filter at every possible point in the input, instead we're going to move
the filter by two pixels in the input every time we
move the filter by one pixel, every time we move by
one pixel in the output. Right so this stride
of two gives us a ratio between how much do we move in the input versus how much do we move in the output. So when you do a strided
convolution with stride two this ends up downsampling
the image or the feature map by a factor of two in
kind of a learnable way. And now a transpose convolution
is sort of the opposite in a way so here our input
will be a two by two region and our output will be
a four by four region. But now the operation that we perform with transpose convolution
is a little bit different. Now so rather than taking an inner product instead what we're going
to do is we're going to take the value of our input feature map at that upper left hand
corner and that'll be some scalar value in the
upper left hand corner. We're going to multiply the
filter by that scalar value and then copy those values
over to this three by three region in the output so rather
than taking an inner product with our filter and the
input, instead our input gives weights that we will
use to weight the filter and then our output will be
weighted copies of the filter that are weighted by
the values in the input. And now we can do this
sort of same ratio trick in order to upsample so
now when we move one pixel in the input now we can
plop our filter down two pixels away in the output
and it's the same trick that now the blue pixel in
the input is some scalar value and we'll take that scalar value, multiply it by the values in the filter, and copy those weighted filter values into this new region in the output. The tricky part is that
sometimes these receptive fields in the output can overlap
now and now when these receptive fields in the output overlap we just sum the results in the output. So then you can imagine
repeating this everywhere and repeating this process everywhere and this ends up doing sort
of a learnable upsampling where we use these learned
convolutional filter weights to upsample the image and
increase the spatial size. By the way, you'll see this operation go by a lot of different names in literature. Sometimes this gets called
things like deconvolution which I think is kind of a
bad name but you'll see it out there in papers so from a
signal processing perspective deconvolution means the inverse
operation to convolution which this is not however
you'll frequently see this type of layer called a deconvolution layer in some deep learning
papers so be aware of that, watch out for that terminology. You'll also sometimes see
this called upconvolution which is kind of a cute name. Sometimes it gets called
fractionally strided convolution because if we think of the
stride as the ratio in step between the input and the output
then now this is something like a stride one half
convolution because of this ratio of one to two between steps in the input and steps in the output. This also sometimes gets
called a backwards strided convolution because if you think about it and work through the math
this ends up being the same, the forward pass of a
transpose convolution ends up being the same
mathematical operation as the backwards pass
in a normal convolution so you might have to take my word for it, that might not be super obvious
when you first look at this but that's kind of a neat
fact so you'll sometimes see that name as well. And as maybe a bit of
a more concrete example of what this looks like I think
it's maybe a little easier to see in one dimension so if we imagine, so here we're doing a three
by three transpose convolution in one dimension. Sorry, not three by three, a three by one transpose convolution in one dimension. So our filter here is just three numbers. Our input is two numbers
and now you can see that in our output we've
taken the values in the input, used them to weight the
values of the filter and plopped down those
weighted filters in the output with a stride of two and now
where these receptive fields overlap in the output then we sum. So you might be wondering,
this is kind of a funny name. Where does the name transpose
convolution come from and why is that actually my preferred name for this operation? So that comes from this kind of neat interpretation of convolution. So it turns out that any
time you do convolution you can always write convolution
as a matrix multiplication. So again, this is kind of easier to see with a one-dimensional example but here we've got some weight. So we're doing a
one-dimensional convolution of a weight vector x
which has three elements, and an input vector, a vector,
which has four elements, A, B, C, D. So here we're doing a
three by one convolution with stride one and you
can see that we can frame this whole operation as
a matrix multiplication where we take our convolutional kernel x and turn it into some matrix capital X which contains copies of
that convolutional kernel that are offset by different regions. And now we can take this
giant weight matrix X and do a matrix vector
multiplication between x and our input a and this
just produces the same result as convolution. And now with transpose convolution means that we're going to take
this same weight matrix but now we're going to
multiply by the transpose of that same weight matrix. So here you can see the same
example for this stride one convolution on the left and
the corresponding stride one transpose convolution on the right. And if you work through
the details you'll see that when it comes to stride one, a stride one transpose
convolution also ends up being a stride one normal convolution
so there's a little bit of details in the way that
the border and the padding are handled but it's
fundamentally the same operation. But now things look different when you talk about a stride of two. So again, here on the left
we can take a stride two convolution and write out
this stride two convolution as a matrix multiplication. And now the corresponding
transpose convolution is no longer a convolution so if you look through this weight matrix and think about how convolutions end up
getting represented in this way then now this transposed
matrix for the stride two convolution is something
fundamentally different from the original normal
convolution operation so that's kind of the
reasoning behind the name and that's why I think that's
kind of the nicest name to call this operation by. Sorry, was there a question? Sorry? It's very possible there's
a typo in the slide so please point out on
Piazza and I'll fix it but I hope the idea was clear. Is there another question? Okay, thank you [laughing]. Yeah, so, oh no lots of questions. Yeah, so the issue is why
do we sum and not average? So the reason we sum is due
to this transpose convolution formula zone so that's
the reason why we sum but you're right that you actually, this is kind of a problem
that the magnitudes will actually vary in the output depending on how many receptive
fields were in the output. So actually in practice this
is something that people started to point out very
recently and somewhat switched away from this
stride, so using three by three stride two transpose
convolution upsampling can sometimes produce some
checkerboard artifacts in the output exactly due to that problem. So what I've seen in a
couple more recent papers is maybe to use four by four stride two or two by two stride two
transpose convolution for upsampling and that helps alleviate that problem a little bit. Yeah, so the question is what
is a stride half convolution and where does that terminology come from? I think that was from my paper. So that was actually, yes
that was definitely this. So at the time I was writing that paper I was kind of into the name
fractionally strided convolution but after thinking about
it a bit more I think transpose convolution is
probably the right name. So then this idea of semantic segmentation actually ends up being pretty natural. You just have this giant
convolutional network with downsampling and
upsampling inside the network and now our downsampling will
be by strided convolution or pooling, our upsampling will
be by transpose convolution or various types of
unpooling or upsampling and we can train this
whole thing end to end with back propagation using
this cross entropy loss over every pixel. So this is actually pretty
cool that we can take a lot of the same machinery
that we already learned for image classification
and now just apply it very easily to extend
to new types of problems so that's super cool. So the next task that I want
to talk about is this idea of classification plus localization. So we've talked about
image classification a lot where we want to just
assign a category label to the input image but
sometimes you might want to know a little bit more about the image. In addition to predicting
what the category is, in this case the cat, you
might also want to know where is that object in the image? So in addition to predicting
the category label cat, you might also want to draw a bounding box around the region of
the cat in that image. And classification plus localization, the distinction here between
this and object detection is that in the localization
scenario you assume ahead of time that you know
there's exactly one object in the image that you're looking
for or maybe more than one but you know ahead of time
that we're going to make some classification
decision about this image and we're going to produce
exactly one bounding box that's going to tell us
where that object is located in the image so we
sometimes call that task classification plus localization. And again, we can reuse a
lot of the same machinery that we've already learned
from image classification in order to tackle this problem. So kind of a basic
architecture for this problem looks something like this. So again, we have our input image, we feed our input image through some giant convolutional network, this is Alex, this is AlexNet for
example, which will give us some final vector summarizing
the content of the image. Then just like before we'll
have some fully connected layer that goes from that final
vector to our class scores. But now we'll also have
another fully connected layer that goes from that
vector to four numbers. Where the four numbers are something like the height, the width,
and the x and y positions of that bounding box. And now our network will
produce these two different outputs, one is this set of class scores, and the other are these four
numbers giving the coordinates of the bounding box in the input image. And now during training time,
when we train this network we'll actually have two
losses so in this scenario we're sort of assuming a
fully supervised setting so we assume that each
of our training images is annotated with both a
category label and also a ground truth bounding box
for that category in the image. So now we have two loss functions. We have our favorite
softmax loss that we compute using the ground truth category label and the predicted class scores, and we also have some
kind of loss that gives us some measure of dissimilarity
between our predicted coordinates for the bounding box and our actual coordinates
for the bounding box. So one very simple thing
is to just take an L2 loss between those two and that's
kind of the simplest thing that you'll see in
practice although sometimes people play around with
this and maybe use L1 or smooth L1 or they
parametrize the bounding box a little bit differently but
the idea is always the same, that you have some regression loss between your predicted
bounding box coordinates and the ground truth
bounding box coordinates. Question? Sorry, go ahead. So the question is, is this a good idea to do all at the same time? Like what happens if you misclassify, should you even look
at the box coordinates? So sometimes people get fancy with it, so in general it works okay. It's not a big problem, you
can actually train a network to do both of these
things at the same time and it'll figure it out but
sometimes things can get tricky in terms of misclassification
so sometimes what you'll see for example is that rather
than predicting a single box you might make predictions
like a separate prediction of the box for each category
and then only apply loss to the predicted box corresponding to the ground truth category. So people do get a little
bit fancy with these things that sometimes helps a bit in practice. But at least this basic
setup, it might not be perfect or it might not be
optimal but it will work and it will do something. Was there a question in the back? Yeah, so that's the
question is do these losses have different units, do
they dominate the gradient? So this is what we call a multi-task loss so whenever we're taking
derivatives we always want to take derivative
of a scalar with respect to our network parameters
and use that derivative to take gradient steps. But now we've got two scalars
that we want to both minimize so what you tend to do in
practice is have some additional hyperparameter that
gives you some weighting between these two losses so
you'll take a weighted sum of these two different loss functions to give our final scalar loss. And then you'll take your
gradients with respect to this weighted sum of the two losses. And this ends up being
really really tricky because this weighting
parameter is a hyperparameter that you need to set but
it's kind of different from some of the other hyperparameters that we've seen so far in the past right because this weighting hyperparameter actually changes the
value of the loss function so one thing that you might often look at when you're trying to set hyperparameters is you might make different
hyperparameter choices and see what happens to the loss under different choices
of hyperparameters. But in this case because
the loss actually, because the hyperparameter
affects the absolute value of the loss making those
comparisons becomes kind of tricky. So setting that hyperparameter
is somewhat difficult. And in practice, you
kind of need to take it on a case by case basis
for exactly the problem you're solving but my
general strategy for this is to have some other
metric of performance that you care about other
than the actual loss value which then you actually use
that final performance metric to make your cross validation
choices rather than looking at the value of the loss
to make those choices. Question? So the question is why do
we do this all at once? Why not do this separately? Yeah, so the question is why
don't we fix the big network and then just only learn
separate fully connected layers for these two tasks? People do do that sometimes
and in fact that's probably the first thing you
should try if you're faced with a situation like this but in general whenever you're doing transfer learning you always get better
performance if you fine tune the whole system jointly
because there's probably some mismatch between the features, if you train on ImageNet and
then you use that network for your data set you're going
to get better performance on your data set if you can
also change the network. But one trick you might
see in practice sometimes is that you might freeze that network then train those two things
separately until convergence and then after they
converge then you go back and jointly fine tune the whole system. So that's a trick that sometimes people do in practice in that situation. And as I've kind of
alluded to this big network is often a pre-trained
network that is taken from ImageNet for example. So a bit of an aside,
this idea of predicting some fixed number of
positions in the image can be applied to a lot
of different problems beyond just classification
plus localization. One kind of cool example
is human pose estimation. So here we want to take an input image is a picture of a person. We want to output the
positions of the joints for that person and this
actually allows the network to predict what is the pose of the human. Where are his arms, where are
his legs, stuff like that, and generally most people have
the same number of joints. That's a bit of a simplifying assumption, it might not always be true
but it works for the network. So for example one
parameterization that you might see in some data sets is
define a person's pose by 14 joint positions. Their feet and their knees and their hips and something like that and
now when we train the network then we're going to input
this image of a person and now we're going to output
14 numbers in this case giving the x and y coordinates
for each of those 14 joints. And then you apply some
kind of regression loss on each of those 14
different predicted points and just train this network
with back propagation again. Yeah, so you might see an L2
loss but people play around with other regression losses here as well. Question? So the question is what do I mean when I say regression loss? So I mean something
other than cross entropy or softmax right. When I say regression loss I usually mean like an L2 Euclidean loss or an L1 loss or sometimes a smooth L1 loss. But in general classification
versus regression is whether your output is
categorical or continuous so if you're expecting
a categorical output like you ultimately want to
make a classification decision over some fixed number of categories then you'll think about
a cross entropy loss, softmax loss or these
SVM margin type losses that we talked about already in the class. But if your expected output is
to be some continuous value, in this case the position of these points, then your output is
continuous so you tend to use different types of losses
in those situations. Typically an L2, L1, different
kinds of things there. So sorry for not clarifying that earlier. But the bigger point
here is that for any time you know that you want
to make some fixed number of outputs from your network,
if you know for example. Maybe you knew that you wanted to, you knew that you always
are going to have pictures of a cat and a dog and
you want to predict both the bounding box of the cat
and the bounding box of the dog in that case you'd know
that you have a fixed number of outputs for each input
so you might imagine hooking up this type of regression classification plus localization framework for that problem as well. So this idea of some fixed
number of regression outputs can be applied to a lot
of different problems including pose estimation. So the next task that I want to
talk about is object detection and this is a really meaty topic. This is kind of a core
problem in computer vision and you could probably
teach a whole seminar class on just the history of object detection and various techniques applied there. So I'll be relatively
brief and try to go over the main big ideas of object
detection plus deep learning that have been used in
the last couple of years. But the idea in object detection is that we again start with some
fixed set of categories that we care about, maybe cats
and dogs and fish or whatever but some fixed set of categories
that we're interested in. And now our task is that
given our input image, every time one of those
categories appears in the image, we want to draw a box around
it and we want to predict the category of that
box so this is different from classification plus localization because there might be a
varying number of outputs for every input image. You don't know ahead of time
how many objects you expect to find in each image so that's, this ends up being a
pretty challenging problem. So we've seen graphs, so
this is kind of interesting. We've seen this graph
many times of the ImageNet classification performance
as a function of years and we saw that it just got
better and better every year and there's been a similar
trend with object detection because object detection
has again been one of these core problems in computer vision that people have cared
about for a very long time. So this slide is due to Ross Girshick who's worked on this
problem a lot and it shows the progression of object
detection performance on this one particular
data set called PASCAL VOC which has been relatively
used for a long time in the object detection community. And you can see that up until about 2012 performance on object
detection started to stagnate and slow down a little
bit and then in 2013 was when some of the first
deep learning approaches to object detection came
around and you could see that performance just shot up very quickly getting better and better year over year. One thing you might notice is
that this plot ends in 2015 and it's actually continued
to go up since then so the current state of
the art in this data set is well over 80 and in
fact a lot of recent papers don't even report results
on this data set anymore because it's considered too easy. So it's a little bit hard to know, I'm not actually sure what is
the state of the art number on this data set but it's
off the top of this plot. Sorry, did you have a question? Nevermind. Okay, so as I already
said this is different from localization because
there might be differing numbers of objects for each image. So for example in this
cat on the upper left there's only one object
so we only need to predict four numbers but now for
this image in the middle there's three animals there
so we need our network to predict 12 numbers, four coordinates for each bounding box. Or in this example of many
many ducks then you want your network to predict
a whole bunch of numbers. Again, four numbers for each duck. So this is quite different
from object detection. Sorry object detection is quite
different from localization because in object detection
you might have varying numbers of objects in the image and
you don't know ahead of time how many you expect to find. So as a result, it's kind of
tricky if you want to think of object detection as
a regression problem. So instead, people tend to
work, use kind of a different paradigm when thinking
about object detection. So one approach that's very
common and has been used for a long time in computer
vision is this idea of sliding window approaches
to object detection. So this is kind of similar to this idea of taking small patches and applying that for semantic segmentation and we can apply a similar idea for object detection. So the ideas is that
we'll take different crops from the input image, in
this case we've got this crop in the lower left hand corner of our image and now we take that crop, feed it through our convolutional network and our convolutional network does a classification decision
on that input crop. It'll say that there's no dog
here, there's no cat here, and then in addition to the
categories that we care about we'll add an additional
category called background and now our network can predict background in case it doesn't see
any of the categories that we care about, so
then when we take this crop from the lower left hand corner here then our network would
hopefully predict background and say that no, there's no object here. Now if we take a different
crop then our network would predict dog yes,
cat no, background no. We take a different crop we get dog yes, cat no, background no. Or a different crop, dog
no, cat yes, background no. Does anyone see a problem here? Yeah, the question is how
do you choose the crops? So this is a huge problem right. Because there could be any
number of objects in this image, these objects could appear
at any location in the image, these objects could appear
at any size in the image, these objects could also
appear at any aspect ratio in the image, so if you want
to do kind of a brute force sliding window approach
you'd end up having to test thousands, tens of thousands,
many many many many different crops in order
to tackle this problem with a brute force
sliding window approach. And in the case where
every one of those crops is going to be fed through a
giant convolutional network, this would be completely
computationally intractable. So in practice people don't
ever do this sort of brute force sliding window approach
for object detection using convolutional networks. Instead there's this cool line of work called region proposals that comes from, this is not using deep learning typically. These are slightly more
traditional computer vision techniques but the idea is
that a region proposal network kind of uses more traditional
signal processing, image processing type
things to make some list of proposals for where,
so given an input image, a region proposal network
will then give you something like a thousand boxes where
an object might be present. So you can imagine that
maybe we do some local, we look for edges in the
image and try to draw boxes that contain closed edges
or something like that. These various types of
image processing approaches, but these region proposal
networks will basically look for blobby regions in our
input image and then give us some set of candidate proposal regions where objects might be potentially found. And these are relatively fast-ish to run so one common example of
a region proposal method that you might see is something
called Selective Search which I think actually gives
you 2000 region proposals, not the 1000 that it says on the slide. So you kind of run this
thing and then after about two seconds of turning on your CPU it'll spit out 2000 region
proposals in the input image where objects are likely to be found so there'll be a lot of noise in those. Most of them will not be true objects but there's a pretty high recall. If there is an object in
the image then it does tend to get covered by these region proposals from Selective Search. So now rather than applying
our classification network to every possible location
and scale in the image instead what we can do is
first apply one of these region proposal networks to get some set of proposal regions where
objects are likely located and now apply a convolutional
network for classification to each of these proposal
regions and this will end up being much more computationally tractable than trying to do all
possible locations and scales. And this idea all came
together in this paper called R-CNN from a few years
ago that does exactly that. So given our input image in this case we'll run some region proposal network to get our proposals, these
are also sometimes called regions of interest or ROI's
so again Selective Search gives you something like
2000 regions of interest. Now one of the problems
here is that these input, these regions in the input
image could have different sizes but if we're going to run them all through a convolutional
network our classification, our convolutional networks
for classification all want images of the
same input size typically due to the fully connected
net layers and whatnot so we need to take each
of these region proposals and warp them to that fixed square size that is expected as input
to our downstream network. So we'll crop out those region proposal, those regions corresponding
to the region proposals, we'll warp them to that fixed size, and then we'll run each of them through a convolutional network which will then use in this case an SVM to make a classification
decision for each of those, to predict categories
for each of those crops. And then I lost a slide. But it'll also, not shown
in the slide right now but in addition R-CNN also
predicts a regression, like a correction to the bounding box in addition for each of
these input region proposals because the problem is that
your input region proposals are kind of generally in the
right position for an object but they might not be perfect
so in addition R-CNN will, in addition to category labels
for each of these proposals, it'll also predict four
numbers that are kind of an offset or a correction to
the box that was predicted at the region proposal stage. So then again, this is a multi-task loss and you would train this whole thing. Sorry was there a question? The question is how much does the change in aspect ratio impact accuracy? It's a little bit hard to say. I think there's some
controlled experiments in some of these papers but I'm not sure I can give a generic answer to that. Question? The question is is it necessary for regions of interest to be rectangles? So they typically are
because it's tough to warp these non-region things but once you move to something like instant segmentation then you sometimes get proposals
that are not rectangles. If you actually do care
about predicting things that are not rectangles. Is there another question? Yeah, so the question is are
the region proposals learned so in R-CNN it's a traditional thing. These are not learned, this is
kind of some fixed algorithm that someone wrote down but
we'll see in a couple minutes that we can actually, we've
changed that a little bit in the last couple of years. Is there another question? The question is is the
offset always inside the region of interest? The answer is no, it doesn't have to be. You might imagine that
suppose the region of interest put a box around a person
but missed the head then you could imagine
the network inferring that oh this is a person but
people usually have heads so the network showed the box
should be a little bit higher. So sometimes the final predicted boxes will be outside the region of interest. Question? Yeah. Yeah the question is
you have a lot of ROI's that don't correspond to true objects? And like we said, in
addition to the classes that you actually care
about you add an additional background class so your
class scores can also predict background to say
that there was no object here. Question? Yeah, so the question is
what kind of data do we need and yeah, this is fully
supervised in the sense that our training data has each
image, consists of images. Each image has all the
object categories marked with bounding boxes for each
instance of that category. There are definitely papers
that try to approach this like oh what if you don't have the data. What if you only have
that data for some images? Or what if that data is noisy but at least in the generic case you
assume full supervision of all objects in the
images at training time. Okay, so I think we've
kind of alluded to this but there's kind of a lot of problems with this R-CNN framework. And actually if you look at
the figure here on the right you can see that additional
bounding box head so I'll put it back. But this is kind of still
computationally pretty expensive because if we've got
2000 region proposals, we're running each of those
proposals independently, that can be pretty expensive. There's also this question
of relying on this fixed region proposal network,
this fixed region proposals, we're not learning them so
that's kind of a problem. And just in practice it
ends up being pretty slow so in the original implementation R-CNN would actually dump all
the features to disk so it'd take hundreds of
gigabytes of disk space to store all these features. Then training would be super
slow since you have to make all these different
forward and backward passes through the image and it
took something like 84 hours is one number they've
recorded for training time so this is super super slow. And now at test time it's also super slow, something like roughly 30
seconds minute per image because you need to run
thousands of forward passes through the convolutional network for each of these region proposals so this ends up being pretty slow. Thankfully we have fast
R-CNN that fixed a lot of these problems so when we do fast R-CNN then it's going to look kind of the same. We're going to start with our input image but now rather than processing
each region of interest separately instead we're
going to run the entire image through some convolutional
layers all at once to give this high resolution
convolutional feature map corresponding to the entire image. And now we still are using
some region proposals from some fixed thing
like Selective Search but rather than cropping
out the pixels of the image corresponding to the region proposals, instead we imagine projecting
those region proposals onto this convolutional feature map and then taking crops from
the convolutional feature map corresponding to each proposal rather than taking crops directly from the image. And this allows us to reuse
a lot of this expensive convolutional computation
across the entire image when we have many many crops per image. But again, if we have some
fully connected layers downstream those fully connected layers are expecting some fixed-size input so now we need to do some
reshaping of those crops from the convolutional feature map and they do that in a differentiable way using something they call
an ROI pooling layer. Once you have these warped crops from the convolutional feature map then you can run these things through some fully connected layers and
predict your classification scores and your linear regression offsets to the bounding boxes. And now when we train
this thing then we again have a multi-task loss that trades off between these two constraints
and during back propagation we can back prop through this entire thing and learn it all jointly. This ROI pooling, it looks
kind of like max pooling. I don't really want to get into the details of that right now. And in terms of speed if we
look at R-CNN versus fast R-CNN versus this other model called SPP net which is kind of in between the two, then you can see that at
training time fast R-CNN is something like 10 times faster to train because we're sharing all this computation between different feature maps. And now at test time
fast R-CNN is super fast and in fact fast R-CNN
is so fast at test time that its computation time
is actually dominated by computing region proposals. So we said that computing
these 2000 region proposals using Selective Search takes
something like two seconds and now once we've got
all these region proposals then because we're processing
them all sort of in a shared way by sharing these
expensive convolutions across the entire image that
we can process all of these region proposals in less
than a second altogether. So fast R-CNN ends up being bottlenecked by just the computing of
these region proposals. Thankfully we've solved this
problem with faster R-CNN. So the idea in faster
R-CNN is to just make, so the problem was the
computing the region proposals using this fixed function
was a bottleneck. So instead we'll just
make the network itself predict its own region proposals. And so the way that this
sort of works is that again, we take our input image,
run the entire input image altogether through some
convolutional layers to get some convolutional feature map representing the entire
high resolution image and now there's a separate
region proposal network which works on top of those
convolutional features and predicts its own region
proposals inside the network. Now once we have those
predicted region proposals then it looks just like fast R-CNN where now we take crops
from those region proposals from the convolutional features, pass them up to the rest of the network. And now we talked about multi-task losses and multi-task training networks to do multiple things at once. Well now we're telling the
network to do four things all at once so balancing out this four-way multi-task loss is kind of tricky. But because the region proposal network needs to do two things: it needs to say for each potential
proposal is it an object or not an object, it
needs to actually regress the bounding box coordinates
for each of those proposals, and now the final network at the end needs to do these two things again. Make final classification decisions for what are the class scores
for each of these proposals, and also have a second round
of bounding box regression to again correct any errors that may have come from the region proposal stage. Question? So the question is that
sometimes multi-task learning might be seen as regularization and are we getting that affect here? I'm not sure if there's been
super controlled studies on that but actually
in the original version of the faster R-CNN paper
they did a little bit of experimentation like what if we share the region proposal network,
what if we don't share? What if we learn separate
convolutional networks for the region proposal network versus the classification network? And I think there were minor differences but it wasn't a dramatic
difference either way. So in practice it's kind
of nicer to only learn one because it's computationally cheaper. Sorry, question? Yeah the question is how do you train this region proposal network
because you don't know, you don't have ground
truth region proposals for the region proposal network. So that's a little bit hairy. I don't want to get too
much into those details but the idea is that at any
time you have a region proposal which has more than some
threshold of overlap with any of the ground truth objects then you say that that is
the positive region proposal and you should predict
that as the region proposal and any potential proposal
which has very low overlap with any ground truth objects should be predicted as a negative. But there's a lot of dark
magic hyperparameters in that process and
that's a little bit hairy. Question? Yeah, so the question is what
is the classification loss on the region proposal
network and the answer is that it's making a binary,
so I didn't want to get into too much of the
details of that architecture 'cause it's a little bit hairy but it's making binary decisions. So it has some set of potential regions that it's considering and it's making a binary decision for each one. Is this an object or not an object? So it's like a binary classification loss. So once you train this
thing then faster R-CNN ends up being pretty darn fast. So now because we've
eliminated this overhead from computing region
proposals outside the network, now faster R-CNN ends
up being very very fast compared to these other alternatives. Also, one interesting thing
is that because we're learning the region proposals
here you might imagine maybe what if there was some mismatch between this fixed region
proposal algorithm and my data? So in this case once you're learning your own region proposals
then you can overcome that mismatch if your region proposals are somewhat weird or
different than other data sets. So this whole family of R-CNN methods, R stands for region, so these
are all region-based methods because there's some
kind of region proposal and then we're doing some processing, some independent processing for each of those potential regions. So this whole family of methods are called these region-based methods
for object detection. But there's another family of methods that you sometimes see
for object detection which is sort of all feed
forward in a single pass. So one of these is YOLO
for You Only Look Once. And another is SSD for
Single Shot Detection and these two came out
somewhat around the same time. But the idea is that rather
than doing independent processing for each of
these potential regions instead we want to try to treat this like a regression problem and just make all these predictions all at once with some big convolutional network. So now given our input image you imagine dividing that input image
into some coarse grid, in this case it's a seven by seven grid and now within each of those grid cells you imagine some set
of base bounding boxes. Here I've drawn three base bounding boxes like a tall one, a wide
one, and a square one but in practice you would
use more than three. So now for each of these grid cells and for each of these base bounding boxes you want to predict several things. One, you want to predict an
offset off the base bounding box to predict what is the true location of the object off this base bounding box. And you also want to predict
classification scores so maybe a classification score for each of these base bounding boxes. How likely is it that an
object of this category appears in this bounding box. So then at the end we end up predicting from our input image, we end up predicting this giant tensor of seven
by seven grid by 5B + C. So that's just where we
have B base bounding boxes, we have five numbers for
each giving our offset and our confidence for
that base bounding box and C classification scores
for our C categories. So then we kind of see object
detection as this input of an image, output of this
three dimensional tensor and you can imagine just
training this whole thing with a giant convolutional network. And that's kind of what
these single shot methods do where they just, and again
matching the ground truth objects into these potential base boxes becomes a little bit hairy but
that's what these methods do. And by the way, the
region proposal network that gets used in faster
R-CNN ends up looking quite similar to these
where they have some set of base bounding boxes
over some gridded image, another region proposal
network does some regression plus some classification. So there's kind of some
overlapping ideas here. So in faster R-CNN we're
kind of treating the object, the region proposal step
as kind of this fixed end-to-end regression problem
and then we do the separate per region processing but now
with these single shot methods we only do that first step and just do all of our object detection
with a single forward pass. So object detection has a
ton of different variables. There could be different
base networks like VGG, ResNet, we've seen
different metastrategies for object detection
including this faster R-CNN type region based family of methods, this single shot detection
family of methods. There's kind of a hybrid
that I didn't talk about called R-FCN which is somewhat in between. There's a lot of different hyperparameters like what is the image size, how many region proposals do you use. And there's actually
this really cool paper that will appear at CVPR this
summer that does a really controlled experimentation
around a lot of these different variables and tries to tell you how do these methods all perform under these different variables. So if you're interested I'd
encourage you to check it out but kind of one of the
key takeaways is that the faster R-CNN style
of region based methods tends to give higher
accuracies but ends up being much slower than the single shot methods because the single shot
methods don't require this per region processing. But I encourage you to
check out this paper if you want more details. Also as a bit of aside,
I had this fun paper with Andre a couple years ago that kind of combined object detection
with image captioning and did this problem
called dense captioning so now the idea is that
rather than predicting a fixed category label for each region, instead we want to write
a caption for each region. And again, we had some data
set that had this sort of data where we had a data set of
regions together with captions and then we sort of trained
this giant end-to-end model that just predicted these
captions all jointly. And this ends up looking
somewhat like faster R-CNN where you have some region proposal stage then a bounding box, then
some per region processing. But rather than a SVM or a softmax loss instead those per region
processing has a whole RNN language model that predicts
a caption for each region. So that ends up looking quite
a bit like faster R-CNN. There's a video here but I think we're running out of time so I'll skip it. But the idea here is
that once you have this, you can kind of tie together
a lot of these ideas and if you have some new
problem that you're interested in tackling like dense captioning, you can recycle a lot of the components that you've learned from other problems like object detection and image captioning and kind of stitch together
one end-to-end network that produces the outputs
that you care about for your problem. So the last task that I want to talk about is this idea of instance segmentation. So here instance segmentation is in some ways like the full problem We're given an input image
and we want to predict one, the locations and identities
of objects in that image similar to object detection,
but rather than just predicting a bounding box
for each of those objects, instead we want to predict
a whole segmentation mask for each of those objects
and predict which pixels in the input image corresponds
to each object instance. So this is kind of like a hybrid between semantic segmentation
and object detection because like object
detection we can handle multiple objects and we
differentiate the identities of different instances so in this example since there are two dogs in the image and instance segmentation method actually distinguishes
between the two dog instances and the output and kind of
like semantic segmentation we have this pixel wise accuracy where for each of these
objects we want to say which pixels belong to that object. So there's been a lot of different methods that people have tackled, for
instance segmentation as well, but the current state of
the art is this new paper called Mask R-CNN that
actually just came out on archive about a month ago
so this is not yet published, this is like super fresh stuff. And this ends up looking
a lot like faster R-CNN. So it has this multi-stage
processing approach where we take our whole input image, that whole input image goes
into some convolutional network and some learned
region proposal network that's exactly the same as faster R-CNN and now once we have our
learned region proposals then we project those proposals onto our convolutional feature map just like we did in fast and faster R-CNN. But now rather than just
making a classification and a bounding box for regression decision for each of those boxes we in addition want to predict a segmentation mask for each of those bounding box, for each of those region proposals. So now it kind of looks like a mini, like a semantic segmentation problem inside each of the region proposals that we're getting from our
region proposal network. So now after we do this
ROI aligning to warp our features corresponding
to the region of proposal into the right shape, then we
have two different branches. One branch will come up that looks exact, and this first branch at
the top looks just like faster R-CNN and it will
predict classification scores telling us what is the
category corresponding to that region of
proposal or alternatively whether or not it's background. And we'll also predict some
bounding box coordinates that regressed off the
region proposal coordinates. And now in addition we'll
have this branch at the bottom which looks basically like
a semantic segmentation mini network which will
classify for each pixel in that input region proposal
whether or not it's an object so this mask R-CNN problem,
this mask R-CNN architecture just kind of unifies all
of these different problems that we've been talking
about today into one nice jointly end-to-end trainable model. And it's really cool and it actually works really really well so when
you look at the examples in the paper they're kind of amazing. They look kind of indistinguishable
from ground truth. So in this example on the left you can see that there are these two people standing in front of motorcycles,
it's drawn the boxes around these people, it's
also gone in and labeled all the pixels of those
people and it's really small but actually in the
background on that image on the left there's also
a whole crowd of people standing very small in the background. It's also drawn boxes around each of those and grabbed the pixels
of each of those images. And you can see that this is just, it ends up working really really well and it's a relatively simple addition on top of the existing
faster R-CNN framework. So I told you that mask
R-CNN unifies everything we talked about today and it also does pose estimation by the way. So we talked about, you
can do pose estimation by predicting these joint coordinates for each of the joints of the person so you can do mask R-CNN to
do joint object detection, pose estimation, and
instance segmentation. And the only addition we need to make is that for each of these region proposals we add an additional little branch that predicts these
coordinates of the joints for the instance of the
current region proposal. So now this is just another loss, like another layer that we add, another head coming out of the network and an additional term
in our multi-task loss. But once we add this one little branch then you can do all of these
different problems jointly and you get results looking
something like this. Where now this network, like
a single feed forward network is deciding how many
people are in the image, detecting where those people are, figuring out the pixels
corresponding to each of those people and also
drawing a skeleton estimating the pose of those people
and this works really well even in crowded scenes like this classroom where there's a ton of people sitting and they all overlap each other and it just seems to work incredibly well. And because it's built on
the faster R-CNN framework it also runs relatively close to real time so this is running something
like five frames per second on a GPU because this is all sort of done in the single forward pass of the network. So this is again, a super new paper but I think that this will probably get a lot of attention in the coming months. So just to recap, we've talked. Sorry question? The question is how much
training data do you need? So all of these instant
segmentation results were trained on the
Microsoft Coco data set so Microsoft Coco is roughly
200,000 training images. It has 80 categories that it cares about so in each of those
200,000 training images it has all the instances of
those 80 categories labeled. So there's something like
200,000 images for training and there's something
like I think an average of fivee or six instances per image. So it actually is quite a lot of data. And for Microsoft Coco for all the people in Microsoft Coco they
also have all the joints annotated as well so this
actually does have quite a lot of supervision at training
time you're right, and actually is trained
with quite a lot of data. So I think one really
interesting topic to study moving forward is that we kind of know that if you have a lot of
data to solve some problem, at this point we're relatively
confident that you can stitch up some convolutional network that can probably do a
reasonable job at that problem but figuring out ways to
get performance like this with less training data
is a super interesting and active area of research and I think that's something people will be spending a lot of their efforts working
on in the next few years. So just to recap, today we
had kind of a whirlwind tour of a whole bunch of different
computer vision topics and we saw how a lot of the
machinery that we built up from image classification can
be applied relatively easily to tackle these different
computer vision topics. And next time we'll talk about, we'll have a really fun lecture
on visualizing CNN features. Well also talk about DeepDream
and neural style transfer. 

- Good morning. So, it's 12:03 so, I want to get started. Welcome to Lecture 12, of CS-231N. Today we are going to talk about
Visualizing and Understanding convolutional networks. This is always a super fun lecture to give because we get to look a
lot of pretty pictures. So, it's, it's one of my favorites. As usual a couple administrative things. So, hopefully your projects
are all going well, because as a reminder your milestones are due on Canvas tonight. It is Canvas, right? Okay, so want to double check, yeah. Due on Canvas tonight, we are working on furiously grading your midterms. So, we'll hope to have those
midterms grades to you back by on grade scope this week. So, I know that was little confusion, you all got registration
email's for grade scope probably in the last week. Something like that, we start
couple of questions on piazo. So, we've decided to use grade
scope to grade the midterms. So, don't be confused, if you
get some emails about that. Another reminder is that assignment three was released last week on Friday. It will be due, a week from
this Friday, on the 26th. This is, an assignment three, is almost entirely brand new this year. So, it we apologize for taking
a little bit longer than expected to get it out. But I think it's super cool. A lot of that stuff, we'll
talk about in today's lecture. You'll actually be implementing
on your assignment. And for the assignment, you'll
get the choice of either Pi torch or tensure flow. To work through these different examples. So, we hope that's really
useful experience for you guys. We also saw a lot of activity on HyperQuest over the weekend. So that's, that's really awesome. The leader board went up yesterday. It seems like you guys are
really trying to battle it out to show off your deep learning neural network training skills. So that's super cool. And we because due to the high interest in HyperQuest and due to
the conflicts with the, with the Milestones submission time. We decided to extend the deadline for extra credit through Sunday. So, anyone who does at
least 12 runs on HyperQuest by Sunday will get little bit
of extra credit in the class. Also those of you who are,
at the top of leader board doing really well, will
get may be little bit extra, extra credit. So, I thanks for
participating we got lot of interest and that was really cool. Final reminder is about
the poster session. So, we have the poster
session will be on June 6th. That date is finalized, I think that, I don't
remember the exact time. But it is June 6th. So that, we have some questions about when exactly that poster session is for those of you who are traveling at the end of quarter
or starting internships or something like that. So, it will be June 6th. Any questions on the admin notes. No, totally clear. So, last time we talked. So, last time we had a pretty jam packed lecture, when we
talked about lot of different computer vision tasks, as a reminder. We talked about semantic segmentation which is this problem, where
you want to sign labels to every pixel in the input image. But does not differentiate the object instances in those images. We talked about classification
plus localization. Where in addition to a class label you also want to draw a box or perhaps several boxes in the image. Where the distinction here is that, in a classification
plus localization setup. You have some fix number of
objects that you are looking for So, we also saw that this type of paradigm can be applied to the things
like pose recognition. Where you want to regress to
different numbers of joints in the human body. We also talked about the object detection where you start with some fixed set of category labels
that you are interested in. Like dogs and cats. And then the task is
to draw a boxes around every instance of those objects that appear in the input image. And object detection
is really distinct from classification plus localization because with object
detection, we don't know ahead of time, how many object instances we're looking for in the image. And we saw that there's
this whole family of methods based on RCNN, Fast RCNN and faster RCNN, as well as the single
shot detection methods for addressing this problem
of object detection. Then finally we talked
pretty briefly about instance segmentation,
which is kind of combining aspects of a semantic
segmentation and object detection where the goal is to
detect all the instances of the categories we care about, as well as label the pixels
belonging to each instance. So, in this case, we
detected two dogs and one cat and for each of those instances we wanted to label all the pixels. So, these are we kind of
covered a lot last lecture but those are really interesting
and exciting problems that you guys might consider to using in parts of your projects. But today we are going to
shift gears a little bit and ask another question. Which is, what's really going on inside convolutional networks. We've seen by this point in the class how to train convolutional networks. How to stitch up different
types of architectures to attack different problems. But one question that you
might have had in your mind, is what exactly is going
on inside these networks? How did they do the things that they do? What kinds of features
are they looking for? And all this source of related questions. So, so far we've sort of seen ConvNets as a little bit of a black box. Where some input image of raw pixels is coming in on one side. It goes to the many layers of convulsion and pooling in different
sorts of transformations. And on the outside, we end up
with some set of class scores or some types of understandable
interpretable output. Such as class scores or
bounding box positions or labeled pixels or something like that. But the question is. What are all these other
layers in the middle doing? What kinds of things in the input image are they looking for? And can we try again intuition for. How ConvNets are working? What types of things in the
image they are looking for? And what kinds of techniques do we have for analyzing this
internals of the network? So, one relatively simple
thing is the first layer. So, we've seen, we've
talked about this before. But recalled that, the
first convolutional layer consists of a filters that, so, for example in AlexNet. The first convolutional layer consists of a number of convolutional filters. Each convolutional of filter
has shape 3 by 11 by 11. And these convolutional filters gets slid over the input image. We take inner products between
some chunk of the image. And the weights of the
convolutional filter. And that gives us our output of the at, at after that first
convolutional layer. So, in AlexNet then we
have 64 of these filters. But now in the first layer
because we are taking in a direct inner product
between the weights of the convolutional layer and the pixels of the image. We can get some since for what
these filters are looking for by simply visualizing the
learned weights of these filters as images themselves. So, for each of those
11 by 11 by 3 filters in AlexNet, we can just
visualize that filter as a little 11 by 11 image
with a three channels give you the red, green and blue values. And then because there
are 64 of these filters we just visualize 64
little 11 by 11 images. And we can repeat... So
we have shown here at the. So, these are filters taken
from the prechain models, in the pi torch model zoo. And we are looking at the
convolutional filters. The weights of the convolutional filters. at the first layer of AlexNet, ResNet-18, ResNet-101 and DenseNet-121. And you can see, kind
of what all these layers what this filters looking for. You see the lot of things
looking for oriented edges. Likes bars of light and dark. At various angles, in various
angles and various positions in the input, we can see opposing colors. Like this are green and pink. opposing colors or this orange
and blue opposing colors. So, this, this kind of
connects back to what we talked about with Hugh and Wiesel. All the way in the first lecture. That remember the human visual system is known to the detect
things like oriented edges. At the very early layers
of the human visual system. And it turns out of that
these convolutional networks tend to do something, somewhat similar. At their first convolutional
layers as well. And what's kind of interesting is that pretty much no matter what type
of architecture you hook up or whatever type of training
data you are train it on. You almost always get
the first layers of your. The first convolutional
weights of any pretty much any convolutional network
looking at images. Ends up looking something like this with oriented edges and opposing colors. Looking at that input image. But this really only, sorry
what was that question? Yes, these are showing the learned weights of the first convolutional layer. Oh, so that the question is. Why does visualizing the
weights of the filters? Tell you what the filter is looking for. So this intuition comes from
sort of template matching and inner products. That if you imagine you have
some, some template vector. And then you imagine you
compute a scaler output by taking inner product
between your template vector and some arbitrary piece of data. Then, the input which
maximizes that activation. Under a norm constraint on the input is exactly when those
two vectors match up. So, in that since that,
when, whenever you're taking inner products, the thing
causes an inner product to excite maximally is a copy of the thing you are
taking an inner product with. So, that, that's why we can
actually visualize these weights and that, why that shows us, what this first layer is looking for. So, for these networks
the first layers always was a convolutional layer. So, generally whenever
you are looking at image. Whenever you are thinking about image data and training convolutional networks, you generally put a convolutional layer at the first, at the first stop. Yeah, so the question is, can we do this same type of procedure in the middle open network. That's actually the next slide. So, good anticipation. So, if we do, if we draw this exact same visualization for the
intermediate convolutional layers. It's actually a lot less interpretable. So, this is, this is performing
exact same visualization. So, remember for this using
the tiny ConvNets demo network that's running on the course website whenever you go there. So, for that network, the first layer is 7 by
7 convulsion 16 filters. So, after the top visualizing
the first layer weights for this network just like
we saw in a previous slide. But now at the second layer weights. After we do a convulsion
then there's some relu and some other non-linearity perhaps. But the second convolutional layer, now receives the 16 channel input. And does 7 by 7 convulsion
with 20 convolutional filters. And we've actually, so the problem is that
you can't really visualize these directly as images. So, you can try, so, here if you this 16 by, so the input is
this has 16 dimensions in depth. And we have these convolutional filters, each convolutional filter is 7 by 7, and is extending along the full depth so has 16 elements. Then we've 20 such of these
convolutional filters, that are producing the output
planes of the next layer. But the problem here is that
we can't, looking at the, looking directly at the weights of these filters, doesn't
really tell us much. So, we, that's really done here is that, now for this single 16 by 7
by 7 convolutional filter. We can spread out those 167
by 7 planes of the filter into a 167 by 7 grayscale images. So, that's what we've done. Up here, which is these little
tiny gray scale images here show us what is, what are the weights in one of the convolutional
filters of the second layer. And now, because there are
20 outputs from this layer. Then this second convolutional
layer, has 2o such of these 16 by 16 or 16 by 7 by 7 filters. So if we visualize the weights of those convolutional filters as images, you can see that there are some kind of spacial structures here. But it doesn't really
give you good intuition for what they are looking at. Because these filters are not
looking, are not connected directly to the input image. Instead recall that the second
layer convolutional filters are connected to the
output of the first layer. So, this is giving visualization of, what type of activation
pattern after the first convulsion, would cause
the second layer convulsion to maximally activate. But, that's not very interpretable because we don't have a good sense for what those first layer
convulsions look like in terms of image pixels. So we'll need to develop some
slightly more fancy technique to get a sense for what is going on in the intermediate layers. Question in the back. Yeah. So the question is that for... all the visualization
on this on the previous slide. We've had the scale the weights
to the zero to 255 range. So in practice those
weights could be unbounded. They could have any range. But to get nice visualizations
we need to scale those. These visualizations also do not take in to account the bias is in these layers. So you should keep that in mind when and not take these
HEPS visualizations to, to literally. Now at the last layer remember when we looking at the last layer of convolutional network. We have these maybe 1000 class scores that are telling us what
are the predicted scores for each of the classes
in our training data set and immediately before the last layer we often have some fully connected layer. In the case of Alex net we have some 4096- dimensional
features representation of our image that then
gets fed into that final our final layer to predict
our final class scores. And one another, another kind of route for tackling the problem
of visual, visualizing and understanding ConvNets is to try to understand what's
happening at the last layer of a convolutional network. So what we can do is how to take some,
some data set of images run a bunch of, run a bunch of images through our trained convolutional network and recorded that 4096 dimensional vector for each of those images. And now go through and try to figure out and visualize that last
layer, that last hidden layer rather than those rather than
the first convolutional layer. So, one thing you might imagine is, is trying a nearest neighbor approach. So, remember, way back
in the second lecture we saw this graphic on the left where we, where we had a
nearest neighbor classifier. Where we were looking at
nearest neighbors in pixels space between CIFAR 10 images. And then when you look
at nearest neighbors in pixel space between CIFAR 10 images you see that you pull up images that looks quite similar
to the query image. So again on the left column
here is some CIFAR 10 image from the CIFAR 10 data set and then these, these next five columns are showing the nearest
neighbors in pixel space to those test set images. And so for example this white dog that you see here, it's nearest neighbors are in pixel space are these kinds of white blobby things that may, may or may not be dogs, but at least the raw pixels of the image are quite similar. So now we can do the same
type of visualization computing and visualizing
these nearest neighbor images. But rather than computing the nearest neighbors in pixel space, instead we can compute nearest neighbors in that 4096 dimensional feature space. Which is computed by the
convolutional network. So here on the right we see some examples. So this, this first column shows us some examples of images from the test set of image that... Of the image
net classification data set and now the, these
subsequent columns show us nearest neighbors to those test set images in the 4096, in the 4096th
dimensional features space computed by Alex net. And you can see here that
this is quite different from the pixel space nearest neighbors, because the pixels are
often quite different. between the image in
it's nearest neighbors and feature space. However, the semantic
content of those images tends to be similar in this feature space. So for example, if you
look at this second layer the query image is this elephant standing on the left side of the image with a screen grass behind him. and now one of these, one of these... it's third nearest
neighbor in the tough set is actually an elephant standing on the right side of the image. So this is really interesting. Because between this
elephant standing on the left and this element stand,
elephant standing on the right the pixels between those two images are almost entirely different. However, in the feature space which is learned by the network those two images and that
being very close to each other. Which means that somehow
this, this last their features is capturing some of those
semantic content of these images. That's really cool and really exciting and, and in general looking at these kind of nearest
neighbor visualizations is really quick and easy way to visualize something about what's going on here. Yes. So the question is that through the... the standard
supervised learning procedure for classific training,
classification network There's nothing in the loss encouraging these features
to be close together. So that, that's true. It just kind of a happy accident that they end up being
close to each other. Because we didn't tell the
network during training these features should be close. However there are sometimes
people do train networks using things called
either contrastive loss or a triplet loss. Which actually explicitly make... assumptions and constraints on the network such that those last their features end up having some metric
space interpretation. But Alex net at least was not
trained specifically for that. The question is, what is the nearest... What is this nearest neighbor thing have to do at the last layer? So we're taking this image we're running it through the network and then the, the second to last like the last hidden layer of the network is of 4096th dimensional vector. Because there's this, this is... This is there, there are
these fully connected layers at the end of the network. So we are doing is... We're writing down that
4096th dimensional vector for each of the images and then we are computing
nearest neighbors according to that 4096th
dimensional vector. Which is computed by,
computed by the network. Maybe, maybe we can chat offline. So another, another, another another angle that we might have for visualizing what's
going on in this last layer is by some concept of
dimensionality reduction. So those of you who have
taken CS229 for example you've seen something like PCA. Which let's you take some high
dimensional representation like these 4096th dimensional features and then compress it
down to two-dimensions. So then you can visualize that
feature space more directly. So, Principle Component Analysis or PCA is kind of one way to do that. But there's real another
really powerful algorithm called t-SNE. Standing for t-distributed
stochastic neighbor embeddings. Which is slightly more powerful method. Which is a non-linear
dimensionality reduction method that people in deep often
use for visualizing features. So here as an, just an
example of what t-SNE can do. This visualization here is, is showing a t-SNE dimensionality reduction
on the emnest data set. So, emnest remember is this date set of hand written digits
between zero and nine. Each image is a gray scale image 20... 28 by 28 gray scale image and now we're... So that Now we've, we've used t-SNE to take that 28 times 28
dimensional features space of the raw pixels for m-nest and now compress it
down to two- dimensions ans then visualize each
of those m-nest digits in this compress
two-dimensional representation and when you do, when you run t-SNE on the raw pixels and m-nest You can see these natural
clusters appearing. Which corresponds to the,
the digits of these m-nest of, of these m-nest data set. So now we can do a similar
type of visualization. Where we apply this t-SNE
dimensionality reduction technique to the features from the last layer of our trained image net classifier. So...To be a little bit more concrete here what we've done is that we take, a large set of images we run them off convolutional network. We record that final 4096th
dimensional feature vector for, from the last layer
of each of those images. Which gives us large collection of 4096th dimensional vectors. Now we apply t-SNE
dimensionality reduction to compute, sort of compress
that 4096the dimensional features space down into a
two-dimensional feature space and now we, layout a grid in that compressed two-dimensional feature space and visualize what types of images appear at each location in the grid in this two-dimensional feature space. So by doing this you get
some very close rough sense of what the geometry of this learned feature space looks like. So these images are
little bit hard to see. So I'd encourage you to check out the high resolution versions online. But at least maybe on
the left you can see that there's sort of one
cluster in the bottom here of, of green things, is a
different kind of flowers and there's other types of clusters for different types of dog breeds and another types of
animals and, and locations. So there's sort of
discontinuous semantic notion in this feature space. Which we can explore by looking through this t-SNE dimensionality reduction version of the, of the features. Is there question? Yeah. So the basic idea is that we're we, we have an image so now we end up with
three different pieces of information about each image. We have the pixels of the image. We have the 4096th dimensional vector. Then we use t-SNE to convert
the 4096th dimensional vector into a two-dimensional coordinate and then we take the
original pixels of the image and place that at the
two-dimensional coordinate corresponding to the
dimensionality reduced version of the 4096th dimensional feature. Yeah, little bit involved here. Question in the front. The question is Roughly how much variants do
these two-dimension explain? Well, I'm not sure of the exact number and I get little bit muddy when you're talking about t-SNE, because it's a non-linear dimensionality reduction technique. So, I'd have to look offline and I'm not sure of exactly
how much it explains. Question? Question is, can you do the same analysis of upper layers of the network? And yes, you can. But no, I don't have those
visualizations here. Sorry. Question? The question is,
Shouldn't we have overlaps of images once we do this
dimensionality reduction? And yes, of course, you would. So this is just kind of taking a, nearest neighbor in
our, in our regular grid and then picking an image
close to that grid point. So, so... they, yeah. this is not showing
you the kind of density in different parts of the feature space. So that's, that's another thing to look at and again at the link
you, there's a couple more visualizations of this nature that, that address that a little bit. Okay. So another, another thing that you can do for some of
these intermediate features is, so we talked a couple of slides ago that visualizing the weights
of these intermediate layers is not so interpretable. But actually visualizing
the activation maps of those intermediate layers is kind of interpretable in some cases. So for, so I, again an
example of Alex Net. Remember the, the conv5
layers of Alex Net. Gives us this 128 by... The for...The conv5 features for any image is now 128 by 13 by 13 dimensional tensor. But we can think of that as 128 different 13 by 132-D grids. So now we can actually go and visualize each of those 13 by 13 elements slices of the feature map as a grayscale image and this gives us some sense
for what types of things in the input are each of those features in that convolutional layer looking for. So this is a, a really
cool interactive tool by Jason Yasenski you can just download. So it's run, so I don't have the video, it has a video on his website. But it's running a convolutional network on the inputs stream of webcam and then visualizing in real time each of those slices of that
intermediate feature map give you a sense of what it's looking for and you can see that,
so here the input image is this, this picture up in, settings... of this picture of a person
in front of the camera and most of these intermediate features are kind of noisy, not much going on. But there's a, but there's
this one highlighted intermediate feature where that is also shown larger here that seems that it's activating on the portions of the feature map corresponding to the person's face. Which is really interesting and that kind of,
suggests that maybe this, this particular slice of the feature map of this layer of this particular network is maybe looking for human
faces or something like that. Which is kind of a nice, kind of a nice and cool finding. Question? The question is, Are the
black activations dead relu's? So you got to be... a little
careful with terminology. We usually say dead relu to mean something that's dead over
the entire training data set. Here I would say that it's a
relu, that, it's not active for this particular input. Question? The question is, If there's
no humans in image net how can it recognize a human face? There definitely are humans in image net I don't think it's, it's one of the cat... I don't think it's one of
the thousand categories for the classification challenge. But people definitely appear
in a lot of these images and that can be useful
signal for detecting other types of things. So that's actually kind of nice results because that shows that, it's
sort of can learn features that are useful for the
classification task at hand. That are even maybe a little bit different from the explicit classification task that we told it to perform. So it's actually really cool results. Okay, question? So at each layer in the
convolutional network our input image is of three,
it's like 3 by 224 by 224 and then it goes through
many stages of convolution. And then, it, after
each convolutional layer is some three dimensional
chunk of numbers. Which are the outputs from that layer of the convolutional network. And that into the entire three
dimensional chunk of numbers which are the output of the
previous convolutional layer, we call, we call, like
an activation volume and then one of those, one of those slices is a, it's an activation map. So the question is, If the image is K by K will the activation map be K by K? Not always because there
can be sub sampling due to pool, straight
convolution and pooling. But in general, the, the
size of each activation map will be linear in the
size of the input image. So another, another kind
of useful thing we can do for visualizing
intermediate features is... Visualizing what types of
patches from input images cause maximal activation in different, different features, different neurons. So what we've done here
is that, we pick... Maybe again the con five
layer from Alex Net? And remember each of
these activation volumes at the con, at the con
five in Alex net gives us a 128 by 13 by 13 chunk of numbers. Then we'll pick one of those 128 channels. Maybe channel 17 and now what we'll do is run many images through this convolutional network. And then, for each of those images record the con five features and then look at the... Right, so, then, then look at the, the... The parts of that 17th feature map that are maximally activated
over our data set of images. And now, because again this
is a convolutional layer each of those neurons in
the convolutional layer has some small receptive
field in the input. Each of those neurons is not
looking at the whole image. They're only looking at
the sub set of the image. Then what we'll do is,
is visualize the patches from the, from this
large data set of images corresponding to the maximal activations of that, of that feature,
of that particular feature in that particular layer. And then we can sorts these out, sort these patches by their activation at that, at that particular layer. So here is a, some examples from this... Network called a, fully... The network doesn't matter. But these are some visualizations of these kind of maximally
activating patches. So, each, each row gives... We've chosen one layer from or one neuron from one layer of a network and then each, and then,
the, they're sorted of these are the patches from
some large data set of images. That maximally activated this one neuron. And these can give you a sense
for what type of features these, these neurons might be looking for. So for example, this top row we see a lot of circly kinds
of things in the image. Some eyes, some, mostly eyes. But also this, kind of blue circly region. So then, maybe this,
this particular neuron in this particular layer of
this network is looking for kind of blue circly things in the input. Or maybe in the middle here we have neurons that are looking for text in different colors or, or maybe curving, curving edges of different colors and orientations. Yeah, so, I've been a little bit loose with terminology here. So, I'm saying that a
neuron is one scaler value in that con five activation map. But because it's convolutional, all the neurons in one channel are all using the same weights. So we've chosen one
channel and then, right? So, you get a lot of neurons
for each convolutional filter at any one layer. So, we, we could have been, so this patches could've
been drawn from anywhere in the image due to the
convolutional nature of the thing. And now at the bottom we also see some maximally activating patches for neurons from a higher up
layer in the same network. And now because they are coming
from higher in the network they have a larger receptive field. So, they're looking at larger
patches of the input image and we can also see
that they're looking for maybe larger structures
in the input image. So this, this second row is maybe looking, it seems to be looking for human, humans or maybe human faces. We have maybe something looking for... Parts of cameras or
different types of larger, larger, larger object like
type things, types of things. Another, another cool experiment we can do which comes from Zeiler
and Fergus ECCV 2014 paper. is this idea of an exclusion experiment. So, what we want to do is figure out which parts of the
input, of the input image cause the network to make
it's classification decision. So, what we'll do is,
we'll take our input image in this case an elephant and then we'll block
out some part of that, some region in that input image and just replace it with
the mean pixel value from the data set. And now, run that
occluded image throughout, through the network and then record what is the predicted probability
of this occluded image? And now slide this occluded
patch over every position in the input image and then
repeat the same process. And then draw this heat map showing, what was the predicted probability
output from the network as a function of where did, which part of the input
image did we occlude? And the idea is that if when we block out
some part of the image if that causes the network
score to change drastically. Then probably that part of the input image was really important for
the classification decision. So here we've shown... I've shown three different examples of... Of this occlusion type experiment. So, maybe this example of
a Go-kart at the bottom, you can see over here that when we, so here, red, the, the red corresponds
to a low probability and the white and yellow
corresponds to a high probability. So when we block out
the region of the image corresponding to this Go-kart in front. Then the predicted probability for the Go-kart class drops a lot. So that gives us some sense that the network is actually
caring a lot about these, these pixels in the input image in order to make it's
classification decision. Question? Yes, the question is that, what's going on in the background? So maybe if the image is a
little bit too small to tell but, there's, this is
actually a Go-kart track and there's a couple other
Go-karts in the background. So I think that, when
you're blocking out these other Go-karts in the background, that's also influencing the score or maybe like the horizon is there and maybe the horizon is an useful feature for detecting Go-karts, it's a little bit hard to tell sometimes. But this is a pretty cool visualization. Yeah, was there another question? So the question is, sorry, sorry, what was the first question? So, the, so the question... So for, for this example
we're taking one image and then masking all parts of one image. The second question
was, how is this useful? It's not, maybe, you don't
really take this information and then loop it directly
into the training process. Instead, this is a way, a tool for humans to understand, what types of computations these train networks are doing. So it's more for your understanding than for improving performance per se. So another, another related idea is this concept of a Saliency Map. Which is something that you
will see in your homeworks. So again, we have the same question of given an input image
of a dog in this case and the predicted class label of dog we want to know which
pixels in the input image are important for classification. We saw masking, is one way
to get at this question. But Saliency Maps are another, another, angle for attacking this problem. And the question is, and
one relatively simple idea from Karen Simonenian's
paper, a couple years ago. Is, this is just computing the gradient of the predicted class score with respect to the
pixels of the input image. And this will directly tell us in this sort of, first
order approximation sense. For each input, for each
pixel in the input image if we wiggle that pixel a little bit then how much will the
classification score for the class change? And this is another way
to get at this question of which pixels in the input
matter for the classification. And when we, and when we run for example Saliency,
where computer Saliency map for this dog, we see kind of a nice outline of a dog in the image. Which tells us that these
are probably the pixels of that, network is actually
looking at, for this image. And when we repeat this type of process for different images, we get some sense that the network is sort of
looking at the right regions. Which is somewhat comforting. Question? The question is, do
people use Saliency Maps for semantic segmentation?
The answer is yes. That actually was... Yeah, you guys are like really
on top of it this lecture. So that was another component,
again in Karen's paper. Where there's this idea
that maybe you can use these Saliency Maps to perform
semantic segmentation without direct, without any labeled data for the, for these, for these segments. So here they're using this
Grabcut Segmentation Algorithm which I don't really want
to get into the details of. But it's kind of an interactive
segmentation algorithm that you can use. So then when you combine this Saliency Map with this Grabcut Segmentation Algorithm then you can in fact,
sometimes segment out the object in the image. Which is really cool. However I'd like to point out that this is a little bit brittle and in general if you,
this will probably work much, much, much, worse than a network which did have access to
supervision and training time. So, I don't, I'm not sure
how, how practical this is. But it is pretty cool
that it works at all. But it probably works much
less than something trained explicitly to segment with supervision. So kind of another, another related idea is this idea of, of
guided back propagation. So again, we still want
to answer the question of for one particular, for
one particular image. Then now instead of
looking at the class score we want to know, we want to
pick some intermediate neuron in the network and ask again, which parts of the input image influence the score of that neuron, that internal neuron in the network. And, and then you could
imagine, again you could imagine computing a Saliency Map
for this again, right? That rather than computing the
gradient of the class scores with respect to the pixels of the image. You could compute the gradient
of some intermediate value in the network with respect
to the pixels of the image. And that would tell us again which parts, which
pixels in the input image influence that value of
that particular neuron. And that would be using
normal back propagation. But it turns out that
there is a slight tweak that we can do to this
back propagation procedure that ends up giving some
slightly cleaner images. So that's this idea of
guided back propagation that again comes from Zeiler
and Fergus's 2014 paper. And I don't really want to get
into the details too much here but, it, you just, it's
kind of weird tweak where you change the way
that you back propagate through relu non-linearities. And you sort of, only, only back propagate positive gradients through relu's and you do not back propagate negative gradients through the relu's. So you're no longer
computing the true gradient instead you're kind of only keeping track of positive influences on throughout the entire network. So maybe you should read
through these, these papers reference to your, if you
want a little bit more details about why that's a good idea. But empirically, when you
do guided back propagation as appose to regular back propagation. You tend to get much
cleaner, nicer images. that tells you, which part,
which pixel of the input image influence that particular neuron. So, again we were seeing
the same visualization we saw a few slides ago of the
maximally activating patches. But now, in addition to visualizing these maximally activating patches. We've also performed
guided back propagation, to tell us exactly which parts
of these patches influence the score of that neuron. So, remember for this example at the top, we saw that, we thought this neuron is may be looking for circly tight things, in the input patch because there're allot
of circly tight patches. Well, when we look at
guided back propagation We can see with that intuition
is somewhat confirmed because it is indeed the circly
parts of that input patch which are influencing
that, that neuron value. So, this is kind of a useful
to all for synthesizing. For understanding what these
different intermediates are looking for. But, one kind of interesting thing about guided back propagation or computing saliency maps. Is that there's always a
function of fixed input image, right, they're telling us
for a fixed input image, which pixel or which parts
of that input image influence the value of the neuron. Another question you might answer is is remove this reliance, on
that, on some input image. And then instead just ask
what type of input in general would cause this neuron to activate and we can answer this question using a technical Gradient ascent so, remember we always use Gradient decent to train our convolutional
networks by minimizing the loss. Instead now, we want to fix the, fix the weight of our trained
convolutional network and instead synthesizing image
by performing Gradient ascent on the pixels of the
image to try and maximize the score of some intermediate
neuron or of some class. So, in a process of Gradient ascent, we're no longer optimizing
over the weights of the network those weights remained fixed instead we're trying to change
pixels of some input image to cause this neuron,
or this neuron value, or this class score to
maximally, to be maximized but, instead but, in addition we need some regularization term so, remember we always a, we before seeing regularization terms to try to prevent the network weights from over fitting to the training data. Now, we need something kind of similar to prevent the pixels
of our generated image from over fitting to the peculiarities of that particular network. So, here we'll often incorporate
some regularization term that, we're kind of, we
want a generated image of two properties one, we wanted to maximally activate some, some score or some neuron value. But, we also wanted to
look like a natural image. we wanted to kind of have, the kind of statistics that we typically see in natural images. So, these regularization
term in the subjective is something to enforce a generated image to look relatively natural. And we'll see a couple
of different examples of regualizers as we go through. But, the general strategy for this is actually pretty simple and again informant allot
of things of this nature on your assignment three. But, what we'll do is start
with some initial image either initializing to zeros
or to uniform or noise. But, initialize your image in some way and I'll repeat where
you forward your image through 3D network and compute the score or, or neuron value
that you're interested. Now, back propagate to
compute the Gradient of that neuron score with respect
to the pixels of the image and then make a small Gradient ascent or Gradient ascent update to the pixels of the images itself. To try and maximize that score. And I'll repeat this
process over and over again, until you have a beautiful image. And, then we talked, we talked
about the image regularizer, well a very simple, a very
simple idea for image regularizer is simply to penalize L2
norm of a generated image This is not so semantically meaningful, it's just does something,
and this was one of the, one of the earliest
regularizer that we've seen in the literature for these
type of generating images type of papers. And, when you run this
on a trained network you can see that now we're
trying to generate images that maximize the dumble score in the upper left hand
corner here for example. And, then you can see that
the synthesized image, it been, it's little
bit hard to see may be but there're allot of
different dumble like shapes, all kind of super impose that different portions of the image. or if we try to generate an image for cups then we can may be see a
bunch of different cups all kind of super imposed the Dalmatian is pretty cool, because now we can see kind of this black and white spotted pattern that's kind of
characteristics of Dalmatians or for lemons we can see
these different kinds of yellow splotches in the image. And there's a couple
of more examples here, I think may be the goose is kind of cool or the kitfox are actually
may be looks like kitfox. Question? The question is, why are
these all rainbow colored and in general getting true colors out of this visualization is pretty tricky. Right, because any, any actual image will be bounded in the range zero to 255. So, it really should be some kind of constrained optimization problem But, if, for using this generic
methods for Gradient ascent then we, that's going to
be unconstrained problem. So, you may be use like projector
Gradient ascent algorithm or your rescaled image at the end. So, the colors that you
see in this visualizations, sometimes are you cannot
take them too seriously. Question? The question is what happens, if you let the thing loose and don't
put any regularizer on it. Well, then you tend to get
an image which maximize the score which is confidently classified as the class you wanted but, usually it doesn't
look like anything. It kind of look likes random noise. So, that's kind of an
interesting property in itself that will go into much more
detail in a future lecture. But, that's why, that
kind of doesn't help you so much for understanding what things the network is looking for. So, if we want to understand, why the network thing makes its decisions then it's kind of useful
to put regularizer on there to generate an
image to look more natural. A question in the back. Yeah, so the question
is that we see a lot of multi modality here, and
other ways to combat that. And actually yes, we'll see that, this is kind of first
step in the whole line of work in improving these visualizations. So, another, another kind
of, so then the angle here is a kind of to improve the regularizer to improve our visualized images. And there's a another
paper from Jason Yesenski and some of his collaborators where they added some additional
impressive regularizers. So, in addition to this
L2 norm constraint, in addition we also periodically
during optimization, and do some gauche and
blurring on the image, we're also clip some,. some small value, some small pixel values all the way to zero, we're
also clip some of the, some of the pixel values
of low Gradients to zero So, you can see this is kind of a projector Gradient ascent algorithm where it reach periodically
we're projecting our generated image onto some nicer set of images with some nicer properties. For example, special smoothness with respect to the gauchian blurring So, when you do this, you
tend to get much nicer images that are much clear to see. So, now these flamingos
look like flamingos the ground beetle is starting
to look more beetle like or this black swan maybe
looks like a black swan. These billiard tables actually
look kind of impressive now, where you can definitely see
this billiard table structure. So, you can see that once you
add in nicer regularizers, then the generated images become a bit, a little bit cleaner. And, now we can perform this procedure not only for the final class course, but also for these
intermediate neurons as well. So, instead of trying to
maximize our billiard table score for example instead we
can get maximize one of the neurons from
some intermediate layer Question. So, the question is what's
with the for example here, so those who remember
initializing our image randomly so, these four images would be different random
initialization of the input image. And again, we can use these
same type of procedure to visualize, to synthesis images which maximally activate
intermediate neurons of the network. And, then you can get a sense from some of these intermediate
neurons are looking for, so may be at layer four there's neuron that's kind of looking for spirally things or there's neuron that's may be looking for like chunks of caterpillars it's a little bit harder to tell. But, in generally as you
go larger up in the image then you can see that
the one, the obviously receptive fields of
these neurons are larger. So, you're looking at the
larger patches in the image. And they tend to be looking
for may be larger structures or more complex patterns
in the input image. That's pretty cool. And, then people have
really gone crazy with this and trying to, they basically
improve these visualization by keeping on extra features So, this was a cool paper kind of explicitly trying to address this multi modality, there's
someone asked question about a few minutes ago. So, here they were
trying to explicitly take a count, take this multi
modality into account in the optimization procedure where they did indeed,
I think see the initial, so they for each of the classes, you run a clustering algorithm to try to separate the
classes into different modes and then initialize with something that is close to one of those modes. And, then when you do that, you kind of account for
this multi modality. so for intuition, on the
right here these eight images are all of grocery stores. But, the top row, is
kind of close up pictures of produce on the shelf and those are labeled as grocery stores And the bottom row kind of shows people walking around grocery stores or at the checkout line
or something like that. And, those are also labeled
those as grocery store, but their visual appearance
is quiet different. So, a lot of these classes
and that being sort multi modal And, if you can take, and
if you explicitly take this more time mortality into account when generating images, then
you can get nicer results. And now, then when you look at some of their example, synthesis
images for classes, you can see like the
bell pepper, the card on, strawberries, jackolantern now they end up with some very beautifully
generated images. And now, I don't want to get to much into detail of the next slide. But, then you can even go crazier. and add an even stronger image prior and generate some very
beautiful images indeed So, these are all synthesized
images that are trying to maximize the class score
or some image in a class. But, the general idea is that rather than optimizing directly the pixels of the input image, instead they're trying to optimize the FC6 representation
of that image instead. And now they need to use some
feature inversion network and I don't want to get
into the details here. You should read the paper,
it's actually really cool But, the point is that, when you start adding additional priors towards modeling natural images and you can end generating
some quiet realistic images they gave you some sense of
what the network is looking for So, that's, that's sort of one cool thing that we can do with this
strategy, but this idea of trying to synthesis
images by using Gradients on image pixels, is
actually super powerful. And, another really cool
thing we can do with this, is this concept of fooling image So, what we can do is
pick some arbitrary image, and then try to maximize the,
so, say we take it picture of an elephant and then
we tell the network that we want to, change the image to maximize the score
of Koala bear instead So, then what we were
doing is trying to change that image of an elephant
to try and instead cause the network to classify as a Koala bear. And, what you might hope for is that, maybe that elephant was
sort of thought more thing into a Koala bear and
maybe he would sprout little cute ears or something like that. But, that's not what happens in practice, which is pretty surprising. Instead if you take this
picture of a elephant and tell them that, tell them that and try to change the
elephant image to instead cause it to be classified as a koala bear What you'll find is that, you is that this second image on the right actually is classified as koala bear but it looks the same to us. So that's pretty fishy
and pretty surprising. So also on the bottom we've
taken this picture of a boat. Schooner is the image in that class and then we told the network
to classified as an iPod. So now the second example looks just, still looks like a boat to us but the network thinks it's an iPod and the difference is in
pixels between these two images are basically nothing. And if you magnify those differences you don't really see any
iPod or Koala like features on these differences, they're just kind of like
random patterns of noise. So the question is what's going here? And like how can this possibly the case? Well, we'll have a guest
lecture from Ian Goodfellow in a week an half two weeks. And he's going to go in much more detail about this type of phenomenon and that will be really exciting. But I did want to mention it here because it is on your homework. Question? Yeah, so that's something, so the question is can we use
fooled images as training data and I think, Ian's going
to go in much more detail on all of these types of strategies. Because that's literally, that's really a whole lecture onto itself. Question? The question is why do we
care about any of this stuff? Basically... Okay, maybe that was a
mischaracterization, I am sorry. Yeah, the question is
what is have in the... understanding this intermediate neurons how does that help our understanding of the final classification. So this is actually, this
whole field of trying to visualize intermediates
is kind of in response to a common criticism of deep learning. So a common criticism of
deep learning is like, you've got this big black box network you trained it on gradient
ascent, you get a good number and that's great but we
don't trust the network because we don't understand as people why it's making the
decisions, that's it's making. So a lot of these type of
visualization techniques were developed to try and address that and try to understand as people why the network are making
their various classification, classification decisions a bit more. Because if you contrast, if you contrast a deep
convolutional neural network with other machine running techniques. Like linear models are much
easier to interpret in general because you can look at
the weights and kind of understand the interpretation
between how much each input feature effect the decision or
if you look at something like a random forest or decision tree. Some other machine learning models end up being a bit more interpretable just by their very nature
then this sort of black box convolutional networks. So a lot of this is sort of
in response to that criticism to say that, yes they are
these large complex models but they are still doing some
interesting and interpretable things under the hood. They are not just totally going out in randomly classifying things. They are doing something meaningful So another cool thing we can do with this gradient based optimization of images is this idea of DeepDream. So this was a really cool blog post that came out from
Google a year or two ago. And the idea is that, this is, so we talked
about scientific value, this is almost entirely for fun. So the point of this exercise is mostly to generate cool images. And aside, you also get
some sense for what features images are looking at. Or these networks are looking at. So we can do is, we take our input image we run it through the convolutional
network up to some layer and now we back propagate and set the gradient
of that, at that layer equal to the activation value. And now back propagate, back to the image and update the image and
repeat, repeat, repeat. So this has the interpretation
of trying to amplify existing features that were
detected by the network in this image. Right? Because whatever features
existed on that layer now we set the gradient
equal to the feature and we just tell the network to amplify whatever features you
already saw in that image. And by the way you can also
see this as trying to maximize the L2 norm of the features
at that layer of the image. And it turns... And when you do this the code ends up looking really simple. So your code for many of
your homework assignments will probably be about this complex or maybe even a little bit a less so. So the idea is that... But there's a couple of tricks here that you'll also see in your assignments. So one trick is to jitter the image before you compute your gradients. So rather than running the
exact image through the network instead you'll shift the
image over by two pixels and kind of wrap the other
two pixels over here. And this is a kind of regularizer to prevent each of these [mumbling] it regularizers a little bit to encourage a little bit of extra special
smoothness in the image. You also see they use L1
normalization of the gradients that's kind of a useful trick sometimes when doing this image generation problems. You also see them clipping the
pixel values once in a while. So again we talked about
images actually should be between zero to 2.55 so this is a kind of
projected gradients decent where we project on to the
space of actual valid images. But now when we do all this then we start, we might start
with some image of a sky and then we get really
cool results like this. So you can see that now we've taken these tiny features on the sky and they get amplified through
this, through this process. And we can see things like this different mutant animals start to pop up or these kind of spiral shapes pop up. Different kinds of houses and cars pop up. So that's all, that's
all pretty interesting. There's a couple patterns in particular that pop up all the time
that people have named. Right, so there's this Admiral
dog, that shows up allot. There's the pig snail, the camel bird this the dog fish. Right, so these are
kind of interesting, but actually this fact that
dog show up so much in these visualization,
actually does tell us something about the data on
which this network was trained. Right, because this is a
network that was trained for image net classification, image that have thousand categories. But 200 of those categories are dogs. So, so it's kind of not
surprising in a sense that when you do these
kind of visualizations then network ends up hallucinating
a lot of dog like stuff in the image often morphed
with other types of animals. When you do this other
layers of the network you get other types of results. So here we're taking one
of these lower layers in the network, the previous
example was relatively high up in the network and now again we have this
interpretation that lower layers maybe computing edges and
swirls and stuff like that and that's kind of borne out
when we running DeepDream at a lower layer. Or if you run this thing for a long time and maybe add in some
multiscale processing you can get some really,
really crazy images. Right, so here they're doing a
kind of multiscale processing where they start with a small image run DeepDream on the small
image then make it bigger and continue DeepDream on the larger image and kind of repeat with
this multiscale processing and then you can get, and then maybe after you
complete the final scale then you restart from the beginning and you just go wild on this thing. And you can get some really crazy images. So these examples were all from networks trained on image net There's another data set from
MIT called MIT Places Data set but instead of 1,000 categories of objects instead it's 200 different types of scenes like bedrooms and kitchens
like stuff like that. And now if we repeat
this DeepDream procedure using an network trained at MIT places. We get some really cool
visualization as well. So now instead of dogs,
slugs and admiral dogs and that's kind of stuff,
instead we often get these kind of roof shapes of these
kind of Japanese style building or these different types of
bridges or mountain ranges. They're like really, really
cool beautiful visualizations. So the code for DeepDream is
online, released by Google you can go check it out and
make your own beautiful pictures So there's another kind of... Sorry question? So the question is, what
are taking gradient of? So like I say, if you, because
like one over x squared on the gradient of that is x. So, if you send back
the volume of activation as the gradient, that's equivalent to max, that's equivalent to taking the
gradient with respect to the like one over x squared some... Some of the values. So it's equivalent to maximizing the norm of that of the features of that layer. But in practice many implementation you'll see not explicitly compute that instead of send gradient back. So another kind of useful,
another kind of useful thing we can do is this concept
of feature inversion. So this again gives us a
sense for what types of, what types of elements
of the image are captured at different layers of the network. So what we're going to
do now is we're going to take an image, run that
image through network record the feature value
for one of those images and now we're going to try
to reconstruct that image from its feature representation. And the question, and now based on the how much, how much like what that reconstructed image looks like that'll give us some sense
for what type of information about the image was captured
in that feature vector. So again, we can do this
with gradient ascent with some regularizer. Where now rather than
maximizing some score instead we want to minimize the distance between this catch feature vector. And between the computed
features of our generated image. To try and again synthesize
a new image that matches the feature back to
that we computed before. And another kind of regularizer
that you frequently see here is the total variation regularizer that you also see on your homework. So here with the total
variation regularizer is panelizing differences
between adjacent pixels on both of the left and
adjacent in left and right and adjacent top to bottom. To again try to encourage
special smoothness in the generated image. So now if we do this
idea of feature inversion so this visualization here on the left we're showing some original image. The elephants or the fruits at the left. And then we run that, we run the image through a VGG-16 network. Record the features of
that network at some layer and then try to synthesize
a new image that matches the recorded features of that layer. And this is, this kind of
give us a sense for what how much information is
stored in this images. In these features of different layers. So for example if we try
to reconstruct the image based on the relu2_2 features
from VGC's, from VGG-16. We see that the image gets
almost perfectly reconstructed. Which means that we're
not really throwing away much information about the raw
pixel values at that layer. But as we move up into the
deeper parts of the network and try to reconstruct
from relu4_3, relu5_1. We see that our reconstructed image now, we've kind of kept the general space, the general spatial
structure of the image. You can still tell that, that
it's a elephant or a banana or a, or an apple. But a lot of the low level details aren't exactly what the pixel values were and exactly what the colors were, exactly what the textures were. These are kind of low level details are kind of lost at these higher
layers of this network. So that gives us some sense that maybe as we move up through
the flairs of the network it's kind of throwing away
this low level information about the exact pixels of the image and instead is maybe trying
to keep around a little bit more semantic information,
it's a little bit invariant for small changes in color and
texture and things like that. So we're building towards
a style transfer here which is really cool. So in addition to
understand style transfer, in addition to feature inversion. We also need to talk
about a related problem called texture synthesis. So in texture synthesis, this
is kind of an old problem in computer graphics. Here the idea is that
we're given some input patch of texture. Something like these little scales here and now we want to build some model and then generate a larger
piece of that same texture. So for example, we might here
want to generate a large image containing many scales that
kind of look like input. And this is again a pretty old
problem in computer graphics. There are nearest neighbor
approaches to textual synthesis that work pretty well. So, there's no neural networks here. Instead, this kind of a simple algorithm where we march through the generated image one pixel at a time in scan line order. And then copy... And then look at a neighborhood
around the current pixel based on the pixels that
we've already generated and now compute a nearest
neighbor of that neighborhood in the patches of the input image and then copy over one
pixel from the input image. So, maybe you don't need to
understand the details here just the idea is that there's
a lot classical algorithms for texture synthesis,
it's a pretty old problem but you can do this without
neural networks basically. And when you run this kind of this kind of classical
texture synthesis algorithm it actually works reasonably
well for simple textures. But as we move to more complex textures these kinds of simple methods
of maybe copying pixels from the input patch directly tend not to work so well. So, in 2015, there was a really cool paper that tried to apply
neural network features to this problem of texture synthesis. And ended up framing it as kind of a gradient ascent procedure, kind of similar to the feature map, the various feature matching objectives that we've seen already. So, in order to perform
neural texture synthesis they use this concept of a gram matrix. So, what we're going to do, is we're going to take our input texture and in this case some pictures of rocks and then take that input texture and pass it through some
convolutional neural network and pull out convolutional features at some layer of the network. So, maybe then this
convolutional feature volume that we've talked about,
might be H by W by C or sorry, C by H by W at
that layer of the network. So, you can think of this
as an H by W spacial grid. And at each point of the grid, we have this C dimensional feature vector describing the rough
appearance of that image at that point. And now, we're going to
use this activation map to compute a descriptor of the
texture of this input image. So, what we're going to do is take, pick out two of these
different feature columns in the input volume. Each of these feature columns will be a C dimensional vector. And now take the outer product
between those two vectors to give us a C by C matrix. This C by C matrix now tells us something about the co-occurrence of the different features at
those two points in the image. Right, so, if an element,
if like element IJ in the C by C matrix is large that means both elements I and
J of those two input vectors were large and something like that. So, this somehow captures
some second order statistics about which features, in that feature map tend to activate to together
at different spacial volumes... At different spacial positions. And now we're going to
repeat this procedure using all different
pairs of feature vectors from all different points
in this H by W grid. Average them all out, and that gives us our C by C gram matrix. And this is then used a
descriptor to describe kind of the texture of that input image. So, what's interesting
about this gram matrix is that it has now thrown
away all spacial information that was in this feature volume. Because we've averaged over
all pairs of feature vectors at every point in the image. Instead, it's just
capturing the second order co-occurrence statistics between features. And this ends up being a
nice descriptor for texture. And by the way, this is
really efficient to compute. So, if you have a C by H by
W three dimensional tensure you can just reshape
it to see times H by W and take that times its own transpose and compute this all in one shot so it's super efficient. But you might be wondering why you don't use an actual covariance matrix or something like that instead
of this funny gram matrix and the answer is that using covariance... Using true covariance matrices also works but it's a little bit
more expensive to compute. So, in practice a lot of people just use this gram matrix descriptor. So then... Then there's this... Now once we have this sort of
neural descriptor of texture then we use a similar type
of gradient ascent procedure to synthesize a new image
that matches the texture of the original image. So, now this looks kind of
like the feature reconstruction that we saw a few slides ago. But instead, I'm trying to
reconstruct the whole feature map from the input image. Instead, we're just going
to try and reconstruct this gram matrix texture descriptor of the input image instead. So, in practice what this
looks like is that well... You'll download some pretrained model, like in feature inversion. Often, people will use
the VGG networks for this. You'll feed your... You'll
take your texture image, feed it through the VGG network, compute the gram matrix
and many different layers of this network. Then you'll initialize your new image from some random initialization and then it looks like
gradient ascent again. Just like for these other
methods that we've seen. So, you take that image, pass it through the same VGG network, Compute the gram matrix at various layers and now compute loss as the L2 norm between the gram matrices of your input texture
and your generated image. And then you back prop,
and compute pixel... A gradient of the pixels
on your generated image. And then make a gradient ascent step to update the pixels of
the image a little bit. And now, repeat this process many times, go forward, compute your gram matrices, compute your losses, back prop.. Gradient on the image and repeat. And once you do this, eventually
you'll end up generating a texture that matches your
input texture quite nicely. So, this was all from Nip's 2015 paper by a group in Germany. And they had some really cool
results for texture synthesis. So, here on the top, we're showing four
different input textures. And now, on the bottom, we're showing doing this texture synthesis approach by gram matrix matching. Using, by computing the gram
matrix at different layers at this pretrained convolutional network. So, you can see that, if we
use these very low layers in the convolutional network then we kind of match the general... We generally get splotches
of the right colors but the overall spacial structure doesn't get preserved so much. And now, as we move to large
down further in the image and you compute these gram
matrices at higher layers you see that they tend to
reconstruct larger patterns from the input image. For example, these whole rocks
or these whole cranberries. And now, this works pretty well that now we can synthesize
these new images that kind of match the
general spacial statistics of your inputs. But they are quite different pixel wise from the actual input itself. Question? So, the question is, where
do we compute the loss? And in practice, we
want to get good results typically people will
compute gram matrices at many different layers and then the final loss
will be a sum of all those potentially a weighted sum. But I think for this visualization, to try to pin point the
effect of the different layers I think these were doing reconstruction from just one layer. So, now something really... Then, then they had a
really brilliant idea kind of after this paper which is, what if we do this
texture synthesis approach but instead of using an image
like rocks or cranberries what if we set it equal
to a piece of artwork. So then, for example, if you... If you do the same texture
synthesis algorithm by maximizing gram
matrices, but instead of... But now we take, for example, Vincent Van Gogh's Starry night or the Muse by Picasso as our texture... As our input texture,
and then run this same texture synthesis algorithm. Then we can see our generated images tend to reconstruct interesting pieces from those pieces of artwork. And now, something really
interesting happens when you combine this
idea of texture synthesis by gram matrix matching with feature inversion
by feature matching. And then this brings us to
this really cool algorithm called style transfer. So, in style transfer, we're
going to take two images as input. One, we're going to take a content image that will guide like what
type of thing we want. What we generally want
our output to look like. Also, a style image that will tell us what is the general texture or style that we want our generated image to have and then we will jointly
do feature recon... We will generate a new image by minimizing the feature
reconstruction loss of the content image and the gram matrix
loss of the style image. And when we do these two things we a get a really cool image that kind of renders the content image
kind of in the artistic style of the style image. And now this is really cool. And you can get these
really beautiful figures. So again, what this kind of looks like is that you'll take your style
image and your content image pass them into your network
to compute your gram matrices and your features. Now, you'll initialize your output image with some random noise. Go forward, compute your losses go backward, compute your
gradients on the image and repeat this process over and over doing gradient ascent on the
pixels of your generated image. And after a few hundred iterations, generally you'll get a beautiful image. So, I have implementation of this online on my Gethub, that a
lot of people are using. And it's really cool. So, you can, this is kind of... Gives you a lot more control over the generated image
as compared to DeepDream. Right, so in DeepDream, you
don't have a lot of control about exactly what types of
things are going to happen coming out at the end. You just kind of pick different
layers of the networks maybe set different numbers of iterations and then dog slugs pop up everywhere. But with style transfer, you get a lot more fine grain control over what you want the
result to look like. Right, by now, picking
different style images with the same content image you can generate whole
different types of results which is really cool. Also, you can play around with
the hyper parameters here. Right, because we're doing
a joint reconstruct... We're minimizing this
feature reconstruction loss of the content image. And this gram matrix reconstruction
loss of the style image. If you trade off the constant, the waiting between those
two terms and the loss. Then you can get control
about how much we want to match the content versus how much we want to match the style. There's a lot of other hyper
parameters you can play with. For example, if you resize the style image before you compute the gram matrix that can give you some control over what the scale of features are that you want to reconstruct from the style image. So, you can see that here, we've done this same reconstruction the only difference is how
big was the style image before we computed the gram matrix. And this gives you another axis over which you can control these things. You can also actually do style transfer with multiple style images if you just match sort
of multiple gram matrices at the same time. And that's kind of a cool result. We also saw this multi-scale process... So, another cool thing you can do. We talked about this multi-scale
processing for DeepDream and saw how multi scale
processing in DeepDream can give you some really
cool resolution results. And you can do a similar type
of multi-scale processing in style transfer as well. So, then we can compute images like this. That a super high resolution,
this is I think a 4k image of our favorite school, like rendered in the
style of Starry night. But this is actually super
expensive to compute. I think this one took four GPU's. So, a little expensive. We can also other style,
other style images. And get some really cool results from the same content image. Again, at high resolution. Another fun thing you can do is you know, you can actually
do joint style transfer and DeepDream at the same time. So, now we'll have three
losses, the content loss the style loss and this... And this DeepDream loss that
tries to maximize the norm. And get something like this. So, now it's Van Gogh with the dog slug's coming out everywhere. [laughing] So, that's really cool. But there's kind of a problem with this style transfer for algorithms which is that they are pretty slow. Right, you need to produce... You need to compute a lot of
forward and backward passes through your pretrained network in order to complete these images. And especially for these high
resolution results that we saw in the previous slide. Each forward and backward
pass of a 4k image is going to take a lot of
compute and a lot of memory. And if you need to do several
hundred of those iterations generating these images could take many, like tons of minutes even on a powerful GPU. So, it's really not so
practical to apply these things in practice. The solution is to now,
train another neural network to do the style transfer for us. So, I had a paper about this last year and the idea is that we're
going to fix some style that we care about at the beginning. In this case, Starry night. And now rather than running a separate optimization procedure for each image that we want to synthesize instead we're going to train
a single feed forward network that can input the content image and then directly output
the stylized result. And now the way that we train this network is that we compute the same content and style losses during training
of our feed forward network and use that same gradient
to update the weights of the feed forward network. And now this thing takes
maybe a few hours to train but once it's trained, then in order to produce stylized images you just need to do a single forward pass through the trained network. So, I have a code for this online and you can see that it
ends up looking about... Relatively comparable
quality in some cases to this very slow optimization base method but now it runs in real time it's about a thousand times faster. So, here you can see, this is
like a demo of it running live off my webcam. So, this is not running
live right now obviously, but if you have a big GPU you can easily run four different styles in real time all simultaneously because it's so efficient. There was... There was
another group from Russia that had a very similar out... That had a very similar paper concurrently and their results are about as good. They also had this kind
of tweek on the algorithm. So, this feed forward
network that we're training ends up looking a lot like these... These segmentation models that we saw. So, these segmentation networks, for semantic segmentation
we're doing down sampling and then many, and then many layers then some up sampling [mumbling] With transposed convulsion in order to down sample an up sample
to be more efficient. The only difference is
that this final layer produces a three channel output for the RGB of that final image. And inside this network,
we have batch normalization in the various layers. But in this paper, they introduce... They swap out the batch normalization for something else called
instance normalization tends to give you much better results. So, one drawback of these
types of methods is that we're now training one new
style transfer network... For every... For style
that we want to apply. So that could be expensive if now you need to keep a lot of different trained networks around. So, there was a paper from
Google that just came... Pretty recently that addressed this by using one feed forward trained network to apply many different
styles to the input image. So now, they can train one network to apply many different
styles at test time using one trained network. So, here's it's going to
take the content images input as well as the identity of
the style you want to apply and then this is using one network to apply many different types of styles. And again, runs in real time. That same algorithm can also
do this kind of style blending in real time with one trained network. So now, once you trained this network on these four different styles you can actually specify
a blend of these styles to be applied at test
time which is really cool. So, these kinds of real
time style transfer methods are on various apps and
you can see these out in practice a lot now these days. So, kind of the summary
of what we've seen today is that we've talked about
many different methods for understanding CNN representations. We've talked about some of
these activation based methods like nearest neighbor,
dimensionality reduction, maximal patches, occlusion images to try to understand based
on the activation values of what the features are looking for. We also talked about a bunch
of gradient based methods where you can use gradients
to synthesize new images to understand your features
such as saliency maps class visualizations, fooling images, feature inversion. And we also had fun by seeing how a lot of these similar ideas can be applied to things like
Style Transfer and DeepDream to generate really cool images. So, next time, we'll talk
about unsupervised learning Autoencoders, Variational Autoencoders and generative adversarial networks so that should be a fun lecture. 

- Okay we have a lot to cover
today so let's get started. Today we'll be talking
about Generative Models. And before we start, a few
administrative details. So midterm grades will be
released on Gradescope this week A reminder that A3 is
due next Friday May 26th. The HyperQuest deadline for
extra credit you can do this still until Sunday May 21st. And our poster session is
June 6th from 12 to 3 P.M.. Okay so an overview of what
we're going to talk about today we're going to switch gears a little bit and take a look at
unsupervised learning today. And in particular we're going
to talk about generative models which is a type
of unsupervised learning. And we'll look at three
types of generative models. So pixelRNNs and pixelCNNs
variational autoencoders and Generative Adversarial networks. So so far in this class we've
talked a lot about supervised learning and different kinds of supervised learning problems. So in the supervised learning
set up we have our data X and then we have some labels Y. And our goal is to learn
a function that's mapping from our data X to our labels Y. And these labels can take
many different types of forms. So for example, we've
looked at classification where our input is an image and we want to output Y, a
class label for the category. We've talked about object
detection where now our input is still an image but
here we want to output the bounding boxes of instances of
up to multiple dogs or cats. We've talked about semantic
segmentation where here we have a label for every pixel the
category that every pixel belongs to. And we've also talked
about image captioning where here our label is now a sentence and so it's now in the
form of natural language. So unsupervised learning in this set up, it's a type of learning where here we have unlabeled training data and
our goal now is to learn some underlying hidden structure of the data. Right, so an example of
this can be something like clustering which you guys
might have seen before where here the goal is to
find groups within the data that are similar through
some type of metric. For example, K means clustering. Another example of an
unsupervised learning task is a dimensionality reduction. So in this problem want
to find axes along which our training data has the most variation, and so these axes are part
of the underlying structure of the data. And then we can use this
to reduce of dimensionality of the data such that the
data has significant variation among each of the remaining dimensions. Right, so this example
here we start off with data in three dimensions and
we're going to find two axes of variation in this case and reduce our data projected down to 2D. Another example of unsupervised learning is learning feature
representations for data. We've seen how to do this
in supervised ways before where we used the supervised loss, for example classification. Where we have the classification label. We have something like a Softmax loss And we can train a neural network where we can interpret activations for example our FC7 layers as some kind of future representation for the data. And in an unsupervised setting, for example here
autoencoders which we'll talk more about later In this case our loss is now trying to reconstruct the input data to basically, you have a good reconstruction
of our input data and use this to learn features. So we're learning a feature
representation without using any additional external labels. And finally another example
of unsupervised learning is density estimation where
in this case we want to estimate the underlying
distribution of our data. So for example in this top case over here, we have points in 1-d and we can try and fit a Gaussian into this density and in this bottom example
over here it's 2D data and here again we're trying
to estimate the density and we can model this density. We want to fit a model such
that the density is higher where there's more points concentrated. And so to summarize the
differences in unsupervised learning which we've looked a lot so far, we want to use label data to learn a function mapping from X to Y and an unsupervised
learning we use no labels and instead we try to learn
some underlying hidden structure of the data,
whether this is grouping, acts as a variation or
underlying density estimation. And unsupervised learning is a huge and really exciting area of research and and some of the reasons are
that training data is really cheap, it doesn't use labels
so we're able to learn from a lot of data at one time
and basically utilize a lot more data than if we required annotating or finding labels for data. And unsupervised learning
is still relatively unsolved research area by comparison. There's a lot of open problems in this, but it also, it holds the potential of if you're able to successfully learn and represent a lot of
the underlying structure in the data then this also takes you a long way towards the Holy Grail
of trying to understand the structure of the visual world. So that's a little bit of kind
of a high-level big picture view of unsupervised learning. And today will focus more
specifically on generative models which is a class of
models for unsupervised learning where given training
data our goal is to try and generate new samples from
the same distribution. Right, so we have training
data over here generated from some distribution P data and we want to learn a model, P model to generate samples from
the same distribution and so we want to learn P
model to be similar to P data. And generative models
address density estimations. So this problem that we
saw earlier of trying to estimate the underlying
distribution of your training data which is a core problem in unsupervised learning. And we'll see that there's
several flavors of this. We can use generative models
to do explicit density estimation where we're
going to explicitly define and solve for our P model or we can also do implicit
density estimation where in this case we'll
learn a model that can produce samples from P model
without explicitly defining it. So, why do we care
about generative models? Why is this a really
interesting core problem in unsupervised learning? Well there's a lot of
things that we can do with generative models. If we're able to create
realistic samples from the data distributions that we want
we can do really cool things with this, right? We can generate just
beautiful samples to start with so on the left you can
see a completely new samples of just generated by these generative models. Also in the center here
generated samples of images we can also do tasks
like super resolution, colorization so hallucinating
or filling in these edges with generated ideas of colors and what the purse should look like. We can also use generative
models of time series data for simulation and planning
and so this will be useful in for reinforcement learning applications which we'll talk a bit more
about reinforcement learning in a later lecture. And training generative
models can also enable inference of latent representations. Learning latent features
that can be useful as general features for downstream tasks. So if we look at types
of generative models these can be organized
into the taxonomy here where we have these two major
branches that we talked about, explicit density models and
implicit density models. And then we can also get down into many of these other sub categories. And well we can refer to
this figure is adapted from a tutorial on GANs
from Ian Goodfellow and so if you're interested in some of these different taxonomy
and categorizations of generative models this is a
good resource that you can take a look at. But today we're going to
discuss three of the most popular types of generative
models that are in use and in research today. And so we'll talk first briefly
about pixelRNNs and CNNs And then we'll talk about
variational autoencoders. These are both types of
explicit density models. One that's using a tractable density and another that's using
an approximate density And then we'll talk about
generative adversarial networks, GANs which are a type of
implicit density estimation. So let's first talk
about pixelRNNs and CNNs. So these are a type of fully
visible belief networks which are modeling a density explicitly so in this case what
they do is we have this image data X that we have
and we want to model the probability or likelihood
of this image P of X. Right and so in this case,
for these kinds of models, we use the chain rule to
decompose this likelihood into a product of one
dimensional distribution. So we have here the
probability of each pixel X I conditioned on all previous
pixels X1 through XI - 1. and your likelihood all
right, your joint likelihood of all the pixels in your image
is going to be the product of all of these pixels together, all of these likelihoods together. And then once we define this likelihood, in order to train this
model we can just maximize the likelihood of our training data under this defined density. So if we look at this this
distribution over pixel values right, we have this P of
XI given all the previous pixel values, well this is a
really complex distribution. So how can we model this? Well we've seen before that
if we want to have complex transformations we can do
these using neural networks. Neural networks are a good
way to express complex transformations. And so what we'll do is
we'll use a neural network to express this complex
function that we have of the distribution. And one thing you'll see here is that, okay even if we're going to
use a neural network for this another thing we have to take
care of is how do we order the pixels. Right, I said here that
we have a distribution for P of XI given all previous pixels but what does all
previous the pixels mean? So we'll take a look at that. So PixelRNN was a model proposed in 2016 that basically defines a way
for setting up and optimizing this problem and so
how this model works is that we're going to
generate pixels starting in a corner of the image. So we can look at this grid
as basically the pixels of your image and so what
we're going to do is start from the pixel in the
upper left-hand corner and then we're going to
sequentially generate pixels based on these connections from the arrows that you can see here. And each of the dependencies
on the previous pixels in this ordering is going
to be modeled using an RNN or more specifically an
LSTM which we've seen before in lecture. Right so using this we can
basically continue to move forward just moving
down a long is diagonal and generating all of these
pixel values dependent on the pixels that they're connected to. And so this works really
well but the drawback here is that this sequential generation, right, so it's actually quite slow to do this. You can imagine you know if
you're going to generate a new image instead of all of these
feed forward networks that we see, we've seen with CNNs. Here we're going to have
to iteratively go through and generate all these
images, all these pixels. So a little bit later, after a pixelRNN, another model called
pixelCNN was introduced. And this has very
similar setup as pixelCNN and we're still going to
do this image generation starting from the corner of
the of the image and expanding outwards but the difference now
is that now instead of using
 
255
00:12:43,074 --> 00:12:45,480
an RNN to model all these dependencies we're going to use the CNN instead. And we're now going to use a
CNN over a a context region that you can see here around
in the particular pixel that we're going to generate now. Right so we take the pixels around it, this gray area within the
region that's already been generated and then we can
pass this through a CNN and use that to generate
our next pixel value. And so what this is going to
give is this is going to give This is a CNN, a neural
network at each pixel location right and so the output of
this is going to be a soft max loss over the pixel values here. In this case we have a 0 to
255 and then we can train this by maximizing the likelihood
of the training images. Right so we say that basically
we want to take a training image we're going to do
this generation process and at each pixel location
we have the ground truth training data image
value that we have here and this is a quick basically the label or the the the classification
label that we want our pixel to be which of these 255 values and we can train this
using a Softmax loss. Right and so basically
the effect of doing this is that we're going to
maximize the likelihood of our training data
pixels being generated. Okay any questions about this? Yes. [student's words obscured
due to lack of microphone] Yeah, so the question is,
I thought we were talking about unsupervised learning,
why do we have basically a classification label here? The reason is that this loss,
this output that we have is the value of the input training data. So we have no external labels, right? We didn't go and have to
manually collect any labels for this, we're just taking our input data and saying that this is what
we used for the last function. [student's words obscured
due to lack of microphone] The question is, is
this like bag of words? I would say it's not really bag of words, it's more saying that we
want where we're outputting a distribution over pixel
values at each location of our image right, and what we want to do is we want to maximize the
likelihood of our input, our training data being
produced, being generated. Right so, in that sense, this
is why it's using our input data to create our loss. So using pixelCNN training
is faster than pixelRNN because here now right
at every pixel location we want to maximize the value of our, we want to maximize the
likelihood of our training data showing up and so we have all
of these values already right, just from our training data
and so we can do this much faster but a generation time
for a test time we want to generate a completely new
image right, just starting from the corner and we're not,
we're not trying to do any type of learning so in that
generation time we still have to generate each
of these pixel locations before we can generate the next location. And so generation time here
it still slow even though training time is faster. Question. [student's words obscured
due to lack of microphone] So the question is, is
this training a sensitive distribution to what you
pick for the first pixel? Yeah, so it is dependent on
what you have as the initial pixel distribution and then
everything is conditioned based on that. So again, how do you
pick this distribution? So at training time you
have these distributions from your training data
and then at generation time you can just initialize
this with either uniform or from your training
data, however you want. And then once you have that
everything else is conditioned based on that. Question. [student's words obscured
due to lack of microphone] Yeah so the question is is
there a way that we define this in this chain rule
fashion instead of predicting all the pixels at one time? And so we'll see, we'll see
models later that do do this, but what the chain rule allows
us to do is it allows us to find this very tractable
density that we can then basically optimize and do,
directly optimizes likelihood Okay so these are some
examples of generations from this model and so here
on the left you can see generations where the
training data is CIFAR-10, CIFAR-10 dataset. And so you can see that in
general they are starting to capture statistics of natural images. You can see general types of blobs and kind of things that look
like parts of natural images coming out. On the right here it's ImageNet,
we can again see samples from here and these are starting to
look like natural images but they're still not, there's
still room for improvement. You can still see that there
are differences obviously with regional training images
and some of the semantics are not clear in here. So, to summarize this,
pixelRNNs and CNNs allow you to explicitly compute likelihood P of X. It's an explicit density
that we can optimize. And being able to do this
also has another benefit of giving a good evaluation metric. You know you can kind of measure
how good your samples are by this likelihood of the
data that you can compute. And it's able to produce
pretty good samples but it's still an active area of research and the main disadvantage
of these methods is that the generation is sequential
and so it can be pretty slow. And these kinds of methods
have also been used for generating audio for example. And you can look online for
some pretty interesting examples of this, but again the drawback
is that it takes a long time to generate these samples. And so there's a lot of work,
has been work since then on still on improving pixelCNN performance And so all kinds of different
you know architecture changes add the loss function
formulating this differently on different types of training tricks And so if you're interested
in learning more about this you can look at some of
these papers on PixelCNN and then other pixelCNN plus
plus better improved version that came out this year. Okay so now we're going
to talk about another type of generative models call
variational autoencoders. And so far we saw that
pixelCNNs defined a tractable density function, right,
using this this definition and based on that we can
optimize directly optimize the likelihood of the training data. So with variational autoencoders
now we're going to define an intractable density function. We're now going to model this
with an additional latent variable Z and we'll talk in more detail about how this looks. And so our data likelihood
P of X is now basically has to be this integral right, taking the expectation over
all possible values of Z. And so this now is going to be a problem. We'll see that we cannot
optimize this directly. And so instead what we have
to do is we have to derive and optimize a lower bound
on the likelihood instead. Yeah, question. So the question is is what is Z? Z is a latent variable
and I'll go through this in much more detail. So let's talk about some background first. Variational autoencoders
are related to a type of unsupervised learning
model called autoencoders. And so we'll talk little bit
more first about autoencoders and what they are and then
I'll explain how variational autoencoders are related
and build off of this and allow you to generate data. So with autoencoders we don't
use this to generate data, but it's an unsupervised
approach for learning a lower dimensional feature representation from unlabeled training data. All right so in this case
we have our input data X and then we're going to
want to learn some features that we call Z. And then we'll have an encoder
that's going to be a mapping, a function mapping
from this input data to our feature Z. And this encoder can take
many different forms right, they would generally use
neural networks so originally these models have been
around, autoencoders have been around for a long time. So in the 2000s we used linear
layers of non-linearities, then later on we had fully
connected deeper networks and then after that we moved
on to using CNNs for these encoders. So we take our input data
X and then we map this to some feature Z. And Z we usually have as,
we usually specify this to be smaller than X and we
perform basically dimensionality reduction because of that. So the question who has an
idea of why do we want to do dimensionality reduction here? Why do we want Z to be smaller than X? Yeah. [student's words obscured
due to lack of microphone] So the answer I heard is Z
should represent the most important features in
X and that's correct. So we want Z to be able to
learn features that can capture meaningful factors of
variation in the data. Right this makes them good features. So how can we learn this
feature representation? Well the way autoencoders
do this is that we train the model such that the features
can be used to reconstruct our original data. So what we want is we want to
have input data that we use an encoder to map it to some
lower dimensional features Z. This is the output of the encoder network, and we want to be able to
take these features that were produced based on this input
data and then use a decoder a second network and be
able to output now something of the same size dimensionality
as X and have it be similar to X right so we want to be
able to reconstruct the original data. And again for the decoder we
are basically using same types of networks as encoders so
it's usually a little bit symmetric and now we can use CNN networks for most of these. Okay so the process is going
to be we're going to take our input data right we pass
it through our encoder first which is going to be something
for example like a four layer convolutional network and
then we're going to pass it, get these features and then
we're going to pass it through a decoder which is a four layer
for example upconvolutional network and then get a
reconstructed data out at the end of this. Right in the reason why we
have a convolutional network for the encoder and an
upconvolutional network for the decoder is because at
the encoder we're basically taking it from this high
dimensional input to these lower dimensional features and now
we want to go the other way go from our low dimensional
features back out to our high dimensional reconstructed input. And so in order to get this
effect that we said we wanted before of being able to
reconstruct our input data we'll use something like
an L2 loss function. Right that basically just
says let me make my pixels of my input data to be the same as my, my pixels in my reconstructed
data to be the same as the pixels of my input data. An important thing to notice here, this relates back to a
question that we had earlier, is that even though we have
this loss function here, there's no, there's no external
labels that are being used in training this. All we have is our training
data that we're going to use both to pass through the
network as well as to compute our loss function. So once we have this
after training this model what we can do is we can
throw away this decoder. All this was used was too
to be able to produce our reconstruction input and
be able to compute our loss function. And we can use the encoder
that we have which produces our feature mapping and we
can use this to initialize a supervised model. Right and so for example we
can now go from this input to our features and then
have an additional classifier network on top of this that
now we can use to output a class label for example for
classification problem we can have external labels from here and use our standard loss
functions like Softmax. And so the value of this is
that we basically were able to use a lot of unlabeled
training data to try and learn good general feature representations. Right, and now we can use this
to initialize a supervised learning problem where sometimes
we don't have so much data we only have small data. And we've seen in previous
homeworks and classes that with small data it's
hard to learn a model, right? You can have over fitting
and all kinds of problems and so this allows you to
initialize your model first with better features. Okay so we saw that autoencoders
are able to reconstruct data and are able to, as
a result, learn features to initialize, that we can
use to initialize a supervised model. And we saw that these
features that we learned have this intuition of being
able to capture factors of variation in the training data. All right so based on this
intuition of okay these, we can have this latent
this vector Z which has factors of variation in our training data. Now a natural question is
well can we use a similar type of setup to generate new images? And so now we will talk about
variational autoencoders which is a probabillstic spin
on autoencoders that will let us sample from the model in
order to generate new data. Okay any questions on autoencoders first? Okay, so variational autoencoders. All right so here we assume
that our training data that we have X I from one to N is generated from some
underlying, unobserved latent representation Z. Right, so it's this intuition
that Z is some vector right which element of Z
is capturing how little or how much of some factor
of variation that we have in our training data. Right so the intuition is,
you know, maybe these could be something like different
kinds of attributes. Let's say we're trying to generate faces, it could be how much of
a smile is on the face, it could be position of the eyebrows hair orientation of the head. These are all possible
types of latent factors that could be learned. Right, and so our generation
process is that we're going to sample from a prior over Z. Right so for each of these
attributes for example, you know, how much smile that there is, we can have a prior over
what sort of distribution we think that there should be for this so, a gaussian is something
that's a natural prior that we can use for each
of these factors of Z and then we're going
to generate our data X by sampling from a conditional,
conditional distribution P of X given Z. So we sample Z first, we sample
a value for each of these latent factors and then we'll use that and sample our image X from here. And so the true parameters
of this generation process are theta, theta star right? So we have the parameters of our prior and our conditional distributions and what we want to do is in
order to have a generative model be able to generate new data we want to estimate these
parameters of our true parameters Okay so let's first talk
about how should we represent this model. All right, so if we're going to
have a model for this generator process, well we've already
said before that we can choose our prior P of Z to be something simple. Something like a Gaussian, right? And this is the reasonable
thing to choose for for latent attributes. Now for our conditional
distribution P of X given Z this is much more complex right, because we need to use
this to generate an image and so for P of X given
Z, well as we saw before, when we have some type of
complex function that we want to represent we can represent
this with a neural network. And so that's a natural
choice for let's try and model P of X given Z with a neural network. And we're going to call
this the decoder network. Right, so we're going to
think about taking some latent representation and trying to
decode this into the image that it's specifying. So now how can we train this model? Right, we want to be able to
train this model so that we can learn an estimate of these parameters. So if we remember our strategy
from training generative models, back from are fully
visible belief networks, our pixelRNNs and CNNs, a straightforward natural
strategy is to try and learn these model
parameters in order to maximize the likelihood of the training data. Right, so we saw earlier
that in this case, with our latent variable
Z, we're going to have to write out P of X taking
expectation over all possible values of Z which is
continuous and so we get this expression here. Right so now we have it with this latent Z and now if we're going to, if
you want to try and maximize its likelihood, well what's the problem? Can we just take this take
gradients and maximize this likelihood? [student's words obscured
due to lack of microphone] Right, so this integral is
not going to be tractable, that's correct. So let's take a look at this
in a little bit more detail. Right, so we have our
data likelihood term here. And the first time is P of Z. And here we already said
earlier, we can just choose this to be a simple Gaussian
prior, so this is fine. P of X given Z, well we
said we were going to specify a decoder neural network. So given any Z, we can get
P of X given Z from here. It's the output of our neural network. But then what's the problem here? Okay this was supposed to
be a different unhappy face but somehow I don't know what happened, in the process of translation, it turned into a crying black ghost but what this is symbolizing
is that basically if we want to compute P of X given Z for every Z this is now intractable right, we cannot compute this integral. So data likelihood is intractable and it turns out that if
we look at other terms in this model if we look
at our posterior density, So P of our posterior of Z given X, then this is going to be P of X given Z times P of Z over P of X by Bayes' rule and this is also going
to be intractable, right. We have P of X given Z
is okay, P of Z is okay, but we have this P of X our likelihood which has the integral
and it's intractable. So we can't directly optimizes this. but we'll see that a solution, a solution that will enable
us to learn this model is if in addition to
using a decoder network defining this neural network
to model P of X given Z. If we now define an
additional encoder network Q of Z given X we're going
to call this an encoder because we want to turn our input X into, get the likelihood of Z given X, we're going to encode this into Z. And defined this network to approximate the P of Z given X. Right this was posterior
density term now is also intractable. If we use this additional
network to approximate this then we'll see that this will
actually allow us to derive a lower bound on the data
likelihood that is tractable and which we can optimize. Okay so first just to be a
little bit more concrete about these encoder and decoder
networks that I specified, in variational autoencoders we
want the model probabilistic generation of data. So in autoencoders we already talked about this concept of having
an encoder going from input X to some feature Z and a
decoder network going from Z back out to some image X. And so here we go to again
have an encoder network and a decoder network but we're going to make these probabilistic. So now our encoder network
Q of Z given X with parameters phi are going to output a mean and a diagonal covariance and from here, this will be the direct
outputs of our encoder network and the same thing for our decoder network which
is going to start from Z and now it's going to output the mean and the diagonal covariance of some X, same dimension as the input given Z And then this decoder network
has different parameters theta. And now in order to
actually get our Z and our, This should be Z given X and X given Z. We'll sample from these distributions. So now our encoder and our decoder network are producing distributions
over Z and X respectively and will sample from this distribution in order to get a value from here. So you can see how this is
taking us on the direction towards being able to sample
and generate new data. And just one thing to note is that these encoder and decoder networks, you'll also hear different terms for them. The encoder network can
also be kind of recognition or inference network because we're trying to form
inference of this latent representation of Z given
X and then for the decoder network, this is what we'll
use to perform generation. Right so you also hear
generation network being used. Okay so now equipped with our
encoder and decoder networks, let's try and work out
the data likelihood again. and we'll use the log of
the data likelihood here. So we'll see that if we
want the log of P of X right we can write this out as like a P of X but take the expectation with respect to Z. So Z samples from our distribution of Q of Z given
X that we've now defined using the encoder network. And we can do this because
P of X doesn't depend on Z. Right 'cause Z is not part of that. And so we'll see that taking
the expectation with respect to Z is going to come in handy later on. Okay so now from this
original expression we can now expand it out to be
log of P of X given Z, P of Z over P of Z given
X using Bayes' rule. And so this is just
directly writing this out. And then taking this we
can also now multiply it by a constant. Right, so Q of Z given
X over Q of Z given X. This is one we can do this. It doesn't change it but it's
going to be helpful later on. So given that what we'll
do is we'll write it out into these three separate terms. And you can work out this
math later on by yourself but it's essentially just
using logarithm rules taking all of these
terms that we had in the line above and just separating it out into these three different terms
that will have nice meanings. Right so if we look at this,
the first term that we get separated out is log of P
given X and then expectation of log of P given X and
then we're going to have two KL terms, right. This is basically KL divergence term to say how close these two distributions are. So how close is a distribution
Q of Z given X to P of Z. So it's just the, it's exactly
this expectation term above. And it's just a distance
metric for distributions. And so we'll see that,
right, we saw that these are nice KL terms that we can write out. And now if we look at these
three terms that we have here, the first term is P of X
given Z, which is provided by our decoder network. And we're able to compute
an estimate of these term through sampling and we'll see that we can do a sampling that's
differentiable through something called the re-parametrization
trick which is a detail that you can look
at this paper if you're interested. But basically we can
now compute this term. And then these KL terms,
the second KL term is a KL between two Gaussians, so our Q of Z given X,
remember our encoder produced this distribution which had
a mean and a covariance, it was a nice Gaussian. And then also our prior P of
Z which is also a Gaussian. And so this has a nice, when you have a KL of two Gaussians you have
a nice closed form solution that you can have. And then this third KL term now, this is a KL of Q given
X with a P of Z given X. But we know that P of Z
given X was this intractable posterior that we saw earlier, right? That we didn't want to
compute that's why we had this approximation using Q. And so this term is still is a problem. But one thing we do know
about this term is that KL divergence, it's a distance
between two distributions is always greater than or
equal to zero by definition. And so what we can do with this is that, well what we have here, the
two terms that we can work nicely with, this is a, this is a tractable lower
bound which we can actually take gradient of and optimize. P of X given Z is
differentiable and the KL terms are also, the close form
solution is also differentiable. And this is a lower bound
because we know that the KL term on the right, the
ugly one is greater than or equal it zero. So we have a lower bound. And so what we'll do to train
a variational autoencoder is that we take this
lower bound and we instead optimize and maximize
this lower bound instead. So we're optimizing a lower
bound on the likelihood of our data. So that means that our data
is always going to have a likelihood that's at
least as high as this lower bound that we're maximizing. And so we want to find
the parameters theta, estimate parameters theta
and phi that allows us to maximize this. And then one last sort of
intuition about this lower bound that we have is that this first term is expectation over all samples of Z sampled from passing our X
through the encoder network sampling Z taking expectation
over all of these samples of likelihood of X given Z and so this is a reconstruction, right? This is basically saying,
if I want this to be big I want this likelihood P
of X given Z to be high, so it's kind of like
trying to do a good job reconstructing the data. So similar to what we had
from our autoencoder before. But the second term here is
saying make this KL small. Make our approximate
posterior distribution close to our prior distribution. And this basically is
saying that well we want our latent variable Z to be following this, have this distribution
type, distribution shape that we would like it to have. Okay so any questions about this? I think this is a lot
of math that if you guys are interested you should go
back and kind of work through all of the derivations yourself. Yeah. [student's words obscured
due to lack of microphone] So the question is why
do we specify the prior and the latent variables as Gaussian? And the reason is that well we're defining some sort of generative process right, of sampling Z first and
then sampling X first. And defining it as a
Gaussian is a reasonable type of prior that we can say
makes sense for these types of latent attributes to
be distributed according to some sort of Gaussian, and
then this lets us now then optimize our model. Okay, so we talked about how
we can deride this lower bound and now let's put this all
together and walk through the process of the training of the AE. Right so here's the bound
that we want to optimize, to maximize. And now for a forward pass. We're going to proceed
in the following manner. We have our input data
X, so we'll a mini batch of input data. And then we'll pass it
through our encoder network so we'll get Q of Z given X. And from this Q of Z given
X, this'll be the terms that we use to compute the KL term. And then from here we'll
sample Z from this distribution of Z given X so we have a
sample of the latent factors that we can infer from X. And then from here we're
going to pass a Z through another, our second decoder network. And from the decoder network
we'll get this output for the mean and variance
on our distribution for X given Z and then
finally we can sample now our X given Z from this distribution and here this will produce
some sample output. And when we're training
we're going to take this distribution and say well
our loss term is going to be log of our training image
pixel values given Z. So our loss functions going
to say let's maximize the likelihood of this original
input being reconstructed. And so now for every mini batch of input we're going to compute this forward pass. Get all these terms that we need and then this is all
differentiable so then we just backprop though all of this
and then get our gradient, we update our model and
we use this to continuously update our parameters,
our generator and decoder network parameters theta
and phi in order to maximize the likelihood of the trained data. Okay so once we've trained our VAE, so now to generate data,
what we can do is we can use just the decoder network. All right, so from here
we can sample Z now, instead of sampling Z from
this posterior that we had during training, while
during generation we sample from our true generative process. So we sample from our
prior that we specify. And then we're going to then
sample our data X from here. And we'll see that this
can produce, in this case, train on MNIST, these are
samples of digits generated from a VAE trained on MNIST. And you can see that, you
know, we talked about this idea of Z representing these
latent factors where we can bury Z right according to
our sample from different parts of our prior and
then get different kind of interpretable meanings from here. So here we can see that this is the data manifold for two dimensional Z. So if we have a two dimensional
Z and we take Z and let's say some range from you know,
from different percentiles of the distribution, and
we vary Z1 and we vary Z2, then you can see how the
image generated from every combination of Z1 and
Z2 that we have here, you can see it's transitioning
smoothly across all of these different variations. And you know our prior on
Z was, it was diagonal, so we chose this in order
to encourage this to be independent latent variables
that can then encode interpretable factors of variation. So because of this now we'll
have different dimensions of Z, encoding different
interpretable factors of variation. So, in this example train now on Faces, we'll see as we vary
Z1, going up and down, you'll see the amount of smile changing. So from a frown at the
top to like a big smile at the bottom and then as we go vary Z2, from left to right, you can
see the head pose changing. From one direction all
the way to the other. And so one additional
thing I want to point out is that as a result of doing this, these Z variables are also good feature representations. Because they encode how
much of these different these different interpretable
semantics that we have. And so we can use our Q of Z given X, the encoder that we've
learned and give it an input images X, we can map this
to Z and use the Z as features that we can
use for downstream tasks like supervision, or
like classification or other tasks. Okay so just another
couple of examples of data generated from VAEs. So on the left here we have
data generated on CIFAR-10, trained on CIFAR-10, and
then on the right we have data trained and generated on Faces. And we'll see so we can
see that in general VAEs are able to generate recognizable data. One of the main drawbacks
of VAEs is that they tend to still have a bit of
a blurry aspect to them. You can see this in the
faces and so this is still an active area of research. Okay so to summarize VAEs, they're a probabilistic spin
on traditional autoencoders. So instead of deterministically
taking your input X and going to Z, feature Z and
then back to reconstructing X, now we have this idea of
distributions and sampling involved which allows us to generate data. And in order to train
this, VAEs are defining an intractable density. So we can derive and
optimize a lower bound, a variational lower bound, so
variational means basically using approximations to handle
these types of intractable expressions. And so this is why this is
called a variational autoencoder. And so some of the
advantages of this approach is that VAEs are, they're
a principled approach to generative models and they
also allow this inference query so being able to infer
things like Q of Z given X. That we said could be useful
feature representations for other tasks. So disadvantages of VAEs are
that while we're maximizing the lower bound of the
likelihood, which is okay like you know in general this
is still pushing us in the right direction and there's more other theoretical analysis of this. So you know, it's doing okay,
but it's maybe not still as direct an optimization
and evaluation as the pixel RNNs and CNNs that we saw earlier, but which had, and then, also the VAE samples are
tending to be a little bit blurrier and of lower quality
compared to state of the art samples that we can see
from other generative models such as GANs that we'll talk about next. And so VAEs now are still,
they're still an active area of research. People are working on more
flexible approximations, so richer approximate posteriors, so instead of just a
diagonal Gaussian some richer functions for this. And then also, another area
that people have been working on is incorporating more
structure in these latent variables. So now we had all of these
independent latent variables but people are working on
having modeling structure in here, groupings,
other types of structure. Okay, so yeah, question. [student's words obscured
due to lack of microphone] Yeah, so the question is we're
deciding the dimensionality of the latent variable. Yeah, that's something that you specify. Okay, so we've talked so
far about pixelCNNs and VAEs and now we'll take a look
at a third and very popular type of generative model called GANs. So the models that we've seen
so far, pixelCNNs and RNNs define a tractable density function. And they optimize the
likelihood of the trained data. And then VAEs in contrast to
that now have this additional latent variable Z that they
define in the generative process. And so having the Z has
a lot of nice properties that we talked about, but
they are also cause us to have this intractable density
function that we can't optimize directly and so
we derive and optimize a lower bound on the likelihood instead. And so now what if we
just give up on explicitly modeling this density at all? And we say well what we
want is just the ability to sample and to have nice
samples from our distribution. So this is the approach that GANs take. So in GANs we don't work with
an explicit density function, but instead we're going to
take a game-theoretic approach and we're going to learn to
generate from our training distribution through a set
up of a two player game, and we'll talk about this in more detail. So, in the GAN set up we're
saying, okay well what we want, what we care about is we
want to be able to sample from a complex high dimensional
training distribution. So if we think about well
we want to produce samples from this distribution,
there's no direct way that we can do this. We have this very complex distribution, we can't just take samples from here. So the solution that we're
going to take is that we can, however, sample from
simpler distributions. For example random noise, right? Gaussians are, these we can sample from. And so what we're going to
do is we're going to learn a transformation from
these simple distributions directly to the training
distribution that we want. So the question, what can we
used to represent this complex distribution? Neural network, I heard the answer. So when we want to model
some kind of complex function or transformation we use a neural network. Okay so what we're going to
do is we're going to take in the GAN set up, we're
going to take some input which is a vector of some
dimension that we specify of random noise and then we're
going to pass this through a generator network, and then
we're going to get as output directly a sample from
the training distribution. So every input of random
noise we want to correspond to a sample from the training distribution. And so the way we're going to
train and learn this network is that we're going to look
at this as a two player game. So we have two players, a
generator network as well as an additional discriminator
network that I'll show next. And our generator network is
going to try to, as player one, it's going to try to fool the
discriminator by generating real looking images. And then our second player,
our discriminator network is then going to try to
distinguish between real and fake images. So it wants to do as good
a job as possible of trying to determine which of these
images are counterfeit or fake images generated
by this generator. Okay so what this looks like is, we have our random noise going
to our generator network, generator network is generating
these images that we're going to call, they're
fake from our generator. And then we're going to also
have real images that we take from our training
set and then we want the discriminator to be able
to distinguish between real and fake images. Output real and fake for each images. So the idea is if we're
able to have a very good discriminator, we want to
train a good discriminator, if it can do a good job of
discriminating real versus fake, and then if our generator
network is able to generate, if it's able to do well
and generate fake images that can successfully
fool this discriminator, then we have a good generative model. We're generating images that
look like images from the training set. Okay, so we have these two
players and so we're going to train this jointly in a
minimax game formulation. So this minimax objective
function is what we have here. We're going to take, it's going
to be minimum over theta G our parameters of our generator network G, and maximum over parameter Zeta
of our Discriminator network D, of this objective, right, these terms. And so if we look at these
terms, what this is saying is well this first thing,
expectation over data of log of D given X. This log of D of X is
the discriminator output for real data X. This is going to be likelihood
of real data being real from the data distribution P data. And then the second term
here, expectation of Z drawn from P of Z, Z drawn from
P of Z means samples from our generator network and
this term D of G of Z that we have here is the output
of our discriminator for generated fake data for our, what does the discriminator
output of G of Z which is our fake data. And so if we think about
this is trying to do, our discriminator wants to
maximize this objective, right, it's a max over theta D such
that D of X is close to one. It's close to real, it's
high for the real data. And then D of G of X, what
it thinks of the fake data on the left here is small, we
want this to be close to zero. So if we're able to maximize
this, this means discriminator is doing a good job of
distinguishing between real and zero. Basically classifying
between real and fake data. And then our generator, here
we want the generator to minimize this objective such
that D of G of Z is close to one. So if this D of G of Z is
close to one over here, then the one minus side is
small and basically we want to, if we minimize this term
then, then it's having discriminator think that our
fake data's actually real. So that means that our generator
is producing real samples. Okay so this is the
important objective of GANs to try and understand so are
there any questions about this? [student's words obscured
due to lack of microphone] I'm not sure I understand
your question, can you, [student's words obscured
due to lack of microphone] Yeah, so the question is
is this basically trying to have the first network
produce real looking images that our second network,
the discriminator cannot distinguish between. Okay, so the question is how
do we actually label the data or do the training for these networks. We'll see how to train the networks next. But in terms of like what
is the data label basically, this is unsupervised, so
there's no data labeling. But data generated from
the generator network, the fake images have a label
of basically zero or fake. And we can take training
images that are real images and this basically has
a label of one or real. So when we have, the loss
function for our discriminator is using this. It's trying to output a zero
for the generator images and a one for the real images. So there's no external labels. [student's words obscured
due to lack of microphone] So the question is the label
for the generator network will be the output for
the discriminator network. The generator is not really doing, it's not really doing
classifications necessarily. What it's objective is
is here, D of G of Z, it wants this to be high. So given a fixed discriminator,
it wants to learn the generator parameter
such that this is high. So we'll take the fixed
discriminator output and use that to do the backprop. Okay so in order to train
this, what we're going to do is we're going to alternate
between gradient ascent on our discriminator, so we're
trying to learn theta beta to maximizing this objective. And then gradient
descent on the generator. So taking gradient ascent
on these parameters theta G such that we're minimizing
this and this objective. And here we are only taking
this right part over here because that's the only
part that's dependent on theta G parameters. Okay so this is how we can train this GAN. We can alternate between
training our discriminator and our generator in this
game, each trying to fool the other or generator trying
to fool the discriminator. But one thing that is important
to note is that in practice this generator objective as
we've just defined actually doesn't work that well. And the reason for this is
we have to look at the loss landscape. So if we look at the loss
landscape over here for D of G of X, if we apply here one minus D of G of X which is what we want to
minimize for the generator, it has this shape here. So we want to minimize this
and it turns out the slope of this loss is actually going
to be higher towards the right. High when D of G of Z is closer to one. So that means that when our
generator is doing a good job of fooling the discriminator,
we're going to have a high gradient, more
higher gradient terms. And on the other hand
when we have bad samples, our generator has not
learned a good job yet, it's not good at generating yet, then this is when the
discriminator can easily tell it's now closer to this
zero region on the X axis. Then here the gradient's relatively flat. And so what this actually
means is that our our gradient signal is
dominated by region where the sample is already pretty good. Whereas we actually want it to
learn a lot when the samples are bad, right? These are training samples
that we want to learn from. And so in order to, so this
basically makes it hard to learn and so in order
to improve learning, what we're going to do
is define a different, slightly different objective
function for the gradient. Where now we're going to
do gradient ascent instead. And so instead of minimizing
the likelihood of our discriminator being correct,
which is what we had earlier, now we'll kind of flip
it and say let's maximize the likelihood of our
discriminator being wrong. And so this will produce this
objective here of maximizing, maximizing log of D of G of X. And so, now basically we want to, there should be a negative sign here. But basically we want to now
maximize this flip objective instead and what this now does
is if we plot this function on the right here, then we
have a high gradient signal in this region on the left
where we have bad samples, and now the flatter region
is to the right where we would have good samples. So now we're going to
learn more from regions of bad samples. And so this has the same
objective of fooling the discriminator but it
actually works much better in practice and for a lot
of work on GANs that are using these kind of
vanilla GAN formulation is actually using this objective. Okay so just an aside on
that is that jointly training these two networks is
challenging and can be unstable. So as we saw here, like
we're alternating between training a discriminator
and training a generator. This type of alternation is,
basically it's hard to learn two networks at once and
there's also this issue of depending on what our
loss landscape looks at, it can affect our training dynamics. So an active area of research
still is how can we choose objectives with better loss
landscapes that can help training and make it more stable? Okay so now let's put this
all together and look at the full GAN training algorithm. So what we're going to do is
for each iteration of training we're going to first train the generation, train the discriminator network a bit and then train the generator network. So for k steps of training
the discriminator network we'll sample a mini batch
of noise samples from our noise prior Z and then
also sample a mini batch of real samples from our training data X. So what we'll do is we'll
pass the noise through our generator, we'll get our fake images out. So we have a mini batch of
fake images and mini batch of real images. And then we'll pick a gradient
step on the discriminator using this mini batch, our
fake and our real images and then update our
discriminator parameters. And use this and do this a
certain number of iterations to train the discriminator
for a bit basically. And then after that we'll
go to our second step which is training the generator. And so here we'll sample just
a mini batch of noise samples. We'll pass this through our
generator and then now we want to do backpop on this
to basically optimize our generator objective that we saw earlier. So we want to have our
generator fool our discriminator as much as possible. And so we're going to alternate
between these two steps of taking gradient steps
for our discriminator and for the generator. And I said for k steps up here, for training the discriminator
and so this is kind of a topic of debate. Some people think just having
one iteration of discriminator one type of discriminator,
one type of generator is best. Some people think it's better
to train the discriminator for a little bit longer before
switching to the generator. There's no real clear rule
and it's something that people have found different
things to work better depending on the problem. And one thing I want to point
out is that there's been a lot of recent work that
alleviates this problem and makes it so you don't
have to spend so much effort trying to balance how the
training of these two networks. It'll have more stable training
and give better results. And so Wasserstein GAN
is an example of a paper that was an important
work towards doing this. Okay so looking at the whole
picture we've now trained, we have our network setup,
we've trained both our generator network and
our discriminator network and now after training for generation, we can just take our generator
network and use this to generate new images. So we just take noise Z and
pass this through and generate fake images from here. Okay and so now let's look
at some generated samples from these GANs. So here's an example of trained on MNIST and then on the right on Faces. And for each of these you can also see, just for visualization
the closest, on the right, the nearest neighbor from the
training set to the column right next to it. And so you can see that
we're able to generate very realistic samples and
it never directly memorizes the training set. And here are some examples
from the original GAN paper on CIFAR images. And these are still fairly,
not such good quality yet, these were, the original
work is from 2014, so these are some older, simpler networks. And these were using simple,
fully connected networks. And so since that time
there's been a lot of work on improving GANs. One example of a work that
really took a big step towards improving the quality
of samples is this work from Alex Radford in ICLR
2016 on adding convolutional architectures to GANs. In this paper there was
a whole set of guidelines on architectures for helping
GANs to produce better samples. So you can look at this for more details. This is an example of a
convolutional architecture that they're using which
is going from our input Z noise vector Z and
transforming this all the way to the output sample. So now from this large
convolutional architecture we'll see that the samples
from this model are really starting to look very good. So this is trained on
a dataset of bedrooms and we can see all kinds of
very realistic fancy looking bedrooms with windows and night
stands and other furniture around there so these are
some really pretty samples. And we can also try and
interpret a little bit of what these GANs are doing. So in this example here what
we can do is we can take two points of Z, two
different random noise vectors and let's just interpolate
between these points. And each row across here
is an interpolation from one random noise Z to
another random noise vector Z and you can see that as it's changing, it's smoothly interpolating
the image as well all the way over. And so something else that
we can do is we can see that, well, let's try to analyze
further what these vectors Z mean, and so we can try
and do vector math on here. So what this experiment does is it says okay, let's take some images of smiling, samples of smiling women
images and then let's take some samples of neutral women
and then also some samples of neutral men. And so let's try and do take
the average of the Z vectors that produced each of
these samples and if we, Say we take this, mean
vector for the smiling women, subtract the mean vector
for the neutral women and add the mean vector
for the neutral man, what do we get? And we get samples of smiling man. So we can take the Z
vector produced there, generate samples and get
samples of smiling men. And we can have another example of this. Of glasses man minus no glasses
man and plus glasses women. And get women with glasses. So here you can see that
basically the Z has this type of interpretability that
you can use this to generate some pretty cool examples. Okay so this year, 2017 has really been the year of the GAN. There's been tons and tons of work on GANs and it's really sort of
exploded and gotten some really cool results. So on the left here you
can see people working on better training and generation. So we talked about improving
the loss functions, more stable training and this
was able to get really nice generations here of different
types of architectures on the bottom here really
crisp high resolution faces. With GANs you can also do,
there's also been models on source to try to domain
transfer and conditional GANs. And so here, this is an
example of source to try to get domain transfer where,
for example in the upper part here we are trying to go
from source domain of horses to an output domain of zebras. So we can take an image
of horses and train a GAN such that the output is
going to be the same thing but now zebras in the same
image setting as the horses and go the other way around. We can transform apples into oranges. And also the other way around. We can also use this to
do photo enhancement. So producing these, really
taking a standard photo and trying to make really
nice, as if you had, pretending that you have a
really nice expensive camera. That you can get the nice blur effects. On the bottom here we have scene changing, so transforming an image
of Yosemite from the image in winter time to the
image in summer time. And there's really tons of applications. So on the right here there's more. There's also going from a text description and having a GAN that's now
conditioned on this text description and producing an image. So there's something
here about a small bird with a pink breast and crown
and now we're going to generate images of this. And there's also examples
down here of filling in edges. So given conditions on some sketch that we have, can we fill in a color version
of what this would look like. Can we take a Google, a
map grid and put something that looks like Google Earth on, and turn it into something
that looks like Google Earth. Go in and hallucinate all
of these buildings and trees and so on. And so there's lots of
really cool examples of this. And there's also this
website for pics to pics which did a lot of these
kind of conditional GAN type examples. I encourage you to go look
at for more interesting applications that people
have done with GANs. And in terms of research
papers there's also there's a huge number of papers
about GANs this year now. There's a website called
the GAN Zoo that kind of is trying to compile a whole list of these. And so here this has only
taken me from A through C on the left here and
through like L on the right. So it won't even fit on the slide. There's tons of papers as
well that you can look at if you're interested. And then one last pointer
is also for tips and tricks for training GANs, here's
a nice little website that has pointers if you're
trying to train these GANs in practice. Okay, so summary of GANs. GANs don't work with an
explicit density function. Instead we're going to represent
this implicitly through samples and they take a
game-theoretic approach to training so we're going to learn to
generate from our training distribution through a
two player game setup. And the pros of GANs are
that they're really having gorgeous state of the art
samples and you can do a lot with these. The cons are that they are
trickier and more unstable to train, we're not
just directly optimizing a one objective function
that we can just do backpop and train easily. Instead we have these two
networks that we're trying to balance training with so
it can be a bit more unstable. And we also can lose out
on not being able to do some of the inference queries,
P of X, P of Z given X that we had for example in our VAE. And GANs are still an
active area of research, this is a relatively new type
of model that we're starting to see a lot of and you'll
be seeing a lot more of. And so people are still working
now on better loss functions more stable training, so Wasserstein GAN for those of you who are
interested is basically an improvement in this direction. That now a lot of people are
also using and basing models off of. There's also other works
like LSGAN, Least Square's GAN, Least Square's GAN and others. So you can look into this more. And a lot of times for these new models in terms of actually implementing this, they're not necessarily big changes. They're different loss
functions that you can change a little bit and get
like a big improvement in training. And so this is, some of
these are worth looking into and you'll also get some
practice on your homework assignment. And there's also a lot of
work on different types of conditional GANs and GANs
for all kinds of different problem setups and applications. Okay so a recap of today. We talked about generative models. We talked about three of the
most common kinds of generative models that people are using
and doing research on today. So we talked first about
pixelRNN and pixelCNN, which is an explicit density model. It optimizes the exact
likelihood and it produces good samples but it's pretty
inefficient because of the sequential generation. We looked at VAE which
optimizes a variational or lower bound on the likelihood
and this also produces useful a latent representation. You can do inference queries. But the example quality
is still not the best. So even though it has a
lot of promise, it's still a very active area of
research and has a lot of open problems. And then GANs we talked
about is a game-theoretic approach for training and
it's what currently achieves the best state of the art examples. But it can also be tricky
and unstable to train and it loses out a bit
on the inference queries. And so what you'll also
see is a lot of recent work on combinations of these kinds of models. So for example adversarial autoencoders. Something like a VAE
trained with an additional adversarial loss on top which
improves the sample quality. There's also things like
pixelVAE is now a combination of pixelCNN and VAE so
there's a lot of combinations basically trying to take
the best of all these worlds and put them together. Okay so today we talked
about generative models and next time we'll talk
about reinforcement learning. Thanks. 

- Okay let's get started. Alright, so welcome to lecture 14, and today we'll be talking
about reinforcement learning. So some administrative details first, update on grades. Midterm grades were released last night, so see Piazza for more information and statistics about that. And we also have A2 and milestone grades scheduled for later this week. Also, about your projects, all teams must register your projects. So on Piazza we have a form posted, so you should go there and
this is required, every team should go and fill out
this form with information about your project, that
we'll use for final grading and the poster session. And the Tiny ImageNet
evaluation servers are also now online for those of you who are doing the Tiny ImageNet challenge. We also have a link to a
course survey on Piazza that was released a few days ago, so, please fill it out if
you guys haven't already. We'd love to have your
feedback and know how we can improve this class. Okay, so the topic of today,
reinforcement learning. Alright, so so far we've talked
about supervised learning, which is about a type of
problem where we have data x and then we have labels y
and our goal is to learn a function that is mapping from x to y. So, for example, the
classification problem that we've been working with. We also talked last lecture
about unsupervised learning, which is the problem
where we have just data and no labels, and our goal is to learn some underlying, hidden
structure of the data. So, an example of this
is the generative models that we talked about last lecture. And so today we're going
to talk about a different kind of problem set-up, the
reinforcement learning problem. And so here we have an agent that can take actions in its environment, and it can receive rewards
for for its action. And its goal is going to be
to learn how to take actions in a way that can maximize its reward. And so we'll talk about this
in a lot more detail today. So, the outline for today,
we're going to first talk about the reinforcement
learning problem, and then we'll talk about
Markov decision processes, which is a formalism of the
reinforcement learning problem, and then we'll talk
about two major classes of RL algorithms, Q-learning
and policy gradients. So, in the reinforcement
learning set up, what we have is we have an agent and
we have an environment. And so the environment
gives the agent a state. In turn, the agent is
going to take an action, and then the environment is
going to give back a reward, as well as the next state. And so this is going to
keep going on in this loop, on and on, until the environment gives back a terminal state, which then ends the episode. So, let's see some examples of this. First we have here the cart-pole problem, which is a classic problem
that some of you may have seen, in, for example, 229 before. And so this objective
here is that you want to balance a pole on top of a movable cart. Alright, so the state
that you have here is your current description of the system. So, for example, angular, angular speed of your pole, your
position, and the horizontal velocity of your cart. And the actions you can
take are horizontal forces that you apply onto the cart, right? So you're basically trying
to move this cart around to try and balance this pole on top of it. And the reward that you're getting from this environment
is one at each time step if your pole is upright. So you basically want to keep this pole balanced for as long as you can. Okay, so here's another example
of a classic RL problem. Here is robot locomotion. So we have here an example
of a humanoid robot, as well as an ant robot model. And our objective here is to
make the robot move forward. And so the state that we have describing our system is
the angle and the positions of all the joints of our robots. And then the actions that we can take are the torques applied onto these joints, right, and so these are trying to make the robot move forward and then
the reward that we get is our forward movement as well
as, I think, in the time of, in the case of the humanoid,
also, you can have something like a reward of one for
each time step that this robot is upright. So, games are also a big class of problems that
can be formulated with RL. So, for example, here we have Atari games which are a classic success
of deep reinforcement learning and so here the objective
is to complete these games with the highest possible score, right. So, your agent is basically a player that's trying to play these games. And the state that you have is going to be the raw pixels of the game state. Right, so these are just the pixels on the screen that you would see as you're playing the game. And then the actions that you have are your game controls, so for example, in some games maybe moving
left to right, up or down. And then the score that you
have is your score increase or decrease at each time step,
and your goal is going to be to maximize your total score
over the course of the game. And, finally, here we have
another example of a game here. It's Go, which is something that was a huge achievement of deep
reinforcement learning last year, when Deep Minds AlphaGo beats Lee Sedol, which is one of the best Go players of the last few years, and this is actually in the news again for, as some of you may have
seen, there's another Go competition going on now with AlphaGo versus a top-ranked Go player. And so the objective here is to win the game, and our
state is the position of all the pieces, the action
is where to put the next piece down, and the reward
is, one, if you win at the end of the game, and zero otherwise. And we'll also talk about this one in a little bit more detail, later. Okay, so how can we mathematically formalize the RL problem, right? This loop that we talked about earlier, of environments giving agents states, and then agents taking actions. So, a Markov decision process is the mathematical formulation
of the RL problem, and an MDP satisfies the Markov property, which is that the current state completely characterizes the state of the world. And an MDP here is defined
by tuple of objects, consisting of S, which is
the set of possible states. We have A, our set of possible actions, we also have R, our
distribution of our reward, given a state, action pair, so it's a function
mapping from state action to your reward. You also have P, which is
a transition probability distribution over your
next state, that you're going to transition to given
your state, action pair. And then finally we have a
Gamma, a discount factor, which is basically
saying how much we value rewards coming up soon versus later on. So, the way the Markov
Decision Process works is that at our initial time step t equals zero, the environment is going to sample some initial state as zero, from
the initial state distribution, p of s zero. And then, once it has that,
then from time t equals zero until it's done, we're going
to iterate through this loop where the agent is going to
select an action, a sub t. The environment is going to
sample a reward from here, so reward given your state and the action that you just took. It's also going to sample the next state, at time t plus one, given
your probability distribution and then the agent is going to receive the reward, as well as the
next state, and then we're going to through this process again, and keep looping; agent
will select the next action, and so on until the episode is over. Okay, so now based on this, we
can define a policy pi, which is a function from
your states to your actions that specifies what action
to take in each state. And this can be either
deterministic or stochastic. And our objective now is
to going to be to find your optimal policy pi
star, that maximizes your cumulative discounted reward. So we can see here we have our some of our future
rewards, which can be also discounted by your discount factor. So, let's look at an
example of a simple MDP. And here we have Grid World, which is this task where we have this grid of states. So you can be in any of these cells of your grid, which are your states. And you can take actions from your states, and so these actions are going to be simple movements, moving to your right, to your left, up or down. And you're going to get a
negative reward for each transition or each time step,
basically, that happens. Each movement that you take, and this can be something
like R equals negative one. And so your objective is going to be to reach one of the terminal states, which are the gray states shown here, in the least number of actions. Right, so the longer
that you take to reach your terminal state, you're going to keep accumulating these negative rewards. Okay, so if you look at
a random policy here, a random policy would
consist of, basically, at any given state or cell that you're in just sampling randomly which direction that you're going to move in next. Right, so all of these
have equal probability. On the other hand, an optimal policy that we would like to have is basically taking the action, the direction that will move us closest
to a terminal state. So you can see here, if we're right next to one of the terminal states we should always move in the direction that gets us to this terminal state. And otherwise, if you're in
one of these other states, you want to take the
direction that will take you closest to one of these states. Okay, so now given this description of our MDP, what we want to do is we want to find our
optimal policy pi star. Right, our policy that's
maximizing the sum of the rewards. And so this optimal policy
is going to tell us, given any state that we're
in, what is the action that we should take in order
to maximize the sum of the rewards that we'll get. And so one question is how do we handle the randomness in the MDP, right? We have randomness in terms of our initial
state that we're sampling, in therms of this transition probability distribution that will give us distribution of our
next states, and so on. Also what we'll do is we'll
work, then, with maximizing our expected sum of the rewards. So, formally, we can write
our optimal policy pi star as maximizing this expected
sum of future rewards over policy's pi, where
we have our initial state sampled from our state distribution. We have our actions, sampled from our policy, given the state. And then we have our next states sampled from our transition
probability distributions. Okay, so before we talk about exactly how we're going
to find this policy, let's first talk about a few definitions that's going to be helpful
for us in doing so. So, specifically, the value function and the Q-value function. So, as we follow the policy, we're going to sample trajectories or paths, right, for every episode. And we're going to have
our initial state as zero, a-zero, r-zero, s-one,
a-one, r-one, and so on. We're going to have this trajectory of states, actions, and
rewards that we get. And so, how good is a state
that we're currently in? Well, the value function at any state s, is the expected cumulative reward following the policy from
state s, from here on out. Right, so it's going to be expected value of our expected cumulative reward, starting from our current state. And then how good is a state, action pair? So how good is taking action a in state s? And we define this using
a Q-value function, which is, the expected
cumulative reward from taking action a in state s and
then following the policy. Right, so then, the
optimal Q-value function that we can get is going to be
Q star, which is the maximum expected cumulative reward that we can get from a given state action
pair, defined here. So now we're going to
see one important thing in reinforcement learning, which is called the Bellman equation. So let's consider this a Q-value function from the optimal policy Q star, which is then going to
satisfy this Bellman equation, which is this identity shown here, and what this means is that given any state, action pair, s and a, the value of this pair
is going to be the reward that you're going to get, r,
plus the value of whatever state that you end up in. So, let's say, s prime. And since we know that we
have the optimal policy, then we also know that we're going to play the best action that we can, right, at our state s prime. And so then, the value at state s prime is just going to be the
maximum over our actions, a prime, of Q star at s prime, a prime. And so then we get this identity here, for optimal Q-value. Right, and then also, as always, we have this expectation here, because we have randomness over what state that we're going to end up in. And then we can also
infer, from here, that our optimal policy, right, is going to consist of taking the best action in any state, as specified by Q star. Q star is going to tell us of the maximum future reward that we can
get from any of our actions, so we should just take a policy that's following this and just taking the action that's going to lead to best reward. Okay, so how can we solve
for this optimal policy? So, one way we can solve for this is something called a value
iteration algorithm, where we're going to use
this Bellman equation as an iterative update. So at each step, we're going
to refine our approximation of Q star by trying to
enforce the Bellman equation. And so, under some
mathematical conditions, we also know that this sequence Q, i of our Q-function is going
to converge to our optimal Q star as i approaches infinity. And so this, this works well, but what's the problem with this? Well, an important problem
is that this is not scalable. Right? We have to compute Q of s, a here for
every state, action pair in order to make our iterative updates. Right, but then this is a problem if, for example, if we look at these the state of, for example, an Atari game that we had earlier, it's going to be your screen of pixels. And this is a huge state
space, and it's basically computationally infeasible to compute this for
the entire state space. Okay, so what's the solution to this? Well, we can use a function approximator to estimate Q of s, a so, for example, a neural network, right. So, we've seen before that
any time, if we have some really complex function that
don't know, that we want to estimate, a neural network is a good way to estimate this. Okay, so this is going to take us to our formulation of Q-learning
that we're going to look at. And so, what we're going
to do is we're going to use a function approximator in order to estimate our
action value function. Right? And if this function approximator is a deep neural network, which is what's been used recently, then this is going to be
called deep Q-learning. And so this is something that you'll hear around as one
of the common approaches to deep reinforcement
learning that's in use. Right, and so in this case, we also have our function parameters theta here, so our Q-value function is determined by these weights, theta, of our neural network. Okay, so given this
function approximation, how do we solve for our optimal policy? So remember that we want to find a Q-function that's satisfying
the Bellman equation. Right, and so we want to
enforce this Bellman equation to happen, so what we
can do when we have this neural network approximating
our Q-function is that we can train this where our loss function is going to try and minimize the error of our Bellman equation, right? Or how far q of s, a is from its target, which is the Y_i here,
the right hand side of the Bellman equation
that we saw earlier. So, we're basically going to take these forward passes of our loss function, trying
to minimize this error and then our backward
pass, our gradient update, is just going to be you just take the gradient of this loss, with respect to our
network parameter's theta. Right, and so our goal is again to have this effect as we're
taking gradient steps of iteratively trying
to make our Q-function closer to our target value. So, any questions about this? Okay. So let's look at a case
study of an example where one of the classic examples
of deep reinforcement learning where this approach was applied. And so we're going to look at
this problem that we saw earlier of playing Atari games,
where our objective was to complete the game
with the highest score and remember our state is
going to be the raw pixel inputs of the game state, and we can take these actions of moving left, right, up, down, or whatever actions of
the particular game. And our reward at each time
step, we're going to get a reward of our score
increase or decrease that we got at this time step, and
so our cumulative total reward is this total reward
that we'll usually see at the top of the screen. Okay, so the network that
we're going to use for our Q-function is going to
look something like this, right, where we have our
Q-network, with weight's theta. And then our input, our
state s, is going to be our current game screen. And in practice we're going to take a stack of the last four
frames, so we have some history. And so we'll take these raw pixel values, we'll do some, you know, RGB
to gray-scale conversions, some down-sampling, some cropping, so, some pre-processing. And what we'll get out of
this is this 84 by 84 by four stack of the last four frames. Yeah, question. [inaudible question from audience] Okay, so the question
is, are we saying here that our network is
going to approximate our Q-value function for
different state, action pairs, for example, four of these? Yeah, that's correct. We'll see, we'll talk about that in a few slides. [inaudible question from audience] So, no. So, we don't have a Softmax
layer after the connected, because here our goal
is to directly predict our Q-value functions. [inaudible question from audience] Q-values. [inaudible question from audience] Yes, so it's more doing
regression to our Q-values. Okay, so we have our input to this network and then on top of this,
we're going to have a couple of familiar convolutional layers, and a fully-connected layer, so here we have an eight-by-eight
convolutions and we have some four-by-four convolutions. Then we have a FC 256 layer, so this is just a standard kind of networK that you've seen before. And then, finally, our last
fully-connected layer has a vector of outputs, which
is corresponding to your Q-value for each action, right, given the state that you've input. And so, for example, if
you have four actions, then here we have this
four-dimensional output corresponding to Q of
current s, as well as a-one, and then a-two, a-three, and a-four. Right so this is going
to be one scalar value for each of our actions. And then the number of
actions that we have can vary between, for example, 4 to 18,
depending on the Atari game. And one nice thing here is that using this network structure, a single feedforward
pass is able to compute the Q-values for all functions from the current state. And so this is really efficient. Right, so basically we
take our current state in and then because we have
this output of an action for each, or Q-value for each
action, as our output layer, we're able to do one pass and
get all of these values out. And then in order to train this, we're just going to use our
loss function from before. Remember, we're trying to
enforce this Bellman equation and so, on our forward
pass, our loss function we're going to try and
iteratively make our Q-value close to our target value, that it should have. And then our backward pass is just directly taking the gradient of this loss function that we have and then taking a gradient step based on that. So one other thing that's used
here that I want to mention is something called experience replay. And so this addresses a
problem with just using the plain two network
that I just described, which is that learning from batches of consecutive samples is bad. And so the reason because of this, right, is so for just playing the game, taking samples of state action rewards that we have and just taking consecutive
samples of these and training with these, well all of these samples are correlated and so this leads to inefficient learning, first of all, and also, because of this,
our current Q-network parameters, right, this
determines the policy that we're going to follow,
it determines our next samples that we're going to get that we're going to use for training. And so this leads to problems where you can have bad feedback loops. So, for example, if
currently the maximizing action that's going to take left, well this is going to bias all of my upcoming training examples to be dominated by samples from the left-hand side. And so this is a problem, right? And so the way that we
are going to address these problems is by using something called experience replay, where
we're going to keep this replay memory table of
transitions of state, as state, action, reward, next state, transitions that we have, and we're going to continuously update this
table with new transitions that we're getting as
game episodes are played, as we're getting more experience. Right, and so now what we can do is that we can now train
our Q-network on random, mini-batches of transitions
from the replay memory. Right, so instead of
using consecutive samples, we're now going to sample across these transitions that we've accumulated
random samples of these, and this breaks all of the, these correlation problems
that we had earlier. And then also, as another side benefit is that
each of these transitions can also contribute to potentially
multiple weight updates. We're just sampling from this table and so we could sample one multiple times. And so, this is going to lead also to greater data efficiency. Okay, so let's put this all together and let's look at the full algorithm for deep Q-learning
with experience replay. So we're going to start off with
initializing our replay memory to some capacity that we
choose, N, and then we're also going to initialize our Q-network, just with our random weights or initial weights. And then we're going to play
M episodes, or full games. This is going to be our training episodes. And then what we're going to do is we're going to initialize our state, using the starting game screen pixels at the beginning of each episode. And remember, we go through
the pre-processing step to get to our actual input state. And then for each time step of a game that we're currently playing, we're going to, with a small probability, select a random action, so one thing that's
important in these algorithms is to have sufficient exploration, so we want to make sure that we are sampling different
parts of the state space. And then otherwise, we're going to select from the greedy action from the current policy. Right, so most of the time
we'll take the greedy action that we think is a good policy of the type of
actions that we want to take and states that we want to see,
and with a small probability we'll sample something random. Okay, so then we'll take this action, a, t, and we'll observe the
next reward and the next state. So r, t and s, t plus one. And then we'll take this and
we'll store this transition in our replay memory
that we're building up. And then we're going to take, we're going to train a
network a little bit. So we're going to do experience replay and we'll take a sample
of a random mini-batches of transitions that we have from the replay memory,
and then we'll perform a gradient descent step on this. Right, so this is going to
be our full training loop. We're going to be
continuously playing this game and then also sampling minibatches, using
experienced replay to update our weights of our Q-network and then continuing in this fashion. Okay, so let's see. Let's see if I can, is this playing? Okay, so let's take a look at this deep Q-learning algorithm from Google DeepMind, trained
on an Atari game of Breakout. Alright, so it's saying
here that our input is just going to be our
state are raw game pixels. And so here we're looking
at what's happening at the beginning of training. So we've just started training a bit. And right, so it's going to look to it's learned to kind of hit the ball, but it's not doing a very
good job of sustaining it. But it is looking for the ball. Okay, so now after some more training, it looks like a couple hours. Okay, so now it's learning
to do a pretty good job here. So it's able to continuously follow this ball and be able to to remove most of the blocks. Right, so after 240 minutes. Okay, so here it's found
the pro strategy, right? You want to get all the
way to the top and then have it go by itself. Okay, so this is an example of using deep Q-learning in order to train an agent to be
able to play Atari games. It's able to do this on many Atari games and so you can check out some more of this online. Okay, so we've talked about Q-learning. But there is a problem
with Q-learning, right? It can be challenging
and what's the problem? Well, the problem can be that the Q-function is very complicated. Right, so we have to, we're
saying that we want to learn the value of every state action pair. So, if, let's say you have
something, for example, a robot grasping, wanting
to grasp an object. Right, you're going to have a
really high dimensional state. You have, I mean, let's
say you have all of your even just joint, joint
positions, and angles. Right, and so learning the
exact value of every state action pair that you have, right, can be really, really hard to do. But on the other hand, your
policy can be much simpler. Right, like what you want this robot to do maybe just to have this simple motion of just closing your hand, right? Just, move your fingers in this particular direction and keep going. And so, that leads to the question of can we just learn this policy directly? Right, is it possible,
maybe, to just find the best policy from a collection of policies, without trying to go through this process of estimating your Q-value and then using that to infer your policy. So, this is an approach that oh, so, okay, this is an approach that we're going to call policy gradients. And so, formally, let's define a class of parametrized policies. Parametrized by weights theta, and so for each policy let's define the value of the policy. So, J, our value J,
given parameters theta, is going to be, or expected some cumulative sum of future
rewards that we care about. So, the same reward that we've been using. And so our goal then, under this setup is that we want to find an optimal policy, theta star, which is the maximum, right, arg max over theta of J of theta. So we want to find the
policy, the policy parameters that gives our best expected reward. So, how can we do this? Any ideas? Okay, well, what we can do is just a gradient assent on
our policy parameters, right? We've learned that given
some objective that we have, some parameters we can
just use gradient asscent and gradient assent in order to continuously improve our parameters. And so let's talk more
specifically about how we can do this, which we're going to call here the reinforce algorithm. So, mathematically, we can write out our expected future reward over trajectories, and
so we're going to sample these trajectories of experience, right, like for example episodes of game play that we talked about earlier. S-zero, a-zero, r-zero, s-one, a-one, r-one, and so on. Using some policy pi of theta. Right, and then so, for each trajectory we can compute a reward
for that trajectory. It's the cumulative reward that we got from following this trajectory. And then the value of a policy, pi sub theta, is going
to be just the expected reward of these
trajectories that we can get from the following pi sub theta. So that's here, this
expectation over trajectories that we can get, sampling
trajectories from our policy. Okay. So, we want to do gradient ascent, right? So let's differentiate this. Once we differentiate
this, then we can just take gradient steps, like normal. So, the problem is that
now if we try and just differentiate this exactly, this is intractable, right? So, the gradient of an
expectation is problematic when p is dependent on
theta here, because here we want to take this gradient of p of tau, given theta, but this is going to be, we want to take this integral over tau. Right, so this is intractable. However, we can use a trick
here to get around this. And this trick is taking this
gradient that we want, of p. We can rewrite this by just multiplying this by one, by multiplying top and bottom, both by p of tau given theta. Right, and then if we look at these terms that we have now here, in the
way that I've written this, on the left and the right, this is actually going to be equivalent to p of tau times our gradient with respect to theta, of log, of p. Right, because the gradient
of the log of p is just going to be one over p times gradient of p. Okay, so if we then inject this back into our expression that we
had earlier for this gradient, we can see that, what this
will actually look like, right, because now we
have a gradient of log p times our probabilities of
all of these trajectories and then taking this
integral here, over tau. This is now going to be an expectation over our trajectories tau,
and so what we've done here is that we've taken a
gradient of an expectation and we've transformed it into
an expectation of gradients. Right, and so now we can use sample trajectories that we can get in order to estimate our gradient. And so we do this using
Monte Carlo sampling, and this is one of the
core ideas of reinforce. Okay, so looking at this expression that we want to compute, can we compute these
quantities that we had here without knowing the
transition probabilities? Alright, so we have that
p of tau is going to be the probability of a trajectory. It's going to be the product of all of our transition
probabilities of the next state that we get, given our
current state and action as well as our probability
of the actions that we've taken under our policy pi. Right, so we're going to
multiply all of these together, and get our probability of our trajectory. So this log of p of tau
that we want to compute is going to be we just
take this log and this will separate this out into a sum of pushing the logs inside. And then here, when we differentiate this, we can see we want to
differentiate with respect to theta, but this first
term that we have here, log p of the state
transition probabilities there's no theta term here, and so the only place where we have
theta is the second term that we have, of log of pi sub theta, of our action, given our
state, and so this is the only term that we keep in our gradient estimate,
and so we can see here that this doesn't depend on the
transition probabilities, right, so we actually don't need to know our transition probabilities
in order to computer our gradient estimate. And then, so, therefore
when we're sampling these, for any given trajectory tau,
we can estimate J of theta using this gradient estimate. This is here shown for a single trajectory from what we had earlier, and then we can also sample
over multiple trajectories to get the expectation. Okay, so given this gradient
estimator that we've derived, the interpretation that we can
make from this here, is that if our reward for a trajectory
is high, if the reward that we got from taking the
sequence of actions was good, then let's push up the
probabilities of all the actions that we've seen. Right, we're just going to say that these were good actions that we took. And then if the reward is low, we want to push down these probabilities. We want to say these were bad actions, let's try and not sample these so much. Right and so we can see
that's what's happening here, where we have pi of a, given s. This is the likelihood of
the actions that we've taken and then we're going to scale
this, we're going to take the gradient and the gradient
is going to tell us how much should we change the
parameters in order to increase our likelihood of our action, a, right? And then we're going to
take this and scale it by how much reward we actually got from it, so how good were these
actions, in reality. Okay, so this might seem simplistic to say that, you know, if a trajectory
is good, then we're saying here that all of its actions were good. Right? But, in expectation, this
actually averages out. So we have an unbiased estimator here, and so if you have many samples of this, then we will get an accurate
estimate of our gradient. And this is nice because we can just take gradient steps and we know
that we're going to be improving our loss
function and getting closer to, at least some local optimum of our policy parameters theta. Alright, but there is a problem with this, and the problem is that this also suffers from high variance. Because this credit
assignment is really hard. Right, we're saying that given a reward that we
got, we're going to say all of the actions were good,
we're just going to hope that this assignment of
which actions were actually the best actions, that mattered, are going to average out over time. And so this is really hard
and we need a lot of samples in order to have a good estimate. Alright, so this leads to the
question of, is there anything that we can do to reduce the variance and improve the estimator? And so variance reduction is an important area of research
in policy gradients, and in coming up with
ways in order to improve the estimator and require fewer samples. Alright, so let's look
at a couple of ideas of how we can do this. So given our gradient estimator, so the first idea is that we can push up the probabilities of an action only by it's affect on future rewards from that state, right? So, now with instead of scaling this likelihood, or
pushing up this likelihood of this action by the total
reward of its trajectory, let's look more
specifically at just the sum of rewards coming from this time step on to the end, right? And so, this is basically saying that how good an action is, is
only specified by how much future reward it generates. Which makes sense. Okay, so a second idea
that we can also use is using a discount factor in order to ignore delayed effects. Alright so here we've added
back in this discount factor, that we've seen before,
which is saying that we are, you know, our discount
factor's going to tell us how much we care about just the rewards that are coming up soon, versus rewards that came much later on. Right, so we were going to now say how good or bad an action is, looking more at the local neighborhood of action set it generates
in the immediate near future and down weighting the the
ones that come later on. Okay so these are some straightforward ideas that are generally used in practice. So, a third idea is this idea of using a baseline in order to
reduce your variance. And so, a problem with
just using the raw value of your trajectories, is that this isn't necessarily meaningful, right? So, for example, if your
rewards are all positive, then you're just going to keep pushing up the probabilities of all your actions. And of course, you'll push
them up to various degrees, but what's really important
is whether a reward is better or worse than what you're
expecting to be getting. Alright, so in order to
address this, we can introduce a baseline function that's
dependent on the state. Right, so this baseline function tell us what's, how much we, what's
our guess and what we expect to get from this state, and then our reward or our scaling
factor that we're going to use to be pushing up or
down our probabilities, can now just be our expected
sum of future rewards, minus this baseline, so now
it's the relative of how much better or worse is
the reward that we got from what we expected. And so how can we choose this baseline? Well, a very simple baseline, the
most simple you can use, is just taking a moving average of rewards that you've experienced so far. So you can even do this
overall trajectories, and this is just an
average of what rewards have I been seeing as I've been training, and as I've been playing these episodes? Right, and so this gives
some idea of whether the reward that I currently get
was relatively better or worse. And so there's some variance
on this that you can use but so far the variance
reductions that we've seen so far are all used in what's typically called "vanilla REINFORCE" algorithm. Right, so looking at the
cumulative future reward, having a discount factor,
and some simple baselines. Now let's talk about how we can think about this idea of baseline and potentially choose better baselines. Right, so if we're going to
think about what's a better baseline that we can choose, what we want to do is we want
to push up the probability of an action from a state,
if the action was better than the expected value of what we
should get from that state. So, thinking about the value
of what we're going to expect from the state, what
does this remind you of? Does this remind you of anything that we talked about
earlier in this lecture? Yes. [inaudible from audience] Yeah, so the value functions, right? The value functions that we
talked about with Q-learning. So, exactly. So Q-functions and value functions and so, the intuition is that well, we're happy with an action, taking an action in a state s, if our Q-value of taking a specific action from
this state is larger than the value function or expected value of the cumulative future reward that we can get from this state. Right, so this means that
this action was better than other actions that we could've taken. And on the contrary, we're
unhappy if this action, if this value or this
difference is negative or small. Right, so now if we plug
this in, in order to, as our scaling factor of how much we want to push up or down, our
probabilities of our actions, then we can get this estimator here. Right, so, it's going to be exactly the same as before, but now where we've had before our
cumulative expected reward, with our various reduction,
variance reduction techniques and baselines in,
here we can just plug in now this difference of how much better our current action was,
based on our Q-function minus our value function from that state. Right, but what we talked
about so far with our REINFORCE algorithm, we don't know what Q and V actually are. So can we learn these? And the answer is yes, using Q-learning. What we've already talked about before. So we can combine policy gradients while we've just been talking
about, with Q-learning, by training both an actor,
which is the policy, as well as a critic, right, a Q-function, which is going to tell us
how good we think a state is, and an action in a state. Right, so using this in approach, an actor is going to
decide which action to take and then the critic, or
Q-function, is going to tell the actor how good its action
was and how it should adjust. And so, and this also alleviates
a little bit of the task of this critic compared
to the Q-learning problems that we talked about earlier
of having to have this learning a Q-value for
every state, action pair, because here it only has to learn this for the state-action pairs that
are generated by the policy. It only needs to know this where it matters for
computing this scaling factor. Right, and then we can also,
as we're learning this, incorporate all of the
Q-learning tricks that we saw earlier, such as experience replay. And so, now I'm also going to just define this term that we saw earlier, Q of s of a, how much,
how good was an action in a given state, minus V of s? Our expected value of
how good the state is by this term advantage function. Right, so the advantage
function is how much advantage did we get from playing this action? How much better the
action was than expected. So, using this, we can
put together our full actor-critic algorithm. And so what this looks like,
is that we're going to start off with by initializing
our policy parameters theta, and our critic parameters
that we'll call phi. And then for each, for
iterations of training, we're going to sample M trajectories, under the current policy. Right, we're going to play
our policy and get these trajectories as s-zero, a-zero,
r-zero, s-one and so on. Okay, and then we're going to compute the gradients that we want. Right, so for each of these trajectories and in each time step, we're going to compute this advantage function, and then we're going to use this advantage function, right? And then we're going to use
that in our gradient estimator that we showed earlier, and accumulate our gradient estimate that we have for here. And then we're also going to train our critic parameters phi
by exactly the same way, so as we saw earlier,
basically trying to enforce this value function, right,
to learn our value function, which is going to be pulled
into, just minimizing this advantage function and this will encourage it to be closer
to this Bellman equation that we saw earlier, right? And so, this is basically
just iterating between learning and optimizing
our policy function, as well as our critic function. And so then we're going to update the gradients and then we're
going to go through and just continuously repeat this process. Okay, so now let's look at
some examples of REINFORCE in action, and let's look
first here at something called the Recurrent Attention Model,
which is something that, which is a model also
referred to as hard attention, but you'll see a lot in,
recently, in computer vision tasks for various purposes. Right, and so the idea behind this is here, I've talked about the
original work on hard attention, which is on image
classification, and your goal is to still predict the image class, but now you're going to do
this by taking a sequence of glimpses around the image. You're going to look at local
regions around the image and you're basically going
to selectively focus on these parts and build up information
as you're looking around. Right, and so the reason
that we want to do this is, well, first of all it
has some nice inspiration from human perception in eye movement. Let's say we're looking at a complex image and we want to determine
what's in the image. Well, you know, we might,
maybe look at a low-resolution of it first, and then
look specifically at parts of the image that will give us clues about what's in this image. And then, this approach of just looking
at, looking around at an image at local regions, is also
going to help you save computational resources, right? You don't need to process the full image. In practice, what usually
happens is you look at a low-resolution image
first, of a full image, to decide how to get started,
and then you look at high-res portions of the image after that. And so this saves a lot
of computational resources and you can think about,
then, benefits of this to scalability, right,
being able to, let's say process larger images more efficiently. And then, finally, this
could also actually help with actual classification performance, because now you're able to ignore clutter and irrelevant
parts of the image. Right? Like, you know, instead
of always putting through your ConvNet, all the parts of your image, you can use this to, maybe,
first prune out what are the relevant parts that I
actually want to process, using my ConvNet. Okay, so what's the reinforcement learning formulation of this problem? Well, our state is going to be the glimpses that we've
seen so far, right? Our what's the information that we've seen? Our action is then going to be where to look next in the image. Right, so in practice,
this can be something like the x, y-coordinates,
maybe centered around some fixed-sized glimpse that
you want to look at next. And then the reward for
the classification problem is going to be one, at
the final time step, if our image is correctly
classified, and zero otherwise. And so, because this glimpsing, taking these
glimpses around the image is a non-differentiable operation, this is why we need to use reinforcement learning formulation, and learn policies for how
to take these glimpse actions and we can train this using REINFORCE. So, given the state of glimpses so far, the core of our model is going to be this RNN that we're going
to use to model the state, and then we're going to
use our policy parameters in order to output the next action. Okay, so what this model looks
like is we're going to take an input image. Right, and then we're going to
take a glimpse at this image. So here, this glimpse is the red box here, and this is all blank, zeroes. And so we'll pass what
we see so far into some neural network, and this can be any kind of network depending on your task. In the original experiments
that I'm showing here, on MNIST, this is very
simple, so you can just use a couple of small,
fully-connected layers, but you can imagine
for more complex images and other tasks you may want
to use fancier ConvNets. Right, so you've passed this
into some neural network, and then, remember I said
we're also going to be integrating our state of,
glimpses that we've seen so far, using a recurrent network. So, I'm just going to we'll see that later on, but
this is going to go through that, and then it's going to output my x, y-coordinates, of where
I'm going to see next. And in practice, this is going to be We want to output a
distribution over actions, right, and so, what this is
going to be it's going to be a gaussian distribution and
we're going to output the mean. You can also output a mean and variance of this distribution in practice. The variance can also be fixed. Okay, so we're going to take this action that we're now going to sample a specific x, y location
from our action distribution and then we're going to put
this in to get the next, extract the next glimpse from our image. Right, so here we've moved to the end of the two,
this tail part of the two. And so now we're actually
starting to get some signal of what we want to see, right? Like, what we want to do is we
want to look at the relevant parts of the image that are
useful for classification. So we pass this through, again,
our neural network layers, and then also through our recurrent network, right,
that's taking this input as well as this previous hidden
state, and we're going to use this to get a, so this is representing our policy, and then we're going to use this to output our distribution for the next location that we want to glimpse at. So we can continue doing this, you can see in this next glimpse here, we've moved a little bit more
toward the center of the two. Alright, so it's probably learning that, you know, once I've seen
this tail part of the two, that looks like this,
maybe moving in this upper left-hand direction will
get you more towards a center, which will also have a value, valuable information. And then we can keep doing this. And then finally, at the
end, at our last time step, so we can have a fixed
number of time steps here, in practice something like six or eight. And then at the final time
step, since we want to do classification, we'll have our standard Softmax layer that will produce a distribution of
probabilities for each class. And then here the max class was a two, so we can predict that this was a two. Right, and so this is going
to be the set up of our model and our policy, and then we have our estimate for the gradient
of this policy that we've said earlier we could compute by taking trajectories from here and using those to do back prop. And so we can just do this
in order to train this model and learn the parameters
of our policy, right? All of the weights that you can see here. Okay, so here's an example of a policies trained on MNIST, and so you can see that, in general, from wherever it's
starting, usually learns to go closer to where the digit is, and then looking at the relevant
parts of the digit, right? So this is pretty cool and this you know, follows kind of
what you would expect, right, if you were to choose places to look next in order to most efficiently determine what digit this is. Right, and so this idea of hard attention, of recurrent attention
models, has also been used in a lot of tasks in
computer vision in the last couple of years, so you'll
see this, used, for example, fine-grained image recognition. So, I mentioned earlier that one of the useful benefits of this can be also to both save on computational efficiency as well as to ignore
clutter and irrelevant parts of the image, and
when you have fine-grained image classification problems, you usually want both of these. You want to keep high-resolution,
so that you can look at, you know, important differences. And then you also want to
focus on these differences and ignore irrelevant parts. Yeah, question. [inaudible question from audience] Okay, so yeah, so the question is how is there is
computational efficiency, because we also have this
recurrent neural network in place. So that's true, it depends
on exactly what's your, what is your problem, what
is your network, and so on, but you can imagine that
if you had some really hi- resolution image and you don't want to process
the entire parts of this image with some huge ConvNet
or some huge, you know, network, now you can
get some savings by just focusing on specific
smaller parts of the image. You only process those parts of the image. But, you're right, that
it depends on exactly what problem set-up you have. This has also been used
in image captioning, so if we're going to produce
an caption for an image, we can choose, you know,
we can have the image use this attention model
to generate this caption and what it usually ends up
learning is these policies where it'll focus on
specific parts of the image, in sequence, and as it
focuses on each part, it'll generate some words
or the part of the caption referring to that part of the image. And then it's also been used, also tasks such as visual
question answering, where we ask a question about the image and you want the model
to output some answer to your question, for
example, I don't know, how many chairs are around the table? And so you can see how
this attention mechanism might be a good type of model for learning how to
answer these questions. Okay, so that was an
example of policy gradients in these hard attention models. And so, now I'm going to
talk about one more example, that also uses policy gradients, which is learning how to play Go. Right, so DeepMind had this agent for playing Go, called AlphGo, that's been in the news a lot in the past, last year and this year. So, sorry? [inaudible comment from audience] And yesterday, yes, that's correct. So this is very exciting,
recent news as well. So last year, a first version of AlphaGo was put into a competition against one
of the best Go players of recent years, Lee Sedol, and the agent was able to beat him four to one, in a game of five matches. And actually, right now, just there's another match with
Ke Jie, which is current world number one, and
so it's best of three in China right now. And so the first game was yesterday. AlphaGo won. I think it was by just
half a point, and so, so there's two more games to watch. These are all live-stream, so you guys, should also go
online and watch these games. It's pretty interesting
to hear the commentary. But, so what is this AlphaGo
agent, right, from DeepMind? And it's based on a lot
of what we've talked about so far in this lecture. And what it is it's a mixed
of supervised learning and reinforcement learning, as well as a mix of some older methods for Go, Monte Carlo Tree Search, as well as recent deep RL approaches. So, okay, so how does AlphaGo
beat the Go world champion? Well, what it first does is to train AlphaGo, what it
takes as input is going to be a few featurization of the board. So it's basically, right,
your board and the positions of the pieces on the board. That's your natural state representation. And what they do in order
to improve performance a little bit is that
they featurize this into some more channels of one is all
the different stone colors, so this is kind of like your configuration of your board. Also some channels, for
example, where, which moves are legal, some bias
channels, some various things and then, given this state, right, it's going to first train a network that's initialized with
supervised training from professional Go games. So, given the current board configuration or features, featurization of this, what's the correct next action to take? Alright, so given examples of professional games played, you know, just collected over time, we can just take all of
these professional Go moves, train a standard, supervised mapping, from board state to action to take. Alright, so they take this,
which is a pretty good start, and then they're going
to use this to initialize a policy network. Right, so policy network,
it's just going to take the exact same structure of input is your board state and your output is the actions that you're going to take. And this was the set-up
for the policy gradients that we just saw, right? So now we're going to just
continue training this using policy gradients. And it's going to do this
reinforcement learning training by playing against itself for
random, previous iterations. So self play, and the
reward it's going to get is one, if it wins, and a
negative one if it loses. And what we're also going to
do is we're also going to learn a value network, so,
something like a critic. And then, the final AlphaGo
is going to be combining all of these together, so
policy and value networks as well as with a Monte Carlo Tree Search
algorithm, in order to select actions by look ahead search. Right, so after putting all this together, a value of a node, of
where you are in play, and what you do next, is
going to be a combination of your value function, as well as roll at outcome that you're
computing from standard Monte Carlo Tree Search roll outs. Okay, so, yeah, so this is basically the various, the components of AlphaGo. If you're interested in
reading more about this, there's a nature paper about this in 2016, and they trained this, I think, over, the version of AlphaGo
that's being used in these matches is, like, I think
a couple thousand CPUs plus a couple hundred GPUs,
putting all of this together, so it's a huge amount of
training that's going on, right. And yeah, so you guys should, follow the game this week. It's pretty exciting. Okay, so in summary,
today we've talked about policy gradients, right,
which are general. They, you're just directly taking gradient descent or
ascent on your policy parameters, so this works well for a
large class of problems, but it also suffers from high variance, so it requires a lot of samples, and your challenge here
is sample efficiency. We also talked about
Q-learning, which doesn't always work, it's harder to
sometimes get it to work because of this problem
that we talked earlier where you are trying to compute this exact state, action value for many, for very high
dimensions, but when it does work, for problems, for example,
the Atari we saw earlier, then it's usually more sample efficient than policy gradients. Right, and one of the
challenges in Q-learning is that you want to make sure that you're doing sufficient exploration. Yeah? [inaudible question from audience] Oh, so for Q-learning can
you do this process where you're, okay, where you're
trying to start this off by some supervised training? So, I guess the direct
approach for Q-learning doesn't do that because you're
trying to regress to these Q-values, right, instead of
policy gradients over this distribution, but I think there
are ways in which you can, like, massage this type of thing to also bootstrap. Because I think bootstrapping
in general or like behavior cloning is a good way to warm start these policies. Okay, so, right, so we've
talked about policy gradients and Q-learning, and just
another look at some of these, some of the guarantees that you have, right, with policy gradients. One thing we do know
that's really nice is that this will always converge to
a local minimum of J of theta, because we're just directly
doing gradient ascent, and so this is often, and this local minimum is
often just pretty good, right. And in Q-learning, on the
other hand, we don't have any guarantees because here
we're trying to approximate this Bellman equation with
a complicated function approximator and so, in this
case, this is the problem with Q-learning being a
little bit trickier to train in terms of applicability
to a wide range of problems. Alright, so today you got basically very, brief, kind of high-level
overview of reinforcement learning and some major classes
of algorithms in RL. And next time we're going to have a guest lecturer from, Song
Han, who's done a lot of pioneering work in model compression and energy efficient deep learning, and so he's going to talk some
of this, about some of this. Thank you. 

- Hello everyone, welcome to CS231. I'm Song Han. Today I'm
going to give a guest lecture on the efficient methods and
hardware for deep learning. So I'm a fifth year PhD
candidate here at Stanford, advised by Professor Bill Dally. So, in this course we have seen
a lot of convolution neural networks, recurrent
neural networks, or even since last time, the
reinforcement learning. They are spanning a lot of applications. For example, the self-=driving
car, machine translation, AlphaGo and Smart Robots. And it's changing our
lives, but there is a recent trend that in order to
achieve such high accuracy, the models are getting larger and larger. For example for ImageNet
recognition, the winner from 2012 to 2015, the model
size increased by 16X. And just in one year,
for Baidu's deep speech just in one year, the training
operations, the number of training operations increased by 10X. So such large model
creates lots of problems, for example the model size
becomes larger and larger so it's difficult for
them to be deployed either on those for example,
on the mobile phones. If the item is larger
than 100 megabytes, you cannot download until
you connect to Wi-Fi. So those product managers
and for example Baidu, Facebook, they are very sensitive
to the size of the binary size of their model. And also for example, the
self-driving car, you can only do those on over-the-air
update for the model if the model is too large,
it's also difficult. And the second challenge
for those large models is that the training speed is extremely slow. For example, the ResNet152,
which is only a few, less than 1% actually, more
accurate than ResNet101. Takes 1.5 weeks to train on four Maxwell M40 GPUs for example. Which greatly limits either
we are doing homework or if the researcher's
designing new models is getting pretty slow. And the third challenge
for those bulky model is the energy efficiency. For example, the AlphaGo
beating Lee Sedol last year, took 2000 CPUs and 300
GPUs, which cost $3,000 just to pay for the electric
bill, which is insane. So either on those embedded
devices, those models are draining your battery
power for on data-center increases the total cost
of ownership of maintaining a large data-center. For example, Google in
their blog, they mentioned if all the users using the
Google Voice Search for just three minutes, they have
to double their data-center. So that's a large cost. So reducing such cost is very important. And let's see where is
actually the energy consumed. The large model means
lots of memory access. You have to access, load
those models from the memory means more energy. If you look at how much
energy is consumed by loading the memory versus how much is
consumed by multiplications and add those arithmetic
operations, the memory access is more than two or three
orders of magnitude, more energy consuming than
those arithmetic operations. So how to make deep
learning more efficient. So we have to improve
energy efficiency by this Algorithm and Hardware Co-Design. So this is the previous
way, which is our hardware. For example, we have some
benchmarks say Spec 2006 and then run those
benchmarks and tune your CPU architectures for those benchmarks. Now what we should do is
to open up the box to see what can we do from algorithm
side first and see what is the optimum question
mark processing unit. That breaks the
boundary between the algorithm hardware to improve
the overall efficiency. So today's talk, I'm going
to have the following agenda. We are going to cover four
aspects: The algorithm hardware and inference and training. So they form a small two by
two matrix, so includes the algorithm for efficient inference, hardware for efficient inference and the algorithm for efficient training, and lastly, the hardware
for efficient training. For example, I'm going
to cover the TPU, I'm going to cover the Volta. But before I cover those
things, let's have three slides for Hardware 101. A brief introduction of
the families of hardware in such a tree. So in general, we can
have roughly two branches. One is general purpose hardware. It can do any applications
versus the specialized hardware, which is tuned
for a specific kind of applications, a domain of applications. So the general purpose
hardware includes, the CPU or the GPU, and their
difference is that CPU is latency oriented, single threaded. It's like a big elephant. While the GPU is throughput oriented. It has many small though
weak threads, but there are thousands of such small weak cores. Like a group of small ants,
where there are so many ants. And specialized hardware,
roughly there are FPGAs and ASICs. So FPGA stand for Field
Programmable Gate Array. So it is programmable, hardware
programmable so its logic can be changed. So it's cheaper for you to try
new ideas and do prototype, but it's less efficient. It's in the middle between
the general purpose and pure ASIC. So ASIC stands for Application
Specific Integrated Circuit. It has a fixed logic, just designed for a certain application. For example deep learning. And Google's TPU is a kind of
ASIC and the neural networks we train on, the earlier GPUs is here. And another slide for
Hardware 101 is the number representations. So in this slide, I'm going
to convey you the idea that all the numbers in computer
are not represented by a real number. It's not a real number, but
they are actually discrete. Even for those floating
point with your 32 Bit. Floating point numbers, their
resolution is not perfect. It's not continuous, but it's discrete. So for example FP32, meaning
using a 32 bit to represent a floating point number. So there are three components
in the representation. The sign bit, the
exponent bit, the mantissa, and the number it represents
is shown by minus 1 to the S times 1.M times 2 to the exponent. So similar there is FP16,
using a 16 bit to represent a floating point number. In particular, I'm going
to introduce Int8, where the core TPU use, using an
integer to represent a fixed point number. So we have a certain number
of bits for the integer. Followed by a radix point,
if we put different layers. And lastly, the fractional bits. So why do we prefer those
eight bit, or 16 bit rather than those traditional like the 32 bit floating point. That's the cost. So, I generated the figure
from 45 nanometer technology about the energy cost versus
the area cost for different operations. In particular, let's see
here, go you from 32 bit to 16 bit, we have about four
times reduction in energy and also about four times
reduction in the area. Area means money. Every millimeter square takes
money to take out a chip So it's very beneficial for
hardware design to go from 32 bit to 16 bit. That's why you hear NVIDIA
from Pascal Architecture, they said they're
starting to support FP16. That's the reason why it's so beneficial. For example, previous battery
level could last four hours, now it becomes 16 hours. That's what it means to reduce the energy cost by four times. But here still, there's a
problem of large energy costs for reading the memory. And let's see how can we deal
with this memory reference so expensive, how do we deal
with this problem better? So let's switch gear and
come to our topic directly. So let's first introduce
algorithm for efficient inference. So I'm going to cover six topics,
this is a really long slide. So I'm going to relatively fast. So the first idea I'm going
to talk about is pruning. Pruning the neural networks. For example, this is
original neural network. So what I'm trying to do is,
can we remove some of the weight and still have the same accuracy? It's like pruning a tree, get rid of those redundant connections. This is first proposed by
Professor Yann LeCun back in 1989, and I revisited this problem,
26 years later, on those modern deep neural nets
to see how it works. So not all parameters are useful actually. For example, in this case, if
you want to fit a single line, but you're using a quadratic
term, apparently the 0.01 is a redundant parameter. So I'm going to train the
connectivity first and then prune some of the connections. And then train the remaining weights, and through this process, it regulates. And as a result, I can reduce
the number of connections, and annex that from 16
million parameters to only six million parameters,
which is 10 times less the computation. So this is the accuracy. So the x-axis is how much
parameters to prune away and the y-axis is the accuracy you have. So we want to have less
parameters, but we also want to have the same accuracy as before. We don't want to sacrifice accuracy, For example at 80%, we
locked zero away left 80% of the parameters, but
accuracy jumped by 4%. That's intolerable. But the good thing is that
if we retrain the remaining weights, the accuracy
can fully recover here. And if we do this process iteratively by pruning and retraining,
pruning and retraining, we can fully recover the
accuracy not until we are prune away 90% of the parameters. So if you go back to home
and try it on your Ipad or notebook, just zero away
50% of the parameters say you went on your homework,
you will astonishingly find that accuracy actually doesn't hurt. So we just mentioned
convolution neural nets, how about RNNs and LSTMs, so I
tried with this neural talk. Again, pruning away 90% of
the rates doesn't hurt the blue score. And here are some visualizations. For example, the original
picture, the neural talk says a basketball player in a
white uniform is playing with a ball. Versus pruning away 90% it
says, a basketball player in a white uniform is
playing with a basketball. And on and so on. But if you're too aggressive,
say you prune away 95% of the weights, the
network is going to get drunk. It says, a man in a red shirt
and white and black shirt is running through a field. So there's really a limit,
a threshold, you have to take care of during the pruning. So interestingly, after
I did the work, did some resource and research and
find actually the same pruning procedure actually
happens to human brain as well. So when we were born, there
are about 50 trillion synapses in the brain. And at one year old, this number
surged into 1,000 trillion. And as we become adolescent,
it becomes smaller actually, 500 trillion in the end,
according to the study by Nature. So this is very interesting. And also, the pruning changed
the weight distribution because we are removing
those small connections and after we retrain them,
that's why it becomes soft in the end. Yeah, question. - [Student] Are you trying
to mean that it terms of your mixed weights
during the training will be just set at zero and
just start from scratch? And these start from the
things that are at zero. - Yeah. So the question is,
how do we deal with those zero connections? So we force them to be zero
in all the other iterations. Question? - [Student] How do you
pick which rates to drop? - Yeah so very simple. Small
weights, drop it, sort it. If it's small, just-- - [Student] Any threshold that I decide? - Exactly, yeah. So the next idea, weight sharing. So now we have, remember
our end goal is to remove connections so that we can
have less memory footprint so that we can have more
energy efficient deployment. Now we have less number
of parameters by pruning. We want to have less number
of bits per parameter so they're multiplied together
they get a small model. So the idea is like this. Not all numbers, not all the weights has to be the exact number. For example, 2.09, 2.12 or
all these four weights, you just put them using 2.0 to represent them. That's enough. Otherwise too accurate number
is just leads to overfitting. So the idea is I can
cluster the weights if they are similar, just using
a centroid to represent the number instead of using
the full precision weight. So that every time I do the
inference, I just do inference on this single number. For example, this is a
four by four weight matrix in a certain layer. And what I'm going to do is do
k-means clustering by having the similar weight
sharing the same centroid. For example, 2.09, 2.12, I store index of three pointing to here. So that, the good thing is
we need to only store the two bit index rather than the
32 bit, floating point number. That's 16 times saving. And how do we train such neural network? They are binded together, so
after we get the gradient, we color them in the same
pattern as the weight and then we do a group by
operation by having all the in that weights with the
same index grouped together. And then we do a reduction
by summing them up. And then multiplied by the learning rate subtracted from the original centroid. That's one iteration of
the SGD for such weight shared neural network. So remember previously,
after pruning this is what the weight
distribution like and after weight sharing, they become discrete. There are only 16 different
values here, meaning we can use four bits to
represent each number. And by training on such
weight shared neural network, training on such extremely
shared neural network, these weights can adjust. It is the subtle changes
that compensated for the loss of accuracy. So let's see, this is the
number of bits we give it, this is the accuracy
for convolution layers. Not until four bits, does
the accuracy begin to drop and for those fully connected
layers, very astonishingly, it's not until two bits,
only four number, does the accuracy begins to drop. And this result is per layer. So we have covered two methods,
pruning and weight sharing. What if we combine these
two methods together. Do they work well? So by combining those methods,
this is the compression ratio with the smaller on the left. And this is the accuracy. We can combine it together
and make the model about 3% of its original
size without hurting the accuracy at all. Compared with the each
working individual data by 10%, accuracy begins to drop. And compared with the
cheap SVD method, this has a better compression ratio. And final idea is we can
apply the Huffman Coding to use more number of bits
for those infrequent numbers, infrequently appearing weights
and less number of bits for those more frequently
appearing weights. So by combining these three
methods, pruning, weight sharing, and also Huffman
Coding, we can compress the neural networks, state-of-the-art 
neural networks, ranging from 10x to
49x without hurting the prediction accuracy. Sometimes a little bit better. But maybe that is noise. So the next question is, these
models are just pre-trained models by say Google, Microsoft. Can we make a compact
model, a pump compact model to begin with? Even before such compression? So SqueezeNet, you may have
already worked with this neural network model in a homework. So the idea is we are having
a squeeze layer here to shield at the three by three
convolution with fewer number of channels. So that's where squeeze comes from. And here we have two branches,
rather than four branches as in the inception model. So as a result, the model
is extremely compact. It doesn't have any 
fully connected layers. Everything is fully convolutional. The last layer is a global pooling. So what if we apply deep
compression algorithm on such already compact
model will it be getting even smaller? So this is AlexNet after
compression, this is SqueezeNet. Even before compression, it's
50x smaller than AlexNet, but has the same accuracy. After compression 510x
smaller, but the same accuracy only less than half a megabyte. This means it's very easy
to fit such a small model on the cache, which is literally tens of megabyte SRAM. So what does it mean? It's possible to achieve speed up. So this is the speedup, I
measured if all these fully connected layers only for
now, on the CPU, GPU, and the mobile GPU, before pruning and after pruning the weights, and on average, I observed
a 3x speedup in a CPU, about 3X speedup on the GPU, and roughly 5x speedup on
the mobile GPU, which is a TK1. And so is the energy efficiency. In an average improvement
from 3x to 6x on a CPU, GPU, and mobile GPU. And these ideas are
used in these companies. Having talked about when
pruning and when sharing, which is a non-linear quantization method and we're going to talk about
quantization, which is, why do they use in the TPU design? All the TPU designs use at
only eight bit for inference. And the way, how they can
use that is because of the quantization. And let's see how does it work. So quantization has this
complicated figure, but the intuition is very simple. You run the neural network
and train it with the normal floating point numbers. And quantize the weight
and activations by gather the statistics for each layer. For example, what is the maximum number,
minimum number, and how many bits are enough to represent this dynamic range. Then you use that number of
bits for the integer part and the rest of the eight bit or seven bit for the other part of
the 8 bit representation. And also we can fine tune in
the floating point format. Or we can also use feed
forward with fixed point and back propagation with
update with the floating point number. There are lots of different
ideas to have better accuracy. And this is the result,
for how many number of bits versus what is the accuracy. For example, using a fixed,
8 bit, the accuracy for GoogleNet doesn't drop significantly. And for VGG-16, it also
remains pretty well for the accuracy. While circling down to
a six bit, the accuracy begins to drop pretty dramatically. Next idea, low rank approximation. It turned out that for
a convolution layer, you can break it into
two convolution layers. One convolution here, followed
by a one by one convolution. So that it's like you
break a complicated problem into two separate small problems. This is for convolution layer. As we can see, achieving about 2x speedup, there's almost
no loss of accuracy. And achieving a speedup
of 5x, roughly a 6% loss of accuracy. And this also works for
fully connected layers. The simplest idea is using
the SVD to break it into one matrix into two matrices. And follow this idea, this
paper proposes to use the Tensor Tree to break down one
fully connected layer into a tree, lots of fully connected layers. That's why it's called a tree. So going even more crazy, can we use only two weights or three weights
to represent a neural network? A ternary weight or a binary weight. We already seen this distribution
before, after pruning. There's some positive
weights and negative weights. Can we just use three numbers,
just use one, minus one, zero to represent the neural network. This is our recent paper
clear that we maintain a full precision weight
during training time, but at inference time, we
only keep the scaling factor and the ternary weight. So during inference, we
only need three weights. That's very efficient and
making the model very small. This is the proportion
of the positive zero and negative weights, they can
change during the training. So is their absolute value. And this is the visualization of kernels by this trained ternary quantization. We can see some of them are
a corner detector like here. And also here. Some of them are maybe edge detector. For example, this filter some of them are corner detector like here this filter. Actually we don't need
such fine grain resolution. Just three weights are enough. So this is the validation
accuracy on ImageNet with AlexNet. So the threshline is the baseline accuracy with floating point 32. And the red line is our result. Pretty much the same accuracy
converged compared with the full precision weights. Last idea, Winograd Transformation. So this about how do we
implement deep neural nets, how do we implement the convolutions. So this is the conventional direct convolution implementation method. The slide credited to
Julien, a friend from Nvidia. So originally, we just do the element wise do a dot product for those
nine elements in the filter and nine elements in the
image and then sum it up. For example, for every
output we need nine times C number of multiplication and adds. Winograd Convolution is another
method, equivalent method. It's not lost, it's an
equivalent method proposed at first through this paper, Fast Algorithms for Convolution Neural Networks. That instead of directly
doing the convolution, move it one by one, at first it
transforms the input feature map to another feature map. Which contains only the
weight, contains only 1, 0.5, 2 that can efficiently
implement it with shift. And also transform the filter
into a four by four tensor. So what we are going to do here
is sum over c and do an element-wise element-wise product. So there are only 16
multiplications happening here. And then we do a inverse
transform to get four outputs. So the transform and the
inverse transform can be amortized and the multiplications,
whether it can ignored. So in order to get four output,
we need nine times channel times four, which is 36 times channel. Multiplications originally
for the direct convolution but now we need 16
times C of our output So that is 2.25x less
number of multiplications to perform the exact same multiplication. And here is a speedup. 2.25x, so theoretically,
2.25x speedup and in real, from cuDNN 5 they incorporated such Winograd Convolution algorithm. This is on the VGG net I
believe, the speedup is roughly 1.7 to 2x speedup. Pretty significant. And after cuDNN 5, the
cuDNN begins to use the Winograd Convolution algorithm. Okay, so far we have covered
those efficient algorithms for efficient inference. We covered pruning, weight
sharing, quantization, and also Winograd binary and ternary. So now let's see what is the
optimal hardware for those efficient inference? And what is a Google TPU? So there are a wide
range of domain specific architectures or ASICS
for deep neural networks. They have a common goal
is to minimize the memory access to save power. For example the Eyeriss from
MIT by using the RS Dataflow to minimize the off chip direct access. And DaDiannao from China
Academy of Science, buffered all the weights on
chip DRAM instead of having to go to off-chip DRAM. So the TPU from Google is
using eight bit integer to represent the numbers. And at Stanford I proposed
the EIE architecture that support those compressed and sparse deep neural network inference. So this is what the TPU looks like. It's actually smartly, can
be put into the disk drive up to four cards per server. And this is the high-level architecture for the Google TPU. Don't be overwhelmed, it's
actually, the kernel part here, is this giant matrix
multiplication unit. So it's a 256 by 256
matrix multiplication unit. So in one single cycle,
it can perform 64 kilo those number of multiplication
and accumulate operations. So running 700 Megahertz,
the throughput is 92 Teraops per second because it's actually integer operation. So we just about 25x as GPU
and more than 100x at the CPU. And notice, TPU has a really
large software-managed on-chip buffer. It is 24 megabytes. The cache for the CPU the
L3 cache is already 16 megabytes. This is 24 megabytes
which is pretty large. And it's powered by
two DDR3 DRAM channels. So this is a little weak
because the bandwidth is only 30 gigabytes per second
compared with the most recent GPU that HBM, 900
Gigabytes per second. The DDR4 is released in 2014,
so that makes sense because the design is a little during
that day, used the DDR3. But if you're using DDR4 or
even high-bandwidth memory, the performance can be even boosted. So this is a comparison
about Google's TPU compared with the CPU, GPU of this K80
GPU by the way, and the TPU. So the area is pretty much
smaller, like half the size of a CPU and GPU and the power
consumption is roughly 75 watts. And see this number, the
peak teraops per second is much higher than the
CPU and GPU is, about 90 teraops per second, which is pretty high. So here is a workload. Thanks to David sharing the slide. This is the workload at Google. They did a benchmark on these TPUs. So it's a little interesting
that convolution neural nets only account for 5% of
data-center workload. Most of them is multilayer perception, those fully connected layers. About 61% maybe for ads, I'm not sure. And about 29% of the workload
in data-center is the Long Short Term Memory. For example, speech recognition, or machine translation, I suspect. Remember just now we have seen there are 90 teraops per second. But what actually number
of teraops per second can be achieved? This is a basic tool to
measure the bottleneck of a computer system. Whether you are bottlenecked
by the arithmetic or you are bottlenecked by
the memory bandwidth. It's like if you have a bucket, the lowest part of the
bucket determines how much water we can hold in the bucket. So in this region, you are bottlenecked by the memory bandwidth. So the x-axis is the arithmetic intensity. Which is number of floating
point operations per byte the ratio between the
computation and memory of bandwidth overhead. So the y-axis, is the actual
attainable performance. Here is the peak performance for example. When you do a lot of operation
after you fetch a single piece of data, if you
can do a lot of operation on top of it, then you are
bottlenecked by the arithmetic. But after you fetch a lot
of data from the memory, but you just do a tiny
little bit of arithmetic, then you will be bottlenecked
by the memory bandwidth. So how much you can fetch
from the memory determines how much real performance you can get. And remember there is a ratio. When it is one here, this
region it happens to be the same as the turning point is the actual memory bandwidth of your system. So let's see what is the life for the TPU. The TPU's peak performance is really high, about 90 Tops per second. For those convolution nets,
they are pretty much saturating the peak performance. But there are lot of neural
networks that has a utlitization less than 10%, meaning that 90 T-ops
per second is actually achieves about three to 12
T-ops per second in real case. But why is it like that? The reason is, in order to
have those real-time guarantee that the user not wait for
too long, you cannot batch a lot of user's images
or speech voice data at the same time. So as a result, for those
fully connect layers, they have very little reuse,
so they are bottlenecked by the memory bandwidth. For those convolution neural
nets, for example this one, this blue one, that
achieve 86, which is CNN0. The ratio between the ops and the number of memory is the highest. It's pretty high, more than
2,000 compared with other multilayer perceptron or
long short term memory the ratio is pretty low. So this figure compares, this
is the TPU and this one is the CPU, this is the GPU. Here is memory bandwidth,
the peak memory bandwidth at a ratio of one here. So TPU has the highest memory bandwidth. And here is where are
these neural networks lie on this curve. So the asterisk is for the TPU. It's still higher than other dots, but if you're not comfortable
with this log scale figure, this is what it's like
putting it in linear roofline. So pretty much everything
disappeared except for the TPU results. So still, all these lines,
although they are higher than the CPU and GPU,
it's still way below the theoretical peak operations per second. So as I mentioned before,
it is really bottlenecked by the low latency requirement
so that it can have a large batch size. That's why you have low
operations per byte. And how do you solve this problem? You want to have less
number of memory footprint so that it can reduce the
memory bandwidth requirement. One solution is to compress
the model and the challenge is how do we build a hardware
that can do inference directly on the compressed model? So I'm going to introduce my
design of EIE, the Efficient Inference Engine, which
deals with those sparse and the compressed model to
save the memory bandwidth. And the rule of thumb, like
we mentioned before is taking out one bit of sparsity first. Anything times zero is zero. So don't store it, don't compute on it. And second idea is, you don't
need that much full precision, but you can approximate it. So by taking advantage
of the sparse weight, we get about a 10x saving in
the computation, 5x less memory footprint. The 2x difference is
due to index overhead. And by taking advantage
of the sparse activation, meaning after bandwidth,
if activation is zero, then ignore it. You save another 3x of computation. And then by such weight sharing mechanism, you can use four bits to
represent each weight rather than 32 bit. That's another eight times
saving in the memory footprint. So this is physically, logically
how the weights are stored. A four by eight matrix,
and this is how physically they are stored. Only the non-zero weights are stored. So you don't need to store those zeroes. You'll save the bandwidth
fetching those zeroes. And also I'm using the
relative index to further save the number of memory overhead. So in the computation
like this figure shows, we are running the
multiplication only on non-zero. If it's zero, then skip it. Only broadcast it to the non-zero weights and if it is zero, skip it. If it's a non-zero, do the multiplication. In another cycle, do the multiplication. So the idea is anything
multiplied by zero is zero. So this is a little complicated, I'm going to go very quickly. I'm going to have a lookup
table that decode the four bit weight into the 16 bit
weight and using the four bit relative index passed
through address accumulator to get the 16 bit absolute index. And this is what the hardware architecture like in the high level. You can feel free to refer
to my paper for detail. Okay speedup. So using such efficient
hardware architecture and also model compression,
this is the original result we have seen for
CPU, GPU, mobile GPU. Now EIE is here. 189 times faster than the
CPU and about 13 times faster than the GPU. So this is the energy
efficiency on the log scale, it's about 24,000x more
energy efficient than a CPU and about 3000x more energy
efficient than a GPU. It means for example,
previously if your battery can last for one hour, now it can last for 3000 hours for example. So if you say, ASIC is always
better than CPUs and GPUs because it's customized hardware. So this is comparing EIE with
the peer ASIC, for example DaDianNao and the TrueNorth. It has a better throughput,
better energy efficiency by order of magnitude,
compared with other ASICs. Not to mention that CPU, GPU and FPGAs. So we have covered half of the journey. We mentioned inference, we pretty much covered everything for inference. Now we are going to switch
gear and talk about training. How do we train neural
networks efficiently, how do we train it faster? So again, we are starting
with algorithm first, efficient algorithms
followed by the hardware for efficient training. So for efficient training
algorithms, I'm going to mention four topics. The first one is parallelization,
and then mixed precision training, which was just
released about one month ago and at NVIDIA GTC,
so it's fresh knowledge. And then model distillation,
followed by my work on Dense-Sparse-Dense training,
or better Regularization technique. So let's start with parallelization. So this figure shows, anyone in the hardware community. Most are very familiar with this figure. So as time goes by, what is the trend? For the number of transistors
is keeping increasing. But the single threaded
performance is getting plateaued in recent years. And also the frequency is getting
plateaued in recent years. Because of the power
constraint, to stop not scaling. And interesting thing is the
number of cores is increasing. So what we really need
to do is parallelization. How do we parallelize the
problem to take advantage of parallel processing? Actually there are a lot of
opportunities for parallelism in deep neural networks. For example, we can do data parallel. For example, feeding two
images into the same model and run them at the same time. This doesn't affect
latency for a single input. It doesn't make it shorter,
but it makes batch size larger basically if you have four
machines our effective batch size becomes four times as before. So it requires the
coordinated weight update. For example, this is a paper from Google. There is a parameter server
as a master and a couple of slaves running their own piece
of training data and update the gradient to the parameter
server and get the updated weight for them individually, that's how data parallelism is handled. Another idea is there could
be a model parallelism. You can sublet your model and handle it to different processors
or different threads. For example, there's this image,
you want to run convolution on this image that is
six dimension for loop. What you can do is you
can cut the input image by two by two blocks so that
each thread, or each processor handles one fourth of the image. Although there's a small
halo here in between you have to take care of. And also, you can parallelize by the output or input feature map. And for those fully connect layers, how do we parallelize the model? It's even simpler. You can cut the model into half and hand it to different threads. And the third idea, you can even do hyper-parameter parallel. For example, you can tune
your learning rate, your weight decay for different machines for those coarse-grained parallelism. So there are so many
alternatives you have to tune. Small summary of the parallelism. There are lots of parallelisms
in deep neural networks. For example, with data
parallelism, you can run multiple training images, but you
cannot have unlimited number of processors because you
are limited by batch size. If it's too large, stochastic gradient descent becomes gradient descent, that's not good. You can also run the model parallelism. Split the model, either
by cutting the image or cutting the convolution weights. Either cutting the image or cutting the fully connected layers. So it's very easy to get 16
to 64 GPUs training one model in parallel, having very good speedup. Almost linear speedup. Okay, next interesting
thing, mixed precision with FP16 or FP32. So remember in the
beginning of this lecture, I had a chart showing the
energy and area overhead for a 16 bit versus a 32 bit. Going from 32 bit to 16 bit,
you save about 4x the energy and 4x the area. So can we train a deep
neural network with such low precision with floating point
16 bit rather than 32 bit? It turns out we can do that partially. By partially, I mean we
need FP32 in some places. And where are those places? So we can do the multiplication
in 16 bit as input. And then we have to do the summation in 32 bit accumulation. And then convert the result
to 32 bit to store the weight. So that's where the mixed
precision comes from. So for example, we have
a master weight stored in floating point 32, we down
converted it to floating point 16 and then we do the
feed forward with 16 bit weight, 16 bit activation,
we get a 16 bit activation here in the end when we
are doing back propagation of the computation is also done
with floating point 16 bit. Very interesting here, for
the weights we get a floating point 16 bit gradient here for the weight. But when we are doing the
update, so W plus learning rate times the gradient,
that operation has to be done in 32 bit. That's where the mixed
precision is coming from. And see there are two
colors, which here is 16 bit, here is the 32 bit. That's where the mixed
precision comes from. So does such low precision
sacrifice your prediction accuracy for your model? So this is the figure from
NVIDIA just released a couple of weeks ago actually. Thanks to Paulius giving me the slide. The convergence between
floating point 32 versus the multi tensor up, which
is basically the mixed precision training, are
actually pretty much the same for convergence. If you zoom it in a little bit, they are pretty much the same. And for ResNet, the mixed
precision sometimes behaves a little better than the
full precision weight. Maybe because of noise. But in the end, after you
train the model, this is the result of AlexNet,
Inception V3, and ResNet-50 with FP32 versus FP16
mixed precision training. The accuracy is pretty much the same for these two methods. A little bit worse, but not by too much. So having talked about the
mixed precision training, the next idea is to train
with model distillation. For example, you can have
multiple neural networks, Googlenet, Vggnet, Resnet for example. And the question is, can
we take advantage of these different models? Of course we can do model
ensemble, can we utilitze them as teacher, to teach a small
junior neural network to have it perform as good as the
senior neural network. So this is the idea. You have multiple large
powerful senior neural networks to teach this student model. And hopefully it can get better results. And the idea to do that
is, instead of using this hard label, for example for
car, dog, cat, the probability for dog is 100%, but the
output of the geometric ensemble of those large
teacher neural networks maybe the dog has 90%
and the cat is about 10%, and the magic happens here. You want to have a
softened result label here. For example, the dog
is 30%, the cat is 20%. Still the dog is higher than the cat. So the prediction is
still correct, but it uses this soft label to train
the student neural network rather than use this hard label to train the student neural network. And mathematically, you
control how much do you make it soft by this temperature
during the soft max controlling by this temperature. And the result is that,
starting with the trained model that classifies 58.9% of
the test frames correctly, the new model converges to 57%. Only train on 3% of the data. So that's the magic for model distillation using this soft label. And the last idea is my recent paper using a better regularization
to train deep neural nets. We have seen these two figures before. We pruned the neural
network, having less number of weights, but have the same accuracy. Now what I did is to
recover and to retrain those weights shown in red
and make everything train out together to increase
the model capacity after it is trained at a low dimensional space. It's like you learn the trunk
first and then gradually add those leaves and
learn everything together. It turns out, on ImageNet it
performs relatively about 1% to 4% absolute improvement of accuracy. And is also general purpose,
works on long-short term memory and also recurrent neural
nets collaborated with Baidu. So I also open sourced
this special training model on the DSD Model Zoo, where
there are trained, all these models, GoogleNet, VGG,
ResNet, and also SqueezeNet, and also AlexNet. So if you are interested,
feel free to check out this Model Zoo and compare it
with the Caffe Model Zoo. Here's some examples on
dense-spare-dense training helps with image capture. For example, this is a
very challenging figure. The original baseline of
neural talk says a boy in a red shirt is climbing a rock wall. And the sparse model says
a young girl is jumping off a tree, probably
mistaking the hair with either the rock or the tree. But then sparse-dense
training by using this kind of regularization on a low
dimensional space, it says a young girl in a pink shirt
is swinging on a swing. And there are a lot of examples
due to the limit of time, I will not go over them one by one. For example, a group of
people are standing in front of a building, there's no building. A group of people are walking in the park. Feel free to check out the
paper and see more interesting results. Okay finally, we come to
hardware for efficient training. How to we take advantage of the algorithms we just mentioned. For example, parallelism,
mixed precision, how are the hardware designed to actually take advantage of such features. First GPUs, this is the
Nvidia PASCAL GPU, GP100, which was released last year. So it supports up to 20 Teraflops on FP16. It has 16 gigabytes of
high bandwidth memory. 750 gigabytes per second. So remember, computation
and memory bandwidth are the two factors determines
your overall performance. Whichever is lower, it will suffer. So this is a really high
bandwidth, 700 gigabytes compared with DDR3 is just 10
or 30 gigabytes per second. Consumes 300 Watts and it's done in 16 nanometer process and have a 160 gigabytes
per second NV Link. So remember we have
computation, we have memory, and the third thing is the communication. All three factors has to
be balanced in order to achieve a good performance. So this is very powerful,
but even more exciting, just about a month ago,
Jensen released the newest architecture called the Volta GPUs. And let's see what is
inside the Volta GPU. Just released less than a
month ago, so it has 15 of FP32 teraflops and what
is new here, there is 120 Tensor T-OPS, so specifically
designed for deep learning. And we'll later cover
what is the tensor core. And what is this 120 coming from. And rather than 750
gigabytes per second, this year, the HBM2, they are
using 900 gigabytes per second memory bandwidth. Very exciting. And 12 nanometer process has
a die size of more than 800 millimeters square. A really large chip and
supported by 300 gigabytes per second NVLink. So what's new in Volta, the
most interesting thing for us for deep learning, is this
thing called Tensor Core. So what is a Tensor Core? Tensor Core is actually
an instruction that can do the four by four matrix
times a four by four matrix. The fused FMA stands Fused
Multiplication and Add in this mixed precision operation. Just in one single clock cycle. So let's discern for a little
bit what does this mean. So mixed precision is exactly
as we mentioned in the last chapter, so we are having
FP16 for the multiplication, but for accumulation, we
are doing it with FP32. That's where the mixed
precision comes from. So let's say how many
operations, if it's four by four by four, it's 64
multiplications then just in one single cycle. That's 12x increase in
the speedup of the Volta compared with the Pascal, which
is released just less year. So this is the result for
matrix multiplication on different sizes. The speedup of Volta over
Pascal is roughly 3x faster doing these matrix multiplications. What we care more is not
only matrix multiplication but actually running the deep neural nets. So both for training and for inference. And for training on
ResNet-50, by taking advantage of this Tensor Core in this V100, it is 2.4x faster than
the P100 using FP32. So on the right hand side,
it compares the inference speedup, given a 7 microsecond
latency requirement. What is the number of images
per second it can process? It has a measurement of throughput. Again, the V100 over
P100, by taking advantage of the Tensor Core, is
3.7 faster than the P100. So this figure gives roughly
an idea, what is a Tensor Core, what is an integer unit, what
is a floating point unit. So this whole figure is a single SM stream multiprocessor. So SM is partitioned into
four processing blocks. One, two, three, four, right? And in each block there
are eight FP64 cores here and 16 FP32 and 16 INT32
cores here, units here. And then there are two of
the new mixed precision Tensor cores specifically
designed for deep learning. And also there are the one
warp scheduler, dispatch unit and Register File, as before. So what is new here is
the Tensor core unit here. So here is a figure comparing
the recent generations of Nvidia GPUs from Kepler to Maxwell to Pascal to Volta. We can see everything
is keeping improving. For example, the boost clock
has been increased from about 800 MHz to 1.4 GHz. And from the Volta generation
there begins to have the Tensor core units here,
which has never existed before. And before the Maxwell, the GPUs are using the GDDR5, and after the Pascal GPU, the HBM begins to came into place, the high-bandwidth memory. 750 gigabytes per second here. 900 gigabytes per second
compared with DDR3, 30 gigabytes per second. And memory size actually
didn't increase by too much, and the power consumption is actually also remaining roughly the same. But giving the increase of
computation, you can fit them in the fixed power envelope
that's still an exciting thing. And the manufacturing process
is actually improving from 28 nanometer, 16 nanometer,
all the way to 12 nanometer. And the chip area are also increasing to 800 millimeter-squared,
that's really huge. So, you may be interested
in the comparison of the GPU with the TPU, right? So how do they compare with each other? So in the original TPU paper, TPU actually designed
roughly in the year of 2015, and this is comparison
of the Pascal P40 GPU released in 2016. So, TPU, the power consumption is lower, is larger on chip memory of 24 megabytes, really large on-chip SRAM
managed by the software. And then both of them
support INT8 operations, while the inferences per second
given a 10 nanometer latency the comparison for TPU is 1X. For the P40 it's about 2X. So, just last week, in the Google I/O, a new nuclear bomb is landed on the Earth. That is the Google Cloud TPU. So now TPU not only support inference, but also support training. So there is a very limited
information we can get beyond this Google Blog. So their Cloud TPU delivers
up to 180 teraflops to train and run machine learning models. And this is multiple Cloud TPU, making it into a TPU pod, which is built with 16
the second generation TPUs and delivers up to 11.5 teraflops of machine learning acceleration. So in the Google Blog, they mentioned that one of the large scale translation models, Google translation models, used
to take a full day to train on 32 of best commercially-available
GPUs, probably P40 or P100, maybe. And now it trains to the same accuracy, just within one afternoon,
with just 1/8 of a TPU pod, which is pretty exciting. Okay, so as a little wrap-up. We covered a lot of stuff, we've mentioned the four dimension space
of algorithm and hardware, inference and training, we
covered the algorithms for inference, for example,
pruning and quantization, Winograd Convolution, binary, ternary, weight sharing, for example. And then the hardware for
the efficient inference. For example, the TPU, that take advantage of INT8, integer 8. And also my design of EIE
accelerator that take advantage of the sparsity, anything
multiplied by zero is zero, so don't store it, don't compute on it. And also the efficient algorithm
for training, for example, how do we do parallelization
and the most recent research on how do we use mixed precision
training by taking advantage of FP16 rather than FP32 to do training which is four times saving the energy and four times saving in the area, which doesn't quite sacrifice
the accuracy you'll get from the training. And also Dense-Sparse-Dense
training using better regularization sparse regularization, and also
the teacher-student model. You have multiple teacher on
your network and have a small student network that you
can distill the knowledge from the teacher in your
network by a temperature. And finally we covered the
hardware for efficient training and introduced two nuclear bombs. One is the Volta GPU, the
other is the TPU version two, the Cloud TPU and also
the amazing Tensor cores in the newest generation of Nvidia GPUs. And we also revealed the
progression of a wide range, the recent Nvidia GPUs
from the Kepler K40, that's actually when
I started my research, what we used in the beginning, all the way to and then K40, M40, and then Pascal and then
finally the exciting Volta GPU. So every year there is a
nuclear bomb in the spring. Okay, a little look ahead in the future. So in the future of the city
we can imagine there are a lot of AI applications using
smart society, smart care, IOT devices, smart retail,
for example, the Amazon Go, and also smart home, a lot of scenarios. And it poses a lot of challenges
on the hardware design that requires the low
latency, privacy, mobility and energy efficiency. You don't want your battery
to drain very quickly. So it's both challenging
and very exciting era for the code design for
both the machine learning deep neural network model architectures and also the hardware architecture. So we have moved from
PC era to mobile era. Now we are in the AI-First era, and hope you are as excited
as I am for this kind of brain-inspired cognitive
computing research. Thank you for your attention,
I'm glad to take questions. [applause] We have five minutes. Of course. - [Student] Can you commercialize
the deep architecture? - The architecture, yeah, some
of the ideas are pretty good. I think there's opportunity. Yeah. Yeah. The question is, what can we
do to make the hardware better? Oh, right, the question is how do we, the challenges and what
opportunity for those small embedded devices around
deep neural network or in general AI algorithms. Yeah, so those are the
algorithm I discussed in the beginning about inference. Here. These are the techniques
that can enable such inference or AI running
on embedded devices, by having less number of
weights, fewer bits per weight, and also quantization,
low rank approximation. The small matrix, same
accuracy, even going to binary, or ternary weights having just two bits to do the computation rather
than 16 or even 32 bit and also the Winograd Transformation. Those are also the enabling
algorithms for those low-power embedded devices. Okay, the question is, if it's
binary weight, the software developers may be not able
to take advantage of it. There is a way to take
advantage of binary weight. So in one register there are 32 bit. Now you can think of it
as a 32-way parallelism. Each bit is a single operation. So say previously we
have 10 ops per second. Now you get 330 ops per second. You can do this bitwise operations. For example, XOR operations. So one register file, one operation becomes 32 operation. So there is a paper called XORmad, they very amazing implemented on the Raspberry Pi using this feature to do real-time detection,
very cool stuff. Yeah. Yeah, so the trade-off is
always so the power area and performance in general,
all the hardware design have to take into account
the performance, the power, and also the area. When machine learning
comes, there's a fourth figure of merit which is the accuracy. What is the accuracy? And there is a fifth one
which is programmability. So how general is your hardware? For example, if Google just
want to use that for AI and deep learning, it's totally fine that we can have a fully
very specialized architecture just for deep learning
to support convolution, multi-layered perception,
long-short-term memory, but GPUS, you also want
to have support for those scientific computing
or graphics, AR and VR. So that's a difference, first of all. And TPU basically is a ASIC, right? It's a very fixed function
but you can still program it with those coarse instructions
so people from Google roughly designed those coarse
granularity instruction. For example, one instruction
just load the matrix, store a matrix, do convolutions, do matrix multiplications. Those coarse-grain instructions and they have a software-managed memory, also called a scratchpad. It's different from
cache where it determines where to evict something
from the cache, but now, since you know the computation pattern, there's no need to do out-of-order execution, to do branch prediction, no such things. Everything is determined,
so you can take the multi of it and maintain a fully
software-managed scratchpad to reduce the data movement
and remember, data movement is the key for reducing
the memory footprint and energy consumption. So, yeah. Mobilia and Nobana architectures
actually I'm not quite familiar, didn't prepare those slides, so, comment it a little bit later, no. Oh, yeah, of course. Those are always and
can certainly be applied to low-power embedded devices. If you're interested, I can show you a... Whoops. Some examples of, oops. Where is that? Of my previous projects
running deep neural nets. For example, on a drone,
this is using a Nvidia TK1 mobile GPU to do real-time
tracking and detection. This is me playing my nunchaku. Filmed by a drone to do the
detection and tracking. And also, this FPGA doing
the deep neural network. It's pretty small. This large, doing the face-alignment and detecting the eyes,
the nose and the mouth, at a pretty high framerate. Consuming only three watts. This is a project I did
at Facebook doing the deep neural nets on the mobile phone to do image classification, for
example, it says it's a laptop, or you can feed it with
an image and it says it's a selfie, has person
and the face, et cetera. So there's lots of opportunity for those embedded or mobile-deployment
of deep neural nets. No, there is a team doing that, but I cannot comment too much, probably. There is a team at Google
doing that sort of stuff, yeah. Okay, thanks, everyone. If you have any questions,
feel free to drop me a e-mail. 

- Okay, sounds like it is. I'll be telling you about
adversarial examples and adversarial training today. Thank you. As an overview, I will
start off by telling you what adversarial examples are, and then I'll explain why they happen, why it's possible for them to exist. I'll talk a little bit about
how adversarial examples pose real world security threats, that they can actually
be used to compromise systems built on machine learning. I'll tell you what the
defenses are so far, but mostly defenses are
an open research problem that I hope some of you
will move on to tackle. And then finally I'll tell you how to use adversarial examples to improve other machine
learning algorithms even if you want to build a
machine learning algorithm that won't face a real world adversary. Looking at the big picture and
the context for this lecture, I think most of you are probably here because you've heard
how incredibly powerful and successful machine learning is, that very many different tasks that could not be solved
with software before are now solvable thanks to deep learning and convolutional networks
and gradient descent. All of these technologies
that are working really well. Until just a few years ago, these technologies didn't really work. In about 2013, we started to see that deep learning achieved
human level performance at a lot of different tasks. We saw that convolutional nets could recognize objects and images and score about the same as
people in those benchmarks, with the caveat that
part of the reason that algorithms score as well as people is that people can't tell Alaskan Huskies from
Siberian Huskies very well, but modulo the strangeness
of the benchmarks deep learning caught up to
about human level performance for object recognition in about 2013. That same year, we also
saw that object recognition applied to human faces caught
up to about human level. That suddenly we had computers that could recognize
faces about as well as you or I could recognize
faces of strangers. You can recognize the faces
of your friends and family better than a computer,
but when you're dealing with people that you haven't
had a lot of experience with the computer caught up
to us in about 2013. We also saw that computers caught up to humans for reading type
written fonts in photos in about 2013. It even got the point that we
could no longer use CAPTCHAs to tell whether a user of
a webpage is human or not because the convolutional network is better at reading obfuscated
text than a human is. So with this context today of deep learning working really well especially for computer vision it's a little bit unusual to think about the computer making a mistake. Before about 2013,
nobody was ever surprised if the computer made a mistake. That was the rule not the exception, and so today's topic which is all about unusual mistakes that deep
learning algorithms make this topic wasn't really
a serious avenue of study until the algorithms started
to work well most of the time, and now people study
the way that they break now that that's actually the
exception rather than the rule. An adversarial example is an example that has been carefully
computed to be misclassified. In a lot of cases we're
able to make the new image indistinguishable to a human observer from the original image. Here, I show you one where
we start with a panda. On the left this is a panda that has not been modified in any way, and the convolutional
network trained on the image in that dataset is able to
recognize it as being a panda. One interesting thing is that the model doesn't have a whole lot of
confidence in that decision. It assigns about 60% probability to this image being a panda. If we then compute exactly the way that we could modify the image to cause the convolutional
network to make a mistake we find that the optimal direction to move all the pixels is given
by this image in the middle. To a human it looks a lot like noise. It's not actually noise. It's carefully computed as a function of the parameters of the network. There's actually a lot of structure there. If we multiply that image
of the structured attack by a very small coefficient and
add it to the original panda we get an image that a human can't tell from the original panda. In fact, on this slide
there is no difference between the panda on the left and the panda on the right. When we present the image
to convolutional network we use 32-bit floating point values. The monitor here can
only display eight bits of color resolution, and
we have made a change that's just barely too small to affect the smallest
of those eight bits, but it effects the other 24 of the 32-bit floating
point representation, and that little tiny change is enough to fool the convolutional network into recognizing this image
of a panda as being a gibbon. Another interesting thing is that it doesn't just change the class. It's not that we just barely
found the decision boundary and just barely stepped across it. The convolutional network
actually has much more confidence in its incorrect prediction, that the image on the right is a gibbon, than it had for the
original being a panda. On the right, it believes that the image is a gibbon with 99.9% probability, so before it thought that there was about 1/3 chance that it was
something other than a panda, and now it's about as
certain as it can possibly be that it's a gibbon. As a little bit of history,
people have studied ways of computing attacks to fool different machine learning models since at least about
2004, and maybe earlier. For a long time this
was done in the context of fooling spam detectors. In about 2013, Battista Biggio found that you could fool neural
networks in this way, and around the same time my
colleague, Christian Szegedy, found that you could
make this kind of attack against deep neural networks just by using an optimization algorithm to search on the input of the image. A lot of what I'll be
telling you about today is my own follow-up work on this topic, but I've spent a lot of my
career over the past few years understanding why these
attacks are possible and why it's so easy to fool
these convolutional networks. When my colleague, Christian, first discovered this phenomenon independently from Battista
Biggio but around the same time, he found that it was actually a result of a visualization he was trying to make. He wasn't studying security. He wasn't studying how
to fool a neural network. Instead, he had a convolutional network that could recognize objects very well, and he wants to understand how it worked, so he thought that maybe he
could take an image of a scene, for example a picture of a ship, and he could gradually
transform that image into something that the
network would recognize as being an airplane. Over the course of that transformation, he could see how the
features of the input change. You might expect that maybe the background
                                                                           
167
00:07:34,360 --> 00:07:37,692
would turn blue to look like
the sky behind an airplane, or you might expect that the ship would grow wings to look
more like an airplane. You could conclude from
that that the convolution uses the blue sky or uses the
wings to recognize airplanes. That's actually not really
what happened at all. Each of these panels
here shows an animation that you read left to
right, top to bottom. Each panel is another
step of gradient ascent on the log probability that
the input is an airplane according to a convolutional net model, and then we follow the gradient
on the input to the image. You're probably used to
following the gradient on the parameters of a model. You can use the back propagation algorithm to compute the gradient on the input image using exactly the same procedure that you would use to compute the gradient on the parameters. In this animation of the
ship in the upper left, we see five panels that all
look basically the same. Gradient descent doesn't seem to have moved the image at all, but by the last panel the
network is completely confident that this is an airplane. When you first code up
this kind of experiment, especially if you don't
know what's going to happen, it feels a little bit like
you have a bug in your script and you're just displaying the same image over and over again. The first time I did it, I couldn't believe it was happening, and I had to open up the images in NumPy, and take the difference of them, and make sure that there was actually a non-zero difference
in there, but there is. I show several different animations here of a ship, a car, a cat, and a truck. The only one where I actually
see any change at all is the image of the cat. The color of the cat's
face changes a little bit, and maybe it becomes a little bit more like the color of a metal airplane. Other than that, I don't see any changes in any of these animations, and I don't see anything very
suggestive of an airplane. So gradient descent, rather
than turning the input into an example of an airplane, has found an image that fools the network into thinking that the
input is an airplane. And if we were malicious attackers we didn't even have to work
very hard to figure out how to fool the network. We just asked the network to give us an image of an airplane, and it gave us something
that fools it into thinking that the input is an airplane. When Christian first published this work, a lot of articles came
out with titles like, The Flaw Looking At Every
Deep Neural Network, or Deep Learning has Deep Flaws. It's important to remember
that these vulnerabilities apply to essentially every
machine learning algorithm that we've studied so far. Some of them like RBF networks and partisan density estimators are able to resist this effect somewhat, but even very simple
machine learning algorithms are highly vulnerable
to adversarial examples. In this image, I show an animation of what happens when we
attack a linear model, so it's not a deep algorithm at all. It's just a shallow softmax model. You multiply by a matrix, you
add a vector of bias terms, you apply the softmax function, and you've got your
probability distribution over the 10 MNIST classes. At the upper left, I start
with an image of a nine, and then as we move left
to right, top to bottom, I gradually transform it to be a zero. Where I've drawn the yellow box, the model assigns high
probability to it being a zero. I forget exactly what my threshold
was for high probability, but I think it was around 0.9 or so. Then as we move to the second row, I transform it into a one, and the second yellow box indicates where we've successfully fooled the model into thinking it's a one
with high probability. And then as you read the
rest of the yellow boxes left to right, top to bottom, we go through the twos,
threes, fours, and so on, until finally at the lower right we have a nine that has
a yellow box around it, and it actually looks like a nine, but in this case the only reason it actually looks like a nine is that we started the
whole process with a nine. We successfully swept through
all 10 classes of MNIST without substantially changing
the image of the digit in any way that would interfere
with human recognition. This linear model was actually
extremely easy to fool. Besides linear models, we've also seen that we can fool many different
kinds of linear models including logistic regression and SVMs. We've also found that we
can fool decision trees, and to a lesser extent,
nearest neighbors classifiers. We wanted to explain
exactly why this happens. Back in about 2014, after we'd
published the original paper where we'd said that these problems exist, we were trying to figure
out why they happen. When we wrote our first paper, we thought that basically
this is a form of overfitting, that you have a very
complicated deep neural network, it learns to fit the training set, its behavior on the test
set is somewhat undefined, and then it makes random mistakes that an attacker can exploit. Let's walk through what
that story looks like somewhat concretely. I have here a training
set of three blue X's and three green O's. We want to make a classifier that can recognize X's and recognize O's. We have a very complicated classifier that can easily fit the training set, so we represent everywhere it believes X's should be with blobs of blue color, and I've drawn a blob of blue around all of the training set X's, so it correctly classifies
the training set. It also has a blob of green
mass showing where the O's are, and it successfully fits all
of the green training set O's, but then because this is a
very complicated function and it has just way more parameters than it actually needs to
represent the training task, it throws little blobs of probability mass around the rest of space randomly. On the left there's a blob of green space that's kind of near the training set X's, and I've drawn a red X there to show that maybe this would be
an adversarial example where we expect the
classification to be X, but the model assigns O. On the right, I've shown
that there's a red O where we have another adversarial example. We're very near the other O's. We might expect the model to
assign this class to be an O, and yet because it's drawn blue mass there it's actually assigning it to be an X. If overfitting is really the story then each adversarial
example is more or less the result of bad luck and
also more or less unique. If we fit the model again or we fit a slightly different model we would expect to make
different random mistakes on this points that are
off the training set, but that was actually
not what we found at all. We found that many different
models would misclassify the same adversarial examples, and they would assign
the same class to them. We also found that if
we took the difference between an original example
and an adversarial example then we had a direction in input space and we could add that same offset vector to any clean example, and
we would almost always get an adversarial example as a result. So we started to realize that there was systematic
effect going on here, not just a random effect. That led us to another idea which is that adversarial examples might actually be more like underfitting rather than overfitting. They might actually come from
the model being too linear. Here I draw the same task again where we have the same manifold of O's and the same line of X's, and this time I fit a
linear model to the data set rather than fitting a high
capacity, non-linear model to it. We see that we get a dividing hyperplane running in between the two classes. This hyperplane doesn't really capture the true structure of the classes. The O's are clearly arranged
in a C-shaped manifold. If we keep walking past
the end of the O's, we've crossed the decision
boundary and we've drawn a red O where even though we're very
near the decision boundary and near other O's we
believe that it is now an X. Similarly we can take
steps that go from near X's to just over the line that
are classified as O's. Another thing that's somewhat
unusual about this plot is that if we look at the lower
left or upper right corners these corners are very
confidently classified as being X's on the lower
left or O's on the upper right even though we've never seen
any data over there at all. The linear model family forces the model to have very high
confidence in these regions that are very far from
the decision boundary. We've seen that linear
models can actually assign really unusual confidence
as you move very far from the decision boundary, even if there isn't any data there, but are deep neural networks actually anything like linear models? Could linear models
actually explain anything about how it is that
deep neural nets fail? It turns out that modern deep neural nets are actually very piecewise linear, so rather than being a
single linear function they are piecewise linear with maybe not that many linear pieces. If we use rectified linear units then the mapping from the input
image to the output logits is literally a piecewise linear function. By the logits I mean the
un-normalized log probabilities before we apply the softmax
op at the output of the model. There are other neural networks like maxout networks that are also literally piecewise linear. And then there are several
that become very close to it. Before rectified linear
units became popular most people used to use sigmoid
units of one form or another either logistic sigmoid or
hyperbolic tangent units. These sigmoidal units have
to be carefully tuned, especially at initialization so that you spend most of your time near the center of the sigmoid where the sigmoid is approximately linear. Then finally, the LSTM, a
kind of recurrent network that is one of the most popular
recurrent networks today, uses addition from one
time step to the next in order to accumulate and
remember information over time. Addition is a particularly
simple form of linearity, so we can see that the interaction from a very distant time step
in the past and the present is highly linear within an LSTM. Now to be clear, I'm
speaking about the mapping from the input of the model
to the output of the model. That's what I'm saying
is close to being linear or is piecewise linear
with relatively few pieces. The mapping from the
parameters of the network to the output of the network is non-linear because the weight matrices
at each layer of the network are multiplied together. So we actually get extremely
non-linear reactions between parameters and the output. That's what makes training a
neural network so difficult. But the mapping from
the input to the output is much more linear and predictable, and it means that optimization problems that aim to optimize
the input to the model are much easier than optimization problems that aim to optimize the parameters. If we go and look for
this happening in practice we can take a convolutional network and trace out a one-dimensional path through its input space. So what we're doing here is
we're choosing a clean example. It's an image of a white
car on a red background, and we are choosing a direction that will travel through space. We are going to have a coefficient epsilon that we multiply by this direction. When epsilon is negative 30, like at the left end of the plot, we're subtracting off a lot
of this unit vector direction. When epsilon is zero, like
in the middle of the plot, we're visiting the original
image from the data set, and when epsilon is positive 30, like at the right end of the plot, we're adding this
direction onto the input. In the panel on the left,
I show you an animation where we move from
epsilon equals negative 30 as up to epsilon equals positive 30. You read the animation left
to right, top to bottom, and everywhere that there's a yellow box the input has correctly
recognized as being a car. On the upper left, you see
that it looks mostly blue. On the lower right, it's
hard to tell what's going on. It's kind of reddish and so on. In the middle row, just after
where the yellow boxes end you can see pretty clearly that it's a car on a red background, though the image is small on these slides. What's interesting to
look at here is the logits that the model outputs. This is a deep convolutional
rectified linear unit network. Because it uses rectified linear units, we know that the output is
a piecewise linear function of the input to the model. The main question we're
asking by making this plot is how many different pieces does this piecewise linear function have if we look at one
particular cross section. You might think that maybe a deep net is going to represent some extremely wiggly complicated
function with lots and lots of linear pieces no matter
which cross section you look in. Or we might find that it
has more or less two pieces for each function we look at. Each of the different curves on this plot is the logits for a different class. We see that out at the tails of the plot that the frog class is the most likely, and the frog class basically looks like a big v-shaped function. The logits for the frog
class become very high when epsilon is negative
30 or positive 30, and they drop down and
become a little bit negative when epsilon is zero. The car class, listed as automobile here, it's actually high in the middle, and the car is correctly recognized. As we sweep out to very negative epsilon, the logits for the car class do increase, but they don't increase nearly as quickly as the logits for the frog class. So, we've found a direction that's associated with the frog class and as we follow it out to a
relatively large perturbation, we find that the model
extrapolates linearly and begins to make a very
unreasonable prediction that the frog class is extremely likely just because we've moved for a long time in this direction that
was locally associated with the frog class being more likely. When we actually go and
construct adversarial examples, we need to remember that we're able to get quite a large perturbation without changing the image very much as far as a human being is concerned. So here I show you a
handwritten digit three, and I'm going to change it
in several different ways, and all of these changes have the same L2 norm perturbation. In the top row, I'm going to
change the three into a seven just by looking for the nearest
seven in the training set. The difference between those two is this image that looks a
little bit like the seven wrapped in some black lines. So here white pixels in the middle image in the perturbation column, the white pixels
represent adding something and black pixels represent
subtracting something as you move from the left
column to the right column. So when we take the three and
we apply this perturbation that transforms it into a seven, we can measure the L2
norm of that perturbation. And it turns out to
have an L2 norm of 3.96. That gives you kind of a reference for how big these perturbations can be. In the middle row, we apply a perturbation of exactly the same size, but with the direction chosen randomly. In this case we don't actually change the class of the three at all, we just get some random noise that didn't really change the class. A human could still easily
read it as being a three. And then finally at the very bottom row, we take the three and we
just erase a piece of it with a perturbation of the same norm and we turn it into something that doesn't have any class at all. It's not a three, it's not a seven, it's just a defective input. All of these changes can happen with the same L2 norm perturbation. And actually a lot of the time
with adversarial examples, you make perturbations that
have an even larger L2 norm. What's going on is that there are several different
pixels in the image, and so small changes to individual pixels can add up to relatively large vectors. For larger datasets like ImageNet, where there's even more pixels, you can make very small
changes to each pixel that travel very far in vector space as measured by the L2 norm. That means that you can
actually make changes that are almost imperceptible but actually move you really far and get a large dot product with the coefficients
of the linear function that the model represents. It also means that when we're constructing adversarial examples, we need to make sure that the
adversarial example procedure isn't able to do what happened in the top row of this slide here. So in the top row of this slide, we took the three and we actually just changed it into a seven. So when the model says that the image in the upper right is a
seven, it's not a mistake. We actually just changed the input class. When we build adversarial examples, we want to make sure that
we're measuring real mistakes. If we're experimenters studying how easy a network is to fool, we want to make sure that
we're actually fooling it and not just changing the input class. And if we're an attacker, we
actually want to make sure that we're causing
misbehavior in the system. To do that, when we build
adversarial examples, we use the maxnorm to
constrain the perturbation. Basically this says
that no pixel can change by more than some amount epsilon. So the L2 norm can get really big, but you can't concentrate all the changes for that L2 norm to erase
pieces of the digit, like in the bottom row here
we erased the top of a three. One very fast way to build
an adversarial example is just to take the gradient of the cost that you used to train the network with respect to the input, and then take the sign of that gradient. The sign is essentially
enforcing the maxnorm constraint. You're only allowed to change the input by up to epsilon at each pixel, so if you just take the sign it tells you whether you want to add
epsilon or subtract epsilon in order to hurt the network. You can view this as
taking the observation that the network is more or less linear, as we showed on this slide, and using that to motivate building a first order
Taylor series approximation of the neural network's cost. And then subject to that
Taylor series approximation, we want to maximize the cost following this maxnorm constraint. And that gives us this
technique that we call the fast gradient sign method. If you want to just get your hands dirty and start making adversarial
examples really quickly, or if you have an algorithm
where you want to train on adversarial examples in
the inner loop of learning, this method will make
adversarial examples for you very, very quickly. In practice you should
also use other methods, like Nicholas Carlini's attack based on multiple steps of the Adam optimizer, to make sure that you
have a very strong attack that you bring out when
you think you have a model that might be more powerful. A lot of the time people
find that they can defeat the fast gradient sign method and think that they've
built a successful defense, but then when you bring
out a more powerful method that takes longer to evaluate, they find that they can't overcome the more computationally expensive attack. I've told you that
adversarial examples happen because the model is very linear. And then I told you that we could use this linearity assumption to build this attack, the
fast gradient sign method. This method, when applied
to a regular neural network that doesn't have any special defenses, will get over a 99% attack success rate. So that seems to confirm, somewhat, this hypothesis that adversarial examples come from the model being far too linear and extrapolating in linear
fashions when it shouldn't. Well we can actually go
looking for some more evidence. My friend David Warde-Farley
and I built these maps of the decision boundaries
of neural networks. And we found that they are consistent with the linearity hypothesis. So the FGSM is that attack method that I described in the previous slide, where we take the sign of the gradient. We'd like to build a map of a two-dimensional cross
section of input space and show which classes are assigned to the data at each point. In the grid on the right,
each different cell, each little square within the grid, is a map of a CIFAR-10
classifier's decision boundary, with each cell
corresponding to a different CIFAR-10 testing sample. On the left I show you a little legend where you can understand
what each cell means. The very center of each
cell corresponds to the original example
from the CIFAR-10 dataset with no modification. As we move left to right in the cell, we're moving in the direction of the fast gradient sign method attack. So just the sign of the gradient. As we move up and down within the cell, we're moving in a random
direction that's orthogonal to the fast gradient sign method direction. So we get to see a cross
section, a 2D cross section of CIFAR-10 decision space. At each pixel within this map, we plot a color that tells us
which class is assigned there. We use white pixels to indicate that the correct class was chosen, and then we used different
colors to represent all of the other incorrect classes. You can see that in nearly all of the grid cells on the right, roughly the left half
of the image is white. So roughly the left half of the image has been correctly classified. As we move to the right, we
see that there is usually a different color on the right half. And the boundaries between these regions are approximately linear. What's going on here is that
the fast gradient sign method has identified a direction where if we get a large dot
product with that direction we can get an adversarial example. And from this we can see
that adversarial examples live more or less in linear subspaces. When we first discovered
adversarial examples, we thought that they might
live in little tiny pockets. In the first paper we
actually speculated that maybe they're a little bit
like the rational numbers, hiding out finely tiled
among the real numbers, with nearly every real number
being near a rational number. We thought that because
we were able to find an adversarial example corresponding to every clean example that
we loaded into the network. After doing this further analysis, we found that what's happening
is that every real example is near one of these
linear decision boundaries where you cross over into
an adversarial subspace. And once you're in that
adversarial subspace, all the other points nearby
are also adversarial examples that will be misclassified. This has security implications because it means you only need
to get the direction right. You don't need to find an
exact coordinate in space. You just need to find a direction that has a large dot product
with the sign of the gradient. And once you move more
or less approximately in that direction, you can fool the model. We also made another cross section where after using the left-right axis as the fast gradient sign method, we looked for a second direction that has high dot
product with the gradient so we could make both axes adversarial. And in this case you see that we get linear decision boundaries. They're now oriented diagonally
rather than vertically, but you can see that there's actually this two-dimensional subspace of adversarial examples
that we can cross into. Finally it's important to remember that adversarial examples are not noise. You can add a lot of noise
to an adversarial example and it will stay adversarial. You can add a lot of
noise to a clean example and it will stay clean. Here we make random cross sections where both axes are
randomly chosen directions. And you see that on CIFAR-10, most of the cells are completely white, meaning that they're correctly
classified to start with, and when you add noise they
stay correctly classified. We also see that the
model makes some mistakes because this is the test set. And generally if a test example
starts out misclassified, adding the noise doesn't change it. There are a few exceptions where, if you look in the
third row, third column, noise actually can make the
model misclassify the example for especially large noise values. And there's even some where, in the top row there's one
example you can see where the model is misclassifying
the test example to start with but then noise can change it
to be correctly classified. For the most part, noise
has very little effect on the classification decision compared to adversarial examples. What's going on here is that
in high dimensional spaces, if you choose some reference vector and then you choose a random vector in that high dimensional space, the random vector will, on average, have zero dot product
with the reference vector. So if you think about making a first order Taylor series
approximation of your cost, and thinking about how your
Taylor series approximation predicts that random vectors
will change your cost. You see that random vectors on average have no effect on the cost. But adversarial examples
are chosen to maximize it. In these plots we looked
in two dimensions. More recently, Florian
Tramer here at Stanford got interested in finding out just how many dimensions
there are to these subspaces where the adversarial examples lie in a thick contiguous region. And we came up with an algorithm together where you actually look for several different orthogonal vectors that all have a large dot
product with the gradient. By looking in several different orthogonal directions simultaneously, we can map out this kind of polytope where many different
adversarial examples live. We found out that this adversarial region has on average about 25 dimensions. If you look at different
examples you'll find different numbers of
adversarial dimensions. But on average on MNIST
we found it was about 25. So what's interesting
here is the dimensionality actually tells you something about how likely you are to find
an adversarial example by generating random noise. If every direction were adversarial, then any change would
cause a misclassification. If most of the directions
were adversarial, then random directions would
end up being adversarial just by accident most of the time. And then if there was only
one adversarial direction, you'd almost never find that direction just by adding random noise. When there's 25 you have a
chance of doing it sometimes. Another interesting thing
is that different models will often misclassify the
same adversarial examples. The subspace dimensionality
of the adversarial subspace relates to that transfer property. The larger the dimensionality
of the subspace, the more likely it is that the subspaces for two models will intersect. So if you have two different models that have a very large
adversarial subspace, you know that you can probably transfer adversarial examples
from one to the other. But if the adversarial
subspace is very small, then unless there's some kind
of really systematic effect forcing them to share
exactly the same subspace, it seems less likely that
you'll be able to transfer examples just due to the
subspaces randomly aligning. A lot of the time in
the adversarial example research community, we refer back to the story of Clever Hans. This comes from an essay
by Bob Sturm called Clever Hans, Clever Algorithms. Because Clever Hans is
a pretty good metaphor for what's happening with
machine learning algorithms. So Clever Hans was a horse
that lived in the early 1900s. His owner trained him to
do arithmetic problems. So you could ask him, "Clever Hans, "what's two plus one?" And he would answer by tapping his hoof. And after the third tap,
everybody would start cheering and clapping and looking excited because he'd actually done
an arithmetic problem. Well it turned out that he hadn't actually
learned to do arithmetic. But it was actually
pretty hard to figure out what was going on. His owner was not trying
to defraud anybody, his owner actually believed
he could do arithmetic. And presumably Clever Hans himself was not trying to trick anybody. But eventually a psychologist examined him and found that if he
was put in a room alone without an audience, and the person asking the
questions wore a mask, he couldn't figure out
when to stop tapping. You'd ask him, "Clever Hans, "what's one plus one?" And he'd just [knocking] keep staring at your face, waiting for you to give him some sign
that he was done tapping. So everybody in this situation was trying to do the right thing. Clever Hans was trying
to do whatever it took to get the apple that
his owner would give him when he answered an arithmetic problem. His owner did his best
to train him correctly with real arithmetic questions and real rewards for correct answers. And what happened was that Clever Hans inadvertently focused on the wrong cue. He found this cue of
people's social reactions that could reliably help
him solve the problem, but then it didn't
generalize to a test set where you intentionally
took that cue away. It did generalize to a
naturally occurring test set, where he had an audience. So that's more or less what's happening with machine learning algorithms. They've found these very linear patterns that can fit the training data, and these linear patterns even
generalize to the test data. They've learned to handle
any example that comes from the same distribution
as their training data. But then if you shift the distribution that you test them on, if a malicious adversary
actually creates examples that are intended to fool them, they're very easily fooled. In fact we find that modern
machine learning algorithms are wrong almost everywhere. We tend to think of them as
being correct most of the time, because when we run them on
naturally occurring inputs they achieve very high
accuracy percentages. But if we look instead
of as the percentage of samples from an IID test set, if we look at the percentage
of the space in RN that is correctly classified, we find that they
misclassify almost everything and they behave reasonably
only on a very thin manifold surrounding the data
that we train them on. In this plot, I show you
several different examples of Gaussian noise that I've run through
a CIFAR-10 classifier. Everywhere that there is a pink box, the classifier thinks
that there is something rather than nothing. I'll come back to what
that means in a second. Everywhere that there is a yellow box, one step of the fast gradient sign method was able to persuade the
model that it was looking specifically at an airplane. I chose the airplane class because it was the one with
the lowest success rate. It had about a 25% success rate. That means an attacker
would need four chances to get noise recognized as
an airplane on this model. An interesting thing,
and appropriate enough given the story of Clever Hans, is that this model found
that about 70% of RN was classified as a horse. So I mentioned that this model will say that noise is something
rather than nothing. And it's actually kind of
important to think about how we evaluate that. If you have a softmax classifier, it has to give you a distribution over the n different classes
that you train it on. So there's a few ways that you can argue that the model is telling you that there's something
rather than nothing. One is you can say, if it
assigns something like 90% to one particular class, that seems to be voting
for that class being there. We'd much rather see it give us something like a uniform
distribution saying this noise doesn't look like
anything in the training set so it's equally likely
to be a horse or a car. And that's not what the model does. It'll say, this is very
definitely a horse. Another thing that you
can do is you can replace the last layer of the model. For example, you can use a
sigmoid output for each class. And then the model is actually
capable of telling you that any subset of classes is present. It could actually tell you that an image is both a horse and a car. And what we would like
it to do for the noise is tell us that none of
the classes is present, that all of the sigmoids
should have a value of less than 1/2. And 1/2 isn't even
particularly a low threshold. We could reasonably expect that
all of the sigmoids would be less than 0.01 for such a
defective input as this. But what we find instead
is that the sigmoids tend to have at least one class present just when we run Gaussian noise of sufficient norm through the model. We've also found that we
can do adversarial examples for reinforcement learning. And there's a video for this. I'll upload the slides after the talk and you can follow the link. Unfortunately I wasn't able
to get the WiFi to work so I can't show you the video animated. But I can describe
basically what's going on from this still here. There's a game Seaquest on Atari where you can train
reinforcement learning agents to play that game. And you can take the raw input pixels and you can take the
fast gradient sign method or other attacks that use other
norms besides the max norm, and compute perturbations
that are intended to change the action that
the policy would select. So the reinforcement learning policy, you can think of it as just
being like a classifier that looks at a frame. And instead of categorizing the input into a particular category, it gives you a softmax
distribution over actions to take. So if we just take that and
say that the most likely action should have its accuracy be
decreased by the adversary. Sorry, to have its probability be decreased by the adversary, you'll get these
perturbations of input frames that you can then apply
and cause the agent to play different actions
than it would have otherwise. And using this you can make the agent play Seaquest very, very badly. It's maybe not the most
interesting possible thing. What we'd really like is an environment where there are many different
reward functions available for us to study. So for example, if you had a robot that was intended to cook scrambled eggs, and you had a reward function measuring how well it's cooking scrambled eggs, and you had another reward function measuring how well it's
cooking chocolate cake, it would be really
interesting if we could make adversarial examples that cause the robot to make a chocolate cake when the user intended for
it to make scrambled eggs. That's because it's very
difficult to succeed at something and it's relatively straightforward
to make a system fail. So right now, adversarial examples for RL are very good at showing that
we can make RL agents fail. But we haven't yet been
able to hijack them and make them do a complicated task that's different from
what their owner intended. Seems like it's one of the next steps in adversarial example research though. If we look at high-dimension
linear models, we can actually see that a lot of this is very simple and straightforward. Here we have a logistic regression model that classifies sevens and threes. So the whole model can be
described just by a weight vector and a single scalar bias term. We don't really need to see the
bias term for this exercise. If you look on the left
I've plotted the weights that we used to discriminate
sevens and threes. The weights should look a
little bit like the difference between the average seven
and the average three. And then down at the bottom we've taken the sign of the weights. So the gradient for a
logistic regression model is going to be proportional
to the weights. And then the sign of the weights gives you essentially the sign of the gradient. So we can do the fast gradient sign method to attack this model just
by looking at its weights. In the examples in the panel that's the second column from the left we can see clean examples. And then on the right we've
just added or subtracted this image of the sign of
the weights off of them. To you and me as human observers, the sign of the weights
is just like garbage that's in the background, and we more or less filter it out. It doesn't look particularly
interesting to us. It doesn't grab our attention. To the logistic regression model this image of the sign of the weights is the most salient thing that could ever appear in the image. When it's positive it looks like the world's most quintessential seven. When it's negative it looks like the world's most quintessential three. And so the model makes its decision almost entirely based on this perturbation we added to the image, rather
than on the background. You could also take this same procedure, and my colleague Andrej at
OpenAI showed how you can modify the image on ImageNet
using this same approach, and turn this goldfish into a daisy. Because ImageNet is
much higher dimensional, you don't need to use quite
as large of a coefficient on the image of the weights. So we can make a more
persuasive fooling attack. You can see that this
same image of the weights, when applied to any different input image, will actually reliably
cause a misclassification. What's going on is that there
are many different classes, and it means that if
you choose the weights for any particular class, it's very unlikely that a new test image will belong to that class. So on ImageNet, if we're using the weights for the daisy class, and there are 1,000 different classes, then we have about a 99.9% chance that a test image will not be a daisy. If we then go ahead and add the weights for the daisy class to that image, then we get a daisy,
and because that's not the correct class, it's
a misclassification. So there's a paper at CVPR this year called Universal Adversarial Perturbations that expands a lot more
on this observation that we had going back in 2014. But basically these weight vectors, when applied to many different images, can cause misclassification
in all of them. I've spent a lot of time telling you that these linear models
are just terrible, and at some point you've
probably been hoping I would give you some sort
of a control experiment to convince you that there's another model that's not terrible. So it turns out that some quadratic models actually perform really well. In particular a shallow RBF network is able to resist adversarial
perturbations very well. Earlier I showed you an animation where I took a nine and I turned it into a zero, one, two, and so on, without really changing
its appearance at all. And I was able to fool a linear softmax regression classifier. Here I've got an RBF network where it outputs a separate probability of each class being absent or present, and that probability is given
by e to the negative square of the difference between a template image and the input image. And if we actually follow the
gradient of this classifier, it does actually turn the image into a zero, a one, a two, a three, and so on, and we can actually
recognize those changes. The problem is, this
classifier does not get very good accuracy on the training set. It's a shallow model. It's basically just a template matcher. It is literally a template matcher. And if you try to make
it more sophisticated by making it deeper, it turns out that the gradient
of these RBF units is zero, or very near zero, throughout most of RN. So they're extremely difficult to train, even with batch normalization
and methods like that. I haven't managed to train
a deep RBF network yet. But I think if somebody comes
up with better hyperparameters or a new, more powerful
optimization algorithm, it might be possible to solve the adversarial example problem by training a deep RBF network where the model is so nonlinear
and has such wide flat areas that the adversary is not
able to push the cost uphill just by making small changes
to the model's input. One of the things that's the most alarming about adversarial examples is that they generalize
from one dataset to another and one model to another. Here I've trained two different models on two different training sets. The training sets are tiny in both cases. It's just MNIST three
versus seven classification, and this is really just for
the purpose of making a slide. If you train a logistic regression model on the digits shown in the left panel, you get the weights shown on
the left in the lower panel. If you train a logistic regression model on the digits shown in the upper right, you get the weights shown on
the right in the lower panel. So you've got two different training sets and we learn weight vectors that look very similar to each other. That's just because machine
learning algorithms generalize. You want them to learn a function that's somewhat independent of the
data that you train them on. It shouldn't matter which particular training examples you choose. If you want to generalize from the training set to the test set, you've also got to expect
that different training sets will give you more or
less the same result. And that means that
because they've learned more or less similar functions, they're vulnerable to
similar adversarial examples. An adversary can compute
an image that fools one and use it to fool the other. In fact we can actually
go ahead and measure the transfer rate between several different machine
learning techniques, not just different data sets. Nicolas Papernot and his collaborators have spent a lot of time exploring this transferability effect. And they found that for example, logistic regression makes
adversarial examples that transfer to decision
trees with 87.4% probability. Wherever you see dark
squares in this matrix, that shows that there's a
high amount of transfer. That means that it's very
possible for an attacker using the model on the left to create adversarial examples
for the model on the right. The procedure overall is that, suppose the attacker wants to fool a model that they don't actually have access to. They don't know the
architecture that's used to train the model. They may not even know which
algorithm is being used. They may not know
whether they're attacking a decision tree or a deep neural net. And they also don't know the parameters of the model that they're going to attack. So what they can do is
train their own model that they'll use to build the attack. There's two different ways
you can train your own model. One is you can label your own training set for the same task that you want to attack. Say that somebody is using
an ImageNet classifier, and for whatever reason you
don't have access to ImageNet, you can take your own
photos and label them, train your own object recognizer. It's going to share adversarial examples with an ImageNet model. The other thing you can do is, say that you can't afford to
gather your own training set. What you can do instead is if you can get limited access to the model where you just have the ability
to send inputs to the model and observe its outputs, then you can send those
inputs, observe the outputs, and use those as your training set. This'll work even if the output that you get from the target model is only the class label that it chooses. A lot of people read this and assume that you need to have access to all the probability values it outputs. But even just the class
labels are sufficient. So once you've used one
of these two methods, either gather your own training set or observing the outputs
of a target model, you can train your own model and then make adversarial
examples for your model. Those adversarial examples
are very likely to transfer and affect the target model. So you can then go and
send those out and fool it, even if you didn't have
access to it directly. We've also measured the transferability across different data sets, and for most models we find that they're kind of in an intermediate zone where different data sets will result in a transfer rate of, like, 60% to 80%. There's a few models like SVMs
that are very data dependent because SVMs end up focusing
on a very small subset of the training data to form
their final decision boundary. But most models that we care about are somewhere in the intermediate zone. Now that's just assuming that you rely on the transfer happening naturally. You make an adversarial example and you hope that it will
transfer to your target. What if you do something to
stack the deck in your favor and improve the odds that you'll get your adversarial examples to transfer? Dawn Song's group at UC
Berkeley studied this. They found that if they take
an ensemble of different models and they use gradient
descent to search for an adversarial example that will fool every member of their ensemble, then it's extremely likely
that it will transfer and fool a new machine learning model. So if you have an ensemble of five models, you can get it to the point where there's essentially a 100% chance that you'll fool a sixth model out of the set of models
that they compared. They looked at things like
ResNets of different depths, VGG, and GoogLeNet. So in the labels for each
of the different rows you can see that they
made ensembles that lacked each of these different models, and then they would test it on
the different target models. So like if you make an
ensemble that omits GoogLeNet, you have only about a
5% chance of GoogLeNet correctly classifying
the adversarial example you make for that ensemble. If you make an ensemble
that omits ResNet-152, in their experiments they found that there was a 0% chance of
ResNet-152 resisting that attack. That probably indicates
they should have run some more adversarial examples until they found a non-zero success rate, but it does show that the
attack is very powerful. And then when you go look into intentionally cause the transfer effect, you can really make it quite strong. A lot of people often
ask me if the human brain is vulnerable to adversarial examples. And for this lecture I can't
use copyrighted material, but there's some really
hilarious things on the Internet if you go looking for, like, the fake CAPTCHA with
images of Mark Hamill, you'll find something
that my perception system definitely can't handle. So here's another one
that's actually published with a license where I was
confident I'm allowed to use it. You can look at this image
of different circles here, and they appear to be intertwined spirals. But in fact they are concentric circles. The orientation of the
edges of the squares is interfering with the edge
detectors in your brain, making it look like the
circles are spiraling. So you can think of
these optical illusions as being adversarial
examples in the human brain. What's interesting is that
we don't seem to share many adversarial examples in common with machine learning models. Adversarial examples
transfer extremely reliably between different machine learning models, especially if you use that ensemble trick that was developed at UC Berkeley. But those adversarial
examples don't fool us. It tells us that we must be using a very different algorithm or model family than current convolutional networks. We don't really know what
the difference is yet, but it would be very
interesting to figure that out. It seems to suggest that
studying adversarial examples could tell us how to significantly improve our existing machine learning models. Even if you don't care
about having an adversary, we might figure out
something or other about how to make machine learning algorithms deal with ambiguity and unexpected inputs more like a human does. If we actually want to go out
and do attacks in practice, there's started to be a body
of research on this subject. Nicolas Papernot showed that he could use the transfer effect to fool classifiers hosted by MetaMind, Amazon, and Google. So these are all just
different machine learning APIs where you can upload a dataset and the API will train the model for you. And then you don't actually
know, in most cases, which model is trained for you. You don't have access to its
weights or anything like that. So Nicolas would train
his own copy of the model using the API, and then build a model on
his own personal desktop where he could fool the API hosted model. Later, Berkeley showed you
could fool Clarifai in this way. Yeah? - [Man] What did you mean when you said machine having adversarial
models don't generally fool us? Because I thought that
was part of the point that we generally do
machine-generated adversarial models where just a few pixels change. - Oh, so if we look at, for example, like this picture of the panda. To us it looks like a panda. To most machine learning
models it looks like a gibbon. And so this change isn't
interfering with our brains, but it fools reliably
with lots of different machine learning models. I saw somebody actually took
this image of the perturbation out of our paper, and they pasted it on their Facebook profile picture to see if it could interfere
with Facebook recognizing them. And they said that it did. I don't think that Facebook
has a gibbon tag though, so we don't know if they managed to make it think that they were a gibbon. And one of the other
things that you can do that's of fairly high
practical significance is you can actually
fool malware detectors. Catherine Gross at the
University of Saarland wrote a paper about this. And there's starting to be a few others. There's a model called MalGAN
that actually uses a GAN to generate adversarial
examples for malware detectors. Another thing that matters
a lot if you are interested in using these attacks in the real world and defending against
them in the real world is that a lot of the
time you don't actually have access to the
digital input to a model. If you're interested in
the perception system for a self-driving car or a robot, you probably don't get to
actually write to the buffer on the robot itself. You just get to show the robot objects that it can see through a camera lens. So my colleague Alexey
Kurakin and Samy Bengio and I wrote a paper where we studied
if we can actually fool an object recognition
system running on a phone, where it perceives the
world through a camera. Our methodology was
really straightforward. We just printed out several pictures of adversarial examples. And we found that the
object recognition system run by the camera was fooled by them. The system on the camera
is actually different from the model that we used to generate the adversarial examples. So we're showing not just transfer across the changes that happen
when you use the camera, we're also showing that
those transfer across the model that you use. So the attacker could conceivably fool a system that's deployed
in a physical agent, even if they don't have access
to the model on that agent and even if they can't interface
directly with the agent but just subtly modify objects that it can
see in its environment. Yeah? - [Man] Why does the, for the low quality camera image noise not affect the adversarial example? Because that's what one would expect. - Yeah, so I think a lot of that comes back to the maps
that I showed earlier. If you cross over the
boundary into the realm of adversarial examples, they occupy a pretty wide space and they're very densely packed in there. So if you jostle around a little bit, you're not going to recover
from the adversarial attack. If the camera noise, somehow or other, was aligned with the negative
gradient of the cost, then the camera could take a
gradient descent step downhill and rescue you from the uphill
step that the adversary took. But probably the camera's
taking more or less something that you could
model as a random direction. Like clearly when you use
the camera more than once it's going to do the same thing each time, but from the point of
view of how that direction relates to the image
classification problem, it's more or less a random
variable that you sample once. And it seems unlikely to align exactly with the normal to this class boundary. There's a lot of different
defenses that we'd like to build. And it's a little bit disappointing that I'm mostly here to
tell you about attacks. I'd like to tell you how to
make your systems more robust. But basically every attack we've tried has failed pretty badly. And in fact, even when
people have published that they successfully defended. Well, there's been several papers on arXiv over the last several months. Nicholas Carlini at Berkeley
just released a paper where he shows that 10 of
those defenses are broken. So this is a really, really hard problem. You can't just make it go away by using traditional regularization techniques. Particular, generative
models are not enough to solve the problem. A lot of people say, "Oh the
problem that's going on here "is you don't know anything
about the distribution "over the input pixels. "If you could just tell "whether the input is realistic or not "then you'd be able to resist it." It turns out that what's going on here is what matters more than getting
the right distributions over the inputs x, is getting the right
posterior distribution over the class of labels y given inputs x. So just using a generative model is not enough to solve the problem. I think a very carefully
designed generative model could possibly do it. Here I show two different modes
of a bimodal distribution, and we have two different
generative models that try to capture these modes. On the left we have a
mixture of two Gaussians. On the right we have a
mixture of two Laplacians. You can not really tell
the difference visually between the distribution
they impose over x, and the difference in the
likelihood they assign to the training data is negligible. But the posterior distribution
they assign over classes is extremely different. On the left we get a logistic
regression classifier that has very high confidence out in the tails of the distribution where there is never any training data. On the right, with the
Laplacian distribution, we level off to more or less 50-50. Yeah? [speaker drowned out] The issue is that it's a
nonstationary distribution. So if you train it to recognize one kind of adversarial example, then it will become
vulnerable to another kind that's designed to fool its detector. That's one of the category of
defenses that Nicholas broke in his latest paper that he put out. So here basically the choice of exactly the family of generative
model has a big effect in whether the posterior becomes
deterministic or uniform, as the model extrapolates. And if we could design a really
rich, deep generative model that can generate
realistic ImageNet images and also correctly calculate
its posterior distribution, then maybe something like
this approach could work. But at the moment it's
really difficult to get any of those probabilistic
calculations correct. And what usually happens is, somewhere or other we
make an approximation that causes the posterior distribution to extrapolate very linearly again. It's been a difficult
engineering challenge to build generative models that actually capture these
distributions accurately. The universal approximator
theorem tells us that whatever shape we would like our classification function to have, a neural net that's big enough ought to be able to represent it. It's an open question whether
we can train the neural net to have that function, but we know that we should be able to at least give the right shape. So so far we've been getting neural nets that give us these very
linear decision functions, and we'd like to get something that looks a little bit
more like a step function. So what if we actually just
train on adversarial examples? For every input x in the training set, we also say we want you to
train x plus an attack to map to the same class label as the original. It turns out that this sort of works. You can generally resist the same kind of attack that you train on. And an important consideration is making sure that you could
run your attack very quickly so that you can train on lots of examples. So here the green curve at the very top, the one that doesn't
really descend much at all, that's the test set error
on adversarial examples if you train on clean examples only. The cyan curve that descends
more or less diagonally through the middle of the plot, that's the tester on adversarial examples if you train on adversarial examples. You can see that it does
actually reduce significantly. It gets down to a little
bit less than 1% error. And the important thing to
keep in mind here is that this is fast gradient sign
method adversarial examples. It's much harder to resist iterative multi-step adversarial examples where you run an optimizer for a long time searching for a vulnerability. And another thing to keep in mind is that we're testing on the same kind of adversarial
examples that we train on. It's harder to generalize from one optimization
algorithm to another. By comparison, if you look at what happens on clean examples, the blue curve shows what happens on the clean test set error rate if you train only on clean examples. The red curve shows what happens if you train on both clean
and adversarial examples. We see that the red curve actually drops lower than the blue curve. So on this task, training
on adversarial examples actually helped us to do
the original task better. This is because in the original
task we were overfitting. Training on adversarial
examples is good regularizer. If you're overfitting it
can make you overfit less. If you're underfitting it'll
just make you underfit worse. Other kinds of models
besides deep neural nets don't benefit as much
from adversarial training. So when we started this
whole topic of study we thought that deep neural nets might be uniquely vulnerable
to adversarial examples. But it turns out that actually they're one of the few models that has a clear path to resisting them. Linear models are just
always going to be linear. They don't have much hope of
resisting adversarial examples. Deep neural nets can be
trained to be nonlinear, and so it seems like there's
a path to a solution for them. Even with adversarial training, we still find that we aren't able to make models where if
you optimize the input to belong to different classes, you get examples in those classes. Here I start with a CIFAR-10
truck and I turn it into each of the 10 different CIFAR-10 classes. Toward the middle of the plot you can see that the truck has started to look a little bit like a bird. But the bird class is the only one that we've come anywhere near hitting. So even with adversarial training, we're still very far from
solving this problem. When we do adversarial training, we rely on having labels
for all the examples. We have an image that's labeled as a bird. We make a perturbation that's designed to decrease the probability
of the bird class, and we train the model that the image should still be a bird. But what if you don't have labels? It turns out that you can
actually train without labels. You ask the model to predict
the label of the first image. So if you've trained for a little while and your model isn't perfect yet, it might say, oh, maybe this
is a bird, maybe it's a plane. There's some blue sky there, I'm not sure which of
these two classes it is. Then we make an adversarial perturbation that's intended to change the guess and we just try to make it
say, oh this is a truck, or something like that. It's not whatever you
believed it was before. You can then train it to say that the distribution of our classes should still be the same as it was before, but this should still be considered probably a bird or a plane. This technique is called
virtual adversarial training, and it was invented by Takeru Miyato. He was my Intern at Google
after he did this work. At Google we invited him to
come and apply his invention to text classification, because this ability to
learn from unlabeled examples makes it possible to do
semi-supervised learning where you learn from both
unlabeled and labeled examples. And there's quite a lot of
unlabeled text in the world. So we were able to bring
down the error rate on several different
text classification tasks by using this virtual
adversarial training. Finally, there's a lot of problems where we'd like to use neural nets to guide optimization procedures. If we want to make a very, very fast car, we could imagine a neural net that looks at the blueprints for a car and predicts how fast it will go. If we could then optimize with respect to the
input of the neural net and find the blueprint that it predicts would go the fastest, we could build an incredibly fast car. Unfortunately, what we get right now is not a blueprint for a fast car. We get an adversarial
example that the model thinks is going to be very fast. If we're able to solve the
adversarial example problem, we'll be able to solve this model-based optimization problem. I like to call model-based optimization the universal engineering machine. If we're able to do
model-based optimization, we'll be able to write down
a function that describes a thing that doesn't exist
yet but we wish that we had. And then gradient descent and neural nets will figure out how to build it for us. We can use that to design
new genes and new molecules for medicinal drugs, and new circuits to make GPUs run faster
and things like that. So I think overall, solving this problem could unlock a lot of potential
technological advances. In conclusion, attacking
machine learning models is extremely easy, and defending them is extremely difficult. If you use adversarial training you can get a little bit of a defense, but there's still many caveats associated with that defense. Adversarial training and
virtual adversarial training also make it possible
to regularize your model and even learn from unlabeled data so you can do better on
regular test examples, even if you're not concerned
about facing an adversary. And finally, if we're able to
solve all of these problems, we'll be able to build a black
box model-based optimization system that can solve all
kinds of engineering problems that are holding us back
in many different fields. I think I have a few
minutes left for questions. [audience applauds] [speaker drowned out] Yeah. Oh, so, there's some determinism to the choice of those 50 directions. Oh right, yeah. So repeating the questions. I've said that the same perturbation can fool many different models or the same perturbation can be applied to many different clean examples. I've also said that the subspace of adversarial perturbations
is only about 50 dimensional, even if the input dimension
is 3,000 dimensional. So how is it that these
subspaces intersect? The reason is that the choice
of the subspace directions is not completely random. It's generally going to be something like pointing from one class centroid
to another class centroid. And if you look at that vector
and visualize it as an image, it might not be meaningful to a human just because humans aren't very good at imagining what class
centroids look like. And we're really bad at imagining differences between centroids. But there is more or less
this systematic effect that causes different models to learn similar linear functions, just because they're trying
to solve the same task. [speaker drowned out] Yeah, so the question is,
is it possible to identify which layer contributes
the most to this issue? One thing is that if you, the last layer is somewhat important. Because, say that you
made a feature extractor that's completely robust to
adversarial perturbations and can shrink them to
be very, very small, and then the last layer is still linear. Then it has all the problems
that are typically associated with linear models. And generally you can
do adversarial training where you perturb all
the different layers, all the hidden layers
as well as the input. In this lecture I only
described perturbing the input because it seems like that's where most of the benefit comes from. The one thing that you can't
do with adversarial training is perturb the very last
layer before the softmax, because that linear layer at the end has no way of learning to
resist the perturbations. Doing adversarial training at that layer usually just breaks the whole process. But other than that, it
seems very problem dependent. There's a paper by Sara
Sabour and her collaborators called Adversarial Manipulation
of Deep Representations, where they design adversarial examples that are intended to fool
different layers of the net. They report some things about, like, how large of a perturbation
is needed at the input to get different sizes of perturbation at different hidden layers. I suspect that if you trained the model to resist perturbations at one layer, then another layer would
become more vulnerable and it would be like a moving target. [speaker drowned out] Yes, so the question is, how many adversarial examples are needed to improve the misclassification rate? Some of our plots we
include learning curves. Or some of our papers we
include learning curves, so you can actually see, like in this one here. Every time we do an epoch
we've generated the same number of adversarial examples as there are training examples. So every epoch here is
50,000 adversarial examples. You can see that adversarial
training is a very data hungry process. You need to make new adversarial examples every time you update the weights. And they're constantly
changing in reaction to whatever the model has
learned most recently. [speaker drowned out] Oh, the model-based optimization, yeah. Yeah, so the question is just to elaborate further on this problem. So most of the time that we
have a machine learning model, it's something like a
classifier or a regression model where we give it an
input from the test set and it gives us an output. And usually that input
is randomly occurring and comes from the same
distribution as the training set. We usually just run the
model, get its prediction, and then we're done with it. Sometimes we have feedback loops, like for recommender systems. If you work at Netflix and you recommend a movie to a viewer,
then they're more likely to watch that movie and then rate it, and then there's going
to be more ratings of it in your training set so you'll recommend it to
more people in the future. So there's this feedback loop from the output of your
model to the input. Most of the time when we
build machine vision systems, there's no feedback loop from
their output to their input. If we imagine a setting where we start using an
optimization algorithm to find inputs that maximize
some property of the output, like if we have a model that looks at the blueprints of a car and outputs the expected speed of the car, then we could use gradient ascent to look for the blueprints that correspond to the fastest possible car. Or for example if we're
designing a medicine, we could look for the molecular structure that we think is most likely
to cure some form of cancer, or the least likely to cause some kind of liver toxicity effect. The problem is that once
we start using optimization to look for these inputs that maximize the output of the model, the input is no longer
an independent sample from the same distribution as we used at the training set time. The model is now guiding the process that generates the data. So we end up finding essentially
adversarial examples. Instead of the model telling us how we can improve the input, what we usually find in practice is that we've got an
input that fools the model into thinking that the input
corresponds to something great. So we'd find molecules that are very toxic but the model thinks
they're very non-toxic. Or we'd find cars that are very slow but the model thinks are very fast. [speaker drowned out] Yeah, so the question is, here the frog class is boosted by going in either the positive or
negative adversarial direction. And in some of the other
slides, like these maps, you don't get that effect
where subtracting epsilon off eventually boosts the adversarial class. Part of what's going on is I think I'm using larger epsilon here. And so you might
eventually see that effect if I'd made these maps wider. I made the maps narrower because it's like quadratic time to build a 2D map and it's linear time to
build a 1D cross section. So I just didn't afford the GPU time to make the maps quite as wide. I also think that this might just be a weird effect that happened
randomly on this one example. It's not something that I
remember being used to seeing a lot of the time. Most things that I observe don't happen perfectly consistently. But if they happen, like, 80% of the time then I'll put them in my slide. A lot of what we're doing is
trying trying to figure out more or less what's going on, and so if we find that something
happens 80% of the time, then I consider it to be
the dominant phenomenon that we're trying to explain. And after we've got a
better explanation for that then I might start to try to explain some of the weirder things that happen, like the frog happening
with negative epsilon. [speaker drowned out] I didn't fully understand the question. It's about the dimensionality
of the adversarial? Oh, okay. So the question is, how is the dimension of the adversarial subspace related to the dimension of the input? And my answer is somewhat embarrassing, which is that we've only run
this method on two datasets, so we actually don't have a good idea yet. But I think it's something
interesting to study. If I remember correctly, my
coauthors open sourced our code. So you could probably run it on ImageNet without too much trouble. My contribution to that paper was in the week that I was unemployed between working at OpenAI
and working at Google, so I had access to no GPUS and I ran that experiment
on my laptop on CPU, so it's only really small
datasets. [chuckles] [speaker drowned out] Oh, so the question is,
do we end up perturbing clean examples to low
confidence adversarial examples? Yeah, in practice we usually find that we can get very high confidence
on the output examples. One thing in high dimensions
that's a little bit unintuitive is that just getting the sign right on very many of the input pixels is enough to get a really strong response. So the angle between the weight vector matters a lot more than
the exact coordinates in high dimensional systems. Does that make enough sense? Yeah, okay. - [Man] So we're actually
going to [mumbles]. So if you guys need to leave, that's fine. But let's thank our speaker one more time for getting--
[audience applauds] 

