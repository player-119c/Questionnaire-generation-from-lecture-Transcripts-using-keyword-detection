Okay it's 9:35 let's go ahead and get started. 
Welcome to 6.036/6.862, Introduction to Machine   Learning. So as you are experiencing right 
now, our lecture will start at 9:35 a.m   every Tuesday, so in particular we're always 
going to start at MIT time, so that's five   minutes past and we'll end five minutes before 
11, so that should give you time to get between   classes and things like that. The lecture is 
synchronous. This is totally live right now,   so as you'll see in a moment if you have questions 
that can be a part of our discussion today.   Something to keep in mind is that everything we're 
doing is in Boston's time zone and so the reason I   say that is because at some point in November, 
U.S. daylight savings is going to change and   so it's not exactly EDT or EST, you just want to 
look up what is Boston time zone just, you know,   you can google it and that'll be the appropriate 
time zone. So a note about class numbers:   so 6.036 is the class that we're in here. There's 
a graduate version, 6.862, that some of you are   in. 6.862 acceptance has been finalized 
at this point and so you should have heard   already if you're in 6.862 and you'll want 
to register appropriately based on that. Now   that being said if you were accepted to 6.862 but 
you want to take 6.036 that's still totally fine,   you can just switch into 6.036. If you're not 
signed up for 6.036, that's also totally fine   you can just get into 6.036 fresh. So 
6.036 is still open for signing up.   Now our main course website is listed here. 
If you ever have any doubt, if you're looking   for some information, just go to the course 
website. Everything is there with one caveat   and that caveat being if you are in 6.862 
then there's a separate Canvas website that   has information there and so you'll also hopefully 
already have access to that and be accessing that.   We're going to be taking questions at Discourse, 
so if you haven't already gone to this link,   there's also a nice link that our awesome 
TA Crystal has been posting in the chat.   Then I encourage you to go to it. At the very 
least I encourage you to check it out after   the lecture is over and go to that. That's 
where we're going to be fielding questions.   You can also find this at the forum link on 
the course website, so again you can really   get to everything from the course website. 
You can ask about logistics at Discourse,   you can ask about actual content at the Discourse 
website, anything related to the course. They will be reading the Discourse, they'll 
be responding to the questions on Discourse   and in particular if something comes up that you 
know everybody could benefit from or there's an   interesting question that we all want to discuss, 
they'll bring it back to me and so you can   start asking right now. The reason we're having 
questions at Discourse rather than in Zoom chat   is particularly because we want to hear a lot of 
questions from you and we're hoping that by having   it on Discourse, we can handle a higher volume, 
you know not just things that I can possibly read   in the Zoom chat and the time that I have, and so 
I really encourage you to go there. The one thing   that I'll mention is that if you're interested in 
a lecture question because you're participating in   lecture, then we encourage you to use the lecture 
one category. So when you go over to Discourse,   check out, there's this great example 
question that sort of tells you how to,   you know, set things up so lecture one category is 
obviously for today. Today is lecture one and then   there will be a similar category in the future 
for different lectures that we have in the future.   Okay, so I do want to mention that all of our 
materials are going to be available at the course   website, so the slides for this lecture are going 
to be available at the course website. If I write   anything on my iPad I'm going to try to save 
it and make it available at the course website.   In fact, there will be a recording of this lecture 
available at the course website and so if you're   not able to view it live for any reason or if 
you want to look over it again in the future that   should be there as well and so everything should 
be available there and you should be able to check   that out. We will not be monitoring the Zoom 
chat for questions, so again I encourage those   to go over to Discourse. Okay so today's plan is: 
we've already covered some logistics but I'm going   to be covering some more logistics. There's a lot 
of logistics in the beginning and then we're going   to get to the good stuff, we're going to start 
talking about machine learning, we're going to   set that up, and then we're going to dive into 
some details with linear classifiers and that'll   really just be the tip of the iceberg for the rest 
of the semester, so that's where we're going here.   Okay, so I said we're going to do 
some logistics. Let's start by talking   about prerequisites. So this course has a number 
of prerequisites. I'm just going to briefly   go over them here. We have computer science 
prerequisites, so in particular we want you   to be familiar with Python programming. 
We're going to be using numpy a lot.   You want to be familiar with algorithms, you 
should be able to read and understand pseudocode.   We're going to be seeing a lot of pseudocode and 
talking through it together. We have a number of   math prerequisites we're going to see that that's 
just really integral to machine learning. So, you   should be familiar with matrix manipulation things 
like the inverse and transpose and multiplication,   sort of the standard things we do with matrices. 
We're going to be talking about points and planes   and dimension greater than two even starting today 
and so that should definitely be something that   you're able to you know be familiar with and deal 
with gradients. They'll be gradients all the time   so you'll definitely want to be familiar 
with those and be able to take gradients   and basic discrete probability. We'll talk about 
randomness and random variables, independence,   conditioning and things like that. Now the most 
important thing here is that you're really going   to want to do this readiness assessment, so what 
what I'm looking at here is a drop down menu,   so if you go to that website that I talked about, 
our main course website on the previous slide,   you'll find a welcome to 6.036 thing that you can 
click on. You'll see this drop down menu and then   you're going to want to go to this readiness 
assessment and just check that in fact you know   you you feel comfortable with the sort of material 
that's ready for this course and so if you haven't   already done that we strongly encourage you to go 
and do that readiness assessment. Okay, so once   you've decided whether 6.036 or 6.862 is right 
for you, if in fact it is, then let's keep going.   So we have an amazing, amazing course staff for 
this. So I am just one of a number of instructors   that you'll be meeting, so I'm gonna be doing 
the lectures mainly, but we're gonna see all   these other instructors. Some of them are 
here right now, in particular, Duane and Ike   and then you're going to be seeing all of these 
instructors in your labs and your office hours   in talking about, you know, questions and things 
like that so you're going to have a lot of really   exciting opportunities to talk with everybody 
and the same goes for our amazing set of teaching   assistants which I think this list may even grow 
in the near future so do not consider it to be   a complete list for the rest of the course, but 
they are fantastic. They've been doing all this   amazing work already setting up the infrastructure 
for what we're doing, getting everything ready   for you to be a part of this class. I believe 
we have right now with us Crystal and Satvat,   but again you're gonna be meeting all these other 
fantastic TAs in your labs and office hours and   then these other activities you're gonna have a 
lot of hands-on time with them and also with the   lab assistants. I do not have a set of pictures 
for the lab assistants because there are so many   of them, but they're also fantastic and you'll 
be interacting with them a lot so I just want to   say, you know, there's a fantastic staff here and 
I think we're going to have a great time together. Okay so let's talk a little bit about our weekly 
plan. We sort of have a weekly calendar in some   sense that is going to go on in this course. 
Now part of that is going to require getting   some information from you about your schedule 
and so a really important thing that you want   to do is if you have not already, complete 
or update your schedule survey by noon today,   so that's tuesday, the day that 
we're having our first lecture.   Make sure to fill in your information 
about your schedule so that we can use   it to plan where you are in the week and 
I'll say where that comes up in a moment.   So first, of course, we have the lecture every 
tuesday. We're going to have the lecture and   in addition to the lecture, which again is both 
live and recorded, we're going to have a number   of course notes that are available at the website 
and so this is sort of this base of information on   which you're going to build to then really engage 
with all of this material and so basically we're   going to have a number of other components to the 
week that are all about using this material and   applying it and I think that's, you know, where 
we believe that real learning is going to happen,   this combination of taking in this information 
but then actually applying it and so where   is that going to happen, well a few different 
places: so the first component is, in general,   there are going to be a set of exercises due 9 
a.m before lecture, so those were not due today,   there was nothing due today, so don't worry about 
that. The first set will be due next week before   the lecture next week and the idea here is to just 
prepare just a little bit so you get the most out   of lecture, so making sure that you've done the 
reading, that you're ready to go in the lecture.   We're going to have a lab, now this is a really 
cool and fun component of this course and I hope   that you'll enjoy it as much as I have in previous 
semesters. It is synchronous because we're all   going to be chatting together. This is going 
to be very, you know, highly interactive and   involved and so you really need to show up to your 
assigned lab time at the time that it's happening   and so in particular this is why we need to 
do this schedule survey because we need to   know when you're available for the lab. The 
first one is this week, it's going to be very   chill. We're mostly just going to be making 
sure everything works and so don't feel like,   you know, you need to be on top of absolutely 
everything in terms of that lab. We're really   just going to be sort of checking things out. 
After everyone fills out the scheduling surveys,   the staff are going to make the assignments to 
labs, so by this afternoon we expect to have   a first round of assignments so you'll 
be able to see those later. There'll be   instructions about that, as well as how to 
self swap. So especially in the beginning   we'll be letting people swap times and 
that will be subject to space availability.   Okay so what exactly is happening here? So what's 
going to happen is we're going to break everybody   down into so-called MLyPod, so that's how you 
pronounce this ML-y pod thing: it's a MLyPod.   It's 10 students, you're going to basically be, 
you know, subject to maybe some changing in the   beginning with the same 10 students throughout 
the semester, so you'll be seeing some familiar   faces as well as TAs and LAs and you're going to 
work each time in a group of sort of two to three   and work through a bunch of really cool problems 
that are interesting to talk about and then at   the end you're gonna check off with staff and 
during you can also ask questions of staff and   engage with staff or we're gonna be there and I 
think this is just a really fun thing to do that   you get to have a discussion about what's going 
on, you get to ask questions, you get to really   sort of engage with the material and so hopefully 
you'll find that that is a fun thing as well.   Okay so now there's the classic weekly homework, 
so that's another part of our weekly setup.   Now here, what you're going to do to find 
this sort of thing is you're going to go   to the same home page as before but now you're 
going to see a link that says week one basics.   Basically it's the link for week one stuff, you 
can scroll down and or once you activate it,   you'll see a bunch of options. One of these is the 
homework, it's due on Wednesday and you can just   sort of see everything that's going on in week one 
and you'll see there's no exercises here because   there are no exercises for week one, and that'll 
change in week two. We'll have that done. So the   first one again is due September 9th, as you can 
see that'll be next Wednesday. There is a nano   quiz each week, so we are not having a midterm in 
this class, and we are not having a final. There's   no really big exam. Instead, we're going to have 
this sort of smaller exam every week. It's timed,   the first one is this week, but it's just going to 
be in lab and there's basically no content to it,   it's just checking the mechanics, trying it out, 
so don't worry about that. It's also ungraded,   but we're going to go through that and make sure 
that you're comfortable with the nano quiz format   because we're going to start doing it for real 
next week and starting next week you're going to   have 24 hours to complete this before your lab 
section. As soon as you start it, it is time.   Okay, so other components of your week that you 
may choose to participate in are office hours.   So we have tons of office hours, almost every day 
has office hours options, they're at all kinds of   different times and I believe that they're going 
to start this Sunday as the first office hours,   and then finally if you're in 6.862, you'll also 
have various project aspects that you're involved   with. You have a project that you're completing, 
and again, the information for that is the one   thing that is not at this first course webpage 
that I’ve pointed out for 6.036, it's in Canvas,   so you'll want to check that out. Okay so that's 
all the logistics. I'm just going to take a second   and see if anything came up in the Discourse 
that we absolutely have to cover right now,   but I think a lot of this is just going to be, you 
know, you're going to be getting familiar with all   of this during the semester. It'll become, you 
know, much more smooth and clear as we go on and   a lot of this week is just making sure 
that you are familiar with these logistics,   that you're you're sort of ready to 
go and getting used to the setup. Okay so with that, let's get into the 
good stuff, which is machine learning.   So I think, you know, even though you're here 
for a machine learning course, it's worth asking,   you know, why are we talking about machine 
learning? Why do we have a whole course   on this, and in fact many courses? I think 
a really short answer which we're gonna   expand upon quite a lot is it's everywhere, 
so let's let's see some examples to dig a   little deeper and see if we can come up with 
a better answer to why are we talking about   machine learning and what is 
machine learning for that matter,   and so in order to do that I actually went on 
Google News the other night and just figured out,   you know, what were some recent articles about 
machine learning and unsurprisingly there were a   ton just in the past week or two and so let's just 
talk about a few of these. So here's one: machine   learning algorithm confirms 50 new exoplanets in 
historic first. So exoplanets, in case you're not   familiar, are these planets that are outside our 
solar system. Scientists are really interested   in discovering them, they'll be super interested 
to learn about new ones. So here scientists have   data which are roughly something like images from 
telescopes. They see certain types of signal that   tell them: “hey maybe there's an exoplanet here,” 
and then they use machine learning to decide   which of these candidate exoplanets are really 
exoplanets and which ones are something else. So here's another news article that I saw, or 
another article that came up in Google News. It's   from “The Lancet”: a machine learning algorithm 
for neonatal seizure recognition, a randomized   controlled trial, and so here, they're looking 
at newborns and they want to be able to detect   seizures in them so that scientists or medical 
practitioners can provide appropriate medical   care, and so the data that they have is EEG data 
that provides some monitoring of the brain here,   but it's super labor intensive to interpret it 
very quickly and so what they do is they want   something that's going to be much more automated 
to be able to tell them, well if a newborn makes   some kind of movement, is that a seizure 
and something we're really concerned about,   or is it just something normal and something we 
don't have to worry about? So they want to take   each candidate movement and decide, you know, 
is this newborn experiencing a seizure or not?   So here's another one, this one's 
from actually a little bit longer ago,   but it's this analysis by Reuters of the 
Supreme Court in the U.S., in the United States.   So as part of this analysis, they did the sort 
of in-depth news analysis and they saw all these   petitions that go before the Supreme Court, sort 
of decide the Supreme Court takes these decisions   and petitions and decides are they going to 
hear the case that's associated with them,   and as one part of the analysis, these news 
people took thousands of petitions and they   looked at the text of those petitions and decided 
what are the topics or the themes that are in   those petitions? They want to know sort of what 
are they about, but in an automated way, because   it's hard to read so many petitions, that's 
really sort of an involved kind of thing to do. Another aspect of machine learning and sort of 
government is in the Bureau of Labor Statistics.   So this one is also in the United States, so here 
the Bureau of Labor Statistics has text data on   interviews and surveys on various occupations 
and events that happen, and they want to turn   those into codes: they want to know “oh, was this 
about a particular occupation, like say a janitor?   Was it about a particular event like a workplace 
injury?” and that'll help them figure out what's   going on in workplaces and, you know, plan things 
appropriately, but it turns out people are super   expensive to train to do this task and actually 
people aren't that great at it. They often don't   agree and so they'd like to have a better way to 
do it, to use machine learning to automate that.   Phishing and spam detection are a super classic 
example of machine learning and yet this   article you can see is just from the past few 
days. So phishing is when somebody tries to get   your password or your sensitive information by 
pretending to be somebody else often in an email   and so if you're an email client like gmail 
you really want to detect this kind of thing   before it even hits somebody's inbox so they 
can't possibly be duped by it. And so the data   that they have is the text of the email and they 
want to decide: “Is that email phishing or not?”   Now machine learning can sometimes 
be controversial: there's some really   controversial examples that have come up recently, 
so for instance, this is an article talking about   the controversy around facial recognition. 
So in facial recognition, the data that's   available is often surveillance footage of 
images of people, maybe security cameras,   and a decision might be: “Who is this? Who is 
this person?” and perhaps, you know, that person   might then be arrested if they were caught doing 
some kind of crime, and so one aspect that might   make this controversial is what if you catch the 
wrong person? What if you misidentify who that   person is? What if you say it's somebody else 
and so that could be, you know, an interesting   and important part of machine learning in society 
and we want to know what's going on there.   Machine learning is also used in things like 
finance, so here somebody is deciding how to   distribute loans in India. So the data that 
they have is they're interested in: “Should   I give a loan to some farmer?” And they have 
data about satellite images of their farm, the   weather information, and other data and so they 
can figure out: “Is this farm gonna make enough   money to repay the loan and should they give this 
farm the loan?” Okay so that's a bunch of examples   of machine learning in the news, often from just 
the past week or two, and so now let's try to,   you know, answer these questions that that I posed 
at the top of the slide: what is machine learning?   Well if we look at all of these examples, 
we see that they're all cases where somebody   has a bunch of data and they're using 
some kind of method to make a decision,   so we saw, you know, various types of data like 
maybe I have data about newborns and I'm trying to   make a decision about are they having a seizure 
and whether I'm going to give them a certain   type of medical care or not. Okay so why are we 
studying machine learning? You know, I think that   in a lot of cases, a really natural answer is that 
we want to apply it, you know, it seems like it's   a very powerful set of tools. We've seen already 
in these examples that it often has the potential   to save time and energy and resources, and so that 
can be really helpful in a lot of cases. We want   to be able to apply it to new areas and get the 
benefits of that automation in new areas but I   think it's worth highlighting at least two other 
reasons that we're studying machine learning: so   one is to understand, you know, people are already 
using machine learning in many different areas   or they have big plans to use machine learning 
and we can see here that that has the potential   to impact your medical care, your finance, 
your security, your experience of government   and so these are important decisions that affect 
you on a day-to-day basis and to understand them,   to really understand what's going on, you have to 
understand how machine learning works and that's   also sort of related to this issue of evaluation. 
So lots of people are using machine learning,   but also lots of people are claiming to use 
machine learning these days and so you want   to ask when somebody says “hey, I've got this new 
machine learning method,” does that really work?   And maybe even more to the point, does it work 
as intended? What are the effects of that method?   What is it doing exactly? Even if it's something 
that works well, does it need improvement?   Are there ways that we can make it even better? 
And so these are the sorts of things that we're   hoping that, you know, throughout this course. 
Obviously this is not a full answer to “what is   machine learning?” and why studying machine 
learning, and I think we're really going to   be going to be spending the rest of this course 
trying to answer these questions in some sense.   Okay, so before we leave this sort of overview 
slide, let me just make a couple more points that   I think are important. So one is machine learning 
is a tool: it's not magic, it's not going to,   you know, just answer any question that you 
possibly have. Just like any other tool it has   times it's useful and times it's not useful and 
actually there are some famous examples of it,   of this not being useful sometimes. So there was 
this interesting study that was done very recently   where a collaboration of international researchers 
had this data, this extremely rich sociology data,   and sort of the life course of various children 
and they wanted to predict what was going to   happen in the future with these children and even 
though they said anybody could participate and   they had tons of fantastic groups using, you know, 
all the best modern machine learning methods,   they just couldn't predict very well and I think 
one of the things that we're going to talk about   in the course of this is when does machine 
learning work well, when can you expect it to work   well, and when might it not work well, and how can 
you detect that as well. I'm also going to bring   up here that machine learning is built on math, 
fundamentally math is its real foundation, and so   that's why we have all these math prerequisites 
because we're definitely going to be using math   and I think the farther you go in machine 
learning, the more you're going to see that you do   need math and you do build on some really 
interesting mathematics that goes on there.   Okay so in some sense this slide 
is about motivating us to to care   not just about machine learning but about 
understanding the deep inner workings of machine   learning, but it's all pretty high level and 
vague at some sense and so let's dive deeper into   an example and into making a lot of 
this formal to get a better handle on   exactly what's going on and, you know, start 
really starting to answer these questions. Okay so let's go ahead and get started.   So let's think about, you know, essentially what 
do we have at our disposal and what questions   are we trying to answer? So what do we have at 
our disposal? Well essentially we have data,   so this is what we saw in all those examples 
before and let's maybe make a cartoon version of   one of those examples. Let's focus on this idea of 
we have data on a bunch of newborns, for instance,   and we're interested in trying to say “hey if a 
new newborn comes into my hospital I want to know   if it's going to have a seizure” and so from 
that perspective what we have right now could   be seen as training data, because we're going 
to train a method, a machine learning method,   that we can then later use to predict, you know, 
to decide what's going to happen with future   newborns and so the only thing we have access 
to right now is the data we're going to use to   train and so we're going to say that the number of 
data points that we have let's call that little n.   So for instance you know maybe we have observed 
little n newborns in our hospital in the past.   Now what exactly makes a data point? So in this 
particular example with the newborns we have,   well first of all let's say that i is the index 
of our data point so we can talk about data point   one, data point two all the way up to data point 
n. So if we look at data point i it's going to   have associated with a feature vector, so this is 
sort of everything we measure about. For instance,   this newborn in our example, let's call that 
feature vector x^(i). So that superscript denotes   which data point it is and then x here itself is a 
vector and so we can denote the different elements   of the vector by x_1 through x_d. So d is the 
dimension of the vector and we're saying this all   lives in the Euclidean space of length d, so I'm 
going to draw a cartoon of this now. A fundamental   limitation that we should always be aware of is 
that we as humans can only see in two dimensions   and so the reality is that when you're doing 
machine learning d is probably going to be bigger   than 2. So for instance in the actual newborn 
seizure study that we just looked at, d was 55.   Now here though, I'm going to draw my cartoon 
in two dimensions because that's all we can see,   so what's data going to look like 
here? Well a particular feature vector   is going to have an x_1 value and an x_2 value 
so it's going to be a point in two dimensions   and so we'll just get a few of these points 
for our data and what's going to happen here   is that we're going to consider labels for this 
data, so basically one way to think about this is   that in the particular case at the newborns, some 
expert has come in and told us: was this newborn   experiencing a seizure or was it not? And so 
for instance, in this example let's imagine   that x_1 could be how much oxygen the newborn 
is breathing. This is totally a cartoon. If you   actually care about the medical example, go read 
that paper, but for my cartoon today let's say   x_1 is how much oxygen the newborn is breathing, 
x_2 is how much the newborn is moving, and then -1   is that this newborn did not have a seizure and 
1 is that they did have a seizure. Now something   that's worth keeping in mind here is that getting 
the x's is super non-trivial, so if you actually   look at what happened for instance in this paper, 
they have this very complex EEG data and then   they find this 55-dimensional representation of 
features that expresses important points about   that and so for the moment, we're just going 
to assume we have these features but it's worth   keeping in mind that how you turn a newborn into a 
feature is a really involved process and something   that you know you should be aware of. Okay so 
what's this going to look like? So actually we   had this expert come in and we got all of these 
labels on our data, so here x^(1) and x^(2)   represent newborns who did not have a seizure, 
x^(3) represents one that did have a seizure,   and in fact we collect a lot of data, 
you know, on these various newborns,   and so once you have all of this data I think 
you can look at this and you have this intuition   that, you know, if a new newborn came 
by and you saw this you could probably   predict whether they were going to have a seizure 
or not. You could probably predict whether it was   going to get a minus value or a plus value based 
on the x's that we're seeing and that's basically   what we're going to be doing for the rest of our 
time in some sense is formalizing that intuition.   Okay, so this is all called our training data and 
we're just going to give it a name, we're going to   call it D_n so D_n is going to represent all of 
the training data, so each data point is a pair.   It's got the feature vector and the label, it's 
the x and the y, and then we collect all of these   pairs into our set of training data D_n and now 
we want to ask, “okay well we have all this data   but what are we doing with it, you know, what's 
the point what are we trying to accomplish?” Okay, well somehow at least in this example what 
we really want to do is we want to say “hey,   we saw all of these newborns before. 
An expert labeled all of them,   but we want to know if a new newborn came in, 
are they going to have a seizure or not”, because   if we knew that then we could provide better 
medical care, we can make sure that, you know,   somebody's really monitoring them and careful and 
we could have, you know, sort of good actions that   we could take and so we want a good way to be 
able to label new points and somehow there are   two things that are implicit in this statement 
that we're going to try to make concrete: one is   how do we label new points? What is a way to label 
new points, and separately what makes it good   you know? We have intuitions, I think, 
about both of these things at this point,   but we're not precise yet and so we're going to 
make both of these ideas precise going forward   and we're going to start by focusing on a 
way to label new points. How can we label   new points and then we'll get to the question 
of what is a good way to label new points.   Okay, so let's start by saying how to label new 
points. So okay here's a new point. So look at   this: I've got this little “x”, this little black 
“x” that's out in my space of points in x_1, x_2   and I want to say “hey, here's a new point. How 
do I label it?” and I want to be able to do this   for any point that comes along, any possible 
value of these covariates, these features   x, this feature vector x, and so if you think 
about it, what I'm essentially describing   is a function. What this function, let's call 
it h, should do is it should take in any value   of this Euclidean space, any value, potential 
value, of the feature vectors, and it should   return a label, in this case -1 or 1 and so 
we're going to call this function a hypothesis.   Now, just another way to think about this is that, 
again, this is a function that takes values of x,   it goes through our function h, and 
it returns a value y, a value label.   Okay, so this x could be anywhere, 
so here I'm just moving around   this little x, you know up here in my data, 
and I just want that for any possible x. I   could have that this is going to give 
me some label y. Okay, so here's an h:   so I have an h, I'm just going to define an h that 
says for any x, h(x) = 1. Okay, so now we're going   to try something out: so I told you that all your 
questions are going to be on Discourse but every   now and then, I'm going to ask you, the audience, 
a question and I'm going to see if you can respond   to that question and I'm going to see if you 
can respond to me on Zoom. So go to Zoom,   find the private chat, and find the ability to 
write to Tamara Broderick. Don't write to Tamara's   iPad, don't write to anybody else, just to 
Tamara Broderick, and here's my question for you:   is this a hypothesis I just wrote down, an 
h for any x, h(x) = 1, is this a hypothesis? Okay awesome, everybody is responding totally 
correctly. Absolutely, it's a hypothesis. Now   here's a follow-up question: so we asked is this 
a hypothesis, now here's my second question.   So yes it is a hypothesis because it's 
a function. Any function that goes from   R^d to -1 and 1 is going to be a hypothesis. Now 
here's my new question: is this a good hypothesis?   Okay awesome, everybody's totally nailing 
it. No this is not a good hypothesis   because this hypothesis is telling me to just 
say that every newborn had a seizure and that's   just not useful. You know we have this intuition 
that a good hypothesis should be able to discern   between the newborns who had seizures and 
the ones who didn't and so now we're going   to start thinking about, you know, clearly 
we need some better hypotheses to choose from   than this one that I just talked about here. Great 
yeah it's as somebody said it's terrible, I agree.   Okay awesome. Okay so now what we're gonna do is 
we're going to start by talking about a richer   set of hypotheses and then we're going to 
talk about what makes a good hypothesis. We,   right now, we're still talking about intuition 
about what makes something good, which is great.   You all clearly have fantastic intuition but we'll 
make that precise, but first let's come up with   a richer set of hypotheses: the so-called linear 
classifiers. Okay so first, let's define something   called a hypothesis class, it's just a collection 
of hypotheses so let's call that “script H”. So here's an example hypothesis class, it's 
the class of all hypotheses that label 1 on   one side of a line and -1 on the other side 
of a line. So let's see some examples of this.   So here's a line. That's not a hypothesis 
though, right, because a hypothesis is a function   on all of the x's and so I need to say 
that for every possible x, I could get   every value of x_1 and x_2. What is my hypothesis 
going to tell me, so it's got to be a function   over all the x's but with a line I can now specify 
such a function. So here's one such function.   I'm going to predict plus on the upper side of 
this line if I go sort of up and to the right,   and I'm going to predict minus on the 
other side so that's one hypothesis   in this class. Here's a different hypothesis 
that's based on the same line, let's take the   same line but now let's predict plus on this side 
of the line and minus on this side of the line   so that's a different hypothesis in this class. 
Here's yet another hypothesis in this class:   here's a line and I'm going to 
predict plus on this side of the line   and minus on this side line. Now I think you'll 
probably already have some intuition that   there's three hypotheses we just named, 
some of them are better than others.   Now what we're going to do on the rest of this 
slide is we're going to make this idea of having   a collection of lines with a label of plus one on 
one side and minus one on the other side concrete   and mathematically precise and so we're 
going to go into some math for this.   If it doesn't all immediately click, don't worry 
about it because you're going to be spending a   lot of time and problems, you know, engaging with 
this idea of a line and this linear classifier and   if all you get from this slide is that we're going 
to come up with a set of hypotheses that look like   this that's fine, you can always go back to 
the math later because that's literally all   we're doing for the rest of the slide is taking 
what you already see here and making it precise.   Okay so let's do that, so it's time for some math 
facts. Okay get ready for the math facts. So here,   we're taking the exact same space 
as before. I'm drawing a cartoon   of x_1 and x_2, but you should really 
think of this as the whole R^d space. So this could be much more than two dimensions, 
that's certainly what we expect in a real   machine learning problem, and suppose I look at 
a particular point x. Now, I can think of this   point x as a vector if I draw a line, a ray, 
from the origin to x_1, x_2, that's a vector.   Now suppose I come up with another vector, 
let's call it theta for the moment. This is   just any other vector, it's a vector I 
chose, it's just some vector out there.   Now something I can do if I have these two vectors 
is I can take what's known as their dot product,   you can also just think of this as theta transpose 
x. Now something you should always do whenever   you're doing matrix vector multiplication, is do 
a little unit test to make sure that this is even   something that makes sense and so what I mean 
by that is let's do a dimensionality analysis.   So x is a d by one vector so this is an 
important point that I sort of, you know,   went by pretty quickly on the previous slide 
but we're going to be thinking of x as a column   vector in general so it's really going to be the 
number of dimensions d by 1. It's a column vector.   Now theta is also a column vector, so when I 
take its transpose, and it's in the same space,   so when I take its transpose it's 
going to be 1 by d. So first question,   can I multiply these two vectors together? 
Yes because their inner dimensions agree,   or more to the point, you could think of these as 
two matrices: a one by d and a d by one matrix.   Their inner dimensions agree so this is an okay 
multiplication to do. What am I going to get out   when I do this multiplication? Well I'm going 
to look at the outer dimensions. I'm going to   get out a one by one matrix, aka a scalar. It's 
just a number. I'm going to get out a number.   Okay so here's the real math fact that's 
going to come up: what does this number mean?   Here's one way to interpret this number. If I took 
x and I looked at its projection onto theta. What   I mean is how much of x is in the direction of 
theta? So some of it's in the direction of theta   and some of it's in a direction 
perpendicular to theta and I want   to ask how much of it is in the direction 
of theta? It turns out that that is exactly   this dot product divided by the size of 
theta, the length of theta, so that's what   that notation means: it just means the length 
of theta and so that's what we have here. So that's the meaning of this, it's 
just the projection of x onto theta.   Now here's another unit test. It's always good 
to sort of unit test any idea that you have.   Let's think about what's going on here. Okay let's 
do a little check. Notice that if I multiply theta   by a constant, what would happen to this fraction? 
Well the constant would come out of the numerator   and it would come out of the denominator and then 
I would get back the exact same value and so,   does that make sense? Well yeah, if I multiply 
theta by a constant I just make it bigger or   smaller but the projection of x in its direction 
doesn't change, you know, the amount of x that's   in the direction of theta doesn't change and so 
that seems like a nice little unit test there.   Okay so if this quantity represents the amount of 
x in direction of theta and I choose an x that is   perpendicular to theta, what is this quantity, 
this theta transpose x divided by size of x   for this x? Sorry divided by size of theta for 
this x. So what is the projection of this x, which   is perpendicular to theta and you're all answering 
in Zoom and you're totally nailing it. It is zero.   The projection of x here onto theta 
zero because there's no amount of x   that's in the direction of theta. 
Awesome, fantastic answers.   Okay so here's another x, here's a different x 
that's also perpendicular to theta. Same question:   what's the projection of this x, this new x, onto 
the theta vector? Great and I love that some of   you are saying “still zero” to distinguish from 
your previous answer. Great it indeed is still   zero, this is also zero. Fantastic, great, okay, 
perfect. So we saw for each of these two points   the projection of x onto theta is zero. I think 
that you can see that by a similar argument,   we're expecting that for these points, if we 
looked at these values of x, the projection of x   onto theta would also be zero. In fact 
it's not just these points, right,   it's any x on this line. If I take any x for 
which its vector is perpendicular to theta,   I'm going to get that its projection is zero and 
so here's one way I could write that observation.   I could say, let's look at the set of x such 
that (so this colon should be interpreted as   the word “such that”) I'm interested in the 
set of x such that the projection onto theta   is equal to zero and that describes a line and 
that's exactly the line that we're seeing here. You can play the same game and ask yourself 
okay, well suppose I'm not interested in the   set of x whose projection is zero. Suppose I'm 
interested in the set of x whose projection is a. Well you can go through the same set of reasoning, 
you know, just think it over, go back to our   notion of a projection and what that means, and 
you'll see that you're also going to get a line   and it's going to be essentially a 
distance away from our first line,   and this line is the set of x such 
that the projection of x onto theta   is equal to a. Okay now here's something that's 
just a little bit tricky that'll make sense   hopefully if all of this has made sense so far. 
What happens if we go in the other direction?   What if we say the distance is b but it's 
in sort of the opposite direction from theta   so then if you think about it for a second, the 
projection of x onto theta has size b but we're   going to get a negative in there because we're 
sort of going in the negative theta direction. Okay so we've defined three lines at this 
point and in fact what's cool about this   is we've basically defined lines, we have a 
way to define lines. Now, I want to emphasize   in defining lines here we've totally thrown 
out if you remember maybe from high school   or something this notion of a line being y = mx + 
b. You just want to completely forget about that   partly because you'll notice y doesn't 
make an appearance here anywhere.   y and y = mx + b is just a totally 
different y. It doesn't mean the same   thing as what we're doing here. We want 
to treat all the x's somehow the same   because they're all different features and the 
y's vary differently because they're labels   and so this way of defining a line is going to 
be really useful for us for that reason. Now   I want to emphasize what we haven't yet done, 
though, is we haven't come up with a hypothesis,   right, because at this point all we've done is 
we've defined a line but we haven't said how   to label everything in the space and 
that's what makes a hypothesis. It's   got to be a function over the whole space 
and so let's think about how we can do that   and in order to go in that direction, I’m 
going to focus just on this single line   and I'm going to make the observation 
that as we go in the direction of theta,   the projection becomes more positive, right, 
so we just saw a couple of lines where the   projection was equal to zero. It was equal 
to a, it's more positive than negative b,   and if we go in the opposite direction—so here 
the projection's going to be greater than negative   b—and if we go in the opposite direction, we'll 
see the opposite effect. The projection will   be even more negative, the projection will 
be less than negative b. Okay so now we're   pretty close to actually having a hypothesis, 
an ability to label things that are going on in   the space. What we're essentially going to do 
is we're going to say “hey once we have a line and a direction defined by theta, we can say 
as you go in the direction of theta away from   that line, you can have one label, and 
as you go in the direction of theta,   or the opposite direction from theta, away from 
that line, you're going to have another label.   Okay, now in order to write this out, 
I'm just going to slightly change   the way that we've been writing 
things. So in particular here,   a completely equivalent way to write the set of 
x such that the projection is equal to negative b   is the following. So all that's happened 
here is we multiplied both sides   by the length of theta and then we 
brought that b length of theta over   to the left-hand side, so a completely equivalent 
way to describe this line is to say the set of x   such that theta transpose x plus b length of 
theta equals zero. Okay now we're going to   find it useful to not have to worry about the 
length of theta and how it interacts with b,   we're just going to call that a particular 
new constant let's call that theta naught.   So here all we're saying is that instead of using 
b length of theta, we're going to use a constant   called theta naught again, completely equivalent 
so long as we choose theta naught appropriately,   or equivalently if I choose a theta naught that 
implies that there's a particular value of b. Okay great, so at this point we have a 
way to define a line and we know what's   happening on both sides of that line 
and so let's write that out carefully. So what I'm doing next is I'm just taking 
what we have on this slide and compressing it,   so i'm just getting rid of that middle equation 
there. So we have these three equations,   all completely equivalent ways to write the 
same thing, so long as we have this relationship   between b and theta naught, and I'm just going 
to get rid of that middle equation so that's all   that's happening here. So now we just have these 
two equivalent ways to write the same equation. Okay so now we're ready to define 
a particular linear classifier. So a particular linear classifier, it's going 
to be some h and h has got to be a function   that takes inputs that are in our x space, so 
that's what we have here. We have a function   h and our function h is the following: it says 
let's look at the sign of this value that we've   just been calculating. In particular, on one 
side of the line where that sign is positive,   where theta transpose x + theta naught 
> 0, we're going to apply the label 1.   On the other side of that 
line, the line that is defined,   remember the line itself is defined by 
theta transpose x + theta naught = zero,   so on the other side of that line, theta transpose 
x + theta naught < 0 and we're going to assign   the label -1 there. So these are just 
two completely equivalent ways of writing   the same thing, which is that we'll assign a 
label 1 on one side of the line, a label -1 on   the other side of the line, and the line itself 
is theta transpose x + theta naught = 0. Okay,   so there's this sort of annoying thing, which is 
that technically we also need to assign a label   on the line itself and we haven't done that just 
yet, and it's a little bit arbitrary, but this is   what we're going to do. We're just going to take 
one of these directions and add equality to it,   so we're going to say that we're 
going to assign the label -1   on the line itself. It's a choice, it's not super 
important, but we're going to make that choice.   Okay so now, we have the definition 
of a linear classifier, so this is   a linear classifier, but something 
that's going to be really useful to us   is to be able to distinguish different 
linear classifiers. We can't call them all h   you know. If I talk about h and I want to 
compare to h and then how about this other h,   I mean that's going to be a problem right? I 
need to be able to talk about a particular h   versus another h and so on, and so we're going 
to introduce this new notation which indicates   which h I'm talking about. So if you look at 
this h, you can tell that this h is defined by   the values of theta and theta naught. Once I know 
those values, I know what h I'm talking about,   and so we're going to add those 
values into our notation of h   and we're going to put them after a semicolon to 
show that they are not inputs to our h function.   So h is still a function that goes from x's 
to y's but sometimes it has values that tell   you about it, you know, that index a particular h 
function, and we call these values “parameters,”   again to distinguish from the inputs of the 
function, to distinguish from the feature itself.   These just tell you how to apply h or let 
you distinguish between different h's.   Okay so in this case our parameters 
are theta and theta naught. And now our hypothesis class H. So we said at the 
beginning of this slide, a hypothesis class is a   set of hypotheses that makes sense. I mean it's 
almost tautological, it's a class of hypotheses,   but then we wanted to define 
a particular hypothesis class,   we wanted to define a particular hypothesis class. 
That was all the hypotheses that label 1 on one   side of some line and -1 on the other side and so 
now we define a hypothesis that labels 1 on one   side of a line and -1 on the other side. We can 
do this for every line, but not just every line,   every direction that you might label plus and 
minus, and we can see that that's determined   by theta, and so now when we collect all of those 
linear classifiers together, we can define exactly   the hypothesis class that we wanted to, which is 
all the hypotheses that label 1 on one side of a   line and -1 on the other, and we call these the 
linear classifiers. So any example of this is   a linear classifier and then the particular, 
you know, example we might be looking at   is defined by theta and theta naught but then 
the collection of all of them is this script H. Okay so let's take a step 
back. Where are we right now?   We have our data and then we decided we want to do 
something with it, we wanted to be able to, say,   you know, make some prediction in the future based 
on that data. In order to do that, we needed a way   to make predictions, and we needed it to be good 
and at this point, essentially we have just come   up with ways to make prediction. They're not even 
informed by the data, we've just named a bunch   of ways that one could make a prediction, but you 
don't need any data to define a linear classifier,   you could just define a linear classifier. You can 
choose your favorite theta and your theta naught   and so essentially what we really need to do now 
is to talk about what is a good linear classifier.   We've talked about that in intuition, but we 
haven't specified very precisely what that means,   and then we need to be able to find a good linear 
classifier, and so that's what we're going to   talk about next. First, what makes a good linear 
classifier, and then how do we find such a good   linear classifier. Okay so let's first 
talk about how good is a classifier. Again, we're just going to be taking the intuition 
that you already have and formalizing it here. Okay, so what makes a good classifier? What would 
be a good classifier? Well in some sense again,   let's think about, you know, what are our data   analysis goals? I think it's always good 
to come back to that, to ask ourselves:   what are we trying to do? And so if we have a 
bunch of data on newborns who have had seizures,   we don't just want to look at that data, we 
want to be able to say for future newborns who   come into this hospital: are they going to have 
a seizure? We're trying to make predictions   about future data points, so in that sense, 
what we really care about is that we want to   get those right. We don't want to say that these 
newborns aren't going to have seizures and then   they actually have seizures. We would feel really 
bad about that. We want something that's going   to be really accurate, so that people can make 
the best possible judgments about medical care,   and so in that sense again, we want to predict 
well on future data, future data that comes along.   Now in some sense what's really going to happen 
is that a number of newborns are going to enter   into this hospital, we're going to be getting, you 
know, data on all these newborns as they come in   and maybe it's going to be hundreds or 
thousands of them and we'd like to do well   on all of them, but in order to talk about all of 
them, I want to be able to talk about a particular   point and then we can talk about multiple points. 
By the way, I'm just going to remind people very   quickly if you have any questions about the 
lecture, make sure to go to the Discourse link,   and so hopefully you can see it in the chat, 
maybe one of the staff can just repost it.   So I'm only using the chat on Zoom for answers 
to my questions, but if you have any questions   in general, make sure to post them on Discourse 
and then the staff will either filter them to me   or answer them on Discourse. Great, thanks very 
much. But okay great, so let's start by asking:   we want to figure out how good is a classified 
single point. Oh I see that there actually is   a question, sorry about that yeah do 
you wanna just put that out there?   Yes, so the question was just a 
clarification of what theta naught   is with respect to the linear classifier. Oh 
great okay yeah so let me just go back to that   slide really briefly and then we'll come back 
here to talk about how good is a classifier. Okay so something that might 
even help us here a little bit,   is yeah. I'll stop here for a moment. So remember 
the way that we set up this linear classifier   was that we said we had a particular line, 
that line is defined as the projection being   equal to a particular value, so this projection 
was theta transpose x divided by size of theta   and then it's equal to a particular value, let's 
just call that value -b. It could be any value,   we're just talking about being interested in 
a particular value, so that defines a line.   That tells us a line and now the classifier is, 
okay, well if I go in the direction of theta,   we'll call that plus. What we'll label those plus, 
and if I go in the direction opposite theta, I'll   label those minus. Now in order to talk about that 
line, we could say this is the projection equal   to -b, but a completely equivalent way to write 
that is theta transpose x plus b times the size   of theta = 0, and a completely equivalent way to 
write that is theta transpose x + theta naught = 0   if I define theta naught to be b times the size 
of theta, and so what is theta naught? Here,   one way to think about theta naught is it 
is the distance from my line to this vector   times the vector theta itself. Another 
way you can think about it is that—you   don't even have to go through this by the 
way to think about this. You could just say,   I have an equation defined by theta transpose x 
+ theta naught = 0. That is just a well-defined   equation and you don't need to have understood 
anything I said about projections or anything   I said about b or anything like that to make 
sense of that equation. That's just an equation   and it's an equation that you can check certain 
values of x will satisfy and if you check all   the values of x that satisfy that they'll 
define a line, and in that sense, theta naught   is just part of a definition of our line. 
Together theta and theta naught define that   and you can see something that I think you'll 
be doing probably in one of the problems coming   up but you could even plot this right now for 
yourself, is just try changing the values of   theta and plotting this and seeing what you get 
and I think you'll see how theta naught affects   this definition of a line, and therefore 
the definition of the classifier. Great. Okay, let's just go back to 
what makes a good classifier.   Okay, so we want it to predict well in 
future data but we're going to start by   asking how good is it at a single point 
in order to talk about multiple points,   and so, in particular, let's imagine I have this 
new point that comes along and I think you have   a sense again, an intuition by looking at this 
plot, of what would be a good label for this point   and so let's again, let's make this precise. 
What does it mean to label a point well?   Well in order to do that we're going to have 
to introduce a function that we call the loss,   so L is the name of the loss function and it 
takes two arguments: those arguments are g   which is our guess, and a which is the actual 
value, and so somehow we're going to be exploring   a lot of different losses in this class. 
We're going to talk about some right now for   classification but you can have them for other 
machine learning problems. You can have even   different ones than the ones I'm going to mention 
right here, but somehow the whole idea of a loss   is always going to be that our guess should 
be close to the actual value. You know,   somehow we want to guess something that is like 
the actual value and so how can we express that? By the way, if you have ever experienced perhaps 
in another class or your other work the notion   of a utility, a loss could be thought of as a 
negative utility. Somehow a worse/ a larger loss   is bad, whereas a larger utility is good. Okay, 
so here's an example of a loss, sort of the most   basic loss you can have for classification: it's a 
zero/one loss, and so the idea of a zero/one loss   is that, if my guess was right on, if it 
was perfect, if it agreed with the actual   value what actually happened, then I don't 
lose anything, that's like the best possible   thing that could happen, I haven't lost anything. 
But if it was not right on, if it was not equal,   then I incur a loss of one, and any positive 
loss is bad. It's sort of like how sad am I   that I got this wrong, and so here we're 
saying that we are sad to a level of one. Okay, so a problem with this that 
you can almost immediately see,   especially in the example we've been talking 
about, is that it's symmetric. So for instance,   let's think about this newborn case. Suppose 
that I have a newborn who comes in and they   are not having seizures and they never have 
seizures and I diagnose them as having seizures.   Well, what's going to happen is a doctor is going 
to come in, we're going to spend some time with   that doctor, but ultimately the newborn's 
going to be okay, because they don't even   have seizures. Alternatively, here's another way 
I could get my prediction wrong: a newborn could   have seizures but I diagnose them as never having 
seizures and so we never have a doctor come in,   nobody sees the newborn, the newborn has seizures, 
and then they have all kinds of really bad medical   outcomes that could have been prevented if we 
had correctly diagnosed them as having seizures,   and so this seems just much worse than the first 
scenario. There's a real difference in this case   between false positives and false negatives and 
so sometimes we want our loss to express that,   so here's an example of an asymmetric loss and 
what we mean by asymmetric here is that guessing 1   when the actual is -1 is very different in terms 
of, you know, how much it matters to us versus   guessing -1 when the outcome, the actual, is 1 
and so here I might say, oh well if I guess that   the newborn was going to have a seizure, but the 
newborn didn't, I incur some loss because, you   know, that was some resources that we spent that 
we didn't have to. You know, we could have saved   money, we could have saved time, that doctor could 
have done something else, and so there is a loss,   but if I guessed that the newborn was not going 
to have a seizure, -1, and then the newborn did,   that's a much bigger loss because we've lost the 
ability to really medically help this newborn.   Maybe they have some bad medical 
outcome and so we want to say that   that is a much worse outcome and therefore 
the loss is much bigger. Now in practice,   it's going to be a difficult and important 
question to say: well exactly how do you   balance these losses? You know, 
how do they relate to each other?   And that's something that you're really going to 
have to think about in any particular application,   but I just want to open up this possibility 
of having this asymmetry, because it can be   really important in a lot of cases, including 
this one, the one we've been talking about. Okay so now we said we want to predict 
well on future data, and so what is   future data? Well let's imagine that n’ new 
points come in, so we had our original n   training data, and now we have our n’ new points, 
and so now we could say, what's our loss over all   of those new points? We might call that the test 
error to distinguish it from our training data.   So what are we defining here? Well what's 
happening is we're defining our error,   let's call it E, it's a function of h, so 
our loss is obviously going to depend on   however we classify things, and h is what 
tells us how to do that. We're doing a sum   over all of the new data, so we imagine that the 
new data is indexed from n + 1 to n + n’ prime   because there are these n’ new data points, and 
let's just start the indexing after we did this.   Okay and now what we're going to do, is for 
each of those, we're going to take the loss   and we're going to compare h(x), so that is what 
we are guessing at x, h(x) is exactly our guess,   and then our actual value is the actual observed 
y, and then we might average this all up to say:   what's the average error? So that's the 1 over 
n’ in the beginning. Okay so this is, I mean   this is sort of exactly what we want. We want 
to be able to predict well in the future data,   and the whole problem with this is that we do not 
have access to the future data, so this is a total   fantasy that we can never actually do in real 
life. So yes, I would love to be able to say,   ah yes in the next 500 newborns that come in, I'm 
going to get it right, you know, 99 of the time,   and in order to even make that statement I 
would have to know whether those newborns   were going to have a seizure or not, in which 
case I should have just used that information,   and so being able to calculate something like this 
for actual future data would require knowing that   future data, which we don't have, and so we're 
gonna spend a lot of time thinking about, you   know, how can we actually think about classifier 
quality, and how do we, you know, deal with this,   but for the moment, here's another idea: something 
that we can calculate which we might think of as   some kind of proxy which is, let's look at the 
loss on the data that we do have, and we'll   see that there are pluses and minuses to thinking 
about this kind of thing, but it will be a useful   thing to think about, and it's basically the 
same idea: we're going to take our training data,   our n data points, and we're going to add up the 
losses across those data points between our guess   and the actual, which you can see in the case 
of linear classification doesn't always have   to be the same. We saw some linear classifiers 
earlier that would have misclassified some points   and then we get all of this together and 
we'll call it our training error E_n. Okay so now what we're going to do is we're going 
to say that once we have this notion of error,   this E_n, we can decide between two classifiers: 
we can say “I prefer classifier h to classifier   h tilde, so let's just say these are two 
classifiers, h and h tilde, if the error is lower,   if this notion of, sort of an average loss, is 
lower, then i'm going to choose E_n, or sorry,   I'm going to choose h.” Okay so at this point, 
I have a way to decide between two classifiers,   but that's not what I want to do, right? I want to 
have my data and come in and choose a classifier,   in fact I'd like to choose the best classifier in 
some sense, now that I have a notion of what it   means to be a good classifier, what I'd love to do 
is just say, “hey here's my class of classifiers,   let's just pick the best one, let's pick the one 
with the lowest error”, and that just turns out to   be very computationally difficult and so that's 
why we're going to have to think about other   things, other methods to deal with this, because 
if we could just do that we might just do that. Okay so at this point, we have a 
notion of what makes a good classifier,   and now let's start thinking about   how we can learn a good classifier. If I have some 
data, how can I choose a classifier that is good? Okay, so imagine that I have data   and I have my hypothesis class and I want to 
choose my good classifier, so this is where we are   at this point. We know, we know, what data looks 
like, we know what a hypothesis class looks like,   like maybe I chose all these linear classifiers 
and I want to choose a good classifier,   now first of all let's just remember what is 
a classifier. A classifier is a function that   takes in the values of x and gives out the values 
of y and so this is an example of a classifier,   and it's always useful to think: what does it 
do when it acts so if I take this classifier   and I want to put an input in, that 
input is going to look like a value of x   and then what I'm going to get out is I'm 
going to get out a label for that input. Now something we're going to start 
talking about now is a different idea   called a learning algorithm. So the learning 
algorithm is going to take in a whole data set,   now you'll notice that for a classifier, 
we didn't need a data set at all,   it's just a function over 
all of the possible x values.   So in order to define that, there's 
absolutely no reference to data,   but for a learning algorithm, we're going to 
take in a data set, D_n, and we're going to   spit out a classifier, and we hope it's a good 
classifier, and so for instance, what might   that look like? Well maybe, this is my data. I 
put that as an input to my learning algorithm   and I'm going to get out a classifier and 
maybe this is the classifier I get, and if   I got off this classifier, I might be pretty 
happy it looks like a pretty good classifier. Okay here's another data set. So since I have a 
new data set, I can apply my learning algorithm   again and I can get out a new classifier and 
you'll notice that it doesn't have to be the   same classifier, just like if I apply a classifier 
to different x values, I don't have to get out the   same label y. Here if I apply a learning algorithm 
to different data sets, I don't have to get out   the same classifier h, these are both functions 
but they're functions on very different spaces,   they apply to very different things. Okay so just as we've seen examples of h, let's 
now look at an example of a learning algorithm.   Now I want to say we're talking 
about values, possible things for h,   we're talking about examples of h, we’re 
talking about examples of learning algorithms,   just as we saw some examples of h that were 
not good, we will see some examples of learning   algorithms that are not necessarily good learning 
algorithms, but they're still learning algorithms,   and so let's take a look at a 
potential learning algorithm. So here's our example learning algorithm. Now 
before we get into the learning algorithm itself,   let's imagine that my friend did some work for 
me and came up with a bunch of classifiers, so   in particular, my friend went and for a trillion 
different times, generated some classifiers for   me. They randomly sampled some lines in this 
x space, I don't know exactly how they did it:   they have some distribution over lines, 
they got all these different lines,   you know, maybe they'll tell me if I ask, but they 
came up with a whole bunch of different lines,   in fact a trillion different lines, and they made 
them all into classifiers, so now they gave to me   a trillion different lines and they're all 
different and they're all really interesting. Okay so there's a question about how good is a 
classifier. So to compare between two classifiers   which is better, to use test error or training 
error, now I want to emphasize this point:   what we really want to do at the end 
of the day is we want to predict on,   let's say the next 500 newborns that come into my 
hospital. And so that’s the best way to compare   is to say “hey the next 500 newborns that come 
into my hospital, how well did I do at predicting   whether they actually had seizures or 
not?” I mean that's what I really care   about the end of the day, was were those 
newborns given appropriate medical care?   So that's the best way to compare. The problem 
is that I can't do that. It's definitely   what I want to do, but it's impossible 
because I haven't yet seen those newborns   and so I'm going to have to think about how do 
I get around the fact that I didn't have access   to the future newborns who have not yet come 
into my hospital. How can I possibly deal with   that fact? I don't want to suggest that we have 
answered that in this lecture because we have not,   that's going to be a question that we're dealing 
with in the future in this class. In fact,   in next week, we're going to be talking about it a 
lot more, and so I just want to put in your minds   that the this error on the future data that we 
really want to predict on, that we haven't seen,   that's what we wish we could do, that's what we 
really want to evaluate on, and the whole problem   is that it's in the future so we can't, so how 
could we possibly deal with that fact, and so all   we're doing for the moment is we're saying that 
one possible idea, one possible notion of goodness   use is training error. I'm not saying 
it's the best, I'm not saying it's what   you should use in absolutely everything that 
you do when you're thinking about goodness,   but it's something that is useful for the moment, 
in fact we're about to use it in this learning   algorithm in just a second, but we're going to 
be thinking about this question so much more   again next week and throughout this course, and so 
don't think that we have, we have learned it all,   that this is the end of the discussion about 
what is good. Okay great, great question and I   hope you keep, you know, wrestling with 
that question even after the lecture.   Okay, so let's go back here, so my friend 
has generated this really, really long,   trillion long list of classifiers and now 
here's my algorithm, here's my example   learning algorithm. In fact, let's call it 
“example learning algorithm”. Now remember a   learning algorithm takes as input a data set, 
so this is going to take as input a data set,   but just as we had a parameter that we could 
set in h, something that sort of modulates h,   that changes h, so we are going to here 
have a hyper parameter. So a parameter   is something that changes our hypothesis h, 
a hyper parameter is something we change in   our learning algorithm that isn't the input, 
it's like an adjustment to our code, you know,   maybe just some some value that we'd like to be 
able to change and so here we're going to choose   some hyper parameter, let's call it k, and suppose 
that I restrict k to be less than a trillion. Okay, so here is my algorithm, I'm going to 
explain what this notation says. So first of all,   what we're going to do, is for every classifier, 
from classifier 1, classifier 2, up to classifier   k, we're going to calculate the training error 
for that classifier, that's that E_n(h^(j)).   Now what the next part says, this arg min, it 
says once I've calculated all of these errors,   I'm going to find which one is the smallest,   the min, and then I’m going to figure out 
which j, which argument, went into that,   so which particular index gives me the classifier 
with the smallest error, the minimum error   and let's call that j*, and then what I'm going to 
do is I'm going to return the classifier for j*. Okay so, this is a learning algorithm 
right: it takes in a data set and it returns   a classifier. Let's say that I only had k = 1. 
Can anybody tell me in the zoom chat this time   what classifier will be returned? Awesome, great. So I'm seeing a bunch of people 
saying h^(1). h^(1) is what's going to be a return   so what's going to happen here is if I have k=1, 
then this is going to say, okay let's calculate   the error for all the classifiers from 1 to 
1. Okay there's only one classifier, so I'm   going to calculate just the error for that, I'm 
going to say what minimizes all of those errors,   well there's only one choice, it's the first 
classifier, it's the only one I'm looking at,   and then I’m going to return that first 
classifier. Now this only gets interesting   when I have two classifiers. So here's a 
question: let's say I take the training error   of the classifier when I run this algorithm 
with k=1 and now I run the algorithm with k=2,   how does that training error compare? Okay great, so let's talk through this. I'm seeing 
some great answers here, so we just said that if   I run this algorithm with k=1, I just get out 
the first classifier, I get out h1. If I run   this algorithm k=2 which happens, what happens 
is I compare the training error of h1 to h2,   whichever one is lower, I choose that, and 
so by definition, that has to have at least   as low error as if I ran this algorithm with 
k=1, and so it doesn't have to have strictly   lower error, maybe they have the same error, but 
it can't be any greater when I run it with k=2.   Okay so at this point, we've seen an example of a 
classifier, we've seen an example of a hypothesis   class, a whole class of classifiers, the linear 
classifiers, we've seen an example of a learning   algorithm, we've seen an example of data, and 
how we can take this learning algorithm and   turn data into a classifier that has some kind of 
error, and in fact something you might check for   yourself is that as k increases, you're going to 
see progressively lower training error. You might   want to just double check that that's true, see if 
you can, you know, figure that out for yourself,   and so now what we want to think about is can 
we do better? You know, we've talked about ways   to evaluate and there was a great question about 
is this how we should evaluate. Well let's think   about that. Let's engage with that question. 
Here's a learning algorithm. Is this the learning   algorithm we should use, you know, probably it 
will be no surprise to you that it's not because   we still have many lectures to have together and 
so surely we will do better in those remaining   lectures and so we're going to be talking about 
these things. How can we have better learning   algorithms? How can we accomplish our goals? 
What exactly are our goals? What is the best   way to encapsulate those and make them rigorous? 
Okay we're at the end of our lecture time today,   we will see you all for labs this week. I think 
that'll be exciting and fun, we'll probably   mostly be ironing out bugs this time, but I think 
ultimately these are gonna be a really great time   and I will see you all for lecture again 
same time on next Tuesday, have a good one. Bye. 

Okay, I think it's MIT time so let's go ahead and 
get started for today. A lot of the information   at the top of this slide is basically some 
of the logistical stuff that we've covered   in previous lectures but I wanna especially 
highlight the Discourse site which Crystal   has also helpfully just put in the chat. Remember 
today's category for the live lecture is “Lecture   2” and so there's a nice example question in the 
Discourse with “Lecture 2” and so just make sure   to use that category for your questions today. 
Okay so last time we talked about, you know,   why are we even talking about machine learning 
and some basic setup questions and sort of,   you know, did the notation for setup—what's the 
data we're talking about, etc—and started in on   an example type of machine learning problem where 
we're interested in linear classifiers and we're   interested in learning algorithms to find a really 
good linear classifier and so we really just got   very much started and we're gonna, you know, 
continue on that journey today by talking about:   “hey, what are some problems with the linear 
classifier?” You know, we found a learning   algorithm last time. We talked about a learning 
algorithm, so why are we still talking, you know?   What is problematic about that learning algorithm? 
And we're gonna see that it's not perfect   and that we can do better perhaps with 
something like a perceptron algorithm so   we're gonna introduce this alternative learning 
algorithm, the perceptron. We're gonna talk about   different types of linear classification, how 
a particular data set might represent a harder   or easier problem in these cases and that will 
help us talk about how well the perceptron   performs in fact in a mathematical theorem. Okay 
so let's go ahead and just recall some key facts   that we're going to be using throughout the day 
today about classifiers. So in particular, last   time again we focused on linear classifiers. So we 
have our classifier h and recall that a classifier   takes in a set of features x and it outputs a 
label y and here it has some parameters theta   and theta naught that sort of determine its shape, 
so in this case recall that our linear classifier   basically predicts a value of +1, a label of 
+1, in some part of the space and a label of -1   in another part of the space. In particular, we 
saw that we have this theta which is sort of a   normal vector to our hyperplane, we have a theta 
naught which is the offset of our hyperplane,   and together they define a hyperplane and so 
on one side that hyperplane we predict +1,   on the other side of that hyperplane we 
predict -1, and so in two dimensions,   you know, if our features were x_1 and x_2. For 
instance, last time we talked about, perhaps, we   have a problem or we're interested in predicting 
whether newborns might have a seizure. That would   be an important medical piece of information and 
we might have information about maybe, you know,   sort of their oxygen uptake and whether they're 
moving a lot and we might want to predict that,   and so the types of classifiers we might be 
interested in are these linear classifiers. At   least certainly that's true for the moment and so 
we have our normal, our theta, we have our offset   or theta naught and that's going to define a line. 
That's the set of x's that satisfy this equation.   Now you know we said this line on its own isn't 
enough. It's together with this direction, theta,   that defines the linear classifier. In particular, 
on one side of this line we're going to get   a positive label, a +1 label, on the other side of 
this line we're going to get a negative label, a   -1 label. Now, something that you'll have noticed 
is that we don't always have to put theta naught,   or sorry, we don't always have to put this vector 
theta directly at the origin. So, we can move it   around this normal vector, so that's what I'm 
doing here, I'm moving it. In fact we could move   it around to all kinds of different places, the 
key thing is it's always normal to the hyperplane,   it's always perpendicular to the hyperplane, and 
what really defines it is going from its base to   its top. So what's the difference there? 
That's the thing that is going to define theta.   Okay, now in general this won't be a line, this 
dividing thing, this, you know, divider. It'll   be a hyperplane. So if my data exists, my 
features exist in two dimensions, like here,   it'll be one dimensional lower. In fact, it will 
be a line, but if my features existed in three   dimensions, it would be one dimensional lower: it 
would be a plane. If my data features existed in   four dimensions, it would be one dimension 
lower: it would be a three dimensional,   you know, object and so on and so forth. So, you 
know, again it's worth keeping in mind that we   as humans can only see in two dimensions, our 
illustrations are necessarily two-dimensional,   but it's worth thinking what would happen in 
higher dimensions and so it's, you know, at least   worth trying to visualize the three-dimensional 
version of this and convince yourself   what that would look like. Okay, so we said: 
“hey, now that we have this notion of a linear   classifier, we can look at the hypothesis class of 
all linear classifiers.” This is just, you know,   the set of them all and then we're going to try 
to choose a good one for some data is our plan.   How are we going to define what it means to 
have a good classifier? Well we're going to   look at a loss. In particular, a very common one 
for classification is this 0/1 loss. So remember,   g is our guess and a is the actual value and so 
what we're saying here is that we'll have a loss   of 0, which is the best loss, if we guess 
the actual value, if we're exactly right,   and we'll have a loss of 1 if we're wrong in any 
way. And so now, once we have a notion of a loss,   we can look at training error. So in particular, 
for our classification problem with the 0/1 loss,   what this training error says is we're gonna count 
up ones for every time we get something wrong and   then we're gonna divide by n, the total number of 
data points. So the training error is exactly the   fraction of data we get wrong, the percent of data 
that we get it's prediction wrong on the training   data because this is the training error. Okay 
so last time we proposed a learning algorithm.   So remember the idea of a learning algorithm 
is that we take in a set of data and we return   a hypothesis and ideally there should be a good 
hypothesis for that data, that's sort of the goal.   You know you can have bad learning algorithms, 
that's still a learning algorithm as we saw last   time, but we'd like to have something where this 
hypothesis is going to be a good classifier for   data and, you know, we feel like this is something 
we'd like to use and so for our example learning   algorithm from last time we said “hey my friend 
went out and generated a whole bunch of hypotheses   like totally randomly, just made some random 
hypotheses in a list, and then gave them to me and   then walked away.” And then I'm going to do the 
following with those hypotheses: I'm going to say   I'm going to take in my data, so this is the 
first time I'm even seeing my data. Here,   I have a hyper parameter k which tells me 
how many of these hypotheses to look at.   I'm going to look at the training 
error on each one of these hypotheses,   I'm going to figure out which one of them has the 
minimum training error among those k hypotheses,   and then I'm going to return that. Now another way 
that I could do this is I could say: “okay let's   let me look at my first hypothesis. What's its 
training error?” Okay that's my sort of putative   hypothesis, the one I'm probably going to return 
at the end. So far now I'm going to look at the   next hypothesis: does it improve the training 
error? If so, that's my new best hypothesis so   far, if not I stick with the one I already had 
and then I just keep doing this. I say “ okay,   for the third one, is it better than the ones I've 
seen so far?” If so, it's my new best hypothesis   and I keep going and I can keep increasing 
the number of hypotheses. So let's actually   let's try this out, let's see it in a demo, 
and so that is what I'm going to do right now. So okay great. So what you're 
seeing here are two different   plots. Let me just explain the top one first and 
then we'll get to the bottom one. So in the top   plot, first of all, you'll notice that the axes 
are x_1 and x_2. So this is a plot of our data   and so we have our data at its feature vector x 
and then you have a label, so it's either plus   in case it was labeled +1, or minus 
if it was labeled -1. So particularly,   this is our training data, we know all the 
labels and we're interested in finding some   classifier. Now I hope you'll just take a moment 
and look at this training data and ask yourself,   you know, “is there a classifier here that 
you might think would be a decent classifier?”   I hope that you see that there indeed should 
be such a classifier. What if I had the best   classifier here in the sense of training 
data, like if I minimize the training data.   Now this is going to be a question that I'm going 
to ask you to respond to me privately in the chat:   what would be the best training data that I could, 
with the best training error that I could achieve   on this data set with a linear classifier? Great 
lots of folks are saying 0. Indeed, it looks like   there should be a line that perfectly tells you 
which side is plus and which side is minus so   that's exactly right. Okay so you know that and 
I know that, but let's find out if our algorithm   is gonna find that, and so what's going on with 
this line here, so the line that we're seeing here   is the first hypothesis proposal. So 
it was randomly generated by my friend,   my friend didn't see this data at all, they 
just gave me this random line and it's my first   hypothesis and so what's happening in the title 
is I'm saying “well before I saw this hypothesis,   I had no idea what the training error was and 
so let's just say it's 1, because that's the   worst training error that I could get for this.” 
The training error we saw before is between 0   and 1 because it's a fraction wrong, and this 
new hypothesis has a proposal, is a proposal,   it has a training error of 0.47. So let's just 
take a moment to see is that a reasonable thing.   Okay well this hypothesis is this line here and 
it says predict plus on this sided line, so it's   getting these points right and it's getting these 
points right, but it's getting all of these points   wrong and so it seems plausible that that could 
be in fact the training error for this proposal.   Okay so now what we're also going to do is, 
because we're going to go through a few different   proposals as we go along, we're going to plot the 
best training error so far down here. So here,   we've only seen one proposal so far. We've just 
looked at k = 1 and that training error is 0.47.   So that's why we're seeing a little dot here. 
Okay so now I'm going to go one step forward.   Okay so what's happening here is the 
best proposal I saw before this step   is in blue. That's just the last proposal because 
there was only one proposal before this step.   Now my new proposal is that dashed black line and 
so, hey, this is a better proposal. You'll see   that so far, my best training error, which is just 
the training error of the previous line, was 0.47   but my new proposal has a training error 
of 0.2 and, you know, just looking at this   proposal it looks like a better proposal: it's 
closer to saying the pluses are on one side   and the minuses are on the other side, and 
so let's accept that. Let's say that you know   that's the best proposal we've seen so far, so 
that's going to be the classifier, that's our   best classifier so far and in fact it has a lower 
training error. Okay let's go one step forward.   So now we're getting another new proposal, so 
again my friend just generated these before we   saw anything. They're not based on the data in any 
way, it's just another new proposal and that's in   black and our best proposal so far was in blue 
and now you can just see this is a worse one.   So even though our current training error of 
the best proposal is 0.2, the new proposal has a   training error of 0.3. That's not better, so we're 
not going to use that, we're just going to throw   it away, and so far our best proposal still has a 
training error of 0.2. Okay let's see what was my   friend's next proposal, it was this line in black. 
This has a training error of 0.7. This is just a   horrible classifier, I mean you could do better 
by guessing at random than 0.7. That's, you know,   getting 70 percent basically of the labels wrong, 
and so we're definitely not gonna keep that.   Okay so we can do this again, we can 
see, okay, what's the next proposal?   It's some other literally random line and it's got 
a training error of 0.45 which also isn't good.   Okay so there's this pattern emerging where, 
remember, these lines were generated before   we saw any of the data: they're not based 
on the data, and so they're not, you know,   systematically getting better in any way. We're 
really just hoping to get lucky and hoping that   one of these lines is just going to happen to fall 
in the right spot, and so we can keep doing this,   we can keep, you know, looking at these proposals 
that our friend generated for us before and   they just keep looking bad and we're not accepting 
them because they're not in the particular place   that we wanted them to be, and this can just keep 
going on and on. In fact, let's like step, you   know, 100 steps into the future. We've looked at 
100 steps and our training error still isn't zero.   It's like we got very close, we got very close to 
zero, but it doesn't help us in the sense that,   well, we now have to wait, we have to keep waiting 
until something just happens to fall in that gap,   and it's not like we've learned anything. It's not 
like we're proposing better hypotheses based on   what we've done so far, we're still just proposing 
random hypotheses that somebody generated   beforehand and so nothing's getting any better and 
in fact, you know, we can keep doing this, we can   keep looking at 100 more hypotheses and 100 more 
hypotheses, you know, now we've looked at a total   of 413 hypotheses and the training error still 
isn't zero, and this just seems like a pretty   simple problem. It seems like we should be able to 
find a classifier that divides these two sides of   the line and so that's really where we're going 
next. We'd like to find an algorithm, a learning   algorithm, that's a bit smarter, that somehow uses 
what we've done so far to try to propose a better   hypothesis based on that information rather than 
just looking through basically every hypothesis   and hoping for the best. Okay so before I go on, 
I just want to check if there were any clarifying   questions or questions just understanding 
what's going on in this, in this graph. Someone asked, “how do we ensure that we're 
generating different hypotheses each time   and that we're not repeating the hypothesis?” 
So we don’t. This is, this is great. What's   going on here, my friend just gave me a list 
of hypotheses. Maybe my friend’s hypotheses   are actually going to be all the same at some 
point, I don't know that. This is not a good   learning algorithm because there's, we're not 
generating hypotheses in a smart way. I agree   that it would be good to make sure that all of 
our hypotheses are unique at the very least,   but I think we can do even better than that. 
It would be nice if our hypotheses weren't just   unique but somehow got better, you know, somehow 
tried to be closer to dividing the pluses and   the minuses, and so these seem like things that we 
would like in our algorithm and let's try to think   about whether we can go in that direction. Spoiler 
alert: we might not finish that quest today. Another question is: “what is k in this?” Great, 
okay yeah. So k is the number of hypotheses   we have tried so far. So you could think of it 
as the hyper parameter to the learning algorithm   in the slide. Basically it's just the number of 
hypotheses we've tried so far. So we have this   list of hypotheses for our friend and we're saying 
“what's the best of the first k?”, and so when k   is one, we're just saying, “what's the best of 
the first hypothesis?” It's always going to be   the first hypothesis, but at this point we've 
looked at 413 hypotheses and the best one still   isn't actually dividing our data and we think that 
maybe after 413 steps of effort that we should be   able to find something that actually classifies 
everything correctly here because you did it on   the first try, right, like you just looked at 
this and you said “I know a good classifier that   can get training error 0.” And so I feel like we 
should be able to automate that kind of process.   One more question: “and if the point is 
on the line does it count as positive   or negative?” Great yeah, so let me use 
that question to go back to my slides. So as we talked about before, this 
is a somewhat arbitrary decision,   but for the purposes of this class, we're going 
to say that if a point is exactly on the line,   then we're going to give it a 
negative label with this hypothesis. Okay so unless there's anything 
pressing, I'm going to go forward.   So this was our learning algorithm. We 
said, “hey, it is a learning algorithm,   the training error you'll notice is going 
to always, it's never going to get worse.”   We never add a new hypothesis and then get a 
worse error, but we'd like it to be faster.   We'd like it to go down to a good error as 
quickly as possible and so in some sense this   is what we're going to try to address now: 
can we get better performance in that sense?   Okay so we looked at our demo, let's try to find 
a better learning algorithm and in particular   we're going to introduce the perceptron algorithm 
and then we're going to ask ourselves is this a,   is this what we want? Is this a better learning 
algorithm? Okay so the perceptron is also a   learning algorithm. So it's going to take in data 
as its input and it's going to output a hypothesis   as its output and it's going to have a 
hyperparameter tau which is much like k   in the previous algorithm in that it sort of gives 
us an upper bound of number of things to look at:   it'll be a number of iterations, really. Okay so 
we start by initializing our values of theta and   theta naught. Here's just a really quick question, 
again this is going to be for the private chat,   so if you could just think about how 
many zeros is theta going to have.   Do you have any thoughts about how many zeros 
theta is going to have? Now remember the way   that we're interpreting theta and theta naught 
is exactly as on the previous slide. We want   to do a dot product between theta and x and add 
theta naught to it so it, this has to satisfy,   you know, that dot product dimensionality 
analysis that we did last time. It has to   be something we can take a dot product of with 
x. so remember x is a column vector of size d,   and so theta has to be something we can take a dot 
product with size d and so yes, exactly. A number   of you are saying it has to have d zeros because 
again, that's the only thing that we could take a   dot product of x with when x is a column vector of 
size d. Okay great, so the answer here is d, it's   going to have d zeros just so that we can even 
take a dot product as we were thinking of before.   Now we're also going to initialize theta naught. 
Something that's a little bit tricky here   that you might notice if you're paying extra 
attention is that this actually does not define   a hyperplane, the set of x such that zero dot 
product x plus theta naught equals zero. Well   that's just everything. Everything is going 
to equal zero there and so we're gonna have   to quickly get to the point where we have a 
hyperplane and we have a classifier, so let's   pay attention to that as we go forward in the 
algorithm. Okay so as I said, tau is the number   of iterations in this algorithm, so we're going 
to step through these iterations one through tau.   We’re gonna change something, and so 
here's just an indicator of whether   we've changed it yet. So far we have 
not changed it, so “changed” is false.   Now we're going to go through each of our data 
points. So remember we have n data points and   we're going to go through the data indices i 
equals 1 to n and we're going to check something.   Okay so let's take a moment to think what does 
this check tell us. Now if y^(i) is positive   and this second term theta transpose x plus theta 
naught is negative? Then this statement will be   true, this whole thing will be negative. Well what 
does that mean? Well if y^(i) is positive, our   label is +1. If this second term is negative, that 
means our predicted label, our guess label is -1,   and so that means the actual was +1, the guess was 
-1 and we were wrong. Now similarly if y^(i) is -1   and our predicted label is +1, then we're also 
going to get something negative, and this whole   statement will be true. And so one way to think 
about this statement is roughly that it just says:   did we get this classification wrong? Did 
we classify this particular point wrong   given our current iteration of theta and theta 
naught, our current values of theta and theta   naught? And that makes sense, if we get something 
wrong, that's when we want to make a change,   because we want to correct it and so that's 
what this if statement is effectively doing.   Now there's a little bit more nuance here that 
relates to a question that we just had too.   So one, if this prediction is wrong and we're not 
on the line, which is the case we just discussed,   then this if statement is going to be true. 
You'll also notice that if a point is on the line,   this if statement will just be true, because if a 
point is on the line, this quantity is equal to 0   and so the whole thing is 0 and 
0 is less than or equal to 0.   Also this if statement will be true if we're on 
the very first step, because in the very first   step theta is 0 and theta naught is 0 and so this 
whole thing is 0 and 0 is less than or equal to   0. And so this is nice because we said in the 
beginning, “hey this theta and theta naught,   that's not really a linear, describing a linear 
classifier” and so what's nice is that we're   going to immediately change them on the first step 
here. Okay so what's going to happen if we decide   to change, if we find that there's an error and we 
want to make a change? Well we're going to update   theta and theta naught and I'll explain in a 
moment what these updates do and why they might   be sensible, but for the moment let's just 
say we're going to make some kind of update   to theta and theta naught and we're going to say 
that we in fact changed theta and theta naught,   so we're going to make our indicator 
say we did make a change on this round. Okay so if we went through every data point and we 
didn't change theta and theta naught at this point   we're gonna break out of the algorithm and we're 
gonna finish up and once we go through all the   iterations or we hit that breakpoint we're gonna 
return. Now remember we said that the output of a   learning algorithm is a hypothesis and 
here we're returning theta and theta   naught and the idea being that this will 
describe our linear classifier hypothesis. Okay so let's just briefly say what's going on 
with these updates. Why not just add five to   everything, you know, why are we adding these 
particular values? In order to get at that   question, what we really want to do is we want 
to sort of look at, what was the thing that led   to the update, the update being this 
change in theta and theta naught?   Well the reason that we made this update is that 
this quantity was less than or equal to zero and   we said that roughly that expresses that somehow 
we're not classifying this point correctly,   you know, that we're guessing the wrong 
label for this point, and so let's see   if that changes when we make this update. So 
what we're going to do is we're going to look   at the same quantity but with the updated value 
of theta and the updated value of theta naught. Okay so all we're doing is we're saying we had the 
original value of theta and theta naught, we made   the update, and now we're going to look at the 
updated values. And so what I'm going to do here   is I'm going to substitute for this updated 
value of theta the actual updated value,   and for the updated value of theta naught, 
I'm going to substitute the actual updated   value. So I'm just taking those equations, you 
know, where we say we set theta to a new value,   we set theta naught to a new value, and 
I'm putting them in to this equation. Okay so now what's happening is I'm just 
rearranging this slightly. So let's go through   this term by term. So the first term here is this 
one, y^(i) * theta * x^(i) and that just goes down   here. The second term in that first equation 
is (y^(i))^2 * x^(i) transpose * x^(i), so that   just goes down here. The third term in the first 
equation is y^(i) * theta naught, so that just   goes down here, and the fourth and final term is 
(y^(i))^2, which is equivalent to (y^(i))^2 * 1.   Okay so this was just some algebra, we just 
sort of rearranged things in this equation.   Not too much happened, but why are we rearranging 
things this way? Well now you'll notice   that this equation takes the form of our original 
thing that we were comparing to zero. You know,   if we look at that if statement, this is the 
equation in that if statement, the thing that   we were saying: “hey, if this is less than zero, 
then we've made an incorrect classification and   we've added something to it.” So you can think of 
the second term as being the change, what changed   by changing this theta and this theta naught, what 
changed about this if statement. Okay well let's   simplify this second part a little bit. The first 
part we already know, it's sort of, you know,   what we were originally comparing to zero but 
what's going on in the second part? Well first,   we have (y^(i))^2. y^(i) is +1 or -1, and so 
actually we can just evaluate this, it's just one.   Whether I square +1 or -1, it's always going to 
be one. Okay now we have x^(i) transpose * x^(i),   that's just equal to the length of (x^(i))^2. Okay 
so let's put that down here. So again we just have   the thing that we were originally comparing 
to zero, that told us in the old theta and   the old theta naught if we were misclassifying, 
and we have this second term which is: x^(i),   its distance, its magnitude squared + 1. And the 
key thing to notice here is this second term is   strictly positive because (x^(i))^2, this 
magnitude squared has to be zero or above,   and then we're adding 1 to it, so we're adding 
this strictly positive term to our old value that   we were using in the if statement and remember in 
the if statement we're checking is this value less   than or equal to zero and so now we've added 
something strictly positive to it and by doing   that we think that we should be more likely to 
get something that this if statement does not   satisfy. In particular we should be more likely to 
get a correct classification because again this if   statement represents sort of did I get a correct 
or incorrect classification and so that's. This is   just sort of a way to think about why are we doing 
this update to theta, this update to theta naught,   it's sort of moving us in the direction of a more 
correct classification for this particular point. Okay so that's the if statement, that's 
what's going on there. We're saying again   if I’m making an incorrect classification, let's 
try to move to a more correct one and then let   me just briefly remind you of this “break”, and 
I think it's worth here verifying for yourself,   this is a question in the reading as well, but I 
just want to point it out. You want to verify for   yourself that putting in this break doesn't change 
anything about the output of this algorithm, that   if we got rid of this break, the output would be 
literally exactly the same, nothing would change.   The only purpose of this break is to 
save us some extra effort and not bother   evaluating if statements where we already know 
that we're not going to get it true for those if   statements and so you just want to make sure that 
you can convince yourself that that is the case. Okay so let's look at this and practice with 
a demo. Oh, there's a question before I go on.   One quick question is how do you decide on what's 
n and what's tau? Great okay, so how do I decide   on what's n and what's tau? So for n that's 
an easy one, that's just the size of my data   and so whatever training data I have, I just 
say how many data points there are and that's   n and there's sort of no decision involved, I'm 
just given n. Now of course, in reality, sometimes   you can go out and collect more data and then it's 
sort of a question of like cost benefit trade-off,   but for our purposes here, and honestly in a lot 
of this course, we're going to be thinking of n   as just being something that's given to us, in the 
sense that the training data is given to us. Now   tau is a trickier question, because in some sense, 
we don't know when we're going to hit the end of   this, although we're going to get some insights 
into that with our perceptron theorem. So in some   sense, the perceptron theorem will actually give 
us some really great insight into how to set tau,   but until then let me just say that one way 
you could think about tau is just being that   there's some point at which you don't want to 
keep going. Like suppose I had an algorithm   that could keep going forever and that's actually 
true of our previous algorithm. There is some   chance that it could just keep going ad infinitum 
and never reach, for instance, zero training error   and at some point I just have to stop. Right, 
like I have other things to do with my life,   I have other projects to go to, and so you 
can think of tau as representing how much   time you're willing to spend on this and sort 
of just making an upper bound. It's saying:   “okay at a certain point, I'm just 
going to cut off no matter where I am   and I'm going to stop and that's tau.” 
Now again, we're going to complicate that   question a little bit more, we're going to get 
a better answer to what is tau going forward,   but I think that's a perfectly reasonable 
way to think about it right now. Great. Okay cool and just a reminder to 
everybody if you have any questions,   adding them to the Discourse is the right thing 
to do. Great. Okay, so let's go to that demo. Okay so again, let's just talk through what we're 
seeing in this demo. So here, what's happening is   we have our exact same data set from before. This 
is actually the exact same data set that we ran   our previous learning algorithm on, and 
that'll become important in a moment.   Now remember that the way that the perceptron 
algorithm works is that it starts from   setting theta to zero and theta naught to 
zero, and that doesn't really mean anything,   but at the very first step we're going to pick out 
a point and we know for certain that we're going   to update theta and theta naught on that first 
step. And so here we've picked out a point, that's   that circled point with that black circle around 
it. We updated and now we have a hypothesis,   so that's what's in blue, this is our 
hypothesis and you can see that it   predicts plus on this side and minus on this 
side and so it's going to get all of these right,   it's going to get all of these points 
wrong. Okay so that's our hypothesis   on the step one of this learning algorithm. 
Now let's talk about what's going on down here   in the second plot. So this tells us the number 
of steps we've taken so far, that is to say the   number of times we've checked that if statement 
basically. So, so far we've only done it once.   The number of updates so far is the number 
of times that if statement was true and we   actually made a change to theta and theta 
naught. And so we did change it once so far,   we started from theta being 0 and theta naught 
being 0. We updated it, we made a change,   and now we have this hypothesis up here in 
blue. Now we can ask ourselves this is going   to be the training error on whatever the latest 
hypothesis is because that's the way that this   algorithm works, is it just keeps updating 
the hypothesis and that returns whatever is,   you know, out there at the end, and so that's 
what we're looking at. The training error here   is 0.38 and that's also plotted in blue as 
a blue dot down here. Now we just ran this   other learning algorithm and so let's plot its 
training error too, why not. Let's compare them.   So after that had taken one step, its training 
error was here in red, it was a little bit worse.   Okay, so now what we're going to do is we're 
going to take a new step of the perceptron. So   we're going to find a new data point and so I’m 
now turning our old classifier, old hypothesis,   to this dashed line because we're 
going to propose something new.   Potentially we're going to propose something 
new if this point is misclassified, right, and   so first let's ask ourselves: under the current 
hypothesis, what do you think about this point?   Is it misclassified? This is a 
great one to put in the chat. Great, lots of great answers here. Yes, 
misclassified, bad. This is all accurate.   It's points up here in the current hypothesis 
that'll be classified as plus, points down   here will be classified as minus, and so this 
point is misclassified, and so the perceptron   algorithm tells us we're going to make an update. 
Okay so let's go ahead and make that update,   and so we get a different linear classifier, and 
whoa this one went straight to the right answer.   So here we're seeing that after two steps and two 
updates the perceptron has zero training error.   Now we're going to look at another example in 
just a moment to see if this is typical behavior.   Spoiler alert: it doesn't always do this. But 
in this particular case that's what happened.   It said, “hey let's use this new data point 
and it's misclassification to find a better   classifier,” and so the training error is zero 
now. And so let's ask ourselves, here's another   point, let's just, you know, keep going with this 
algorithm. Here's another point. Is this point   misclassified according to the blue line, the 
blue classifier, which is our latest classifier?   Is this point now misclassified? Keep the answers 
coming in the private chat these are all great. Okay perfect, everybody's doing well. The answer 
is no, it is not misclassified, it is correctly   classified because this blue line is saying 
that everything over here is going to be a plus   and everything over here is going to be minus. 
In fact, it's correctly classifying every point   and so because this point is correctly 
classified, we're not going to make an update,   we're not going to go into that if statement. 
Okay the next point also is, of course,   going to be correctly classified because we 
said they all are, so we're not going to go   into that update, we're not going to go into that 
if statement and we're going to stay where we are.   And the next point is correctly classified, so 
we're not going to go into that update, we're not   going to go into that if statement, we're going 
to stay exactly where we are. And I think you're   hopefully seeing a pattern here that every one of 
these points is going to be correctly classified,   so when we evaluate that if statement, 
we're not going to go into it.   It's going to be just fine and we're not going to 
make any updates and so this is it, we've reached   the end. Basically this is the hypothesis that 
this perceptron algorithm is going to return.   Okay now this is one particular data set 
and so I think you know it's always hard   to read too much into one particular data 
set when you're thinking about an algorithm.   Let's try out another data set, even if it's 
a very similar data set we might get some   pretty different behavior and so 
that's what we're going to look at now. So we're just going to do the exact same 
thing in the sense of looking at a demo,   but it's a very slightly different data set. Now 
this is a very similar looking dataset, you know,   it's got a lot of the same sort of things 
going on in it and yet it's not going to be   exactly the same. So here let's again think about 
how the perceptron is going to go. By the way   without us looking I also ran that that other 
learning algorithm from the beginning of lecture   so we can compare training error down here. 
Okay so the first step in the perceptron again   is going to be: let's look at a data point 
and then let's find our first classifier.   So here we found this data point and so now we can 
plot our first classifier and it's going to say   everything over here should be classified minus 
and everything over here should be classified   plus, so it's got a training error in this case of 
0.43 and that's what we're plotting here in blue.   So now, that's our old classifier and 
we're going to update to a new classifier,   potentially by looking at this data point. 
So this data point is on the plus side and   it's already labeled plus and so actually when we 
state we're not going to make any updates here,   we're just going to stay where we are and we're 
going to keep the old classifier. Okay let's go to   the next data point. This data point is classified 
as minus, its actual true label is minus,   and it's being classified as minus by our 
existing classifier, and so again we're not   going to make an update, we're not going to do 
anything, we're just going to stay where we are.   This classifier, this point, 
is also correctly classified   and we're just going to keep going until we 
hit a point that isn't correctly classified.   So this point, also correctly 
classified, so nothing changes.   Now this point is incorrectly classified, it's 
on the minus side of the line, but it's a plus,   and so now we're actually going to make an 
update, the perceptron is going to make an update.   And so we get a new classifier that's in 
blue and now we start from that when we   play the same game: what about this point? 
It's correctly classified, so no update.   What about this point? It's 
correctly classified, so no update.   What about this point? It's correctly classified, 
so no update. Just keeps going until we hit   something that's incorrectly classified and of 
course as we go farther along, more things are   correctly classified and so we're not making too 
many updates, so let's let's skip ahead a little   bit, let's skip a few steps ahead, let's say 10 
steps. Now here's a really interesting thing. You'll notice the error went up. So we went 
from a training error that was lower to a   training error that was higher. That's a little 
weird, but it's a thing that can happen here   because we haven't specifically said that we 
have to have a decrease in training error.   Let's go a few more steps into the future. Okay it's still up where it was. How about a few 
more steps? And now we're down to training error   of 0 and you'll notice that this other learning 
algorithm that we tried out at the beginning   still isn't at a training error of 0, it's 
still something that's non-zero. It's up here,   but the perceptron, you know, around 
this iteration got to a training error 0,   and as we saw before, once it hits the training 
error of 0, it's done, it's not going to be making   any more updates, you know. It might check that 
if statement but that if statement will always,   you know, be false. It will not be having any 
incorrect things and so it won't be making any   changes. Okay so again I'll just pause for a 
second and see if there are any questions or   clarifications or things that might be helpful to 
cover about this demo we've been going through. I think you covered most of the questions for 
now, just yeah I guess a question could be:   “how does the perceptron geometrically move based 
off of a given misclassification?” Yeah, yeah, and   so okay. Let me, let me again use that question 
to come back to our slides for a second. So how   does it move when we have a misclassification? 
And so let's head back to our slides here. Oops let me share my slides and 
then head back to my slides. Okay. Okay so this is what we had in the slides as 
the perceptron algorithm and honestly I think   something that's useful, because you're going to 
be making your own perceptron algorithm in our   problems, is to be looking at this at the same 
time that you're looking at it running and seeing,   you know, what what exactly is happening here and 
making sure that everything makes sense there.   But what you'll see in this algorithm is that when 
we make roughly a misclassification, when that if   statement is true, then the update that we're 
going to make is exactly this theta update and   this theta naught update, and so what's happening 
there in some sense is that we're trying to get   the angles right between the thetas and the x's 
such that we're more likely to correctly classify   this particular data point. Right, so this is 
what we saw in this “what does an update do”   that we're making it so the left-hand side of the 
inequality in this if statement is more positive,   and so we're hoping that it'll be positive 
enough that it'll actually be positive,   that it'll be something where in the 
future when we hit this if statement,   it will be false and so we won't have to make 
an update because this will be correct. So we're   trying to move in that direction effectively. 
Now that being said, something we've already seen   is that it doesn't have to strictly decrease the 
error. We saw this in the example that, you know,   by moving in this direction we're hoping it's 
decreasing error, we're hoping that it's going   to make a better judgment call in this data point. 
But one, this particular data point doesn't tell   us about the other ones, we sort of have to deal 
with those in separate steps. But two, even in   this data point that doesn't tell us it has to 
get it correctly, it just sort of tells us it's   moving in that direction. And I think these are 
all things that we're thinking about, you know,   the perceptron algorithm we're seeing has benefits 
over the existing learning algorithm that we   talked about and we're going to see that even more 
concretely with a theorem, but it also might have   minuses and we might try to do even better and 
so you know you keep both of them in mind. Great. Okay so with that, unless there's anything 
else, I'm just going to go ahead and move on.   And so in particular, what we're going 
to do now is we're going to try to answer   some of these questions more concretely. So 
there's questions about how do we set tau,   you know, how do we know about how the perceptron 
is performing, and in order to answer those   questions, it'll help us to have a little bit of 
ability to talk about what makes sort of an easier   and a harder learning problem, a data set that's 
easier and harder to find a linear classifier for.   So let's talk a little bit about that, 
let's talk about classifier quality.   Now the first idea that we're going to talk about 
is something that came up implicitly in these data   sets that we were just looking at. You'll notice 
that they all had the setup such that you could   find a line where all the pluses were on one 
side and all the minuses were on the other side.   It seems to me that if you could find such a 
line if that exists, that that's probably an   easier data set to find a linear classifier that's 
good, rather than if that weren't the case. And so   let's give that a name. Let's say that a training 
set’s—this is going to be a property of a training   set—we're going to say that training set is 
linearly separable if there exists a line (so   a line is defined, our hyperplane is defined by 
theta and theta naught) such that for every point,   we have this equation. Well, what does this 
equation mean? Remember we just sort of talked   through this. If both of its components, both 
of its factors are positive, strictly positive,   that means that the actual label was plus and the 
actual guess, or the guess, was plus and so they   agree. Or another way to get the whole thing being 
positive is that they're both strictly negative:   that the actual label is minus and our guess 
was minus. And so basically this, you know,   equation is saying that we made a correct 
prediction and nothing was exactly on the line,   and so that's why we have this strict greater 
than rather than the greater than or equal to.   Okay, so let's look at an example. Here's 
an example, and so my question for you,   for the private chat: is this a linearly 
separable data set that I am showing right here? Getting lots of great answers here, 
really fantastic, keep them coming.   Okay great, you're all you're all looking 
fantastic here. Yes, it is a linearly separable   data set, and the way to show something 
is linearly separable is to just provide a   proof of concept. Here is a line that linearly 
separates the pluses and the minuses. Once I   have such a line, I have proved that this data 
set is linearly separable. Okay how about now,   this data set: is this data set, this new data 
set on the right, is it linearly separable? Great lots of fantastic answers here. I see some 
no’s, I see some nahs. These are all correct.   A no with a frowny face. Yes this is, this 
is not a linearly separable data set. You   can see that if I had a line that separated the 
pluses from the minuses, it would have to have,   for these two points (this plus on one 
side and the minus on the other side),   and there's just no line for which that's true, 
that doesn't also misclassify something else,   and so this is not going to be a linearly 
separable data set. And again something that   seems true about these is that somehow a linearly 
separable data set we think, you know, might be a   bit easier for a learning algorithm to deal with, 
and a not linearly separable data set might be a   little harder. Now in fact we can take this a bit 
further and just describe sort of a notion of like   how linearly separable our data is in particular, 
you know: how much of a gap is there between the   two sides of the data when it exists? And so 
that's what we're going to do on the next slide.   Okay so it's time for some more math facts. Some 
math facts, which hopefully you enjoyed last time,   and hopefully will enjoy this time. So the math 
facts that we're going to address today are:   suppose I have a point x in my feature space 
and suppose I have a line, a hyperplane,   in my feature space. I want to know what's 
the distance between that point and that line,   and in particular what's the signed 
distance? So the sign is going to come   from the fact that usually when we're dealing 
with hyperplanes, we're dealing with a direction.   Because remember we're thinking about classifiers, 
we're thinking about direction as defined by   theta. And so when that's true, what we want 
is effectively, if we just put our theta vector   from our line to our x, we want to know how long 
it's going to be from that line to the point x.   Okay and here's where we're going to use our math 
facts from last time. Now I'm going to do a little   derivation here. In some sense all you could do 
is just take the end of the derivation out and   use that for your own purposes and that's what 
we're going to do. But hopefully it's a little   bit illuminating to see where this comes from. 
So another way that we can write this distance   which I've now marked in green is: if just like 
before we put our theta vector at the origin   and now we're looking at the 
same distance in green over here.   Now one way to think about this signed distance in 
green is that it's the signed distance in purple   minus the signed distance in orange and we know 
these distances. In particular, the purple value   is exactly the projection of x* onto theta, 
which is something we talked about last time.   The orange value is the signed distance of the 
line to the origin which is also something that   we talked about last time. It's the projection of 
all of the points in that line on theta. Okay so   I'm just going to cite some results from last time 
to fill this in. The projection of x* onto theta   is exactly the scalar projection that again we 
talked about last time. So you can go back to   the math facts from lecture one if this is if this 
is feeling new and then the signed distance of the   line to the origin if you just play around with 
that b quantity from lecture one you can check   that this is going to be negative theta naught 
over the distance in theta, or the size of theta.   Now if we put these all together, we can say: 
what is the signed distance from a hyperplane to a   point? It's exactly this formula. Again you could 
just use this formula out in the end if you're   interested in the sign distance from a point to 
a line, but this is one way that you can think   about getting it that uses the scalar projection 
ideas that we talked about in the last lecture. Okay so now that we have this signed distance, 
again, what we want to do with this is we want   to say a notion of how big of a gap is there, 
somehow, in our data when we have this linear   separability. And so here, we're going to 
introduce another new concept, another new idea:   the margin of a labeled point with respect 
to a hyperplane. So a labeled point   is just a particular point, just a particular 
point in the feature space, but now with a label.   So let's say that that feature value is x* and its 
label is y* and our hyperplane is defined by theta   and theta naught and the margin is going to be the 
signed distance from the hyperplane to the point   but times the label y*. Okay, 
so why is this important?   Well this contains the information of this thing 
we've been talking about where we multiply y*   by theta transpose x plus theta naught. So if 
y* is positive and theta transpose x plus theta   naught is positive, our actual is positive and our 
guess is positive, and everything agrees and we're   getting our guess right. If y* is negative and 
theta transpose x plus theta naught is negative,   then our actual is negative and our guess is 
negative and so we're still getting our guess   right. We're still predicting right and so the 
sign of the margin is telling us “is our guess   right,” and the size of the margin is telling us 
the distance between the hyperplane and the point. Okay so let's look at an example. Here's a 
big data set. It seems like there's a big   gap between the pluses and the minuses and so in 
this case we think there should be a big margin,   but let's let's first look at 
the margin between a particular,   between a particular line and a particular point. 
So that's what we're looking at here. So this is a   particular line defined by theta and theta naught 
and let's look at a particular point. Here's   a point, it's just a point that I've circled. 
What's the margin? Well the point is labeled minus   and our prediction is minus and so the margin is 
going to be positive and this is the distance and   so it's going to be the positive value of the 
distance. Okay so now we still want to define   this notion of a gap in the data and so now let's 
talk about the margin of the whole training set   with respect to a hyperplane,not just one 
particular point, but the whole training set   all of the different points. And so what we're 
going to do for that is we're going to say:   “let's take all of the margins of the individual 
points and find which one is smallest.”   So what we're doing here is we're saying, 
for each point, we calculate its margin.   So that's from point one, point 
two, point three, up to point n,   because remember i indexes the points. And 
then we take the minimum value. So this is   different from the argmin. When we talked 
about the argmin we said: “which value of i   indexes the minimum value?” When we take the min, 
we're taking the actual value, the actual smallest   value. Okay so let's think about what's going on 
here. So if any of these points are misclassified,   any at all, then the margin for that particular 
point will be negative. Right? So the minimum,   then, over the whole data set must be negative. 
So if we get absolutely any point wrong at all,   the margin of the training set with respect 
to this hyperplane is going to be negative.   But, if absolutely every point is correctly 
classified, then the margin will be positive.   And so we can see that here, so if we look at 
this particular data set again over under the   “math facts”. On this example data set, in x_1 and 
x_2, you can see that for this particular line,   every data point is correctly classified: all the 
pluses are on the side in the direction of the   normal, all the minuses are on the other side, and 
so the margin is going to be a positive number,   and it's going to be the distance to the 
closest point. This looks like it's the   closest point to me, and so this is going 
to be our margin, the size of that distance.   Okay, so this is sort of a useful way to describe, 
again, sort of how difficult a problem is. So if   the margin is positive, we have this, you know, 
linear separability as shown by this particular   theta, theta naught, and we're able to say, sort 
of, how far away the points are from that line.   And so now that we have this ability to talk 
about margin and linear separability and things   like that, we can have a theorem about perceptron 
performance. So let me just maybe pause here for   a second to see if there's anything we should 
discuss and then I'll go on to this theorem.   Yeah just a quick question about the perceptron 
update: why do we go through the data points   one at a time and not necessarily in 
batches, for example? Yeah that's an   interesting idea. So there's two ways that you 
can answer this question. So one is, you know,   that you would have a different algorithm if 
you went through the data points in batches.   Which isn't to say that's a bad idea, it's just 
to say that it's not this particular algorithm,   and so one way to notice that is to notice if you 
make an update on a particular point, then the   next point will clearly depend on having made that 
update, and so if you did something with batches,   you would change the sequential nature of those 
updates. So again one way to answer this question   is just that if I used a batched version, it would 
just be a different algorithm than the perceptron.   Another answer is why don't we do this, 
because maybe we think it's better.   And I think that that's an interesting question 
that I'm not going to answer right now, but I'm   hoping that we're developing the tools to help you 
answer, to start thinking about how would I know   that an algorithm is better? How would I would 
evaluate that it's better and so you can try out   different algorithms and maybe they are better, 
maybe they are things that you would, you know,   like to do in general. I think I see a raised 
hand again. I want to encourage people to write   all of your questions in the Discourse, and then 
some of them will make it back here. Great thanks.   Okay another question. Oh yes, please. All right, 
another question is: why do we look at the signed   distance from the hyperplane to x instead of 
the other way around? Oh you can do distance in,   you know, either way. I think what really 
matters in all of this is that we want to ask:   are we correctly classifying points? 
So this notion of the margin,   and then how far away are those correctly 
classified points? And so I think it's not   so important, you know, what we choose as the 
directional. Although note that you would then   have to, if you, change sort of the sign in 
one direction, you'd have to think about what y   matches with that. So one thing that's nice about 
this is the y being positive matches with theta   transpose x* plus theta naught being positive. So 
the guess being positive matches with the actual   being positive when those two are positive, that 
we're correctly classifying things. So we want   those to agree for everything that we're setting 
up here, but you could imagine just flipping both   signs simultaneously, you just want to make sure 
that they agree is the most important part. Cool. Okay great so let's go on to this next point 
here. Now let's care about trying to figure out   if we can say something about perceptron 
performance, something a bit more concrete,   and we would hope that because we're, 
you know, we're making these steps and   these hopefully good directions, can we, 
you know, say maybe how many steps is it   going to take? And this is what this theorem 
is going to to point us in the direction of. Okay so there are going to be some assumptions in 
this theorem and I think it's always worth asking,   you know, are these reasonable assumptions? The 
first assumption seems pretty strong on the face   of it. The first assumption is that our hypothesis 
class is not just all of the linear classifiers,   it's all going to be all the linear classifiers 
with separating hyperplanes that pass through   the origin. So we're only considering linear 
classifiers, that's what we've been doing so far,   that's fine. But now, we're only considering the 
linear classifiers where theta naught equals zero,   and so, for instance, you know, here 
are some examples. Here's an example,   here's an example, here's an example. These 
are all examples that pass to the origin   but that's more limiting. I mean there are 
some data sets that could be separated by a   linear classifier that could not be separated by 
a linear classifier passing through the origin,   and so on the face of it, it seems like this is 
a pretty strong assumption. Although, you know,   if you've checked out the reading, and we'll talk 
about this, you know, momentarily, it turns out   this is not actually a very strong assumption 
and that we can always reduce to this case and   so this is actually okay, but I think you want to 
be convinced of that. Okay, B: what is assumption   B? We're assuming that there exists a theta* and 
a gamma. So theta* because we're only looking at   linear classifiers for which theta naught is 
zero, theta star completely defines our linear   classifier. Now so we're essentially saying there 
exists some hyperplane, some directed hyperplane,   and there exists some gamma greater than 
zero such that, for every data point, we have   that this quantity is greater than gamma. So 
hopefully you remember from the previous slide   that this quantity is exactly the margin. 
The reason that it's changed slightly,   the only thing that's changed from the previous 
slide, is that theta naught equals to zero,   but otherwise this is the margin 
between a hyperplane and a data point.   And we're saying that that margin is always 
greater than gamma and in particular this   tells us that the margin of the whole data set 
if we look over all the different data points   is greater than gamma. So what we're saying here 
is that there must be a line such that the data   is linearly separable, everything is correctly 
classified, and there's some sufficient gap in it,   that's that gamma. So for instance, 
if we look at this data set down here,   here is a line. I have found a 
line, it is given by my theta*,   and here is a gamma. So that's all I have to 
provide. It doesn't have to be the best line,   it doesn't have to be the best gamma, but 
I have to find a line and a positive gamma   such that this assumption is satisfied and 
I have done that here for this data set.   Okay our third assumption: there exists some 
R—so this is going to be another real value—there   exists some R such that, for every data index i, 
essentially the magnitude of the data point is   less than or equal to R. So again, it doesn't have 
to be the best R, it just has to be some R. So   here I put all of my data in a ball of radius R, 
and here's my R, so I found some R for this data. So somehow this is just telling me how big does, 
how far does the data get out from the origin.   Okay so once I've made these assumptions, so 
namely I'm only looking at hypotheses that are   linear classifiers to the origin, I'm looking at 
data that can be separated with a non-zero gap in   between data points, and I'm looking at data that 
all fits into a ball somehow. Then I can conclude   that there is a maximum number of steps, of 
updates, that the perceptron algorithm will make.   It will make it most (R/gamma)^2 updates 
to theta. Now remember there's a difference   between steps and updates. So the way we were 
talking about steps before it was like every   time we visited that if statement we're looking 
at a step. An update is when we actually go into   the if statement and change our theta and in 
this case, theta, because it's only, you know,   through the origin, but in general theta and theta 
naught. And once it goes through a pass of all   the data points without any changes, the training 
error of the final hypothesis is going to be zero. Okay so here's something that's pretty cool about 
this, is it's telling us some notion of a maximum   number of updates that we have to make. So 
we know that there is a bound on the number   of updates if we can check these things, if we 
can check this margin, if we can check this R,   and so that's kind of exciting. That kind of tells 
us, you know, “hey, there's only so far we can go,   you know.” There's really this limit as opposed 
to that earlier learning algorithm with the random   hypotheses or whatever hypotheses our friend gave 
us in which case, like you know, we don't really   know that we would ever hit training error zero 
even when we can, even when things are really   linearly separable, and we should be able to. Now, 
in general, it's worth noting that if I set—then   this comes back to the question of the number 
of iterations—if I set my number of iterations   such that I can't make all of these updates, if I 
said it's very small, I am not guaranteed that my   training error will be zero. There's a really easy 
version of this: set your number of iterations   to zero or one and you know, in general, you 
certainly can't guarantee that your training error   will be zero at that point. So this does tell you 
some sense, you know, how that you're gonna have   to have a sufficient number of iterations 
in order to be able to get to that point. Okay so this is cool. We have we have a 
theorem about perceptron performance and   again I'll just pause for just a second to see 
if there are any questions about this theorem. Or anything so far. I don't think so. Cool, great, awesome. Okay so 
now what I'd like to talk about is, you know,   whether this theorem is everything, in some sense, 
and before I get to that actually, I should talk   about why classifiers through the origin. So let 
me backtrack just a second. So remember this first   assumption was that our hypothesis class were 
classifiers through the origin and an important   point about this is, again, these are just not all 
the classifiers. I think you can easily imagine   data sets that are linearly separable—in fact, I 
think we've seen them so far in this class—but are   not linearly separable with classifiers through 
the origin. You know, they have pluses on one   side and minuses on one side but that's not true 
strictly with a classifier through the origin.   If that's not immediately clear, definitely 
meditate on it, maybe draw yourself a picture,   but that's certainly true. Now that being said, 
what I'm about to go through is a derivation,   a quick derivation, that you can always reduce 
to the case of classifiers to the origin. You   can always reframe your problem such that 
you can look at classifiers to the origin,   that's sort of a different observation. So we're 
not saying that everything is linearly separable   through the origin but we're saying that there 
exists another space in which it is, but we have   to go to that other space first. Okay so why is 
it okay to look at classifiers through the origin?   Well the observation here is that if we're 
clever, if we do work in a different space,   we don't lose any flexibility. Okay so let's 
think about this. So first let's think about   a classifier with an offset. So remember 
we're thinking of theta as our normal vector   for our hyperplane and theta naught as our offset 
and so in general this is the setup that we have   for our linear classifiers: we have, we look 
at all of the x's in some feature space R^d,   we look at all the thetas (they have to have the 
same dimension as we talked about really early   in today's lecture so that we can take this 
dot product, so that it even makes sense to   take this dot product), and then theta naught 
is going to be a scalar. And we said that our   line that defines the linear classifier 
is defined by, it's the set of x,   that satisfy the equation theta transpose x plus 
theta naught equals zero. So this is sort of how   we defined a line, we talked about this in lecture 
one, and we talked about what this looks like.   Okay so this is our classifier, it's defined 
by this line and the normal vector theta. And now we want to say is there some 
way, some space in which we could work   that would be equivalent that would 
involve classifiers without offsets?   And so the answer is yes and so let's let's 
just double check what that would look like.   So basically we're going to work in a new 
space, let's call it new and so for each   x in our original space, the one that we sort 
of really care about, the one where, you know,   maybe we actually measured things about newborns 
like, you know, how much oxygen they're getting   and how much movement they're making and so on, 
we're gonna make a new space and we're gonna   change the original space very simply. We're 
just gonna take our x_new here, we're going to   take our original data point x_1, x_2, up to x_d 
and we're just going to append “1”. That's it,   it's always going to be one. Every data point 
is going to end with 1 at the end. So that's   sort of our expanded feature vector, it's 
like every data point got a new feature and   that feature was 1. That seems like incredibly 
simple, like there's not too much going on here,   and that's kind of true, but it's still a really 
nice and clever trick here. And so now we're   gonna have a new parameter vector, theta_new, 
and what theta_new is gonna be is it's going   to be our original theta (theta_1, theta_2, up to 
theta_d) and then we add a theta_0 on at the end,   and the thing to notice is that if you 
take the transpose of theta_new with x_new,   you get back exactly theta transpose x plus 
theta naught. And so what I'm saying here   is let's look at the set of x's in the new 
space, x_new, and let's look at their first d   coordinates. So that's what I mean by 1:d. 
I mean the first d coordinates of this x. I think these should all have transposes to be 
column vectors, but modulo that small thing,   then then I think we're going right here. Okay 
so now, what I want to do is I want to look at   the set of x_new and their first d coordinates 
such that x transpose new times x new equals 0.   That's going to be exactly the same set of 
x that satisfy the first linear equation   and so we can get out basically that 
set of x from this expanded space.   And so something to notice, remember, is that 
it's not just a line that defines a classifier,   it's the two sides of the line, you know, it tells 
us do we when do we classify plus and what do we   classify minus, and so it's also true that the 
set of x that are less than zero in the first case   or the set of x_new 1 through d that satisfy less 
than zero in the second case same thing if we look   at greater than zero and so we're really going 
to get the same labels, the same plus and minus   and all of that. And so the proposal here is that 
if, let's suppose, we want to apply our perceptron   theorem which we've just developed, well first 
you can convert to this expanded feature space   and so if something was linearly separable in 
the original feature space, it's going to be   linearly separable in the expanded feature space, 
but now this expanded feature space we're really   just looking at classifiers without offsets and 
so it's not only going to be linear separable,   it's going to be linearly separable with 
classifiers without offsets, so that's something   sort of stronger, but it will be in this expanded 
feature space and then we can apply the theorem.   And so this is all to say that we didn't, that 
that first assumption wasn't so strong after all,   that really we can still apply things, 
we can still do all this in sort of,   you know, the full linear space. We just have 
to use this trick of using an expanded feature   space which will turn out to be an extremely 
important trick that we're going to spend a lot   more time on in more generality, but I want to 
mention it in this sort of simple version here. Okay so now we know about the perceptron theorem,   and now I want to talk about 
limitations of the perceptron theorem. So here's a big problem: a lot of real data is not 
linearly separable. So we've been talking about,   you know, here's what you can do and here's 
what you can say about linearly separable data,   but what if your data isn't linearly separable? So 
here is a real data set about penguins, which is   adorable. So here are three penguin species, these 
are penguin species. I'm not a penguin expert so   I’m, you know, borrowing this graph from the 
link at the bottom, but something you'll notice   is that individuals have measured the flipper 
length of individual penguins (so that's on the   the horizontal axis) and body mass of the 
individual penguins (that's on the vertical axis)   and then what you're seeing is that for each 
flipper length and body mass—so that's like our   x—that you get a particular type of penguin 
plotted as a color, it's like a label,   and so even if we were doing two class 
classification, just looking at two classes   at a time, like maybe we're just comparing the 
chin strap of the gentoo penguins or you know   any two penguins, you'll see that no class two 
classes of penguins are linearly separable here,   there's always some overlap in all of these 
data sets. And so if we wanted to classify this   it seems like we can't apply our theorem because 
things aren't linearly separable. And so in   fact let's look at what happens to our, to our 
perceptron when things aren't linearly separable. And again we'll just do that. With our example. Okay, so here we're looking at 
a data set, just like before,   but now you can immediately see this data set 
is not linearly separable. Hopefully you can   immediately see that. So we have our pluses, we 
have our minuses, and they're jumbled together,   and yet, you kind of have a sense that you could 
get a reasonably good classifier, like something   that doesn't misclassify too many points, like the 
pluses tend to be on one side, the minuses tend   to be on the other side, and so we'd still like to 
find a good classifier for this. Now in this case,   I'm not going to bother to calculate too many of 
the random training algorithm, although I think   you can think about how that might perform, 
but I am going to focus on the perceptron.   So here the perceptron’s still going to do 
the same thing as usual: on the first point   it's going to just make an update, no matter 
what. The first time it always makes an update   and we get a classifier, and now each 
time it goes to a point and it says   “is there a mistake?” In this case there 
is and so it's going to make an update   and it's going to go to a new point, it's 
going to say “is this incorrect for the   current classifier?” In this case, it's actually 
correct for the current classifier, so it won't   make an update and it'll just keep going forward 
and so now let's go a bunch of steps forward. So I've gone a total of 352 steps forward, 
so we've made 352 steps. It's a lot of steps,   we'd like to think that we're going to get to 
a pretty good classifier after that many steps   and you can see that the error which started 
around, maybe you know, 0.4 or 0.3 or so on   actually went up and then it went down, but 
then it went up again, and now we're actually   at a pretty high error, 0.41, and it seems like 
we're actually really struggling on this data set,   you know, that we're not doing 
too well. We think that, you know,   we could do better than an error of 0.41. Yes 
some of the points are kind of interspersed   but it still seems to me like we should be 
able to do better and also, in particular,   that like maybe we should at least choose the best 
of the classifiers we've tried so far instead of   just this last one which just happened to be not 
that great. And so something that we're seeing   here is this can be a problem with something like 
the perceptron: that if I don't have linearly   separable data, maybe I need to ask myself, “can 
I do better? Are there better things that I could   do?” Okay, before I step forward, I'll just check 
briefly if there are any questions about this. It's cool, great okay. Let's 
go back to the slides then. Okay so we've seen that the perceptron 
has certain advantages over, you know,   this not so great learning algorithm that we 
started with, but it has disadvantages too.   It seems like maybe it struggles with this not 
linearly separable data that we do care about   and so we're going to ask what can we do? 
And in some sense that is a question that   you will be asking and answering on your 
upcoming problems, on our next few lectures,   and so this is, if anything, the cliffhanger to 
get us to be thinking about this, to be saying,   in some sense, you know, you always want 
to be asking, you know, why do we have any   more lectures? Why don't we just stop at 
this lecture? Why isn't everything solved?   And here we're seeing, you know, that we've 
stepped in some interesting directions where   we're able to make some some better choices than 
we did before, like maybe update our classifiers,   our hypotheses adaptively, and yet maybe 
there are yet better choices that we can   make. Maybe there are things that we could do 
even better and we're starting to see that here. Okay so before I go, before I finish up for today, 
I just want to kind of situate where we are in a   sort of pantheon of machine learning tasks. So 
something that we've been focusing on so far is   binary or two class classification. That's where 
we learn this mapping, this hypothesis that takes   us from our features to a set of labels which 
are—there's just two of them, it's minus and plus.   Now we've been looking at the example of linear 
classification but that's certainly not the   only example that we might look at. I mean you 
can imagine data sets. So here, I'm imagining,   you know, maybe my features are only one 
dimensional but it seems like a linear classifier   isn't enough here, I'd like to do more than that. 
So linear classification is an example of binary   or two-class classification, but it's not the 
only type of binary or two-class classification.   Likewise, we just saw an example of multi-class 
classification. This is where I have more than two   label values. In the example we just saw, there 
were more than two types of penguins. We'd like   to probably classify all the different types of 
penguins, and so I might be interested in three or   more label values. Now these are both examples of 
classification. So in classification, I'm learning   a mapping to a discrete set that those labels, 
you know, the different types of penguins or,   you know, whether a newborn is having a seizure 
or not, but sometimes I don't want my mapping   to just be a discrete set. Sometimes I want it to 
be something continuous and that's where we look   at something like regression. So here I want to 
learn a mapping to continuous values like maybe,   you know, how long my car is going to last, 
that's a continuous value, it's a length of   time. You know, maybe I want to predict the 
temperature outside, that's a continuous value.   There are a lot of things that I might want to 
predict that aren't simply a discrete set and   that would be a regression. Now both regression 
and classification are examples where we have   labeled training data: we're learning a mapping 
from a set of features to a set of labels,   those labels could be discrete, they could be 
a set of penguins, different types of penguins,   or they could be continuous, but they're still 
labels. We're still taking each feature x and   labeling it with some y but that's not the only 
type of learning that we might be interested in   doing, there's also unsupervised learning. In 
this case, there are no labels and we're just   trying to find patterns. So if you look 
back to the examples from lecture one,   we actually talked about an example of 
unsupervised learning. This was in the   Reuters analysis: they took a bunch of text 
documents, these were these petitions before the   court and they said “oh what are the topics in 
these documents?” And there were no labels there,   they just wanted to find out what are words that 
goes together, what are what are themes in the   documents? They don't tell them what are the 
themes to begin with and so that would be an   example of unsupervised learning. So this is all 
to say we're focusing on classification right now,   we're focusing on two-class classification, we're 
focusing on linear classification, but that's part   of a broader picture in machine learning and 
hopefully we'll spend some time on all of these   other ideas, but first let's really, you know, 
nail classification and figure out what's going   there. Okay great, so that's the end of lecture 
two today and we look forward to seeing you in   labs and office hours and everything else and 
I’ll see you for the next lecture next Tuesday. 

Okay good morning and welcome to lecture three. 
So just a reminder as usual, make sure to   put all of your questions up at the discourse, we 
have our “lecture 3” category there for today and   so you can start that just immediately. So 
recall that in the past couple of lectures,   in addition to the labs and everything else you've 
been working on, we've been covering things like   linear classifiers, understanding what is a linear 
classifier, and some algorithms to come up with   linear classifiers. So in particular, at this 
point, we now have the perceptron algorithm. We   saw that when our data was linearly separable, 
there were super cool things that we could do   with the perceptron algorithm. In fact, we had 
this theorem that gave us a bound on, you know,   how many mistakes or sort of how many updates we 
would make before we were done and we found this   sort of perfect linear separator. And so today 
now that we have, sort of, some algorithms and   the background to be able to talk about it, we're 
gonna put together a sort of more complete machine   learning analysis. So we're going to think about 
starting from the very beginning, gathering data,   and going to interpretation and evaluation and 
things like that and so as part of that we're   going to talk about choosing good features. 
Now this won't be an absolutely complete   discussion—so I say more complete rather than 
fully complete—but I think we're going to be going   in the right direction towards, you know, you 
really being able to have a collection of data and   run your machine learning algorithm on that 
data. So we'll talk about all of that today. Okay so let's just briefly 
recap. So remember as we said,   we've been talking about these linear 
classifiers, we've been calling them   h, for instance, and so in general we have 
some features, maybe x_1, x_2, x_3, x_4.   Remember that we're only drawing things in 
two dimensions because of our fundamental   limitations and vision as humans, the reality 
is our feature space is definitely, in general,   going to be higher than two-dimensional and we'll 
even talk about that today but our visualization   here is in two dimensions. And so in general we've 
been seeing that we find these linear classifiers,   they're defined by a theta which is giving 
us the normal to the hyperplane, that's doing   the classification. It tells us on what side we 
predict minus and on what side we predict a plus.   Sort of on one side, we predict minus plus one and 
on one side we predict -1, and we have our theta   naught which is our offset and again in general 
this will be a hyperplane not just a line. Now   when we want to ask, you know, how well is this 
linear classifier doing on some data, we have to   talk about some kind of loss and classification. 
We've been focusing on the 0-1 loss although,   again, that's not the only loss that you would use 
in classification, we talked about the asymmetric   loss. Here we're saying that if our guess is 
equal to the actual value then we get a loss of 0,   which is the best, and if we're wrong then we get 
a loss of one. And so we talked about how we can   look at training error which tells us sort of the 
average loss over our training data points as one   way to look at how well our classifier is doing 
and actually it's something we've been using in   our classification algorithm. Okay so this is 
sort of, you know, a bunch of things that we   have in our toolkit at this point and so let's ask 
ourselves: what would a machine learning analysis   look like at this point? Could you run a machine 
learning analysis on some data? Well first you   kind of have to establish a goal and find some 
data and for that matter check that that goal   is in fact something that you could accomplish 
with your machine learning algorithm. So we'll see   an example of that in a moment: we're gonna have 
sort of a cartoon where we consider diagnosing   whether people have heart disease based on some 
available information about those people. I will   emphasize this is very much a cartoon: if you 
really care about heart disease then I encourage   you to check out like a paper about heart disease, 
there's actually some cool data sets out there   that you can play with. But I still think this 
will get us a lot of insight into this process.   Okay, so once we have some data, it will often be 
the case as we're going to see that you can't just   immediately apply the machine learning algorithms 
that we've established, that you have to do some   transformations of that data to put it into 
a useful form, and so we're going to see that   process, a lot of that process, today. Once you've 
done that, now you can run that machine learning   algorithm. So it could be, for instance, one 
of the ones that we've talked about before:   we talked about choosing the best classifier 
from some finite list of classifiers,   we've talked about using the perceptron. Now you 
in your explorations and homeworks and labs and   such have also encountered the average perceptron, 
so you have a lot of machine learning algorithms   in your toolkit and you're going to have more 
coming up. We're going to see that there are,   you know, even cooler things that we can do, 
but you're going to run some machine learning   algorithm at this point and return a classifier 
and an important thing that we want to do is also   interpret and evaluate what you get out at the 
end, so we'll talk about that a little bit today   as well. And it's also something that you've been 
exploring again in labs and other exercises. Okay   so let's launch into this analysis again on this 
example that we're going for with heart disease.   So in particular, we need to start off with our 
goal and our data and and it's worth noting that   these are very intertwined in some way, you 
know, typically you'd want to gather data with   some question in mind being able to, you know, 
answer that question but also gathering data is   very difficult and expensive. Probably that's most 
of the work that goes into, you know, any of these   steps, so you might find that you have some data 
that's available to you and you can ask a question   of that data. So to make this concrete, 
again we're going to talk about diagnosing   whether people have heart disease based on some 
information available about those individuals   and, you know, let's imagine that thankfully 
there's actually a bunch of data already available   to us. So somebody has measured, you know, do a 
bunch of individuals have heart disease? So here   we're looking at four, data about four 
individuals, but we imagine that our data   set is actually much bigger and it's just hard 
for me to fit on a slide, so we're going to   look at the first four people. Okay and this 
person has also measured or this, you know,   group of people has measured the resting heart 
rate of these individuals in beats per minute,   they've measured if they've been experiencing 
pain in some time period, maybe the last month.   Maybe we're looking at individuals who work at 
a hospital and so we could ask what's their job   within the hospital, maybe that has some bearing 
on this diagnosis, we can look at what medicines   they're taking, we can look at their age, 
maybe just roughly which decade they're in,   and maybe the income of their entire family, not 
just their own personal income. Okay so let's   say we've gathered this information and we would 
like to predict do these people have heart disease   and so, you know, well one: we want to check 
is this something we could even turn into a   two-class classification problem, right? And so 
this seems like something that has two labels:   people have a heart, have heart disease, 
or they do not, so that that sounds good.   We need labels: well we seem to have them, it 
seems like somebody has labeled this data at   least for us. We need features: that's going to be 
basically everything except for the labels here.   We need training data: basically labeled data with 
features. That's what we have here. That's great   and we'd eventually like to apply that 
to new predictions. So on the face of it,   it looks like this is something that we should be 
able to apply the learning algorithms that we've   developed, these two class classification learning 
algorithms, and now let's ask ourselves: can we   just put this into our learning algorithms? Well, 
not quite yet, right. So we need to identify a y,   we need to identify an x, and that x is something 
we need to be able to take dot products with.   And so you'll notice that some of the entries here 
are strings, like they're not even numbers, and so   we couldn't possibly take a dot product yet, and 
that's what our algorithms so far have required,   and so let's ask ourselves, you know, can we 
break this down? Can we put it into a form that we   actually could even apply our learning algorithms? 
Okay, so we'll notice again that there's already   this breakdown into labels and features, but 
let's make that explicit. And so that'll be our   first order of business, let's first isolate our 
labels and make sure that we can put those into   our algorithm. Okay so we want to encode this data 
in a usable form by the algorithms that we have,   and we've sort of already talked about this 
with labels, but let's just make this explicit.   We want to identify them and encode them as 
real numbers and so, in particular, the way   that we've been doing that so far as we've been 
saying: we'd like to turn this into +1s and -1s,   and so we might say that, let's say here, that 
yes is going to be +1 and no is going to be -1   and so we can turn this set of labels now into +1s 
and -1s. And so in particular here, the label for   our first data point which we've been calling 
y^(1) will be -1 in this particular case. Now   a couple of notes about this: one there's no 
reason that you have to use +1 and -1. In fact,   we're gonna see pretty soon some algorithms 
that actually use a different set of two labels:   0 and 1. The reason that we've chosen +1 and -1 so 
far is that it's been very convenient, it's been   a really convenient choice of two labels, but 
we're gonna see that, again, that can change,   but for today we're gonna go, we're gonna keep 
going, with +1 and -1 because that works for the   algorithms that we have so far. And two, you 
definitely wanna save this mapping, right. So   you have chosen that +1 represents yes here and -1 
represents no, and so you wanna make sure to save   that somehow, so that when you get out predictions 
in the future and you say: “ah this person is -1”,   well does that mean no or yes? So definitely make 
sure to save that mapping, so you can go back   and find out, you know, what was the actual 
string that that corresponded to? Okay so   now we've got our labels, we've got our y's and 
so we want to go back and get those features.   Okay so now, let's do the same thing but 
we're going to identify the features rather   than the labels and encode them as real 
numbers. Now just to be explicit here,   a feature just means any function of the data and 
typically if we're doing something like supervised   learning, which we are in this case (remember 
classification is a form of supervised learning),   in that case, we typically exclude the labels 
from that. So it's a function of the data,   except for the labels, and I mean that as there 
is a bunch of data that is not the labels and it   can be any function of what that is. In fact, it 
can just be the data itself, that's a perfectly   fine set of features: that would just be the 
identity mapping from the data to the data, but   we're going to look at some other functions today. 
And in particular today, what we're going to do,   is we're going to start with some data, let's 
call it x. You could think of those as our old   features, sort of the original features. In some 
sense, like it's worth noticing that those were   feature engineered to begin with, somebody 
chose what to measure about these patients,   somebody chose what exactly was going to be in 
this table, and so it's not like, you know, those   are pure and perfect in every way, like those 
were choices as well, but we're going to make some   further choices and turn them into new features: 
let's call them phi and sometimes in this   lecture I'll be sort of, just for convenience, 
not writing phi as a function of x, I'll just   write phi, but it's worth noticing that we're just 
getting these new features from our old features.   Okay so let's look at our data and see why 
are we even bothering to do this? Why don't   we just keep our data exactly as it is? Okay and 
one thing I just want to note before we go on   is that I'm slightly changing the direction 
of what we're looking at. So typically,   we've been thinking of each data point as 
a column vector and so when you look at a   data point here it's like I've transposed that, 
so just in the notation that we've been using   for this class and the setup we've been using 
for this class, notice that a data point here   I'm showing as a row even though we've 
been writing it as a column vector.   Okay, so let's look at this first dimension: our 
first of our old features, the resting heart rate.   Well, that's already a number, that's something 
that we could take things like dot products with   and so maybe we don't have to worry about this 
one at least for the moment. Let's go to another   feature, you know, have these individuals been 
experiencing pain, and notice: hey this is not   a number, we can't take a dot product with the 
string, “no” or “yes”, and so we have to turn this   into some kind of number. Well in this case, we 
kind of already have an idea of how we might do   this based on the labels. Remember with the labels 
we just turn things into +1 or -1, or 0 or 1,   and we can do the same thing here. So let's maybe 
make the no's into 0s and the yeses into 1s.   Great and then of course again remember 
we definitely need to remember the mapping   back if we want to interpret this in the end. 
Okay, so now we're getting into something a little   bit trickier: let's look at jobs. So with jobs 
there are a bunch of different jobs, you know,   and we're imagining here for the moment that 
these are jobs within the hospital, so we might   be looking at things like nurse, administrative 
assistant, doctor, pharmacist and so on.   So we're gonna explore some ideas for how we might 
turn this into numbers and I just want to note   a general principle of our lecture is that when I 
put out an idea it's not necessarily a good idea.   You know, sometimes we're going to put 
out ideas and we're going to ask ourselves   about whether they're good ideas. I mean I 
think in reality when you go out into the world,   you're going to have some ideas, some of them 
will be good, some of them will be bad, but   more important is your ability to evaluate those 
ideas and so let's put out some ideas right now.   So here's an idea: let's turn each category 
into a number, that seems really natural right.   So I'm going to say that nurse is 1, doctor is 2, 
administrative assistant is 3, pharmacist is 4,   and social worker is 5, and let's say those 
are, you know, the set of jobs that we see.   And now, just for illustrative purposes let's 
imagine that, you know, let's just plot our   data points with both occupation and resting 
heart rate. And so maybe we find in general that   people with a higher resting heart rate, maybe we 
see, you tend to see, them as having heart disease   and people with a lower resting heart rate 
maybe we don't see as much heart disease.   And we see that, you know, things really maybe do 
change by occupation and we'd like to pick that up   with our classifier, that you have some predictive 
power in occupation, that it can help you predict.   Okay well here's an issue: if I use a linear 
classifier, it's pretty clear that the ordering   matters, you know, so I'm assuming pretty 
explicitly here that nurse has a lower chance   or a higher cut off for having heart disease than 
doctor does, and doctor has a higher cutoff for   having heart disease than admin does, and admin 
has a higher cutoff for having heart disease than   a pharmacist does. Or alternatively, the other 
way around, but that it's monotonic, that it's   either all increasing across these numbers or 
all decreasing across these numbers and going in   I just don't think there's really an ordering on 
nurse, doctor, admin, pharmacist, social worker.   I don't think that that has some particular linear 
relationship that I want to codify and I'm forcing   them to have that by making this codification into 
the natural numbers. An extreme version of this   is, suppose that in fact my classification really 
just came down to which occupation people have:   it turns out that some occupations, you know, 
are just super predictive of heart disease   and some are not predictive and I would like to 
codify that and here I can only do a breakpoint   between two numbers that are next to each other 
and so I'm allowed to group nurse, doctor,   and admin but I'm not allowed to group nurse 
and admin without doctor. And because these are   categories that don't inherently have an ordering, 
that's just not something that I want to do.   Okay I think I have a question: what was, can I 
take that now? Yeah a question about mapping the   data to features: does this mapping have to be 
invertible? Yeah great question. So it doesn't   have to be invertible, you could make some 
function of your data that you plan to learn   on and in some sense, you see that in the original 
data. Right so let's say that I, you know, I think   about these individuals. Clearly I am not putting 
all of their information into my data, like   there's some information about them, it could even 
be super important information for this problem,   that I'm not encoding in the original data, you 
know, it could be the amount of plaque in their   heart that would be really great probably if I'm 
looking at, you know, some kind of heart disease   or some kind of issue, but the reality is that's 
very expensive or difficult or impossible to   measure and so that's a part of that individual's 
data in some sense, that's information about them,   but it's not encoded in the features. Now, 
that being said, it's important to understand   the encoding that you're using so that you can 
interpret it, so when you have something like a   one-to-one mapping, it's really easy to do that 
interpretation because you can go back and say:   oh what string did this mean, what did this mean? 
If you don't do a one-to-one mapping, if you do   some kind of function that says, oh all of these 
values get the same value, you just want to know   what were all of the input values that gave you 
that output value so you can interpret it later. Okay cool, so at this point we understand that 
we probably don't want to turn our categories   into unique natural numbers. It seems 
like that's basically a bad idea when   there's no ordering because we're basically 
implicitly assuming that this order matters   and so let's think about what are other 
things that we could do in this case. Why don't we turn each category into a binary 
number? Again, let's just throw out some ideas   it's a, you know, let's brainstorm. Okay well 
first of all let's think about what do we mean   by turning each category into a binary number? I 
mean in some sense, you know, we can just encode   every natural number as a binary number so 
there's no difference in what we just did.   I think what we really mean here is that we want 
to introduce actually, instead of one new feature,   three new features, and these three new features 
essentially end up being binary. So we'll say that   a nurse has encoding 0, 0, 0. Let's just call our 
new features phi_d, phi_(d +1) and phi_(d + 2).   So the nurse will be 0 in all of them, the admin 
will be 0, 0, 1, the pharmacist will be 0, 1, 0,   and so on. And so let's think about, you 
know, what happens when we use this encoding?   Well let's again, let's try plotting. I think 
it's just one of the best things you can possibly   do with your data and so let's do that here. So 
what are we looking at here? We're seeing that,   we're looking in particular, the two end features. 
We're not looking actually at all the features but   because we can only plot two, so let's plot two 
of them the end features phi_(d +1) and phi_(d   + 2) and so we see here that 0, 0 which is the 
nurse or the social worker has a minus label. 0,   1 which is the admin has a minus label. 1, 0 which 
is the pharmacist has a plus label and 1, 1, the   doctor has a plus label. And so this is nice, this 
is something where we see we can get a division   between, in this case, the pharmacist and the 
doctor and the nurse and the admin and that's by   having a cut, having a linear classifier 
and essentially that second feature.   Okay but something I could totally 
imagine, it's totally plausible,   is that maybe 0, 0, the nurse say, and 1,1, the 
doctor, maybe they have a lot in common, a lot of   predictive power in common, maybe they tend to 
suffer heart disease more. At least this seems   plausible a priori before we've learned anything, 
it's something that we want to say is possible.   And it's possible that the admin and the 
pharmacists maybe don't have heart disease   and so the issue here is that, well as 
you've seen in your reading for this week,   we just can't have a linear classifier that 
classifies this correctly. And this doesn't   seem that hard. Again it seems like if we just 
have a few categories, we want to be able to say   that some of the categories go together 
and some don't and the problem with   binary encoding is that we're enforcing those 
divisions again. You are allowed to do a break   on the first feature, that means you're 
allowed to group nurse, admin, pharmacist,   and doctor separately from social worker. You 
are allowed to do a break on the second feature:   you are allowed to group pharmacist and doctor 
away from everybody else. You're allowed to do   a break on the third feature: you're allowed to 
group admin and doctor together and nobody else,   but those aren't the only breaks that we want, 
we want other breaks, we want to be able do,   we want to be able to group any, you 
know, few of these jobs together and   not group the other jobs with that. And so 
how could we get the ability to put together   arbitrary groups of jobs? That's what we want in 
our encoding here. And so here is yet another idea   that perhaps will finally let us do this, 
you know, how could we possibly do this.   We essentially want to say for each job we can 
either put it in the group or out of the group   and so that's the idea of giving each category 
its own feature essentially. So we're going to   say there is a feature that is “are you a 
nurse” and so the nurse has that feature   and they don't have the other features: they have 
a 1 and a 0 for the rest. There is a feature that   is “are you an admin”, “are you an administrative 
assistant,” so the admin has a 1 for that feature   and a 0 for the rest. There is a feature that 
says “are you a pharmacist,” so the pharmacist   has a 1 for that feature and a 0 for the rest 
and so on. And so what's great about this is,   let's again, let's look at plotting this. So if I 
plot any one of these features, well if I have any   individual there'll be a 1 in that feature and 
0 and everything else and if I plop it against   another feature, well any other individual 
job, we'll have a 1 in that feature and a 0   everywhere else and so we're always going to be 
able to find a nice dividing line between these. Okay so this has a special name: 
it's called “one-hot encoding.”   The reason for this is that you can think of 
the 1 (and there's only one 1) as being “on,” as   being “hot.” So there's one bit that's “hot,” 
that's “on,” and all the other bits are “cold”,   they’re 0. And so this is really the standard for 
categorical class data, when we're doing some kind   of feature encoding and we want to encode our 
categorical data. And hopefully at this point   you sort of see why it's the standard at least 
relative to the other ideas that we've explored   because we're assuming this ordering, if we use 
just a single number, you know, natural number   for each one of these, we are assuming a grouping. 
If we assume the binary coding and here we don't   have that, we have the ability to sort of break 
at any point that we want for categorical data.   Okay so now what we can do is we can take 
our jobs, remember there were five of them,   and so what we're going to do is 
we're going to replace that original   set of job labels that were all strings with 
five new features, maybe we can call them   jobs 1 through jobs 5 or j1 through j5 and 
we're going to put our 0s and 1s in there. Okay so now we have this nice breakdown for jobs,   let's go to medicines. Hey this looks 
like what we just did in some sense,   right? I mean there's like four different types 
of medicines, you could be taking pain medicines,   you could be taking beta blockers which turn out 
to be this type of medicine that I believe helps   with slowing down your heart and might be useful 
in heart disease, you could be taking only beta   blockers, you could be taking no medicines and 
so that's a set of categories, right? That's   four categories and so should we do the same thing 
that we just did? Actually this is a question that   I'll ask you: please respond in the private 
chat. Should should we just encode these in   this nice one hot encoding that we just saw? Would 
this be four categories for the one hot encoding? Okay I'm seeing a lot of no's and I'm seeing 
some people say no because they have overlap,   that the categories aren't exclusive. 
Exactly. Okay so let's explore that idea. Okay so here's what would happen 
if we did one hot encoding,   should we use one hot encoding here. Well we would 
make our four categories or four new features and   we'd have a feature for each category, one 
for pain, one for pain and beta blockers,   one for beta blockers, one for no medications. 
Okay but as many of you are observing,   there's something a little bit unnatural 
about this because, in some sense, what's   really happening here is there are two features 
and we have just squished them together, right?   One feature is “are you taking a pain medication” 
and one feature is “are you taking a beta blocker   medication” and yes we can squish them together 
like we could squish any two features together.   Like you could say, “hey my new feature is resting 
heart rate times income” but maybe I just don't   think that that's a very useful feature, you know, 
maybe I think it's more useful to ask “what am I   going to do with resting heart rate, or how does 
that affect my prediction, how does income affect   my prediction?” And same thing here: you can 
think about this in terms of interpretability,   you know, at the end of the day I'm going to 
ask well what was really predictive and how is   it predictive and it might be more useful for me 
to really look at was pain medication predictive,   was beta blocker medication predictive, 
rather than each individual combination.   So here's an idea: let's use factored encoding 
and all that is saying exactly what we just said,   I think a very common sense kind of thing, 
which is let's break this into two features.   So let's say there is a feature which is “are you 
taking pain medication,” there is a feature which   is “are you taking this beta blocker medication” 
and that's it, that's all that these represent.   Now here, one of the benefits of this is also 
something that I think is a little bit less   apparent from this small dimensional problem 
but if you could imagine that if we had like   a lot more medications, you're going to get 
this combinatorial explosion of combinations   of medications, you know, you're just getting 
a huge number if you can have each one on and   off and you had k medications you could have 2^k 
combinations of medications, whereas you're just   going to have k features of “are you taking a 
medication or not” and so that's going to be   a lot simpler to deal with and potentially better 
for algorithms that perform poorly in really high   dimensions. You're going to see here, you know, 
we have this a similar thing to before. If I have   some effect that really changes based on having 
pain medication or not having pain medication,   I have the ability to make a linear divisor there. 
If I have some effect that really, you know,   or something that's very predictive when I have 
beta blockers versus not beta blockers then I'm   able to make a linear separator here. Something 
that's worth noting is that this looks a lot like   binary encoding. Right, I mean technically 
it is a binary encoding: we have a 0, 0, we   have a 0, 1, we have a 1, 0, we have a 1, 1, 
but it's not quite binary encoding in the sense   that we didn't just use any binary encoding. We 
were very careful to say that the 1s represented   having a medication and the 0s represented not 
having it. If we weren't careful about that, we   could get some weird situation where again, even 
though pain medication was the predictive thing,   it could just be weird in the 0 and 1s and you get 
this issue where you can't do a linear separation   and so we want to be again careful that we're 
not just saying oh let's do a binary encoding,   it's really what that means here, what are the 0s 
and ones mean, whether I'm taking the medication   or not. It's also worth noting, you know, there 
are a lot of tasks in life and just because one   approach is good for one task doesn't mean 
it's good for all the tasks. And so binary   encoding is great when you care about compression 
of information and that is just not our task here,   we are not trying to compress the information 
that we have in some super efficient way,   we're trying to make it easy for our algorithm   to learn the things that are in the data and 
we're trying to make it easy for ourselves to   interpret what went on and those could overlap 
with compression but they don't necessarily   need to and it's always better to start from the 
goal that you have rather than some other goal. Okay so here we have our set of medicines 
and so now we've seen that it would be useful   to break this down into a feature that encodes 
“do I have this particular medicine,” let's say   pain medication in my, is this individual taking 
this pain medication, and that'll be a 1 or a 0,   and is this individual taking a beta blocker and 
that'll be a 0 or a 1. Now I also want to mention   that this factoring, which really just means sort 
of breaking off into into, you know, sort of sub   features as it were, doesn't have to be into 0s 
and 1s, and there's an application in the reading   or an example of this in the reading where you 
can see that in fact there are different levels   of the factoring and you'll see more examples of 
this I think as as we go forward in like the labs   and the homeworks and so on and so just keep that 
in mind. I believe there's another question, yes.   Yes the question is: if we have a one hot 
encoding with k labels or k categories   can you use k -1 features with 
one label represented by all 0s?   Yeah absolutely so this is totally true and 
this is a great observation that, you know,   once, you know, something doesn't belong to 
the other categories then clearly it, you know,   doesn't belong to the other categories. 
That could be something that you pick out.   It is often useful though, to still say that that 
other category is explicitly represented by a   feature. One for symmetry, because there's nothing 
special usually about that last category and so   you might not want to treat it asymmetrically. 
Two, for interpretability because what we're   going to do at the end of the day is look at our 
thetas and see, you know, which what things are   predictive for what and, you know, what goes into 
what and so it would be nice to have a theta that   corresponds to that category. And this kind of 
relates again to this discussion of, you know,   do we care about being efficient versus do we 
care about making our predictions, making our   algorithms run well, and making our predictions 
interpretable and things like that. And so that's   typically the kind of trade-off that you're 
facing and so this might be one set of reasons   why you would perhaps encode every category. 
Cool. By the way I should say too, you know,   there's some sense in which the real answer to all 
of these questions is: it depends on what you're   doing, and so every time you come across a new 
data analysis, I think it's worth asking yourself:   how are things going to perform differently 
based on the choices that I make and it is   totally possible that there are, and in fact it's 
probably almost always certainly true, that if we   come up with some rule that there's going to be 
some data analysis where you want to do something   differently just because that's a different 
type of data analysis and so just every choice   that you make in a data analysis it's worth: 
asking how does that affect what we're doing? Okay cool let's go to age. So here in age, 
we see that we have a few different decades   represented. Like maybe age wasn't actually 
recorded very specifically, we just got decades   maybe for privacy reasons. So people said, you 
know, I'm in my 40s or I'm in my 20s or I'm in   my 50s in response to some kind of survey and so 
there's some interesting choices that we can make   here. So one, and certainly a choice that you 
would commonly see in practice, is to use some   representative h like maybe, you know, 45 for the 
40s, 25 for the 20s, 55 for the 50s, and so on.   I've also seen things like 15 for the 
middle of a month if you don't know   what day something is in the month, you 
know, some kind of representative number.   It's just worth noting that there are some 
potential pitfalls to this. One is that whenever   you introduce a level of detail, it can be treated 
as meaningful by you or by others who use the data   and so, you know, just as you should comment 
your code for the future people who are going   to be reading your code, it's good to think about 
your data engineering for future people who might   be using your engine, your data, the features 
that you make down the road. You might be using   somebody else's features that they made and so 
you might want to ask what's going on there. So   for instance, you know, if you saw that a lot of 
things were happening on the 15th of the month,   then you might think there's something really 
important about the 15th and it turns out that was   just the day people put in when they didn't know 
what day of the month something was happening.   There's actually a really extreme version of 
this that I think is just really fascinating. So   there's this just like fantastic piece of tech 
journalism which you, if you've never read it,   I strongly recommend reading it. Basically so 
Kashmir Hill is this amazing tech journalist   and she dug into this really weird thing that was 
going on, where a family and their renters were   getting a huge amount of harassment, so they 
were getting all these sort of horrible things   happening to them, they were getting visited by 
FBI agents, federal marshals, IRS collectors,   ambulances, police officers, like you name it, 
weird people just lurking around their place   and it was happening for a decade and they had 
no idea what was happening. And so it turned   out that there is a tech company that provides 
physical location information for IP addresses   and they often can't do it precisely, like often 
they can map an IP address to a physical location,   but sometimes they can't. And so when they can't, 
they have default locations at the city level   and if they don't know where it is at the city 
level, they put it at the state level, if they   don't know where it is at the state level they put 
it at some default location at the country level.   Well these people lived at the default location 
at the country level and that just happens to   be in Kansas and so everybody where they 
didn't know where that IP address mapped,   suddenly it mapped to these people and so 
it was just horrible. They got all of this,   you know, horrible stuff happening to them and 
again the amazing thing is nobody knew. So the   family certainly didn't know, I mean they had 
no idea why anything was happening, the local   police and anybody they talked to didn't know, 
but conversely the company that was doing this   had no idea, you know, they had this default value 
but they didn't sort of realize what that meant in   terms of these people's lives and it was only when 
this journalist came in and sort of asked “hey,   what are some locations with an unusual number of 
IP addresses mapped to them?” and it turned out   this one has like something like 600 
million IP addresses mapped to it,   that she was able to figure out why this is going 
on. Of course this isn't even the only location   that has this issue, it's just a particularly 
egregious form of this issue at this location   and so, you know, they were able to identify 
other locations that had a lot of IP addresses   mapped to them and weird stuff happening, but 
I think it's just worth noting a few things.   One, the seemingly innocuous choices that you 
make in tech and computer science can have really,   really big impacts on people's lives and so it's 
worth thinking them through and being very careful   about them, even when you think oh, you know, 
I'm working in IP address mapping, it's not   healthcare, you know, what's the difference? It 
can make a big difference. And then two, there's   a great way to have diagnosed this problem. You 
didn't even have to anticipate this problem,   you didn't have to know that problem could exist, 
but if you plotted your data, you would notice   that something weird is going on and I think that 
that's, if you take maybe one metapoint from this   lecture, it's that just plot your data to see 
if something weird is going on. If you plotted   like a histogram of this data, you'd see that 
there's this one IP address that's just getting   everything, like absolutely everything, or this 
one location that's getting all the IP addresses.   Okay so maybe in this particular case we 
might not use a representative number,   although I don't think anything is as 
intense as this is going to happen if we do.   Maybe instead we'll do something like decade. 
We'll just say, hey what decade are these   individuals in? Because that conveys exactly the 
amount of significant information that we have,   you know, you remember hopefully from high school 
this notion of sig figs that you want to say for,   you know, whatever information you have you want 
to convey the significant figures that, you know,   but you don't want to suggest that you have more 
information that you have that that, you know,   something to a greater significance than you 
really know it and so this lets us do that.   Okay, so that's decade. Now I'm just going to 
take a little bit of a detour for something that   we don't have in our data here but could come up 
and that's something called ordinal data. Okay   so we talked about a few different types of data 
at this point: we've talked about numerical data   where you have an order on your data values 
and the differences in value are meaningful,   this is sort of the default type of data 
that we've been talking about in this class,   it's the data that we assumed x was in to begin 
with, it's sort of the easiest type of data in   some sense. But now we've also talked about 
categorical data where there's no ordering   on the data values. And there's a different 
type of data that's kind of in between these,   where there's an ordering on the data values but 
the differences are not necessarily meaningful.   So some examples of this, if you've ever seen 
anything in the social sciences, an example of   this is the Likert scale. So somebody asks: “hey 
I have, you know, some maybe political opinion,   do you strongly disagree, do you disagree, are 
you neutral, are you agreeing, or are you strongly   agreeing?” There's an ordering there, there's 
nothing that tells you the difference between   strongly agree and, you know, strongly disagree 
and disagree is the same as the difference between   disagree and neutral. They're ordered but that 
difference isn't meaningful. Same thing with,   you'll often get asked in a doctor, your pain 
scale: do you have a pain at a level of one,   do you have pain at a level of two, 
do you have pain at a level of three,   and that just doesn't have the same meaning as 
what is your beats per minute of your heart. And   so what could go wrong here? Well let's again plot 
some data. So here suppose that our doctor has   measured our resting heart rate and maybe again 
with a higher resting heart rate we tend to see   more heart disease and maybe they've also asked 
us what's our degree of agreement that, you know,   we're not in pain. And so here we have an ordering 
and so very naturally maybe these are already   encoded as the numbers one through five and so we 
might think “gosh these are numbers I don't have   to do anything else with my data, I'm all set” 
but we might also notice that, actually, again   these are different kinds of numbers, that 
they aren't meaningful in the way that   beats per minute are meaningful. On particular, 
again, the difference between four and five might   not really be the same as three and four 
here, maybe it's actually quite humongous   and that really affects how we use linear 
classifiers. So if I have a linear classifier,   I'm not just assuming an ordering on my data, I'm 
assuming a sort of rate of change: I'm assuming   that the difference between one and two is the 
same as the difference between two and three   and so on. And so if that's not true I could 
get some really different linear classifiers   depending on what that difference is. This 
is actually pretty different from this. And so, for that reason, even though 
this actually is already numerical data,   I might consider encoding it differently 
when it truly represents something that   we would call ordinal rather than numerical. 
Now an idea here is something that is called   unary or thermometer code and so that's exactly 
the following: so I've replaced 1, 2, 3, 4, and 5   with 1 followed by all 0s, 1, 1 followed by 
all 0s, 1, 1, 1 followed by all 0s and so on.   Now the idea here is that whenever I split, I 
split in order. I can split such that strongly   disagree, disagree, and neutral are together 
and agree and strongly are grouped together.   Unlike categorical data, I don't just take out a 
particular item when I split. I do a split that is   in order but there's no implicit ordering on the 
data, if I do a split it is just between those   two adjacent elements. Now why is this called 
thermometer code? You could think of those 1s as   they go along as filling up this thermometer with 
mercury. So it's sort of like very low temperature   at 1, 0, 0, 0, and it's very high temperature at 
1, 1, 1, 1 so that's that's the origin of that   term. Okay so we've looked at a lot of 
different types of data at this point,   we've seen our numerical data, our 
categorical data, our ordinal data,   and in fact now the last thing we have 
here looks to be numerical data so,   you know, we can keep it as it is and we're 
all set. So the thing to notice at this point   is that we have numbers. These are numbers that we 
can take dot products with and so from purely the   perspective of our algorithm will run and will not 
give us bugs, we're all set. But now let's ask:   are there still potential problems or 
things that we could do better by again   having some feature transformation of our 
data, transforming to a new set of features?   So in order to talk about that let's actually 
talk about numerical data which we thought was   the easiest type of data and, you know, in some 
sense it is but there are still some potential   issues. So let's look at the output of our 
linear classifier and let's first look at,   you know, something that we might do when we're 
interpreting our classifier after we've done our   classification. So suppose that there are a bunch 
of people on the internet who are just saying “hey   I really think that the number of garlic cloves 
that you eat in a week is super predictive of   your heart disease” and we want to check is that 
true, is that something that is absolutely true   especially when, you know, all these other things 
about individuals. And so we might check “hey I   am interested in looking at this weekly number 
of garlic cloves eaten and resting heart rate.”   By the way I should say the only thing we can talk 
about here is predictivity. We can't talk about   causality because that's not what we're measuring, 
that's not what we're doing. We can't say is this   causing uh, you know, something like the number 
of or the prevalence of heart disease or whether   you have heart disease, you have to do some real 
science to check that or be much more careful,   but we can check whether this is predictive 
given all the other information that we know.   And so we might do this by the following: let's 
run our linear classifier and let's find our theta   and we see in this particular case that our theta 
and our linear classifier just doesn't really   depend on the number of garlic cloves that you're 
eating and one way you can see this is that, if   you look at theta, it's change in the garlic clove 
direction is 0 or roughly 0 and its change in the   BPM, the resting heart rate direction, 
is non-zero, it's highly non-zero,   and so something that sometimes people do is 
they'll look at their theta after the fact and   they'll say “ah, you know, it's really small in 
some of these directions, maybe those features   weren't so helpful in our prediction.” And in 
this case, you know, that seems to be true.   Okay so now that we've talked about this 
thing that people might do in terms of   interpretability. let's see how numerical data and 
our coding of a numerical data enters into this. Okay so now let's look at 
resting heart rate and income.   So suppose I'm interested in heart disease, 
protecting heart disease, from these two features   and I run my linear classifier and this is what 
I get. And now suppose that my friend actually   encoded resting heart rate on a slightly different 
scale than I did, you're going to notice that they   actually get out a different linear classifier. 
Now it kind of means the same thing in terms   of prediction but if I were to look at the theta 
values and I tried to interpret the theta values,   I'd say something different because 
I get these different theta values.   Now there's an extreme version of this where 
I notice that actually income is on the order   of tens of thousands of us dollars typically 
and resting heart rate is on the order of tens   and so if I just plotted them on the scales 
that they are typically, you know, recorded at,   you know, we just said “hey you should plot your 
data to see what's going on” so I plotted my data   and this is the scale that it's at I would say 
“wow, you know, there's really nothing going on   with resting heart rate here, this is nothing 
too interesting.” And if I looked at the theta,   if I just looked at the theta without, you 
know, plotting my data or thinking about this,   I would say gosh look there's, you know, you 
know there's almost 0 in the income direction   and it's really highly non-zero in the resting 
heart rate direction, so it might be misled   in various ways by doing this. And so there's 
something that we can do which I think you often   just do without thinking, or more to the point 
your plotting program does without thinking,   which is let's just make sure that we plot our 
data on a scale that we can see the differences. And this is called standardization. So one way you 
can do this, I mean really again the main idea is   just let's plot our data on a scale where we can 
see the differences, but one way you can do that   that's very sort of automatic is to say, for each 
dimension where I'm interested in standardizing,   I'm going to take the empirical mean of my data 
across that dimension, across that feature,   I'm going to take the empirical standard 
deviation across that feature and I'm going   to do the following transformation: so first by 
subtracting the mean I move everything around 0,   and then by dividing by the standard deviation I'm 
basically zooming in or zooming out as appropriate   and so I'll end up plotting something that looks 
like this once I do standardization. Again I think   we take this for granted because our plotting 
programs are typically doing this or something   like it—they're finding what are the extreme 
points and then plotting those—but that's similar   in spirit, you know, we're just doing something so 
that we can actually see the variation in the data   and that's what this is accomplishing. This also 
makes it the case that when you look at a value   of theta, when you do your linear classifier and 
you get some theta out, that it has some kind of   meaning, as opposed to being totally determined 
by the scale that you just happen to be plotting. Okay. Okay, so we've seen a lot of benefits 
of plotting our data at this point,   again as a metapoint, I think that's just a 
really important point: you can't go wrong   plotting your data. But let's just talk about one 
other benefit and also a really important benefit   of talking to experts. You know, we just made this 
observation that maybe what you're going to do is   you're going to interpret your theta and if you 
see that theta's kind of 0 in one direction that   maybe not too much is going on there. Well let's 
suppose I do that. Suppose I look at this data   and it's, you know, it's basically on the right 
scale, like certainly we can see any variation   in this data, there's nothing where there's an 
issue of scaling that's going on at this data.   So we're plotting people's resting heart rate 
versus where they seen at a particular hospital,   and so it turns out that which hospital they were 
seen at is super predictive of what is going on,   of whether they have heart disease, and 
resting heart rate just doesn't really matter   and so if we weren't thinking about our data, if 
we weren't plotting it and we weren't talking to   experts, we would say “yeah okay I have a great 
classifier, all you do is ask are these people,   were these people seen at a particular 
hospital and then I know if they have   heart disease. Great yeah it's perfect.” And 
so it turns out if I talk to experts here,   maybe it's the case that actually everybody with 
heart disease goes to one hospital, there's like   the hospital where you treat the people with heart 
disease and anybody who, you know, doesn't have   heart disease doesn't have to go there, so they'll 
be at other hospitals. And so it's not the case   that this is a useful prediction, I mean it's a 
great prediction— basically the doctors did the   predicting already for us—and then we just, you 
know, copy their answer but if we're trying to   come up with something that's going to be helpful 
to doctors, like a new useful tool for them,   this is not going to be useful. It's like 
saying “hey you already made your decision   and now we're just going to piggyback on that” and 
predict based on that. And so this is the kind of   thing that, you know, we might not know going in, 
there's no reason that we would know all of these   details especially if, you know, somebody gave us 
this data and then we're analyzing it for them,   but by a quick talk with some expert about the 
data, we would be able to establish this issue   and then maybe get rid of that feature because 
it's not so useful for what we're trying to do.   Conversely, if you are an expert who is 
running machine learning for your problem,   talk with yourself in the sense of plot your data, 
interpret what's going on with that classifier,   and then think about is this actually 
what I'm trying to accomplish or   is it something else and I need to rethink 
my features and rethink my problem. Okay, so now we have all of our features, we've 
encoded them in these different ways, we're going   to standardize our numerical features: so we're 
going to take them and we're going to perform   the standardization that we just talked about 
and so we've got all this fantastic data that   not only can we now apply our machine learning 
algorithms too, we don't have any issues with   taking dot products, and we know that these thetas 
that we get out are going to be somehow meaningful   and there's going to be some interpretability and 
so let's ask ourselves: okay we're ready to go,   we're going to run our algorithm and, you know, 
I just I learned today that I should plot my data   and so let's go ahead and do that and I plot 
my data and let's say here's my data. Okay so   something that's very common in health and 
medicine is that there's like a sweet spot   where the human body exists. Right so if your 
blood pressure is way too high or way too low, you   might have some kind of health problem. If your 
breathing rate is way too high or way too low,   you might have some kind of health problem. If 
your BMI is way too high or way too low, you know,   there's there's all kinds of points where there's 
a sweet spot and so we might not be surprised   to see data that looks like the following 
where somehow, you know, we want there to be,   you know, we want to recognize that there is some 
typical range where things are happening and that   we might predict some health bad health outcome 
outside of that area. And of course, something   that you'll just immediately recognize about this 
data given all of the work that you've been doing,   is that this is not a linear classifier 
and this is not linearly separable data.   And yet it's so perfectly separable, you know, 
like it's separable in some other way that there's   some simple function that somehow separates the 
places where we want to predict -1 and the places   where we want to predict +1 and so we might ask 
ourselves is there something that we can do,   is there something that we can do 
to deal with this, to approach this? And because today we're talking about how to deal 
with this or how to do pre-processing, we'll ask   if there's pre-processing that we can do. Sorry I 
missed that there were some backlogged questions,   if we could just handle those now that would 
be great. Going back to thermometer encoding,   why is thermometer coding more meaningful than 
just encoding from like 1 through 5 for example?   Yeah so I guess I wouldn't necessarily describe 
thermometer coding as more meaningful, just to say   that it encodes a different set of information. So 
what's going on with the encoding of 1 through 5   is that you're not just saying ordering, 
you're saying that the difference between   1 and 2 is the same as the difference between 
2 and 3 and so maybe I have a pain scale which   says “I'm fine”, “I'm in pain”, and “I'm in 
excruciating pain.” So we could call that 1, 2,   and 3 but maybe excruciating pain is just really 
different from a little bit of pain and it could   be that when I do classifications that that 
really matters, you know, especially as we saw   for linear classifiers that a linear classifier 
assumes that the difference between 1 and 2 is   the same as the difference between 2 and 3 and 
so when we have a situation where our actual,   the correct interpretation of the data that we've 
got, does not include that—so for instance in this   pain example I don't know that the difference 
between 1 and 2 is the same as the difference   between 2 and 3—then I really want to make sure 
that my encoding reflects that. So in some sense   it's like I'm getting rid of meaning, I'm taking 
away some meaning, I'm trying to take away   information that I had. So in the 1, 2, 3 it's 
like I'm trying to encode that there's more   information , that these differences are the same. 
With thermometer encoding, I'm encoding just that   there is an ordering but I'm not encoding that the 
differences are the same, I'm saying essentially   that there's a dimension for each difference 
and that lets me have a different sort of   slope or theta in each of those dimensions to 
encode that different difference instead of having   the same theta, the same change in everyone. 
Cool was there anything else that I had missed? Yeah a couple questions on standardization: 
so does standardization always like, does   standardizing data actually affect the results 
of our machine learning algorithm or is it just   mostly used for picturing? Yeah great question. So 
it depends on your algorithm and so for instance   we're gonna see algorithms like decision trees 
where it really just does not matter because   you're just splitting on particular values. You 
can think through what happens in the perceptron   whether it would matter or not and I will leave 
that as an exercise to you to think about right   now and I think that you can figure this out. 
But we're going to see examples of algorithms   where it actually matters to the algorithm. So 
in particular we're going to see in neural nets,   for instance, that it matters and so 
that's not something that we've talked   about so far and so I'm not going to go 
too deep into that but keep it in mind   as we go forward to these other machine learning 
algorithms because there are algorithms where   it actually matters for performance and it's not 
just an issue of interpreting and how we view it. And from standardizing, do we get the mean 
and standard deviation from the data itself,   or some sort of general statistics? Yeah so 
let me give two answers to that. So when I   described it and what we describe in the notes, 
you get it from the data itself. So certainly   a typical way to standardize is that you take 
the empirical mean, which is to say you add up   all of the values in that particular dimension, 
in that particular feature, and then you divide   by the number of values you've added. You take 
the empirical standard deviation, which is to say   you get the standard deviation just by calculating 
it from the values you have. Now that being said,   again I want to emphasize that if our goal is 
really just to get the data on sort of the same   scale across our different features, this is not 
the only way to accomplish it and so if you had   other information about some, you know, generic 
mean and standard deviation about your data,   you could potentially use that as well or you 
could use quantiles as another thing that people   can use in practice but again I want to emphasize 
this difference between, you know, typically   when people talk about standardization—and 
what we're doing in the notes is exactly   taking the empirical mean and empirical standard 
deviation—just that there are other things you   can do that are perfectly valid and often very 
useful. Cool okay great, so let's come back here,   and we've got this problem where, you know, we 
have this data, very reasonable realistic data,   where we want to find some classification that 
works for this data and it's clearly just not   going to be a linear classifier and so we want 
to ask ourselves is there something that we could   do here and in particular, because we're sort of 
talking about pre-processing the data today, we're   going to ask is there some kind of pre-processing 
of our data that would help with this   because there are certainly other things you can 
do and we'll explore many of them later in the   class but for the moment let's ask, you know, 
is there some pre-processing that would help.   Okay in order to ask this question about sort 
of non-linear boundaries or answer this question   about non-linear boundaries, 
it'll help us to revisit   thinking about classification boundaries for the 
linear classifier. So here's, again, our cartoon   of our linear classifier. We have some data we 
want to find a classifier that says “hey we're   going to predict +1 on one side of some hyperplane 
and -1 on some other side.” So let me just   recall first, you know, this is our linear 
classifier, this is the side we're predicting +1,   this is the side we're predicting -1 and then 
on the line itself we predict -1 by convention,   but let's think about a different way 
that we could visualize what's going on.   So what's happening here is that 
I've introduced a new dimension,   let's call that dimension z, so on the 
horizontal axis we have x_1. x_2 is the   axis that's kind of hidden behind this colorful 
hyperplane and then z is popping out of the page. This hyperplane is actually theta 
transpose x plus theta naught equals z. This plane that I've just highlighted in gray   is where our original data lies. It lies 
with x_1 and x_2 varying but z equals 0. The line that defines the linear classifier   is where theta transpose x 
plus theta naught equals 0. That's exactly how we defined our line 
before. And if we want to look at the two   sides of the line, there’s one side where this 
colorful hyperplane is below the z = 0 plane,   the colorful hyperplane that theta transpose x 
plus theta naught is less than 0 and so we predict   -1. On the other side, theta transpose x plus 
theta naught is greater than 0 so we predict   +1. Why is this useful? Because we could have 
other functions, we don't just have to have   a hyperplane, we could have some more 
general function then we could ask   where is it greater than 0 and where 
is it less than 0. So for instance,   let's say we cared about this data we were 
talking about before with the minuses and pluses. What if we can make a function like this? Let's just call it f(x). Here's z = f(x). Well 
in this region, the function is below z = 0, the   function is less than 0, so we might predict -1 
for the set of x for the function is less than 0. And then if we look on the rest of this 
plane where the function is greater than 0,   that's where we're going to predict +1   for that area. So if we have the ability 
to use very general functions instead of   just hyperplanes we could get some really funky 
different regions where we do our classification. Now really really general functions, that's 
asking a lot. Is there some way we could,   sort of, simplify this and 
so there's a familiar idea,   hopefully familiar to you from your earlier 
courses of a Taylor expansion, you know,   I can get pretty close to a very general 
function by just looking at a polynomial   up to a certain order. In fact it turns out this 
f(x) is actually just a quadratic polynomial. And so how can we do this? 
Well let's look at polynomials. So we just said if I want a really general 
function, general smooth function, I can   approximate that pretty well especially within 
a particular bounded region with a k-th order   polynomial, kth order Taylor polynomial. 
For instance, let's say it's around 0. So if, in particular, I was looking 
in one dimension, if my features   only existed in one dimension, then just recall 
that the sort of 0th order Taylor expansion   is basically a constant or a constant times one. If I take the first order Taylor expansion of 
a function, I'm going to get a constant times   1 and a term that looks like a constant 
times x_1, where x_1 is our only variable,   our only feature. If I look at 
the second order Taylor expansion,   I'm gonna get a constant times one and 
a term: I'm gonna add a term that looks   like a constant times x_1 and I'm gonna add a 
term that looks like a constant times (x_1)^2.   If I look at the third order Taylor expansion, I'm 
gonna add a term that looks like a constant times   one, and a term that looks like a constant times 
x_1, and a term that looks like a constant times   (x_1)^2, and a term that looks like a constant 
times (x_1)^3, and hopefully you get the pattern   that's happening here. And so an observation is 
that if I had one dimension and I wanted some   really flexible boundaries in my classification, 
I could have an expanded feature space   where maybe I take my original feature, x_1, and 
now I look at new features 1, x_1, and (x_1)^2   or if I wanted even more flexible boundaries 
I could look at 1, x_1 (x_1)^2 and (x_1)^3   as my new features. Now, there's a version 
of all this that exists in higher dimensions.   So the 0th dimension is just the same, it's 
just a constant. If I did a first order Taylor   expansion in my d dimensions but only first order 
then I'm going to pick up x_1 up to x_d terms. If I'm looking at second order, I'm going to get 
all of the quadratic terms: x_1 * x_2, x_2 * x_3,   (x_1)^2, (x_d)^2, all that and it just gets really 
messy after that, we get a lot of terms. But the   idea here is that we can get some really flexible 
boundaries by expanding our set of features using   these polynomials and this will often be 
called something like a polynomial basis.   It's not the only way we can come up with flexible 
functions, there are definitely other ways,   but this one's kind of convenient and certainly 
an easy thing to do: all you have to do is take   whatever my original, you know, feature was 
and make some new features 1, x_1 and (x_1)^2,   for instance, and then I can get these super 
flexible boundaries that we saw before.   So again, just to look at what this looks like,   you know, if I use this quadratic function—so 
this is something I could get by using   all the polynomials up to degree two—then I 
could classify all of the area where my function   is less than 0 as being -1 and all of the area 
where my function is greater than 0 as being 1. What if I had some funky data like this?   Well if I use up to third order polynomials and 
I can look at all of the area where this function   is less than 0, and that's 
exactly this weird shape—so   blue is the z, it's the one that's sort of coming 
out the middle here, so hopefully you can see that   we get sort of this weird shape by looking at 
where the function is less than 0 and then if   we look at where the function is greater than 0 
we get this weird shape. This is all just to say   that if you use this polynomial basis, if you 
do this feature change, you change your features   by expanding them with these polynomials, you can 
get some really flexible and different boundaries,   certainly highly non-linear boundaries and 
the reality is that a lot of life is is pretty   non-linear, you know, we saw this example 
in health, again, there are a lot of health   examples where there's some sweet spot for the 
human body, there's a temperature that's a good   temperature. If you're really far away from that 
temperature you might be experiencing a health   issue and we'd like to be able to detect that and 
so we'd like to be able to say hey, you know, not   everything is a linear boundary and so how can we 
do that and we're starting to see some examples. Okay so we have this ability now to fit some very 
flexible models with some very flexible boundaries   and so imagine   that here is my data and something we often also 
really see in real data is this kind of overlap,   you know, maybe if my weekly exercise amount goes 
up I tend to, you know, not have heart disease   as much. If you look at people whose resting 
heart rate is going out maybe they tend to have   more prevalence of heart disease but there's 
often this sort of wishy-washy in between,   you know, some people who exercise a certain 
amount and have a certain resting heart rate   have heart disease and some people don't and so 
I could fit my super flexible polynomial model,   I took a super high degree polynomial, and 
I get this like really great classifier:   so here I predict +1, here I predict -1, and 
the training error is 0. So this is fantastic,   right? This is a question for you in the 
chat: is this a fantastic classifier? Great, lots of no's, “no way,” “it's 
overfitting.” Exactly, yes the training error is 0   and it's super flexible but sometimes flexibility 
can be a little bit too much as we're seeing here   that, you know, maybe I have the ability to get so 
much nuance in my data that I'm worried how it's   going to perform on new data and that's exactly 
the concern with overfitting that, you know,   we have to ask ourselves again what's the point of 
doing any of this machine learning? Well the point   is we'd like for somebody else to come into the 
hospital or to be at home and to be able to say,   you know, something about their diagnosis, you 
know, do they have a particular health problem   and our intuition here is telling us 
that, you know, it's probably not the case   that if you exercise down to this decimal 
amount of, you know, exercising in terms of   minutes of exercising and if your heart 
rate is exactly 88 but if it's 87 then   you definitely have heart disease—that just 
doesn't seem physically meaningful. In some sense,   you know, we have some expertise about medical 
problems, you know, it's not maybe as much as,   of course, a doctor would have or somebody who's 
really in that area, although it's quite possible   that people in this audience and maybe our 
doctors or EMTs, but even from what we know,   we're consulting our expertise and we're saying 
this doesn't really seem like what's going on:   it seems like we're overfitting here, it 
seems like this is not going to perform well   on future data, and so we want to ask ourselves 
how can we detect overfitting and how can we avoid   overfitting? And so for the rest of this class 
we're just going to talk about how can we detect   overfitting, but next class we're going to start 
talking about how can we avoid overfitting.   And of course we can start thinking 
about it right now, but let's go   a little bit into detecting overfitting. Now this 
is something that you've been exploring again   in the labs and the reading but let's just 
sort of cement some of those ideas here. So in order to detect it, I mean 
in some sense what we're asking is   how does our learning algorithm do, 
you know, is it the type of thing that   in general if I apply it am I going to 
get some super overfitting, you know,   if I apply a super, you know, high degree 
polynomial it seems like I could probably   fit things that are maybe a little bit too 
flexible and I'd like to be able to know   if that's true. So how good, you want to ask, how 
good is our learning algorithm on data like ours? Okay so in some sense the issue that we have and 
this is an issue that you've been exploring in   various problems is that we only have the 
data that we have. It would be great if we   had sort of an infinite amount of data and we 
could try out all kinds of different things,   but we only have the data that we have and so 
perhaps we could use all of that for training   and then report the training error. This sounds 
so silly and we've talked about it a million times   but it is surprisingly common, you read a news 
article and somebody reports accuracy and then it   just turns out it was training error and you have 
to be really skeptical of that and so I, you know,   it's one of those things that feels at this point 
like belabored but I just hope that the next time   you read a news article about machine learning 
you ask “hey what exactly are they reporting?”   Okay so we know that this is a bad idea, we know 
that we should not use the full data for training   and then report the training error. We could 
have—exactly the issue that we just saw on   the previous slide that that was, you know—0 
training error. Great but not so great really.   Okay so one idea is to reserve some data for 
testing. So just as a little bit of a cartoon,   let's imagine that this bar represents all of 
our data from the first to the nth data point   and so maybe we could use 75% of it for training 
and 25% of it for testing, or maybe we could use   50 of it for training and 50 of it for testing. 
The reality is that there's a real trade-off here.   If we have more training data, that's closer 
to training on the full data that we actually   have available, so we have a better sense of how 
our algorithm performs on data sets of size n.   On the other hand, if we have more testing data, 
we have a better estimate of performance for what   it is we're looking at. In the extreme version 
of this is, suppose I test on one data point,   I can only get a testing error of 1 or a 
testing error of 0 and so that's just very   noisy. I don't really get a sense of what 
really is the testing error of my method. Also an issue here, and certainly one that you've 
been exploring, is that if I just train on some   part of my data and test on some part of my 
data, that's just looking at one classifier   and that might not be representative of my 
algorithm, you know, maybe I got unlucky, maybe   I got some unlucky training data, and some unlucky 
testing data, or one or the other. Now there are,   of course, ways to help yourself be less likely 
to get unlucky: one of those is shuffling your   data so, for instance, a very realistic thing that 
could have happened, you know, we've been talking   about this health example. Suppose that somebody 
went into the hospital where all the heart disease   patients are not, and they went through all 
of them and they asked them all their data   and then they went to the hospital where all the 
heart disease patients are and they got all of   their data and so if I just took my first bunch 
of data as my training data and my last bunch   of data as my testing data, I'd have this awkward 
thing where I train on all of my negative examples   and then I test on all of my positive examples or 
something that looks like that. And so you just,   you probably, want to, in general, shuffle your 
data and this comes back to an actually a sort   of interesting point that came up in some of 
the discourse questions after lecture two,   you know, when we were showing this demo of the 
perceptron, we were looking at data point one   and then we did the updates for the perceptron for 
data point one, and then we looked at data point   two and we did the updates for the perceptron 
at the data point two, but something some   people noticed being very paying close attention 
which is great is that data point one was kind   of in the middle of everything, and data point 
two was kind of in the middle of everything,   and then data point three was kind of in the 
middle of everything, and what's important to   notice here is that unless you know something 
about your data, you just can't assume anything   about the data ordering. It could be all over 
the place which happened to be the case there,   it could be very structured and in order which 
happened to be the case here. You want to be very   cognizant and very conscious about what's going 
on with your data and make choices appropriately.   And so on the chance that it could be very much 
an order, it actually helps many algorithms, as   well as your ability to evaluate, to shuffle and 
that's certainly what we would want to do here.   Okay so that all being said, we've seen 
there are some potential downsides to this,   namely that we could get this noisy 
estimator performance if we have,   you know, too little testing data and only 
one classifier might not be representative   and so there's an algorithm that helps 
us with this and it's in, I believe,   chapter one of your notes, certainly one of the 
first two chapters, and that's cross validation. And so let's just go briefly through 
cross-validation. So the idea of cross-validation   is that I'm going to have some way 
to evaluate my learning algorithm   and in particular it's going to take 
in a dataset, let's call it D_n,   and k, which tells us how much we break down 
our data, so we'll see that in just a second.   So in particular, we're going to take our data and 
divide it into k chunks, these are often called   folds, so we call this k-fold cross validation. 
So here we have our data and let's say k is 10,   so we would divide it into these k, these 
10 chunks. So this would be the first one,   this would be the second one, the third one and 
so on and we're going to do this for loop over all   of these different chunks. So let's just take one 
of the ones from the middle, let's say this is i,   the i that we're on right now. And what we're going to do is we're 
going to take our classifier—we   have, in particular, our learning 
algorithm—we're going to take our   learning algorithm and as input to that learning 
algorithm—remember our learning algorithm takes in   a dataset, a training dataset, and it 
outputs a classifier—and so here the input   to our learning algorithm is going to be all of 
the data except for the i-th chunk. So that's what   this notation means, the D_n \ D_n,i means we're 
going to take all of the D_n except we're going to   remove the i-th chunk. And so we can think of that 
in this illustration here at the top as being all   of the orange data, so we're basically going to 
take, you know, one through five chunks and six   through ten chunks or sorry seven through ten 
chunks and we're gonna leave out the sixth one   and then what we're gonna get out from 
that is what we're gonna call h_i,   so that'll be the classifier returned by our 
learning algorithm. Now what we're going to do,   is we're going to compute test error. So we say 
I'm putting tests in quotes here because we had   talked about test errors being on some new totally 
different data that we had never seen before. Well   our training algorithm hasn't seen this data and 
so we can think of it as being new to our learning   algorithm, so we're going to compute the error 
on this new data and the new data here is D_n,i.   So the important thing is just that it was not 
seen by our learning algorithm and here I'm just   using this notation E, it's a little bit different 
from how we had defined test error before, but   what I mean by this is let's take our classifier 
h_i and look at its error on D_n,i. So we can   look at its loss on each point in D_n,i and then 
average that loss over all the points in D_n,i. Okay so we can think of that as a sort of test 
error. Again it's for data that wasn't used by   this particular learning algorithm or sorry by 
this particular instance of the learning algorithm   and then finally what we're going to do is 
we're going to take all of these different   errors and average them. And so just as a sort 
of check on what we're going to get out here,   well we're going to get a value 
between 0 and 1 for each test error,   and so when we take this average over 
the different folds over the different   chunks of the test error we're going 
to get another value between 0 and 1.   So it's sort of how we've been thinking 
about error, it's on sort of the same scale   and that's again because we're using 0-1 loss. If 
we used a different loss we might get something   different, but if we use 0-1 loss and then 
we average over that loss, over the data,   and then we average over all the folds and we 
can get something between 0 and 1. And so let's   think about, you know, how does this relate to 
what we had on the previous slide? Well we said   that we would like more training data because that 
gets closer to our full dataset size and so here   we're able to get closer the bigger k is in some 
sense—or sorry, the yeah, the bigger k is—because   we're basically training on everything 
except for that little case sliver.   Now we said that a big issue is that we could 
have noisy test error if we have a very small   thing that we test on and a way that we're getting 
around that here is that, even though individually   one of these folds might give us kind of noisy 
test error, we're averaging over all the folds   and so we get rid of some of that noise. And 
then another issue that we said previously   was that we were looking at just one instance of 
the classifier and so if we do that we, you know,   we might be worried hey we just got unlucky 
this one time. Here we're increasing the number   of instances that we're looking at, by making k 
different classifiers, we're able to average over   more versions. Now it's not perfect, it's not as 
great as if we had actually k different data sets,   because these are clearly very related data sets, 
but it's better than just having one classifier,   we're at least getting the ability to average 
over different instances and so if something   really unlucky or weird happened, we might 
be able to isolate it a little bit better.   Now that being said, it's still a good idea 
to shuffle the order of your data. you could   still have this issue where, you know, maybe 
somebody looked at all the healthy patients first   and then all of the patients who are having some 
health issue or vice versa. Now here it would   only affect maybe a fewer number of folds, but it 
still might have a big effect and so it's still   probably a pretty important idea to to shuffle 
your data to be sure that's not what's going on.   Okay so today we've talked about more of a full 
ML analysis. We've talked about, you know, how   you can get your data, how you can transform your 
data into a way that's usable by your algorithm   that gets, you know, better performance from your 
algorithm, and then talk about evaluation but also   interpretation at the end. Now one issue that we 
didn't resolve that was a cliffhanger from last   time was this issue of “yes, maybe the reason that 
a linear classifier is bad is that you just have   some weird shape,” but another reason you might 
not want perfect linear separability is what we   saw in another example today is that, maybe your 
data is just a little noisy and so what do you   do then? That's not something that—we've done 
a little bit to talk about this with average   perceptron—but there's still some open questions 
there so we have a few cliffhangers including,   you know, how do we deal with overfitting that 
hopefully we'll talk about next lecture. Okay   have a good one everybody catch you later. And 
one small thing I noticed that a number of people   have been writing a thank you at the end of 
our lecture in the chat and I just wanted to   say I see that and I appreciate it and that's 
a very kind gesture. Okay see you next time. Bye. 

Okay, good morning, it's MIT time 
so let's go ahead and get started.   So today we are in lecture four, there have been 
so many great questions at discourse during all   the previous lectures, hopefully we'll keep 
that up with this lecture, we've got a lot of   great staff answering those and of course you can 
answer them as well. So last time and last times   we've been talking about linear classifiers for 
a bit now. We have developed various algorithms,   one of the big ones that we've been focusing on 
is this perceptron algorithm and last time, the   very last time, we talked about the, sort of, more 
complete machine learning analysis, really going   a little bit more from the start, you know, you 
have some data thinking about what are good sets   of features to be using running your algorithm, 
you know, be it perceptron or something else   and then doing something with that, with what you 
get out, interpreting it, using it for prediction,   or something else. And so today, we're gonna step 
back into thinking about algorithms, so focusing   on that algorithm piece, and learn about what 
is really just an absolute workhorse algorithm.   If you analyze data in real life, at some point 
in your life, there's just an almost surety that   you will be using logistic regression at some 
point. Now this is a classification algorithm,   and we talked about how regression is different 
from classification, and it just so happens that,   you know, for historical reasons and sort 
of subtle nuanced reasons it happens to be   called logistic regression but we're going 
to be learning about basically this really   fantastic classification algorithm today 
and we're going to see that gradient descent   is a more general algorithm that lets us minimize 
or come close to minimizing a general function   that we can sort of slot in and use for the 
purposes of logistic regression. So first,   before we get started on what exactly is logistic 
regression, let's recall why are we doing this,   you know, why don't we just use the perceptron 
for everything because we have an algorithm   now? And something that we saw in lecture two is 
that the perceptron struggles with data that's   not linearly separable and the reality of life is 
that probably most data that you're interested in   is not linearly separable. So, for instance, 
we saw this example of some penguin data:   maybe we've collected data on the flipper length 
and body mass of penguins and we'd like to   classify the penguins into their different species 
and we can just immediately see that that's not   linearly separable and we saw some examples in 
the health data that we were thinking about,   the medical data that we were thinking about, last 
time as well. And, you know, technically speaking   you can use something like average perceptron 
to get around this but it's very ad hoc and   there's no sort of nice guarantee about how that's 
going to perform and in fact it can perform pretty   poorly with data that's not linearly separable, so 
we'd like to have something to deal with that. Now   let's notice another issue with perceptron that 
may or may not have come up in your discussions   already, which is that it sort of doesn't have 
a notion of uncertainty, like not just what's   our best guess right now but how good is that 
guess or what do we actually know in this problem   and that can be a real issue and this is actually 
pretty related to the linear separability.   So for instance, it is fall and I'm going to have 
to start thinking about whether I'm going to wear   a coat outside and especially if you're in a big 
building with a lot of apartments you really want   to make the right call before you get outside 
and, you know, you want to decide whether you   have a coat and in general I might think as the 
temperature increases I'm less likely to wear a   coat and maybe as the wind speed picks up I'm more 
likely to wear one and so I imagine that perhaps   I've collected a bunch of data on whether I've 
worn a coat or not in the past and I'd like to   make a nice little classifier for myself 
so I can decide whether to wear a coat   as the fall approaches in the future and 
perhaps it just so happens that I did   not collect a lot of data at these really key 
fall temperatures which are sort of in between   and so I might run perceptron but an 
issue with the perceptron is that sort   of any classifier that separates this data is 
equally good in its eyes. So this seems fine,   it would stop here, this is fine, but this is just 
as fine and somehow that seems a little bit off   I think if we think about this. You know, the 
reality is we don't have any data in this area and   we'd like to express that as a result we're not 
so certain about how we should make a decision in   this area. Now let's suppose that we've gone and 
collected more data. So this is just the same set   of data but now we've actually collected some data 
in that intermediate area and so maybe it turns   out that this is a good classifier, maybe I like 
to wear a coat when it's a little bit warmer out   just to, you know, maybe I run a little bit cold 
and so that's well and good but the reality is   I definitely don't wear a coat at some perfect 
temperature wind speed boundary. I think if you   if you took actual data of when I'm 
wearing a coat, the reality is sometimes   I'd be wearing a coat at slightly higher 
temperatures and sometimes I wouldn't,   sometimes I'd be wearing a coat at slightly higher 
wind speeds and sometimes I wouldn't, and this is   the way that a lot of things work. You know, in 
the medical example that we saw last week, you   know, it's not like there's this perfect division 
on any medical indicator of whether somebody has   heart disease or not, the reality is there's so 
much that we don't know about this person or that   we're not measuring or that might just be a little 
random and so we don't capture that with the   perceptron, we just say “hey here's a boundary” 
but we don't say “well actually some of this data,   there's a region where I'm just not really certain 
exactly what's going on and I'd like to be able   to capture that.” Okay so we're going to 
ask, you know, how can we do that, how can   we capture these different forms of uncertainty 
and deal with this not linearly separable data?   Okay so let's think about how we could capture 
uncertainty. Well let's start by thinking about   how we can't capture uncertainty. So let's 
imagine that I might plot my probability of   wearing a coat at different temperatures and so if 
I were absolutely, you know, 100% certain that I   would wear a coat below a certain temperature and 
absolutely 100% certain that I would not wear a   coat above a certain temperature I might put this. 
So the 1 probability represents that I'm 100%   certain that I will wear a coat in some range of 
temperatures and the 0 probability represents that   I'm certain that I won't wear a coat. And now, I 
could imagine if this is actually how I behaved,   that I could generate some data about, you 
know, what I'm going to do on a given day.   So particularly, you know, if it's a warm day, 
let's get a label of “am I wearing a coat,”   if it's a really warm day I'm not 
going to wear a coat because I have   0% probability of wearing a coat. Any warm 
day, there's 0% probability of wearing a coat   and now I could go to a colder day and now there's 
100% probability that I'm wearing a coat and so   it's very easy to generate these labels, it's 
just I look “am I wearing a coat or am I not,”   the probability is either 0 or 1. And when we look 
at this, we realize, at least for me, this is not   an accurate representation of how I wear a coat, 
a slightly more accurate representation would be   sort of a smoother version of this. It would be 
to say something like oh, you know, actually,   yes, there are some temperatures where 100% I'm 
wearing a coat, you know, if it's like, you know,   well below zero celsius, I'm definitely 
going to be wearing a coat, but there's   some range of temperatures, you know, maybe 
somewhere around 15 degrees celsius or so where   it's a little bit of a toss-up and I'd like to say 
that my probability of wearing a coat is really   between 0 and 1: sometimes I will and sometimes 
I won't and that that sort of varies smoothly   across the temperatures, that as the temperature 
increases, my probability of wearing a coat goes   down until eventually, you know, it gets to be 
super hot and I'm definitely not wearing a coat.   Now if we had this as the model of me wearing 
a coat and then I were to follow this exactly,   I were to generate whether I wear a coat based on 
these probabilities, I'm going to get something a   little different. When it's really hot out, again, 
I'm basically definitely wearing not wearing a   coat because the probability is almost exactly 
0 but as it gets colder, as we get into fall,   it starts to be the case that sometimes I will 
wear a coat and that probability is increasing as   it gets even colder until we get to a point where 
it's basically 1 and I'm always wearing a coat.   Okay so this seems like a sort of better 
representation of whether I'm wearing a coat   or not and so it'd be nice to see, you know, how 
do we make this shape, the shape of going from,   you know, a 100% probability of wearing a coat, 
you know, down to 0 but in sort of a smooth way?   Okay so it turns out this has a name, this is 
called the sigmoid or logistic function, sigmoid   because it looks sort of like an s. So let's just 
draw out the canonical sigmoid function, so sigma   here is a function: it takes as an input z, it 
is (1 / 1 + exp(-z)) so let's just draw that.   So we're going to draw it as a function 
of z, we're going to draw sigma(z),   and let's just check that we agree with this 
drawing. So in particular, as z gets very, very,   very low, like it gets very negative, 
we notice that -z gets really big,   so the exponential function of -z gets 
really big, and so the denominator gets   really big and so we're, you know, taking 1 over 
a really big number, that's going to be like 0.   Now on the other end, we're gonna have 
our z getting really, really, really big   so -z gets really, really, really negative so 
exponential of -z gets really close to 0 and   so we're dividing 1 over 1 plus something 
really close to 0, so that will be near 1.   So we see that all of this function is 
always going to be basically between   0 and 1 and we can ask what happens at the origin. 
So the origin, we'll have z = 0. Exponential of   0 is 1 and so we're gonna have 1 over 1 plus 1, 
that'll be 0.5. Okay so this seems to check out,   this is a function, but uh oh it's not quite 
the function that we drew at the top, there's a   few things that are a little bit different. You 
know, if we look at this original s curve that   we made that describes, you know, how I behave in 
different temperatures in terms of wearing a coat,   it's a little bit different than the s curve that 
we just made, this sigmoid logistic function,   and so we want to ask how can we go from this 
canonical sigmoid logistic function to this   one that describes, you know, the probability 
of wearing a coat in different temperatures?   And we can do that basically by just 
stretching and pulling and moving around   this bottom function. So in particular, something 
that we can do with this bottom function here is   we can say, okay, I could have z as an input, 
but if I just scale that input, like if I had   slightly different, you know, units for z or, 
you know, I just multiply it by some number,   some constant, let's call that theta, then 
I can sort of squish it or pull it out.   And so here what we've done is we've just 
said by multiplying by this constant we sort   of squished it but you could also pull it 
out so that it has sort of a bigger width.   Now another thing that we could do is, you know, 
and certainly that we want to do here is flip it,   but we can include that in theta as 
well. We could just have a negative theta   and so then we would get something that's flipped.   Now similarly we actually might want to move this 
around, we might want the point where it's 0.5 or   sort of the area where we're a little uncertain 
about what's going on to to be at a different   point along the z-axis and so in order to 
accomplish that we can just add a constant to z.   And so we can move this around there. And so 
in general, we can start from this canonical   sigmoid and again just by stretching it with theta 
and moving around its offset with theta naught,   we can get basically these different sigmoid 
functions in different spaces that behave,   you know, in these ways that we want. And so just 
going back up to our temperature example up here,   here we're thinking of x as a feature. So earlier 
I was just using z as some generic input so that   we didn't confuse it with our our data, but 
here we're thinking of x as our actual feature,   you know, it's our feature that we observe 
and perhaps some classification problem   and so let's describe this 
curve of probabilities as g(x)   and so then let's ask what would be the 
formula for g(x) based on what we just said?   Well g(x) would be some application of 
the sigma function, but to this sort of   stretched and offset version of x, so we'll take 
sigma and apply it to theta x plus theta naught   to get this g(x) and then all I'm going to do here 
is write exactly that formula. So I've just taken   sigma(z) which we defined below and I put 
in this new input theta x plus theta naught.   So this is just exactly the sigma function that we 
see at the bottom of the slide but with this input   theta x plus theta naught and so that's 
the g(x) that we're considering here,   that'll let us have this sort of smooth change 
instead of having this abrupt sort of one-zero   function, we're having a smooth change 
between, you know, always doing something   and always not doing something, like always 
wearing a coat and always not wearing a coat.   Okay, so at this point we've sort of described 
how you could take, you know, some smooth notion   of probabilities and turning it into sort of 
simulated data. Now what we really want to do   is the other way around, what we really want to do 
is have our real data and then learn this smooth   function of probabilities and we're going to do 
that in a moment, but first I just want to look at   what this simulation looks like in two dimensions. 
So we have a sense of what higher dimensions   look like, not just one dimension. Because in 
general, of course, our features aren't just   one dimensional, they could be higher dimensional, 
in fact they typically are higher dimensions. Okay   so let's take a look at that, so first recall, we 
just looked at one dimensions with one feature.   So in that case our g(x), this is just the g(x) 
that I just wrote on the previous slide, we saw   that it looks like this nice sort of s curve and 
that I could generate sort of my simulated data   by drawing, you know, “am I wearing a coat or 
not wearing a coat”, according to the appropriate   probability, so let's look at two features now. 
Okay so what's going to go on with two features?   Well I'm going to propose that we do a g(x) 
much like we did last time, that we have a sigma   function and we apply it now to, instead of 
theta x plus theta naught, theta transpose   x plus theta naught, which is starting to 
look pretty familiar if it wasn't already,   definitely theta transpose x plus theta is very 
familiar from linear classifiers at this point.   Now first of all, let's just do a dimensionality 
analysis to make sure this makes sense.   So remember theta transpose x 
is going to give us a scalar,   theta naught's a scalar, so the whole input to 
sigma is still a scalar which is good because   that is what we assume that sigma takes as 
input, just a one-dimensional scalar. So even   though x is high dimensional, the input to sigma 
is just one dimensional. Okay so that checks out.   Now let's think about what this actually looks 
like as a function. So remember in two dimensions   our feature set is going to be x_1 and 
x_2 and now we're going to draw our g(x),   that's going to be some function of both 
x_1 and x_2 and to figure out why this   shape is appropriate, let's start sort of 
looking at this function a little bit more.   So first of all, this looks very familiar again: 
theta transpose x plus theta naught. What do we   know about this function? Well theta transpose 
x plus theta naught, when that's equal to zero,   that's a line. Now at the same time, when that's 
equal to zero, we just said that g(x) equals 1/2,   so theta transpose x plus theta naught describes 
the line where g(x) is equal to 1/2 and in fact,   in general, theta transpose x plus theta naught 
equals to a constant describes a line where g(x)   has the same value across that line. Okay so 
this is sort of, this dividing line this is,   you know, this 1/2 line and in general this will 
be a hyperplane and in this case it's a line.   Okay so now let's ask ourselves, well what happens 
in this direction, as we go away from the line in   the direction of theta? So remember theta was 
sort of our normal vector, it still is here.   As x increases or gets more in the theta 
direction than this line, well as we do that   this dot product is going to increase, so 
this whole quantity is going to increase.   So the negative of that quantity 
is going to become very negative,   so the exponential is going to become very small, 
and so we're going to get something like 1 over 1   plus a very small number, and that'll be near 1. 
And so as we move x in the direction of theta,   we're getting numbers that are increasingly near 
1 and that's what we see in the plot. Conversely,   if we move x in the direction of negative 
theta, then we're going to have the opposite   thing occurring: we're going to get that 
exponential input being very small, we're gonna   get something—sorry that being very negative—and 
so the negative of that input is going to be very   large and so we're gonna get a very large value, 
we're gonna get 1 divided by a very large value,   and we'll get something near 0. And so it's still 
that “s” sort of function, but now you can see   that in higher dimensions it's like it's spread 
out along this theta transpose x plus theta naught   equals zero line. And then what does it look like 
to generate data according to these probabilities?   You know, suppose again that x_1 was temperature 
and x_2 was wind speed and so what I could do is   I could say, “okay, here's a particular day and it 
has maybe a low temperature and a high wind speed”   and then I could draw, you know, according to 
this probability, that's extremely near 1, am I   going to wear a coat so I'm going to probably wear 
a coat. I can make some draws along that boundary,   where the probability is really closer 
to something between 0 and 1—I mean   it's always between 0 and 1, but it'll be 
closer to, you know, maybe 0.5 or so—and   then sometimes I'll wear a coat and sometimes 
I won't. And then we can look at that sort of   lower corner where the temperature is very high 
and the wind speed is very low and there I see   my probability of wearing a code is near 0, and so 
in general when I flip a coin that has, you know,   probability or, you know, roll a dice or something 
that has probability near 0 of wearing a coat,   I'm not going to be wearing a coat most of 
the time. Okay so again what we've done here   is we've said, “hey let's imagine a world in 
which I decide whether or not to wear a coat   by, you know, randomly choosing whether to wear a 
coat according to this sort of sigmoid function,   here's how it can generate data.” But again, 
of course, the world that we live in is   one where we're given the data and then we want 
to learn what's a good sigmoid function that   fits that, what's a good, you know, smooth set of 
probabilities that describes that data that we're   seeing and so that's what we're going to do next, 
we're going to sort of flip this on its head. Okay, so when we do that, that 
is called one of two things.   One way we might call it is linear logistic 
classification, because it's classification:   we're trying to decide whether I wear a coat or 
not, that's a classification problem, that's a   binary classification problem. We used a logistic 
function, also known as a sigmoid function,   and we saw that there was that linear component, 
that theta transpose x plus theta naught. Now the   reality is when you go out into the world and you 
use the method that we're going to be describing,   it's called logistic regression. We're going 
to cover regression in a lot more detail in,   I think, next week and we've already 
talked about how regression is a little   bit different from classification, so don't 
get too hung up on this, it's just a name,   but this is definitely a classification algorithm. 
Okay so we have two big questions at this point:   one, how do we learn a classifier? Suppose we have 
a bunch of data, how do we learn these parameters,   theta and theta naught? And so the idea 
there will be we'll have a bunch of data,   you know, somebody will give us data, we'll have 
collected data, and then we're going to want to   learn theta and theta naught, and we've already 
seen here that theta and theta naught actually   describe this sort of curve of probabilities, this 
sigmoid function. Okay but that's not quite what   we've done in the past, right? What we've done 
in the past is we've said we've come up with some   data and then we come up with a way to predict on 
future data and so what we'd like to do too is to   say, “well once we have this theta and theta 
naught, how do we make predictions with it?”   You know, how do we say “hey, on this part of the 
space I'm going to predict that I'm going to wear   a coat, and in this part of the space, I'm going 
to predict that I'm not going to wear a coat?”   Okay so we need to answer both of these questions 
but I'm going to start by answering the prediction   question and then I'm going to come back to 
the “how do we learn a classifier” question.   So let's start by saying “hey suppose 
that I have a theta and theta naught,   what predictions does that imply?” So that's 
the question we're focusing on on this slide.   Okay so we need to decide whether we're going 
to predict +1 or predict the other option. And here's a way to think about it well, 
you know, if we lose just as much by being   wrong in either direction, we might say, “hey 
I'm going to predict +1 if it's more likely   that I'm wearing a coat, so if the probability 
of me wearing a coat is greater than 0.5.” Okay well what is that? Well we just said 
that the probability of me wearing a coat,   the probability of a +1 label, is sigma 
applied to theta transpose x plus theta naught   and so just another way to write the probability   of me wearing a coat being greater than 
0.5 is exactly this equation in sigma. Okay well just to see what this means, 
let's write this out. So just as we said   on the previous slides that a different way you 
could just write this out—mainly you just use   the definition of the sigma function, the sigmoid 
function—is we'll just write it it's 1 over 1 plus   exponential of negative the argument. So this 
is another completely equivalent way to write   that the probability is greater than 0.5 and 
now we can sort of solve for this. We can say,   “hey this is completely equivalent to 
the exponential here being less than 1.” That's how I can get this quantity to be greater 
than 0.5 and then you can recall facts about the   exponential function to see that this is 
completely equivalent to theta transpose x   plus theta naught being greater than 0. So we're 
going to predict that I'm going to wear a coat,   we're going to predict a label of +1, if theta 
transpose x plus theta naught is greater than 0   and we're going to predict the other label, in 
this case -1, if theta transpose x plus theta   naught is less than or equal to 0. That should 
sound super familiar, this is like exactly the   hypothesis class of linear classifiers that 
we have studied throughout this course so far.   We are looking at the classifiers that predict +1 
if you're greater than the theta transpose x plus   theta naught equals 0 line and -1 on the other 
side and -1 on the line. That's something that's   just exactly what we've looked at extensively and 
so what's new here, you know, if that's just the   same hypothesis class that we've looked at this 
whole time, what are we changing? Well there's   two super key things that we're changing here: 
one, something we have not gotten out in the   past and certainly you don't get for free 
from this hypothesis class is uncertainties.   So we're going to come up with a method that 
learns this theta and theta naught, but crucially   when we have our theta and theta naught, we can 
put them into the sigmoid function and get this   whole function, so not just a prediction of “am 
I wearing a coat or am I not wearing a coat,”   but a sense of “well, am I wearing a coat most 
of the time or am I definitely wearing a coat?”   These are the types of questions that we can 
ask and answer once we have this sigmoid.   But also, just because our hypothesis class 
is the same, doesn't mean that we're learning   it in the same way. In fact, we're going 
to be learning it in a very different way,   we're going to be using a totally 
new learning algorithm that we   haven't seen before and that learning 
algorithm, unlike our previous ones   is going to have nicer guarantees for 
when data is not linearly separable. And so we're going to see that going 
forward: what are those guarantees,   what is the performance when the data 
is not linearly separable? Which is   something that again we just really care about 
in practice and so this is just a new algorithm,   it will have new performance guarantees. It's just 
that it's operating on the same class, the same   class of classifiers that we had 
before this hypothesis class. Okay so at this point we have this sigmoid,   we have this hypothesis class, we have the 
ability to predict, but what we haven't yet done   is to say if we have a bunch of data, how do 
we learn this sigmoid, how do we get that out?   And so that's what we're going to look at 
next: how do we actually learn a classifier? Okay so let's look at that now. Now let's start by 
just getting a little intuition for how could this   work. You know, what's in some sense, you know, 
the way that we've been approaching learning a   classifier throughout this class is going to be 
the way that we approach it here, which is that we   kind of want to make a classifier that fits with 
the data, we want to choose a classifier that kind   of fits with the data, and so let's ask ourselves 
what does it mean to fit with the data here? So here in particular, you know, if we 
were looking at this wearing a coat example   over different temperatures, 
this looks like a pretty good   description of this data, a pretty 
good fit to this data, this curve,   because it says the probability of wearing 
a coat is high where I'm wearing a coat. And it says that the probability of wearing a coat 
is low, almost 0, where I'm not wearing a coat.   And it also says the probability of 
wearing a coat is somewhere in between,   where I'm sometimes wearing a coat and 
sometimes not wearing a coat, so there's   a lot that sounds pretty good about that. 
Conversely, if I was looking at this sigmoid,   this one seems pretty bad because it says the 
probability of wearing coat is really high   where I'm not wearing a coat and it says that 
the probability of wearing a coat is really low   where I am wearing a coat, so somehow 
this curve needs to align with the data   and, in particular, somehow the probability 
of the data should be high for the curve.   This is a way to describe this kind of alignment,   so what we're going to do is we're going to 
look at what's the probability of the data   under a particular curve. So for a particular 
theta and theta naught which describes one   of these curves, we're going to ask 
what's the probability of the data?   And then one way to go about doing things is to 
say well how can we maximize that probability? Okay so in order to accomplish this 
goal, first we need to write out   what's the probability of the 
data, so let's start by doing that. Okay now here I'm just going to make a point about 
probability, it's not a super important point   if you haven't taken probability, I'll 
just sort of say where this comes from,   but it's not something that we'll, in 
general, be expecting you to be on top of.   But this idea that if each of these data points, 
you know, given this curve is independent,   you know, we're just making a draw and it doesn't 
depend on the other data points, if that's true,   then the probability of a bunch of independent 
events is the product of their probability,   so that's the only thing that's happening here. 
Now I do want to describe this notation, this big,   you know, product symbol is what's happening 
here. So you're probably familiar, hopefully   you're familiar because we've already done it a 
lot with the summation symbol. So the summation   symbol tells us “hey there's a bunch of, you know, 
things that we're summing over and, in particular,   we're going to say let's take, you know, the thing 
after the summation symbol with i = 1 and add it   to the thing after the summation symbol with i = 
2, and the thing after the summation symbol with   i = 3 and so on.” And it's just the same thing 
with this product symbol: we're going to take   the thing after the product symbol for i = 1 and 
multiply it by the thing after the product symbol   for i = 2 and multiply it by the thing after the 
product symbol for i = 3 all the way up to n.   So we're just saying that the probability of all 
the data is the product over the probability of   the data points for each data point, so we 
go from data point 1 up to data point n.   Okay well what is the probability of a 
data point? We said that the probability of   wearing a coat, of having a +1 label, is this 
g function applied at x. So let's call g^(i)   exactly that function applied at x^(i), at the 
i-th data point. So this is the probability of   the i-th data point having a positive label of 
me wearing a coat. Now, that's not quite the   same thing as the probability of data point 
i, because some of them have negative labels   and so what we want to say is that if data point 
i has a positive label, its probability is g^(i),   if it has a negative label, well 
that's the the only other thing   that can happen so that probability is 1 - g^(i). So those are the only two options and so a 
way of expressing probability of data point i   is to say it's got g^(i) probability if 
y^(i) is +1and 1 - g^(i) in the other case   because there's only one other case. Okay and now all we're gonna do is 
just write this in a clever way. So let me explain what's happening here. So 
first of all this notation, where there's a 1   and then these brackets, you should evaluate 
that to be 1 if the thing inside the brackets   is true and 0 otherwise. So this quantity is 1 
if y^(i) is +1 and it's 0 if y^(i) is not +1. And so what's happening here, well if y^(i) is 
+1 then this evaluates to 1 and so effectively   we have a factor of g^(i). If y^(i) is not +1, 
then that exponent evaluates to 0 and anything   to 0 is just going to be 1, and so we don't have 
a factor of g^(i). Similarly, if y^(i) is not +1,   then this evaluates to 1 and 
we get a factor of 1 - g^(i).   If y^(i) is not +1—sorry if y^(i), yeah if y^(i) 
is not +1—then we get a factor of 1 - g^(i). If   y^(i) is +1 then that's false, so the exponent 
is 0 and so that term goes away. And so you just   wanna convince yourself that this is literally 
just a fancy way of writing the line above. Feel free to to take some time later and think 
about that if it's not immediately obvious   but it's really just a fancy way of writing 
this line and it's one that comes up a lot. Okay so one observation here is that every 
one of these probabilities is between 0 and 1   and as soon as you take a product of a 
bunch of things that are between 0 and 1,   they get really small really fast. And that's 
fine if you were working with all of the exact   numbers and the reality of our lives is that we 
work with computers and so computer precision is   a real thing that can trick us up and so taking 
these products can actually be a real issue in   terms of running into computer precision and 
running into sort of numerical instabilities,   and so purely, purely because this will run better 
on our computers, it helps to look at the log   of the probability instead of the probability. 
So whereas the probability is between 0 and 1,   the log of the probability will take these real 
values that can be much more spread out, and that   we can actually get sort of a handle on. And it's 
worth noting that if we were just trying to find   the theta and theta naught that maximize the 
probability, that'll be the exact same theta   and theta naught that maximize the log 
probability, because log is a monotonic   function and it doesn't change the ordering 
of these different theta and theta naughts.   Okay one more observation: in the past we haven't 
talked about things being better than other   things, we've talked about things being worse 
than other things, that's the idea of loss, right,   that we sort of don't want the worst thing, 
instead of we do want the best thing.   And so if we want to turn this into our loss 
framework, if we want to change this so it's   in our loss framework, we can just take 
the negative. So instead of looking at   something that we want to be more positive, we're 
now looking at something that we want to minimize.   We want to minimize the loss whereas we wanted to 
maximize the probability or the log probability. Okay now let's write this out. Now in 
order to write out the log probability,   I'm just going to take a log of 
the thing that's at the end there. So it feels like there's a lot going on here, 
but I really just took the log of the thing   at the end and put a minus in front because 
we're looking at the minus log probability.   So let's just go through briefly how this 
happened. So we started off with a product.   When we take the log of a 
product, we get a sum of logs. When we take the log of something with 
an exponent, that exponent comes down. And then we're left with log of g^(i). And so finally we have this whole quantity which 
we got by saying “hey this is the negative log   probability of the data, it depends on 
theta and theta naught through the g^(i)”   and so we can think of this as our loss. Now this, 
also if you think about it, looks very familiar.   So in particular, when we define training 
loss in the beginning we said that it was   basically the sum of losses on individual 
data points and that's what we have here.   Now that's not quite the definition we use 
though, we actually said it was the average,   so it was 1 / n times the sum, and so we can just 
add that here too. So here we have a sum, but from   the perspective of, you know, how do different 
theta and theta naught compare, it would be   completely equivalent to minimize the loss which 
is 1 / n times that sum. And so I'm just going to   go back here and say hey our loss can actually 
be negative: 1 / n times the log probability. It's sort of like the same trick 
we played with taking the log:   that it didn't change the orderings of things 
and it was convenient, and so that's all that's   happening here, it's just something that doesn't 
change the order of things and it's convenient. And now once we've done this, we have a 
loss that really fits into the framework   that we talked about from the beginning: 
it's 1 / n times the sum of losses on   individual data points, so we look at 
the loss of the i-th data point here. I realized that some of these i-s should be 
superscripts, I will correct that in the slides   before we share them, but apologies for that, they 
should be superscripts. Okay so now let's just   finally identify what is the loss on an individual 
data point. So we did all this work, we said “hey   we have something that looks like 1 / n times the 
sum of losses on individual data points,” so that   would be exactly this term for any particular 
i is the loss on an individual data point.   And so let's give that a name. A really natural 
name is negative log likelihood loss. It's a loss,   likelihood's basically another name for 
probability, we're looking at probabilities,   we took the log of the probabilities, that's 
that log of g, because again we're thinking of g   as a probability and we took the negative. 
So this is literally just describing   negative log likelihood loss and again we 
we have this sort of thing where we think   of g as being our guess and a as being our actual 
value and so writing that in this is what we get. So we're taking this negative log likelihood loss, 
so L_nll. Our guess is a little bit different than   in the past. In the past, our guess was something 
that was really binary, it could just take two   values. Here we're really letting it actually 
be a probability, something between 0 and 1. But our actual, what we actually observe, 
is we never actually observe a probability.   What you actually observe is “did I wear 
a coat or not.” I just did or did not   that's, you know, that's life. 
I'm not wearing like half a coat   and so the actual is something that 
really is just one of two values. Okay so what have we done here? Well we're trying 
to learn a classifier, we're trying to learn theta   and theta naught from this data, we have 
classification data and we want to learn this   sigmoid. And what we've here defined is a loss, 
a totally new loss that we hadn't seen before,   and just as before we kind of, you know, tried 
to find a value of theta and theta naught   that got as close to the lowest loss as we 
could on our training data. We're basically   going to do the same thing here, only a bit more 
systematically. In particular, here we have this   function of theta and theta naught and because 
it's actually pretty nice, it's continuous,   it's differentiable, we're going to be able to use 
really powerful optimization tools to actually try   to minimize that and theta and theta naught. So 
let's just make this a little bit more explicit. Okay so how are we learning this classifier? 
We're going to try to find parameter values   of theta and theta naught to minimize this sort 
of training error loss that we've talked about,   this average loss, specifically looking at 
negative log likelihood across the data. Okay so this is exactly what we defined on the 
previous slide. So we defined what it was to have   a negative log likelihood loss, we said it took a 
guess and an actual, where in this case the guess   is actually a probability: it's between 
0 and 1, and the way that we get that   is this sigma function. That gives us the 
probability of a particular x. Our actual is our   actually observed y, that's the y^(i). And then 
we take this average, this 1 / n sum i = 1 to n.   Okay so let's give this a name because we're 
going to be looking with working with it.   The sub lr, the subscript lr, that can stand for 
logistic regression because again we're doing this   classification, you might call linear logistic 
classification if you wanted to really describe   it in exact words that are pretty clear, but 
in general it's called logistic regression   and so let's call it logistic regression. And so 
we can say “hey, this is the loss that we want   to try to minimize, this j_lr.” Now because we're 
going to be trying to minimize it and we're going   to be taking sort of an optimization approach, 
how can we optimize this, how can we minimize this   over theta and theta naught, another word that 
you might use to describe this is an “objective,”   an optimization objective. It's sort of 
your objective to minimize that number. Now one last notational thing here: because we're 
going to be talking pretty quickly about general   optimization approaches that don't have to be 
for classification, that don't have to be for   this particular loss, that could be for really 
any function, it'll help us to have a way to talk   about really general inputs and so we're going 
to say this capital theta is the input to our   objective. In this particular case, it's equal to 
theta and theta naught together, it's all of our   parameters, but in general it's just, you know, 
it's some collection of inputs to an objective. Okay. Oh there's a question. Yes, going back 
to the last slide when we were looking at the   objective, why do we multiply all of the data 
points together to learn the classifier, the   probability? Yeah so great, so if I understand the 
question correctly, I think it's about the first   line here, which is in some sense: why is the 
probability of data equal to the product of the   probability of the individual data points? This 
is a fact of probability that is really outside   the scope of this course but you will find if you 
take a probability course, that the probability   of a bunch of independent events, so events 
that essentially don't depend on each other,   is the product of the probabilities of those 
events. And so here the events are “am I wearing   a coat on the day that is described by i with the 
features that are described by i,” “am I wearing   a coat on the day that is described by i +1,” etc. 
And even if those features were exactly the same,   we could still think of those as independent, 
because yes today might be 55 degrees and tomorrow   might be 55 degrees but I actually might, you 
know, just forget my coat one of those days or,   you know, something else random might happen. And 
so here we're just using this definition of the   probability of the data as being the product of 
the probabilities of the data points. It turns out   that that's the same as having the sum of 
the log probabilities of the data points,   so if you just really like sums then that's 
a really natural thing to do. But there's no   similarly nice way of thinking about the 
sum of the probabilities, it just turns out   that multiplication is the more natural thing 
when we're talking about probabilities here.   Hopefully that answers the question. Great. Okay so we have this loss. It's a function of the parameters, 
that's the thing we're trying to learn:   we're going to try to find the 
parameters that minimize this loss. And so again let's ask ourselves: well 
sort of what is this going to look like?   We're going to imagine that, much like in 
our previous algorithms, we're going to start   from some values theta, theta naught that probably 
aren't that good because in general we don't know   what we're doing when we start our algorithm, 
and then we're going to look at them and we're   going to say “hey for this particular, for this 
particular curve it's saying that there's a really   low probability for these times I wear my coat 
so it's not really good, so we should move it to   make those a higher probability. Hey we can make 
them an even higher probability, hey we could make   them an even higher probability.” And we just 
keep doing this until we get to a point where   the minuses, the times I'm not wearing my 
coat, have a low probability of wearing my   coat. The times where I am wearing my coat 
have a high probability of wearing my coat   and the times where we're not sure are somewhere 
in between. Now what's really cool about this   objective function and this loss, instead 
of the previous losses we were working with   like zero one loss and so on, is that it has so 
much more information about how do we do this,   about how do we move in that direction. What is 
the best direction to be moving? And in particular   the reason for that is that it is smooth and in 
particular it has a gradient: it's differentiable   and the derivative tells us basically a good 
direction to be moving and so what we're going to   do on the next slide is we're going to make that 
idea a bit more precise but we're going to talk   about more general functions and then we're going 
to come back to logistic regression. So basically   we have this linear logistic classification, this 
logistic regression set up this thing that we want   to minimize and now we're going to talk about a 
tool for minimizing functions that we can apply   to this, but it's going to be a much more general 
tool and that tool is called gradient descent. Okay so to talk about gradient descent, 
let's start by talking about a function.   So here we have inputs, theta_1 and theta_2, 
and this is just totally general inputs and a   totally general function f. You know, forget 
about logistic regression for the moment,   we just have some function and we would like to 
minimize it and it seems like we should. I feel   like I see a minimum on this and I would 
like my algorithm to see that minimum too.   Now another useful way to draw a function 
like this, is with what's known as a contour   plot. If you have ever looked at a topographic 
map, you have looked at a contour plot. So the   idea of a contour plot is just to say: let's 
draw lines at points that have the same height   in the original plot. And so this is a contour 
plot for exactly the f that's just above it   and in particular what we're seeing here is in 
that sort of valley there are little contours   and then they get bigger and bigger and bigger as 
we go out for things that have the same height. Okay, now how does a gradient help us 
if we're trying to minimize this? Well,   we can look at a particular point and if we 
look at the line that is tangent to the contour,   that tells us where to go 
to stay at the same height.   The whole idea of the contours is if we keep 
walking on it, we stay at exactly the same height.   But we don't want to stay at the same height, 
we want to drastically change our height.   The gradient tells us the direction 
to increase our height the most:   where could we go from where we are right now to 
most increase our height? That's the gradient.   Now the gradient has a precise 
mathematical description:   suppose our theta is in R^m. So here our theta is 
in R^2, but in general it could be m-dimensional.   The gradient is the vector of partial derivatives.   So the partial derivative of f with respect to the 
first variable, the partial derivative of f with   respect to the second variable, up to the partial 
derivative of f with respect to the n-th variable. Okay so that all sounds really good, but 
actually we didn't want to go in the direction   of increasing height, we want it to go in the 
direction of decreasing height, we want to get   to the lowest point. Now in order to do that, 
we won't go in the direction of the gradient,   we'll go in the direction of the negative 
gradient. So we just have to go in the exact   opposite direction to decrease the height as 
much as possible from where we are right now.   This is the idea of gradient descent, 
we're just going to keep doing it.   Okay so let's define gradient descent: this will 
be an algorithm, it takes a bunch of inputs,   but let's talk about those inputs as 
we go along, we'll explain them all. So first we have to start somewhere. So theta 
super something is going to tell us where   we are in a particular step. So 0 is where we 
start and we have to tell the algorithm where   we're going to start. That's a tough problem, 
initialization is a tough problem. You want to   have a good initialization, it helps the more that 
you have it, and that is just a really, you know,   difficult thing to do in general. Although we're 
going to see that for some types of problems,   at least it doesn't matter in terms of 
whether you get to the right answer,   it might matter in terms of how 
quickly you get to the right answer. Okay then we're just gonna have a counter for 
what round we're on. So this just says t = 0, we   haven't yet started, we're about to start. We're 
going to repeat these steps of trying to go lower. So now we're going to start our first round, 
so t = t + 1. And then here's the meat of it.   So what this is saying is let's take the gradient,   that's the last thing in this line, that's the 
thing that tells us what's the direction to go   for steepest descent, or let's take the steepest 
ascent which is the gradient and then when we   take the negative we're now looking at steepest 
descent. Let's multiply it by eta. So eta tells   us how far we're walking in this space and so 
it's called a step size parameter because it   tells us how far we're going in this direction 
of steepest descent. And so what we do is we   take our previous spot where we were before 
and we walk that far to a new spot theta^(t). Okay, so in this plot we've done that 
once: we started from our big red dot,   we followed this arrow to a new spot, but we 
can do it again. We can say, “okay now we're at   our theta^(1), let's calculate the gradient 
there. Let's go in the direction of the   negative gradient, let's do that by some step size 
eta, and let's end up at a new spot theta^(2).”   Okay at theta^(2), we'll calculate the gradient. 
We'll go in the direction of the negative   gradient, we'll do that by some step size eta 
and we'll get a theta^(3) and so on and so forth.   And so then the question just becomes when do 
we stop? And there are a few different ways   to stop. One is to say well, you know, think 
about what happens when we get to this valley   at the bottom. Eventually the valley, you 
know, we hope that we're going to be making   smaller and smaller and smaller step sizes as 
we get closer to this minimum and so eventually   we'll stop just when the change in step sizes is 
sufficiently small. So epsilon is an input to this   algorithm it just says, you know, when is too 
small to keep going. It could be something like   machine precision, because what's the point of 
going beyond machine precision. Even easier would   be if you're only going to report two decimal 
places, why bother keep going after that, so you   could do something like that, choose epsilon that 
way, but basically it'll tell you when to stop.   Now there are a few other ways that 
you could stop and return theta^(t).   One really important one is to have a max number 
of iterations. I actually highly recommend that   no matter what you do, no matter what algorithm 
you make, you always incorporate a max number of   iterations. If something goes horribly wrong 
in that algorithm, if you run out of time,   you just want to have stopped and so a 
really good idea here would be to say if   I reach some other stopping criterion, like 
the change becomes really small, I can stop   or if I run out of time because I'm running out 
of time in lab or homework or, you know, I'm just   running out of time in my analysis, I will also 
stop. Now there are other things that you could   do to stop besides the change in f, another one is 
to just say I'm not moving really much, that much   in theta, so that would be a way to stop. Also, 
at this optimum we expect the gradient to be zero,   you know, that's basically saying there's nowhere 
else to move to, you know, descend further   and so that would be a way to stop 
when the gradient is very small.   Okay so this is a very general algorithm, 
this is not specific to logistic regression,   this is not specific to loss 
minimization, this is just a way   to minimize or come very close to minimizing a 
general function f, but what's really cool about   it is that it has guarantees on quality 
and so let's check out those guarantees. Now before we describe those guarantees, 
it'll be helpful for us to have a particular   definition which is: what is convexity? So 
we're going to say that a function is convex   if we can look at its graph, pick out 
two points on that graph, make a line   segment between those points, and if that lies 
above or on the graph, always, it's convex. So here's a graph of a quadratic. So I say that a function on R^m is convex. What 
I mean by that is a function with inputs in R^m.   So here we're looking at a function, the 
horizontal axis is the input, the vertical   axis is the output. So what is m in this function? 
What is m? Could you put that in the chat? Great so I'm seeing a few different answers. So   if you're writing 1, you are correct. So 1, 
so m here, is the input to the function. So   we're saying: what is the dimensionality of 
the input? So that's what we mean by on R^m.   So in this particular case, this function has 
dimension 1 for input, that's the horizontal axis,   so that would be like z and then f(z) or theta and 
f(theta), f(theta) is the vertical axis. So here   we're looking at a function on one dimension, but 
then the function value is what's providing the   second dimension that we're seeing. Great. Okay 
so now, to ask if this is convex, we're gonna take   any two points, we're gonna draw the line segment 
between them, and we're gonna ask: is that line   segment above the graph? And here it is and it 
turns out that because this is quadratic, you   can show that for any two points that you take, 
the line segment will always be above the graph.   Okay now here's another question for you: 
here's a new function, is this function convex?   This is one for the chat again, 
is this a convex function? Okay lots of great answers including a no with 
tons of o's and a bunch of exclamation points,   so well done, it is not convex. And so 
here's the thing: yes you can find two points   for which the line is above the function, but it's 
not convex because there exists any two points   for which the line is sometimes below the function 
and so just because I could find these two points,   and you see in the middle that the black line 
is below the blue line, that's why it is not   convex and somebody said but not overall. Indeed 
there's some locally convex looking things but   the whole function is not convex. Okay but 
we can do this in higher dimensions too.   So this happens to be a fourth degree polynomial 
that is convex: no matter which lines I find,   it's always going to be above the function.   And again here's a question for the chat: is 
this function convex? Is this a convex function? You're all totally right. No it is not convex, 
frowny face. Indeed, again it's just a proof   of concept. Could you find an example of a 
line that goes below it and if you took these   two dips that it has and put a line between 
those dips, that would go below the function,   so this is not convex. Okay, so why are we talking 
about convexity? Because convex functions are   easier to minimize. So in particular, here's 
a theorem about gradient descent performance.   If, now in my assumptions I'm going to choose some 
epsilon tilde, this is basically going to ask me   how good am I, we'll see that again in the end. 
I'm going to assume f is sufficiently smooth and   convex: I can differentiate it, it's continuous in 
the right ways—there's a little bit more advanced   math than we're going to get into here—but 
it's a nice enough function and it's convex.   If it has at least one global optimum,   so what do I mean by global optimum? 
Let's look at this example up here:   a global optimum is a point such that its f 
value is at least as low as every other f value. A local optimum, which is the diamond 
here, is a point that locally that's true,   that if I just look at a small area around 
it, it's at least as low as everything else,   but it doesn't have to be the case that it's 
at least as low as absolutely everything else.   So global optimum is an example of a local 
optimum but not every local optimum has to   be a global optimum. An f has to have at least 
one global optimum for our theorem to hold. Now eta has to be sufficiently small. You're 
going to explore in your lab how a big eta   can have big problems, but 
if it's sufficiently small   then this theorem will hold. That doesn't mean 
it'll hold quickly but it means it will hold. Then if we run long enough, so if we 
don't cut off with our criteria too soon,   because we do have this termination criteria so 
we have to make sure that we run long enough,   you know, that we don't cut off with that too 
soon, then gradient descent will return a value   as close as we want to the global optimum, 
so that's what this epsilon tilde is saying:   I can choose however close I want to be and I 
can get that close again by running long enough. So this is pretty cool. This is telling us that, 
you know, under some conditions that just so   happen to hold for logistic regression, so that's 
nice, we can get this nice convergence property. Okay so let's make this a little 
bit more explicit, let's come back   to logistic regression here. Okay I 
normally I have the questions go to   discourse, but there is this question about 
optimum equals minimum. So when we talk about   an optimum, it can refer to a minimum, but 
it can also refer to a maximum and so here   I am specifically thinking of a minimum, so 
that's a great clarification, yes this should be,   we're thinking of minimum, because we're thinking 
of gradient descent performance. So yeah good   point, I'll probably just correct that in the 
slides because that is a good point. Okay cool. Okay so now let's look at gradient 
descent for logistic regression. Okay our loss, first of all, is differentiable. 
You'll notice that in gradient descent,   we take the gradient, we take the derivatives, 
and so you can't even apply it if you don't   have derivatives. And so here our loss is 
differentiable, so we can apply gradient descent,   so that's the first big thing. And then the 
second thing to notice which, again is a little   bit beyond the scope here, but I'll just tell you: 
it's convex. And so that's really nice, because   if all of our assumptions in the theorem hold, 
that means the theorem will hold. Now remember   there were some other assumptions in the theorem 
and we'll come back to that. But this means we   can apply gradient descent and we're going 
to get good performance a lot of the time.   So let's take a look at this, let's run gradient 
descent for our problem, so what will happen there   if we run gradient descent for our logistic 
regression problem, instead of just a generic f,   we'll put in the loss for logistic regression, 
we'll put in its derivatives, it's gradient,   and then everything else we have to choose. 
We still have to choose an initial value,   we still have to choose a step size, 
we still have to choose an epsilon,   but we can run gradient descent on our problem 
and so let's ask what that's going to look like.   So in particular, we had this “wear coat” 
problem from before: ”am I going to wear a   coat?”. This could be the data that I get: I run 
gradient descent and this is what I would get out.   In this particular case, I'm going to get 
out this nice, you know, I'm going to get   out theta and theta naught but that describes one 
of these sigmoid functions that says I have a high   probability of wearing a coat in one part of the 
temperature spectrum, in particular the cold part,   I have a low probability of wearing a coat in 
the hot part, and I'm not so sure in the middle.   Okay, suppose I have this data, so I'm deciding 
whether I'm going to wear a swimsuit today.   So in this case, I probably tend 
to wear a swimsuit when it's hotter   and not wear one in the middle of winter and 
maybe there are some days where I'm willing to   wear one but it's a little bit chilly. So how 
is the output of our learning algorithm going   to look different here? This is a question 
for the chat: what's going to look different   about this function that we return, this sigmoid 
relative to the one that we're seeing on the left? Okay so people are definitely saying 
it's going to be flipped horizontally. It's going to be offset, it's 
going to be one difference. There's actually potentially one more 
difference. So let's take a look at this.   So first of all, we see that it's flipped 
because I'm very likely to wear a swimsuit   in hot temperatures and very unlikely in low 
temperatures. Great and the last one is steepness.   Okay so we've got them all. Fantastic, so the 
first one we're seeing is that it's flipped,   the next one we're seeing is that it's offset, 
so the temperature where I start wearing a coat,   at least for me, is different than the 
temperature where I'm willing to wear a swimsuit,   it's much warmer where I might be willing to wear 
a swimsuit and then we see that in this particular   case it's a little steeper because there's not 
such a large range where I might do either. Okay so this is after running the entirety 
of gradient descent, so this is where I've   gone for a really, really long time. I've gotten 
to something where it's not changing too much,   this is how I got this theta and theta 
naught and I got this on this curve out.   Let's actually look at some of the steps 
along the way for a different problem. Okay so here's yet another problem that is very 
Boston relevant. So when it gets really cold   in the winter, I will often wear a base layer so, 
you know, wearing a layer of wool or fleece under   your pants can be fantastic if you're walking 
outside and it's really cold, I think it really   makes a difference. So something I might do 
is I might say when it's sufficiently cold,   I'm going to wear a base layer, I'm going to wear 
a sort of a second pair of trousers essentially   and when it's not that cold, I'm 
not going to bother with that.   Now something you'll notice here is that, in 
this particular data set, I don't have data   between the pluses and the minuses, so in the last 
two data sets we saw that there was an area where   sometimes I was wearing the coat and sometimes I 
wasn't, and here I just happened to not collect   data in that area and we said we wanted 
to express that we're just uncertain   about that, so let's see what it looks 
like when I run gradient descent. Okay so I'm going to initialize my gradient 
descent with a theta and a theta naught and   maybe that theta and theta naught look like this, 
so I'm not plotting the theta and theta naught,   I'm plotting what the sigmoid that depends on 
those theta and theta naught looks like, but every   theta and theta naught describes one of these 
functions. And now I'm going to take a step of   gradient descent and the thing to notice is, well, 
what's going to happen? Let's think this through.   My pluses right now have high 
probability and my minuses   have low probability but I could increase their 
probability even more by making this steeper,   and the whole idea of our loss is that we 
want to increase that probability and all   it's saying is that we want 
to increase the probability.   And so we could do that by making this steeper. So 
a step of gradient descent will make this steeper.   Okay well I could still increase the 
probability of this data set even more   by making this steeper and so I take another 
step of gradient descent and this gets steeper. And it's still the case that I could increase 
the probability of this data set even more   by making this steeper and so gradient 
descent will continue to make this steeper.   And it's still the case that I could increase 
this even more by making it steeper and I think   somebody has asked exactly the right question: 
“what if it gets too steep?” This is definitely   too steep, this doesn't express what we wanted 
to about this data, which is that we're uncertain   in between. So let's see what went wrong 
and how could we correct this. And so in   order to understand what went wrong, 
I'm going to draw the loss function. So now technically remember this loss, is a 
function of two things: it's a function of   theta and theta naught but let's just pretend 
that theta naught is always set to zero to make   this an easier thing to draw. So this is 
the loss function as a function of theta.   Now what's happening here on the left hand side 
makes sense: as theta gets more and more negative,   our loss goes up as well it should, because 
we shouldn't have a negative theta here,   that would be like flipping this around. 
But now, what we really want to say is that   as theta gets way too positive, the loss should 
also go up, because that's making it too steep.   But instead what's happening, it's a little bit 
hard to see because it's so small on the right   there, but what's actually happening is that the 
loss continues to go down as theta gets larger,   no matter how large theta gets. And again, 
that's for exactly the reason we just described:   as theta gets larger and larger, that's 
just making all the probabilities higher   and so absolutely that's going to make the loss, 
you know, get lower but we don't want that,   we don't want that to happen. We kind of want 
to express that somehow a humongous theta is   just too competent, like you would need a lot 
of data and information to get past that level   of confidence that we're not comfortable 
just asserting that level of confidence. And the way to do that, to assert 
that there's such a thing as being   too confident and we need data, like a lot 
of data to overcome, that is a regularizer.   Okay so let's see what we can do to get 
beyond this. There's an interesting question:   “is this what epsilon is for?” There's actually 
something called early stopping, which can also   play a form of regularization, but 
let's not get into that just now.   Actually we're just going to focus on what happens 
when we add a regularizer to deal with this,   but there are actually other forms of 
regularization than the one we're talking about   and we'll probably talk about some of those later 
on. Okay but we have one idea right here, which is   we want to say that too big theta is bad, and 
our way of saying that things are bad is our   loss function, and so how can we modify our 
loss function to incorporate this idea that a   theta that's too big is being too confident and 
it's bad? We can do that, again, by adding in   a regularizer. So here was the logistic 
regression loss that I showed you so far   and here's the full logistic regression 
loss with what's known as an L2 regularizer,   it's a particular type of regularizer. 
It basically has lambda, which is some   constant that's greater than equal to 0, times 
the size of theta. Now previously it's not   like we weren't showing logistic regression 
loss, we absolutely were, we just showed it   for lambda equals 0, so this is like a more 
general version of what we looked at before. Okay so this is known as a regularizer, or 
sometimes a penalty. It's something we add to a   loss and it's usually to express some information 
that we have that isn't purely expressed in the   loss. In this particular case, it's to express 
that we think that a really large theta   isn't natural, we shouldn't just naturally be 
that confident, that we need a lot of data to   overcome that. And so here, that's exactly what 
we're saying. We're saying if we have a really   large in magnitude theta, because 
magnitude's the relevant thing here,   that we really need a lot of data to choose 
that, we can't just choose that for fun   and so whenever we have a really large magnitude 
theta, that's going to increase our loss. In this particular case, this exactly penalizes 
being overly certain, because if we just increase   the magnitude of theta here without doing anything 
else, that's like saying we're more certain,   that's increasing that steepness that we saw. 
Now something that you can check is that the   subjective is still differentiable, 
so we can still run gradient descent,   and it's still convex, so we're going to 
have nice properties of gradient descent.   Now something you might ask is “why didn't our 
gradient descent theorem apply before, because it   didn't, there was no global optimum that it was 
going to because there was no global optimum.”   So in particular, when we looked at this lambda 
equals 0, because this is always decreasing,   there is no global optimum and that was one 
of the assumptions of our theorem. But now,   when we include our regularizer, there is a global 
optimum and so now our theorem will actually   apply and we can say something about actually 
getting to that global optimum which is nice. Now this seems preferable not only because the 
theorem applies, but also because we're going   to have this optimum that we can get to and we're 
not going to just choose this sort of arbitrarily   steep curve like we would have without it. Now, 
we can choose different lambdas and we'll get   different losses as a result. So in particular, 
something that you'll notice is that, as we   increase lambda (you can think of lambda sort of 
a trade-off between the loss and the penalty),   as we increase lambda, eventually if lambda's big 
enough, the penalty is what's going to dominate,   we're just going to have a quadratic. But if we 
decrease lambda, then the loss that's what's going   to dominate and you can really see here that 
we start from this loss with lambda equals 0,   we get something that looks more and more 
quadratic as we increase lambda. In fact,   if we kept doing that we would get something 
that really looks even more quadratic. Now a question that has come up a few different 
times now is how do we choose these hyper   parameters? You know, now we have this lambda 
that we have to choose, how could we possibly   choose it? And this certainly is not the only 
hyperparameter that we have encountered, we've   encountered, you know, how do you choose the size 
of the polynomial basis if you're using something   like polynomials for your features? You know, how 
do you choose, even in our random algorithm for   the very first day, how do you choose, you know, 
how many different things that you look at? The k,   that was the number of things that you look 
at. Hyperparameter selection, in general, is an   interesting question just as parameter selection 
is an interesting question. You can, you know,   think of it much like that, you know, it's called 
a hyperparameter for a reason, it's sort of like   just the next level up parameter. But one idea 
and one that's worth thinking about is to consider   a handful of possible values and compare them 
with cross-validation. I see there's a question. You just answered it. The question was: “how 
do you usually, how big do you usually do that   before the regularizer?” Awesome great, yeah 
and something that I would recommend too is,   so this is one way, just as we are talking about 
many learning algorithm, it's not the only way,   but it's certainly a very popular way is to 
like maybe try out a few orders of magnitude and   compare them with cross validation. Another thing 
that is definitely worth doing is checking the   values you get out of the end, like take a look at 
the loss function, see if it's a reasonable loss   function. Like in this case, if your lambda was 
such that it totally dominated, you know, if your   whole loss function was basically a big quadratic 
in the penalty and you lost all the interesting   stuff that was going on in the classification 
loss, the negative log likelihood loss,   you might be a little bit weirded out, you might 
think “hey am I really doing classification   or is it just the penalty that's driving 
everything?” And so it's really worth, you know,   once you've chosen a lambda, thinking about it and 
really, you know, plotting it, thinking about what   it means relative to your loss, figuring 
out what it really means in your problem. Okay so now we have all of the tools for logistic 
regression at our disposal: we have the loss,   the full loss including penalty that describes 
logistic regression, we can do classification   with it, we can run gradient descent applied 
to logistic regression, and we can put it all   together and this is something that we can 
run on data. And so let's just write that   out. I just want to emphasize how much this is 
literally just gradient descent with f given by   the logistic regression objective and so if, 
you know, gradient descent and, you know, the   logistic regression objective, you can just write 
this out and you should. You should check that   you can derive that from just those two things 
because this isn't like its own special thing,   this is just, you know, applying exactly that. So 
here is a logistic regression learning algorithm   based on gradient descent. I do want to emphasize 
that gradient descent isn't the only optimization   algorithm that you can use, in fact, we're going 
to see another one pretty shortly and have in our   reading and so this is logistic regression with 
gradient descent, it's not the only thing you   can do for the logistic regression objective. Okay 
so let's look at logistic regression with gradient   descent, it's going to take initializations, 
it's going to take a step size parameter,   and it's going to take a stopping parameter, 
hyperparameter epsilon and hyperparameter   eta. It doesn't take in the function f now 
because that's defined by logistic regression. So we initialize, we start at particular 
values of the parameters, the theta in it   and the theta naught in it. So 
those are just initial values.   We have our counter of what step 
that we're on, that's our t = 0. And we're going to repeat the steps of 
gradient descent just applied specifically   to logistic regression. Okay so this looks 
pretty crazy. I think if you just saw this,   you'd be like “whoa math, what happened?” 
But the reality is this is literally just you   take the logistic regression objective and you 
take the gradients and you just stick them in.   And so if this looks like a lot, literally 
the best thing you can do is just go home,   sit for a moment, and derive what are the 
gradients in logistic regression and just   make sure that this just pops out for you, that 
this is exactly what you get. So again it looks   like a lot, but it really, it really isn't: it 
is just the gradients for logistic regression.   And remember, we apply the penalty to 
theta to make sure that we weren't being   super overconfident which was respected 
by theta, which are expressed by theta,   and so we see that that penalty is showing up in 
our update for theta but not for theta naught. Okay and then we keep going until we're not 
changing that much and again every single   one of those stopping criteria that we considered 
was basically expressing not changing that much.   There's a question: shouldn't this take lambda as 
an input? You could think of lambda as an input,   either way. Yeah I think that's that's a 
totally reasonable thing. Here I happen to be   thinking of it as part of the logistic regression 
objective and I'm not bothering to put that in,   but yeah you could absolutely think of that as 
an input to this algorithm as well as a hyper   parameter specifically for the algorithm. 
Okay so here we have our stopping criteria,   this is a particular stopping criteria just based 
on the objective function not changing too much,   and then finally we return the theta, the 
thetas, that we have learned along the way.   Okay so again, just gradient descent applied 
to our logistic regression objective, that's   all that's happening here. It again looks like a 
lot, but it's not when you go in and derive it,   this should be just that application. 
But the reason that I've written it out,   is that there is something that is 
interesting, that we want to understand here   and that's the following: so let's look at one 
of these steps. Let's look at, for instance,   the theta t step what's happening in the theta^(t) 
step. So in the theta^(t) step, what we do   is we take all of our data points and we 
calculate something whatever's in that sum   and we do it for every data point and then 
we sum up over all the data points, and   once we have done that sum over absolutely all of 
the data points, we make an update to theta^(t).   Now this is pretty different from what 
we did with the perceptron, for instance,   where we did something to one of the data 
points and then we immediately made an update,   and there's something to be said for that. Like 
let's say that you have a data set that's in the   millions, that's in the billions, then this 
is just going to take a really long time,   you're going to sum up over all of those millions 
or billions of data points and then finally you're   allowed to move in gradient descent. And it feels 
like if you look at one data point or even like 10   or 20 or 100, you probably already have some 
information about a good direction to move,   that you don't need all of the data points to 
figure out a roughly good direction to move,   to go in the direction roughly of the 
gradient and to minimize your function.   And so actually, there's a different learning 
algorithm. So we talked about gradient   descent as an optimization algorithm, there's a 
different optimization algorithm called stochastic   gradient descent, that's in your reading 
for this week, that takes advantage of that,   that actually says we only need to 
look at a few data points before we   make a move or one data point even. There's 
a question: so for the gradient to step why,   isn't there a 2 lambda theta. Oh maybe that 
might actually just be a typo, yeah I think so. Um I'll check that out, but yeah I agree with 
you, good taking of derivatives on the fly.   Great, somebody was gonna find one at 
some point and so I think we've done well   in going this far but that's fantastic. Okay 
great, so these are exactly these gradients,   even with a two in here, my point holds about 
you have to make this sum up over all of the data   and that's just going to take a really long 
time and we might be able to be more efficient,   we might be able to take steps 
before we have to do all of that,   and that's the idea of stochastic gradient 
descent, to be able to start making those steps   before we've seen the whole data. Okay, so just 
to recap: what have we done today? We introduced   logistic regression. If you are going to use a 
classification algorithm in person, in reality,   this is really one that you will really 
use. This is going to come up and, you know,   probably any data analysis that you do in your 
life, there's a good chance that this will be in   it. We saw how gradient descent can let us use 
the logistic regression objective and optimize   it and get very very close to an optimum. 
It turns out in order to have an optimum,   we have to be careful and we might have 
to have a penalty. So we covered all that   today and we're going to move on to, I believe, 
regression on next week, so I'll see you then. 

Okay it's MIT time, so let's go ahead and get 
started. So let's just recall: previously we've   been talking really about classification in 
this class, you know, we had a little moment   where we were talking about how this fits into 
broader machine learning, how classification is   a particular type of supervised learning, and 
regression is a different type of supervised   learning, but today we're actually going to really 
get into the details and actually do something   with regression, which is new at this point. So 
again, we've been talking about classification,   in particular, last time we really started talking 
about logistic regression which, despite the name,   is a form of classifier, it's an algorithm for 
classification, so we also called it linear   logistic classification and we talked about 
how we can get good parameters with this using   gradient descent, which is a much more 
general algorithm: it can be applied to   optimization in general, it doesn't just have 
to be classification and in fact, hopefully,   we'll spend some time seeing that today. So 
today, we're going to focus on regression,   just as in classification, within 
classification, we did a lot with linear   classification, here we're going 
to do a lot with linear regression.   We're going to see that we can add a penalty or a 
regularizer just as we did in classification, in   particular, in logistic regression last time and 
we're going to get what's called ridge regression   and then we're going to talk about how even though 
it looks like we can solve these problems exactly,   we still actually care about things like gradient 
descent and hopefully we'll have some time to   talk about stochastic gradient descent. Okay, so 
that's the plan for today, so let's just start   by recalling some of the nomenclature and notation 
that we had around classification because we're   going to find sort of the analogues in regression. 
So in classification, we had a bunch of data.   So we could think of data as being plural and 
one particular data point you might call a datum   and so let's look at our data as just an example. 
You know, last time we were talking about,   you know, when would I wear a coat and I 
might decide that based on the temperature.   You know, here maybe it's the other way: when 
will I wear, you know, a coat? As I get to a   higher temperature, I'm not wearing a coat, as 
I'm in a lower temperature, I am wearing a coat   and so we can think of each of these data points 
as having sort of two parts: there's the feature   vector, the set of x's. In this particular 
case, it's a really simple feature vector,   it's just one dimensional, d is one dimension, 
but in general, I could have a lot more features   as part of my x and then each feature for my 
particular data point has a label. And in this   case, it's -1 or it's +1: so -1, I'm not wearing 
a coat, +1, I am wearing a coat and so now we want   to find a hypothesis, a hypothesis ideally that's 
a good hypothesis that we can use to predict,   you know, when I'm going to wear a coat in the 
future. And so right now, there's no hypothesis   in this plot, there's nothing that tells me, for 
every possible x value, what might I predict,   why might I predict here. But I can add one: 
here's a hypothesis. Again, a hypothesis is just   a function that tells me, for every possible 
x value, what's a prediction? It doesn't have   to be a good function, it's just any function, 
although of course I'd like to find a good one,   one that will perform well when I get test 
data in the future, when I get new data.   Now, we talked about a lot of different losses, 
actually, that you could use for classification.   So one is 0-1 loss: I incur a loss if I'm wrong, 
I don't incur a loss if I'm right. I might care   about asymmetric loss: I might be wrong in 
different ways and that might matter to me.   I might look at negative log likelihood loss, 
which is something that we looked at last time,   and what's cool about this is it gives us a way 
to deal with data points that sort of overlap,   that aren't perfectly linearly separable, and 
this notion of uncertainty that we talked about.   And now, an example of classification, because 
classification, you know, could be pretty general,   but an example of classification is linear 
classification. So in linear classification,   we have our set of features and we find a 
hyperplane. So that's a pretty simple concept in   just one dimension, it'll just be sort of a point 
and a direction, but so here we're looking now at   a linear classifier that says on one side of my 
hyperplane I predict +1, on the other side of   my hyperplane I predict -1. Now, something that 
we've certainly seen is that I could actually   have a much higher dimensional complex feature 
space and so that when I plot things in a lower   dimension maybe it doesn't look linear, but if 
I'm doing linear classification, there is some   high dimensional space with all of my features 
where I'm ultimately going to have this linear   classifier. Okay, so now let's start developing 
regression and linear regression sort of in   comparison to this classification development that 
we've had. So here, let's think about regression.   So in regression, you know, a lot of 
things are actually pretty similar:   I'm still going to have a bunch of data points. 
So here, let's look at a regression problem now   where I have a single feature and I have my label,   y, and the big difference here is that my label 
can take more values, it doesn't have to be just   -1 or +1 or just a discrete set of values which 
is usually what we're assuming in classification   or, in particular, it doesn't have to be sort 
of this unordered discrete set of values,   it can be more general. So here, our feature 
vectors are really kind of unchanged, you know,   we still have a d-dimensional feature vector in 
this cartoon, it's just one-dimensional (d is 1),   but our label is the thing that's really 
different. Again, we're letting it   take more values essentially and really be, sort 
of, continuously valued. So as an example of this,   suppose I am looking at the temperature. So again, 
let's assume x_1 is the temperature, but now I'm   looking at my air conditioning bill, and so maybe 
if it gets really, really, really hot, I expect   that if I have air conditioning, my bill will go 
up and so I might expect this kind of relationship   between those and that's not something where 
it's just going to be “is my air conditioning   on or not,” I actually might be spending more for 
air conditioning as the average temperature goes   up. Okay, so again I'm going to want a hypothesis 
to express: how do I predict on new data? And just   as in classification, I can have good hypotheses 
and I can have bad hypotheses. So a hypothesis   is really just going to be some function from 
my features to my new set of labels, my labels   here again being anything in the reals, but of 
course again I'd like a good one, I'd like one   that helps me predict on new data points in a good 
way. So here's an example of a hypothesis: it is a   function that goes from my features to my labels. 
Okay, but again, as we said, we want it to be good   and just as in classification we came up with this 
notion of loss to help us decide, you know, what's   good and what's bad, we're gonna have notions 
of loss here. Now just as in classification,   there can be actually many losses that we might 
use, it's going to be the case here in regression   that there are many losses that we can use but 
the loss that we're going to focus on is what's   known as “squared error loss.” So remember the 
idea of a loss, as we've discussed it before,   is that we have a guess, let's call it g, and 
an actual value, let's call it a, and we want to   compare those two in some way. So for instance, 
let's look at this particular data point here   in our graph. So there is an actual value for 
that data point, we can just look at its y value   in particular. So here, when we say actual value, 
we're really focusing on the labels. Now we can   also compare to our guess and our guess is where 
that regression line is at this particular set   of x's and so if we drew a little segment from our 
guess to our actual here, it might look like this,   this little red line. So it goes from our guess, 
which is in blue, to our actual, which is in   black, and then of course to actually get the loss 
we're gonna square the length of that little line   and then we could do this at all the other data 
points to get something like our training error   that we've talked about and just sum up over all 
of that. And so again, the loss is just a way to   ask about, you know, how well are we doing, how 
good is this hypothesis, what does it mean to do   well in this problem? And it's worth reflecting, 
you know, why did we have to come up with a new   loss? Why couldn't we just use all of these losses 
that we had already developed for classification?   Well let's look at them. So 0-1 loss: well one 
version of this says that I have to predict   1 or -1 and that's definitely not going to 
be true in regression, but even if I think   I'm asking “is my prediction exactly the same as 
my actual,” that's just not generally going to be   true in regression. If I look at any of these 
points in the graph on the right hand side and   the figure on the right hand side, none of the 
actuals are the same as the guesses but I could   still actually have a really good hypothesis 
even if that's not exactly right. Like if I'm   predicting my air conditioning bill and I'm off 
by one cent, I don't think that's so bad, and this   squared error loss is expressing that if I'm off 
by one cent, that's not so bad, but if I'm off by   a hundred dollars, that actually might be pretty 
bad and I might not be too happy about that.   So I wanna express that. I also don't want 
asymmetric loss for the same reason, that's gonna   have the same problems as 0-1 loss. And negative 
log likelihood assumes that my guesses are between   0 and 1, which again doesn't make sense for this 
regression problem. There are plenty of times that   my air conditioning bill will not be between 
0 and 1, I wouldn't generally expect it to.   Okay. And again, just as we had this notion 
of a linear classifier, we will have a notion   of a linear regressor and so in particular, in 
linear classification, we said “hey, there's this   hyperplane, so that's sort of just one dimension 
lower than this full space of the x's and y's” and   what we did with the hyperplane classification, 
was we used it to divide: to say on one side,   we're going to have our our -1s, on the other 
side we're gonna have our +1s and then we   actually used it in a slightly more involved way 
when we were talking about logistic regression,   because actually, sort of, just even the magnitude 
of the theta could tell us something about our   uncertainty. And so here, though what we're going 
to be doing with our hyperplane is we're going   to be saying “hey we can use this hyperplane to 
actually make our predictions.” And so here, if   we have a hyperplane for linear regression, that's 
actually our hypothesis and now we're going to say   our guesses are along that hyperplane. Also a big 
difference here is that, at least for this type of   loss, we're not really specifying a direction, 
we're just looking at that hyperplane itself.   Okay, and we can also do this 
in higher dimensions so we,   you know, we've certainly talked about this with 
classification and classification we could have,   you know, two-dimensional features, higher 
dimensional features, and now our hyperplane is   just dividing things in that higher dimensional 
space and same thing with regression. So in,   regression we can have two-dimensional features 
or higher dimensional features, so here's x+1 and   x_2, and then what's gonna happen is we're going 
to have a hyperplane prediction over this and that   will give us our y predictions and something 
that's sort of worth pointing out here, and   we'll come back to this too, is that the nature, 
even though we're calling it linear regression,   we called it linear classification, there's not 
what we might think of as a line going on here,   like always a one-dimensional thing, like this 
is definitely a hyperplane that we're using   for our function, for our predictions and 
not just, you know, always one dimension.   Okay, so that's classification, that's the general 
idea of regression, and this example of linear   regression, and so let's dive deeper into this. So 
in particular, let's actually write out what is,   what are the hypotheses that we're thinking of. So 
we have these hypotheses for linear classification   and now we're talking about hypotheses for linear 
regression and hopefully this looks somewhat   familiar, you know, like we're still talking about 
hyperplanes, we still have the same geometries   that we've been working on this whole time, that 
we've developed, that hopefully you've gotten more   and more intuition on as you've gone along in this 
course, and the big difference here is that we're   not looking at the sign on one side or the other 
here, we're actually just looking at the values   on this hyperplane and that's, you know, 
that's very different from classification.   Okay, so again we want to talk about training 
error, just as we talked about training error   for classification, and actually if you look 
back in your notes from either this lecture   or just from the readings, you'll see that we 
actually defined training error very generally   in the beginning of this course, we said 
that it was just this notion of average loss   over the training data points and that's not 
specific to any particular thing that we might be   doing, classification or regression or whatever. 
We're just saying for some supervised learning   problem—so we have a bunch of features and a 
label—what's the loss between our guess, which   comes from the hypothesis, and our actual, which 
comes from the label? And so this is very general,   this is exactly what we saw before and what's 
going to be different here is the choice of loss   and the choice of the hypothesis. And so 
if we put in exactly the hypothesis above,   this linear regression hypothesis, and we put in 
this squared error loss that we just described,   we're going to get out a new training error, 
something you might call mean squared error   or average mean. A root mean squared error 
is sometimes you might take the root,   but basically what's happening is we're taking 
a mean, we're taking an average (that's the 1 /   n times the sum) and then there's this squared 
difference between our hypothesis and our y. Okay so this is really going to be the 
loss that we're focusing on a lot today,   the thing that we're trying to do something with 
and in particular, you know, just as in the past,   we basically wanted to try to get this as low as 
possible. We've always wanted to try to get a loss   and then choose some parameters that make it as 
low as possible, we're going to be doing that   again today. Now, we're going to do some sort of 
mathematical trickery occasionally and this is   one of those points and this is a mathematical 
trick that you have already seen in the context   of classification but we're going to do 
it again here: if we augment our features   with a feature that is just one, like 
the last x, we're just going to make   the value 1, then that's equivalent 
to having had both our theta   and our offset theta naught. So basically it's a 
way to get rid of theta naught, while still having   the same expressive power. So you can look back 
to what we did with classification, we're going   to be doing the same thing here, but effectively 
what this lets us do is it lets us get rid of   that theta naught term while still expressing 
all of the same linear regressors as before,   so this is just sort of convenient for 
math but it doesn't really change anything. So we're just doing exactly the same 
thing we did in the line above but   making this little trick where we have 
augmented our features with feature “1”. Okay and we're going to be dealing with a lot 
of matrices and vectors that are all multiplying   each other and I just think a really great unit 
test as you go along whenever you're doing this,   when you're coding, when you're writing an 
equation, whatever, is to check the dimensions,   you know, check that this makes sense. And 
so let's just start doing that: let's notice   that theta, we've always said is a column 
vector, it's d by 1 so theta transpose is 1 by d.   Same thing with x: it's a column vector, 
it's d by 1. And so these are two things   that we can multiply by each other, we're 
going to get out a 1 by 1, a scalar.   Now y^(i) itself is also a scalar, it's just 
a label, it's some number and so together when   we can take, you know, we could subtract one 
of these from the other and we'll get a scalar   and that's something we can take the square of. 
So this is all legit, this is all things that,   you know, we can do within the realms of sort of 
matrix vector multiplication and so we're good.   Now it's worth noting that this whole time, 
we never had to write theta transpose x^(i),   we could have just written x^(i) transpose 
theta. You should definitely convince yourself   that those are just exactly the same thing, 
they're two ways of writing the dot product,   certainly the dimensions work out: x^(i) transpose 
is going to be 1 by d, theta is d by 1, y^(i) is   still 1 by 1, and so this whole thing gives you 
a scalar, you take the square, and you're good.   Okay so now, we're going to make this 
observation that what we're doing here   is we're adding up a bunch of things 
squared. In fact, n different things,   little n (those are the things in the sum). And 
so for adding up n different things and each one,   we're squaring them, that's basically the norm 
of some vector, the square norm of some vector.   And so we can ask ourselves: what is 
that vector? What is the vector that   we're taking the norm of here? And in order 
to establish that, we're going to add in a   little bit of new notation. Namely, we're 
going to collect all of our x's in a matrix.   So let's call—so this is the vector we're 
interested in, the one whose i component   is this thing I'm highlighting here—so let's 
call our our new matrix X tilde. So you can   see that X tilde collects all of the information 
that we have on x, so it has the n data points   as its rows and the d dimensions as its column, so 
the whole thing is going to be an n by d matrix.   Why are we calling it X tilde with a tilde on 
top and not just X? Well to sort of emphasize   that we've slightly changed things. You know, this 
whole time we've been talking about a data point   as being a column vector so it goes from, you 
know, x_1 to x_d, and here we're slightly changing   things by making it a row vector. So this is a 
really typical thing to do in linear regression,   this is going to help us get out formulas that 
will look familiar if you look at for instance   Wikipedia or, you know, another textbook, 
and so this is just a way to write all the   x's together in a matrix. And we can do the 
same thing with y, so just as we collected   all of the x's in this matrix and we had the 
each data point as essentially a row, that's   what we're doing with y here too. So Y tilde has 
each data point as a row, each label as a row.   And so now, the observation that you want 
to make is that, you know, we said we were   interested—so this y is just down by one—we said 
we were interested in the vector whose elements,   whose i element, was this guy, was 
this thing that we're highlighting,   that's exactly this vector. If that's not 
immediately obvious to you, that's totally fine,   but you should check it. You should check. I 
mean, everything we do with a matrix vector   multiplication is just something you can write 
down, it's just effectively a quick way of writing   a sum and so you can just double check that on 
your own. Now that is something that you can   and should do, I'm just going to do a unit test 
on our understanding to make sure that this even   seems plausible. So I'm not going to prove it 
right here, but I'm going to just make sure it   seems plausible and in particular what I mean 
by that is I'm going to check the dimensions.   So we just said that X tildes is n by d, 
we know that theta is d by 1, and so when   I multiply them together I'm going to get an n 
by 1 vector. Y: We just said Y tilde is n by 1,   so I can take an n by 1 vector and subtract 
another n by 1 vector, that's allowable,   and I'm going to get out an n by 1 vector and 
that's exactly what we said we wanted. We wanted   an n long vector whose elements we were going 
to square and add up. And so finally, again   you should definitely check that you agree with 
this if this isn't immediately obvious and I don't   mean you have to do it right now just sit down 
later and do it, this thing that we're trying   to minimize, this function of theta that we're 
trying to minimize, can be expressed as the norm   of this vector squared times 1 / n. And something 
that we've already seen and used in this class is   that another way that you can write the norm 
of a vector is as a dot product with itself.   Okay so here's a question for you: so here's a 
couple of vectors and I want to take their dot   product, so I'm going to have to put a transpose 
on something, do I put it on the left term or   do I put it on the right term? Where do I put 
the transpose? This is a question for the chat. Lots of great answers coming in. I'm 
just going to give time for a few more   so people have time to think about it. Okay great, so totally nailing it. 
It's the first term, or the left term,   and this actually is really important and let's 
see why. So first of all, let's notice that   this—remember we just said that this X 
tilde theta minus Y tilde is an n by 1   vector, it's a column vector—and so X tilde 
theta minus Y tilde transpose is 1 by n   and so when I take that whole thing together 
I get a dot product and I get a scalar.   If I did it in the other direction, if I 
put the transpose on the second term—you   should check for yourself, just do this kind 
of dimensionality analysis—you're going to   get an n by n matrix. That's not a scalar, 
that's not a value that we can minimize,   that's just something that's totally different. 
And here, what we're looking for is a single   value, something that's a function of theta that 
returns a real value, that we interpret as a loss,   that we interpret as sort of how poorly we're 
doing, and then we want to minimize that value.   And so the direction here really matters, it's 
really important that that transpose be on the   first term here. Now I also want to note that 
that's because each of these is an n by 1 vector.   If we were working with row vectors instead of 
column vectors, the answer wouldn't be the same. Okay, so basically what we did on the slide was 
just a bunch of math to write this loss in a   fancy way but really it's really just the loss of 
the start, this training error, the squared error   loss, across our data and what we want to do with 
it is we want to find some really good choices   of theta that will make this as low as possible, 
that will give us a really good loss aka a really   low loss and that, you know, hopefully the goal is 
that will perform well on future data as a result. Okay so our goal is to minimize this loss. 
So all I'm doing here is writing one of the   many ways that we wrote the loss on the 
previous slide, so this is just exactly   this linear regression loss 
from the previous slide.   And we want to find a theta that's going to 
minimize this, that's going to make a really   nice low value of this loss because this tells 
us how poorly we're doing on our training data. Now here's something that's really cool and really 
different from everything we've done so far.   So everything we've done so far, we've 
kind of had to beat around the bush   because we couldn't actually minimize our 
losses. So, when we talked about 0-1 loss,   when we talked about perceptron, when we talked 
about that random algorithm from the very first   lecture, we were sort of thinking about “here 
are some ways that we can try to get the loss low   but, you know, they have pluses and minuses and 
we certainly can't guarantee that we'll get the   lowest loss.” Even when we talked about logistic 
regression and gradient descent, we come close to   the lowest possible loss. In fact, we could get 
guarantees we saw that we might get arbitrarily   close, as close as we want, the lowest possible 
loss. And here's what's different in this case:   in many cases we can just find the lowest possible 
loss. We don't have to do any of these sort of   iterative procedures, we can just write it down, 
and that's pretty exciting. So let's do that. Now   why can we do that? Why is this special in a way 
that those other cases weren't? And the reason is   that this is a quadratic function of theta and 
quadratic functions are really nice. If I gave   you this quadratic function, you can just sort of 
find, as we're going to see in a moment, the point   that minimizes that function. Now, the caveat 
is: just because I have a quadratic function,   doesn't mean that I can find a unique global 
minimum. So let's just see some quick examples   of why that won't always be the case. Here is a 
quadratic function. There is no global minimum.   You can just always get lower no matter where you 
are by just going farther out. So this would be   a case where there would not be a unique global 
minimum and I would not be able to minimize this.   Here's a slightly more nuanced case: there 
are global minima but there's just a lot of   them and they're all equally good from the 
perspective of minimizing this function,   but there's just not a unique one and this is 
going to turn out to have some really practical   and important consequences in linear regression 
and so it's just important to keep in mind that,   just because I have a quadratic function, doesn't 
mean that there definitely is always a unique   global minimum, but when there is things are 
good and life is good as we'll see in a moment. Okay so what we'd like is this first picture. We'd 
like to say, “hey when there is a unique global   minimum, can we just find it? Can we just write it 
down?” and that's exactly what we're about to do. Okay so I'm going to get rid of these pictures 
and in a moment we're going to find this unique   minimizing point. And the thing I want to notice 
with these pictures before we do get rid of them   is that: how will this happen? Well the 
function will be uniquely minimized at a   point if the gradient at that point is zero and 
if the function sort of curves up everywhere.   There's a precise way to talk about this if 
you take a linear algebra course. Basically   the idea is that the matrix of second derivatives 
is positive definite but for the moment let's just   say it curves up, that's basically the intuition 
anyway that if this function curves up everywhere,   then there will be this unique global minimum 
and we can sort of see that from these pictures. Okay so now let's do this, let's take the 
gradient and let's set it to 0. This is   something that you're probably familiar with from 
your calculus courses and the big difference here   is that we might be doing it in higher dimensions 
perhaps than you have worked on in the past.   Okay so we're going to take the gradient. My 
first question to you, and again this is one for   the chat. Oh before I asked my question, I missed 
that there was a question. Sorry let's cover that. How can [unintelligible] change if 
the labels are multi-dimensional or a   complex number? Oops sorry could you just say that 
again? And let me turn my volume up a little bit. [unintelligible] change if the labels are 
or if y are like common [unintelligible].   I'm sorry, I am just not hearing super 
well, I'm not sure if that's just me. Are the other staff able to 
hear that well? Or is it…   Okay, other people can't hear it as well 
either, maybe just typing the question   would be okay? Feel free to like copy 
and paste or something. Sorry about that. Okay great how will the loss function change   if the labels are multi-dimensional or if they 
are complex numbers? Okay so the first one, the   labels being multidimensional, let's just first 
think for a moment like how would this arise?   Basically I would be trying to predict multiple 
things, right, so I would be trying to predict,   you know, what is my air conditioning bill, what 
is my gas bill, and maybe a few other things.   And so if, in fact, everything that I am trying to 
predict is like a bill, then what I might do is I   might add up my losses because that'll be saying 
“hey these are all literally numerical losses,   they are money that is out of my pocket” and so I 
want to add them up and I want to say, across all   of them, the total loss is the amount that I spent 
this month or, you know, something like that.   Now sometimes, though, what I'm predicting might 
be pretty different, you know, I'm predicting   my air conditioning bill but I'm also predicting 
how many days a week I'm gonna get sick. This is   more difficult because it's not immediately 
obvious how these compare, right, when I want   to say what is my loss, well these both seem like 
losses: I don't want to get sick too many days of   the week and I also don't want to spend too much 
on my air conditioning bill, but I'm going to have   to do some real thought to think about how do I 
add them together and how do I put them on the   same scale. And so I'll have to think about, you 
know, maybe there should be some factor that tells   me how many days a week converts into units of 
dollars but it won't be as obvious how I make that   loss comparison, but I can and it's something that 
I can do. For instance, by making that comparison   in terms of y being complex numbers, I guess it 
would again depend and I think this is the real   underlying answer here on what am I trying 
to do. Because really all the losses,   at the end of the day, is saying “hey I have this 
practical problem, how much do I lose by getting   things wrong?” and that doesn't have to be squared 
error loss, it doesn't have to be purely additive,   it could be something else, but it's really 
how much do I lose and so you really want to   ask yourself for any particular practical problem 
that comes along, what's a natural way to express   how much I lose by getting things wrong and 
then once you've done that, that just tells   you your loss and so this is sort of an example 
of how you might do that. But the real answer is:   ask yourself for the problem that you have how to 
do that. Great. Okay, so now let's come back to my   question here. Gradients, we've done them before. 
In theory, we know something about them. What is   the dimension of this gradient if I—remember our 
theta is d dimensional, it's a d by 1 vector—and   so if I'm going to take this gradient, what is 
its dimension? This is a question for the chat. Getting some good answers here, keep them coming. I see some people putting question marks 
but you're getting it right so well done. Okay so the answer that many of you are getting 
here is d by 1. So remember the idea of the   gradient is that I'm in the theta space and it's a 
vector that tells me where to point in that theta   space and so it's got to be the same dimension as 
theta. So this is going to be a d by 1 vector and,   in general, I think this is a good way to 
think about these things. Don't feel like   you're just going in and you have all these, 
you know, quantities you're just memorizing   their dimensions. I think a better way to think 
about it is what is it doing? What's the point?   What are we trying to accomplish with these 
quantities? And then hopefully that will point   you to what is the appropriate dimension. Okay so 
we should get a d by 1 vector out of doing this   and so here is another case where what I 
have done is I have taken the gradient.   You could interpret that in the way that we 
talked about last time, it's the, you know,   for each element of theta (theta_1, theta_2, 
theta_3, etc.), it's the partial derivative in   each of those elements. But it turns out there's 
a nice neat matrix form that I can write this in.   Again, I'm going to ask you, you know, in your own 
time later on, if this is not immediately obvious   to you, to work this out and I think there are at 
least two things that will be really helpful to   do: one, check the case where n is 1 and d is 1. 
That's the case where everything here is a scalar.   We're totally forgetting about all this matrix 
vector stuff that's going on, you just have an   x times x theta minus y. Everything is a scalar. 
And you should find that this basically reduces to   familiar concepts from calculus: you're just 
taking a regular derivative, you're just taking,   you know, a regular square of a value and so you 
should find that this is something where you can   just do the usual derivative calculations that 
you would do and this should look familiar once   you've done that. The second piece of advice 
is you can check the vector elements. Again,   all that this is expressing, all that these, 
you know, matrix vector multiplications,   vector multiplications are expressing is this fast 
and easy way to express these sums and derivatives   that are happening. And so you can just check from 
first principles, you know, what are all of the   elements, the gradient? How can you express that 
in terms of sums? And then check that it's exactly   what these vectors are expressing. And 
so again, if this is unclear to you,   I think the first exercise, checking the n = 1, 
will give you intuition about why are we getting   these values, you know, and help them seem 
to look familiar and if you just want to   convince yourself that these are the right matrix 
vector operations, check those vector elements. Okay again let's just do our own little unit 
testing to make sure that this even makes sense.   So again, when we do this sort of dimensionality 
analysis, we're not proving that these two things   are equal, you know, there are plenty of things 
that we could multiply that would satisfy these   dimensions but would not be equal, but we're 
checking that this even makes sense and this is   always a great idea, again, in your code and your 
math and everything to do this kind of double   check. Okay so we know that X tilde is 
n by d, we know that theta is d by 1,   and we know that Y tilde is n by 1 
and actually, technically speaking,   we've already sort of gone through this and 
we know that X tilde theta minus Y tilde   is an n by 1 vector and here 
this is just reiterating that. So, X tilde transpose is d by n because it's 
just the transpose of X tilde. So we take a   d by n matrix times an n by 1 vector, those n’s 
agree, so these are two things we can multiply.   The outer values are d by 1 and so we get exactly 
a d by 1 vector out and so that looks good, that   looks appropriate, that's a nice little check that 
this looks okay. Okay now the point here, though,   wasn't just to find the gradient. So if we were 
doing gradient descent we would just, you know,   find this gradient and then sort of move in the 
direction of this gradient. We're trying to find   the point where the gradient is equal to zero and 
solve for that directly, so that's a little bit   different. So here what we're gonna do is we're 
gonna set this equal to zero and solve for theta. Okay so let's go ahead and do that. So all we're 
doing in this line is noticing that (2 / n) is   just a constant and so we can multiply both sides 
by (n / 2) and the zero is not affected and that   goes away. And then we're taking that X tilde 
transpose and just putting it into both terms.   Okay, so now once we have this equation, well, 
we can just take one of the terms and put it   on the other side. So all we did was we said we 
have minus something equals zero. So we can just   bring that something over to the other side. 
It's like we added X tilde transpose Y tilde   to both sides. And that's basically all 
we're doing in any of these manipulations   is multiplying by one and, you know, adding 
zero and so that's what we've done here. Now let's notice that if we multiply both sides by 
the same constant, this equality would not change.   If we multiply both sides by the same 
matrix, this equality will not change,   and so let's choose a particularly 
good choice of a matrix to multiply by.   Here, we've multiplied both sides 
by X tilde transpose X tilde,   all of that inverted. So we just took the equation 
that we already had and pre-multiplied it by the   same thing on both sides and so that's not 
going to change that this equality is true. Now why was this a good choice of matrix to 
multiply by? Well we ultimately want something   of the form theta equals something, because we’re 
trying to solve for theta. The whole idea here was   to find where is the gradient, what choice of 
theta is there at which the gradient is equal   to zero? And so we're hoping to end up with an 
equation that looks like theta equals something   and so we notice that now we have something 
of the form: a matrix inverse times a matrix.   And the way that matrix inverses work is that 
when you multiply a matrix inverse times a matrix   or a matrix times its matrix inverse, either 
direction, those are just going to cancel out.   And so those cancel out on the left-hand 
side, we're just left with theta,   and we're left with a formula on the right-hand 
side and crucially it doesn't have theta in it,   so we've solved for theta at this point. Okay now we said that this is a unique minimizer.   If the gradient is equal 
to zero, so we check that. It's only a unique minimizer if 
the function curves up though. Now again, this is beyond the linear algebra 
we're going to get into in this class.   So I'll just tell you: you 
can check this kind of thing,   that it curves up, with what's known 
as the matrix of second derivatives.   Do not feel that you have to derive this, 
just don't worry about this. The thing that   I want to tell you though is is that, if the 
function curves up, this matrix is invertible.   You'll notice this matrix, up to a constant, 
is exactly what we need to invert to get theta.   And so if there is a unique minimizer, then that 
formula for theta we just talked about will be   fine because you can take that inverse. If there's 
not, you won't be able to take that inverse. And so this is how you see it in 
the formula is what happens with   that inverse and we're going 
to talk in just a moment about   what's actually going on, why would this ever not 
be invertible from a more intuitive perspective. Okay let's, before we do that though, let's just 
back up for a second and assume for the moment   that we have a problem where we actually can't 
invert that and we can solve for theta. So what is   this going to look like? What it's going to look 
like is: you're going to start off with some data.   So we said that for our data, we have a 
bunch of features and a bunch of labels.   So here's an example of a one-dimensional 
feature space with a bunch of labels.   Once I have this, this defines a loss function j   and it's a quadratic in theta. Now here, remember 
we played this trick where we made the offset be   actually the last element of the theta vector. So 
here we really do have a theta_1 and an offset,   we're just calling it theta_2, which is a 
little bit confusing but again it's just   because we did this trick where we said that our 
last feature is 1 and so our offset goes to the n.   It's annoying but that's what's happening here. So 
really, theta is two dimensional. We've got our,   you know, sort of slope effectively. I mean, 
you know, we saw it's not exactly the slope   but the thing that's defining it and the offset 
and so that defines a j and now what's really   cool is that we don't have to do gradient 
descent or anything in this, we can just go   straight to the optimum with our formula that 
we just came up with, this function of theta. That defines our particular   theta. That defines a particular line because 
it defines a particular theta and so now we can   draw that line and, in particular, this is 
actually the line that we get by doing this   minimization here. So in this particular 
case, I just randomly generated some data,   I plugged in exactly this matrix, and this is the 
line I got out and it looks pretty good, right: it   looks like it minimizes the loss in the sense that 
the predictions are near the training data points. Okay so that's what this formula is doing, 
it's letting us pick out just a particular   value of the theta which is effectively 
giving us a line and then we're plotting   that line. Now let me just show the same 
thing but in a two-dimensional feature space. So in a two-dimensional feature space, it's going 
to be much the same: we're going to start off   with a bunch of data, but now that data 
has x_1 values and x_2 values and labels. Now that gives us a J(theta), but in this case, 
because we have to say what's the direction   in the x_1 space and the x_2 space and 
the offset, that's a three-dimensional   theta vector and so I can't really draw this. 
I mean we could get clever and sort of try to   find ways to look at it, but I can't really draw 
this function so I'm not going to but I'm going to   note that it exists. And still, whether I can 
draw it or not, this formula is going to give us   the formula for the particular 
theta that's going to optimize it   and so now once I have that theta 
that corresponds to a hyperplane—   this is the hyperplane it corresponds 
to—and I can just draw that hyperplane. And so again, I just want to emphasize 
that despite the name linear regression,   just as in classification, linear 
here really means hyperplane.   So if we look at what does this prediction look 
like when we have this two-dimensional feature   space, it's not a one-dimensional line, 
it's this higher dimensional hyperplane. Okay so this is what everything looks like when, 
in fact, we have a nice function that curves up   and this unique minimizer but now 
let's talk about what can go wrong.   Why would I not have a nice function that curves 
up and a unique minimizer? And it turns out   this is stuff that really happens in practice 
and so it's definitely worth being aware of. Okay so before we say what goes wrong, 
let's just recap we just said “hey, what   what we want to happen and what often 
happens is that I have a bunch of data,   you know, it spans some nice values in x_1 
and x_2 or whatever my feature space is,   and it has some nice labels and then I can use 
that theta formula that we just came up with   on the previous page to find this best hyperplane, 
that's one that minimizes the squared error loss.” Okay when would that go wrong? What would go 
wrong is if there wasn't a best hyperplane.   So here's an example: suppose I want to predict 
my air conditioning bill and so what I did was I   measured the temperature outside for my 
first feature, so let's call that x_1,   and then for my second feature I decided to also 
express the temperature outside but in Celsius   instead of Fahrenheit. So x_1 is the temperature 
in Fahrenheit, x_2 is the temperature in Celsius.   They encode basically exactly the same information 
and so no matter what my y's are, the x_1 and x_2   are going to be on this perfect line with each 
other and so now if I go in to fit a hyperplane,   which we just said was sort of the whole 
thing that we're doing in this case and the   whole thing that this theta solution is solving 
for, well this is a hyperplane that fits this.   But this is a hyper plane that fits it just 
as well and in fact you can just imagine   if you just take this hyperplane and sort of 
rotate it around this data, you're just going   to keep getting hyper planes that all fit this 
data exactly as well from squared error laws. And so there isn't a unique best hyperplane, in 
fact, there are infinity best hyperplanes that   are all equally good from the perspective 
of squared error laws. When this happens,   that inverse, that matrix inverse, will not exist. 
Okay, I think there's a question. Hoping my mic   works now. Oh great. Yeah okay, so does theta 
have the same geometrical interpretation here   as it has before? So is it still perpendicular to 
the hyperplane? Yes exactly, that's exactly right,   and you're thinking about it absolutely perfectly. 
So theta is still perpendicular to the hyperplane,   that is how it describes the hyperplane. So we've 
got our theta that is, sort of, again, exactly the   normal vector to the hyperplane and our theta 
naught is still the offset, just as before. The   only difference is what we do with the hyperplane. 
So the geometric intuition that this question   has is absolutely perfect and absolutely worth 
enforcing and the only key difference is that,   before, once we had the hyperplane, we said “oh 
we'll predict +1 on one side and -1 on the other   side.” In this case, we're using the hyperplane 
directly to do our predictions. We're saying,   “hey if I have a new x value, I'm going to look 
at what is the exact value of the hyperplane at   that x value and then I'm going to say that's 
my prediction.” But yes, all of the geometric   intuition is exactly the same and it's just what 
we do with the hyperplane. So perfect, great.   Okay so and that's true of all these hyperplanes 
too. The only issue is just that all of these   hyperplanes from the perspective of our squared 
error loss are equally good and because of that,   that matrix won't be invertible and we won't 
have this unique sort of best hyperplane and so,   you know, you couldn't just use this formula 
directly that we just derived because   you would get some sort of error, you know, you 
would say your linear algebra system or whatever   you're using and Python would say, you know, “hey 
there's an error, this isn't invertible.” Now,   there are ways to get around that, but essentially 
what we're going to have to do, is we're going to   have to decide “okay, between these infinity of 
hyperplanes, how do we decide which was the one   that we're going to report? Which is the one that 
we want?” And that's the question that we'll be   answering very shortly. Before we do that, before 
we find the answer, before we discuss an answer,   let's also talk about more problems that 
can arise. Now a related problem is:   yes, sometimes you might have a unique 
best hyper plane but on a technicality.   In particular, imagine that you had this exact 
situation I just set up but my data was a little   bit noisier. So here's an example: suppose 
that I'm interested in my air conditioning   bill and so I have a thermometer that measures 
the temperature right outside my apartment and   it says “hey here's the temperature, you know, say 
in Fahrenheit” and then I have another thermometer   that's like one door down from that, you 
know, and it also measures the temperature.   Now in this case, technically speaking, there 
just is going to be a little bit of noise,   because no two thermometers are absolutely 
perfect, they're gonna, you know, have a little   bit of noise and it'll be a little bit off and 
I think that you will find that actually you can   invert this matrix, you will not get an error from 
your program that says “hey matrix not invertible”   but the problem hasn't gone away, you know, yes 
technically your program will let you do it,   it will let you find this best theta, but should 
it? You know, you still have so many of these   planes that, yes technically, one of them is very 
slightly better than the other one, but they're   all sort of effectively very, very close in how 
well they perform and we should think a little   bit more about which one really is the one that we 
want to report, not just put it into the formula   and go and not think. In general, I think that's a 
great meta point for machine learning and probably   for life: never just put stuff in the formula and 
go and not think about it, always think about it.   Always think about what's going on and this is a 
case where I think you will benefit by doing that.   Okay so we still have this question. Even 
if we could invert this matrix, we still   kind of want to ask ourselves which hyperplane 
should we really report? Maybe we should think   about it a little bit more and we will so no 
worries, we will address this problem shortly.   Before I do that, again, I want to emphasize 
how much this can come up very easily. So back   when we talked about features, this actually 
even came up, I think, in a student question,   we had redundant features. If you think about one 
hot encoding, one of our features is exactly a   function of our other features, and so this just 
immediately will come up then, right? You have   that one of your features is exactly a function of 
your other features and so there won't be a unique   hyper plane because of that. Another issue is that 
when you measure a lot of things in real life,   they often tend to be correlated, so they often 
tend to have this problem. Just because if you   measure enough things and they're all sort of 
around each other and you're thinking about the   same problem, it's just very common that this 
would come up. Another way that this can arise   is that if you just have a lot of 
feature dimensions. Now unfortunately,   again, we can't really plot things above two 
dimensions so this is hard for me to plot,   but I will give you a plot and then you're going 
to explore this so much more in the lab and so   basically let me start with a very simple plot. 
Suppose that I have a one-dimensional feature   and I have a single data point. I think you will 
all agree that there are many hyperplanes that   go through this data point, here are some of 
them. Again, there's an infinity of hyperplanes   that all go through this data point. If I had a 
two-dimensional feature and I had two data points,   there would be an infinity of hyperplanes that 
would go through those two data points. If I   had a thousand dimensions of features and a 
thousand data points, there would still be an   infinity of hyperplanes that go through those data 
points. So why would I have a thousand features,   why would I have more than a thousand features? 
Well this can often arise in biology, in genetics.   It can also often arise, you know, if you think 
about the sort of tricks that we've been playing,   right? Like think back to polynomial features. 
We just made a whole ton of features out of   nowhere by using polynomial features so that's 
an easy way to get into a really high dimensional   space and you're going to have exactly this 
problem that we just discussed that there's this   infinity of hyperplanes and therefore no matrix 
inverse if you get too high dimensional. So again,   you're going to be exploring that so much more 
in the lab and you're gonna get to see, you know,   sort of what are the consequences, but it's 
really coming back down to this exact principle. Okay so fundamentally, for all of these reasons, 
we need to think how do we choose among the   hyperplanes. Well we've already developed a 
little intuition about this. So you actually had   a problem and I believe it was homework three 
that asked you to say “oh, what are the most   influential of your features” and those are the 
ones with sort of the biggest magnitude, right?   And so something that we could say is, well, if we 
don't really know what's going on, if there's not   really a good way to distinguish between different 
hyperplanes, maybe we should choose or we should   prefer the theta values that are near zero unless 
there's a strong reason not to from our data. We   shouldn't go and say yes, this feature really 
matters when we just don't have the evidence   to support that. This is very much like what we 
did with logistic regression, right? With logistic   regression, we said “hey, you know, if left to our 
own devices, if we didn't change anything, we make   our thetas have humongous magnitudes” and that 
doesn't express the uncertainty that we really   have. We wanted to instead choose thetas that were 
near zero unless we had a strong reason to believe   that that shouldn't be the case. And so we're 
going to do exactly the same thing here: we're   going to express a preference for the theta as 
being near zero unless we have a strong reason not   to and we're actually going to do it in exactly 
the same way that we did for logistic regression. Okay so we have all of these problems. We think 
that all of them are fundamentally about choosing   a particular hyperplane among many that seem 
roughly equivalent. We're going to express   a preference for our theta components being near 
zero, how can we do that? Well we can regularize. So we're going to take linear 
regression, everything we just did,   and we're going to add a penalty, a penalty to big 
thetas just like we did with logistic regression.   When we do that, we get something 
that is called ridge regression   if we do it in the exact same way. So here, 
this was our existing linear regression   training loss, so I'm just rewriting it, it was 
something that we had on the previous slides.   This is one of the many forms that we wrote it in, 
and now we're going to form the ridge regression   optimization objective—so right now, what we have 
is just our regular linear regression optimization   objective—by adding this square penalty. Okay, so 
a few points about this: one, this is not the only   penalty that one could add that would penalize 
very large theta. This is a particular choice of   a penalty that penalizes large theta. It's the 
same one that we had before, but actually there   are people who absolutely use other versions, 
this is just going to be the one that we focus on. Two, notice that when lambda is equal to 
zero, we get out exactly the original,   the original optimization 
objective for linear regression.   Here, we're going to set lambda greater 
than equal to zero—or sorry, in particular,   greater than zero—to make sure that there is some 
penalty associated with having a large lambda.   So, sorry, with having a large theta. 
So basically when we have a large theta,   we incur more loss, we say that that's bad, we 
have things that we add on that make that bad.   Incidentally, and this is a question from the 
chat, for the chat, what happens if lambda is less   than zero? Why, you know, should we sometimes set 
lambda less than zero, would that be a good idea? Great okay, so everybody notices, one, 
that if I have lambda less than zero,   then I can decrease my optimization objective 
by choosing thetas that are larger and larger,   and so in fact, I could get the optimization 
objective arbitrarily small just by choosing   larger and larger theta and it doesn't even 
matter what direction, just any direction,   and so this is sort of useless because all 
it is is telling me “here is a penalty for   really large theta, it doesn't matter how small 
lambda is in magnitude as long as it's negative,   I'm just preferring an extremely 
large and arbitrarily large theta,   one that sort of goes off without bound.” And 
so that's not really useful to us. What we   really want to suggest is that there's really 
a trade-off here, that we get some loss from   not being near the data. That's what's happening 
with the first part of this optimization objective   and we get some loss from just having 
a really large theta and so that lambda   is just controlling the trade-off and again, 
because we don't really know exactly how to make   that trade-off, we'll often use something 
like cross-validation as we said before. Okay so now, what's going to happen is, just 
as we just solved for the theta that is the   unique global minimizer when that unique global 
minimizer exists, we're now going to do it here   for the special case with no offset. The reason 
for that is entirely because it's easy to write   down, you can absolutely do this for the case 
with an offset, that could be a fun exercise,   but I just want to say we're only going to 
do it for this. Okay there's a question. Yeah a couple questions about the regularization 
term. Yeah. So why would we want the theta   magnitude to be near zero? Since if you have a 
greater magnitude that means the feature is more   impactful in the prediction. Yeah exactly, so 
let's go back to the previous slide, so the   question is, you know, why do we want that theta 
magnitude near zero? So on the previous slide,   we said “hey, if in fact I have a super impactful 
feature and my data can really pick it up,   then linear regression, forget about 
regularizers, is going to pick that up.”   But here are some cases where I run linear 
regression and there are a bunch of totally   equivalent things, there are a bunch of 
totally equivalent settings of theta,   that all give me the exact same results, 
that all give me the exact same loss, or   in the noisy case, they give me almost exactly 
the same loss. Like it's not exactly the same,   but it's so small as to be, you know, not really 
something that's going on. I can't really tell,   you know, I can't really tell. And so what we're 
saying is that, in these cases where there is   literally basically nothing in the data 
to distinguish between different thetas,   then I should probably set the thetas as 
close to zero as possible because I don't   want to erroneously say “yeah these were super 
impactful features.” Because I totally agree   with this question that if I had a really large 
theta, I would probably think to myself those are   really impactful features, those really matter, 
but if I don't have evidence to support that,   then I don't want to say it. And so when I don't 
have evidence to support that, I would prefer   to choose the theta that is near zero and so 
that's what we're doing here. We're saying “hey,   when there isn't evidence in my data to 
support choosing a really big theta, one that   demonstrates that this future has a really big 
impact, then I would prefer to choose that theta   near zero,” and so that's what's happening now in 
this penalty is we're saying there's a trade-off,   you know, if there's a lot of evidence in my data 
to support there being a really impactful feature,   then that should come out in the first part of 
this optimization objective, but if there's not   a lot and everything's kind of the same, then 
the second part of this optimization objective   kicks in and it says “hey let's prefer that theta 
is there near zero.” Great. Another question is:   so, how does the ridge regression prevent the 
coefficients from essentially being noise in   theta, it's like very small but not zero? Oh so it 
doesn't, yeah, so this is an interesting question.   So let's say that I ran this. All it's gonna do is 
it's going to ask my theta values to move towards   zero, in general, and it's going to prefer ones 
that are near zero when everything is equivalent,   but it's not necessarily going to take them 
all the way to zero and so actually if that's   something you're interested in—and people 
absolutely are, so there are some people   who want to say “hey I want to pick a subset of 
my features that are the ones that matter and   I would like something to very clearly tell me 
which ones are included and which ones aren't”.   People often use what's known as an L1 penalty, 
instead of an L2 penalty. This is something   called a “lasso” instead of the ridge regression, 
there are other ways to do it too and this will   make some of the theta values just exactly equal 
to zero and then you can just look at the non-zero   ones and do something with those. So absolutely, 
this is something that people do and ridge   regression does not do it, it just tends to move 
things smaller, it won't set them exactly to zero. Cool okay. So now we have this 
regularizer. We said that we're   gonna take lambda greater than zero or equal 
to zero, if we're not using the regularizer,   but we're gonna use the regularizer here and 
we're not gonna take lambda less than zero   because we saw some things that could go really 
wrong there. And now we're just going to solve   for theta in the same way that we did before: 
we're going to find the optimal theta and we're   going to ignore the offset purely for mathematical 
convenience. Okay, so if we do that, this is   our ridge regression, this is—I'm literally just 
rewriting the same thing but without the offset—so   this is our whole optimization objective. We 
also saw that it could be written this way,   completely equivalent, this is just sort of the 
manipulations we did on that previous slide. And now we're going to optimize this. 
So in particular, we're going to say,   “let's take the gradient and set it to zero.” 
Now something that I did on the previous slide,   something that [unintelligible] do, and 
something that I'm doing here, is a little bit,   I'm pulling a fast one on you. So when I 
say set to zero, what's the dimension of   zero here? So this is one for the chat. Good stuff. Okay great yes, you're all totally 
nailing it. It's d by 1. It's got to be the same   as the gradient because we couldn't put this 
equality here, but it's a little bit of a fast   one, right? I mean usually when we say set to 
zero, we think that we mean the number is zero   and this is something that happened in the notes, 
that happened when we set it to zero on the other   slide, and it's happening here. What we really 
mean is that we're setting it to the vector zero,   that is to say we're saying each element of 
this gradient we're setting to zero separately   and so that's worth keeping in mind that this 
might not be the zero that you're familiar with,   this is actually the vector zero that is a column 
vector, that's a d by 1 column vector. So that's   a little bit of a fast one, make sure you go 
back in the notes and and feel comfortable   with what's really going on there. Okay, so 
we're going to set this gradient to zero. We're not going to go through the derivation again 
this time, it's in the notes and also you can just   redo everything that we did on the previous slide 
and you're going to get out the following. But   I do want to talk through this. So the first 
thing to notice is, if lambda is equal to 0,   this is exactly the solution we got last 
time, so that's a good check to begin with.   There's this new matrix called I, 
let's just briefly say what is I. I   is known as the identity matrix, it's one 
on the diagonal and zero everywhere else.   Okay another quick question for the 
chat: what is the dimension of I here? Okay we're getting a few different answers on 
this one, so I'm glad that we talked about it.   So let's first do some unit tests on our thinking 
about this. One, I is always a square matrix,   but you might not have known that. Here's how 
you could know: we're taking its inverse. We're   typically going to do that for square matrices, so 
it should be something of dimension by dimension.   Okay, let's figure out what is its dimension.   Well it's got to be something that we could 
add to this x tilde transpose x tilde,   so let's remember what are those dimensions. 
Well, x tilde transpose is n by d. Sorry, x tilde is n by d so x tilde transpose 
is d by n and so we're adding I. Now n and   lambda are just scalars so they're not changing 
anything about the dimensions here. We're adding I   to what seems to be a d by d matrix and so I 
is going to have to be d by d in this case. Now   here's a tricky thing: this I is much like zero: 
whenever I appears, it doesn't have to be a d by d   I, it doesn't have to be an m by m I, it could 
be anything and you always have to figure it   out from context and so this is another one to be 
careful about. Just like the zero, you have to ask   yourself “okay what are the things that I'm doing 
with this, what am I adding and multiplying?” and   therefore I'll know what the dimension is. And so 
in this particular case, it's going to be d by d. Okay, now again, something that we're not getting 
into, but I'm just going to tell you, is that the   matrix of derivatives, we can calculate it and it 
tells you whether this curves up and here's a fun   fact: this particular matrix derivative, this 
matrix of second derivative, tells us that this   always curves up and it is always invertible 
when lambda is greater than zero. So that's very   different from what we just did when we were 
just doing this least squares this, you know,   without the penalty, sometimes it's not invertible 
or sometimes something goes wrong there. In this   case, we're safe: it's always invertible as long 
as lambda is greater than zero. There have been a   few questions now. I'm not sure if that's a new 
question or not, is it? Yeah, should there be   like sum from i equals 1 to n in the matrix form? 
Oh no that's totally a typo, thanks for catching   that. Yes okay so I will correct this in the notes 
we post online. I like to think that these typos   are just keeping you on your toes, so well done, 
but yes, so once we've taken the norm, there is no   more sum from i equals 1 to n and so that should 
just not be there, that's totally a typo. Thanks. Great. Okay cool. So this all sounds really 
good, I'll just again mention that you can   also solve for the minimizing parameters in the 
case with an offset, it's just a bit more math,   we're just not doing that math, but it's 
not like a deep thing that's happening here.   Okay so you can do this, you can solve for this, 
you can get out, you know, again the whole idea is   you solve this, you get your theta, you get out 
your hyperplane, you can plot your hyperplane,   and that's great. So are we all done? You know, 
have we solved everything in linear regression?   Okay, well, let's make a couple notes on this 
and particularly let's make a couple of notes   on features. You know, we talked previously about, 
in classification, how it's not just a matter of,   you know, somebody gives you some data and 
then you just immediately run a classification   algorithm on it, that often somebody gives you 
some data and you have to be really careful,   you have to be really conscious about the features 
and turning them into useful features for your   algorithm and that's absolutely the case here 
as well. And so here I'm just showing again the   linear regression with the squared penalty. 
This is exactly from the previous slide,   we're going to take some notes on the features. Now a big important point, and this is true for 
the last time we used a square penalty as well,   for logistic regression, is that we are 
implicitly assuming that the features are   on the same scale by using this penalty. 
If you use this penalty, you're saying   that lambda times (theta_1)^2 is penalized 
just as much as lambda times (theta_2)^2, is   penalized just as much as lambda times (theta_3)^2 
squared. If you go back to our features lecture,   so this is lecture 3, we saw that 
if you change the scale of the data,   you change the scale of the theta. That was not a 
conscious rhyme, but it turned out to rhyme. But   if you change the scale of the features, you know, 
if you just happen to put them in different units,   you know, or something like that, you're going 
to change the scale that the thetas are on and,   in particular, I mean the normal vector theta 
not the offset. And so here, and that's what   we're looking at here anyway, and so here, you 
have to notice that if you don't do something to   make sure that your x's, your features, are on the 
same scale, then implicitly your thetas will be on   the same scale and they'll be sort of penalized 
differently and so if you want to make sure that   these penalizations really make sense, you've 
got to do something like standardization and,   you know, these general techniques to make sure 
that all of your features are on the same scale. Now related note is features still matter. 
You know, we went through this whole lecture   on featurization, on turning your data into useful 
features, and all of that is still true: you could   equally apply basically everything in that lecture 
to the regression case and I will just point out   that this matters and it's extremely 
important in so many aspects of life.   Just as one example, among many: featurization 
for regression was actually a big news item this   summer. So I don't know if anybody followed 
the Bolivian election, there was an extremely   contentious and bitter election in Bolivia. 
There was an analysis of whether vote rigging had   happened. There is a huge amount of nuance around 
this issue and I'm just not going to get into the   whole issue, I encourage you to read up on it if 
you're interested, but if you look at this, one   of the big issues in this analysis—so somebody did 
this analysis, it was a regression style analysis   to decide whether vote rigging had occurred—and it 
turned out that they did not featurize correctly,   the time stamps were sorted alphanumerically 
instead of chronologically and this made a huge   difference. So in particular something like, you 
know, 1:01 pm was listed before 1:00am because   of the alphanumeric rather than chronological 
and what they were trying to detect was if a   big change had happened and so this might 
make it look like a change had happened.   So this is all just to say featurization really 
matters and it comes up in very real life,   this had huge consequences. Again, I don't 
want to suggest this was the only thing that   was happening, that everything hinged on this one 
element, but this one element was very important   and it did matter. Now a couple of notes about 
how this could come up in your life: first of all,   I think that there is sometimes this idea, this 
fantasy, that what happens is that somebody who   has applied gives you a data set and then 
you, as the machine learning researcher,   do your machine learning magic and then you give 
it back with some machine learning magic output   to that person and you can just see from a lot of 
the things we've looked at here, including this,   that that's just not the case, that you 
can't just not look at your features,   you can't just not think about your features 
and about your data and what your goal is.   You have to give that thought, you have to 
be very careful and very conscious about it   and you have to really engage with 
it and probably talk to people.   Now a second meta point that I just 
want to make while we're on here is that   this is not the end of the world to make a 
mistake, you're going to make mistakes in   your analyses. Literally anybody who's done any 
kind of meaningful analysis has made a mistake,   but what's really important is that it be 
detectable and so a real issue that arose   in this particular case was that people did not 
share their code in their data and so the mistake   was not known for a very long time and people 
couldn't correct it and, you know, the analysis   wasn't replicable. If you share your code in 
your data and you communicate with other people,   then together you can find any bugs or mistakes 
and it's just not the end of the world to make   a mistake but by working with other people you 
can avoid that. Okay so those are just some sort   of general machine learning life lessons but they 
very much relate to this issue of featurization. Okay so great, we have this awesome way to 
optimize linear regression, you know, we don't   have to have to worry about gradient descent, 
right, we talked about in the past how, you know,   if we had an optimization objective like the one 
that we have here, we could use gradient descent:   we could start from a point and 
we could progressively try to get   closer and closer to the optimum but here we 
don't have to do that, you know, we have this   what you might call a direct solution, a closed 
form solution, analytical solution. Basically what   we mean by that is you can just write it down, 
you don't have to do this iterative procedure,   you can just say “here's the solution. Boom 
there it is, that's the optimum” And yet,   some people actually use gradient descent 
for linear regression, for ridge regression,   for all of these things that we've been talking 
about. So let's ask ourselves why is that?   Why would you use gradient descent here 
when we have this awesome, you know,   analytical closed form direct solution, why not 
just go straight to the answer if you can do that? The reason is that, even though you have a closed 
form solution, that doesn't mean that you can get   it in zero time. So we can't talk about accuracy, 
about saying “hey this is such a better solution,   we go straight to the optimum and everything 
is great.” You know, there's actually a few   answers to this question, but this is one of them: 
that, you know, we can go straight to the optimum,   we have to talk about running time and we have to 
say well what are the actual running times that we   see in practice here? And it's going to turn out 
that this going straight to the optimum solution   sometimes is very costly. It might not seem like 
that, it might seem like an iterative solution   would always be less costly, but that's 
not really the case. So you have to talk,   whenever you talk about accuracy, 
you have to talk about running time   at the same time. So if I have a solution that 
I can go and get the perfect solution and it   just takes me six months to run, I effectively 
don't have a solution for many problems because   if I don't have six months, then I'm just 
not going to get that solution right. There   are some problems in machine learning that turn 
out to effectively take the age of the universe.   I'm not going to run them, so I effectively 
don't have the answer to that problem.   This happens to go the other way. This is, 
again, just like machine learning life lesson:   running time doesn't mean anything without 
accuracy. So if somebody tells you they have   this cool new machine learning algorithm and it 
takes constant time or it's super duper fast,   they have to also tell you about accuracy. 
It's very easy to come up with methods   that have a fast running time. It's much 
harder to come up with methods that have   a fast running time and give you a good 
result, especially a provably good result.   Okay so we need to measure accuracy for the 
running time that we have. What is the issue   with running time that arises here? Well let's 
look at our formula again. So this is the formula   that we're saying, it’s a nice closed form 
formula, it takes us straight to the optimum.   Why would this take a long time to run? The 
issue is the matrix inverse. If you are ever   working on very large data sets, you should 
always be skeptical of matrix inverses and   you should always ask yourself, you know, what's 
the size of the matrix inverse? In particular,   we just said on a previous slide that this 
is a d by d matrix within the inverse and a   fact of life is that matrix inversion is 
somewhere between quadratic and cubic in d,   nearer to cubic, and that's just extremely 
expensive if d is large and actually there   are some very practical, very real-life d's, you 
can get easily into the millions and billions   and then this just can become prohibitive 
and it's not really something that you're   going to run in practice. And so, if gradient 
descent can get super close to your optimum   in a reasonable amount of time and this exact 
solution cannot converge, cannot even give you   any kind of answer in a reasonable amount of 
time, you actually will prefer gradient descent. Okay, so we can write out gradient descent. So it 
turns out we have this nice convex problem. Again,   just as logistic regression was 
convex, linear regression is convex.   And then once you add that penalty, you get an 
even nicer convex problem that's really going   to have, you know, sort of a nice unique optimum 
as we saw and so we can apply gradient descent. So I'm not going to spend too much time on this, 
it's really, in some sense, formulaic at this   point. You want to take your gradient descent 
algorithm and just apply it to our objective,   either our linear regression objective or 
our ridge regression objective, and as usual,   it's going to take some initialization: 
it's going to take some step size parameter,   it's going to take some kind of ending parameter. 
Here, let's just say it's a number of steps t. And so we initialize all of our 
thetas, we go through all of our steps,   we update according to the gradient and just like 
logistic regression, there's a lot that's being   written here, but I wouldn't worry about it. 
This is like you just apply linear regression,   you know, you apply gradient descent to linear 
regression and this is what you get out.   And hopefully that's actually true this time 
unlike logistic regression where we had a typo   but, you know, great for checking and this is 
I think a good thing to, you know, go through   an exercise and check yourself. And then we 
should sort of finish up and return. And so here,   remember, we talked about there are a lot of ways 
that we could decide when to stop, here we're just   stopping based on number of iterations, but an 
emphasis that I want to make, which is an emphasis   that I made with logistic regression, is that this 
depends again on stepping through all of your data   before you make any change in theta. So we just 
talked about how if you do this exact solution,   you have this problem that you have a really slow 
algorithm in the dimension. Here you have a really   slow algorithm in the number of data points, which 
can also be extremely large and is very large in a   number of important machine learning problems and 
so that might be something that you don't want.   And so there is an alternative. So again I'm going 
pretty quickly through this slide because we're   already familiar with gradient descent and most 
of this is really just applying linear regression   to gradient descent just so that I can talk 
about stochastic gradient descent very briefly.   So with gradient descent, we had our objective 
and we would just find the gradient for it.   With stochastic gradient descent, what we 
do is instead we notice that this objective   has a particular form, that's just the sum of 
these individual terms, these f_i’s. So this is   something that is true for linear regression 
but it's also true for logistic regression. So basically, there's some lost 
term per data point. In fact,   general training error loss as we've 
set it up would look like this. And so the idea of stochastic gradient 
descent is to look at one of these at a   time rather than the entire data 
set. It's going to be noisier,   it's going to be, you know, not as good 
over a particular iteration, but because   the iterations are so fast and so cheap, it 
can be better for the time that you have. And so with stochastic gradient 
descent, we again have initialization,   we can have a step size parameter and we 
again have some kind of stopping criteria. We initialize our parameter, we stop after, 
in this case, just some number of iterations. Now again, instead of going over a whole 
data set, we go over one data point at   a time. So we randomly select with equal 
probability among all of our data points   and we step in the gradient 
direction for one data point. And so there are two things that are different 
here: eta depends on t, it will get smaller   as t gets larger. We actually could have done this 
for gradient descent but we can do that here too   and it's important to do that here. And now, 
instead of doing a gradient in the whole data set,   we do it with just one data point and then we 
return. Okay, I'll encourage you to read more   about stochastic gradient descent in the notes, 
it's going to come up again and again. This   is an extremely widely used algorithm and it's 
very important because we deal with these very   large data sets and it has a number of other 
nice properties. So today, we've talked about   linear regression, we've talked about regression 
in general but linear regression in particular,   about ridge regression with this penalty, 
and about various ways to optimize that   and it's not always clear that you want to 
just do this immediate optimum, sometimes   you want to do gradient descent or stochastic 
gradient descent. Okay I'll see you next time. 

good morning let's get started on today's lecture um so in our previous lectures uh we've been talking about all of these different hypothesis classes and how to learn in them in particular we talked about linear classification and linear regression and we also talked about how you can get um these really interesting boundaries and classification and really interesting regression um curves by choosing different features um by choosing not just sort of our default x1 and x2 and x3 but actually having different features and we're going to sort of explore more of that today we're going to look at a potential set of features that we could use based on step functions and that'll lead us to discussing neural nets as a hypothesis class and learning in neural nets analogous to what we did for both linear classification and linear regression okay so let's just review briefly what we did with linear classification with the default feature so when i say default features i mean just x1 x2 we haven't applied any additional transformations here okay so we have a bunch of data and what we saw in a previous lecture is that a way that we could approach uh classifying this data is we could look at a certain set of functions in a slightly higher dimensional space so here we have x1 x x1 on the sort of east-west axis we have x2 on the north-south axis and this additional z on sort of an up down axis and so we could make this curve this theta transpose x plus theta naught which is linear in the thetas and also in this case happens to be linear in the x's and we could ask ourselves um for any particular theta and theta naught where is this curve below zero and that will give us one of our labels you know maybe it's minus one and where is this curve above zero and that's where we'll apply a different label maybe it's plus one and so in general any particular theta and theta naught defines such a curve which happens to be a hyperplane and therefore defines a set of predictions where we apply and predict uh plus one and where we predict minus one and then what we'll do is we'll say okay well that's a huge class um with all kinds of different thetas and theta knots um but we'll try to get as low as possible loss to figure out which one will apply to our particular data okay so we saw this picture before and now um we've also seen another picture we saw in in the features case that we could do this and we can get these these more um complex non-linear boundaries here as well um so in particular one way that we could do that that we saw um oh and and of course uh again just just uh reiterating we predict uh one one particular label on the side where this is greater than zero and a label on the side where this is less than zero okay so now we can get these more complex non-linear boundaries with for instance different types of features like one example we saw was polynomial features so with polynomial features we said hey we have these different features like phi 1 is now x1 phi 2 is x2 by 3 is x1 squared etc and so maybe i have a bunch of data and maybe i don't think that there's a really good single linear boundary here so i might use my polynomial features and so we saw we could do the same thing we can make this curve that is still linear in theta so we might still call this linear classification it's just non-linear in the x so we have this still theta transpose but now times phi of x instead of x plus theta naught and we're going to predict a label say minus one where this curve is below zero and a different label say plus one where the curve is above zero so it's the same thing it's just now a sort of more interesting more complex curve okay so again we get a curve like that for every possible theta and theta naught but then what we do is learning we say hey let's try to get as low of possible loss so we'll use that to choose our theta and theta naught and that's what we'll apply to our data and then in particular used to predict um at new points okay and so basically what we're going to be doing today is we're going to be thinking about other possible features we could use um there's nothing that says we have to use polynomial features and in fact maybe something that we might ask ourselves is do we have to have a fixed set of features maybe we could actually learn good features maybe the features themselves could have parameters that we learn and so that's essentially what we're going to be going into a lot of depth on today now before we do that let me just um go back to this picture here and let's just notice that what's happening here is we're essentially applying a function to apply this hypothesis to apply this prediction for a particular theta and theta naught we're predicting one label above a line and another label below the line and so we could actually plot that function so here's that function where we have one value on one side of the x1 x2 line and another value on the other side so for instance if we were predicting one and zero the upper value here in in sort of the up down axis would be one and the lower value would be zero okay and so in particular another way that we could represent that which we saw in a previous class is that we could use um this indicator function remember the idea here is that whatever's inside the parentheses we're gonna return one if it's true and zero if it's not and so what we're doing here is we're actually plotting this indicator function applied to this to this linear boundary so we see that it's one on one side where this is true and it's zero on the other side okay so this is called the step function where we have sort of you know one on one side and zero on the other side we're used to using step functions to classify we've been doing this for essentially many uh classes now and the really big difference about what we're gonna do today is we're going to start using step functions as features so we're still going to use them to classify but we're going to use them as features as well with their own parameters okay so let's go ahead and check that out and do that and see what happens okay so we have some new features there's step functions so because these are going to be features let's give them their own parameter names like let's say maybe w transpose and w naught instead of theta transpose and theta naught which is what we were using for our classifier and so this is a feature so i'm going to call it phi 1. it's going to be our first feature and it's going to be some function of x because we've seen that that's what features are they're just functions of x the original sort of default features that we got in the beginning of the data okay so let's draw this well depending on my choice of w and w naught i'm going to get something that looks like this and it's just that breaking point that line that's going to be different depending on how i choose w and w not but this is a feature it's a function of x it's something that we can use as a feature and it's got this step function property um we can make another one of these and we need to give it its own new parameters so maybe let's call them w tilde and w not tilde for the moment and again that's going to define sort of where we break and on what side we get one and on what side we get zero so this is just another feature it's just another function of x any function of x could be our features um we happen to be choosing these at the moment and so now we have two features we have two functions of x that we can use as features and so let's ask ourselves what would classification look like with these features if i were to use these features for classification okay well we've been doing linear classification let's do linear classification again and what i mean by that again is that we're doing something that's linear in the parameters and the theta and theta naught but now we have these interesting features the phi's in this case we have phi 1 and phi 2 and so this this function that i've just written down the z equals theta transpose phi of x plus theta naught this is nothing new this is what we've been doing with classification this whole time we've considered polynomial features we've considered other things the thing that's new is one our new step function features we haven't considered step function features before and two our features themselves have parameters that's a little bit new we we have not done that before okay so let's see what what is this going to look like well in our particular case we're looking at two features and so if we write out what does this dot product look like there's going to be two components there's going to be our theta 1 times phi 1 or theta 2 times pi 2 plus theta naught and then just so i can plot this i'm going to choose some particular values of the thetas that'll be a particular hypothesis whenever i choose all of my parameters to have a value that's one hypothesis later on i'm going to try to learn those parameters but here i'm just trying to plot what do the hypotheses even look like and so here's just an example set of values that i could use to plot my hypothesis okay and so once i have those values i can go ahead and plot a hypothesis and here we go so this is my hypothesis based on these two features and this particular linear combination of these features and again just as before what's happening is we're having these labels so we're applying a label of one somewhere and a label of zero somewhere and we can find out what those are by sort of looking down from above so if we look you know down from above along that z axis we're gonna see where we apply the different labels so in one region of the space in fact on two different sides but wherever wherever you're seeing um that that check pattern we're getting that we're gonna get a label of one in this case and in that in between part we're going to get a label of 0. so it's just like before where we say hey we have this function we're going to ask where is that function above 0 and where is it below 0 that's where we're going to get our label of 1 and our label of 0 respectively and what's different now is just we get a different type of function this isn't a function that we saw when we did vanilla linear classification this wasn't a function that we saw when we did polynomial linear classification with the polynomial features this is a new function for a new set of features and it actually might be pretty useful so just as an example of where you might want this kind of classifier let's think about exoplanets so exoplanets again are these stars that are outside of our solar system or sorry there are these planets that are around stars that are not in our solar system and so one thing that we might want to ask is which exoplanets are habitable you know which ones could could life actually exist on and if you look at this well there are a lot of different sizes of stars that exist out in the world or out in the universe and for each of those stars if as the star gets bigger the habitable zone is going to be sort of farther out because if you get too close to the star everything's going to be too hot if you get too far away from the star everything's going to be too cold you want to be in that like perfect little goldilocks area and so this kind of hypothesis can express that that as the size of the star increases there's this nice little band of distance where things aren't too hot and aren't too cold and you know maybe life could exist on this star and so from that perspective this is a useful hypothesis to have you know we wouldn't want to just use a linear classifier for this because we wouldn't be able to say that there's that nice little band that's separate from everything else now we have a little bit more um flexibility that we didn't have before with just a linear classifier now we have the ability to have sort of you know two different divisions and to say something's happening on either side of them okay so this is one particular setting of these weights in the features and one particular setting of these theta and theta naught in the classifier but let's see what would happen if we changed those settings just a little bit remember the idea of hypothesis classes is that you know we can have a lot of different shapes that we can get by changing our parameters and so let's just change our parameters a little bit and see what else might happen so in particular let's look at this first feature what would happen if i change say the w and w naught well i'm gonna move around the line that defines where i'm labeling zero and where i'm labeling one in this feature well if i do that then that's gonna change my plot down here because it's still this linear combination of future one and feature two instead it's going to look like this so i've just changed one of my features and that therefore changes my prediction here and now when i look at it from above and see what gets labeled one and what gets labeled zero it's going to look different i'm going to get a different kind of boundary now we can do this with the other feature too so here's another feature here's our second feature we could change the weights in the second feature remember you know we've seen this sort of thing before that these the sort of normal and the offset defines a line and so if we change those values we're going to change where that line is and so we can do that too so here i just happen to have changed it to a different set of values and now if we keep the same weights in our final classifier we're going to get something different now what happened here in this case you'll notice there was some overlap of the two features and because we're just adding them up we're going to get a value of 2 instead of just a value of one or zero and that's fine i mean in general we expect that we could have any values here for this linear combination but in the end of the day if we if we take this step function perspective we're still going to look down on this and we're going to ask where is this function above 0 and where is it below 0 to make our classification and so this is what our final classifier is going to look like our final hypothesis is going to look like it's going to predict 1 in the sort of upper corner it's going to predict zero in this lower corner and again this is sort of a potentially useful classifier to have just as an example maybe i'm trying to decide you know when do i go for a run i want to look at the temperature maybe in the precipitation and decide and from here you can sort of see you know maybe x1 is precipitation on this horizontal axis and i might say to myself if it's you know snowing or raining too much i'm just not going to go for a run and i don't go for a run then um conversely if it's way way way too hot outside i'm not gonna go for a run and so we're seeing that as well here and so this is again something that i couldn't get necessarily from a straight linear classifier with just the default features as we saw before that i need something a little bit different and here i'm able to express this now what if i also don't go for a run if it's too cold i don't go for a run if it's raining or snowing too much but i also don't go for a run if it's either too hot or too cold i'd like to be able to express that i'd like to be able to express that in my hypothesis so how could i do that well i might add in add in a third feature so here so far we have two features five one and phi two they're both step functions but there's nothing that stops us from going further and making yet another feature so we have the two so far let's let's go for a third so here i'm just adding yet another step function i'm running out of notation so i'm calling it w tilde tilde and w not tilde tilde this is clearly a problem that we're going to have to resolve and we're going to resolve it on the next slide but bear with me for the moment it's just another step function it's just another line with one value on one side and one value on the other side and now i want to ask myself how does this change you know once i have this new feature how does this change the function that i create with linear combinations and then my final classifier that i can make from that function okay well let's start with this function i make with linear combinations in order to look at this function i'm going to need to change what i was doing over here because i so far i only have two features over here so let's look at this first line the first line is generic this is for any number of features um it's it's going to work out because it's just the features as a vector the second line is the problem it's only got two features in it so far we're going to need to add a third feature and so this is really what that first line is going to look like when we have three features so all i did was i added in that theta 3 phi 3 term that wasn't there before so this is really just writing out that dot product from the line above but now with three features instead of two okay so now let's choose some particular values just so we can plot this because we're going to need some particular values in order to have a a particular hypothesis so here i had some values for two features i'm going to add in another weight for a third feature it's pretty arbitrary i'm just choosing some weights here so we can see what do these hypotheses even look like so if i do these weights now i can plot what does my function look like the function defined right there on the left it's just a function of x and here we go so now we see that there's multiple values that we can get but again some of them are above one and some of them are below one and that's what we were doing with our classification for polynomials that's what we were doing with our classification for just vanilla features and that's what we're gonna do now we're gonna ask what's above one and what's below one and so we're gonna get the following hypothesis for classification i'm not going to go for a run if it's raining or snowing too much i'm not going to go for a run if it's too hot or it's too cold but if it's in that nice little middle place where it's not running or snowing too much and the temperature is just right i'll go for a run and so again this is the kind of hypothesis that we couldn't have expressed if we just had our traditional you know no nonsense not changing anything features we just had our x1 and our x2 but now that we've created these more complex step function features we're able to say this sort of complex statement about going for a run okay yes there's a question um please could you say that it's more it's more confidently predicting a one if the graph is higher above one or oh yeah that's a great intuition so this idea that we kind of feel like it should be more confidently predicting a one if the graph is very much above one versus if not it's sort of saying like oh here are all the reasons i wouldn't go for a run you know it's raining too much and it's too cold and so it's like wow if all of those things are true i'm super not gonna go for a run and that's such a great intuition and that's the kind of thing that we can express if instead of a straight stem function we had a sigmoid because a sigmoid is basically telling us exactly that kind of thing remember like when are we most likely to go for a run and when are we least likely and so we're going to see that later basically so right now we're just saying hey am i going for a run or not um but absolutely that's great intuition um and uh and we're definitely gonna go there later in the lecture great okay cool so right now we have this set of hypotheses um but we were running into an issue was which was namely that i was running out of tildes you know like the notation was just getting way out of hand at some point i can't just keep adding tildes to my weights i'm gonna need to you know what if i want four features what's going to happen then what if i want four of these step functions and so let's just take a step back now and try to develop a much more general set of notation for the idea that we've just been talking about basically the idea again being that we create a whole bunch of step functions we're going to use them as features and then we're going to put them into a classification problem okay so now we're just going to spend time on that notation so let's get some new notation again just for exactly the thing that we just did okay so the first layer was constructing the features there were kind of two layers that were going on here we first constructed some features then we put them all into a linear classifier okay so let's talk about constructing the features and let's develop some notation for that same thing we did just with some new notation okay our input is going to be a data point so just as before we said we have these phi of x they take a data point and they output our features we're going to have our data point and we're going to output some features so just as an example maybe our data exists in two dimensions and this could be our data point that we would use an input this little star in the two dimensions okay now something that we're going to do is we're going to have this sort of general notation for each layer so the superscripts on this page are going to represent the layers so in this first layer we're constructing the features we're in the first layer so we're layer one and we're going to say that the number of inputs to layer one is m super one so that's just going to be a notation that we use um but in this particular example in this drawing that i have of x right here can somebody tell me what is m1 as a number what would what would the size of the data point b so this is uh for the private chat great lots of lots of great answers here okay so the the answer here in in this particular drawing that i have on the right this is a two-dimensional data point so we're seeing that it has size you know we've been thinking about these things as column vectors so it has size two by one and so m one here is two and the size of the data point is two by one okay great now we've called this something different in the past and so i'm just going to point that out we've called this d in the past so m1 is just equal to d the d the dimension of the data that we talked about before okay now our output in this layer is going to be the feature values so let's just go back to what we had on the previous slide only in miniature so these were our three features on the previous slide they're each a function of x1 and x2 although i've only labeled those axes in the third feature but each one is a function of x1 and x2 and so what a1 is going to be is it's going to be those features evaluated at x so if i take my particular x so this is just the same x that i have above here it's got a pretty low value of x1 and sort of a mid value of x2 and i ask what's phi 1 of x at that x well it looks like it's 0 in this case in this particular case phi 2 of x also looks like it's 0 and phi 3 of x looks like it's zero but it's basically what we're going to do is we're going to evaluate our features at this particular x and that's how we're going to get a 1. okay so we get these particular values we line them all up into a vector that's going to be a1 and so that's going to have size we're going to say that the output size is a column vector of size n1 and so in this particular illustration where we have exactly these features this is another question for the chat what's the size of n1 or what exactly is n1 can you tell me what n1 is again just for this illustration that we're showing here okay great we're getting a lot of threes just for this illustration and one is three so basically the idea is we're going to take in data of whatever data dimension we have in this particular illustration the data dimension was 2 and then we're going to output a set of feature values in this particular illustration the number of feature values is 3. now a1 itself will be those values it just has size and one by one it's a column vector okay so how do we get these feature values well we apply our feature and so you'll recall in the particular case of a step function we had some function f1 that's basically the step function and then we applied it to some linear function of the x and so here i've gotten around this issue of okay well i was running out of tildes with my ws by just saying hey i'm gonna give a different w to each one of these problems ah i i see that there is a small typo here the w naught should also be indexed by i um i will fix that for the the online um slides when i show them later but basically we're going to have a set of w so that's both the offsets and the normals here that we're used to and we're going to have one of those for each of the features so we're going to have a w not i and a wi as well and then the superscript again everywhere here the superscript just means we're in the first layer we're constructing the features so that's what that superscript one means okay so in particular if we were looking at i equals three we're constructing the third feature so here i'm focusing it on the third feature that's i equals three um and the way that we would do that is exactly we would construct our function of x in the following way so this is phi 3. this is the phi 3 function of x for this particular step function feature and then we would say hey let's evaluate at this particular x to get out our a i 1. so in this particular case in this particular case if we're looking at i equals 3 and we evaluate our feature and we get out our ai1 what is the value of ai1 again remembering that this phi 3 has two values it's either zero or one zero is below and one is above so what is a i1 here this is a question for the the private chat again okay great so the answer is zero in this particular case and the reason that we know that is we look at that particular x that's that little star we evaluate it at phi three of x and we see that at that particular x here the value is zero there were two choices sort of zero one and this one happened to be zero great so lots of lots of great um answers on that one okay so now though we actually want to do this multiple times we don't just want to do this for one particular i we want to do this over all the features i and so we're going to do all the features at once and put them into this vector a1 and that's going to look like this now let's talk carefully through this equation because there's some weird stuff going on here so first w one capital w one here is collecting the w i's the w i super ones the little wi super ones so it's going to be an m1 by n1 matrix and it's just collecting all of the w i ones that we had from those i features similarly w naught is collecting all the w knots so i made this typo it should be w not i in that i feature area um and so this would be collecting for each of those eyes the w not ones okay now here's where the weirdness comes in this thing is going to be a vector so you should check using the kind of tools that we used in previous lectures what is the dimension of that vector you can get it from the w1s the size of the w1s the size of x the size of w not one i'll tell you i mean you can kind of see just by looking at w not one if i'm going to add w not one to it this has got to have the same size as w not one and so this whole thing is going to be n one by one and what's weird about that is we've never really applied a function to a vector before that's not something that we've been doing in this class we've always applied it to a scalar and when you apply a function to a vector that could mean a lot of different things and so i want to be really clear what does this particular usage mean it means we're going to apply it element-wise that doesn't always have to mean that there are different ways to apply functions to vectors but here that's what we mean by this so f1 is applied to each element in that vector to get a1 out so this is really you know this this a1 equation with all the features at once is just a shorthand of writing what we did for the i feature i times or n1 times you know for every i and that's really the story of this slide and the notation of neural nets in some sense is that it's all this sort of shorthand that seems laborious at the time and is like kind of a headache but ultimately will make our lives easier because we're going to have too many things floating around to have the notation that we were using before okay so that's the first layer where we constructed the features now let's go to the second layer where we take those features and we assign a label or potentially multiple labels so this is really kind of like what we were doing before you know in previous classes like just doing linear classification with different features um and so we're just going to do that again only with like slightly different notation okay so let's do this so first in this layer the input is going to be the features just like if you were doing linear classification with polynomial features you would take in those polynomial features so now we're going to call the size of any layer m super that layers index by one so in the first layer we call the m1x1 in this layer we're going to call it m2x1 um is m two equal to anything else on this slide could you could you say in the private chat is there something that m2 would be equal to it's basically great yes it's the number of features which we just said was the output of the previous layer which we just said was n1 perfect great great um observations here another good one is it's the dimension of a1 totally another great way to phrase this um it's the number of elements in the vector a1 um so these are these are all correct um good stuff so i'm just going to write that explicitly here m2 is the number of inputs to this layer so it's got to be the number of outputs in the previous layer because we're taking the number of features and in this particular illustration that we have on the side again that's going to be three because we have three features okay so now here our output is going to be the labels now in everything else we've done in this class so far we've had a single label we've said okay are we classifying this you know as uh an exoplanet in the habitable zone or not are we classifying this as i'm gonna go for a run or not you know are we classifying this as um you know one thing or the other and so this is a little bit different potentially than that and that we could have multiple outputs so in what we just saw in the illustration that we saw in the previous slide we were just having one output we were doing this classification of am i going to go for a run or not so that's either one or zero and so in this particular case our vector of labels is actually just a one by one vector there's there's nothing to it but you can imagine that maybe i want to you know say multiple things i want to say am i going to go for a run based on the precipitation and the temperature and is my best friend gonna go for a run based on the precipitation and the temperature and you know is our neighbor gonna go for a run based on the precipitation and the temperature and if i asked all of those questions using the same set of features then i would have a vector of labels here incidentally i'm going to call this whole function that starts from the data goes through the first layer and through the second layer n for neural net because we're basically building up a neural net which hopefully um is something that you have read about in the reading before today um and and did your exercises uh and we'll we'll talk much more about you know this this wording of neural nets but basically we're just building up a complex function here and so and it depends on these parameters w okay so finally we're going to get that label and what that label is is we're just evaluating that complex function that we're building up at a particular point so we put in that point to get our feature values the a1s now we put in the a1s to get our final value a2 maybe final values but in this particular example it's just one and so this is gonna have a size we'll call it n two by two by one that's the number of outputs but again typically n2 is just going to be one because you're going to be doing one classification or later we'll see you might do one regression okay and so again we have the same idea that we're going to apply our in this case classifier to get the ith label so for instance if we have a single label we apply the classifier in the following way and this is our typical classification this is the kind of thing that you know we've been doing for a while now and the only thing that's changed essentially is how we express it in notation so previously we had theta and theta naught now we have w i super two because this is the second layer and again unfortunately i made a typo this is w not i it depends on which output you have um and so you're going to have a w not i again super two because this is the second layer so that's why we have that two superscript and then again we might do this fancy thing where we collect all of them together at once by applying this component-wise f you know which in in the case that we've been looking at so far it's been the step function but we're going to see some alternatives so we apply this component y is f um and we you know collect everything in these w2 and w not two matrices very much analogous to what we did in the first layer okay so the whole idea here and i think the important thing to take home from this is that all we've done is constructed a really complex hypothesis so remember what is a hypothesis it's it's just a way to go from data to a label and it depends on some parameters and that's exactly what we have here we have a function that goes from our original data through to some features and then finally to a label and so what i'm plotting here is that whole function that function just takes in data and outputs a label and here we're sort of not seeing the hidden calculations along the way we're just seeing here's the final output we can see that as a result this is a pretty complex interesting function and the other important thing to note is it depends on the choices of the parameters the w and w not so just like in the past our hypotheses for say just linear classification with default parameters depended on the choice of our parameters here we get the same thing if i choose different parameters if i move the parameters around i get a different hypothesis and that's what's let us do learning you know ultimately what we want to do is we want to say hey there's a lot of different hypotheses for a lot of different parameters and when we get a particular set of data we're going to ask well which of these hypotheses has a particularly low loss can we minimize that loss and so what we're doing here is really just building up what even are these hypotheses what do these look like and in particular for neural nets okay so this is a big old slide of notation which i think is the kind of thing you know all notation looks weird when you first see it and you you get used to it by working with it by working with a lot and so that's what you're going to be doing in your labs your exercises you know everything that you're doing you're going to be getting really familiar with this notation and it'll hopefully become you know second nature it'll become something that you're more used to but certainly it does feel like a lot in the beginning okay so now what we're going to do is we're going to find another way to express the same idea which can also be a very useful way to express the same idea and that's with a function graph okay so again let's just reiterate so there's there's sort of a couple of key things going on in this slide and i'm just going to extract them this is basically the whole slide we had the first layer where we constructed the features and we output the feature values and we had the second layer where we took those feature values and we turned them into labels and then we said this whole thing we'll call this a neural net where nn and it's just a big function that takes in our data and outputs some labels a2 okay and now what we're going to do is we're going to express the exact same idea this way to express our function as a function graph again we're just sort of describing our hypotheses at this point we're saying what do they look like um and here's just another way to describe them and i think it's it's useful to have these multiple descriptions because we're getting into these pretty complex hypotheses and so we want to be able to understand all the little components that go into them um and to understand you know how does this work so let's let's look at this idea of a function graph okay so the first thing we're going to do to build up our function graph is we're going to look at what does a function graph look like if we're constructing one feature let's say the i feature in the first layer okay it's going to look like the following so we have our inputs x1 x2 up to xm1 because we said that that's our cool new way to describe the number of inputs but m1 is just d it's just the d that we saw from before now we see this circle what does a circle mean in a function graph a circle means that a function evaluation is happening so this first circle where all the x1s are going x1 x2 x3 etc are going into there's this little summation symbol and that essentially represents this dot product plus adding w naught because the dot product's just a sum you know if you think about it you're really just summing over the elements of that vector and then we're adding in the w naught and so we're doing that whole dot product and we're getting out an output and we can call that z we could call that output z this is basically the thing that's going to go into our function f1 now why doesn't z have a circle around it because there's no function evaluation going on there so we have a circle around the sum because we're taking all those x ones and doing something to them and out making an output with z we just that is a value and now we're going to take that and put it into a new function f1 and because with f1 we're doing a function evaluation getting an output that's going to have a circle on it again and so now we do that function evaluation we take this z and we put it into f1 so again z is just this w1 transpose x plus w naught for the i component and then we get out this a i 1 by applying f 1 and this is representing that component wise evaluation that we talked about from before so for particular for the i value for the i feature we have a particular value a scalar zi1 that goes into f1 and then we evaluate ai1 okay and now if we want to look at the whole first layer it's just doing this a bunch of times so this is exactly what i drew for one particular i and now we have the same thing for i equals 1 i equals 2 i equals 3 up to i equals m1 because again there were n1 different features now something that's kind of interesting about this though is i didn't just repeat the same graph over and over you'll see that the inputs are shared and so that's something that this graph is expressing is that it's not just a total repeat of the graph again and again we have the x1 x2 up to xm1 going to every single one of these summations because the inputs are shared by every feature um there's a question uh in the chat shouldn't there be a w not going into the sum that's a good point um so here i actually haven't really represented the w's so something that you'll be familiar with um from seeing some of your representations in the course um and in the reading is is w knots and w's here i just haven't even shown them um and so if i were showing the ws i would definitely want to make sure that like both i have the w's and the w knots i guess i'm kind of just avoiding even showing them on this so they're implicit in this particular sum but that's a good point it's worth noting that really you are using the x's and the w's going into this and it's it's sort of you know here i'm kind of using it the same way that um you know x's are the inputs and w's are the parameters like they're still inputs but we kind of treat them as like different inputs um and so uh but yeah but they definitely are still inputs and they're definitely going into every one of these sums and it's a different w set of w's going into each sum okay great so now we have these features a1 a2 up to a n and what i want to do now is i want to look at the second layer how we're going to represent the second layer on this function graph representation well it's going to be a similar deal now our inputs into the second layer are going to be the features from the first layer i'm going to write a second layer where there's only one output but there could be more outputs and if there were more outputs then we would have the same features for each one of those summations okay so we have our first layer we have our second layer and then the whole thing you know each one of these individually was a set of functions the whole thing is one giant function it takes in a bunch of x's and it spits out in this particular case one label at the end it could do more labels one label is pretty standard okay so now we're just going to make a few notes about this function graph representation the first note is that it has directionality there is a notion of going forward and a notion of going backward in it something you'll notice is that there's a direction defined by going from the inputs to the outputs and all the arrows accord with that direction and that direction is what we're going to call forward so forward means going to the inputs from the inputs to the outputs there is also accordingly a notion of going backward once we have a notion of forward backward is the other direction so we can also draw backward so backward we'll be going from the outputs to the inputs and this is something that's going to come up when you're actually doing inference when you're actually doing learning in these models um it turns out you're going to do a lot of chain rule and the easiest way to do that chain rule is often by going backwards and so that's why we have something called back propagation because you're going backwards in this graph now this particular network that we've shown here is what's known as a feed forward neural network because all of the lines go forwards it is possible to have a network that is not a fee-forward network where some of the lines go backwards so for instance here is such a network this is just an example of a network that would not be a fee forward network because one of the further along outputs then becomes an input it has a it has a line that goes backwards an arrow that goes backwards and this is actually something that we're going to see later in this course with recurrent neural nets but it's not something we're doing right now the networks that we're looking at right now like this network before i added this extra bit are feed forward neural networks so now we're back to a feed-forward neural network now that i've gotten rid of that that weird backwards line okay let's also label some of the things that are going on here so we have inputs you can actually have input standing layer in this particular case in in this first layer our inputs are going to be the data we have essentially this dot product that's going on where we're adding in the the w knots too but you can think of that as part of a dot product um perhaps with a feature that is equal to one you know we've seen seen that interpretation before we have this thing which is the output of doing the dot product and adding w naught but before we apply this function f1 and so we're going to call that the pre-activation it's what goes into the function f1 and a reason for that name pre-activation is in some sense because the function f1 will be called the activation function and so it's before you apply the activation function so it's the pre-activation and then finally what we get when we apply the activation function is perhaps not surprisingly at this point the activation now why might this be called the activation well if you think about these step functions that we've been talking about that have a zero and a one you can think of them as being sort of activated when you hit a one and not active when you hit a zero so this is sort of telling us you know did we did we activate uh this particular what we're going to find out is a node but did we activate this particular feature okay so now a neuron or sometimes a unit or a node is basically the sequence of inputs through the dot product and the activation function to a particular activation so not going through any other activations just getting to that nearest activation um and taking all the inputs that go into that node that's going to be a neuron or a node then a layer oh here's just another neuron or node you know we can have them in any layer and now implicitly we've been saying the word layer a lot let's let's just say a layer is this collection of neurons here at a certain distance relative to the inputs or the outputs so for instance this is a particular layer this is sort of the first one um it's it's out of all of these neurons or a particular distance relative to the inputs and then we have we'll see we have another layer as well okay and then finally we go through all of this stuff and we eventually get a set of final predictions um or guesses and which is which is what we get from anything that we do any hypothesis that we do whenever we put in a particular data point we get a final prediction or guess out of that hypothesis for each possible data point value and so this is no different one the one thing that is a little bit different is that we could have more than one that's not something that we've talked about too much before um i mean implicitly i guess we could have done it but here we're sort of explicitly including it in our notation as a possibility again in this particular function graph i've only shown one output but i could have more okay this also happens to be what's known as a fully connected network so in particular we say that a layer is fully connected if all of the inputs to every neuron in the layer are the same and in this particular case that's true of all of the layers so the whole network is fully connected now one more note we can actually name some of these layers you could say we have this output layer so that's the layer that eventually we get that final prediction that final gets out of the thing that we actually report at the end of the day and the stuff before it we don't see you know we don't get a final prediction or a final output out of and so we might call those the hidden well in this case it's one hidden layer spoiler alert we can have more than one layer that is hidden and in that case we will have hidden layers but it's basically it's the stuff that's not the output layer it's the stuff that makes the features okay so now we have multiple ways to think about neural networks um with our original step function idea um with all of the notation that we developed with this function graph representation and so let's think about how we would slot that into the way that we think about hypotheses and the way that we do learning in this course okay so our typical problem setup this is just what we would do for any any learning problem so far is we choose a hypothesis class we've done that every time that hypothesis class could have been the linear classifiers this time we might choose instead the neural networks they're defined by the layers by the number of units in each layer the number of nodes in each layer and by these weights the w's and w knots and so to really nail down a hypothesis class we'll have to say you know we're going to have this many hidden layers and this many outputs and we're going to have this many nodes in each layer and that'll tell us a hypothesis class okay in this particular case we chose two layers with let's say you know one output and some number of hidden units and then the next thing we have to do always is choose a loss and so for instance if we are interested in classification there's a number of losses that we've talked about we've talked about zero one loss we've talked about asymmetric loss we've talked about negative log likelihood loss there's a lot of choices we just choose one of those then we learn the parameters we have some general purpose methods to do that now like gradient descent and stochastic gradient descent so we could we could hope to apply those although we're going to see there's a rub in just a second and then once we have learned some set of parameters that is to say we've we've found some set of parameters that seems to get the loss pretty low and we try to minimize the loss then we can use those parameters to choose our particular hypothesis we don't just have a whole class anymore we have a particular one and we can use that to predict unknown data okay so this is our general setup i mean this is how we approach these problems in this class this is not specific to neural nets it's just you know if we choose this hypothesis class and we're using neural nets um but there are some problems with applying this general setup right now to what we're doing and so let's go into that before we can actually apply it because we're going to have to resolve those problems okay so here's here's the first issue the first problem that arises if we want to use gradient descent or stochastic gradient descent we can't really use these step function activation functions that we were talking about before the reason for that is that their derivatives are zero basically everywhere and so if i apply gradient to center stochastic radiated descent whatever gradient i calculate it's just immediately going to be zero and i'm not gonna move and that's it that's done i'm done with stochastic gradient descent i'm done with gradient descent and that's boring and that's not telling me anything and it's not useful right i really want to use stochastic gradient descent and gradient descent to get to some good set of parameters and so that's not going to happen if i have these step function activations and so i'm really going to have to do something else if i want to apply gradient descent or stochastic gradient doesn't another issue with our current setup is what if i want to do regression you know here so far we've talked we've been talking about f2 as a step function it returns one or it returns zero that's not very helpful for regression we really want some value that could be continuous that could take a range of values certainly not something that is only 0 or 1. and so we're going to have to resolve that and another one is that actually even if i wanted to use negative log likelihood loss you know this loss that we've talked about that can be really useful for classification that's also a problem because the step function returns 0 one and as many people have been having a really nice set of discussions in discourse about i i just can't put in a straight zero or one into that loss i really want something that's between zero and one something that i can interpret as a probability and so i'm going to need to do something else if i want to use that loss in this type of problem okay so let's think about what can we do to solve all of these issues that i want to use some kind of gradient descent or stochastic gradient descent i want to do regression i want to use negative log likelihood loss luckily it turns out there is basically a single solution to all of these issues and that's to use a different activation function that we are not beholden to step functions we don't have to use step functions they provide some nice intuition um they're they're useful for plotting things and understanding what a neural network does but we can use things that are like step functions and still have the properties that we want and so that's why we're now going to turn to choosing a different activation function than just a step function okay let's look at different activation functions so so our hypotheses look like this right now we're looking at neural networks with two layers in the first layer we create the features and in the second layer we get the labels from the features so our sort of typical linear classification or linear regression is going on in the second layer okay so let's ask first what if i want to do regression so the problem that we said was that if f2 was a step function if it only returned zero or one then i'm i'm not doing anything really interesting for regression like i really want something that could be generally real valued that i could get all kinds of different values for um i mean that's sort of the point of regression is to be able to have a range of values like if i'm trying to predict you know what is my um air conditioning bill going to be i don't think it's just going to be zero or one i think it's going to take on all kinds of values and i want to predict that so instead of a particular step function activation for f2 here i could have the identity so if the inputs to f2 can take on a range of values then now my outputs will be able to take on a range of values so i'll be able to get an interesting set of values for my regression okay what if i want to use negative log likelihood loss i want my output of f2 to be between 0 and 1. well if i use f2 equal to the identity that's not going to happen if i use f2 equal to a step function that's not going to happen i'm going to get 0 1. but luckily we have a great you know function that we know returns values between and that's our sigma function so we can use the sigmoid instead here for f2 okay our third question was what if i what if i want to use gradient descent or stochastic gradient descent i have this sort of nice general way of optimizing losses now if i want to use those well i need to be able to take derivatives and i need them to be useful so you can take derivatives of the step function it's just not super useful it just gives you 0 everywhere so what do i want to do here i want to get something that is that is actually non-zero that sort of tells me where to go and so let's think about how we might do that well first of all we need the f2s to be differentiable luckily each of the f2 values that we just said is fine for that z the identity is differentiable the sigmoid is differentiable so we're all set on the f2s on the sort of output activations okay but we need the f1s to be differentiable too so we have to think about what f1s would we choose now f2 and f1 are sort of accomplishing different things in this network when you look at the output layer you're getting the labels you're making the labels and so with f2 we're thinking about hey what are the choices that we've made before to do regression or classification or get these output labels with f1 we're thinking about creating the features or thinking about what features would be useful to have and so with f1 we want to think well what kind of features would be useful but also have useful derivatives and so there are actually multiple choices here because i mean in some sense features aren't as immediately obvious like what do you have to have in a feature we sort of know with regression and with classification what we're looking for in our output but what do you have to have in a feature so there's a few different options so one option is to just choose the sigmoid again you know we know the sigmoid we're familiar with the sigmoid it has derivatives everywhere and you know they're generally useful they tell us you know where to go um and they can tell us things about the sigmoid another option so you can think of the sigmoid as sort of a relaxed zero one function there's something called a hyperbolic tangent tangent which is a relaxed negative 1 1 step function so if for any reason you want something that's sort of a relaxation of negative 1 1 then that might be more convenient so you might use the tange instead another option is what's known as a rectified linear unit or relu so this is a function that's the max of z or zero it does have an interesting non-zero derivative on one side when you're on that side it's extremely easy to take that derivative because it's just the derivative of z so that's super easy it's one it's great um and what's kind of nice about this too is that if you're doing something like regression you can't really get like a really big range of values out you can get something that's like a linear function and so you might find that useful for what you're doing but ultimately at least from our perspective right now these are all perfectly fine functions that we could use to create features and that's what we're trying to do with them we're just trying to create some features that we could use okay so let's just do a quick little look at how this changes our setup that we saw before when we use these different functions like what's what's going to be different about what we're doing so before we said we were going to use a step function activation in our first layer so we make our features by using the step function activation and this is what our features look like as a result we had our sort of linear boundary and then there was this just sharp cut off at that boundary okay and then in our second layer we took those features and we turned them into a final prediction and so here we're going to show is the full function that we get out from x all the way to that final prediction i mean this is just what we saw before and again it's going to have these very sharp boundaries but it's only going to have values of 0 1 because we're using a step function activation and now let's just ask how would this change if we changed our activation function okay well let's go up here and change our activation function to for instance the sigmoid let's use the sigmoid instead and you can think to yourself how do you think that these step functions are going to change you might just take a moment and think you know what what do you think that these are going to look like when we change to the step function activation and in general like the idea that we've seen before about the sigmoid is that it's much like a step function but sort of smoother and so here i'm just showing the exact same linear boundaries that we had before and you can just see when we apply the sigmoid activation function instead of this strict step function activation that things kind of smooth out you know it's it's a little bit gentler we see that you know usually they're in life there isn't a clear dividing line you know things are just more or less likely and that's what this is expressing now we can go to our second layer down here and think about our outputs you know how does this all translate into taking input x all the way out to a final output and so one option is we might care about regression so if we care about regression our output activation will just be the identity or you know it's certainly a very reasonable choice that we might make and in that case we're going to get a very different looking function so one thing that's happening here is that we're actually seeing you know multiple sort of plateaus that are happening because we can actually see what's happening when we're adding in these different you know almost step function features and so we're seeing that here and then of course everything's smoother because not only you know are we choosing z as our output now but we had those smoother um inputs to go in we had those smoother features to go in now something you can see here is that when we're doing regression but we have these sigmoid um features that you still kind of get like these different layers you know there's there's different there's sort of a uh different levels that you can get if you don't want that you can get away from that with a relu if you were using a relu for your features you wouldn't necessarily have to have those levels that could be useful it could not be useful but by changing the weights you really can't get any value here for your output okay now we could also ask what happens if we use our sigmoid for our final output so we're just going to basically take what we have on the left here and smush it down with the sigmoid so with the sigmoid we're just taking all those different values that we get because you know essentially the left is the pre-activation because it was the identity and we're smooshing them down to be between zero and one so this comes back to the question from earlier you know is it that when these values are higher when we see these higher values in our pre-activation does that sort of represent that you know maybe some class is more likely in this particular case that's that's made sort of very concrete and made very um precise yes exactly when we have that higher pre-activation value that gets smushed to a higher value when we apply the sigmoid and so we have a higher probability of a particular class okay so at this point we have this ability to choose all kinds of different features we have this ability to have different types of outputs you know regression style output or classification style output and we can finally ask you know how do we learn these parameters because we can apply these methods that we know from before okay so before we can apply stochastic gradient descent or gradient descent we have to know what are we applying it to you know what what is the objective that we're going to try to minimize with stochastic gradient to center gradient descent and in some sense what i'm writing here is just a completely generic objective you know i'm just saying hey we said from the beginning that the notion of training error is one over the number of data points a sum over the data points of the loss between our guess and our actual for each data point our guess is the hypothesis applied to that data point and our actual is our actual label yi when we're doing supervised learning like this now in this particular case our hypotheses we're thinking of as neural networks and so they're going to have these big w and w naught you know multiple matrices that we're trying to solve for and so those are going to be the parameters here now again what's really interesting about this problem and very different from our problems before is that we have parameters not just for that final classification or regression step but also for all the features that we've built along the way and so all of those are in here all of those are in our hypothesis the parameters you know for that final layer which is doing the classification of the regression but also the parameters for all the hidden layers which are building up those features and so that's different from what we've done before but ultimately at the end of the day what we have is a function of all those parameters and it's a differentiable function and those derivatives are not just zero and so we can apply gradient to centers to casting gradient descent now the one slight thing that is untrue about that is there is this one point in the relu that is not differentiable but you basically almost never go there i mean or you essentially never go there if you're you know running around in stochastic gradient to center gradient descent you pretty much will never be at that point because it's just a single point and you can also even just define what happens at that point like maybe you know define the gradient to be zero there as well and so it's not as big of a concern as it might see on the face of it the bigger concern is that when you're at any other point which you pretty much always are that you have um a non-zero derivative for most of those points and you do still get that with the relu that there's that whole side of it that's non-zero and with the other ones we see that you really do get a non-zero value for the derivative everywhere for the tangent for the the sigmoid okay so let's just review what goes on with gradient descent and stochastic gradient descent before we apply them for our particular problem so let's think about a different problem with some different parameters let's call them theta and we want to apply gradient descent and so with gradient descent remember the idea of gradient descent is essentially hey i've got this objective function it's over some parameters in this particular case let's say theta one and theta two and i would like to find the minimum and so i'll start at some particular point i'll calculate the gradient i'll move in the direction of the gradient times eta say and i'll end up at a new point now here's the thing so we have previously said that the gradient is a vector that's equal to the size of theta so in this case the gradient's a two-dimensional vector because my theta is two-dimensional but it looks like i just plotted a vector in three dimensions so can anybody tell me what's the third dimension here why is there a value in this sort of up down axis so this is another one for the chat what do you think is the third dimension what is what is the value of the up down axis the not theta one or theta two axis okay there's some some answers that are on point um and also i think some um answers that are going the other way so this is a good i think this is a good thing for us to be discussing so what we're seeing here so what this vector that i'm plotting here is it's a three-dimensional vector it's eta times the gradient in the theta 1 theta 2 plane and then its value in the up down direction is the actual objective function so the starting point of that vector is the objective function at our initial point and the ending point of that vector is our objective function after we have moved in the direction of the gradient and so this vector isn't just the gradient and that's a really important thing to notice if we were if we were drawing just the gradient it would just be in the theta one theta two plane here we're drawing the gradient with a little extra information we're also showing where it came from and where it moved to in the objective great okay so once we've done that we've moved to a new point in the theta space and also implicitly in the j of theta space because we can evaluate the objective function of that particular theta and that's what we've done here in this gradient and so we can do it again we can say hey let's take another gradient descent step let's calculate the gradient let's calculate you know where we move to in the theta space and then let's calculate the j of theta at that point and then let's do it again and then let's do it again um and there's a question about the magnitude of this uh vector and yes so what's happening is the size of the vector in the theta plane the theta 1 theta 2 is the gradient times the step size and then the total magnitude of the vector in that direction is the magnitude of the gradient times the step size and then the magnitude or the difference in the j direction is the difference in the j values okay so that's gradient descent and then we've also learned now about stochastic gradient descent and you can think of stochastic gradient descent as like drunk gradient descent it's like you're taking steps but you don't have a lot of information and things are feeling pretty cloudy and so you start somewhere and you go somewhere off to the side because you only have the information from one data point you don't have all the data points like if you actually looked at j j is defined by all of the data points you know the the optimization objective when it's when it's this loss is defined by all the data points and so with stochastic gradient descent you don't have access to this beautiful objective function that we're plotting here you only have access to information from one data point that one data point isn't very reliable so it just sends you off in all kinds of crazy directions and so who knows where you're going next it's maybe over here whatever um and you just keep moving around like that now now again let's just make sure we review why would we ever do this you know why wouldn't we just go with gradient descent when it has this all this great information at every step and again the difference is that the cost of one of these steps is just a huge difference so to make even one gradient descent step we have to wait to go through absolutely every single data point so if we have a ton of data points that's going to take forever whereas with stochastic radiant descent yeah sure it's like a drunken meandering step but it took us so little time and by the time we've taken a lot of these steps that are just kind of in the right direction we'll probably in many cases especially with a lot of data be doing better than if we had waited that whole time just to take one step okay so we have these theorems from our previous um you know talking about gradient descent and stochastic gradient isn't that i'm just going to state very roughly and in very plain english that roughly say okay if our objective is nice and convex then gradient descent stochastic gradient descent should perform well in the sense that if you run them long enough they should get near the you know unique global optimum when that unique global optimum exists that unique global minimum in the case where we're minimizing things we're minimizing the objective okay so this is the challenge of neural nets this is why we're spending a few weeks on them um you know we're gonna have a couple of weeks with lots of different explorations um and if you are just using them in practice this is something that you should absolutely be aware of this is why there are so many different parts to this chapter there are so many different things to read about because the neural net objective is not just not convex it's like super not convex it's got so many different local optimum um it is something that to this day especially in the kinds of neural nets that people are really looking at and trying to use in practice people spend tons of time trying to understand what these objectives even look like you know in this particular cartoon and again it's always worth reiterating and reminding ourselves what we're doing with cartoons in this cartoon our parameter space is two-dimensional even for the simple two-dimensional neural net or sorry two-layer neural net that we've talked about today with maybe just a couple of hidden units or three hidden units we're in so much higher dimensions than two all ready for our parameter space our parameter space is going to be hugely high dimensional in real life it's going to get so big and so it's so hard to visualize what's going on and to get a sense of what's going on especially when we as humans can only see in two dimensions we can only really understand what's going on in two dimensions and so there's a ton of research that goes in just trying to understand these landscapes and to try and understand what to do with these landscapes um and in particular a lot of that's going to mean that we're going to have to do a lot of special things to optimize and to regularize so in general we just can't expect because this is so non-convex that we're going to be getting to a global optimum when we optimize and so what can we do about that you're going to see in the reading you've probably already seen in the reading that there's a huge bag of tricks to deal with this and again i can't emphasize enough this is a area of active research to think about what are good ways to do optimization in neural nets and a really tricky thing about this problem is that it's not clear that you actually want the global optimum and this relates to regularization now we've talked in the past about you know these polynomial bases with just a few different basis functions these polynomial features as being super complex and that you might overfit and so it's probably not going to be a surprise that if we have these super expressive you know many different step function like features that it seems like we could easily overfit you know especially if we just keep adding more and more of them and so we have these conflicting things that we want to do we want to minimize the loss we want to do really well in terms of loss but we also maybe don't want to do it too much we want to regularize somehow we don't want to necessarily over fit to all of that data that we have and so this is again just a huge challenge of neural nets and something that's very different from what we have looked at before in this class in terms of linear classification linear regression where everything was nice and compact so you could have all these nice guarantees about how you're going to perform now there's so much more work that has to go into optimizing and understanding what's the right way to regularize given that you know you don't necessarily want to perfectly optimize this objective and so these are the kind of trade-offs that you see in this bag of tricks and partly i also want to emphasize that you see all of these awesome ideas that people have come up with actually pretty recently in some cases in the reading and that's not the end of the story because this is such an you know a major area that people are working in right now new ideas are getting developed pretty recently and so it might well be the case that you know by the time in a few years we're using this in some other application that there might be new tricks to be using there might be even better things to be using and so i think that's why you really want to take away this very high level idea about what's going on with neural nets and what's going on with their optimization and learning in neural nets because any particular method might become obsolete in a few years it's hard to know and so you want to be just aware of the challenges and what these are trying to address okay so that all being said i just want to spend a few moments at the end saying we don't have to just have two layers so we've talked this whole time about having two layers in our neural nets the one where we create the features and the one where we do sort of the regression or the classification um but why stop there and we're gonna get into this a lot more when we talk about convolutional neural nets and there they definitely don't stop at two layers but let's just think about why we would want to have more than two layers so here this is just another representation as a function graph so here i'm taking the function that takes the x's takes all the w's and turns it just into the output um at any particular layer so i've sort of compressed a few of the functions into one function from what we saw earlier but that's fine i mean a function graph the circle is just a function it can be whatever function i want and here there's actually three layers to this net we first come up with a set of a super ones then we come up with a a super twos and then finally some a super threes and so that final layer sort of in general is where we do the linear classification of regression that was true in the two layer net and that's also true in our three layer net here and in higher layer nuts and the rest of this is sort of where we're making features now before we just made features out of our data but something we could do is sort of recursively make features we could make features and then we can make features out of those features and that's what you can do when you have multiple hidden layers and again we're going to see a really concrete example of this when we get into convolutional neural nets now a corollary of this observation is that in general when i'm adding or subtracting layers what i'm doing is i'm taking it from these hidden layers these layers that are making the features and so if i subtracted enough layers that i only got down to one layer all that would be left in my neural net is that linear classification and regression layer at the end and so just a single layer of a neural net with just one output is linear classification and regression again with the default features it's just what we've been seeing this whole time you can think of it as a special case of this broader idea now okay great so that's neural nets we have defined neural nets we've we've seen some examples of fifor neural nets um fully connected neural nets you're gonna be exploring neural nets and using them and getting into the all these issues of you know how do we actually optimize in them um in your labs and homeworks and everything like that um there is no live lecture next week um although there is a pre-recorded lecture from previous years so you can check that out if you're interested um i will note however that the week after that we'll be back and again we'll be talking about more layers basically what happens with more layers why might we use more layers how does that help us out it turns out to be pretty important so i'll catch you then thanks 

Okay it's that time, so let's go ahead and 
get started on what we will be calling lecture   eight. So you'll have noticed that there was 
no lecture seven this year, we had the holiday,   so our last lecture was lecture six but the 
naming convention that we were using is by week   rather than by how many lectures we've had before, 
so this is lecture eight. So we hope as usual   that you will participate in things like the 
Discourse and so just look for that “Lecture 8”   category for this one. Okay, so last time that 
we were together for a lecture, we talked about   neural networks: we defined neural networks, we 
talked about all kinds of different things like   how to represent them both from a mathematical 
standpoint like notation but also in this function   graph that we had talked about. And in particular, 
we really focused on two layer neural networks. So   in particular, one layer was creating a new set 
of features and that was a new idea for us last   lecture, this idea of learning a set of features 
as part of the learning that we were doing rather   than just having them be something that we specify 
in advance, that we come up with in advance.   So that was a new idea and when we were doing 
that, we were having our layer that was learning   those features be fully connected, so every output 
was connected to every input by a set of weights   and then finally we had a fully connected layer 
that did the classification or did the regression.   In some sense, that wasn't new, that layer, like 
you could think of the neural networks that we   talked about last time with two layers. If you 
got rid of that hidden layer, you're back to   logistic regression or linear regression or one 
of the things we've seen before. So in some sense,   at this point, we've talked about one layer 
neural networks, we've talked about two-layer   neural networks, and today we're going 
to talk about deep neural networks which   basically means more than two layers and, 
in particular, more than one hidden layer.   So last time, we had this output layer that 
did the classification, put the features we've   learned together into either a classification or 
a regression, and then we had the hidden layer   that was constructing the features and so that's 
what we're going to be growing: we're going to   be growing the number of hidden layers. So 
we'll talk about that today. In particular,   there are lots of ways that you could do that, but 
we're going to talk about one particular way and   hopefully in a moment you'll see why. It's been 
a very popular way to have a deep neural network.   This is convolutional neural networks, it 
might be called CNN, might be called ConvNets,   there are lots of different words but we're 
going to talk about that as another hypothesis   class for things like classification, things like 
regression. Along the way, we're going to describe   the layers that really define convolutional 
networks, so things like filters and max pooling   which we'll have seen from the reading and time 
permitting we'll talk about how that learning is   a little bit different than what you had 
seen in fully connected neural networks,   although we'll still be doing these ideas of 
gradient descent, stochastic gradient descent   and then the way that you specifically apply 
those in neural networks like back propagation.   So it's not so different, it's just that the 
back propagation looks a little bit different.   Okay, so let's go ahead and start talking about 
this, but let's also start talking about why   are we talking about convolutional neural networks 
as a particular example of deep learning and the   reality is that there's been this big change, 
this big excitement, around machine learning,   in general, and deep neural nets, in particular, 
for the past few years and a lot of that, in some   ways, really kicked off with convolutional neural 
nets, although I'll add a little nuance to that   statement in a moment. Okay, so there was this 
ImageNet large scale visual recognition challenge.   Basically the idea is that there were tons of 
images that have been labeled with things like   Amazon Mechanical Turk, they're in this particular 
challenge. They restricted to about a thousand   categories, these are like the classes we've been 
talking about in classification. The exact setup   is a little bit more complex than what I've been 
describing. If you're really interested in this,   I strongly recommend you check out the Wikipedia 
article is pretty cool. Also, there's this great   retrospective by Olga Russakovsky and a number 
of other people with Fei-Fei Li and I've cited   that at the bottom of this slide as well, so those 
are pretty cool to check out, but anyway there's   this challenge with a thousand categories—so 
that's a lot more classes than we've typically   been dealing with—and they're all these different 
image categories, like “is there a strawberry,”   “is there a cow,” “what's going on in this image?” 
And the idea was to label the images correctly.   That was the challenge given to the various 
teams and so if you look at how, in this case,   four teams performed in 2011 (so 2010 was the 
first year of the challenge), you can see they're   all getting over 20% of the images wrong. 
So that's the way you should be reading this   vertical axis: it goes from 0 to 1, and 1 would 
mean you're basically getting everything wrong.   And so in 2010 and 2011, the winners of this 
challenge were using things that we haven't really   talked about so much in this class but were really 
big at the time called support vector machines   and then something really different happened in 
2012 where this one particular submission was,   in fact, a deep convolutional neural network and 
it was called AlexNet. This was a convolutional   neural network around eight layers. We'll see 
what that means shortly, it's also, I have to say,   about eight layers because the way people count 
layers can be different from one thing to another,   but you get a sense of how many there were: 
there more than two but not yet a hundred.   And it just really outperformed everything 
else, so much so that by the next year,   basically everybody, certainly most of the 
entries, were convolutional neural networks   and, in fact, in 2015, the ImageNet winner was a 
much deeper convolutional neural network having   over 100 layers by Microsoft, and so this 
seems like it was really changing things.   I think this paper has gotten like 60,000 
citations or something, it's probably even   more now. I feel like these types of papers you 
blink and it's like a thousand more citations.   But it's also worth noting that, like many things 
in science, it really wasn't as much of a phase   change as it looks like from the outside. A lot 
of these tools had been building up for years.   So while there's this very recent AI boom, which 
was very much set off by AlexNet and related work,   in the 60s and 80s and today, neural networks 
were around and they were very popular.   And so the idea of a neural network didn't start 
existing and, in fact, the idea of a convolutional   neural network didn't start existing. In 
1980, there was something like a ConvNet,   this neocognitron. In 1989, Yann LeCun et al 
were looking at ConvNets for classifying images,   they were recognizing digits for post 
office use, they were using back propagation   and so you can ask yourself: what changed from the 
80s and 90s? What really set this off? And I think   there are quite a few different things. So one 
is there's just better computation, in general,   and that's been the story of a lot of the past few 
decades. GPUs really made a huge difference, they   were perfectly aligned with the computations that 
people were doing in convolutional neural nets,   they really massively enhanced parallelization. 
And then also, there was just a lot more labeled   data, so there's things like Amazon Mechanical 
Turk, there's lots of data on the Internet,   for better or for worse. So this is an area of 
controversy sometimes that there's so many images   that people share and they aren't necessarily 
realizing that they're sharing it publicly and   those are getting used and swept up or maybe even 
if they're not sharing them publicly but there's   certainly a lot more labeled images now and that 
can really lend itself to this kind of thing.   Okay so this is just a little teaser for “why 
convolutional neural nets?” They're really a   big part of this AI boom that we've been seeing 
in the past few years and a really key component   of that story. But of course, we also care about 
them because they can do useful things. So there   are a lot of great potential uses of image 
classification: the ability to detect tumors   from medical scans, certainly that's something we 
would love to do, we'd love to detect different   types of tumors there. I mean, in general, 
there are so many things that we'd love to do   with medical imaging and this seems to have a 
lot of possibilities here. Also, of course, image   searches online, we really engage with images. And 
autonomous driving: we want to be able to detect   things like stop signs and do some pretty complex 
things with images and so for all of these reasons   we'd really like the ability to work with these 
kinds of data structures and certainly we haven't   looked at anything that's a very structured data 
set yet in this class, we've looked at individual   data points. Okay so we're gonna be talking a 
lot about images today and so let's just recall   that images are made of pixels. So one of the 
big questions we'll be starting off with is:   how do I encode this image in a way that now I 
can do learning on it in the way that we're used   to in this class? We have to have a data point 
that's like an x_1, x_2, x_3, x_4 and we have to   put it in and put it into our classifier and so 
this is starting to get us to the point where   we see “oh, where are those x's going to be coming 
from?” They're going to be coming from the pixels.   Okay, so in particular today, we're going to 
focus on particularly simple form of images:   images that are gray scale images. And in 
fact, we're going to go even simpler than that:   we're going to look at black and white 
images just for today. You can totally use   other types of images, other types of data 
with convolutional neural nets. This will   basically be purely for illustration. So 
for instance, if I had this image “hi”,   it might be composed of a bunch of pixels. Each 
pixel, in general, when we're looking at grayscale   images is going to take a value between 0 and P. 
Again, in today's particularly simple examples,   we're going to have just 0 being black 
and 1 being white and so P is very simple:   it's just 1. Even for grayscale images, you don't 
have to do that in general. So in particular,   in your lab you're going to see P being a 
more typical grayscale image value of 255. Okay but here, we're gonna be focusing 
on just these these 0-1 images again,   just for ease of illustration. And so if 
I had this image that was the word “hi,”   I would encode it in zeros and ones in the 
following way. Now again, we still want to ask:   okay, well how are we going to actually use 
this image as an input for a neural network?   Because again, we think about what have we been 
doing to our data to turn it into an input to some   kind of machine learning model? Well again, 
we've had something like x_1, x_2, x_3, x_4   and so what we can do is we can take this 
image and essentially flatten it and that   can turn it into the x_1, x_2, x_3, x_4 and so 
what I mean by that is exactly the following:   I just find some ordering of the pixels and I call 
the first one x_1, and I call the second one x_2,   and I call the third one x_3. In some senses, 
just in terms of encoding this, the ordering   isn't so important. In other senses, it will 
be important and we'll get to that shortly. Okay so at this point, I have a vector of values: 
I have my x_1, I have my x_2, I have my x_3,   up to my x_n, or... I always, always get this 
wrong: they should be superscripts. Oh no, no,   no, they shouldn't be, they should be subscripts, 
so these are in fact the dimensions of one data   point. Great okay, so these are all my features 
essentially for this data point, for this image   and so now I want to feed it into a 
neural net and so let's first ask:   what would that look like with the neural 
nets that we were looking at before?   Okay so previously, we saw these 
two layer neural nets in this class   and so this is the example we saw from last 
week. So one thing that we've done here   is we put these parentheses around the 
superscripts just to super emphasize that   they were superscripts. You'll have noticed in 
the reading and the labs and everything that   eventually that just gets to be too much to 
write and so we just write the superscripts   and so that's always a little tricky making sure 
when it's a superscript and when it's a power   but here, we're just emphasizing that. We have 
these two layers and so now we want to say,   “okay, how would we feed an image into this?” 
Well we totally can. I mean we have our x_1   through our number of features that we feed in 
the beginning and so we already have the ability   to put this into a two-layer neural network, to 
put this image in and do something with it. The   only issue is that we could do better essentially, 
that we could do things to have better performance   and that's essentially what we're going to be 
focusing on. So something I want to highlight here   is, remember: we had these two layers and the 
first layer was constructing a set of features   and then the second layer, this output layer, 
was taking those features and then making some   kind of classification. And so what we're going to 
be focusing on today is we're not going to change   anything about the output layer. We're still going 
to take features, do some kind of classification   with it or regression, and that will be 
unchanged, that will look like something like   maybe logistic regression or linear regression or 
what have you. What we're going to be focusing on   is that hidden input layer or the, in general, 
the build up of the feature. Okay so something to   notice about all the layers that we talked about 
last week is that they're all fully connected.   So every input is connected to every output 
by weight. Again, we're not going to change   anything about that in the output layer: we're 
still going to have our usual logistic regression   or linear regression, but we are going 
to change that in the hidden layers. So instead of having this fully connected 
situation where every input is connected   to every output, we're going to think about, 
essentially, is there anything more that we know   about this problem if we're looking at something 
like images? And in fact there is. And in general,   a rule of thumb in machine learning is at least 
to get started if you know something about the   problem, you don't want to force your method 
to try to relearn it, you want to encode that,   you want to add that into the method so it can 
focus on learning the things that are important.   And so what do we know about an image 
problem that we didn't know maybe perhaps   about a general problem? Well there's a lot 
of things that are very special about images.   So one is spatial locality. Like let's say that 
I'm looking at some kind of self-driving car   application and I want to detect stop signs, I 
want to know is there a stop sign in this picture.   Well, in general, I expect the stop sign is 
going to be in a bunch of pixels that are near   each other, I don't expect that the stop sign is 
going to be like up in the upper right hand corner   and also in the lower left hand corner and like 
dispersed throughout the image. I think that the   way that stop signs work is they tend to be in the 
same place, all their parts are in the same place.   So that's something that we have about images 
that we would like to encode. Another thing we   have about images or that we know about images is 
translation and variants. So I could have a stop   sign over on this part of the image, but I could 
also have a stop sign in the middle of the image,   I could also have a stop sign in another part 
of the image and those are all stop signs   and I would not want to have a situation 
where I need to have my neural network   learn a totally new representation of a 
stop sign no matter where it is in the   image. I would like to say I'm going to detect 
this stop sign no matter where it's located. And so we'd like to have a layer or a set 
of features that encodes these principles   and right now our fully connected layer totally 
doesn't. It says that pixel 20 is fundamentally   very different from pixel 1 and the pixels in one 
part of the image are totally different from the   pixels in another part of the image and learns 
weights for those different parts of the images   and so we need to come up with some new 
method that's going to encode these principles   because we don't have them right now and that's 
essentially going to be the idea of these filters   and max pooling and things like that 
that go into convolutional neural nets. Okay so let's start looking 
at a concrete example of this. In particular, we're going to look at a 1d image. 
So we're just looking at these two dimensional   images. I think we tend to think of our images 
as being two-dimensional, but here we're going to   look at a 1d image. Now first, I want to point out 
that a one-dimensional signal is not a toy signal.   There are a lot of real-life one-dimensional 
signals. So an example of this is if you look at   medical time series, like measuring your heart 
rate over time, even measuring something like   the number of COVID cases over time, these 
are one-dimensional signals. And in fact,   people have applied very recently. Here's just one 
example paper among, I'm sure, many convolutional   neural nets to one-dimensional signals. They 
might not be interpreting them as images so much,   but it's still a one-dimensional signal and it's 
going to look like this, it's just maybe not going   to be zeros and ones, it might be something more 
general. Okay we see this in a lot of other areas   too. So you can get time series, for instance, 
in a lot of spatiotemporal applications. So maybe   I'm looking at something like monthly temperature 
at a location. You can get it in genetics,   so the genome can be thought of as basically 
one-dimensional, so you can get data like that.   Another way that I might get a signal like this, 
maybe even a zero-one signal, is maybe I'm doing   some astronomy application: I point my telescope 
at a particular part of the sky and I might be   interested in understanding is there a light 
in that part of the sky, maybe it's a star,   maybe it's a planet, maybe I'm seeing a satellite 
passing through and so I could be just basically   saying, for every maybe 10 seconds, am I seeing 
light in that part of the sky and then maybe I   want to detect the satellites because they're 
going to be passing so quickly and so they'll be   ephemeral lights, they'll be just going for just 
a moment as opposed to a longer time. So this is   just a maybe a motivating example to have in 
your head for what this 1d image might represent.   Okay so now what we're going to do 
is we're going to apply a filter   to this image. So what I mean by a filter, 
here's an example: it's just three numbers   and we're going to take a dot product between 
these three numbers and basically every set of   three numbers in the image and get an output, 
and we're going to call that a convolution.   Okay so for instance, here are three 
numbers in my image, here's my filter.   So what I'm going to do to get my output is I'm 
going to multiply the first two: 0 times -1.   I'm going to multiply the second two numbers: 0 
times 1 and add that in. I'm going to multiply the   last two numbers: 1 times -1 and I'm going to add 
that in. So this is just a dot product between the   vectors [0, 0, 1] and [-1, 1, -1], the filter. 
Okay I can compute this dot product, the whole   thing is going to be -1, and then I put it in. Now 
if you have ever heard the words convolution or   correlation in the past, you might be thinking 
“hey, this really sounds like a correlation,   it doesn't sound like maybe the thing that I've 
heard of as a convolution in the past” and a   genuinely challenging fact of life is that there 
are just overloading of terms in so many different   areas and so you should think of this as being 
a convolution for the purposes of convolutional   neural networks. This is how the term is used 
here, that doesn't mean that it's always going   to be used the same elsewhere. If anything, this 
is one of those things that makes you really   realize you have to be careful when talking to 
somebody else to see does that word mean the same   thing that you think it means. I think a great 
example of this is the word normal: it means like   a million different things in a million different 
areas, across physics, across statistics, across   machine learning, everything, and so convolution 
is just one of those words. But here's what we're   going to be meaning by convolution: essentially 
taking this dot product and moving it along. Okay   so we're going to do the same thing for the next 
three set of contiguous pixels here. So we're   again going to take this dot product between these 
two values and we're going to put that in for the   output and just to check, could everybody respond 
to me or anybody who is up for it, respond to me   in the chat and just say what is the dot product 
going to be for this particular set of pixels? Awesome lots of totally perfect responses, 
everybody's getting that this is going to be zero.   Okay and then we just keep doing this: so 
we we go to the next three set of pixels,   we do the next three set of pixels, 
go to the next three set of pixels   and so on and so on until we've gone through every 
set of three contiguous pixels, every unique set   of three contiguous pixels. Okay so now we've 
performed convolution and a typical thing that   we'll do at this point is then, you can think 
of these as being the pre-activations just like   we had in our two layer neural network: we had 
something where we did some kind of combination,   some linear combination of weights and our data 
and we got pre-activations and then we put them   through one of these activation functions. So 
let's say that in this case we'll use a ReLU.   Okay so if we do that, then we recall that 
basically that's going to put an output of 0 when   our input is 0 or below and it's gonna just put 
out the output again, put in the same value again,   if the input's above 0. So here, we're gonna get a 
zero, here we're gonna get a zero, we're gonna do   zero here, we'll finally get a one and then 
we'll get a bunch of zeros. Okay and so I think   the question that we wanna ask now and this is 
a real question that again I'm asking to you and   you should feel very free to respond in the chat 
is: what does this filter do? We've applied this   filter but why like does it actually do anything 
for us? What does it tell us about this image? Great. I like the description of it as finding a lonely 
1. It's finding an isolated 1. It's finding a 1   that is flanked by two 0s. In this particular 
case, we're only looking at zero-one data and   so basically what's happening is we're doing this 
convolution and we're gonna get a negative number   or a zero if we don't have a one flanked by two 
zeros and we're going to get something above zero   if we do have a one flanked by two zeros. Again, 
if you think of this in terms of the astronomy   example, if we get a lot of ones that could 
be because we're seeing a star and it's just   moving slowly because of the rotation of the earth 
across our telescope, but if we get a single one,   maybe that's a satellite zooming across the 
sky and we want to detect those satellites   and so we would be interested in 
finding where are these isolated ones.   Okay great, now something that you'll notice 
here is that if our satellite zoomed by in   the first pixel or the last pixel, we wouldn't 
pick it out right now because what we've done,   is we've had to take every three pixels that 
come together and so we just won't get things   at the very edge of this image, at the very ends. 
Let's start in the beginning and so if we wanted   to pick that out, something that we might do is 
we might add padding to both sides of the image.   And so a really typical thing to do with padding 
is to make it zeros. I can't emphasize enough   how much everything that we're talking about 
is a choice and so what you should really do,   just like everything we've talked about 
so far in this class, is you should ask   yourself what are you trying to accomplish with 
any particular application and then ask yourself   what type of padding or what choices in general 
are appropriate for that application. But here,   if we wanted to detect these satellites, say, 
then we might say “hey, zero padding is reasonable   because we want to be able to detect something 
that could have been at the edge like a one   at the edge.” Okay so in this case, what we'll 
do is, once we've added our padding, we've added   our zeros to the edge, now we have the ability 
to apply the filter just a little bit further.   So now, we can apply the filter here 
to the three pixels at the beginning,   including the padded pixel and it's just the 
same as before: we just do this dot product,   now with the padded pixel included and 
in this case we'll get out of zero. Okay and then same thing at the end: we'll just do 
the dot product now with the padded pixel included   because that lets us go a little farther and we 
get out of zero. And something you'll notice here,   this is a very typical thing to try to accomplish 
with padding, is that usually we're trying to   get the image after convolution to be the 
same size as the image before convolution.   So you can say, in some sense, the filter is 
telling you what's happening in each pixel   and so now you'll have an answer 
for each pixel essentially.   And so once we've done that, we're then going 
to put these final pixels again through a ReLU,   and in this case we'll just 
get some zeros out at the end. Okay. Now so far with this filter, we've just 
applied it as is, but we can apply it with a bias   and so what we mean by that, is we take the dot 
product that we were taking before and we just add   the bias term at the end. This is essentially 
what we've been doing with biases all along:   we've been having, for instance in logistic 
regression, you had essentially a weight vector   which was theta and then you added this theta 
naught at the end. This is exactly like that: you   have this weight vector which is described by the 
filter and then we add this bias at the end and so   we're doing the same thing here. The only thing 
that's different is that we're moving this along   within our data instead of just applying it to the 
whole data which is what we would have done with   weight vectors before in the class. So here, for 
instance, we would do this dot product. So the dot   product hopefully is pretty clearly zero because 
it's just zero times everything and then we add   the bias at the end, that's going to be one, and 
so we would get one out from this convolution.   Okay and then we would do the same thing 
going forward and we do our dot product,   in this case, we get out negative one, we 
add our bias as one and we get out zero. And then we just keep doing this: that fills in 
all of our numbers and finally we apply the ReLU   as before. In this case, you see that we get 
quite a lot more non-zero numbers once we have   this bias, so this bias really does change the 
kind of output that we're getting from our filter. Okay. Now in general, we don't have 
to have a filter that's just made up   of integers, we could have absolutely any real 
weights here. So in general, if we have this   filter that has size three, so its length is 
three, then we would have these three weights.   Let's call them w_1, w_2, and w_3, 
again they can totally be real values,   the [-1, 1, -1] is nice for illustration, but we 
could have 0.9, we could have -1.2, you have all   these different values for the w's and the same 
things with the bias b: the b will, in general,   be will be something that is real valued as well 
and we just do the same thing, we do this dot   product. Once we have our w_1, w_2, w_3 and our b, 
we do this dot product with each set of 3 pixels:   we add the bias b and we get our convolution and 
then we can put this through that ReLU layer. And it does not have to be a filter of 
size 3: it could be a filter of size two,   it could be a filter of size four, it 
could be a totally different size filter,   it's really a choice and that is part 
of the architecture, the building   of these filters and, in general, 
of the convolutional neural network. Okay. So in this particular case, we 
did an example of size three and we   saw what it did and, in general, it's worth asking   what are these filters doing? What are 
we accomplishing with them and so on? Now just to think for a moment, we 
want to think: what happened here?   We could have taken this one dimensional image 
and we could have applied a fully connected   neural network layer to create our features, we 
could have done everything we did last time with   the activation, with the activation function and 
all of that, and so let's ask ourselves: what's   fundamentally changed here? In this particular 
case, supposing we did have a filter of size 3   with a bias b, this is another real question for 
the chat: how many weights will I have, including   the bias term? So essentially I'm asking how 
many parameters do I have when I use this filter? Great. So in particular, the number of parameters I'm 
gonna have here, as many of you are pointing out,   is four because I have the three weights 
w_1, w_2, w_3, and I have the bias b.   So even though I move those around in the 
image like I'm going to move them to the   next pixel and so on, I'm going to use the same 
weights every time I do that and so essentially   at the end of the day. we're going to build up 
this model that depends on these weights but we're   going to try to then learn our parameters. So 
this is what we've done a million times now: we've   built up a model that depends on some parameters 
and then we try to learn those parameters   and here if we had this filter, the number of 
parameters we'd have to learn would be four,   because it would be the three weights that 
we just keep moving along and this bias b.   Okay, now let's ask ourselves: what if 
instead we had used a fully connected layer?   So what would a fully connected layer 
look like here? First of all, let's notice   that I have 10 inputs, those are my pixels, so 
a data point consists of 10 pixels, in this case   x_1, x_2, x_3, up to x_10. In this case, this 
particular data point is [0, 0, 1, 1, 1, 0, 1,   0, 0, 0], but I would have other data points and 
things to do like that but the input size is 10.   And then my 10 outputs are what I get after this 
convolution and ReLU and so I could have instead   had a fully connected layer with 10 inputs 
and 10 outputs and so now my question   for you, which I see some of you are already 
answering which is awesome, is: how many weights,   including biases, do I have for a fully 
connected layer with 10 inputs and 10 outputs? Okay lots of awesome answers here: let's 
walk through this for anybody who isn’t   immediately thinking of this, or even 
if you are thinking of it just to check   your mental map. So if we had this fully 
connected layer, we would have 10 inputs,   we'd have 10 outputs and so for every output, 
we would have to have a weight for every one   of the inputs and for our bias. And so that 
means, for every one of our 10 outputs, we have   10 weights plus a bias, so 11 total weights. 
Now we just multiply those together and we get   that we'd have 110 weights. And so even for this 
super simple case, because your images generally   aren't going to be 10 pixels long, and this is 
just going to be way more different if you get   to higher levels of pixels, but even for this 
very simple case, we see that there's a huge   difference between the number of weights in the 
convolutional layer and a fully connected layer   and the fully connected layer, because everything 
has to connect to everything else, you get this   huge number of weights and biases whereas with 
this filter we just have a very small number   of weights and biases because we're reusing 
them essentially again and again and again   and so if this is capable of encoding the things 
that we want, this could potentially be a really   big savings. We don't have to learn 110 
parameters. Now we just have to learn   four parameters. Another way to think about 
this, again, is like in terms of that stop sign:   if I had to learn this fully connected layer 
and I wanted to learn the kind of information   that was in this filter, I'd have to relearn 
this filter for every single part of the image,   I'd have to relearn that there could be a stop 
sign in every part of the image and so here,   instead we're saying “hey I just 
want to learn this one filter   over the whole image” and so that's where the 
savings is coming from. Now of course, that   depends on the thing that we want to learn and 
encapsulating what we're actually interested in,   but if that's true, then 
this is a really big savings. Okay so that's a 1d example, let's go on to 
something that's a little bit more representative   of the types of images that we would see 
in real life and that's two dimensions,   at least in terms of images. Again I can't 
emphasize enough: 1d signals absolutely   occur in so many parts of real life, but for 
images I think we're used to two dimensions.   Okay so let's look at a 2d image: this is actually 
our familiar 2d image from before that says “hi”   and it's just composed of a five by five set of 
pixels here. And so with our five by five set of   pixels, we're now going to apply a filter and 
now, because we're dealing with 2d images, our   filter will also be two dimensional. So in this 
case, it happens to be a three by three filter,   and the way that we're gonna apply the filter 
is very analogous to what we did before in terms   of convolution: we're gonna line it up with 
the part of the image that has the same size,   a subset of that image, and then we're 
going to multiply together the values   in each pixel that aligns. So in particular, 
we're going to start by taking this upper corner:   we have a 1 and a -1. When we 
multiply those together, we get a -1. Now, we're going to take the next two sets 
of pixels that align between the filter and   the subset of the image: we have a 0 and a -1. 
We'll multiply those together and we'll get a 0.   Now, we'll take the next two sets of pixels 
that align: it's a 1 and a -1. We'll multiply   those together and we'll get a -1. Now we'll 
go to the next two sets of pixels that align:   we have a 1 and a -1. We multiply 
those together and we get a -1. Okay we get a 0 and a 1. We 
multiply those together, we get a 0. 1 and -1 is a -1. We keep doing the same thing 
and so for every one of these nine elements,   we're multiplying it together. Now this is 
really the same operation as a dot product. One   way that you could think about this, as doing 
this dot product, is that you're flattening   this subset of the image and you're also 
flattening the filter in the same way to   make a vector and then you're doing a 
dot product between those two vectors.   But it's also perfectly legit to think of it 
as just being you're multiplying the pixels   that align in the two things together, you're 
adding them all up and you're getting a value,   in this case, it happens to be negative seven, 
and so we'll put that down here. And eventually,   we're going to fill in something that looks like 
an image by doing these convolution operations. Okay and so the next thing we would do is 
we'd find another unique subset of the image,   say this one, to perform this convolution. Now 
what we could do is we could step through again   and say “okay, I'm multiplying a 0 and a -1 
and I get a 0” and so forth, but something we   might notice about this filter is that it's -1s 
all around the outside and a 1 in the middle,   and so one way you could quickly calculate 
this is to say “oh, I'm going to take   negative times the number of ones in my image 
that aren't in the middle, of my image subset,   and then I'm going to add one if there's a one 
in the middle.” So in this particular case, I'll   notice there are three ones in my image subset, 
that thing that's in the blue square, that aren't   in the middle and there's one 1 in the middle, 
so I can take -3 + 1 and my output will be -2.   Okay so just to check on that, let's say I 
go to the next step. Can you tell me in the   chat what is going to be the convolution of 
this filter with this subset of the image? Awesome looking good. Great, so I think many 
of you used the shorthand and said “hey,   there's four ones that are on the outside, 
not in the center of this subset of the image,   and there's no 1 in the middle 
and so I'm going to get out a -4.” Okay. And then what we would do is 
we would just keep applying this:   so we would say “okay, I've made this convolution 
here, I can go to the next subset of the image   that looks like my filter, this next 3 by 3 subset 
of the image.” I apply this filter convolution,   I get out, in this case, -5, -2, -5. 
I just keep doing these calculations. Now something you'll notice here   is that, at this point, I've applied this filter 
to my image and now it's much more striking than   in the 1d case that I've dramatically reduced the 
size of my image by going through this convolution   step: I went from having a 5x5 image to having 
a 3x3 image and, again, there are a few things   I might be interested in here. One, I might be 
interested in having my output be the same as   my image. In part, though, because I want to say 
something about the pixels around the outside.   If I want to be able to talk about these 
edge pixels, if I want to be able to ask   if my filter applies to them, I'm going to 
need to do something more because right now   I can't center my filter at those edge pixels. 
And so this is what padding will accomplish.   If I apply padding all around the outside, then 
I can center my filter at those edge pixels,   and I can pick up something about the edge pixels. 
So again, it depends what you want to do but this   would be a very typical thing to do here to be 
able to say something about those edge pixels   and to be able to apply our filter there 
we would want to apply this padding. Okay so once we've applied the padding, you can 
go in now and there are new sets of these 3 by   3 image segments that I can apply my filter 
to. So in particular, this is one of them   and it's just the same operation 
as before: we say how many ones are   around the outside of this little image 
segment, this is subset of the image:   there's one, there's one inside, and so we're 
going to get zero by adding those together. Now we can go to the next step and we 
can keep doing this and we can fill in,   now, an outer layer around what we had before. So we just step through, we do the same operations 
as before and we just have a lot more that   we can do because of this padding. And you'll 
notice, now, once we've applied this padding,   by design, we now get back something that has 
the same size as our original image. So our   original image was 5 by 5. Now that we can apply 
this filter with this particular set of padding,   we can get this nice 5 by 5 image out. 
Now it won't always be the case that the   right amount of padding is just a padding of 
size one. That happens to be because of this   particular size of filter but it is a very 
typical thing to use an amount of padding   that will let you get back something that 
looks like the size of the original image. Okay so now we said “hey, the usual 
thing is we don't just apply convolution,   that's our pre-activation. We then typically 
will apply some activation function   just like we did for our fully connected neural 
networks that we were talking about before”   and so now we're going to apply an activation 
function again, let's just say it's a ReLU,   and so if we apply that, we recall that what 
happens in a ReLU is that anything that is zero or   below gets set to zero and anything that's above 
zero gets set to whatever its value is and so in   order to apply a ReLU here, we can just identify 
what is happening here, what values are above   zero, and in this case, there's actually just one, 
there's actually just a single value that's above   zero and so it'll be really easy to know what all 
of the other values are: they will all be zero. Okay so we just set everything to zero. So 
this is the operation or this is the final   activation value, this is what we 
finally get after doing all of that work,   both applying the filter and then applying 
a ReLU to each one of these pixels.   Okay and now again, we can ask ourselves “what did 
this filter do? What was this filter doing? What   was the point of this filter? Did it accomplish 
something that we can explain in words? Again   this is a question for the chat, so if you could 
put your answer in the chat that would be great. Good stuff. Okay it found an even more 
lonely pixel. This one was saying,   much like our previous one, much like our previous 
filter, it's asking—it's not a quarantined one,   that's another great way to put it—it found 
a one that is surrounded by all zeros,   so we're looking for a one that's surrounded by 
all zeros. In this particular case, another thing   that people have noticed, is that that's the dot 
on the “i” right because here we have this word   “hi” that's spelled out and so essentially what 
we've done with this filter is we have detected   this dot, it was a dot detector. In this case, 
because we have an image and we're interpreting   it as an image, we can really think of that as 
being a particular dot that we are detecting   and that's what this filter has detected. Now 
something that I think is worth is interesting   and worth noting here is that we would not have 
detected the dot on the eye without the padding.   So it was specifically the padding that filled 
out that, hey, there's there's just nothing on   this other side of the “i” essentially and so now 
we can go in and find things and because this was   on an edge, it was something that we could really 
only detect once we had done the padding but once   we did that, we can detect that dot and another 
interesting thing about this filter is because   it's center aligned, we're seeing that we can 
really recreate exactly where that dot is because   that's exactly the pixel where that that one ended 
up being after the filter and after the ReLU. Okay and again, you don't have to have just 
negative ones and ones for your filter,   you don't have to have integers for your filter 
and your filter definitely doesn't have to only do   dot detection that was just, I 
think, a nice illustration here.   So in particular, we'll see in a moment that you 
can have more general weights. Before we do that,   let's just recall that you can have a bias 
term and that's no different in this case. You   can also have a bias term even when you're 
in the slightly higher dimensional cases,   so now we're in this two-dimensional case, and the 
bias means the same thing in that you essentially   do this thing that is like a dot product, you take 
your subset of your image, you multiply each value   with the corresponding value in the filter, you 
add them all up and then finally you add the bias. So we have this linear combination between the 
weights and the filter. Think of the elements in   the filters being weights and the data points even 
with padding and then you add the bias at the end.   So in this particular case, we would say “okay,   again there's one, there's a single 
one that's outside the middle one,   so we're gonna get a -1. There's a single one in 
the center for this blue image, so we're gonna get   1. We add those together, we get 0. We add the 
bias 2 and so the final output here will be 2.” Okay just a quick check, again, let's say that I 
apply this filter with the bias to this subset of   the image, what am I going to get for this output, 
for the convolution? Just a question for the chat. Great, good stuff. Okay so most people here are 
saying -2. So the first thing that you do, again,   is you apply just the convolution without 
the bias. Once you apply the convolution   without the bias, you'll get -4. Then you add 
the bias of 2 and then you get -2. So now,   we have this -2 and finally we're just going to 
fill in the rest using the same set of operations. And without going into that, I'll just 
point out again, just emphasize again,   this filter does not have to be negative ones 
and ones. In general, we're going to be allowing,   and this is actually pretty important that 
we're going to be allowing, real values for   these filter weights. So here you might call them 
w_1, w_2, w_3 up to w_9, we might call the bias b. But these are very much operating like the 
thetas from before and the bias is like the   theta naught or the w's from before and the 
bias is like the w naught. We've seen this   kind of template a bunch of times before, this 
is just a slightly different way of applying it. And it's important that these be real valued 
because we're going to try to learn them later,   we're going to try to do things 
like gradient descent on them later   and so we want them to be able to have a really 
great range of values and you'll be able to take   derivatives and stuff like that and apply them and 
still have valid values for the w's. Okay, so this   is a very general 3 by 3 filter with a bias b. 
Again, you don't have to have a 3 by 3 filter,   so we could instead, for instance, 
have like a 2 by 2 filter,   so here we would just have four weights w_1, 
w_2, w_3, w_4. I could have a 4 by 4 filter,   it could go from w_1 up to w_16 and certainly that 
would make more sense as I got to larger images,   images that weren't just 5 by 5 images, but 
that would be something that… Most images   that we would encounter would probably be bigger 
than 5 by 5. Okay question. Oh the question is,   sorry, just go for it. Yeah the question was just 
basically: how do you determine the values of the   filters such that they allow the CNNs to learn 
the specific image properties? Great yeah. So   I think the premise of this question alone is 
already a nice point and I want to abstract it.   So we are going to determine the weights 
of the filters, we're going to learn   the weights of the filters. So one, the way you 
can think about a lot of the things that we've   been doing in this class is that we first start by 
specifying a forward model. We say, for instance,   for linear regression, if I have some data and I 
have some parameters, I can now decide what do I   do for my prediction on that set of data for those 
parameters. Once I specify that and I specify a   loss, I'm able to go back and learn the parameters 
by trying to basically minimize the training laws   and we're going to do the same thing here. 
We're going to say, first, we're going to   build up this forward model that says “hey, 
if I had some data and I had some parameters,   what is my label on a new data point going to be?” 
And so that's essentially what we're working on   right now, that's what we're going through right 
now, and then once I have done that and I have   specified a loss—and at this point you're familiar 
with a lot of losses that we might choose,   like if I'm doing regression, I might do the 
squared error loss. If I'm doing classification,   I might do something like a negative log 
likelihood loss or something along those lines—and   then once I have that, once I have the loss on 
my training data, I can minimize it. And so this   will be no exception to that, it's always the 
case that the math for minimization might look   a little bit different but even then we tend 
to be applying things like gradient descent,   stochastic gradient descent for that minimization 
and so a lot of it looks very similar,   it's just like what do those gradients look like? 
And that is exactly what we'll be doing here too. Okay so the reality is we think of all the 
images we see as being two-dimensional but   the reality is most images that we see are 
probably best described as three-dimensional   and the reason for that is that they don't just 
have a width and a height, which is what we're   very used to thinking about, but they also 
have a depth because, if they are in color,   they have the encoding of that color. So for 
instance, we might have a red, a green, and   a blue value for each one of our pixels and so as 
soon as we have those three values for each pixel,   we further have the depth. And in particular, if 
it's red green blue, that depth has size three   and so even though when you hear 3d image, I think 
you tend to think like oh something that looks 3d   or has some cool property, what I really mean here 
is an image that you're familiar with, like it's   like the computer screen that you have right now 
is a 3d image. What you're looking at right now   is a 3d image just because each pixel has these 
red green blue values that describe the color or   something similar to that. And so for that reason, 
we often want to think about 3d images and you can   think of these 3d images as being a generalization 
of concepts that you've seen before. So when I   think of a structure that is a list of numbers, 
I'm thinking of a 1d image, it's a vector.   When I think of a structure that is a list of 
list of numbers and they all have the same size,   then I'm typically thinking of a 2d structure and 
it's a matrix. So I could have something like,   for a vector, I would just have maybe the width 
of that vector or the length of that vector,   so it has one dimension, and I think of a matrix, 
I think of the width and the height of the matrix   and that describes the matrix the size and 
that's two dimensions. And now a tensor is   just the three-dimensional generalization: I have 
a width, I have a height, but I also have a depth.   Now that's a little bit hard for me to draw all 
the numbers for and part of that is because,   again, we as humans, as we said before, can only 
see in two dimensions and so even getting into   three dimensions sometimes can be a little bit 
challenging but this is certainly an idea that   comes up a lot in image processing and beyond. 
For images, one reason that it comes up a lot   is what we've just seen that typically images are 
expressed in a way that they also have a depth,   specifically just because of that color encoding. 
We'll see in a moment why we also might care   about these tensors even if we were working just 
with two-dimensional images it's gonna come up.   Because tensors are so big and image processing 
and they come up in neural nets and convolutional   neural nets and so on you see this word bandied 
about a lot. So for instance an example of this   would be the software or the platform Tensorflow. 
So this is very common to use for something like   neural nets but also, more generally, machine 
learning and you can see where that logo might be   coming from and why it's being called Tensorflow. 
And just as we had one-dimensional filters and   two-dimensional filters, we could also have 
three-dimensional filters. And so I'm just gonna   give a geometric intuition for this rather than 
running through the numbers like we did before,   but it's all the same. We would take our filter, 
we would go to our image, and we would say “hey,   for this image, where can we put a filter 
of this size to align with the image?”   And then we would step through the pixels of 
that image or the items that are in the width   and the height and the depth, the elements of the 
tensor. And so we would just step slowly through   just like we did before, we would apply again 
this notion of a convolution and we would get out   again something that has this width and height 
and depth, especially if we apply padding.   Okay we would do this all throughout even down 
at the bottom and we would get out our output.   Okay so now I said that there's another way that 
tensors arise and so let me mention that as well.   So even if I have a two-dimensional image, 
something that I might do is I might apply   multiple filters. So maybe I apply my first 
filter and it's something like a point detector   like we had before and maybe let's call that 
filter F_1. And maybe I apply another filter,   that's like an edge detector, we could call 
that F_2. Then maybe I apply another filter   which is maybe a different type of edge detector, 
maybe one of them is a horizontal edge detector   and one's a vertical edge detector, and so I have 
all these different filters. And you'll notice now   that, when I put these filters together, 
I have a bunch of two-dimensional images.   So when I put them together, I have a 
depth which makes this into, in some sense,   a certainly a three-dimensional structure and you 
might think of it as a three-dimensional image. And so this collection of filters in the layer, 
all these different filters F_1 and F_2 and F_3   might be called a filter bank and each of 
their outputs might be called a channel. It's just some terminology but we see 
that this is another way that tensors   arise. So even if you're working just with 
one-dimensional images or two-dimensional images,   you might well find yourself dealing with tensors   and doing further operations on these 
tensors after this particular layer. Okay so at this point, we have these filters, they 
detect various things in the image potentially   and now we want to ask what do we do with that. 
Maybe I've defined my filter that detects dots   or detects edges or whatever it is and it's done 
its work and it's detected some dots or some edges   and now I want to ask, okay I've detected these 
things, what do I do with them? Well one thing   that I might do is I might pull them together, I 
might say “hey, in this little area of the image,   did I detect any dots? Did I detect any edges?” 
And I asked myself over this whole area:   what did I detect? And so this will be the idea of 
a max pooling layer. So here's a two-dimensional   example: you can think of this as being the 
output from our previous layer. So remember,   one thing that we're doing in this lecture 
is we're building up lots of layers,   we're not keeping ourselves to just one layer 
and so we're thinking of, “hey, I've applied my   convolutional layer with its filter in this case, 
I applied my ReLU activation function, and this   is what I might have gotten out and I notice 
there are actually a few ones that popped up,   something was detected.” It would be nice to 
say, “hey in this little area of the image,   was anything detected?” And so that's what we're 
going to be doing with a max pooling layer. So in particular, with the max pooling layer,   it's going to return the max of its arguments. 
So we're going to do a similar thing to what   we did before: we're going to specify 
a subset of the image size, let's say   size 3 by 3 and this will often be a square and 
so we might just call it size 3 for shorthand. And so what we'll do, is we'll again go to our 
image, we'll take a little subset of the image.   So here's that subset that we had before   and we'll ask ourselves, now, to apply the max 
pooling layer. So in the max pooling layer,   we just look at the maps of the arguments and 
so in this case, when we apply our max pooling   instead of our filter, we're just going to get a 
zero because the max of all these zeros is zero. Now we could do the same thing that we did before: 
we could step over by one and apply max pooling   again. And again, it's just a bunch of zeros, so 
we're gonna get an output of a zero. We could step   over one, apply max pooling again and say what's 
the max of all these numbers? Now we've got a one,   so we're gonna output a one. We could step 
over again and say what's the max of these   numbers? It's 1. And we could do the same thing: 
we could step over everything, even at the very   bottom here, and just ask what's the max in this 
little subset of the image? And certainly that   was something that we did with the filter was we 
just stepped through the image and asked “hey, if   I apply this filter, what happens? And now we're 
saying, “hey if I apply this max pooling, what   happens? But if you think about it, kind of what 
we're doing with the max pooling is we're asking   ourselves did I detect something by taking the max 
and asking if it's non-zero. I'm asking was there   any detection and so from that perspective, when 
I just step by one, I'm kind of being redundant,   like I'm asking the same question of a lot of 
pixels. I might instead want to just say “hey,   in this little area of the image, did I detect 
anything? Now in this little area of the image   that's separate, did I detect anything? Now 
in this area of the image that's separate,   did I detect anything?” And so I might do that 
instead. So if I do that, if I skip ahead, then   I'll say that I have a different stride. So you 
can think of all of the things that we did so far,   the max pooling on this slide but also the 
filters on the previous slides, is having a   stride of one. And what I mean by stride of one 
is that we looked at a subset of the image that   fit our filter or our max pooling and then we 
stepped over one pixel and then we did it again.   If we had a different stride, we would 
step over a different number of pixels.   So let's look at an example of that. Instead of 
a stride of one, let's instead look at a stride   of three. So in this case, a stride of three would 
be really natural because our max pooling has size   three and so we're going to ask of one set of the 
image whether something happened and then we'll   go to a different part of the image. So let's 
look at a stride of three: what would the output   be like here? How would things be 
different if we had a stride of three?   Well we'll still start in the corner of 
the image and we'll say, “okay, this is   our max pooling size. It's 3 by 3. I'm gonna 
apply max pooling and I'm going to get a zero   because the max of everything that's in that 3 by 
3 area is zero.” And now,I'm gonna start stepping.   I'm gonna step by one, I'm gonna 
step by two, I'm gonna step by three,   and now that I've made three steps, three pixels 
over because this is a stride of three, I will   again apply max pooling and I will say “okay, for 
this region of the image did I detect anything?   Is there anything that's above zero?” In fact, 
there is: the max of this set of things is one. And I can do the same thing in the other 
direction. I can say “okay, this is where   I was applying max pooling before 
and now I'm going to step by one,   I'm going to step by two, I'm going to step by 
three.” And now I will apply max pooling again   and I will say “okay, what's the max of what 
we have in the pixels that we're seeing here?   We have a bunch of zeros, but we have 
a one and so the max will be one.” And then finally I step by one, I step by 
two, I step by three. I could have also   stepped from above, it'll be the same thing, 
and now that I've taken my stride of three,   I can again ask okay well what 
happens when I do this max pooling?   I get the max of all the zeros 
and that's going to be zero. Okay now in this case, we've dramatically 
reduced the size of the image, but with   a max pooling layer, that's actually 
something that we're often looking for.   So with the filtering we're saying “hey, were 
there things that happened at any of the pixels   that we have?” With max pooling we're really 
combining this information. We're saying “hey,   in these different areas, did something 
happen?” And maybe there aren't so many   areas as before and so we get something 
that's quite a bit smaller potentially.   And you can imagine doing this to build 
up larger and larger features essentially. You can also use stride with filters. We didn't 
talk about that in this lecture but it's certainly   something you can do: you could apply a filter 
after some strides. But I think that the way   that we've talked about it is a natural way, 
a way that you might choose to use strides and   filters and max pooling. Something that's also 
worth noting is that there are just no weights   in max pooling, there was nothing here that was 
a value that we were interested in changing,   that we were trying to learn, we set 
these sizes, we set these strides,   there's no real valued parameter that's 
sitting around here. And so in this case,   we're not going to have something that 
we're going to go back later and say “okay,   let's gradient descent our way to learn some 
parameters, there just weren't any here.” Okay so let's bring this all together then: 
what would a typical architecture look like   for convolutional neural nets? Well, first you 
have your input. So here you can see there's this   picture of a car, that could be potentially 
an input to a convolutional neural network.   And here, you can see that there is a filter—in   this case, in three dimensions—because you 
can see that car has the three dimensions,   probably because it's some kind of color picture 
so it has a depth of three for each of its pixels.   And you can see that filter going over it and 
coming out with some output in a convolutional and   ReLU layer. Now once we have that convolutional 
and ReLU layer, a typical thing is to have a   max pooling layer and you can see again that that 
max pooling layer is going to dramatically reduce   the size of the layer. That would be something 
that we might see. Now once we've done that,   maybe we've detected edges or we've 
detected dots or we've done whatever now,   we can have another convolutional and ReLU layer 
and that might detect some combination of edges,   there's some combination of dots, because now each 
of our pixels is essentially a detection of an   edge or a dot so we can get these slightly higher 
order features. Now we have a max pooling layer   and now we can detect combinations of combinations 
of edges or combinations of combinations of dots   and our hope is that, over time, these become 
features that really matter, maybe we're starting   to detect a tire or we're detecting something that 
is actually a useful feature for what we're doing.   And so we do this a bunch, we do these 
layers a bunch. That's the dot, dot,   dot after the max pool. Finally, eventually, we 
have a bunch of values and we're going to treat   those as our final features and so we're going 
to take them out of this tensor, we're going to   flatten them into a vector, that is to say we'll 
just take the values of the tensor in some order,   treat those as features in a typical 
classification and so now this finer   layer is just exactly what we've been doing for 
many classes. We have a fully connected layer,   so that could be interpreted as logistic 
regression, it could be interpreted if   we were doing just regular regression as a 
linear regression. In this particular case,   this is a classification problem and so we 
might be doing some form of logistic regression,   in this case with multiple classes. 
So we have this fully connected layer   and then finally we apply, in this case, a 
softmax function to get our prediction. So   because we're using softmax, our predictions 
will be soft predictions, they'll be saying:   what's the probability of this being a car? What's 
the probability of this being a truck? What's the   probability of this being a van or a bicycle 
or whatever the set of possible options are? Okay and so we now just have a lot 
more that's going on in the feature   learning. In the previous class, we had just 
one layer that was doing the feature learning,   it was this fully connected layer and it 
was a fully connected neural network there,   and we saw what to do with that. And here, 
there's just a much richer set of layers that   are going into that feature learning and when 
we say that there's learning happening that's   really when we choose those weights later. So 
first we build up, again, this model and then   we're going to choose the weights and then what 
hasn't changed is that that output layer is still   the same, it's still doing classification, and 
still doing the thing that we saw in logistic   regression and still doing the thing that we saw 
in our neural networks for the output layer in our   previous neural networks lecture and so that's all 
the same, it's really this feature construction   that has changed. And so we have specified 
at this point a way to take a data point x,   so that's our image. In this case, it's the image 
of a ca in this particular slide, but in general   it's whatever image input we have and we specified 
a way to turn it into a prediction and we might   call that neural net, this is a neural net, but of 
course we've seen that there were a lot of choices   that went in along the way and so to fully specify 
this, you'd have to say: what was the architecture   you used? What was the filter set that you 
used? What was the filter bank that you used?   Did you use max pooling? What were all of your 
layers and how big were they? And so there's a   lot more to specify than we had to specify in the 
case of linear regression or logistic regression.   But once we do that, we have a prediction. 
It depends on the data point that we put in   and it depends on the parameters. And so now 
we can go back and learn those parameters.   And so let's just briefly recap how this 
relates to the things that we've seen in   this course. There's really this familiar 
pattern, at this point, hopefully familiar.   So in particular, we start by choosing 
how to predict a label. It depends,   so this is the label for a particular data point, 
so it depends on the features of that data point   and it depends on the parameters in general. So an 
example of this, let's say we had our data point,   an example of this was logistic regression. We 
said for the ith data point, we'll use logistic   regression and that'll tell us the prediction 
for the ith point and that might take the form of   probabilities over particular classes. Now once 
we take this over all of the different points,   one through little n, and once we choose a 
loss between our prediction, our guest label   and our actual label, we can get a training 
loss. So we can get an objective to minimize. And then the final thing we've been doing is we've 
been coming up with ways to choose the parameters   to try to minimize the training laws. Now we 
might also have a regularizer in here, but   we're trying to choose a set of parameters that 
minimize maybe some variant of a training loss. Okay and then we did this a bunch more 
times in this class. We said, “okay,   another option is linear regression.” We chose 
how to predict a label, we said we're going to   use linear regression. That tells us, given a 
set of data features and a set of parameters,   what is our prediction for the ith point? We chose 
a loss for instance, it might be squared error   loss in the case of linear regression. Once we 
have that loss, we can go over every single point,   not just one particular point but one up to little 
n and we can calculate our training loss for a   particular set of parameters. And then we can 
move around in the parameter space in step three   to try to choose parameters that are better. And 
our notion of better is minimizing this thing   that's like a training loss. And I say like just 
because there could also be a regularizer there.   Okay and then we did it again. So all we're 
doing in neural nets and convolutional neural   nets is setting up a different model. We're 
saying, “hey, now the way that I'm going to   predict for the ith data point is I'm going 
to use a particular type of neural net.” And that's going to depend on some parameters 
and it's going to depend on the data point.   Once I have chosen a loss, and these will be 
familiar losses, again something like squared   error loss for regression or negative 
log likelihood loss for classification,   then I will have a training loss over all my 
points, that'll be a function of my parameters.   And so I can ask myself: if I move around 
in this parameter space, can I do better?   Can I get a lower training loss? And that's what 
we're doing with things like gradient descent and   stochastic gradient design: we're just moving 
around and asking, “hey, can we get a better   loss by doing that?” Now the one thing that is 
a bit different in neural nets versus logistic   regression or linear regression is that you 
can't just say neural nets and you fully specify   what you're doing. As we've seen at this point, 
there's a lot of different things that you can do   within neural nets: you could do a two layer 
fully connected way of specifying the features,   you could instead learn the features 
with this convolutional neural net.   There are actually lots of other things that 
you could do that we haven't even talked about   and so one difference is to fully specify what 
you're doing, to fully specify your model,   to be really clear and reproducible, you want 
to say all of those details of your architecture   and the best thing you can do, of course, in 
general to be reproducible, is to provide code. Okay so this is a familiar pattern and now let's 
just look in a tiny bit more depth at what might   have changed when we're applying something like 
gradient descent or stochastic gradient descent   in the case of convolutional neural nets versus 
the neural nets that we've previously seen. So in order to do that, we're going to look at 
a particularly simple case. Just as an example,   we're going to look at a regression case with 
a single filter that has size 3. We'll ignore   max pooling for the moment. We'll assume that 
we have padding so that our output image from   this first layer of constructing a feature is 
the same size as our original image and we'll   assume our data points have dimension five, they 
have five features, again just for concreteness.   Okay so here's what a forward pass aka 
just specifying the model might look like. So first we're going to apply our filter. So this 
is what we saw before in the images and now we're   just writing out the words for what that might 
look like. So we're going to say that the ith   preactivation is the application 
of the filter which is given by W^1   and then we dot product that with the X elements 
but they are changing. So the ith pre-activation   has gotten from the elements of X that are around 
i essentially. And because this has a filter of   size 3, that's why we're looking at i - 1, i, 
and i + 1 because those are the three pixels. And this is a one-dimensional image 
because it's just dimension 5 by 1.   Okay we apply our activation function, our ReLU, 
to get out our outputs. These are basically   our learned features once we learn the W's. Now, 
we're going to have our layer that does either the   regression or the classification. Because we're 
doing regression, we don't have to worry about   activation function because it's just going to 
be the identity, so that's implicit in here, and   we're just going to have this linear combination 
with the A_i’s. In this particular case, there are   no biases, I'm just going to ignore them for the 
moment and then finally we have some loss. Because   we're doing regression, we might choose a squared 
error loss, but of course here, we're getting into   this notation we've been seeing this class where 
A^2, that's just a superscript, but the 2 that's   a superscript outside is a square and so you 
just want to keep those separate in your mind. Okay so let's briefly check, first of all, what 
is going to be the size of Z^1 as a vector?   So once we apply this filter, 
we get our pre-activations.   Z^1 is a vector, what is its size? 
This is a question for the chat. Okay great so a number of people are saying it's 5 
by 1. It's gonna be, again, the size of our image,   specifically because we chose enough padding to 
recreate the size of our image. So if we just   applied a filter of size three to our image and we 
didn't do any padding, this wouldn't necessarily   be the case, but because we chose enough 
padding, we will get this 5 by 1, so by design.   So now we apply the ReLU, so we're gonna get a 
5 by 1 vector there as well. Finally we apply   this weeding and so we get out now a one by 
one vector because this is just the label   for our data point in regression, we're just 
giving it some label, we're assuming W^2 is   going to be basically 5 by 1, so this will just 
be a dot product and we'll get out of label.   And then finally, our loss is also a scalar. Now 
something you might notice here is that three   doesn't seem to appear anywhere and that's 
because it's in the filter. So it's in the   definition of Z_i and each i, we're essentially 
taking a dot product over a vector of size three. Okay so now if you were doing 
stochastic gradient descent,   in order to try to figure out how to minimize this 
loss over all the data points, not just a single   data point, but over all the data points, then 
you might do stochastic gradient descent to try   to minimize this loss and you might do it for both 
W^1 and W^2. So I'm saying part of the derivative   for SGD, because I'm going to focus for the 
moment just on W^1. Of course you would do both   if you're doing regular sgd, but I'm just 
going to focus on the part with respect to W^1.   Now just as before, we can apply, so just 
as we did for the case of certain vanilla   neural networks, fully connected neural 
networks, we can apply the chain rule.   And so in particular, we're going to say “okay, 
the loss is a function of A. It's a function of   both A^2 and A^1, but let's go straight to A^1.” 
You could even apply the chain rule within this   term but I'm just going to say it's a function 
of A^1, so we can take the derivative of the loss   with respect to A^1. A^1 is a function of Z^1, 
so I can take the derivative of A^1 with respect   to Z^1 and Z^1 is a function of W^1, so I can 
take the derivative of Z^1 with respect to W^1. Okay and now we want to check that 
our dimensions work out. So what is   the dimension of the gradient of the loss with 
respect to W^1, can anybody say in the chat? Okay so this one's a little bit tricky.   Remember W^1, we're taking a dot product of that 
with a three long snippet of x. And so W^1 has got   to be size three. Now a way that you can think 
about these derivatives, in general, is that   you're taking the derivative, you're taking this 
gradient of a vector with respect to a vector.   The vector on top is going to tell you the 
second size and the vector on the bottom   is going to tell you the first size. So because 
W^1 has length three, we're going to have three   by something. Because the loss is a scalar, 
so it has size one, this is gonna be 3 by 1   and you can play this game for the 
rest of these. So Z and W, well W^1   is on the bottom, it has length three, so 
it's going to be a three by something vector.   Z is on the top, it has length five, 
so it's going to be a 3 by 5 vector. This is going to be the length 
of Z by the length of A,   so it'll be a 5 by 5 vector. And finally the 
loss dA is going to be the length of A by the   length of the loss, so it's going to be a 5 
by 1 vector and then you can just check out   all those inner dimensions agree and this is 
something that will end up with a 3 by 1 vector. Okay so let's look at what would dZ/dW look 
like here? Well we already said it's going to   be 3 by 5, so it's going to be the three 
elements of W by the five elements of Z. If we want to fill in one of its elements,   that's going to be the particular element of 
Z, d particular element of W. So in this case,   essentially, the second element of Z 
with respect to the third element of W. Now in order to calculate that, it will help us 
to remember what is Z as a function of W. And   so here, I've just written out that forward 
pass, Z_2, as a function of the W's and the   X's with the i’s filled in. So I'm taking i 
= 2 here from what's above and so if you look   at this formula, can you tell me what's 
the derivative of Z_2 with respect to W_3? Okay great, lots of great answers here. It's 
going to be X_3 because we just read that   off straight from the formula. You can do the 
same thing and it's worth checking for yourself   that you're going to get X_2 and X_1 here and you 
can fill in all the other elements in this matrix   and something that's pretty interesting 
here is like you're seeing a wave of X's,   you're getting a lot of x's repeated in a way 
that we didn't see before for neural networks,   and that's because we're moving this filter 
along in a way that we weren't before.   Okay, I told you that X has dimension 
five and so what does X_0 or X_6 mean   if X just goes from 1 to 5? Can you 
tell me very quickly in the chat? Great. It's the padding. So what 
we're doing when we're adding padding   is we're essentially increasing 
the size of X just for the purposes   of our algorithm and that's what's happening 
here and that's what I mean by X_0 and X_6,   I mean that we're bringing in the padding 
and so if we didn't have that padding,   then we would get a different size derivative 
here and we wouldn't get quite the same answer. Okay so this is just to 
illustrate how backpropagation   is mostly the same. Certainly the way 
that you apply it is the same: you're just   calculating these derivatives, you're just doing 
gradient descent as before for neural networks,   but the results of that are going to look a little 
bit different and in particular this dZ/dW term   is going to look a little bit different and it's 
worth reflecting on that. Okay so at this point we   talked about not just neural networks of one layer 
like logistic regression or linear regression, not   just neural networks of two layers with a fully 
connected network which is what we did last time,   but deep neural networks. We have covered now 
deep neural networks, there are multiple hidden   layers that construct the features. We've done 
this with a real focus on images and certainly   convolutional nets are really widely used in 
image processing and so we have built up to this   thing that is really responsible, that is what is 
behind all this excitement in AI, and so hopefully   you found that to be an exciting thing. Okay I'll 
catch you next time, enjoy the rest of your week. 

okay it's that time so before we get started today i just want to remind everybody that next tuesday november 3rd is election day in the us um so anybody who is able to vote in the u.s um you know hopefully many of you have already voted um but if you haven't uh now is a great time to make a plan and stick to it uh this is a real life picture of my pointer finger i have voted already in massachusetts um and uh i'm excited for that there's a lot of interesting things on the ballot in massachusetts including ranked choice voting um and it might be the case that uh wherever you are voting that's true as well um also there's this cool website it's a student group at mit um that's all about get out the vote uh so you might check it out vote.mit.edu cool so first of all let's just recall what have we been doing sort of this whole class we've really been talking a lot about supervised learning about regression about classification about these particular instances of supervised learning and in those cases we've been interested in making decisions remember back in lecture one you know we said that machine learning was a set of methods for making decisions from data so you know we talked about decisions like whether i put on a code today or not and so far our decisions are sort of separate um every time a data point comes along i make a decision and that's sort of the end of the story like i decide whether or not to wear a coat today um and now i'm done and of course sometimes that's just really not the case when i make a decision it could change the state of the world um and in ways big and small right so like suppose i own a fish farm and i'm breeding some fish and i make a decision to not feed my fish then my fish will die and now i have to make a totally different set of decisions like how to get a new job and so i have changed the state of the world by making that decision and i can't sort of you know make the same set of decisions with the same losses and gains that i did before and so that's what we're going to talk about today we're going to talk about decisions that can change the state of the world and are going to change what you do and so in particular first we'll talk about changing the state of the world with aptly named state machines and then we'll talk about making decisions with the aptly named markov decision processes so that's where we're going today okay so first let me just note that i'm going to be having an extended farming example in today's lecture so let's just have a little bit of background on farming definitely not a lot i'm not a farming expert um but for centuries farmers have known that planting certain crops depletes the soil so you might go from a rich soil to a poor soil after you plant a certain type of crop and so there are various ways that people deal with this so one is you might rotate crops so some years you plant a crop and some years you let the field life fallow that is to say you don't put anything in it um you don't plant that crop again and so that gives time for the soil to regenerate um if you grew up in the usa you might have heard of basically our most famous agricultural scientist george washington carver he's extremely famous he developed techniques to improve soil between plantings he tended to focus more on different crops that you could have in between planting cotton because cotton would deplete the soil and so he would promote for instance peanut planting in between the cotton plants today we're going to focus more on whether to plant or to fallow and this happens to be the subject of a bunch of research papers so here's a research paper that's more recent than the sort of very old painting and even george washington carver's time but actually you can find research papers on these sort of farming problems to the present day and in fact this research paper and those research papers use exactly the things that we're talking about today these markov decision processes and so they are a bit more complex in the models that they use we're going to use a simplifies model but this is absolutely something that people are using in practice okay so the first thing that we're going to do is we're going to develop this model for the state of the world and for changing the state of the world and that's going to be again this aptly named state machine so in this case if we're talking about a field like maybe i've decided to rent a field and farm in this field then here i might have uh as possible states for my field it might have rich soil or it might have poor soil so here are two possible states in this particular example now i also need a set of possible inputs these are the things that i can input into the system and in this case sort of the the possible inputs that we've discussed so far are planting the field or letting it live valid so those are my possible inputs into this system okay now i'm going to think about sort of a sequence of you know inputs and how they change the state and in order to think about that it'll help me to think about well where did i start what was my initial state and so if i imagine that i'm renting this field you know from somebody else and i started off uh maybe you know afresh and nobody else was using the field before in in my example it might be that my initial state is that it is rich soil you know it's great soil it hasn't been being planted for nothing's planted on it for a while and so it's ready to go it's ready for us to to do some farming now i'm going to need a way to understand how i go between different states and that's what the transition function is going to do for me it's going to say it's going to be a function that takes in first a state and then an input and then outputs another state and so in particular here is an example of a transition i might make if i have my input state or sorry my first state be poor soil and my input is that i decide to let the fuel stay fallow this season then after i do that i'm going to get rich soil so i start with poor soil i let it lie fallow i get rich soil now of course i need to specify what's going to happen over all the different inputs that i might have and so another input i might have is i might plant my field in which case i expect that maybe my poor soil is going to stay poor now i also have to say for every possible state that i start with for every possible action what's going to happen and so i would also need to specify what happens if i start from rich soil and i expect that if i plant on top of rich soil next season my soil will be poor if i plant on top of rich soil and i fat or sorry if i just lie fallow on top of rich soil i'm gonna get rich soil again now implicitly what i'm talking about there's a notion of time and so when we think about this transition function we're thinking about going forward one unit in time here it might be a growing season or a year but the idea is i have my starting state right now i have the the input that i put in and then at the next time step i have this output this new s and so for instance um i might say okay you know season zero before i even rented this field the soil was rich then i took my rich soil i planted now i'm gonna make a transition to find out what happens you know in season one what do i get out what type of soil and so here's a my first question for you in the chat uh what is the output of this based on our specification of the transition function in the diagram above so we've actually fully specified the transition function with this diagram and so my question again for for the chat is what type of soil or what state am i going to get out here for s1 okay great everybody gets it we're getting poor soil and again we just see that from the transition this is this next time step one and we're gonna see that we start from our rich soil that's s naught and we've just specified that we decided to plant that's just a particular action we could choose and then we just see on this diagram where that goes that goes to the poor soil and so the output here is poor soil okay now something that can often happen although we're not going to get into it too much in this lecture is that you might not observe the states directly and so certainly that seems very plausible in this farming example like you don't necessarily know directly if you have richer poor soil you'll probably have some kind of measurement apparatus that will measure something about the soil for it so for instance at least according to this research paper i linked in the beginning um the amount of moisture in soil the amount of water content is really important to whether it's rich or poor and so you might have some sort of like soil water measurement or soil water sensor that you use and so that's what this would represent is what sort of measurements are you actually taking what are the outputs that you're actually observing so of course in order to do that we have to have some way to go from our state to an observed output again throughout most of this lecture and in the notes we're typically going to be assuming that this is the identity function that we actually can observe the state directly but that doesn't have to be the case so again i could have something like a soil moisture sensor or some other type of you know measurement that i take on the soil and then i would just observe that i wouldn't observe directly whether it was rich report okay so in this particular case as i said we're just going to assume the identity output function here we're just going to assume that we exactly observe the state and so then this becomes sort of an easy thing we know that our set of possible outputs are exactly the state values rich and poor and in this particular case when we know that our state is poor then in fact we will observe that it is a poor state okay and then we can do this again we can say let's take another step you know maybe last time we planted and we got this poor soil so we might think hey how do i get back to rich soil maybe i'll i'll let my field like fallow this season and so i can ask you know what it what is going to be the output of my soil the next season after i've let it live fallow well in this case you can again read off from the state diagram up at the top that we're going to end up with rich soil and again just because this is the identity function for the output we're just going to get the exact same state output we're going to get this rich soil observation okay so this is a state machine it is described by exactly these six quantities that we've just talked about we have to say what are the possible states that we can move between what are the ways we can move between states so first of all what are the inputs that we can put in to make a movement happen what is the initial state that we start with how do we move that's the transition function it tells us you know when i make some action on a state what's the new state that i get when i take this input what's the new state that i get and then there's these aspects related to observation so i have a set of possible observations i can make these outputs and i have some output function that says hey if this state is hidden if it's latent you know what are you actually going to observe and that's this g okay so this is all well and good now we have this notion of a state machine a way to to move between states but what we're going to do next is we're going to ask ourselves okay well like this is nice to be able to move between states but we said we wanted to make decisions right again you know again just from lecture one we said machine learning is a set of methods for making decisions from data and so here we don't really have a way to make decisions um you know we could move to rich to poor um but the way that we've made decisions so far in this class is we've had some notion of what things are better to do that's the loss the loss tells us what's better because you make less loss like you want to minimize your loss and so we have to have some notion of a loss or a gain of how well we're doing in order to make decisions you probably have it implicitly in your mind for this example so far and we're about to make that explicit but it so far doesn't exist on the slide we haven't said what makes a good decision and so from that perspective we can't really make decisions yet so we're going to have to come up with a way to have a concept of loss or actually sort of the inverse of loss which is reward so that's what we're going to come up and do in a moment just before we do that i'll also just note you know here we have a particularly simple state machine which is rich soil and poor soil of course you could have so many more states you could have maybe a bunch of different variants on you know every time you plant you get even poorer soil and you know there's a gradation from rich to poor you could have more states from something else but this is our simple example here okay but we want to be able to make decisions so again the thing we're going to focus on right now is coming up with a notion of of a loss or again a very equivalent conception just sort of the reverse of reward okay so let's get rid of this example now in general for everything we do going forward in the lecture you actually could have this output function and it could be non-trivial like you don't have to ever observe the states but that's an extra layer of complexity that we're not covering right now and so for everything we do going forward we're going to assume that we observe the state directly you can think of that as an identity output function so i'm going to get rid of this part too the part about the output on the assumption that we observe the state okay okay so now we're ready to start building up this notion of a reward and only once we have a notion of again like what makes a set of inputs better or worse can we make a decision about those inputs and so that's what we're going to be focusing on here okay so what what is the reward in farming well it's probably your harvest um that seems like the a pretty natural notion of reward and heart farming and in particular you want your harvest to be bigger when you're farming the assumption is you have you have a bigger harvest you can sell it for more you can make money or you can eat that harvest you know it's just generally a good thing and so we're going to say that our reward function here is the number of bushels and a harvest um bushel for the purposes that we have here is just some notion of how much say wheat that we have harvested from our farm according to some kansas wheat website that i looked up which you know who knows how accurate it is but it says for wheat one bushel equals 60 pounds of wheat or approximately 1 million wheat kernels anyway it's a bunch of wheat so we're going to say we want more bushels in our herbs that's going to be our reward function and so in particular something that's going to be generally true about this reward function is that it's going to output a real number so in this case it happens to be you know how many bushels we harvest and we could even have fractional bushels um and so this could be any real number really any positive real number um but in general we could have other types of rewards but we're always going to be thinking of them as real numbers you know it could be dollars it could be something else okay so now we have to think of what's the input to this reward function well certainly some part of this input has got to be our action right i mean somehow you know if i plant i should get some bushels out and if i fallow i should not get any bushels out and so it makes sense that my input is going to be the input to my reward function um and i'm going to get some reward based on that input now in the past when we talked about having a loss we have some input like you know i decide to wear a code or not and then i get my reward and that's again just the end of the story and so what's really different about how we're approaching things today is that it's not just my input that determines the reward but it's also the state that determines the reward and if you think about that for a second in this application it's pretty clear that's going to be true if i plant in rich soil i expect to get a big harvest a really good harvest if i plant in poor soil i expect to get a worse harvest not as good of a harvest i don't expect to get as many good bushels at least okay and so let's make that concrete let's just specify just as we have an example of a set of states a set of inputs and the transitions let's make this specific and say what's a reward function that we might have this is just an example reward function so our example reward function might say if i have a rich soil and i plant then my harvest is going to be say 100 bushels on this field if i have poor soil and i plant i generally expect fewer bushels and so in this particular example let's say we'll get 10 bushels out of the field and then of course if i let the field lie fallow i really shouldn't get any harvest and so in that case i'm going to say that my harvest is zero bushels okay so this is an example reward function it's the example reward function that we're going to be using going forward just as we're sort of using this example set of states and inputs and transitions okay great so now we have a reward we're all set to go we you know this is what we talked about a way to make decisions um but while we're here let's just notice that there's something else that we'd like to change about this setup and that's this transition function namely right now this transition function is totally deterministic if i plant i get poor soil if i let my field lie fallow i get rich soil and the reality of farming is that this is not totally deterministic it's stochastic there's randomness in this so for instance if i have a really good year with lots of rain the soil might be very very full of moisture and i may get rich soil even though i planted that year so there's some randomness for instance due to weather and you know other forces as well and so we'd like to include that in our transition we'd like to account for that and so we're going to have to change something because right now this transition function is totally deterministic okay so in order to see how we can change this let's take our existing um plot here our existing diagram of how we move between rich and poor soil and let's focus on the plant action the plant input so right now we have two inputs that we could have we can have phallo or we can have plant we're just going to focus on the plant and we're going to do this to make our so-called transition model and so this this language you know going from transition function to transition model and this different word or this different notation t will indicate that we're talking about something stochastic and we'll nail this down in just a second okay so we're focusing in over here on just the plant action now another way that i could describe what's going on in this particular part of this diagram this plant action is i could say what what's happening is that if i plant i'm going to pour soil with probability one so here i'm going to say we're looking at this plant action and on each of these arrows i'm going to put the probability of this transition so i go to rich soil from rich soil to poor cell with probability one and implicitly then i go from poor soil to rich soil with probability zero when i plant so these probabilities have to add up to one i have to add up to a hundred percent that's what they do here um but in general they don't have to be just one and zero i could have different probabilities like for instance let's think about starting from rich soil then i have two options either i go from rich soil to rich soil or go from rich soil to poor soil now here i'm saying with probability one if i plant i'll go to porcelain with probability zero i'll stay at rich soil but in general i could have you know any two numbers that add up to one and are non-negative so it might be the case that if i plant with high probability i go to poor soil but maybe i stay in the rich soil if you know the weather was particularly good this season likewise if i start from poor soil it might be that yeah the vast majority of the time if i plant my soil is going to stay poor but every now and then with some small probability the weather is fantastic and i i'm actually able to get rich soil and so especially in something like agriculture where you really do have this you know um randomness this stochasticity um due to the weather you would like to account for something like this okay now of course in order to fully specify this we need to say what happens under each of our actions our plant to action and our fallow action before we do that i just want to say that there's another way that we can represent the information in this diagram in fact by the end we're going to have seen three different ways so the first way we're representing this information is in the diagram itself this is a diagram that fully specifies the plant action in this transition model but a perfectly equivalent way to specify the plant action is with something called a transition matrix so here we're going to write out the transition matrix for the plant action this is going to be a matrix that tells us how to go from basically a starting state to an ending state and so from that perspective the size of this matrix is going to be number of states by number of states to specify the matrix you have to specify the order of the states that you're talking about so in this case i've specified that i'm putting the rich state before the poor state i would get a totally different matrix if i put the poor state before the rich state and so it's very important to say which one you're going to use so here i'm using this particular ordering now we're going to take the convention that the rows are the starting state and the columns are the ending state so that is on whatever growing season i'm at right now that's going to be on the rose and whatever growing season i'm at next season that's going to be the columns okay so my question for everybody in the chat is what is this entry in the matrix so this entry in the matrix should represent what's the probability of starting with rich soil and transitioning to poor soil under the plant action because again this this matrix is just for the plant action great lots of great answers here okay cool folks are recognizing that you know we can look at the diagram above we can see hey what's the probability of going rich to pour under the plant action it is 0.9 and similarly you can get all the other information for this matrix from that diagram you can notice that going from rich to rich the probability of that is point one probably going from quarter rich is 0.01 the probability of going from poor to poor is 0.99 again all of this is specifically for the plant action you would have another one of these for the fallow action which we'll see in a moment okay now a couple of things to note about this matrix first note that every one of its rows adds up to one because no matter where you start with a hundred percent you got to go somewhere and so that's what we're saying here that there's 100 probability that you'll end up somewhere and then the individual choices are the different states there is no such constraint on the columns we don't have to have them add up to one we certainly see that that's not the case here okay so this is a transition matrix it has exactly the information in the diagram above it so if you knew this diagram you could make this transition matrix if you knew this transition matrix you could make the diagram now as i said we also want to specify what happens under the fallow action because we haven't fully specified our transition model if we don't see what happens under each action and so the next thing we're going to do is we're going to say what happens under the fallow action well fallow has to have exactly the same set of states because they're just the set of states that we're using but it might have different probabilities for going between those states and certainly we expect that we expect that in general if i have poor soil and i let it lie fallow or if i have rich soil and i let it life out if i let soil lie fallow then i'll end up with rich soil and if i let soil lie fallow i'm unlikely to end up with porcelain although it could happen you know there could be some you know drought maybe um and then i might end up with personal and so we want to represent that in this sort of random this the stochastic transition model okay so i said that there were going to be three ways that we could represent the transition model so now we have one as the diagram two is we would have a transition matrix for each one of our set of possible inputs and the third is going to be a function and so that function at this point we can kind of anticipate what that function is going to look like it's going to take our starting state it's going to take the input that we make and it's going to say for any ending state what's the probability of ending up there so that's why we have a state an input and another state okay so let's do an example like for instance let's look at this 0.9 here well this point nine represents the probability that on the next round on the next step at time t the teeth growing time we have poor soil given that on the previous time time t minus one we had rich soil and we decided to plant now let me just make a little note about notation here this capital s of t represents what's called a random variable and so the idea is if we were just sort of running through this model we don't know what the state of that soil is going to be at time t because it's stochastic and so we can represent that with st and say hey that's stochastic we don't know what it's going to be but if we refer to any particular state like poor soil is a particular state we could refer to that with a little s we could say hey this is a particular possible state it's one of our set of possible states and we'll refer to it with a little s okay so here is this again the probability of going from pour starting from ranch if i decide to plant and so we're going to call this t the function t of rich plant poor so the first argument in t is my starting state the second argument in t is my input and the third argument in t is my ending state and so that's again a third way to specify the transition model and it's sort of formally what you will typically see in a description of a markov decision process which it turns out we've been defining all along okay so there's two things that we've done at this point we noticed that we were missing when we had this state machine that just told us how to go between states we were missing a ability to talk about what's what's a good set of decisions what's a good set of inputs and we also noticed that we were missing this ability to talk about randomness about stochasticity and so once we include both of those two changes we essentially basically have a markup decision process i say basically because i'm going to make some cosmetic changes to this in a second but these are the two really big changes the big changes are we now have a way to talk about rewards about what makes a set of inputs good what makes it desirable to us and we have a way to talk about sort of stochasticity in the process randomness in the process okay all that remains at this point again are going to be some cosmetic changes so the next thing we're going to do is we're going to make those cosmetic changes okay so the first thing that's sort of a cosmetic change is we were calling these inputs when we were talking about a state machine now we're going to call these actions i was kind of calling them both all along um but they're basically the thing that we're doing that changes the state and so let me just figure out okay where all in this slide was i using this x notation for inputs and i'm just going to change it to the a notation for actions we haven't really changed anything here this one's just sort of a nomenclature thing okay the next cosmetic change is that when we were talking about our state machine we referred to an initial state in this case we're kind of going to think about all the possible initial states as we go forward and so we won't specifically say that an initial state is part of our markov decision process so i'm just going to get rid of this initial state again it's sort of cosmetic you know whenever we actually run this we're going to have to start from somewhere but we're not going to formally say that that's part of our markov decision process okay and our final thing this might not be quite as cosmetic is we have what's known as a discount factor so basically i'm just going to push that off to later in the lecture we'll talk about it later in the lecture but it is part of our markup decision process okay and so now we can get rid of this basically and say what is actually a markup decision process okay so that's gone we have our markup decision process it's got a set of possible states in our example they're rich soil and poor soil we have a set of possible actions that can change our states in our case they are plant and fallow in our example we have a transition model it tells us when we make an action how likely we are to end up in different states and it depends on where we started to it depends what state we started in we have a reward function that tells us what do we get from taking certain actions and what we get the benefit the gain we get from taking certain actions depends on the state that we're in as well and then finally we have a mysterious discount factor that remains to be just you know described but basically this is a markov decision process and again the things that really distinguish it from a state machine are one a reward function you know understanding what's actually good about our actions and this idea of a transition that can be stochastic okay so we said that the reason that we set up this markov decision process was in order to make decisions so now let's start thinking about making decisions what what how are we going to make decisions what are those decisions going to look like and what types of questions can we answer what types of decisions can we make okay so the types of decisions that we're going to tend to make here are are called policies so a policy is going to tell us you know what's our plan you know i've rented this field um you know for a few years and i want to know each year uh how how am i going to treat it am i going to plant it am i going to let it lie fallow what am i going to do and the policy is going to tell me it's going to say hey look at the state that you're in right now again on this assumption that i can't observe my state that that's a totally reasonable output and then i'm going to ask what action will i take this is my plan of action essentially based on the states i have and so now you know these are the types of decisions we're going to make and we're going to ask various things about these so one question i might ask is what's the value of a policy policy you know for instance how many bushels am i going to get from my policy how much um how much profit perhaps revenue can i expect to make and this could be really important because maybe i'm deciding whether or not to even rent the field you know i'm not sure that i want to rent this field um and do some farming and so this would help me make a cost benefit analysis and understand you know is this worth it to me also if i have two different ideas of what i should do with the field um we're gonna see this in a moment you know if i have two different plans that i'm considering if i knew the value of each of the plans i could decide between them i could say oh this one's a better plan this one's a better policy now of course in general once i'm comparing policies why don't i just choose the best policy i should just go straight to the thing that's going to get me the the biggest harvest you know the most reward um and so we're going to we're going to talk about that as our second question how do i choose the best policy out of the policies okay so what we're going to do next is talk about these questions oh but there is a question from the audience first yeah the question is what if we have a situation where the reward isn't always the same for a given input and state um could you also have a sarcastic reward oh yeah absolutely i mean i think i think this is a really interesting question you know once you understand these things these are just a bunch of choices that are modeling reality and essentially what we're trying to do is get close enough to reality to have a useful model that we can use to make good choices um but also typically what we do is we somehow come up short of reality just because a simpler model is often easier so i might choose a non-stochastic reward function just because it's easier to work with but the reality is absolutely in reality a reward function would very easily be stochastic as well you can imagine you know if i have the same moisture of soil or whatever that i could get a different harvest in different years and so i would absolutely want to include that you know something you know while i'm on the subject i'll just note something else that is a limitation of this is is what's known as you know mercavity so we have this markovian-ness we have this markov decision process we're assuming that basically everything we're doing only depends on the previous state but you can imagine in a world in which it depends on more states going back in fact probably that's you know more realistic in a lot of cases but sometimes the simplifying assumption is worth making anyway because it's simpler um we can solve the equations for it we can do the calculations that we're going to do later in this um but absolutely i mean there's some sense in which you know everything that we're doing can be improved and sometimes those improvements are totally worth doing um and you can come up with an even better set of decisions based on them and i think this reward function stochasticity is a great example of that cool okay but for the moment we're going to start with the simpler version um i think a good life lesson is always start with the simpler version and then compare it to the more complex version so let's start with our word function as describe and see what happens okay so first we're going to start by asking what's the value of a policy that was our first question from the previous slide now when we discuss this we're going to have almost exactly the same transition model and markov decision process that i have on the previous slide except just for the purposes of making the calculations easier i've simplified everything to be probabilities of 0.9 and 0.1 sometimes they were like 0.99 and 0.01 on the previous slide um this is just for ease of use you could absolutely have all of the interesting probabilities on the previous slide but here i just simplified it a little bit and we'll assume that our reward function is the same from the previous slide so this is basically the same as the previous slide the numbers are just tweaked a little bit okay let's ask what's the value of a policy well it depends how much time i'm gonna enact the policy you know if i'm renting my field for one year i expect to get a lot less harvest than if i'm renting my field and harvesting crops for ten years and so we need to specify how long we're going to be running this policy and we're going to call that h h is the horizon it's just basically how many time steps we have left in this case how many growing seasons do we have left so in particular i imagine that i'm renting for the purposes of this example a field for each growing season and then it's going to be totally destroyed and they're going to make a strip mall and so there's just no consequences and that last season whatever i do is a total free-for-all um and the field will just not exist at that point so this is the example that we're going to be imagining here this is the back story of my field okay so i have this field i have it for eight growing seasons and i want to ask myself well what's the value of that you know again maybe i'm even thinking about maybe i should run this field i don't really know i want to figure out its value okay so we're going to say let's define something that tells me the expected reward if i enact a certain policy starting at a certain state so clearly v this value has to depend on the state i start has to depend on my decisions that's encapsulated in the policy and it has depend on the horizon as we just said you know sort of how long am i running this okay so we're gonna imagine also that there could be some dueling farmers farmer a has an idea of what's the best thing to do farmer a says let's always plant that is farmers a's policy is to always plant and that seems like a potentially reasonable policy i mean it's it's a greedy policy it says every time let's let's get as much reward as we possibly can on this route farmer b says hey let's let's be careful here i think that we should plant if the soil is rich and let it lie fallow if it's not and let it regenerate and so we have these two farmers and basically in some sense we want to resolve this conflict between them you know which of these is a good policy which of these policies should i use and so in order to do that we're going to calculate expected reward of each of their policies and then compare them okay so let's start with a horizon of zero now here i'm putting a horizon of zero i'm putting in any state and i'm putting in any policy it could be policy a for firmer a or it could be policy b it doesn't matter because if you have a horizon of zero you can't do anything you can't grow you can't plant you can't fallow you can't do anything and so you're it's boring your your reward is zero there's just nothing you can do okay so things start to get interesting on when you have one growing season when you have a horizon of one so if you have one growing season what's your expected reward of a policy starting at s well you just enact the policy for that season and that's all you can do and so your expected reward is the actual reward so here for one growing season we can write out what's the reward starting at us of enacting our policy that would be pi of s okay so let's start by talking about farmer a's policy so here we're talking about farmer a so that's why we have v sub pi a we're talking about horizon of one that's why we have the superscript of one and we're going to talk about both possible states so let's start by assuming we start from rich soil and then we're going to talk about corsol okay so you can ask ourselves what's this going to be well we can just plug in the formula it's r applied it rich that's the starting state and then we take farmer a's policy if we start from rich soil and so my question for you is what is this value it is a number in this particular example what is that number this is a question for the chat looking good good stuff okay let's talk about how we get this this is great um many of you have said 100. now how do we get to that well we first notice that we have to evaluate pi a of rich what is pi a of range well pi a says i always plan farmer a tells me to always plant so farmer a's policy is to plant so if i evaluate pi a i know that i'm evaluating r of rich plant now i'll go over to my reward function it's in that little gray box and the r the reward for being in rich soil and planting is 100. so in this case the value of having the policy of farmer a and starting in rich soil is 100 for a horizon of one okay so i'm just gonna put that over here we've got a hundred now let me ask you the same question what is this value so if i take farmer a's advice i use farmer a's policy i have a horizon of one and i start from poor soil what is the value of that policy starting at poor soil okay great we're getting a lot of tens so let's just walk through this again one more time so we're going to say okay what's the value of starting at poor soil well that's the reward function where the first input is poor soil and the second input is farmer a's action for poor soil well farmer a's action for poor soil is plant because farmers action is always plant so now we're going to ask what's the reward for poor soil and planting i go again up to my little reward box in the corner and it's temp so i'm going to put in 10. okay you can do the same thing for farmer b in fact i'm going to show you many calculations on this slide and the following slides i kind of came up with some numbers and tried things out so you should definitely double check all of these numbers um if you find any uh typos i would love to hear that i will update the slides later um but uh this is the reality of life unfortunately there there is no um checker for my calculations like we have in homework and exercises and everything um you are the checker you are my colleagues who will check so hopefully you will do that and just um report back that you agree with these numbers um but basically the idea is we think that farmer b is going to plant if there's rich soil so we think that there should be a reward of 100 in that case and they'll let it lie fallow if there's poor soil so we think there should be a word of zero in that case okay so that's horizon of zero horizon of one things get tricky when we get to a horizon of two or above and there we're going to start defining a recursive formula so instead of this formula we can replace it with more general formula that also includes this formula which is the following so let's step this through so this is saying now let's look at a more general horizon h so for instance imagine i have five years left on my lease with my form then what's going to happen is starting from the state i'm in i'm going to use my policy to make some decision this time right now i'm either going to plant or i'm going to let the field life out everything else in this is saying what happens when i have four years left on my lease so i started from five years left on my lease now i have four years left on my lease so i have to consider based on the decision that i made this year what's going to be the state of my farm next year is it going to be poor soil or rich soil i have to sum over all those possibilities now each of those possibilities has a probability so that's the t here and each of those possibilities itself has a reward recursively so if i had already done all the calculations for four years when i have a farm for four years now i can use this formula to calculate what's the reward if i have a farm with five years so we already did the calculations for what happens if i have a farm for zero years you can apply this formula in fact we implicitly did to figure out then what will happen if i have what are the rewards for having a farm for one year and now we're going to apply this formula again it'll be a little bit more interesting to understand the value of having a farm for two years okay so let's do that so we're going to say hey suppose i'm renting my farm for two years and then it will get paved over by a strip mall so i'm renting my farm for two years now and the soil was rich when i got it if i follow farmer a's advice what's going to happen so that's what we're about to work through together okay well the first term so i'm just applying the general h formula from above and i'm bringing it down here with h equals two the first term is what happens this year well the soil started out rich and i'm gonna use farmer a's advice to always plant enriched soil and then i'm gonna get some reward from that and then i'm gonna have some terms that represent basically what happens when i get down to one year so here there are two possibilities when i get down to one year after this year i'll either have rich soil or i'll have poor soil it was the only two possibilities i either have rich or poor cell basically i'm going over all the states if i have rich soil i'm going to say well what's the probability of having rich oil that's t times what's the value of having ritual starting from here for that remaining year which we've already calculated here i'll say what's the probability of having poor soil that's t and then i'll say what's the value of my farm starting from poor soil which we've already calculated this is the the beauty of recursion is that you do it for the zero case then you can do it for the one case then you can do it for the two case then you can do it for the three case and so on okay so let's now start solving this so we're going to take this formula we've we've applied the general formula so that h equals 2ks and now we're going to put in some numbers okay so the first number we want to put in is this reward for this year's farming so we started from some rich soil we followed farmer's a advice to plant and so our reward should be 100 based on the logic that we went through before what's the probability of going from rich soil to rich soil if we plant it it's 0.1 what is the value with a horizon one of starting from rich soil this isn't something you would know off the top of your head it's something we calculated already so we're just going to input that number over here now we do the same thing but for poor soil we say what's the probability of going from rich soil and planting and ending up in poor soil that's probability 0.9 just according to our transition model what's the value with one season left starting from poor soil where again you don't expect to know that off the top of your head you already calculated it that was 10. and so now we put this all together and we say that this value is 119. now this is larger than any value we saw for a horizon of one but that's not necessarily too surprising because you expect sort of roughly in general that as you get more seasons to grow you get more harvest and that's what the value is just saying how much do do we expect to sort of harvest how much reward do we expect to get over all the seasons of our horizon now of course depending on the starting soil it doesn't have to be the case that we get more but sort of in general we expect it to increase okay so starting from rich soil taking policy a and having a horizon of two this is the value the expected value that we get from our firm so i'm going to just put that there and you can do the similar thing and this is again the part where i say check my math to calculate the value of having a farm for two years starting from poor soil and following farmer a's advice that's b2a pi a b2 pi a with the poor input then these next two the final two values that we're seeing here are following farmer b's advice starting from rich soil starting from porcelain and you can do this again you know you could just keep recursively applying this once we know the values for two years we can find the values for three years i'm just telling you these numbers again a great idea would be to check them it's also a good exercise and so we can just keep getting these values and remember we started off with some dueling farmers so yes we have these values we could ask you know is it worth renting the farm but here we really need to address this this burning question of which farmer was right you know which which farmer's advice should we follow and something i think we're going to see in a moment is that it really depends on the horizon so let's start by asking we're going gonna start by asking horizon one and what do i mean by who wins i mean that farmer's output that expected reward the value for the farmer has to be at least as good at all the states and strictly better for at least one state that will be our notion of winning for the farmers and so first let's consider horizon of one so if i only rent for one season and then my farm gets paved over by a strip mall is one farmer's advice better than the other according to our definition of winning at the bottom of this slide which farmer's advice is better with a horizon of one great i'm seeing a lot of people saying that farmer a's advice is better farmer a's policy is better so it's very important that we be focusing just on a horizon of one we'll talk about other horizons in a second but if you look at a horizon of one we can look at the rich state if we start from a rich state farmer a has a value of 100 and farmer b has a value of 100 so farmer a is at least as good if we start from the poor state farmer a has a value of 10 and farmer b has a value of zero so farmer a is strictly better and so a way that we can write this is to say that policy a is better than we'll use the greater than sign we'll use a subscript h equals 1 to indicate that this is for a verizon of one which is really important better than part b better than policy b okay what about horizon of three does anything change her horizon of three whose policy is better at a horizon of three you can answer this in the the text awesome people are noticing that it's different here farmer b's policy is better so again we say hey what happens in the rich state at a horizon of three the value of farmer b's policy is 192 that's strictly better than farmer a's value which is 138. if we look at the poor state farmer b's value is 118 which is strictly better than starting from a poor state with farmer 8 is 48. so it looks like in this case farmer b is beating out farmer a okay last question what about a horizon of two who wins at a horizon of two another question for the like the answer eek great there's a lot of people who are saying not sure there there is no clear winner by our definition of winning here now you could come up with a different definition of winning but by our definition of winning here there is not one policy that is strictly better than the other policy if we look at the rich case the rich state at a horizon of two then we see that primary a's policy has a better expected value but if we look the poor state farmer b's policy has a better expected value so no policy wins for each equals two okay and let's just think about this for a moment what's going on here well if i love only one season before everything gets paved over by a parking lot then i should just be as greedy as possible i should just eat out whatever i can from the land and run away because nobody's ever gonna use this land again and that's farmer a winning with a horizon of one if i have multiple seasons like three seasons or even more seasons i expect farmer b to win because farmer b is delaying gratification for a better reward so really seeing this sort of um value of delayed gratification but only if i have enough time to take advantage of it only if i have enough seasons that lying fallow actually does something you know if i let something lie fallow and it gets paved over by a parking lot i don't get a reward from that i only get that if i have enough time to take advantage and so that's what we're seeing here we're seeing you know that it really depends on the horizon which one you want to do cool again if you have a different definition of winning which i see some people are proposing in the chat you might get a different notion of who wins um but here this is our definition of winning um and uh so for just for this definition this is the the um conclusion that we might draw but absolutely you might consider something else okay now something i want to note here is that in everything that we've done we've assumed the policy is exactly what we define you know we said a policy is you take a state and you put out an action but something that you might be starting to think based on the discussion we just have is maybe the policy should also depend on the horizon you know if i only have one year left i should just go free for all and farm anything that i can but i have if i have many years left then maybe i should do this you know fallow plant cycle and so you can actually adapt this formula to allow dependence on the horizon you can adapt the policy to allow dependence on the horizon so let's just see what would change here here are all the places that the policy appears and now if i allow dependence on the horizon we might call this non-stationary so it's stationary if it's the same for every horizon but non-stationary if it could depend on the horizon then i'll introduce some subscripts h perhaps now let me make a little point here what's happening here is when i apply this formula i now use the policy at time h to decide what action i'm going to take when my horizon is h so that's why i have this pi h here i did not add the h subscript here because here i'm just referring to the full policy across all the h's i'm just using it as a subscript so i only applied the h subscript when i'm applying it for time h and then here i just say oh i have my whole policy across all of the h's um and it still can change with the h is but here i'm just sort of referring to it with with my v notation okay so now we have the ability to evaluate a policy to say what's the value of the policy for some horizon we can see that it really can vary by the horizon um that the reward can really change and it might even change you know which is a better policy now let's think about well what if i don't want to just talk about two dueling farmers what if i just want to say what's the best policy what's the best thing that i should be doing in that case i might think well i could consider every possible policy with every possible variation over horizon that's a lot of policies to evaluate and maybe there's a way to more quickly get at the best possible policy now this can still be really useful evaluating the value of a policy we still want to say for instance you know let's say that we know we have some plan and maybe there's some other decisions out there it doesn't have to be the best one we just want to know what's the value and that will help us decide whether to rent the farm or not separately of course we're interested in the best policy because if we can change that plan it would be nice to so that's what we're going to look at next what is the best possible policy okay so in our discussion of what's the best policy we're gonna again just have the same running markov decision process that we've had this whole time so we're keeping that we're still going to have h representing our horizon how many planning seasons are left and as we saw we kind of expect that to affect our policy or at least what the choice we're making at each horizon now we're going to introduce a new notation q so q is the expected reward of starting at state s making action a so that's what we do on this round and then for the rest of the future we always do the best possible thing if i had this you want to convince yourself that i could find an optimal policy if i knew this if i knew what would happen for each of my actions in this case there's two possible actions planning and following but in general for all my actions i could say well i already know what's the best for all the h minus one steps that are left by recursion now i'm going to find the best step right now by just saying well which of the actions i'm making right now is the best so that's what the arg max here means it says okay which possible action is the argument to queue that maximizes q on this round so if we already know what the best possible policy is in all the future rounds all the all the lower horizons then this is how we can find the best policy on this round we're going to go through an example of this but i just want to point out the difference with the v that we were just looking at so some things are kind of similar here you know we we have this dependence on h we have this dependence on state but with v we were evaluating a particular policy so the policy pi appears in queue we're not evaluating a particular policy we're evaluating actions we're saying hey here all the possible actions on this round that i could use and i'm already assuming i'm using the best possible actions in future rounds and so now i'm asking what happens with these actions and so v we use for evaluating a particular policy q we're using for coming up with the best policy but it's also because of that doesn't explicitly have a policy as an argument we're just trying to find the best action on this round okay i think it'll be helpful to walk through an example so let's do that a couple of things before i do that just a couple of notes so one there doesn't have to be a single optimal policy it's possible that multiple policies could give you the same reward so we're just saying we're gonna find an optimal policy one the optimal policy here doesn't have to be stationary it could depend on horizon h and we kind of expect that in this example as we already said i think our intuition tells us in this planting example that probably we're going to do something like farmer b's idea of following you know as we have a higher horizon and we're going to eventually switch to farmer a's idea of just planting okay so now let's actually calculate q for the example that we have so in particular if we start with any state and we take any action and we have absolutely no time to plant anything then our reward is zero because we didn't have time to plant anything so we couldn't possibly harvest okay so now we want to find the expected reward of starting at s making action a and then making the best action for all the steps that are left so if we're at a horizon of one there are no steps that are left after this step there is only this step and so q is just going to be the reward of being at state s and taking action a and so that's sort of easy to fill in so what what for instance and this is a question for the chat is q1 by verizon of one with a state s and an action of plant their state of uh rich and an action of plant okay so awesome everybody's observing i can just say well what is the reward of richard plant it is 100. i can just read it off for my reward chart and so we can do the same thing with every other state in action this is basically just recreating the reward chart we're saying hey what's the reward of rich and fallow what's the reward of corn plant etc and so this should align perfectly with our reward chart okay things get interesting when we get to again an h greater than one now before we do that let's just recall the whole point in some sense of q is that we're trying to come up with the best policy so let's come up with the best policy for a horizon of one what's the best policy for a horizon of one can you write that in the chat what would you how would you describe the best policy for horizon of one based on this cue great always plant so no matter what your state is you just plant this is what farmer a was suggesting last time as some people have said um at this point there's no time left you get the most from just planting you gain nothing by following because there's going to be a parking lot here and so you're just not going to it's not going to help you to have better soil you just need to plant okay now let's go to the interesting case which is again h greater than one so again i'm going to replace this equation with a slightly more general equation of which this is the special case of h equals one so here we have the more general equation so the more general equation says suppose i have five years left so h is five i have five years left of my farm i want to say what's the expected reward of starting at s making action and then making the best action for all the remaining steps well on this step i already said i'm starting at s and making action a and so this is my reward for this step now i consider what happens in the remaining steps when i have four years left well there are all the possible states i could have i could either have rich soil or poor soil so this is a different s prime i consider the probability of each of those states that's t and i say what's the best thing to do in the future and so this is quite different than what we saw before i'm maximizing over all the actions from the recursion so if i do this on round zero well that's boring if i do this on round one i find the best set of actions that i could do for for one horizon left then i apply this to h equals two to find the best set of actions i do with a two horizon and then i find the best set of actions for three horizon etc so let's work this out for a two horizon so suppose i have two years left on my farm two years left on my lease two years until the parking lot comes i'm starting from a state of rich soil and i decide to plant well this season i already said i'm starting from a state of rich soil and i decided to plant so i'm just going to reap the reward from that next season there's some probability that i get to rich soil depending on my action there's some probability that i get to poor soil depending on my action if i get to rich soil then i want to ask what's the best action when i only have one round left when i only have a horizon of one so that'll be the max over the q ones because i'm trying to maximize overall here and if i have poor soil i can similarly ask what's the best thing to do from there what's the best action okay so what is this going to look like in our particular case well our oh there's a question yes could you just clarify again what does it mean when an optimal policy can be non-stationary oh yes so what i mean by that is that the policy can change with horizon so this is exactly the idea that if i have only one season left i'm probably just gonna plant and i'm not gonna worry about what's my state whether it's rich or poor but if i have 10 seasons left i probably want to take poor soil and let it lie fallow so that i can get the most from it next time and so those are different policies and if i restricted myself to have a policy that was the same at every horizon i would get a different policy than i would if i could allow it to change over different horizons in fact we're going to see in a moment that our optimal policy here will change with horizon um so we set our optimal policy at h equals one was to always plant um you may not be surprised to find out that at h equals two it may not be to always plant cool okay so let's look here at this equation and let's just start filling it out so reward for starting it rich and planting we already said was a hundred we just got that from our chart now let's say what's the probability of starting rich at rich soil planting and then ending up in rich soil we can read that off of our transition diagram that's point one now we wanna say what is the best possible thing the best possible action that we could have done with the horizon of one starting from rich soil so i'm going to ask you what is the max value that we're going to get here what is what is what is this thing in yellow this is a question for the chat great so what we want to do to figure this out is we want to look at the two different values of q1 rich some action there's either q1 rich plant which is 100 or q1 rich fallow which is zero q1 rich plant is bigger and so that's what we put in we put in 100. okay so now we go down here the probability of saying we're in a rich state we plant we go to a poor state that's 0.9 we read that off of our transition diagram and here we compare q1 poor plant which is 10 to q1 poor fallow which is zero 10 is bigger than zero and so we put in 10. and so the output of all this is 119. okay and so we can keep doing this again you should check my map but we do this for all of the other q2s for each basically for each state and each action and now once you see these numbers even if you didn't derive them you can just look at them and say what's the best policy at a horizon of two that is what action should we take at a horizon of two and so this is again a question for the chat what action should we take at a horizon of two i see some people saying either plant or fallow at a horizon of two might the policy depend on the state remember in general policy isn't going to be a function of the state so in this case it might depend on the state if it does what is the dependence what should i do if i if i find rich soil what should i do if i find poor soil okay awesome so the observation is first of all that the optimal policy is not the same as a horizon of one and in fact it really will depend on the the soil status this time and we can see that by looking at q2 you know if i look at the rich state and i plant my my total reward by doing the best thing all the time is 119 if i look at the rich state and i follow my total reward overall is 91. and so it seems like i should plan but look at the poor state and i plan my total reward you know the whole time is 29 whereas if i look at the poor state and i fallow my total reward over all the steps is 91. and so this is basically telling you that we want to delay gratification in the second state we want to let it lie fallow this year so that we can reap all of that awesome harvest next year whereas if it's already rich this year well it's going to be you know we don't want to just throw away a perfectly good rich harvest and so we're going to start planting right now okay and so in fact we do see now that the optimal policy here or at least an optimal policy depends on the horizon that in fact it does change with the horizon this algorithm that we basically just went through where you first calculate q naught and then q1 and then q2 across all the states and actions and then you can finally get an optimal policy out of that that is finite horizon value iteration we have a finite horizon the number of times the number of years that i have rented this farm where i'm going to rent this farm is finite that's h and i'm iterating i'm doing this iterative procedure to calculate these best values and so we can call this finite horizon value iteration to contrast with something that we may not get to in this lecture but is in the notes um infinite horizon value iteration which is doing this in the infinite case now that being said we will start talking about infinite horizon next so at least get a little bit of background on there basically the issue that arises here is what if i don't stop farming so i just got some good news in the middle of this lecture while we were doing all of those q calculations that in fact there is no plan to replace my firm with the strip mall anymore they're actually just going to give me the farm and i get to keep it forever awesome so now i have a different kind of set of calculations um i don't just go up to a certain age a certain horizon and then assume that the farm doesn't exist anymore i i keep my farm it keeps going and so we sort of have to ask ourselves can we use the same tools that we just used in this case well they may have to change a little bit okay so what hasn't changed what hasn't changed is the way that farms operate just because i'm farming for a longer time doesn't mean that rich soil magically all you know becomes poor or something like that like it's still the case that from season to season i have the same dynamics and i have the same you know amount that i harvest and so all of that is the same the markov decision process has not changed or at least the actions of it have not changed an issue that really arises when we look at really long time scales is that the value of money and goods today is not the value of the same amount of money and goods in the future and so this is the one thing that we're going to change if i don't stop farming and if i have basically no finite horizon so for instance if i have a thousand bushels of wheat today that's different from me having a thousand bushels of wheat in 10 years if i were hungry and i needed to eat today i couldn't eat bushels of wheat in 10 years i could only eat bushels of wheat that i have today or i could sell the bushels of wheat to get pizza and then i could eat the pizza today i could not eat that pizza if i had the bushels of wheat in 10 years even if i was doing perfectly fine and able to eat i could sell the bushels of wheat today invest money and have lots of money in 10 years whereas if i have the bushels of wheat in 10 years i don't have that money and so for this reason we can't just really add up bushels of wheat across long time spans which is kind of what we were doing before we were saying hey let's add up all these rewards across all the different years and treat them like they're all the same and as we see from this example that's just not true if somebody offers you either a thousand bushels a week today or a thousand bushels in ten years you should always accept a thousand bushels of wheat today and so somehow we want to adjust for this in our calculations and a very typical way to do this is what's known as a discount factor the idea of the discount factor is that some value between 0 and 1 that tells you how the value of a bushel of wheat changes or the value of anything changes over time periods so for instance we could say what is the value of one bushel of wheat after ten time steps by construction here this is the idea of the discount factor we will say that a value of a bushel of wheat now is one one one bushel the value of a bushel of wheat next time step is gamma the value of bushel of wheat in two time steps is gamma squared the value of a bushel of wheat in three time steps is gamma to the third etc and so something that's kind of cool that you can do once you have this conception is you could ask what would i trade somebody who offers me a bushel of weight every year forever you know sometimes you hear about those contests where people are like i'll give you a year's supply of diet coke every year forever um and so if we were in such a situation and somebody was saying i'm gonna give you a bushel of wheat every year forever there is some number of bushels of wheat this year that i would trade for that so let's figure out what that is what is the value of this idea of getting a bushel of wheat every year in today's bushels of wheat is that a new question or that was the old one okay okay so let's call this value v and by construction it is the value of a bushel of wheat this year which is one plus the value of a bushel of wheat next year which is gamma plus the value of a bushel of wheat in two years which is gamma squared dot dot you just keep adding things up like this okay well here's a clever way to write that you could write the exact same thing as one plus gamma times quantity one plus gamma plus gamma squared plus dot dot the amazing thing about infinity is that infinity looks the same today as it does next year and that's kind of what we're taking advantage of here okay well that thing in the parentheses is just b that was our definition of v from right here and now we have an equation in v it's a linear equation in v and so we can solve it and says v is one over one minus gamma and as we said gamma's between zero and one and so this will be a perfectly well defined equation we can we can figure out what is this value so let's look at an example suppose i take gamma to be 0.99 so i'm saying that a bushel of wheat next year is almost the same value as a bushel of wheat this year it's only 0.99 the value of a bushel of wheat this year that's pretty close to one well in that case the value of bushels of wheat forever one bushel of wheat per year forever by our formula is one over one minus 0.99 so it's 1 over 0.01 so it's 100 bushels so if my discount factor here is 0.99 it doesn't even change that much i should be willing to trade somebody who's offering me a bushel of wheat forever 100 bushels right now or vice versa now something that you should definitely ask yourself and think about i'm not asking this in the chat i'm just asking you to meditate on it later is if i change this gamma if i make it higher or lower how does that change the value of a bushel of wheat forever so definitely think through that later give that a little thought if you have any trouble with it ask on discourse that's something that you want to make sure you have in your mind okay so now now that we've talked about the discounted value of money in the future and the discounted value of bushels of wheat in the future we can talk about the expected reward of a policy pie starting at s when we keep our farm forever so now there's no h on this v there's no horizon we're imagining an infinite horizon that we you know we're just always going to keep doing this okay well we can write a recursive formula like before this also looks very similar to the formula that i just wrote for this other v basically what's happening is we're calculating the value of having this policy starting at state s and going forward well it's you know what is the reward i get on this time step then i look at all the future time steps but remember infinity next year looks the same as infinity this year so it's still just v pi s prime we actually haven't changed a horizon because there is no horizon and we just discount everything next year by gamma so you definitely want to just compare this in your mind to sort of what we just did a few lines above because it's a very similar idea and this is also very similar to what we were doing back when we had a finite horizon now we just have this discounting term and also the horizon doesn't change now we're not going to go through an example right now although i'm sure you'll be working through this um at some point um but this is a set of number of states linear equation and number of states unknowns and so you can solve that that's that's the kind of thing that we have linear algebra to solve so this is something that you can totally do now we haven't had time to get to it but basically there's going to be a very analogous idea for the cues so here we've been able to say what's the value of farming forever just like we were going to say what's the value of getting one bushel of wheat forever now we can say what's the value of farming forever and now of course you want to ask well what's the best thing i should do to farm forever what's the optimal policy to farm forever and so that's the remaining piece but again it's going to be very analogous to what we did in the finite horizon case just with this extra sort of discount factor stuff in so now we have the the magical final bit of the markov process the discount factor um that's telling us you know how does this change over time and i'll just mention you don't have to only use this in the infinite horizon case you could have a discount factor in finite horizon too because really this is just the value changing over time um and so it will change in 20 years um you know we just talked about how in 10 years the value of a thousand bushel of wheats will change to you um and so that's something you can do as well okay great so we're gonna be using these ideas going forward these ideas of changing state and so on um and some really cool things in the next few weeks and i will see you then 

Okay, before we get started today, let me just 
preface everything with: it is wonderful how many   of you show up to live lecture. I love how many of 
you participate when I ask a question and answer   it, I think it's actually especially awesome if 
people aren't getting it right and they're still   participating, I think that's one of the best ways 
to learn, is to commit to something and then see   why it's right or why it's wrong. That being said, 
there are sometimes special circumstances that   mean we can't make a live lecture and luckily we 
are able to record this and make it online later   and I think this week, there's an obvious special 
circumstance for many of us. If you are in the   U.S. and you have not voted yet, just go do that, 
just go do it right now, come back, we're gonna   have this all recorded later and you can check it 
out and we'll see you in live lecture next week   and hopefully many of you are doing this already 
and you'll see this recorded later, but if you're   in that situation that you are able to vote in 
the U.S., today's the day to do it if you haven't   already, it's election day so make sure to do that 
and then check out the lecture. Now of course,   if you've already done that or you're not 
able to vote in the U.S., then please join us   along for today's live lecture. Okay so that being 
said, let's get back to our content for today. So   in general, or in these two lectures, so in 
last week's lecture nine for week nine and   this week's lecture ten for week ten, you 
can think of this as a pair of lectures,   this is like the sequel to last week. Last week, 
we built up this idea that you could make actions   that would change the state of the world and 
potentially gain some reward and so we built   up this idea of a Markov decision process, we 
said how we could change the state of the world   via actions, how we could get a reward by looking 
at both the state of the world and our actions and   we did all that, we built that up that Markov 
decision process, and then once we did that,   we said okay if we know everything in this Markov 
decision process, we actually now have the ability   to choose the best action and best in the sense 
of getting the most long-term reward and so we   did all of that and all of that very much required 
us knowing basically everything about the Markov   decision process and so that's what we're going 
to be addressing today: we're going to say,   “well what if you don't actually know in 
advance what reward you're going to get   from being in a particular seat and taking an 
action or what transition function is operating   in this MDP world, then how do you choose the best 
action, again, in the sense of getting the most   long-term reward?” And so that will be our focus 
in the lecture today. Okay so obviously, in order   to talk about that, it'll help to have everything 
that we did last week at the tip of our fingers.   So hopefully, part of that is that you've been 
working on your labs and thinking about this and   everything like that but let's just do a little 
recap anyway. So we had to find a few things,   but we ended up talking about a Markov decision 
process and so there were multiple parts to the   Markov decision process: one was we had to 
say that there were some states in our world   and we had this running example which we'll 
have again as an example today of farming. So   in particular, it might be that we have found 
ourselves buying some kind of farming field   and it could have either rich soil or poor soil. 
So we're going to say that those are our states in   this example and then we can take some actions and 
so in this particular scenario, the two actions   that we said we could take are we could plant 
our farming field or we could let it lie fallow:   those are the actions that are available to us. 
Now once we have those actions, we can apply them   to both states and so we're going to end 
up having this diagram that suggests what   happens when we apply our action of planting to 
either rich or poor soil, what happens when we   apply our action of letting the field by fallow 
to either rich soil or poor soil, so let's just   set up the diagram like so. And then we had a 
transition model. So this transition model tells   us for every state, for every action that we make, 
and for every state that we might go to, what's   the probability of that? So one of the ways that 
we set this up was with this diagram. We actually   saw a few different ways to set this up in terms 
of just writing the transition model function T,   writing these transition matrices, but we're just 
going to go with this diagram for now. And recall,   in the diagram, the idea is that we look at our 
starting state, we take some action, we look at   the potential ending state and we say, “what's the 
probability if we started in the starting state,   we took that action, and how probable would 
it be that we end up in that ending state?”   So that's that probability. So here we have if we 
started rich soil and we let it lie fallow, then   we have a 10% probability of ending up with poor 
soil and a 90% probability of ending up with rich   soil again and, of course, these numbers coming 
out of a particular state should sum to one. Okay so that's our Markov decision 
process. There's still a couple of   things that we need to do in order to 
fully define it. So one is a reward   function. So here we're not just talking 
about how we transition between states,   but also what do we get for doing that and taking 
some action. So our reward function says for each   state and each action, what reward do we get? 
So if I had rich soil and I planted it and I   harvested from that, I would get a harvest of, 
in our example, 100 bushels. If I had poor soil   and I planted it and I harvested, I would 
get a harvest in this example of 10 bushels   and then finally, the last part of the Markov 
decision process was the discount factor. So   this tells us how does a bushel next year 
compare to a bushel this year, how much is   it worth to me this year, and typically we 
say that a bushel in the future is not worth   as much as a bushel right now and so a discount 
factor will be typically strictly less than one. Okay so that's our Markov decision process, these 
were all the things that were involved in it,   it tells us how we interact with the world and 
what rewards we can get from the world and now we   would like to do that and so, in particular, we're 
going to define a policy. So a policy tells us   what's our plan going forward. If I 
am in a particular state, what action   will I take in that state? We explored various 
policies last time, examples of policies:   maybe I just plant all the 
time, maybe I alternate between   planting when the soil is rich and letting it lie 
fallow when it's poor, maybe I do something else,   there are a lot of different policies that I 
could take and we talked about this possibility.   Now, of course, what I'd like to do is somehow 
evaluate these policies and maybe even choose   the best one and so we talked about both the 
finite horizon case of this and the infinite   horizon case. Today we're just going to focus 
on the infinite horizon case, so let's do that.   So in the infinite horizon case, we're imagining 
that I have this farm for infinity: I own it   and I'm going to be using it in all the future 
years and so I'm going to say what's the most   long-term reward that I could get from this 
farm? And so first question I might ask is,   “okay, well under different policies, what is that 
long-term reward, that reward added up over all   the years including discounting if I start in 
a state s?” And so this might be useful if I'm   thinking about selling my farm, I could compare 
that to the reward I might get from the farm   under a particular policy. If I wanted to compare 
two policies, maybe there are only two that are   available to me then I could look at those values.   And so, in particular, we came up with this 
equation: so here, the idea is that the value   of a policy starting from state s (so we imagine 
that the farm is in some state when I get it   and I'd like to be prepared for whatever 
that is) is going to be the reward I get   this season, let's say it's every year, then this 
year, by first enacting the policy this year,   and then for the remainder of time, I'll have some 
additional long-term rewards. So that's all that's   in this chunk and everything after this year. 
So, in particular, everything after this year   will be discounted by gamma because it'll start 
next year. We're going to have the expected value   of the remaining time, so to get that I'm going to 
sum up over all the possible states I could go to   next. I get to those states by enacting my policy, 
so my action here is the pi of s according to my   policy, I get to state s’ with this probability 
and then once I'm in s’, this is the value of my   policy going forward. Now when we talked about the 
finite horizon case, we saw that the horizon could   come in here. Here there is no horizon because 
when I start, I have infinity in front of me   and the nature of infinity is such that next 
year I still have infinite time in front of me   and so with infinity, you get this interesting 
thing where the same value appears on both sides   of the equation and it's still V_pi. We're 
only going to be considering, in this case,   stationary policies for that reason because there 
is no conception over horizon and infinity and so   for this reason, we basically get this equation 
and this equation—it's actually going to be a set   of equations in V_pi—and we could solve these to 
figure out V_pi. So this isn't telling us V_pi,   we can't just read it off, but we could solve 
these equations. These are linear equations   so it's not too bad to solve them, you could 
solve them to get this value of the policy.   Now another thing that we define, in part 
because it'll really help us get the best policy,   is the value of an action given that I'm in a 
state if I make all the best actions after this   action. So the idea is I'm going to make this 
action a and then I'm only going to do the best   thing ever after it. And so this was the idea 
of Q*. So Q* is exactly this value of action a   if I start in state s and then I make all the best 
actions in the future. Now we didn't quite get to   this in the last lecture but it's in the notes and 
it's also extremely analogous to the V equation   and you can derive it in much the same way. So 
here we're saying, “hey, this value of action a   and state s, if I make all the best actions in 
the future, well first, I'm making action a and   I'm in state s and so I'm going to get the reward 
R(s,a) just from this year, from this time step.”   Now I'm going to say, “well, what's the 
value in all the remaining time steps?”   Again I'm going to discount that by gamma to start 
because as soon as I go to the next time step,   that's how much the value of my harvest decreases 
in terms of today's dollars or today's bushels   and I'm going to sum up over all the 
states to get the value going forward   and I sum up over all the possible states I could 
go to, what's the probability of going to each of   those states in turn to s’ and then what's the 
value if I do all the best things in the future?   So if Q* is, in fact, making all the best actions 
in the future, then this will be the value   of making all those best actions in the future 
and just for the same reason that we saw V_pi   on both sides of the equation above, we 
see Q* on both sides of the equation here:   once we have taken a time step, we 
still have infinity in front of us   and so, in that sense, we're still in the 
stationary realm, things still look the   same in the time sense, we just might be in a 
different state and make a different action.   Okay so why is this useful? Why is it useful to 
know the value of an action a in a state s if   we make the best action of the future? Because 
then we can solve for the best actions to make.   Now it's worth noting: here this is also a set of 
equations that, in some sense, we can solve for Q*   just as the previous set of equations we solve for 
V_pi. Here it's just a little bit more difficult   because of that max, it's not just a linear set 
of equations and so we had to come up with this   value iteration idea, in particular, the infinite 
horizon value iteration which would basically   solve this by doing an iterative procedure 
or at least get pretty close to the answer.   Okay and so once I have Q*, I can find the 
best policy. I can say basically, okay,   for every state I'm in, I know the value, 
the best possible value of every action,   and so now I can ask myself, “well, what's the 
best action to take out of all those actions?”   So here what we're saying is there's some 
value for every action, that's the Q*(s, a),   we're going to ask which a maximizes that and then 
the arg max just returns the a that maximizes it,   what is the action. And because, again, life has 
to be stationary when you're in infinite time,   there is no horizon that is changing: you're not 
getting to a point where again your farm is about   to get paved over by a parking lot, it's just 
always going to exist, then we're going to have   a stationary policy here. So this is one that is 
just going to depend on us, there's no horizon   dependence or anything like that. Okay so we get 
the best policy, so let's recap: so our goal here   is to maximize the reward. I want to say, 
“what's my long-term reward?” And I mean that,   in the sense, if I add it up in this discounted 
manner over all the years in the future.   And so, in particular, what I would do if I was a 
farmer And I knew a lot about MDPs is I would say,   “hey, I've just gotten this cool new field so I'm 
going to plug in all the information I know about   my MDPs, my states, my actions, my transition 
model, my reward function, my discount factor,   I'm going to solve for the best policy by first 
figuring out Q* perhaps with infinite horizon   value iteration and then once I know the Q* 
value at every (s, a), I'm gonna plug in,   I'm gonna find my pi_Q*.” So before I even set 
foot on my farm, I figured out my best policy.   Now I'm gonna set foot on my farm, and from 
day one on my farm, I'm gonna follow this   policy. So sounds great: I know exactly what 
I'm going to do, I know exactly how I'm going   to farm everything, I'm all set for life. And 
the observation that we want to make now is:   is this realistic that I would set foot on my 
farm and know all of this off the bat? Well,   everything in machine learning is a little bit 
realistic and a little bit not, but let's try to   make it a little bit more realistic today and 
think about what are the things that I might   reasonably know and might reasonably not know. 
So let's go look at this Markov decision process.   It seems plausible, at least in many cases, that 
I would know the state of my farm, that I could   measure whether I had rich or poor soil, that's 
something that I have measurement tools for.   Certainly I will know the actions that I can take: 
I know whether I can plant or fallow, I know that   these are the actions that are available to me. 
I might be in a situation with other actions   but then I would know those actions too, so it 
seems very plausible that I would know the states   of my farm, I would know the actions. The 
discount factor is my choice: how much is a   bushel going to be for me next year versus this 
year, that's something that I can choose. Okay   I'm gonna say two more things and I'm gonna take 
the question. So let's just briefly note, though,   that even if I'm a super expert farmer And I know 
all about farming (which is, by the way, not true,   I am definitely not an expert farmer), but even if 
I were, it actually seems a little unlikely that I   would know this transition function. It might be 
that this field is different from other fields,   so why would I know exactly the weather conditions 
and the exact probabilities for this field   that lead my rich soil to become poor soil? It 
seems unlikely that I would know this in advance,   that I would know these probabilities of going 
from rich to poor. Maybe also I plant and fallow   different from other farmers and that might have 
a reasonable change on this transition function.   Same thing with the reward function: even if I 
know rewards in other fields, I might not know   the rewards in this field, I might not know how 
many bushels I'm going to get from rich soil and   how many bushels I'm going to get from poor soil 
and so it's totally reasonable that I might know   the states and actions and discount factor that 
I could apply to this field but I might not know   the transition model or the reward function. And 
so in that case, I can't do this thing where I   do all these calculations before I even arrive 
on my farm and then I try to go for this goal   of maximizing reward, like I start off 
with this best policy from the first day,   because I might not know the transition model and 
reward function are and they come up everywhere   in these equations, these equations very much 
clearly depend on knowing both T and R and so   if I don't know them, I'm gonna have to come 
up with a new plan basically. Okay sorry,   what was that question? The question relates 
to the V_pi and Q*: is the V_pi(s) and the   max Q*(s,a) given from past values of previous 
iterations? Right, so okay. So this is a little   bit different than what we did in the finite 
horizon case. So in the finite horizon case,   you could actually calculate each of these in 
this really straightforward manner, the value   of a policy or the value of the action a and state 
sm taking the best actions of the future. What you   would do is you would calculate them for a horizon 
of zerom and then you would calculate them for a   horizon of one, and then you calculate them for 
horizon two, and you would just have the values.   And here it's a little bit trickier because 
what really happens is that you have this set   of equations that they satisfy and now you have 
to solve for them in those equations and so it's   not like you just plug in these equations and 
you've got an answer because unfortunately the   equation for V_pi depends on the value of V_pi, 
the equation for Q* depends on the value of Q*,   and so we have to come up with some way to 
solve them. Well the first one, it turns out,   is a set of linear equations. Like if I had the 
equation x = 2x, I could actually solve that for   x, x in this case would be zero. I could come up 
with more interesting linear equations but once   I have a linear equation, I can solve that and so 
that's what's going to happen with V. In the case   of the Q*, because it's not a linear equation 
(that max makes it non-linear in Q), we have to   use more sophisticated methods to solve and so, in 
this case, what you're going to have to do is, for   instance, something like infinite horizon value 
iteration and what that is is basically a way   to find a Q* or something very close to it 
that solves these equations. That's great. Okay but today we're going to go beyond that, 
we're going to assume that you have the ability   to get this V_pi and this Q* if you have these 
equations, but the problem is that you don't   have these equations and the problem is that you 
don't have T and R and so what can you do? And so,   in particular, we're going to have to 
come up with some different way to behave:   we're not going to have just the best policy 
off the bat and then immediately go forward and   make the most from our farm every moment, we're 
probably going to have to make some mistakes and   figure out what to do and then we'll keep going 
and eventually make some really good choices. Okay so here, we're in a 
situation where we're imagining   that there is a Markov decision process out in 
the world that is operating, that is happening,   that decides all of these things, we 
just don't know the details of it.   So my farm operates according to a Markov 
decision process (you could ask whether or   not that's realistic but we're going to pretend 
that it does) but I just don't know the transition   function and I just don't know the reward function 
and so I have to figure out how to proceed.   And again, proceed under the assumption 
that I would like to harvest as much as   possible in this long-term sense including 
these discount factors and things like that. Okay well let's try out some ideas. I mean I've 
got a farm, I got to do something with it so   let's throw out some ideas. I mean this 
is the idea of brainstorming, right,   let's throw out some ideas even if they're not 
really good ones and then we'll learn from there.   So let me just say: maybe my friend, farmer 
A—remember farmer A and farmer B from last   time—maybe they have stopped by and offered some 
strategies for me. So farmer A stops by and he   offers strategy and he says, “what you should do 
is you should try a random action, see how that   goes, and then do the best thing ever after from 
there, based on what you know so far.” So let's   try it out: so let's say that we follow farmer 
A's advice, let's see what this might look like.   So first, the state of my farm is just the state 
of my farm when I show up and so maybe I show up   and it already had rich soil so I'm pleased 
with that, that seems like a nice situation.   Now I choose what action to make and if I follow 
strategy A, I'm supposed to choose a random action   and maybe I'm gonna make a uniformly random 
action. So I have two actions, plant or fallow,   let's give them equal probability. I'm 
going to flip a coin to decide which one   and so maybe, in this case, it happens that my 
coin flip tells me to plant. Okay so I planted   on my rich soil and now nature is going to tell 
me what's the state of my soil after a season.   So nature runs its course and it happens that now 
the soil is rich. So I didn't have anything to do   with that, that was just nature operating. 
Hey, I got lucky and it's rich again! That   sounds pretty nice and so now I'm going to get 
some reward. Again, this is nature operating:   I just tried to harvest as much as I could and it 
turns out that I was able to harvest 75 bushels,   so this already seems to be a different field than 
the field that we were investigating last time.   I was able to get 75 bushels from this field 
and so, incidentally, it seems like I've   already learned something about this secret 
MDP that's operating where I don't know the   T and I don't know the R. Is there something 
about the rewards that I have learned? So this   is a question for the chat: have I learned 
anything about this reward function here? Okay great, a number of people have noticed that 
if, in fact, my farm is operating according to   an MDP (so it's an MDP, I just don't know 
what it is) and it's got some reward system   and some transition function (I don't know what 
they are going in but I know that they exist),   well here I decided to take some rich soil and 
plant on it. So I took the state “rich” and I   took the action “plant” and I have now observed 
that I was able to get a reward of 75 bushels if,   in fact, this is an MDP that means that the reward 
for taking a state rich and planting must be 75. Because it is deterministic here, 
there is only one reward for that state   and that action, this must have been it so I 
have already learned something about this MDP   just by going through one 
round of planting things. Okay and so that seems pretty cool. I noticed 
that, in fact, I got some non-negative,   non-zero reward, like I actually got 75 
bushels so planting seems like a good idea   and so I said, after one round of trying things 
out, I was going to do whatever seemed the best.   This seems best so I'm always going to plant 
going forward. This is what farmer A told me   to do and I'm following farmer A's advice. Was 
that good advice? Is this a good strategy or,   just to be a little bit more concrete, could 
anything be improved about this strategy? Is there   anything that, perhaps, is sub-optimal about this 
strategy? This is another question for the chat. Okay great. So basically I think the theme that 
is emerging from people's answers is that we don't   know anything about this farm basically. I mean 
we know a little bit—we know the reward for rich   soil and planting is 75—but what if it turns out, 
in this farm and for this crop, that poor soil   is actually really great for planting and I would 
get a thousand bushels if I planted in poor soil.   Maybe I should try these things out. Even if it's 
just like the farm that we saw from last week,   if I plant all the time I'm going to 
end up with poor soil and a low reward   and it's going to be a sub-optimal reward. And 
so it seems like a big issue with this strategy   is that we haven't really explored this farm 
at all, we haven't even gotten a sense of what   are the possible things that might result, what 
are the possible rewards that are out there,   we certainly haven't filled in this reward diagram 
and so we could ask ourselves, “hey, is there some   way that I could get more information about this 
farm before making these big decisions, like, what   am I going to do with the rest of my time?” Okay 
so that seems like a good idea. So okay, farmer B   now walks by my farm because I like hanging out 
with farmer A and farmer B a lot and they're very   welcome to come by and so farmer B says, “hey, 
here's a great way that you could learn about your   farm. Just always try actions uniformly at random, 
there's no reason you have to stop after one time.   Just keep trying these uniform random actions.” 
And so I say, “hey farmer B, that seems like a   good idea, let's try that out, let's see what 
this might look like” and so I'm gonna do that.   So again, I start off with a 
particular state, maybe some rich soil.   I decide to plant in it but then nature decides 
what state I see next. In this case, perhaps,   it's rich. Nature decides what reward I get. In 
this case, it's 75 bushels, and we just said that   once I see that, once I was in the state rich 
and I decided to plant and I got 75 bushels,   that means I've learned something about the 
reward structure of this farm: I know that if   I'm in a state rich and I plant, I'm going to get 
a reward of 75, again on the assumption that this   farm operates according to an MDP. Okay so now, I 
decide what to do in this new state. Well again,   the way that I'm deciding here is I'm trying 
uniformly at random and so again I flip a coin,   I say “am I going to plant or am I going to let 
it lie fallow?” In this case, it happens that I   plant. Nature decides what the state of the soil 
is. In this case, perhaps, it happens to be poor.   Nature decides what reward I get. Can you tell me 
what nature is going to decide for this reward?   This is a question for the chat: do you happen to 
know in advance what this reward is going to be? There's some split thoughts on this 
one but let's double check. So remember   the first time I was in rich soil and I 
planted and I got a word of 75 bushels.   This time I'm in rich soil and I 
planted. If this is, in fact, an MDP,   if nature operates according to an MDP, 
then this has to be the same reward   and so this is going to have to be 75 bushels 
again. It doesn't matter the state I landed   in, it just matters the state I started in, 
the action that I made, at least for an MDP.   Now you might disagree whether nature really 
operates according to an MDP but we're assuming   for the moment that it does and if it does, 
then this is going to be the same reward. Okay so we can't update our reward structure 
because we basically just got the information   we already had, so here I'm going to flip a 
coin. Again, now a fun fact is that people   tend to think that if I randomly flip coins, 
that everything looks like it goes back and   forth between the options. In real life, when 
you flip coins, you get quite a lot of runs.   In this case, it turns out I got another plant 
even though I was uniformly flipping this coin.   Nature decides what the state is going 
to be. In this case, it happens to end   up being poor and let's say I observed that 
I get two bushels because this was really a   new situation. Now I started with poor soil, I 
planted in that poor soil and I got some reward.   In this case, it happens to be two bushels and 
so I can update my reward structure and say hey,   in fact, I observed that there were two bushels 
for a reward when it's poor soil and I plant.   Okay and, again, the question is: is this a 
good strategy? Is there anything that could   be improved about this strategy? I mean it 
seems like I've been harvesting a lot of   great bushels, like I've gotten 75 bushels twice 
already, should I just keep doing this strategy   and will I get the best possible rewards? This 
is a question, again, for the chat and , again,   in particular, you should think about are there 
things that you could improve about this strategy? Okay while you're still answering, I'm 
actually going to respond to one point.   Somebody suggested that this is better than 
strategy A. It's not obvious whether that's true,   actually. So it's totally possible in strategy A, 
we didn't know anything at that point: maybe if I   planted in rich soil and maybe I planted in poor 
soil, then maybe I'd always get a reward of 75.   Based on what we knew at the beginning of strategy 
A, that is possible and there are worlds in which   that is possible. I think what you're getting at, 
though, is that there are also worlds in which   strategy A is very bad and strategy B will yield 
more output than strategy A but the problem is   that we don't know which one we're in in advance 
and that's the real issue that we're dealing with. Okay so I think what some folks are observing 
is that in strategy B, I'm learning a lot,   but the problem is that I'm not 
necessarily exploiting that learning,   I'm not necessarily using that learning. 
So for instance, in strategy B,   suppose I eventually learn that the best 
possible thing I could do is I could plant   whenever there's rich soil and fallow whenever 
there's poor soil. Like if I knew the whole MDP,   that would be the optimal thing to do. Now I'm 
always going to try actions uniformly random,   though, in strategy B and so if I have rich soil, 
half the time I'm going to choose to fallow it,   I'm going to choose to not plant 
anything and that seems like a lost   chance to make a lot of harvest, that seems 
like something that I could have improved on   if I knew all of the information about this MDP 
which, if I do strategy B long enough, I'm pretty   much going to know everything about the MDP and 
that's what we're going to get at the moment,   then I could probably do better by exploiting 
that information, by using that information.   So it's worth thinking about this to yourself: why 
and how these strategies could be made better. But   I think the key that's emerging here is that 
strategy B is really focused on exploring:   it's going to tell us a lot about the 
MDP. We don't know anything going in,   it's going to give us all the information 
about the MDP, but the problem is that   eventually we're going to have tons of information 
and we're not going to be using it for anything,   there are better choices that we might 
be able to make with that information   but we're not going to make those better choices. 
So even if I have this observation that when I   have rich soil and I plant it I make a ton of 
harvest, I might just not plant in rich soil   or sometimes I might have poor soil and I might 
just keep planting it for a whole bunch in a row   even though I know that's not a good idea. These 
are all totally possible things under strategy B.   Conversely, strategy A we saw was focused 
on explaining it, just did it too soon.   We didn't really know anything and it 
was like “yeah, let's go for what we   think is best even though it's based on 
really imperfect partial information.” And so, of course, what we'd like to do is 
we'd like to really trade off these ideas,   we'd like to have a little bit of 
both, we'd like to have the best parts   of both really. Okay, so we're going 
to think about how we can do that.   Okay so this is a super common concept in computer 
science and machine learning, this exploration   exploitation trade-off and it very much is a 
trade-off because each time, each harvest season,   we have to make an action and we can either 
choose to explore or we can choose to exploit.   So at a very high level, abstracted away from this 
problem, what we mean by exploration is that when   we explore, we're trying to understand the world 
in some sense or we're doing different things   in the world and that might reveal something 
about the world to us. In this particular case,   the things that we're learning about the things 
we don't know are T and R, but in general,   exploration could refer to something much more 
general, just trying to understand the world. In exploitation, we take what we know, which 
will always, in general, be something imperfect.   It's not necessarily going to be a perfect 
understanding of the world but we take that,   we try to make the actions that get the best 
reward and so farmer A was going straight to   exploitation without doing exploration, farmer B 
was doing all this awesome exploration but never   taking advantage of it to make all of the best 
actions and so we'd like to have some trade-off   between them. Now this is just a huge area that 
you could spend basically like a whole class,   certainly a whole lecture on, saying how could we 
do this trade-off and so we're gonna take a much   simpler approach and just say, “let's randomly 
decide which one to do.” So I just want to   emphasize so much that this is not the only way to 
make this trade-off but we're gonna make a choice   that does at least encapsulate that the trade-off 
exists and make some kind of decision on it   and this is known as an epsilon-greedy 
strategy. So the epsilon-greedy strategy says,   “okay, it's coming time to make an action. 
It's the next harvesting and planting season   and so I'm going to make my action in the 
following way: I'm going to choose a random   variable, a uniform random variable or, well, 
I guess a way to think about this is a random   variable that has probability 1 minus epsilon of 
being one value and probability epsilon of being   another value. So with probably 1 minus epsilon, 
I will choose to exploit, and with probability   epsilon, I will choose to explore.” And the reason 
that we have this epsilon here, epsilon often   denotes a variable that tends to be a little bit 
smaller, so the assumption here is that most of   the time I'll probably be trying to exploit and 
some of the time I'll be exploring but epsilon   ultimately is something that you get to choose, 
it's a hyper parameter of what we're doing.   Okay so this sounds very good but the reality 
is we have to say what this means, what does it   mean to explore and what does it mean to exploit? 
And so let's try to get concrete here. Now again,   these choices do not have to 
be the choices that you make,   I think that even right now you could come up with 
other ways to explore than what I'm about to say   but one way that we could explore is we 
could choose an action uniformly at random.   Certainly, if all of our transitions are 
sufficiently rich, then we'll eventually get   to every state that we could get to and so this 
will help us explore the space that we're in,   so this is one option, certainly not 
the only option, for exploration. Now we have to say: what does it mean to 
exploit? And in some sense, that's going to be   basically the rest of the lecture: how do we 
exploit the imperfect information that we have   at any finite time step? We saw in 
the beginning we have no information,   we hope that by exploring and also by exploiting 
that we learn something from our MDP or,   in general, the world as we go but now we have to 
take that information and do something with it,   we have to come up with an action from it and so 
that's what we're going to be talking about next. Okay how can we exploit? Okay well let's 
just think for a moment about how we would   do this if we had perfect information. So if 
we knew Q* (remember we said Q* was the value   of taking action a at state s if we make 
all the best actions after this step) then   we know exactly how to exploit. We actually 
did this in the previous chapter of reading,   this is what we set up previously in the case, 
basically, where we know the whole MDP, we know   how to exploit and that is we maximize Q*, we 
find the action that maximizes this, basically the   value, what is the reward that we're going to get. 
So in some sense, we did this work already: we   said if we know everything, we know exactly how to 
get the most reward from it and the whole issue,   the whole reason that we're having a discussion 
about how to exploit is that we don't know Q*,   and there's different ways of not knowing.   So if you knew T and R, if you had the 
values of T and R, you could solve for Q*,   it just might take a little while, it might 
take a long time, you might have to run value   iteration but like somehow it's defined by T 
and R. Here, we're saying we don't even have T   and R, we don't have to have the information, it's 
not just about running a computer program, we just   don't have the information to even solve for this 
and so we have to ask what can we do in that case. Okay so here's the idea that we're going 
to explore: can we guess or estimate,   can we make an educated guess about 
something that hopefully is near Q*   based on the information we have so far? 
And that's exactly what we're gonna do   going forward. So once we have some kind of 
guess or estimate, let's call it Q for Q*,   some notion of “I think this might be the value 
of taking action a at state s and then making all   the best actions after this step.” Then I could 
choose my action based on that Q, I could say,   “okay for this, for this guess q, what's my 
best action?” And that'll be my policy pi_Q. Okay and so this reduces our question to: how 
do we get a good guess or estimate Q for Q*?   And so we're going to explore different ways 
to do that basically, we're going to say:   is there some way that we could get a decent 
guess for Q*? And then that will let us   do something like exploit as well as we can   and we're just not going to be able to exploit 
really well in the beginning because we don't have   a lot of information and then the hope is that by 
learning, we're able to do it better and better. Okay and so, in particular, how are we going to 
learn Q*? How do we get a sense of the value of   taking an action at state s and making all the 
best actions? Well we're gonna see some data.   Now, what do I mean by data? I mean the 
observations that we make in each step.   So you can think of those observations 
as being the state that we started at,   the action that we took, the reward that we 
got and the state that we ended up at. Now   this is a little bit redundant, 
we're going to see, obviously,   some of these states across the steps but you 
can think of it this way: I mean, basically,   when we see a reward at a particular state in 
action, that obviously tells us something about   the R function. I mean that basically completely 
tells us about the R functions. That's going to   be a relatively easy thing to estimate and if 
we see a state s and the state that we go to,   then that's going to tell us something about the 
transition function and so our first idea here   for estimating Q is how about we estimate T and R 
and then plug that in to get an idea of what is Q. Okay so we're asking: can we learn Q because 
that'll help us do this exploitation? And so our first idea is that if we knew the 
transition function T and we knew the reward R, we   could directly solve for Q*. And so if we estimate 
the transition function T, we estimate the reward   R, then we can get a Q that is hopefully near 
our Q*. Okay so what might this look like?   So here is a proposal, a strategy for how to 
proceed. So we had our strategy from farmer A,   we had our strategy from farmer B, now let's try 
out this new strategy. We're going to initialize   with the state of our system, this is just a 
notational thing. I mean the farm is what it   is when we get it, let's call that s_0, and we're 
going to say: what is that state now? What we're   going to be doing is we're going to be estimating 
the transition model T and the reward function R   and so, before we've seen any data, before 
we've tried planting our soil or fallowing   it or taking any actions whatsoever, we have to 
ask ourselves: what could we save for T and R?   Basically nothing. We don't know anything about 
T and R at this point, so we're kind of just   initializing these values to something that's 
an initial value and so here we're gonna say,   just as an initial guess before we know anything, 
that we're gonna say all the rewards are zero   at the very least. That tells us we're 
not erroneously exploiting something   by saying that it has a big reward when it 
doesn't. Let's just pretend all the rewards   have zero and then we will be corrected 
as we go if there are different rewards   for T. Remember T is the probability of starting 
at state s, taking action a and going to state s’   so there are S possible values or there's this 
script S which is the set of possible states   and the absolute value of script S here 
means the number of possible states   and so what we're saying is, “let's just assume, 
because we don't know anything about the states,   that they all have equal probability 
to begin with. We're going to update   this as we get information, but that'll 
be our guess before we've seen anything   okay.” Now once I have any T and R, I 
can get a Q from it using, for instance,   infinite horizon value iteration. So in this 
case, because all the R's are zero, the Q is   just going to be zero everywhere too because, 
remember, the Q represents a value that we get   from rewards and so it should be that if all the 
rewards are zero then you never get any reward. Okay so now we're going to go forward 
with our farm, we're going to go from   year one to year two to year three and so on and 
we're going to aim for the following strategy. The first thing we have to do in 
any given year is select an action   and that action will depend on 
our current state, so that's s^(t)   and we just said we're going to use Q to 
figure out how to exploit what we know so far.   So what would we do here? Well this is where 
something like epsilon-greedy comes in,   so it tells us how to select an action. So 
in epsilon greedy, we say with probability   1 minus epsilon we exploit. So we use Q, we 
choose the best action according to Q. And   with probability epsilon we explore, 
we just choose uniformly at random.   So epsilon-greedy gives us, essentially, an 
algorithm for how to choose that next action.   Now, once I've chosen an 
action, nature takes over.   So what's happening in execute is I'm doing 
my action—for instance, I plant my soil–and   then nature is going to tell me what happens next. 
So nature's going to take the action that I made   together with its current state. So really this 
depends on the state as well, the current state,   and then it's going to say, “okay, given 
your current state and the action you made,   what was the output state?” So for instance, 
if I were planting in rich soil, then nature's   going to run its MDP that I don't know about 
and it's going to decide am I in rich soil or   poor soil now and it's also going to run its 
reward function and say what is my reward. Now this involves the MDP with T and R that I 
don't know but I'm going to try to learn T and R.   Okay learning R is relatively 
straightforward: as soon as I see a reward,   I know that that's the reward that 
I get for state s and action a,   just by the definition of the MDP that must have 
been the reward, and so I'm able to immediately   fill this in. This is a great estimate: it's 
exactly what the reward function is at that point. T is trickier. So for t, we're trying to say if I'm in a state 
s and I take action a, what's my probability   of going to s’? And I want to do this for every 
state s and every action a and every possible   s’. Okay well let's think for a moment. What are 
we trying to do here? We're trying to estimate the   probability of something happening. So let's think 
for a moment of another example. Suppose that you   were trying to estimate the probability 
that you eat dessert after your dinner.   How might you estimate that probability? Well 
one idea is you could record, every now and then,   when you eat dinner what happens. 
So you put a little mark down   every time you're recording and then separately 
record “did I eat dessert.” So in some sense,   you're gonna count a number of times that 
you ate dinner and then you're going to count   a number of times that you ate dinner and 
you had dessert and you're going to divide,   you're going to say, “what's the number of times 
I eat dinner with dessert divided by the total   number of times I eat dinner?” That would be 
a reasonable estimate for the probability that   you eat dessert with dinner and we're going to do 
the exact same thing here. So we're going to say:   what's the probability that I start in state s, 
I make action a and I go to state s’? Well in   order to do that, I can count the total number 
of times I was in state s and I made action a.   So that's all this sum is doing: 
it's putting down a tally of one   every time I was in state s and made action a up 
to the present point (so once I get to some t,   I've gone through time period one, time period 
two, up to time period t) and then I can say   what's the number of times that I was in state s 
and I made action a and I ended up in state s’? Okay so this is a perfectly reasonable way to 
estimate the probability of being in state s   making action a and going to 
state s' with a slight exception   and that exception is the following: let's 
say I come through this and t is one,   then there's a good chance (in fact, it's 
totally unavoidable for any non-trivial state   space and action) that they're going 
to be some cases where I have not yet   observed being in that state and taking that 
action and so this denominator would be zero   and that's problematic, I 
don't want to divide by zero. Another issue that can arise, even if the 
denominator isn't zero, is that I might not have   yet transitioned to s' in which case the numerator 
is zero but I don't want to say it's impossible   that I could have transitioned to s'. It might be 
unlikely but I don't want to say it's impossible. And so a way that I can deal with this 
is what's known as a Laplace correction. So I add one to the numerator and the number 
of states to the denominator and the way to   think about this is to think about what happens 
at the extremes of time. So if my t is zero,   I haven't seen anything yet, then there's 
no summations and I just say that I think   the probability of transitioning to any state 
is equally likely. In fact, that was exactly   the starting states that were the starting 
guesses for these probabilities that we used. Now as I get more and more and more and more data, 
in the sense of I observe more and more things on   my farm, I go through more steps on my farm, I 
go through more seasons, these counts are going   to be really big, they're going to get larger and 
larger and larger and they're going to dominate   the 1 and the number of states 
are going to be relatively small   and so this is basically going to go to, 
asymptotically as we get more and more data,   go to the value as though there wasn't 
even that one and that |S| in there. And so, in some sense, this little class 
correction is balancing between that case   where we start off and we don't really 
have much information or any information   and the ending case where if we've 
actually observed a ton of data,   then we really want it to be basically the 
fraction of time as we went to state s’. Now I've been referring to data a few times and 
so I just want to make it explicit, again, what I   mean here. We're imagining that the data at step 
t is these observations we're making, the state   that we start at, the action we take, the reward 
we get, the state that we end up at. These are   the observations that we're making and it's what 
we're learning from, it's what tells us about T,   that's what lets us make this estimate for T, 
that's what lets us make this estimate for R.   Now I said that we do this for any s, a, and s'. 
It's worth thinking to yourself: do we really   have to update this for every state and every 
action and every s', every time we update T?   I think you will find that if you think about 
this for a moment, that there's only some that   will actually change and there's only some of 
these that you'll have to update. So you might,   if you have some free time later, 
think about which ones those will be. Okay so finally, once we have our estimate 
of R hat and T hat, what we do is we say,   “okay, we can get an estimate of Q from this, an 
estimate of the value of making any action” and   then we can use that to choose the best action 
when we come to the next round of this loop   with our exploit strategy. So the whole idea here 
is, again, we're going to take epsilon-greedy,   epsilon-greedy is what's going to let us 
trade off between exploring and exploiting.   And then for exploiting, what we do is we 
estimate these transition probabilities,   we estimate the reward, and then we use that to 
decide what we think is the best action right now. Okay so let's see just a little bit of an example 
of what this might look like. So maybe I choose   epsilon equals 0.3, I'm going to explore three 
tenths of the time, and I choose gamma equals   0.99 because that's how much a harvest next 
year is worth to me now. And so first, I'm   going to initialize and so maybe it happens that 
my farm has rich soil when I first encounter it. Now I have to initialize all my T hats 
and my R and so I'm going to have my T   hats all be one over the number of states 
and so, in this case, each one of these,   there's two states that we could 
go to, and so it'll all be 0.5. And likewise, I'll initialize 
all my R's to be zero.   Now I have to select my action and if I'm 
going to use epsilon-greedy, what I'm going   to do first is I'm going to have a random draw 
that, with probability epsilon, explores and   with probability 1 minus epsilon exploits and 
so maybe this time I happen to choose explore. And so if I explore, then with probability one 
over the number of actions I choose each action,   and so maybe this time I randomly 
chose to let this field lie fallow. So in this case, the reward happens to be zero, 
I don't harvest anything if I let the field lie   fallow and so that's just gonna get updated, 
it's just gonna be zero. So I execute my action,   I get my reward, turns out it happens to be 
zero here, I get to the next state (again this   is something that nature tells me what state 
I get to), and this one happens to be poor.   Okay again, I update my reward. In this case, it's 
trivial to update because there was no reward.   I update my estimate of T hat. So in 
this case, I started for a rich state   and let it lie fallow and so I'm going to 
update the corresponding probabilities. I learned a little bit of something 
about those probabilities but it's   not a lot because I've only seen 
one random draw at this point.   My hope is that over time I'll see more 
and maybe I'll get some better estimates. Okay now I run my infinite horizon value 
iteration, I get my Q and so I know the   best thing so far to do next time,   although nothing's really changed with Q 
because all the rewards are still zero. So I can go through this again. Maybe this time in 
my epsilon-greedy random draw choose to exploit,   but here's the annoying thing about exploit: 
when Q is all zero, there's nothing to exploit,   like exploit is pretty bad at this point because 
I don't know anything, basically every action   is the same and so maybe it just so happens that 
fallow comes first and I have to have a tiebreaker   and I just choose what's first. Okay well 
in that case, I'm going to take this person,   I'm going to let it lie fallow, nature tells me 
what's the next state. In this case, it's poor,   so I'm going to update my R's and update my T's. 
Nothing really got updated with R, I did update T   a little bit. Maybe again I happen to exploit. 
In this case, again, there's no information:   my exploit function is really bad at this 
point because I haven't learned anything   and so, at this point, it might just 
tell me to let my field lie fallow,   so I do that again. Maybe I happen 
to choose, this time, to exploit.   Again there's no information at this point, so 
maybe I choose to let the field lie fallow even   though it's a rich field. Sounds like it might 
be a good idea but we don't know that yet and so   we'll do this and so we can keep doing this, but 
you'll just notice, in the beginning, we're not   getting too much from the exploit function because 
there's nothing to exploit, there's no information   that we have. Here we finally explored and we 
finally tried planting a rich soil and so we might   get some good information from that, but it took a 
while. So now let's say we go far into the future   when my ancestors are farming or my descendants, 
I guess I should say my future descendants 2000   years from now, are still farming this field and 
at this point, we have so much more information,   we've probably seen all the rewards, we have a 
lot of information about the T function, we really   have a good sense of what are the transitions, 
and so now the exploit function is fantastic:   it tells us, really, what is a good thing to 
do to get some reward from this harvest and,   in this case, it tells us to plant. If I exploit 
when it's poor, it will still learn. If it has a   good information about T and about R, it'll still 
learn this idea of the delayed reward and if I   let things lie fallow this time, then I can get 
more reward next time and, in this case now, when   I'm in this distant future where I've learned so 
much about what's going on, the explore function's   a little bit annoying. I have this chance of 
exploring and when I explore there's a chance   that I might do something that's sub optimal, 
that I might let my field like fallow even though   it's rich soil and I know that it would get so 
much reward from rich soil and so there's this   interesting trade-off that, in the beginning, 
you're not doing too much by exploiting,   maybe there are better ways to handle that, maybe 
we want to explore more in the beginning and later   on you're not doing too much by exploring if 
you really know everything that's going on. And so it's interesting to think 
about what strategies you might use   beyond epsilon-greedy but here we're 
using epsilon-greedy for the moment. Okay so this was an option. One option 
was to learn T and to learn R and then   put them into Q and use that to proceed. 
It turns out another option is to estimate   Q directly and what I mean by that is to basically 
not estimate T and R along the way, to see if we   could just come up with this function because if 
we had any estimate for Q and it was a good one,   then we can use that to get 
the best actions from there. Okay so let's see if we could do this. We're 
gonna have to put a little thought in. So this   is what infinite horizon value iteration looked 
like. So here, we're just pointing out… Well,   to be more precise, this is the function that 
Q* has to satisfy, this is just the iterative   procedure that Q* has to satisfy, and from that 
we got infinite horizon value iteration by saying   what we'll do is we'll start off by initializing 
Q_old to zero and then we'll just iterate through   and get Q_new and then make it Q_old and then just 
keep doing that until we get something that's very   close to Q*. So if we knew T and R, we could do 
this okay. And the issue, of course, as we said is   that we don't know T and R, but as we noticed just 
now, we can pick up R along the way. As we try out   different soil types and different actions, 
eventually we're going to pick up what R is.   We know what gamma is and so somehow 
the issue is, perhaps, this last bit   which we can see is something called the 
expected value of a discrete random variable.   So what I mean by that is 
it takes the following form:   it's a sum over all the states of the probability 
of the state times the value of that state. Okay so this is something that comes 
up in a lot of different areas not   just here in this reinforcement learning lecture.   Another area might be something like insurance. 
So an insurance company really wants to know what   they expect to pay out to you in order to tell 
you what's the price of their insurance policy. So unfortunately fires are very much in the 
news right now. You might be interested in fire   insurance, there's been a lot of fires happening, 
certainly in the US and in many of the Western   states, and so you might be interested in saying, 
“oh, I want to get fire insurance for my house”   and so then the insurance company is going to have 
to ask, “well, how much do I expect to pay out for   you for this fire insurance?” And so they might 
use a formula like the one that we have above.   So we say that we're going to look at the two 
states: fire and no fire. In the fire state,   we have the probability of a fire 
happening. Hopefully it's very low,   but it could still happen. Maybe it's a one 
in ten thousand probability happening across   all of the different people who might have a 
fire. If a fire happens, then they're going to   have to pay to replace your house and maybe 
the average insurance payout or the typical   insurance payout we will say is 100,000. 
Now most of the time, a fire doesn't happen   so the probability of a fire not happening, let's 
say, is 1 - 1 / 10,000, so basically the rest of   the probability. And in that case, they're not 
gonna pay out anything, they're gonna pay out $0. And so if all of this is what might happen 
in a given year, like probability of a fire   in a given year is 1 in 10,000, they'll pay 
out a 100,000 if that fire happens, then the   insurance company could say that their expected 
value or their promised insurance payout is $10   because that's just running this formula that 
we saw here, that's what they expect to pay   out to you and so in order for it to be worth 
it to them to set up this insurance payout,   they're going to have to ask you to pay more 
than $10 in a given year. Now that's a lot   better than a 100,000, so you're probably 
interested in doing this insurance payout,   but it's worth noting that there is still some 
value here, there's still some non-zero value. Now just as in our situation where we don't know 
T, we don't know the transition probabilities,   very realistically this insurance company might 
not go in knowing the probability of there being   a fire for a particular house and there not 
being a fire and so they might want to estimate   what's the expected value of their 
payout. Okay so let's think about   how we might do this estimate and then 
we'll apply it to the case that we have. Okay so this is, just again, the expected 
value of a discrete random variable:   we have our example, the expected 
value of an insurance payout,   and so how could they possibly estimate 
how much they expect to pay out?   Well if they're an insurance company that 
operated last year and they had a ton of   people that were being insured by them last year, 
they could ask, “well, what happened last year?   How many people did they insure and what 
proportion of those people got a payout?” That   could tell you what's the probability, an estimate 
of the probability of the payout, but they could   also ask how much did they pay out and so then 
they would know what's their average payout.   And so, in this case, they might actually estimate 
the expected value by saying, “okay, again, what   did they pay out to every single person? How much 
money did they pay out and how many people were   there overall?” So this could be their estimate 
of the expected value for any particular person. Now there was this chapter that we never had a 
live lecture for. Remember back in chapter seven,   we were talking about various concepts and 
one of them I want to bring up again now. So,   in some sense, it's review: you read about 
this in chapter seven, but in some sense it's   new because we didn't do it in a live lecture, 
but this is the concept of a running average.   So we can take this expected value that we 
have and we can write it in a different way,   or this estimate of the expected value. 
So, in particular, we can write it as   a running average with a particular 
choice of the weights for the average.   So remember the idea of a running average was 
that we started off with an initial value of zero,   it's our estimate before we've seen anything,   and then we update our estimate by saying 
what's the estimate we have so far,   multiply by one minus this alpha weight, so 
we just down weight the things we've seen in   the past, and add the new observations. So every 
time we have an observation, we can make an update   to our estimate and then we'll get our our new 
estimate (E tilde)^(t) and so something that,   if you didn't do it in chapter seven, this was 
one of the questions that was in the reading,   the study question on your own, but 
it's worth doing if you haven't done it   or refreshing your mind if you have, is to 
show that if you use this running average   (so every time you get a new value observation, 
you update your e tilde and you do this t times),   that you get the same thing as the typical 
average, the one over t, sum of the values of t. Okay so this is one way that you could estimate 
this expected value, here's another way,   here's a different running average, we could use 
a different running average. We could instead   use a running average where the alpha^(t) is 
just a constant: it's just equal to alpha. So here, what we're going to 
do is we're again start with,   now e hat, let's call it equal to zero. We're gonna take, iteratively—we're gonna do 
this iterative procedure where we take our   estimate so far—we scale it down, scale 
down everything we've seen in the past   and then we update with the 
new information that we have.   And the big difference between this line and 
the line above is that our alpha is just always   the same in every one of these updates, 
we're not changing it with every update. We can ask ourselves if the running average, 
where we choose the alpha^(t)’s to be 1 over t   is equal to this estimate of the expected value,   is there similarly some choice if we 
make this running average with constant   alpha, is there similarly some way we can express 
our estimate as some sum across the values? In fact this is true, so you can check   that the following formula holds. In fact, 
let's do that super briefly right now. Basically what we want to do is we want to 
check, well, one: that (e hat)^(0) is equal to 0,   but when t equals zero there's nothing in the sum, 
there is no sum, and so this is just equal to zero   and then we want to check 
that the recursion holds. So for the recursion, let's first write 
the right hand side of the recursion:   1 minus alpha (e hat)^(t) minus 
1 plus alpha times the value.   Now what we can do is we can sub in 
our proposed formula for (e hat)^(t). So all we're doing is we're sticking that 
in but we're applying it at t = t - 1   so we get in (1 - alpha)^(t - 1 - i) and then with 
a little algebra, we can see that the first part   is just going to be the first t - 1 summons and 
the last part is going to be the final one and so   this satisfies this recursion, this is a different 
way to write this different running average.   And so now we can ask ourselves: well< 
what's different between this formula   and the typical average, the usual 
empirical average that we would use?   Well what's different is that this latter 
formula gives bigger weights to the more   recently observed value, so let's just double 
check that for a moment. If t is just one,   we're just going to get alpha times this value, 
so we only have this one thing, alpha, times   the value that we have. Oh sorry, let's think 
about if t is very large but if we take i = 1,   then what we're going to do is we're going 
to have a lot of these one minus alpha   things out front because we're gonna have t (very 
large) minus i just equal to one. If t is 1,000,   then 1,000 - 1 is 999 so we're getting a lot of 
these terms that are all between zero and one.   When we multiply them together it's gonna get 
very small whereas if t is 1,000 and i = 1,000,   that whole 1 - alpha part goes away 
and we just have alpha times the value   and so we're giving a much higher weight to the 
values that occur towards the end, the ones that   we've just seen, maybe the very recent people who 
got an insurance payout, we're giving very little   weight to the ones in the beginning. Now what are 
the benefits of using one versus the other? Well   if you use the estimate up top, the e tilde one, 
this has a lot of nice theoretical properties.   If you are actually drawing according to some 
distribution, which is the probability of s'   equals s' for each of these different outputs, the 
fire and no fire, it's always the same, then this   is going to go to the exact expected value as you 
get more and more data. But, it seems plausible   in this fire/no fire case that actually fires 
are becoming more and more probable over time,   at least in certain areas, and so in those cases, 
you might want to weight the more recent values   higher because they're closer to the distribution 
that's happening now and that distribution changed   over time and so one reason that you might prefer 
this alpha weighting, instead of the 1 - alpha^(t)   weighting is if things are changing over 
time and you trust the more recent ones more. Okay so this is just a review of 
running averages, this 1 - alpha   and alpha running average and we're going 
to apply it now to our Q estimation case. Okay so let's go back to where we were. We 
said recall infinite horizon value iteration,   this is something from last week's reading. Now 
another way that we could write this expected   value is we could take it outside the R. So R 
has no s' dependence, gamma has no s' dependence,   the T's all add up to one across the s primes, 
and so we can just take this initial typical   infinite horizon value iteration step and we can 
just write it slightly differently by putting   that sum over the T's up front. So we haven't 
really done anything here, we've just written   it differently but you'll notice this is an 
expected value too: it's a sum across the states   of some probabilities times some values and now 
the values are the R(s, a) + gamma * max Q_old. Okay so here's the proposal: every time we 
observe a state, an action, and a place that we go   (so we make an action and nature 
transitions us to some s'),   we're going to update our estimate of Q by 
using exactly this running average with alphas   that we saw from before. So in particular, 
we're going to take our old estimate of Q,   we're going to scale it by 1 minus alpha, 
we're going to take our new observation of   the value that we just got for this new s' and 
we're going to scale it by alpha and we'll add   them together. So this is just exactly this 
alpha running average we just talked about. Now something that's interesting about this 
is, in theory, what we would do if we wanted   to do infinite horizon value iteration is we would 
update this. Every time we got a new s', we would   eventually get a pretty good estimate of Q_new, 
but now we still have to do infinite horizon value   iteration, so we still have to put that Q_new in 
as Q_old and then make the update and keep going   and so an observation that we could make is, well, 
maybe we don't have to always put in Q_old here,   we could just immediately use the new Q that 
we've just gotten, the new, better estimate of Q   and so a different proposal is to do exactly that. 
So these proposals are exactly the same, the only   difference is that instead of using this Q_old, 
we just take whatever is our latest estimate of Q. And so this is what we're going to do to update 
Q, you can think of it, even if all that went   kind of fast and you just want to get a high level 
idea of what we've done, you’ve taken the ideas   of value iteration, the dynamic programming 
ideas from that, and we've taken the idea of a   running average, to estimate what's going on with 
this random variable, what's going on with this   random T transition, and we've combined the 
ideas from both worlds to make an update from   our Q estimate that we have so far to a new 
Q estimate and so let's put this all together   to learn Q. So the question was: can we learn 
Q? In some sense, we already answered that. Yes,   we could do that by learning T and R and 
then putting that in to our value iteration   algorithm, but let's try it even more directly and 
just learn Q directly using this kind of update. So yes we can learn Q   and, in fact, we're going to call this algorithm 
Q-learning to indicate that it is learning Q. Okay so it's going to take various inputs. 
Again, we're going to assume we know the states,   we know the actions. We're interested in whatever state we 
start in, because now we're really saying,   for the farm that I have, what is the 
set of actions that I'm going to take. We know the discount factor, we're assuming we 
do, and we're assuming now we have this learning   rate alpha that we've chosen. So basically, we 
assume we have these parts of an MDP except for   T and R, we assume we have our starting state, 
and we assume we have the learning rate alpha. Okay so the first thing that we're going to do 
is we're going to start off, just as when we were   doing this for T and R we started off with some 
guess for T and R and then we updated over time,   now we're starting off with some guess for Q.t 
This is basically like assuming all the R's are   zero, we're going to assume all the Qs are zero 
and then we're going to update it over time. Okay so we initialize the state that we're 
in. So again, this is just saying, “hey,   we start off in the starting state, and then 
we're going to say: ad infinitem, into the future,   we're going to follow our strategy. Okay the first thing we do 
is we select our action.   Just like before, we're gonna use epsilon greedy. 
We could do this some other way, but here we're   using this exploitation/exploration trade-off with 
epsilon greedy. So in particular, given the Q we   have so far, which encapsulates what do we know 
about the problem so far (it's not very much in   the beginning, but over time there'll be more) and 
the state we're in right now, which is s, we make   an action. So if it's epsilon-greedy, we say with 
probability 1 - epsilon, we're going to choose to   exploit using Q and with probably epsilon, we're 
going to just choose a random action without using   Q. So technically speaking, this really also 
requires specifying epsilon in the beginning. Okay now we've selected our action, we 
make our action, nature now does its work. We execute our action in the world that we're in,   nature sees what states it's in, nature 
sees our action, and nature returns what   is the reward we get and what is the s' that 
we get, what is the new state that we're in. And now we make our updates, so we can make better 
actions in the future. So before what we did is   was we updated T and we updated R. Now what we're 
going to do is we're going to update Q directly.   So in particular, we have our Q from last time,   our Q(s, a). We observe R, so we're putting 
that directly in, the reward that we observe,   R. We know gamma, we can maximize Q 
based on what we knew about it so far.   And so we can put this whole 
update in and then update Q.   So the right hand side is using our old Q 
estimate and the left-hand side is our new Qs. And now we just say, “hey our new state   or our state that we're in is the 
state that we have transitioned to.” Okay so just one alternative perspective on 
this update, there's just this one update here,   it's this Q update. We can think of 
it, instead, as taking our existing…   In order to get our new Q, we take our 
existing Q and then we change it a little bit. So in particular, we can think of 
this as being like a one step estimate   of how Q changes. This is what you would calculate 
if you were running infinite horizon value   iteration for one step forward. You could also 
think of it as the expectation estimation that you   would get from just one data point and then we're 
going to ask how that compares to the existing Q. If our existing Q is smaller 
than we would have expected,   based on this one step, so if this whole thing is 
negative, then this whole thing will be positive   and we'll add a little bit to Q. So if Q is 
smaller than we expected, we add a little bit.   Conversely, if Q is larger than we expected, 
or this update that we get from this update,   then this whole thing will be positive 
and this whole thing will be negative   and we'll move Q down a little bit. And so it's 
doing a minor adjustment based on this step ahead   versus the Q that we have because, somehow, in 
steady state, these things should just be equal. Okay, so this is just the exact same update, it's 
just written slightly differently. You should   definitely make sure that you agree that these 
are two ways of writing the same thing, especially   when you see the second version, this is sometimes 
called temporal difference learning and the idea   being that you compare the Q from your time step 
right now with this one step ahead look ahead.   So it's like one step in time and you compare that 
difference and you learn based on that difference.   So that's where that name comes from. 
So just a couple of points about this:   one, unlike our previous algorithm which 
is also a totally legitimate algorithm,   this algorithm never learns or stores T or R.
Even R, which is in some sense not too hard to   store, it doesn't: it just looks at the reward 
you got this time and updates based on that.   It definitely doesn't store T: T doesn't 
appear anywhere. It completely bypasses   that. It doesn't instantiate those things and so 
if you really did care about finding out what was   the reward function or, even more specifically, 
what was the transition function, you don't get   that along the way here. If you don't care 
about that, then that's nice, because these   are really easy simple steps. Something that's 
a big difference between this and the previous   algorithm is you don't have to run infinite 
horizon value iteration which could actually   take a really long time to run. Here, you're 
just making this update directly and quickly   to the Q and so you don't have this inner step 
of running this possibly annoying thing to run:   all we're getting here is Q. A quick question. 
Yes, quick question, probably to summarize, but   could you just explain again what we're trying to 
find in Q and an explanation of what Q is? Yeah so   remember Q* is this function that if we had it, we 
could extract what are the best actions to take.   And so if you get nothing else, 
Q is just an approximation to Q*.   More specifically, Q* and this is from 
the last lecture, the last reading,   tells you what is the value of taking a particular 
action in a particular state and then doing the   best thing ever after. So you can think of it as 
being the value of an action in the current state   under the assumption that you do the best 
thing after that action. And that's why it   lets you choose what's the best action, because 
you choose the one that has the best value   and so everything that we're doing in this 
lecture, in some sense, and in this reading,   is saying, “hey, could we approximate that?” 
Because unfortunately we don't have access to Q*.   If we knew the transition functions and 
the reward function in what we were doing,   then we would know Q*, but because we don't 
know the transition and the reward function,   we don't and so we have to somehow approximate 
it. And so the idea here is to somehow say, “hey,   can we come up with an approximation?” It's going 
to be a bad approximation in the beginning because   in order to get a good approximation, you have 
to observe something about the world that we're   interacting with and so what will happen is 
that you'll have a really bad approximation   in the beginning and so when you exploit you're 
not going to do very well. But over time, by   exploring and by some of the exploitation moves, 
you'll get more information about the world,   you'll get a better estimate of Q* (that's Q), 
and then you're going to use that to make better   exploit moves, to make better moves that get more 
reward. And so that's totally the main idea here   is, basically, how can we learn more about 
the world and how can we encapsulate that?   How can we summarize that in a way that we 
can take advantage of to make good actions? I'll just leave you with one last thought: you'll 
notice that there's a difference between this   optimal policy, or the optimal policy based on 
our knowledge of the world, Q, which is what we do   whenever we're exploiting, and the actual set of 
actions we take. We don't just take the best guess   at the optimal policy here, we do some exploration 
moves and some best guess of the optimal policy   moves and so it's a little bit different than 
just exploiting or just trying to exploit.   Okay great. I'll see you next time 
and remember: get out there and vote. 

Okay, good morning, it's about that time. So 
let's just recall what we've been doing in the   past couple of lectures. So first, we developed 
these ideas of state machines and Markov decision   processes to see and model how we interact with 
the world and how that can change the world   and how, in particular, having different inputs 
can change states in the world and, really, in the   past two lectures, we've been focusing on using 
that information to choose a best set of actions   in the world. So on one hand, we said if we know 
everything about how the world operates (we think   it's an MDP and we know it's transition model and 
we know its reward function), then we can just   use value iteration to find the best set of steps 
that we would take in this world, the best policy.   And if we don't know those things and we have 
to muddle around and figure out what's going on,   we can use Q-learning. We learned about that 
last time and today we're going to see that we   can use these ideas of state machines and states 
changing in ways that aren't just what we've   done in the past few lectures which is basically 
reinforcement learning. We'll talk about this on   the next slide a little bit more but we can 
actually use it in supervised learning as well and   so we'll see how that can arise with sequential 
data. We'll look at an example with text   prediction and we'll basically define a recurrent 
neural net. Okay, so first, though, let's again   just spend a little time situating what we've done 
in the past couple of lectures. So, in particular,   we didn't explicitly get to saying it, but now 
let's explicitly say it in the last lecture:   we did some reinforcement learning and the idea 
of reinforcement learning is that we're learning   to maximize rewards by interacting with the 
world. And so this is different from what we saw   in supervised learning, which is basically what 
we've been looking at the whole time beforehand.   There are some similarities: you can think of 
a negative loss as being a reward, so it's not   like the concept of reward is totally not there 
in supervised learning, but in reinforcement   learning our actions affect both the reward but 
also even our ability to observe the environment.   Only by taking certain actions might we even be 
able to see certain parts of what's going on.   Our action this round can affect future 
rewards via the state and so this idea   of interacting while we're doing the learning is 
very different in reinforcement learning instead   of unsupervised learning. Just as a few examples 
too, let's just talk about when you would use   reinforcement learning. The idea is that we would 
learn by doing and so we had this extended example   in the past couple of lectures: if we have a 
farm and by working on the farm we learn about   the soil qualities, how we change them, and 
what rewards we get. In the beginning, we make   a lot of mistakes, but over time we become farm 
experts and then we get really good at it. And   this is the thing that you see in a lot of other 
applications, this kind of theme. A really, really   big one in reinforcement learning is gameplay: you 
start playing a new game like chess or go or World   of Warcraft or whatever is your favorite game or 
game you'd like to try out and in the beginning,   you're going to flounder about—you're not going 
to be very good at it, you're going to make a   lot of mistakes—but you learn by playing it and 
then you get really good at the game over time   and so the idea here is to automate that process. 
You could also see this in digital marketing,   for better or for worse depending on how you feel 
about digital marketing. So multiple interactions   with a potential customer online, perhaps using 
cookies or something like that, you might ask   is there a best policy to get them to do what you 
want if you're a marketer, to get them to sign up   for an email list or buy a product or something 
else? There's a lot of excitement about using   these ideas for other things like automating robot 
tasks in industrial settings or manufacturing   settings, self-driving cars, etc and you can see 
a little bit why there would be a lot of research   in this area because as soon as we start thinking 
about these other applications, maybe even medical   applications, you don't want to make a lot of 
mistakes in the beginning, you don't want to   learn about what happens when you crash your car 
by crashing your car or what happens when you   nosedive your plane by nose diving your plane and 
so there are issues of safety: how do we adapt   this to have a lot of safety? There are issues 
of making sure that you don't need so, so many   training data points or like rounds of learning to 
get somewhere. So those are some challenges that   arise there but I just wanted to take that 
time to contrast with supervised learning.   Something we also saw in the last lecture 
was model-based reinforcement learning   versus model-free reinforcement learning. So in 
model-based reinforcement learning, that's where   we use an explicit conception of the next state 
and the reward given the current state and the   action and so, in particular, we explicitly say 
we have this transition model and we have this   reward function and we are estimating them, 
whereas in model-free reinforcement learning,   we are not doing that, we are just saying, “hey, 
we'll just do this Q-learning thing, for instance,   that we saw that did not explicitly put together 
a T or an R but was still able to learn a good   policy.” Now I want to emphasize something here: 
this is not the usage of the word model that we   would use, probably, in the rest of machine 
learning, certainly in supervised learning.   So this usage of the word model representing the 
transition model and the reward function is pretty   specific to reinforcement learning and so I'll 
just just point that out. Okay so model-based   reinforcement learning and model-free 
reinforcement learning are examples of   reinforcement learning and, in turn, Q-learning 
was an example of model-free reinforcement   learning. So we didn't instantiate T, we didn't 
instantiate R, you could consider estimating them   but that's not what happens in Q-learning, you 
don't explicitly do that and you are able to   get this estimate of the Q* function. So contrast 
Q-learning where we're trying to get an estimate Q   with the actual Q* function. So what the actual Q* 
function would be if we knew the transition model,   if we knew the reward, then we could say what 
is the expected reward of starting at a state   s, making an action a, and then making the best 
action ever after and we saw that we could get   this optimal policy by having this and so what 
we're doing with Q-learning is we're doing this   estimate Q instead so that we can estimate or 
come up with an idea of a best policy but it isn't   necessarily going to be the best one so long as 
Q is not exactly equal to Q* and, in some sense,   what we're trying to do is get as close to Q* as 
possible. And again, you want to contrast this,   also, with value iteration which we had discussed 
in the previous class. There, we assumed we knew   the transition model T, we assumed that we knew 
the reward function R, and we just wanted to find   the best plan, the best policy, going 
forward and so we didn't have to muck around,   we didn't have to try out different things and see 
how they worked, we could just start from the very   beginning by doing the best thing possible and 
that was value iteration. And then separately   from that, so that was just getting the exact Q*. 
Even before that, we talked about how to get V,   the value of a policy, and that could be 
still really useful. So Q* can let us get   the value of the optimal policy, but what 
if your friend comes along and says, “hey,   I've got another policy and it's just an easier 
policy and so if it's almost as good, maybe we   should just use my policy, maybe I've already 
got the equipment for it or something like that”   and she'd still like to evaluate what is the value 
of that policy and if there's a big difference,   then maybe you're going to keep going with 
the exact optimal policy and maybe if there's   a small difference, there might be other 
reasons that you prefer a different policy.   Okay so this situates what we've been doing 
the last couple of classes and this class,   now, we're going to see that what distinguishes 
reinforcement learning from supervised learning   is not just having a state transition model, one 
of these state machines, it's not having an MDP,   it's that you're actually doing this learning 
by interacting with the world and so we're going   to see an example today of supervised learning 
where you do, in fact, have a state based model   and so we are going to see that you can have these 
recursive properties, you can have sequences,   and not be exactly doing reinforcement. Okay 
so our motivating example today will be in   text predictions. So you see this all the 
time in your lives: if I go to compose an   email in Gmail these days, it will help me by 
suggesting some additional characters that I   could append after what I've written so far and 
I could just accept those and that will save me   seconds of my time. If you go into some kind of 
search engine, many search engines, this will   be true. Here's just an example from Wikipedia: 
if I start typing out what I want to look up,   then it will make some suggestions about what 
I might be looking up and some of those are   literally just completions of what I've written so 
far. Interestingly, some of them are other things,   but clearly the first one here is an autocomplete 
of the word autocomplete. Okay, now for us,   this is a convenience, it's nice to save a 
couple of seconds. It's actually a big deal   in some other individuals' lives. So famously, 
Stephen Hawking, especially towards the end   of his life, communicated by pressing a button 
and then eventually just moving a cheek muscle   and so for him, word prediction was a huge deal. 
If you could only type a few words at a time very,   very slowly, having the ability to automate a few 
words in advance can be huge and of course it's   not just Stephen Hawking, there are a number 
of people who face severe motor impairments:   Dawn Faizey-Webster, pictured here, 
amazingly completed, much more recently,   both undergraduate and graduate degrees with 
assistive technology and merely by blinking.   So I think this is a super cool and interesting 
area, we actually do some research in my group   on assistive technology for individuals 
who have these extreme motor impairments,   but in general, this is a reason to really care 
about text prediction and getting it right.   Okay, so we're gonna look at text prediction 
or at least a simplified version of it today.   And here, this is essentially supervised 
learning. What we'd like to do   is we'd like to say, “hey, I've seen some text 
that a person has written so far and I want to   predict the next text that they would write.” 
And we're going to do a very simplified version   of that: we're going to say, let's just for 
the moment, predict the next character and   you'll see in in the lab and other places how 
you might get beyond just that next character.   Okay so how do we get training data? Well luckily, 
nowadays, there's just a huge amount of text   online. You could just take all the text from 
Wikipedia, you can take all the text from your   favorite source, let's say we took some poetry 
text, and so here's an example of some text that   we might use in our training data and how would 
this help us train? Well, we can take each set of   texts that we might have seen and try to predict 
the next character. So in this training data,   one bit that we would have seen if somebody was 
typing this in order would be “w” and then we'd   have to predict the next character and the answer, 
the exact answer, the true label, would be “h”.   We also see, at some point in the course of 
writing this, that somebody has written “wh”   and then the next character which we 
would like to predict would be “a”.   Also in the course of writing this, 
somebody would have written “wha”   and then the next character that we would like to 
predict would be “t”. So we actually get quite a   lot of training data, in some sense, from this 
one sentence, from this one string of characters   and now we have to think a little bit harder. The 
devil's always in the details of these things.   We have to think a little bit harder about how 
does this exactly fit into our frameworks that   we've developed for supervised learning? Okay, 
well it looks like a classification problem:   I'm trying to predict a character and here I have 
26 different possible English language letters   and maybe an underscore for the space, so I'm, 
in some sense, trying to do classification with   27 classes at least as described here. Okay, but 
now how do I featurize this? This is a little bit   tricky. So we had this nice lecture way back in 
lecture three of “oh yeah, I get my data and I   have to turn it into features that I feed to my 
algorithm” and so this is essentially the question   that we're asking here: here's my data, how do I 
turn it into the features that I really feed to   my algorithm? Well one idea is why don't I just 
use the things that are in this column labeled   features? I mean that sounds so convenient. Am 
I done? So just use all the previous characters,   you could call this the context for 
the next character that comes along.   Okay well let's think about what do the algorithms 
that we've developed so far take? Well, they take   real values but that's not 
so bad, we know how to turn   characters into one hot encoding and then that's 
gonna be a set of real numbers so that's okay.   Maybe a little bit different, though, is that 
they take a fixed collection of real values,   a fixed dimension collection of real values. So 
far we've said that if I have some features to   my algorithm, whether it's logistic regression or 
neural nets or whatever is my favorite classifier,   that each of my feature vectors has to be in R^d. 
So we said R isn't so bad because I can turn these   into real values with one hot encoding or 
whatever my favorite encoding is, but the   fixed dimension d is challenging here because 
you can see that we change dimension quite a bit:   we have one character in the beginning, then we 
have two characters, and we have three characters   and so it seems like we're not really keeping 
that fixed and so we'll have to think about that. Okay so here's another idea: I just use the 
last character. So I predict an “h” from a   “w”, I predict the “a” from the “h”, I predict 
the “t” from the “a”, and you kind of see that   I'm probably losing some information from 
that, predicting what is the next character   after “happ”, you probably have a good idea that 
it's pretty likely to be “e”, somebody's probably   spelling “happen” or “happening” or something 
like that. But if I just predict after “p”,   I might have some pretty different predictions, 
it might not be quite the same so you can see   that having more characters potentially helps you 
out. Okay, but we can't have all the characters:   well, one, that's gonna be really hard to learn, 
you're not gonna have every possible sequence   of characters, but you have this issue of fixed 
dimension as well. And so let's, for the moment,   say that we're going to trade this off by looking 
at m characters where m is one or greater but   we're not quite sure what it is just yet and 
then we'll see if there's anything else we can do   that might help us with this fact that we don't 
know quite how much information we need to have.   Okay so we're gonna use the last 
m characters as our features   and we're still gonna have to keep digging a 
little bit deeper because there's still some   details that we haven't quite ironed 
out here and how this is going to work. Now something that'll, maybe, help us think about 
how to do these things is that we can express what   we're doing as a state machine and so what I 
mean by that is there's something inherently   recursive about the choices that 
we're making. So in our state machine,   suppose that we're trying to predict the next 
letter here given that we've so far seen “wha”.   So this context, these last three letters, so 
let's just say that our context here will be the   last three letters, that could be our state. So 
at some point, we have our state: at this point,   it's “wha”. Now maybe, the person on their phone 
or with their assistive technology or whatever,   types another letter, maybe it happened to be 
“t” and so what happens is we're going to want   to update the state. Now, the last three letters 
aren't “wha”, now the last three letters are “hat”   and so we can think of that as the input “t” 
coming in. This is the letter “t”, not the index   t in this particular case. And then we're updating 
the state to be “hat”. You can imagine this keeps   going, so now the person types a space and we want 
to update the last three letters to be “at[space]”   and so here our input is the space and 
we update our state to be “at[space]”.   And you can imagine that this just keeps going 
and you can keep doing this as you go along,   as you read this sentence. Okay so let's just 
double check that this fits everything that we   know about state machines. It's been a couple 
of lectures now, so let's just review what was   a state machine. Well in a state machine, we 
had to say what was the set of possible states.   So in here, the set of possible states should be 
all m characters in a particular order. So here m   is three, so any state that is three characters in 
a row is an acceptable state, that is a possible   state. What are the set of possible inputs? Well 
my inputs are going to be whatever is the next   character, so any character is an acceptable 
input among my vocabulary of characters.   Okay initial state, this one's a little bit 
tricky: how do I initialize my state? Well one   option is I could just initialize from the first 
three letters in this case. Another option is you   can imagine somebody starts up their assistive 
technology program or they start their phone and   you might want to also already start predicting 
things for them. Maybe they just want to type   “the” and that's a pretty easy prediction to make 
and so you'd like to start from the very beginning   and so for that reason, sometimes people like 
to include something called a start character.   So it's a special character that just indicates 
somebody's about to be typing and you would see   this in general, that somebody opens up a texting 
app or something like that, that's your indication   that they're about to be typing. And so if we did 
include this start character in our sentence (in   this case, I'm just using a carrot, it could be 
any character that's not what I'm already using),   then a natural thing that we might do is we might 
have our initial state just be m start characters.   And so what would this look like? Well 
we could start with our initial state   being, in this case, three start characters. 
We're gonna update with the first character we   see that's a start character and so at this 
point we're gonna make our first prediction   and that's fine, because we want to predict 
what is somebody gonna type without having seen   anything yet and this is what this represents. 
Okay next, we see a w and now we're saying,   “hey, we want to predict, given that we have 
nothing typed so far, except a single ‘w’.” Now we have our h that we've seen, and now we 
want to predict, given that we've seen nothing   so far except “wh” and then going forward from 
here, we're going to start having actually three   characters in the context that are not just 
the start character that we can predict from. Okay so the next part—so let's just fill 
in now, this t is the time step, hopefully   it's clear the difference between the uses of the 
character t and the time step t—now we're going to   talk about our transition function. So suppose 
that we have a particular state that we're in   and we make a new observation, we'll make a new 
input, we have a new input: how do we update the   state? Well hopefully it's straightforward 
to see at this point that, basically,   what we're doing is we're saying what were the 
last m characters and we're updating to the new   last time characters. So we're taking whatever 
the x_t value was and putting it at the front   or the back, I guess, depending on how 
you're thinking about this ordering   but we're appending it and then we're getting rid 
of the older character that we don't need anymore.   So we can clearly get this as a 
function from our existing state   and the new character that we've gotten. Okay, 
now we have to talk about what are our outputs.   So one output that we could have is just a pure 
prediction of what is the next letter, like I   see a “w” and I think the next letter is going to 
be an “e” but you can see that if you have a “w”,   maybe the next letter is an “e”, 
maybe it's an “h”, maybe it's an “a”,   and so a really useful thing here would be 
to have a probability distribution over the   different letters. And this is something we're 
familiar with from classification: think about   soft max for multi-class logistic regression 
that gives you a probability distribution   over the different classes that you're 
predicting and so this is something that we   know how to do and so instead of just a single 
letter, something we might output is a vector   and now I'm using abbreviations to fit everything 
in here, but what I mean is a vector of character   probabilities. So for each possible character, 
for “a”, for “b”, for “d”, for “e”, for “f”,   for the space, etc, we're going to say what's 
the probability that that is the next character?   So again, this is something that we'd 
really naturally do with our multi-class   logistic regression, using the soft max function 
instead of just the usual logistic sigmoid   and so at least, in theory, we know how to do 
this and we'll nail it down in just a second.   Okay and so our output function, g(s), will then 
be, exactly, a multi-class linear classifier.   So s, you can think of the state as being the 
features to that linear classifier. So what's   been useful about expressing this as a state 
machine is now we have a fixed dimensional   set of features that are going into our classifier 
and those are exactly the elements of s.   And so we can take those in as features and then 
we can output, again, this probability over all   of our classes, all the potential classes, 
this vector of character probabilities and so   this is our state machine, this is an almost 
totally well-defined state machine. I mean   we're still talking at a high level, we're 
gonna get to some equations in a moment,   but this is how we can fit things into a state 
machine and how state machine can let us come up   with a nice notion or a nice particular way 
of getting our features for this problem,   for specifying what are our features in 
each classification case that we have.   Now, just a little note here: it's a little bit 
different from how we've talked about x when we   were talking about classification problems 
before is, okay, well we have this input x,   but then we have x_1 and x_2 and x_3 and x_4 
as subscripts as we go along within a single   sentence, within a single string, and so we might 
want to collect everything in that string as   one string that we're feeding into this state 
machine and so we can say that this is our x^(1).   So the superscript is denoting that we have 
this full string and then if we do a subscript,   we're indexing individual characters within this 
string and so, of course, if we were doing some   kind of supervised learning, some kind of training 
on this, and we were trying to learn predictions   of these next characters, we wouldn't have 
just a single string, we'd have lots of strings   and so here's just an example of three of 
them that we might have and of course we'd   have so many more and we'd probably look at 
look at quite a lot of text data for this. Okay so this is still a high level description 
of this state machine. Let's do one more high   level description before we get into the exact 
equations that we might use to encode this. In   particular, let's go back to our function graph 
representations. So can we do a function graph to   express what's going on with this state machine? 
So it's going to be a little bit different from   how we've looked at state machines before: before 
we were looking at states and going in between   states, now we're going to say the function graph 
for how we apply f and g and f and g recursively. Okay so what is this function graph going 
to look like? Well first, I have my input.   Sorry first, I have my starting 
state, the initial state.   This is, in this case, a bunch of starting 
characters before I've gotten anything else   and now I have my first input, this is 
the first character I see. In this case,   it's a starting character: it says somebody opened 
up their text application and they're about to   start writing something. Okay so I'm going to put 
those in together into my transition function. Incidentally, what I'm describing here, in 
some sense, is just a state machine, like I'm   illustrating it with this particular application 
in predicting text like the next character,   but in some sense, this illustration, this 
function graph is a pretty generic state machine.   Okay so once I have these, once I have the input 
and my old state, this transition function updates   me to the new state and then that new state, 
I can use in my output function to get my   actual output. Now previously, when we talked 
about state machines, we called this output y,   but here we're going to call it p for prediction 
because we want to reserve y for the actual ground   truth label which is what we had done back when 
we were talking about classification and before   we had gotten into all this state machine stuff 
and now we have this unfortunate overloading of   the terminology, so let's use p for prediction, 
but it's the output of our state machine and then   y will reserve for what is the true label here 
which is the thing the person actually wrote next.   Okay so once I have this new state, I 
can put it, again, into my state machine   with my next input. Those together will go 
into my transition function which will output   the next state and then that will have some 
prediction for the letters I've seen so far:   what is the letter that I want to say is next? 
So if the character that I want to say is next   and then this goes on and on. As you can 
imagine, this just going on-off the page,   hence the dot, dot, dot. That just means 
that we recurse again and again and again. Okay and so here, a couple of numbers that are 
going to come up in our examples here. So we said   m is going to be the number of characters in 
our context. So this is the size of s right now,   it's the number of characters we're looking 
at when we want to predict the next character.   And let's say v is the number of characters in 
our alphabet. So for the moment it's 28 because   we have the starting character and we have our 
26 English language characters and we have the   underscore for space. Now in order to write 
down exact equations for what's going on here,   we're going to choose a 
particularly simple version of this. Okay so in particular, let's suppose we have 
basically the simplest alphabet you could have:   it's just 0 and 1. So the number of characters 
in this alphabet is just two, it's gonna make our   lives really easy. In particular, one thing that's 
really nice about this alphabet is that it takes   exactly one character or one element to one hot 
encoding: I can either say it's zero or it's one.   Once we go beyond that, we have to be a little 
bit more careful. And let's keep m = 3. Let's   say we're still trying to predict the next 
character from our last three characters. Okay so in this case, our state is going to have 
size m by 1 because we're predicting from the   last m characters and each character we can one 
hot encode with just a single vector element.   So for the moment, it's just n by 1. 
But question about notation please. Yes.   Is x_n the same as x^(n)? Oh great 
questions. Yes, so no they're not. So   let me just go back for a second to illustrate 
this. So x^(n), so the superscript denotes which   string we're reading in. So our x^(1) here is 
“^what happens to a dream deferred”, x^(2) is   “^if you can keep your head when all about you”, 
x^(3) is “^you may write me down in history”.   Now x_n would tell us which character we 
were on in this string. So if I had x^(1)_1,   that would be the start character, if I had 
subscript two that would be w, if I had subscript   three that would be h. So probably a better way 
to write this and maybe I can update this in the   offline lecture notes is to put a 
superscript one on this x_t in the   table because we really are taking from 
exactly this first string here. Great. Yeah and in some sense, you can think of this 
state machine as just going through a particular   string and then we're just going to do it again 
with another string and again with another string. Okay great. So let's see where we were. Okay. So the state 
here, in this case, is the number of previous   characters because we can encode each character 
with just a single vector entry. Our x is also   just a single character and here we can code a 
character with a single entry: so it's 1 by 1.   Our p here, again, is going to be a set of 
probabilities over the characters in our alphabet.   So we're going to say “what's the probability 
of a? What's probability b? What’s probability   c?” Here, we're going to say “what's the 
probability of 0? What's the probability of 1?” So in general, it'll be v by 1 because we're 
going to have a probability for each of those.  Okay. So now what we're gonna do is we're gonna 
write an actual equation for f and for g. So   remember we said f was the transition function 
that pushes out the last character and brings   in the new character from x, from our observation 
x, and g is the thing that takes our states in as   features and then outputs our 
classification, our probabilistic   classification for the next character. Okay 
so now that we're thinking of s as m by 1,   I want you to think of it as the latest character 
comes first. So if I had just written “wha”   (so a is the latest thing and so that'll 
be on top and we have h then we have w),   so if that's the case, let's see how 
we're going to construct that from our   x and from our s_(t - 1). Well it would be nice if 
we could do it linearly, that's about the simplest   type of update that we can do so we're going 
to try to do it linearly and see how things go. So remember, we just said s has size m by 1. So 
here, that's 3 by 1 because we're saying that   we're just using the last three characters. 
x_t has size 1 by 1 because it's just the   next character that comes along and so my first 
question for you and this is for the chat is:   what is this? We're going to be 
pre-multiplying something, this big...   Darn, well. So I was going to ask you and maybe 
you can still think about it: we're going to   be pre-multiplying this x_t by something, by a 
matrix, and we want to think what would be the   size of that matrix if we pre-multiplied it to get 
s_t? I sort of just revealed it, but I think you   can still say in chat what is going to be the size 
of that matrix? What is that going to look like?   Great yeah. Okay you guys have got it, fantastic: 
it's 3 by 1 because you have to get a 3 by 1 at   the end so multiply a 3 by 1 by a 1 by 1. Okay so 
now, and I see some of you are already thinking   about this, let's ask what this question mark 
is going to be. And now I think some of you are   getting the idea exactly right but for the moment 
let's assume that the latest character is on top.   So in particular, I basically want 
to move x_t into the top position   in my s_t vector and then I'm going 
to have the second latest character   below it and the second latest character 
below it. So then my question for you is:   what is the, we see now, the 3 by 1 matrix that 
we're going to pre-multiply x_t by to get that? Great. Looking good. Okay. So got some 
nice answers here: [1, 0, 0]. So now,   originally, some folks were thinking [0, 0, 
1]. That would be potentially totally fine,   it's just how you define your state and so if, 
in this particular case, we're defining our state   as the latest character and then the second latest 
character and then the third latest character. So   if I had written “wha”, we would have an “a” and 
then an “h” and then a “w”, then this is what we   would get. If you define your state some other 
way, that's totally fine. It's just this is the   particular way we're doing it right now. Okay 
and, of course, here our alphabet is {0, 1},   but that's why we would do this. Okay, 
so what we're doing here—so let's just   recap—so what we've done so far is we've 
said our state is going to have x_t,   the last character that we just observed, put 
up at the top, that's going to be its value   in the top position. And now what 
we'd like to do is we'd like to have   its next value be the first value in s_(t - 1) and 
its third value be the second value in s_(t - 1).   So now we have to think about how to 
accomplish that with this remaining   big question mark and I'm just gonna show you and 
then you want to think through why this is true. Great and we actually had some people putting 
in the chat which is awesome. Okay but the most   important thing is just to think through what's 
going on here. So in the first row here, we're   multiplying this by s_(t - 1) and we're getting 0, 
so this has no effect on the first element of s_t,   we're just adding 0. In the second row, we're 
picking out the first element of s_(t - 1)   but this is the second row so we're 
putting it into the second element of s_t In the third row, we're picking 
out the second element of s_(t - 1)   but this is the third row of s_t so we're 
putting this into the third row of s_t. And so the thing to observe here is that 
we're just shifting the elements of s_(t - 1).   Again, you could choose your state to be 
represented in a different order. Here,   this is just a particular order that we've chosen 
for the moment but we're gonna get this shift   and then what we're gonna get is that we're just 
putting x into that first element, we're putting   the first two elements of s_(t - 1) into 
the second, the last two elements of s_t   and then we're throwing away the last 
element of s_(t - 1), it has no effect here,   it's not coming in at all. And that makes sense 
because that is too far outside the context now,   that's like the fourth character so we don't 
need it anymore, so we're throwing it away.   This is just a way of saying exactly what we 
said on the previous slide but with an equation.   So we're just expressing that same idea with an 
equation that we're just shifting the context,   we're shifting what are the 
last three letters here. Okay and then we also want to specify g 
with an equation. Now remember the idea of g   is that now we have our features, our s_t. 
We have specified: here's the set of features   that we're going to use for this problem. Our 
features are going to be s_t. It's going to be   what are the last characters. If we want to do 
a classification with those last characters,   we want to ask ourselves how would we do 
a classification where we have a linear   classifier and we return the probabilities 
of the different characters in the alphabet?   This should just be applying things 
from previously in the course but let's   briefly review them. Okay so we're going 
to call our parameters W^o and W^o_0,   where the o stands for observation because 
this is the output of the observation model.   Now we're going to have some function, let's call 
it f_2. WWe haven't defined an f_1 yet, but this   is kind of anticipating that we will define an 
f_1 shortly, so let's just go with it for now.   Let's call it f_2. And hopefully this looks 
mostly familiar with a few small differences.   Like, for instance, if we were doing two 
class logistic regression, we could just   have regular logistic regression: we have the 
probability of one class pop out. In that case,   we could have this be 1 by 3 and 
this be 1 by 1. That's what we did   when we first introduced logistic 
regression way back in the day. But in general, if we only have two options, 
we can't specify all those options with just   a single probability and so, in general, we might 
have, instead, f_2 describing not just two-class   logistic regression, not just a logistic 
sigmoid, but a v-class logistic regression. And so now, if we did something like a soft max, 
we might have v different outputs. Now notably,   here we have an alphabet of size 2 and so 2 
is an awkward stage where you could do either:   you could either do vanilla logistic regression 
where you have the probability of one outcome   and the other outcome is 1 - p or you could do the 
more general soft max function and then you would   have the probability of one class and the other 
and it just so happens they add to 1. And so it's   just worth noting that that's why we're seeing 
that, even in v class logistic regression, you   might have v by 1: that's when we're naming, we're 
writing out explicitly, all the probabilities. Okay now, in general... So in this case, f_2 
is going to be this general soft max function:   we're going to be taking this v by 1 vector 
that we get out here from this whole thing,   we're going to be putting into soft max, 
and we're getting out these v probabilities. And now, in general, s_t doesn't 
have to just be three long:   it could be m long and so 
we could replace m there. Okay so what I want to emphasize at this point is 
that this, in some sense, isn't too much that's   new, we're just combining a lot of ideas that 
we've seen elsewhere in the class. So the way   that we came up with our features was by running 
our state machine forward, so that's a review of   state machines, an application of state machines 
to this problem. Once we had our features,   the last three characters, we just ran 
vanilla logistic regression or soft max   v class logistic regression. So these are ideas 
that we've encountered previously in the class,   there's something new in that we're 
combining them but we haven't really   defined something totally new at this 
point. But at this point we can also say,   well... Oh and actually before I go on, I do 
want to point out one thing that actually is   different here, not substantively, but it looks 
different. So something that you're used to from   our logistic regression lecture, our neural nets 
lecture, every lecture where we've done some form   of logistic regression or softmax or some kind 
of classification, we've always had a transpose   after our first parameter, the parameter 
that we're multiplying by the feature vector.   And here you'll notice that there's 
no transpose, so that is different.   It's not substantively different: you could always 
just have described your other vector, whatever   you were using as you could call the vector the 
transpose or vice versa. The reason—it's just a   convention—but the reason that we're changing the 
convention here is that this is the convention for   recurrent neural nets and that is what we're 
building up to. And so it's just a convention:   it's not meaningful, but it's worth looking out 
for that this is a change that has happened.   And in particular, it highlights too that, in 
reality, we're going through all these topics   pretty quickly in the class but there's a 
whole set of people in whole communities   that work on reinforcement learning, there's a 
whole set of people and communities that work   on recurrent neural nets, there's a whole 
set of people in communities that work on   convolutional neural nets and things like that, 
and they develop these these ways of talking   about things and these standards and this is 
just one thing that is different across them.   Also, it's just worth noting that, in general, 
when you actually read papers about these things,   you can't just assume—unfortunately, I wish—but 
you can't just assume that the notational choices   that we've made are going to be the ones 
in those papers: it may well be different   and it's just worth keeping an eye out for what 
may change. So I'm just highlighting one of the   things that has changed here. Okay, so here I want 
to point out that there's a familiar pattern here   and this is no different where we start 
by choosing how to predict a label given   a set of features and parameters. This is like, 
literally, I'm just copying the things that I said   in lecture eight again because it was a familiar 
pattern then, it's a familiar pattern now.   So here we've chosen how to predict a label 
given a set of features and parameters. We said   given these inputs, people writing text, we 
have a way to predict a label which, here,   is what is going to be the next character that we 
see and we've said our parameters: our parameters   are these W’s, W^o’s. Okay, now we have to choose 
a loss between our guess and the actual label and   then the way that we actually learn anything is we 
have to try to find some parameters. Typically we   might do that by trying to minimize the training 
loss. So we did this for logistic regression,   we did this for linear regression, we did this 
for vanilla neural nets for classification and   regression, we did this for convolutional 
neural nets. Basically everything we've done   in supervised learning has fit into this familiar 
pattern and so now we have another example of   that familiar pattern: we have a way to predict a 
label, so that's our p_t. For every single label,   we have this p_t. We need to choose a loss between 
our guess and our actual label and then finally   we would do something like gradient descent or 
stochastic gradient descent or something on that   loss so long as everything were differentiable 
and if they weren't differentiable, then we'd   have to think about something else. Okay, so what 
can we do here? So we have to choose the loss.   Now if I look at a particular superscript i, 
so that's, again, one string of text like a   line from a poem and a particular subscript 
t, so that's just a particular input letter,   I can say I made some prediction for this letter 
and there was some actual letter that somebody   typed, how do those compare? And again, 
because our predictions are probabilistic,   a really natural loss here for this comparison 
would be something like negative log likelihood.   In particular, the version of negative log 
likelihood where you can have multi-class   classification. Okay, so that's for one 
particular character in one particular string   and here p_i is the output, just to 
emphasize the dependence on the parameters W,   is this output for a particular input x^(i). 
So that was the thing we were reading in   and these parameters W. So we only 
get p_i by having the parameters W. Okay so that's a particular character in the 
string but, of course, we want to say, “well   remember, we can predict this first character, we 
can predict the second character, we can take the   third character and so we want to talk about how 
we do over the whole string?” And so for that,   we can sum up the losses over the whole string 
and so here, n^(i) is just going to be the length   of this i-th string, so like the length of “what 
happens to a dream deferred,” what's the number   of characters there, and so we're going to go 
over each of those characters, say “how do we do   predicting them” and then report that loss and so 
we can say that L_elt is like the element loss for   a particular character and L_seq was for the whole 
sequence, what's the loss of the entire sequence.   And then finally we want to say, “okay, 
well that's one particular sequence,   that's one particular line in a poetry book or 
what have you.” I want to know what's the loss   over all the sequences. Typically I'll read 
a lot of different text strings and I want to   train on a lot of text strings and so let's say 
we have q of those text strings and then finally   that gives us an objective that depends via the 
predictions on the parameters W, W^o, and W^o_0. Okay so these are slightly different 
than our usages in the past,   so here we're saying that i ranges from 1 to q, q 
is the number of sequences. We're saying that the   number of characters per sequence can change. 
I mean certainly that's true: if you look at   even just the English text sequences that we 
saw on a previous page, but in general like   sentences change length so we want to let them 
have different lengths and so we'll call the   length of that sequence n^(i). There's something 
that's a little bit awkward here: there's nothing   that exactly corresponds to our notion of 
number of data points from before because   we have multiple observations within a sequence 
and we have multiple sequences and so there's no   strict thing that's exactly like the n that we 
had from before. These both contribute to that. Okay so now, we've chosen how to predict our label   (we did that with this basically 
multi-class logistic regression),   we chose a loss between our guess and our actual 
label and then finally we would choose parameters   by trying to minimize the training loss and so if 
we made all the choices that we've talked about   here we could totally just use gradient descent or 
stochastic gradient descent or anything like that   because everything is differentiable and so we 
could go ahead with that and this would be just a   typical problem like the ones we've seen 
before, just a little bit more sequential. Now, an observation that we can make is, well, 
this is how we built up to neural nets: we started   with a typical logistic regression problem or 
a typical regression problem but basically just   a linear classifier and then we said, “hey, 
wouldn't it be nice if we took the features   and we actually learned the features instead of 
just putting in different pre-composed features.”   And so we might ask ourselves: could 
we do the same thing here? So here,   we decided, we said, “how do we construct 
features?” Well we take in our inputs,   we take in our existing features and we 
combine them in this way. But maybe we   don't have to combine them in just this way, 
maybe we could combine them in a different way. So something that we did, again, way back 
in neural nets is that we said, “hey,   let's try out different combinations and let's 
let the weights in those combinations be learned.”   Something that we did in convolutional neural 
nets was we said, “hey, here are filters,   here are some useful filters that you might be 
interested in with zeros and ones in them and   then what happens if we learn the filters? What 
happens if we put in different weights in those   filters and then we learn, again, essentially 
learn the features that we're going to be using?”   And so this is exactly the same story we can 
again say, “okay, we've defined a particular   linear classification problem. You have a bunch 
of features, those are just the last few letters,   and you have an output which is the linear 
classification based on those features.   But maybe I want to learn these features. I mean, 
in particular, one of the things we said was,   well, gosh I don't know if I want exactly m 
of the last few letters or a different number   I'm not sure exactly how much information that 
I want to keep for my problem, what's the right   thing to do there? Also, maybe, this isn't the 
best way to keep that information. Maybe I don't   want to just keep it as here's just an enumeration 
of the last three features, maybe I want to do   something more with that, maybe I could get better 
performance if I combine them in a different way.   I won't know without trying so let's do it and so, 
in particular, what we're going to do now, again   just like we did for neural nets, just like we did 
for convolutional neural nets, is we will, instead   of having this strict set of zeros and ones, will 
allow these to be parameters. Another question. Yes in your J(W^o, W^o_0), 
what's the meaning of q here?   Yeah so again, the thing that's a little tricky 
here is that we have nothing exactly like number   of data points from before. So in some sense, if 
you're reading a sentence on “what happened to a   dream deferred” you kind of get a lot of data 
points for training from that because you want   to predict after “w”, you want to predict after 
“wh”, you want to predict after “wha” and so on.   So each one of those is kind of like a data 
point. So those are going to be indexed by t.   Now each time you have a sentence or a string, 
that's going to have a bunch of these t's within   it, so we'll index those by i. So each string is 
like an i here. Now the number of strings that   you have that you're using for learning 
has to be some number, let's call it q,   and the number of characters within the string 
has to have some number and that could change   from string to string, so let's call it n^(i), 
and the reason you might think of neither of   these as being exactly the same as n is because 
they're both determining the amount of training   data that we have here and so previously we 
called the number of training data points “n” and   here they're both contributing to the total number 
of trading data points. So you could think of q   as being like the n that we had from before: 
it's kind of like the number of data points,   but it's really the number of strings, it's really 
the number of sequences, and then n^(i) is the   number of points within the number of time steps, 
within a particular sequence indexed by i. Great,   okay cool. So what we've just done is, again, 
hopefully, somewhat familiar: this idea that   we had these predetermined features and then we 
decided instead of just saying we're going to   have this particular thing that we do with these 
features, that we can actually learn the features.   What's different about it is how we're doing that: 
we're not just taking our x's and then putting a   bunch of weights in front of them and then using 
that to find the features. That would be a more   vanilla neural net that we've seen from before. 
Here what we're doing is we're learning the   features from the new x and the old set of 
features, so this is much more sequential than   anything we've done before: we're keeping those 
old features around and doing something with them.   Also something that's interesting here is 
that, in some sense, the inputs to our feature   construction, the x_t and s_(t - 1), together all 
add up to the same size as our features, our s_t.   And that's because we have to keep this going, 
that the s_(t - 1) has to be the same dimension   as s_t, has to be the same dimension as s_(t + 1) 
and so on and so forth. x_t doesn't technically   have to exactly combine in this way, but the s_(t 
- 1) certainly have to have the same dimension   over and over again, the s_t's, so it's a little 
bit different than things we've done before too.   But essentially here what we're doing is we're 
saying, “hey, there was a useful setting of these   W's, we just identified it. It told us the context 
but maybe it's not the most useful setting, maybe   we could find another useful setting by setting 
these to be W's, by setting these to be parameters   and then learning them together with everything 
else” and this is, again, the story that we had   for neural nets: we thought about learning our 
features and so we introduced parameters there,   we talked about this with convolutional neural 
net, so now we're just doing it for a different   type of data, a different type of 
thing. Now this sequential data with   recurrent neural nets which is what 
we're essentially defining now. Okay so this superscript sx here is just to denote 
that these are the weights that go from x_t to   s_t. So it's just telling you which set of weights 
because we've introduced two sets of weights at   this point. So likewise, this ss is telling you 
that these are the weights that go from s_(t - 1)   to s_t. Now if you think back to what we 
did before when we were doing neural nets,   there are a couple things that are a little bit 
different here. We didn't just combine the inputs   to get features, we can linearly combine them 
together with a potential offset. Same thing   with our filters and convolutional neural nets: we 
said that there could be these weights but there   could also be an offset. So let's just introduce 
the offset and so it's going to have this ss   superscript in this case again and then just a 0, 
a subscript 0, to indicate that it's the offset.   That's just notation, it could really be 
anything. It's just a different, it's a   new set of parameters that are the offset. Now the 
other thing that we did in all these cases that we   haven't yet done here is we had an activation 
function. So in particular, we would take   our inputs, we do this linear combination. So here 
we have a linear combination. This is a particular   linear combination, what we might have called a 
unit before, for instance, and then we applied   an activation function. And so now, let's do the 
same thing, let's apply an activation function.   This will let us get interesting nonlinear things 
going on and so that we can really learn what's   going on in our data. We saw the power of that 
back when we talked about neural nets. We saw how   when you have these non-linearities, you can get 
these really interesting classification boundaries   and useful classification boundaries and so we'll 
do the same thing here. Now it's worth noting that   the way we apply this is component wise. These 
are exactly the same as activation functions,   essentially, that we had before.s So you 
have this 3 by 1 vector that's coming in,   you apply the f_1 at each of those elements and 
you get a 3 by 1 vector coming out. I think it's   not always obvious how these are being applied, so 
it's always worth asking, if you have a particular   function, is it being applied component-wise 
or not? And so in this particular case, when we   think of these activation functions that are being 
applied to each of you might think of the units. Okay now, this of course, is this setup here is 
specific to our choice of m = 3 as the size of the   state. It'll help us to write this in more general 
notation for the cases where it's not just m = 3,   so let's write these as matrices. Rather than all 
the elements in those particular matrices, let's   just write the matrices and so here, we have just 
our capital W^(sx) now, that's just exactly the   matrix that we had before here. The W^(ss) is the 
matrix we had before, W^(ss)_0 is now this offset   that we have. So this is just 
exactly the same thing we had   right before I changed it, so 
this becomes this, but again   two things: so one, now we're just 
writing it with a single symbol, but two,   this will be more general because this will hold 
for things where we're not just setting m = 3. Okay so in particular, we can expand our 
alphabet now to say have l characters,   our state could be the last c characters. We have 
to be a little bit careful like what is the size   of the state vector? Well, it's gonna change here, 
so x_t, if we have an alphabet of l characters,   we might be able to encode a single 
character in one hot encoding with, maybe,   let's say, l elements, that's one option. So if 
we did that, then our x_t would become l by 1   and our s_t should be the c previous characters 
and so, in particular, if we say that we encode   each one of those with one hot encoding, then 
it might be that m = c * l here. Depends how   you encode it, but that would be true. Now of 
course, this was before we learned our W's.   Once we incorporate these W's and we don't just 
set them to be zeros and ones, we allow them to   be learned. It could be that the state is not 
just the last c characters. It could be that   the state is really something much more general. 
It could be that you choose m to be something   that isn't just m = c * l, it's just a general m 
that encodes basically how many features you want   and so this is a useful way to think about how we 
arrived at the state, just the same way we thought   about filters we looked at some examples of useful 
filters, but the reality is you actually learn   the filters because you're learning the 
parameters. It's the same thing here:   this is a useful way to think about the state 
but once you put in those W's, you're actually   going to learn a state. It's not going to 
have this interpretation necessarily anymore. So that's, again, just a pure 
example of this broader idea. Okay so now, the set of parameters that we said, 
our familiar pattern, we choose how to predict a   label given features and parameters. So now we've 
chosen how to predict a label given features and   parameters. We have a new, updated version of 
how we're going to make a prediction and what's   changed from the previous time we talked about 
this familiar pattern mere minutes ago is that now   these aren't our only parameters, we have these 
parameters as well, we have the parameters that   are part of the state construction as well. And so 
now, we can just update this little sidebar about   loss over here and we'll note that the prediction 
depends not just on the W^o’s, but in fact,   all the W's. So maybe we could write it in this 
way but the thing that we're trying to emphasize   here is that it depends on all the parameters and 
we have to learn all the parameters, we have to   encode all the parameters here. So the loss our 
objective depends on all of the parameters and so   now when we try to minimize the training loss, 
we're going to look at all those parameters. And so here, what we have described 
now, is a recurrent neural network. So when we just had the state machine and we had 
our logistic regression, we're getting there,   but here, now, where we're learning these W's 
as well. We have a recurrent neural network,   so it's like the neural networks we saw 
before, but there is this recurrence   because we have this iterative procedure of 
going back to the state. Okay. So let's draw this   in a way that is reminiscent, at 
least a little bit more reminiscent,   of the way we've drawn some of our neural networks 
before. So again, the example that we saw,   just a motivating example, was that we have this 
alphabet of l characters. We might hope that the   state could represent something like the last c 
characters, but we might learn something else,   we'll see. We do, in general, have these 
updates for the state and for the prediction   and now we have both an f_1 and f_2 so it makes 
more sense why we called that second thing f_2   and this is what we're calling or when our 
output is from a recurrent neural network,   it's that it came from these equations, 
these recursive equations on s_t and p_t.   Okay, so let's make this drawing. So before we 
add the general, very general, state space model,   now let's look specifically at this recurrent 
neural network model. So here, now, we have our   starting state or x_1 and they go into some kind 
of summation, this kind of linear combination and   get out what you might call a pre-activation. So 
here we're defining this input to f_1 to be z^1_t. So this is what we used to do in neural 
networks with the hidden unit and the inputs:   we would take all the inputs, we would do some 
kind of summations although we had we had more   units depicted in the picture, but that's still 
going on here, it's just that z is a vector,   a vector of the same size as s. Okay and 
so we can do the same thing over here:   this can be, also, an activation. This 
is z_t^2. So here, the superscript   1 just means that we're in the first layer. We're 
gonna see that there's a conception of layers   and the superscript two just means we're 
in the second layer. You can think of them   as being like the f_1 and f_2. So the z 
gets a superscript from the f subscript. Okay so once we have these activations, we put 
them into our activation or pre-activations.   We put them into our activation function, get 
our activations. Those are exactly the states.   Then those, themselves, go into a linear 
combination to get the pre-activations for f_2.   We put them through the activation function and 
finally we get our output for our final activation   p_1. And then, just like before, we recurse on 
this. So just like we saw in the state space model   before, our old or sorry our new state becomes the 
state that we just got, it becomes our new input,   it becomes the new state that's going 
into the next round and so we started   all over again except now we've incremented the 
subscripts so we're one step forward in time.   We get our outputs, we get our new state, we 
get our outputs and we just keep doing this. Okay now I want to highlight how this is very 
similar and just a slightly different drawing to   the regular vanilla neural nets that we described 
in lecture six. So this is like the hidden layer   in our regular vanilla neural 
nets: we have some inputs.   Before they were our x's, now they are s_0 and 
x_1 or, in general, our state and our new input. We have some kind of sum that we're doing.   When we drew this for neural nets, we separated 
into a bunch of units. You could still think of   there being a bunch of units here, it's just 
that we've put them all in a vector together. Now each of those units is going 
to give us a pre-activation.   So it's worth noting this z_1^1 
has the size of the state. And now what we're going to do with 
these preactivations is we're going   to turn them through an activation 
function. It could be any of our   familiar sigmoids, that would be a typical 
choice and turn them into activations. And those are our states now. Okay so this bit behaves like our hidden 
layer in our two-layer vanilla neural network. And this bit behaves like our output 
layer. So now we take our hidden units,   these s_1’s, we do some kind of summation thing,   we get our pre-activations, we put them through 
an activation function and we get out our p_1.   It depends what you're doing whether f_2 
is applied component wise. In the example   that we gave, we're interested in doing this 
multi-class classification it would not be it   would be a soft max and so it would be across all 
of the components so it depends what you're doing,   but that's just like for regular neural 
nets, that's not some new thing here:   it's actually just like you do for even a straight 
linear classifier, so here it's a similar deal.   Okay but what's different is that now the 
state keeps changing. So instead of just   having that bit there, we keep going. We have 
this thing that just keeps going off the slide. Okay so let's make another way that we could 
represent this. So something that's a little bit   annoying, potentially, about this representation 
that we have right here is that it just keeps   going, like it’s going off on the slide off 
to the left and it's just it's ad infinitum.   Is there a way that we could draw this in a finite 
way? Well yeah we could just wrap it in on itself.   So here what was happening was we were taking 
all of these subscripts and incrementing by   them by one each time and we would go forward and 
forward and we just have those appear right next   to each other every time so we increment this to 
two, to time step three, to time step four, and   that's how we would build this long piece of tape 
that's describing our recurrent neural network.   And so what we can instead do is have a loop to 
represent this. So this is exactly the same thing   that we see on the left here but on the 
right, we've gotten rid of all the subscripts:   they're just going to naturally increment 
each time we go through this picture. And then the other big thing is that we're 
seeing explicitly that the state that we come   up with becomes the input to the next round. 
That's that big line there at the bottom. So this is just another way 
of representing the exact   same recurrent neural network 
but in this finite diagram. Instead of imagining that it goes on forever but 
just off the slides. And again... Oh and we will   call this the folded or rolled version because 
if you think about, maybe you wrote the really,   really long one on like a long piece of tape, and 
if you folded it up, then that would be a way that   you could see it going back into itself every time 
and so this is the folded or the rolled version   and this is the unfolded or the unrolled version 
where we just repeat it again and again and again. Okay so let's just, again, briefly compare 
to other things we've seen in this class,   feed forward neural networks. Especially this 
folded, rolled version looks very, very similar   to our feedforward neural networks: we have 
essentially something like the hidden layer,   we have something like the output layer, and 
of course, the big difference is that we are   not feeding forward anymore. So if you recall 
from our lecture on feedforward neural networks   back in lecture six, we said that the nature of 
a feedforward neural network is that everything   always goes to the inputs to the outputs. But 
here, we have this arrow that's going against   this time ordering, that you can actually 
go back, you can now put that back in as an   input again and so this isn't strictly 
feeding forward from that perspective. This is also different from convolutional neural 
nets, although you can think of the reason for   us doing it as being quite similar. So they're 
both designed, essentially, for a very particular   type of data, they're all just examples of 
neural nets. Fundamentally we're all doing   this similar deal with building up units on top of 
units, but they're constrained in particular ways,   convolutional neural nets and recurrent neural 
nets. So with convolutional neural nets,   we designed them essentially with pixels 
in mind. We said, “hey, we have pictures   and these pictures are composed of pixels.” Then 
there are natural things that we're interested in   there. As we said, we want shift in variants, 
we want these like spatiotemporal properties   that we can encode with things like filters, and 
that was really natural for vision type data.   And here what's really natural about text 
data, but also other types of data, is that   it's sequential: you have it building up in order. 
The way that you type is that you type the first   letters and then you type more letters and then 
eventually you get to the end. It is unusual for   people to type from the outside of the sentence 
in or some other order, you almost always type   from the beginning to the end. And so this is 
representing and respecting that kind of structure   in the data and then trying to learn 
from it and so we're seeing that in these   recurrent neural networks that they're designed 
for a certain type of structure in data.   Just like feed forward neural networks, 
just like convolutional neural networks,   they can be optimized so long as you choose 
all of your activation functions appropriately,   that everything is basically differentiable, you 
can optimize with gradient descent, stochastic   gradient descent and all these ideas. And just 
like convolutional neural networks presented a   bit of a pain for actually doing that optimization 
and actually, really, this is all chapter seven   with the feed forward neural networks, there 
are still, equally, like some pains to actually   using these in practice. So it's a nice idea that 
you're just gonna specify this, you're gonna go   and choose your loss and you're gonna optimize it 
and then, in practice, you have to do some tricks   to really get that to work well but that's the 
overarching idea here. It's also worth noting in   all these things that essentially what we did was 
we said, “hey, we have this existing idea of how   to make a prediction and we noticed something that 
wasn't ideal about it for a certain type of data,   like maybe we could make some extension that would 
make it better.” In convolutional neural networks,   maybe we could use these filter ideas and that 
would help respect the structure of vision data.   Maybe in recurrent neural networks, we could take 
advantage of the fact that we have this sequential   nature to our data and so if you come along with 
some new data, this is not the end of the story.   I mean, certainly, for both convolutional 
networks and recurrent neural networks,   we see a lot of developments beyond these very 
basic architectures that we've covered in class   that help deal with other things that we know 
about the data and that's very much something   that you can do and hopefully you feel that that 
is something that that is within your power,   that you could make further improvements on 
these. Like these are really just the start.   Okay and then I also want to just again say this 
is a contrast with reinforcement learning. So here   the data is just sequential. We're not treating 
our own decisions as changing the state of the   system, not that you couldn't, but for the moment 
that's not what we're doing. We're just saying,   “hey, we trained on a bunch of data and then we 
can predict on some other data. The text and the   training data was written before we got here: we 
just scraped a bunch of poems from the internet or   scraped some text from the internet and we're just 
going to predict the next character on the phone.”   This is very typical supervised learning. Again, 
it's not to say that you can't combine these   ideas. People combine things like convolutional 
neural nets and recurrent neural nets,   they combine reinforcement learning with various 
neural nets, but as we've been showing them here,   that's not what we've been doing. We've just 
been saying here's a supervised learning problem. Okay so finally, we have to decide how to 
actually do that stochastic gradient descent   or that gradient descent. And just as we did 
for CNNs, we're just going to get the various   tastes, hopefully just to illustrate what is 
challenging here. And the real reason any of   this is challenging (I mean to some extent all 
of these problems are solved by the fact that   we have modern automatic differentiation tools and 
you can use them), but it's worth keeping in mind   here why can't we just immediately use what we 
used in neural nets? Why can't we just use exactly   vanilla back propagation in all of these cases? 
In the case of convolutional neural networks, it's   because we had a lot of weights that were set to 
be the same, right? So we have this weight sharing   where, when you apply the filter here 
and when you apply the filter here,   you were using the same set of weights and so 
that's what created this slightly different look   to the back propagation and it's the same 
thing here that every time you increment   one as you go forward in time, you're applying the 
same set of weights and so again there's a form of   weight sharing that's going on and that's going to 
make the back propagation just a little bit more   involved basically. And so if you get anything 
from this, that's really just the key point.   It's easy to get lost in the math and I just don't 
think that's as important. What's really important   is to understand this high level idea that you 
form a way to make predictions, it depends on   some parameters, you form a loss and then you try 
to optimize that loss. We're doing that here with   something like gradient descent or stochastic 
gradient descent and that will just involve   taking derivatives and it could get messy just 
because of these dependencies. That's the main   idea and then everything beyond this is like icing 
on the cake and just getting into the details.   Okay, so let's say we chose to do 
stochastic gradient descent here.   So we would choose our index i uniformly at random 
from the data indices and, in particular here,   that's the sequence we might choose 
a random sequence from our sequences. Okay so once we have a sequence, we 
can look at the loss in that sequence.   In particular, we can run our forward pass. That 
is to say, you just run the recurrent neural net   as we've described it and you can get the loss. 
In particular, you can get the prediction which   will give you the loss together with the label. 
And now, of course, what we'd like to do is we'd   like to use the usual SGD story: we'd like to 
optimize with respect to our parameters. Now   of course, we'd like to do that with all 
the parameters. I'm just going to choose,   almost arbitrarily, a particular parameter 
here, but you would do this with, of course,   each of your parameters that you're trying to 
learn. This is just one of our set of parameters. Okay, so W^(sx). We're interested in learning 
this parameter, just like the other ones, and   so we take these derivatives as part of SGD or GD, 
but here in SGD, we'll do this for each of these   sequences, we'll take these derivatives and we'll 
use our usual SGD updates. And so, in some sense,   we've reduced this problem to: we need to get 
these derivatives. If we know everything about   SGD already, stochastic gradient descent, 
then we just need to get these derivatives.   Okay, well remember, the sequence loss is the 
sum over the element losses for each of the t's,   so each of the characters in our example and so we 
can just take that derivative out over each of the   element wise losses and so now we've reduced 
the problem to: we need to get the derivative   of the element wise classes with respect to W. 
We're just going to keep reducing this problem. Okay, well now if I look at these element 
wise losses, there's two inputs to them:   there's the prediction and there's the 
actual label. Only one of those depends on   the parameters, that's that's the prediction, 
and so yes, we need these element-wise losses   but what we really need is the derivatives of 
the predictions with respect to the parameters. Okay why is it challenging to get the derivatives 
of the predictions with respect to the parameters? So let's focus on getting the derivative of the 
prediction with respect to the parameters. Okay   well where does this W^(sx) come in? Well it comes 
in here: it's what we multiply x_1 by before we do   the summation to get the pre-activations. But it 
also comes in here: it's what we multiply x_2 by   in order to put in the summation to get the 
pre-activations. And it also comes in here: it's   what we multiply x_3 by to get the summation to 
get the pre-activations and it just keeps going,   on and on and on. And so the challenge is that 
p_1 is a function of this W^(sx), but then p_2   is a function of it in two different places and 
p_3 is a function of it in three different places   and p_4 is a function of it in four different 
places and so you have to be careful to make   sure that you're doing that derivative with 
respect to all of those different places.   And so, since we're running out of time, I'll 
just say this essentially involves making sure   you do partial derivatives correctly. The idea 
is a relatively straightforward idea and just,   in practice, it can get a little messy like a 
lot of things. This is why we love automatic   differentiation these days to help out with these 
things, make sure we don't make mistakes, but this   is essentially the problem or not the problem, 
but the thing that makes this a little bit tricky   is that it's not like when we had previous 
parameters there was only this one dependence.   And so what we could do is, if we had something 
was a function of a function of a function of the   thing we cared about, we could just use the chain 
rule automatically. Now we have to be more careful   with partial derivatives because there are these 
multiple places that the same weights come in   and so that's what just makes back propagation a 
little bit more tricky here. Okay so to recap: we   were doing a lot of reinforcement learning, we saw 
these cool reinforcement learning things before   and now we've seen that we can actually use these 
ideas of state machines and sequences in, in fact,   supervised learning as well. So if you have your 
classifier in your mind of whether or not you're   doing reinforcement learning, it's not just a 
matter of “are you doing state machines are using   MDPs?” You really want to know are you learning as 
you go along in the world and are you changing the   states and is that something that you're trying 
to decide how to do and to learn more from.   Now, we've seen recurrent neural nets. That gives 
us a sense of how we can learn with sequence data,   how we can do this in this neural net type 
framework and so we're back, in some sense, to   supervised learning but it's a different type of 
supervised learning for a different type of data   than we've seen before. Okay I'll catch you 
next time, have a good week. Bye everybody. 

Okay it's about that time, let's go ahead and get 
started. So in the past many lectures now, not   even just the last lecture, we've been building 
up a lot of neural nets. So first, I think around   lecture six, we introduced neural nets: we'd built 
on linear regression and logistic regression,   we built up to neural nets where we transformed 
the features and then we developed convolutional   neural nets not too long after that as a way to 
deal with vision style problems. Then we developed   recurrent neural nets not too long after that as 
a way to deal with things with an inherent, maybe,   time structure or recursion structure, something 
that you might see in language, for instance,   where you're building up words and so I think 
at this point, you could really reasonably think   based on the structure of the course that the 
whole course is building up to neural nets and   neural nets are like the epitome of machine 
learning and that's all you should ever use   for every machine learning problem and part of 
that sentence is correct. So there's a very real   sense in which it's natural to build neural 
nets from things like logistic regression,   they add extra elements like learning features. 
It's natural to build convolutional neural nets   and recurrent neural nets from when we've learned 
about neural nets and these are certainly super   important methods, they've really revolutionized 
a lot of applications especially in things like   vision. We might use a lot of these ideas in maybe 
natural language processing but what's incorrect   about my previous statement is that you should 
use these on every machine learning problem.   It's more realistic to think that what 
we're doing is we're building a toolbox   and these are part of that toolbox. 
They're very useful and an important part   but there are other aspects of that toolbox. So 
just because we did linear regression and logistic   regression first doesn't mean that only silly 
people use them. Absolutely people use them all   the time to this day and to great effect and we're 
going to see today some more elements, some more   machine learning tools that should probably be 
in your toolbox as you go forward. In particular,   if you care about interpretability, decision trees 
are a really important part of your toolbox and if   you care purely about predictive performance, then 
ensembles and random forests are really important   parts of your toolbox, so we'll spend some 
time talking about each of these methods today.   Okay so first let me just spend a little 
time hopefully convincing you that predictive   performance isn't everything, that there are 
things that we really do care about beyond pure   predictive performance or perhaps we just need to 
rethink what we mean by predictive performance.   But an example of this I think is very timely 
it's election audits. So if you looked in the   news very recently, but here's actually an article 
from back in October even before the election,   election audits are a very important item 
that people are talking about. 46 states,   at least according to this article in October, 
have some auditing regime in place to just double   check if there is some kind of user error or 
some kind of technical glitch or whatever,   but this isn't a new idea of auditing 
elections. So here's a paper from 2008   that's been very influential actually. The 
article, the news article on the left, was   talking to Philip Stark who wrote this article in 
2008 and has been doing a lot of work in election   auditing. And I'll also mention Ron Rivest in 
our department, he’s at MIT, and has also been   doing a lot of work in election auditing. 
It's a pretty cool area and gets into some   math. But here's a quote that I really want to 
excerpt from this 2008 article which is “the   choices that this researcher made, he made them 
to simplify the exposition and implementation;   his methods needed to be transparent to be 
adopted as part of the election process and to   inspire public confidence. He thought about using 
methods that are more familiar to data analysts   and they might be more efficient (in fact, they 
probably are), but because of their complexity,   they'd likely meet resistance from elections 
officials and voting rights groups.” And so   I think this is a nice explanation of why we 
sometimes need methods that maybe don't reach   the absolute peak of something like predictive 
performance or some other notion of goodness. Well   it might be that we just need to explain them to 
people and I think it's not just that everybody's   being silly and we should all accept neural nets 
as our new overlords. There are plenty of reasons   that we should want these good explanations. I 
mean one is that there are reasons to be skeptical   of methods you don't understand. So lots of folks 
right now seem to say they have machine learning   solutions and maybe they're selling snake oil 
sometimes and so if you're not a machine learning   expert being able to distinguish between those 
items might require some kind of interpretability.   But it's not just about whether you have expertise 
or not. Even if you're a machine learning expert,   you probably actually care about interpretability. 
So here, here's an example you can read in this   paper of a case where machine learning experts 
ran neural nets and also more interpretable models   on this problem where they were interested 
in what is the risk of death from pneumonia   from patients. So this was in a medical context 
and the interpretable model had a very clear step:   if the patient… Once they got all of their methods 
out, they have the neural net and it's like “okay,   here I have a neural net. What do I do 
with this?” But the interpretable model   very clearly and precisely stated “hey, if 
somebody is a patient with asthma and pneumonia at   the same time, then they have lower risk of death 
from pneumonia.” And so that's the thing that,   even if you don't have a lot of medical expertise 
but certainly if you do have medical expertise,   you might think that seems pretty silly that 
somebody who has asthma would probably be at   higher risk for pneumonia. And because they have 
this interpretable model, they could say “hey,   something is going on here, that's a little bit 
weird, let's dig into it more” and, in fact, it   turned out the reason that was this was happening 
was that the patients with asthma and pneumonia   were being admitted to the ICU because they were 
deemed to be such high risk. They got aggressive   care that decreased their risk of dying and so 
this is an example of something that you've seen   earlier in the semester: Simpson’s paradox. So you 
might want to check when that thing is going on   and now you are aware already from the semester 
of Simpson's paradox, but now you have an ability,   maybe if you have an interpretable model, to see 
that that might be happening. Another nice thing   about interpretable models is that there's less 
room for human error. So if you look at Malcolm   Gladwell's “Blink” book, there's an example of Dr. 
Brendan Reilly, the chairman of the Department of   Medicine at Cook County Hospital in Chicago. He 
was facing resource shortfalls (they have lots   of poor patients that they're treating) and 
they needed a way to diagnose heart attacks   in patients that were presenting with chest pains, 
who came in and had chest pains. It was quick   and accurate and so they end up using exactly the 
method we're about to talk about, a decision tree,   but here you could say they care about predictive 
performance, like they care about accurately   diagnosing each person who comes in, but the 
problem is that the machine learning method isn't   exactly the method that they're using. They're 
using a person using the machine learning method   and so that's not the kind of thing that you're 
testing when you're testing cross validation,   when you're looking at test set error. That's 
actually an extra element of the system   beyond the pure method and so that's a reason, 
at least on the face of it, if you can't do   more that you might care about something 
that's very simple to use so humans don't   mess it up when they're using it. Okay, so a lot 
of reasons to care about interpretability, about   things beyond predictive performance. It doesn't 
even just have to be interpretability although   that's what I've been focusing on here. But even 
if you only care about predictive performance, you   should care about today's lecture, you should care 
about trees, because trees are part of the things   that have the best predictive performance in 
many cases. So if you look at this Kaggle (Kaggle   is a subsidiary of Google LLC but it also runs 
machine learning competitions among other things)   and there was this magazine that recently in March 
did an interview with a Kaggle master ranked 19th   in the global Kaggle competitions leaderboard and 
what's interesting is they say what's his toolkit.   And if you look at these algorithms in his 
toolkit, lightgbm, xgboost, catboost, these are   all ensembles of trees. And so while neural nets 
can be really useful again for things like vision   and NLP and so on, the reality is for a lot of the 
data that you're going to encounter, ensembles of   trees are basically the thing that really does 
the best in terms of predictive performance.   Okay so at this point, hopefully, you're 
motivated to hear about what is a tree   and what is an ensemble of trees and 
so that's what we're gonna do today.   Okay, so let's start with what is the decision 
tree. Well decision tree is basically just a   presentation of a series of decisions in 
a tree. So let's let's start with that   and then we'll nail down exactly what it is. 
So let's say that I have someone coming in,   maybe they came in with a heart attack. And we 
know after the first 24 hours whether they've had   a heart attack, they've been in the hospital over 
24 hours, and now we want to assess their risk in   order to decide on their follow-up treatment. 
And so here is a tree that people actually in   the early days of decision trees presented, which 
is the following series of decisions. So first we   asked, let's look at a particular type of blood 
pressure, systolic blood pressure over the last   24 hours, and asked: did it really dip down very 
low? Was it always greater than or equal to 91?   So we have two choices: either yes or no. So 
if no, if it was, if it did dip down below 91,   then this patient is considered high risk. If 
yes, then we're going to ask another question.   The question is: is their patient age greater than 
or equal to 65? And a patient who is a bit older   might be riskier, so if this 
patient has an age less than 65,   then we might just consider them low risk at 
this point, they're probably going to be okay.   But if they're greater than 65, we have 
yet another question. We're going to ask:   is sinus tachycardia present? For the purposes 
of this lecture, let's just say that this is   a medical thing that you can measure and you can 
check and you can check if it's present. It either   is or it isn't. If it is not, then this patient is 
low risk. If it is, this patient is high risk. And   so here is just a series of decisions, a series of 
yes or no questions that, even if you don't have   medical expertise, if you're provided with the 
appropriate data, you can just go through these   and answer them. Okay so this is a nice little 
flowchart, but it's maybe not clear in this form   how it relates to everything we've talked about in 
the class. So let's turn this into something that   we can actually talk about in the words and the 
phrasing and the terminology of this class. Okay,   so let's imagine in real life we have a bunch of 
data. You can imagine each individual who walks in   and is around for 24 hours after a heart attack 
is a data point and we have various features   associated with those individuals like the date 
that they were admitted, their age, their height,   their weight, was sinus tachycardia present , and 
the minimum systolic blood pressure over 24 hours,   and maybe their latest diastolic blood pressure, 
and maybe even there were some other features.   Okay. So actually, we can see 
each of these internal nodes   in this tree as being some kind of cut or split 
on one of these features. So in particular,   minimum systolic blood pressure over 24 hours, 
that's actually our sixth feature in this case.   So we can write this as x_6 >= 91. Is this true? 
We'll go to the right-hand child if it's false,   we'll go to the left-hand child. Okay, we can do 
the same thing with age. Age is our second feature   so we can actually rewrite this as x_2 >= 65. 
If it's true, then we'll go to the right hand   child. If it's false we'll go to the left-hand 
child. Is sinus tachycardia present? Well this   depends on how we encode this. A typical 
way that we might encode, as we've seen,   a binary variable, binary classes is either 
0 or 1 or -1 or 1. Either case, 1 is probably   going to be the yes and so we'll ask: is x_5 >= 
1? And so here, now, we have our decision tree   in terms of our features. Now typically, what we 
do here is we do some kind of classification or   regression. Hopefully it's clear that this looks 
like classification, that we're trying to classify   into two classes: high or low risk and so we 
might give them labels. A typical thing we've   done in this class is give them labels like 0 
or 1 or -1 or 1 and so here we're doing that:   we're having 1 be high risk and -1 be low risk 
and so here we are, so we can change this into   our labels. And so at this point, basically, 
we have created a classifier or, in general,   a predictor. And so let's just double check that 
that's true, that in fact this is a classifier.   Oh and before we do that, let's finish up 
saying what exactly is the decision tree,   what counts as a decision tree? Well it's going to 
be a binary tree with internal nodes and leaves.   The internal nodes, each one of them, is going 
to be defined by a dimension index j and a   split value s. So for instance, if I look at this 
node, so this is one of the internal nodes (I've   color coded the internal nodes here as being white 
and the leaves as being green), so this is an   internal node. It has a dimension index that we're 
splitting on, that's j. In this case, j is 6.   It has a split value s that we're splitting on. In 
this case, that split value is 91. And then it has   two child notes and each one of them can either be 
internal or leaf nodes. And then the leaf nodes,   so this one's a leaf node, this one's an internal 
node. Now the leaf nodes are defined just by   their label. So we get to a leaf node, that's 
the end. It's not going to have any children   but it does have some kind of label and that's 
what we're going to predict at that label. Okay so as we said, this is a classification 
tree. This is doing classification as its form of   decision. And now, we can say that we have defined 
a classifier just like we've done in the past   for logistic regression, for neural nets, for 
any of the things that we've been looking at. And let's just check that that's true. So here, 
let's suppose we have a data point that comes in.   So a person comes into the hospital. That person 
has a bunch of features associated with them,   again: the date, the age, the height, 
the weight, is sinus tachycardia present,   systolic blood pressure, latest diastolic blood 
pressure. And now we're going to ask ourselves:   how would we classify this data point? So in order 
to check “what does this predictor look like” on   our particular data point, we're basically going 
to run through the decision tree. So first,   we're going to ask: is x6, is the sixth feature 
for this data point, greater than or equal to 91.   So here's the sixth feature. In this case, 
it's 115, so it's greater than or equal to 91,   so we say yes. So now we go to the next decision:   is x_2 >= 65? Is the person older than 65? 
And we check this person's age, it's 49,   they're not older than 65, so we go to no. 
And now we're to a leaf and so as soon as we   get to a leaf, that is our prediction. In this 
case, it's -1: we say this person is low risk. Okay so this is a classification tree. 
We can use it to make classifications.   You'll notice that it does not have 
to use absolutely every feature.   We're going to see later that, actually, features 
can repeat too, we can have things like that,   but it is a binary tree of decisions, a finite 
binary tree of decisions, that eventually gives   us some kind of classifier. And of course, we 
can do the same thing for regression, it doesn't   have to just be classification, so let's take 
a look at a regression tree. So here, instead   of a medical example, let's have an example 
that we actually visited way back when we were   talking about neural nets. So here our example 
could be: when am I going to run? Where and how   much am I going to go for a run? And maybe my two 
features that I use to decide this are temperature   and precipitation. So how cold is it and is it 
raining, how much is it precipitating out there? And my label could be kilometers run. Okay so 
what are we gonna do here?Sso we're gonna again   start with the decision, say in this case, x_2 >= 
0.3? Basically, is there a lot of precipitation?   In which case, we'll go to yes. Or is there not so 
much precipitation? In which case we'll go to no. Okay well if there's a lot of precipitation, 
maybe it turns out that I just go inside and I   go on the treadmill and I don't want to run as 
much and so I run, let's say three kilometers.   If there's not so much precipitation, I'm gonna 
ask: is it cool? So we'll have x_1 >= -5 and maybe   I say if it's cold, so if there's a no here, then 
again I'm gonna go on the treadmill. I'm not gonna   run maybe as long, maybe it'll be two kilometers. 
If it's yes, maybe I'll ask is it warm, is it   way too warm? Is x_1 >= 33? So if it's very warm 
or if, first, if it's in intermediate temperature,   so it's not too cold and it's not too hot, maybe 
I'll go outside and I'll run and I'll run a good   five kilometers. But if it's very warm, I don't 
feel like doing anything and so maybe I'll just   run 0. So in this case, what's different between a 
regression and a classification tree, we're seeing   that in the regression, all the decisions and the 
intermediate part are totally the same as before,   the only difference is the labels are real 
valued now. So this is like the difference   between regression and classification, in general, 
that you have a predictor that's real valued   instead of a predictor that's just one class 
or the other or multiple classes is also fine.   Okay, so something to note here and 
this is not specific to regression   or classification, this is generally true 
for these decision trees, that the tree   itself defines an axis-aligned partition of 
the feature space. So let's dig into what all   of those words mean by first seeing 
what happens when we use this tree. Okay so first, when we get to the root node, first 
we have all of the feature space. So in this case,   that's x_1 > 0. Sorry, x_1 is any value because 
we can have any value in degrees Celsius. I mean   technically speaking there's a lower limit to that 
but basically all the allowable degrees Celsius.   Then x_2 has to be greater than zero because 
it's precipitation. Can't usually have   negative precipitation. And so we have some 
whole feature space that's here in orange. Okay so now we're going to look at the first 
split here. So the first split is saying:   let's look at x_2 >= 0.3. And so what we're 
doing with this feature space is we're saying,   “hey, we're going to split it into two 
parts with a line.” So this is x_2 = 0.3,   the line. There's a part where x_2 is 
greater >= 0.3, in which case we go   to the right child and there's a part where x_2 
< 0.3, in which case we go to the left child.   Okay, now in this case, when we go to the 
left child, there's actually another decision,   another split. And so when we're splitting, you 
can really think of the split as being applied   to this feature space. We're actually making a 
split in this feature space. So in particular   here, now what we're going to do is we're 
going to take only the space where x_2 < 0.3   and we're going to split it with x_1 
= -5. So you'll notice this line only   applies when x_2 < 0.3. It's like we've already 
restricted ourselves to the x_2 < 0.3 space.   And on one side we have x_1 = -5. Okay so now, again, we say: where are we in the 
space right now, moving down this regression tree?   And then I'm going to finish this up and then I'll 
do the question. So then we have our x_1 >= 33   split. So again, we're going to split up our space 
so we have x_1 = 33 on one side, we'll have x_1 <   33 but it also conforms to all of the other 
constraints we've imposed so far. So this is   the set right here where x_2  -5 and 
x_1 < 33 and on this side, we have the same two   first set of constraints and then x_1 > 33. Okay 
so the question is: how could regression trees   generate labels with a continuous number when we 
have a binary split node? And so what's happening   is that the binary split node is just partitioning 
the space as we're seeing here but the continuous   value that you actually say is your regressor, 
that you actually say is my prediction at a   particular point, is the value in the leaf node 
and so here I've got kind of lazy and I put those   values as 3, 2, 5 and 0. If I had been slightly 
less lazy, I could make them more obviously   real valued, like maybe sometimes I run 3.2 
kilometers and sometimes I run 5.1 kilometers,   but the idea of regression is just that you are 
predicting a continuous value and so as long as   you are allowing a continuous value in your leaf 
nodes, you are doing regression. Another way to   think about that is what's happening in regression 
is that, for every value in your feature space,   you are predicting a continuous value and so what 
we're actually seeing here in this little plot   that I put in the lower right hand corner, is our 
feature space and if you think of this as being a   three-dimensional plot where the y label is coming 
out of the slides at you, then this would actually   be, you could have that y label and then you would 
have your regression values which, in the case   of orange is 3, in the case of red is 2, in the 
case of blue is 5, and in the case of purple is   0 and so if we could imagine rotating that and 
then you would actually see this step function   that we're predicting and so the difference with 
classification is just that you're only allowing,   in the case of two value classification, those 
two values, but if you were doing three value of   classification, multi-class classification, 
you wouldn't be putting that on a y-axis,   you would be saying it's like a, b and c is what 
you're predicting and you wouldn't just plot those   on the same thing because as we saw before having 
this is back in lecture three with our features,   we saw that having multi-classes isn't 
the same as having an ordering like 1,   2, 3 because there's no ordering on those 
classes and see that's where I think you   can really see that there's a difference in what 
you do with the classification and regression. Now that being said, I think this observation 
about the binary split node is key because it   tells you that we're really doing a simple type of 
regression: we're not getting a really different   prediction from one point to the next, 
we're really doing a step function as our   regression. Now I said in the beginning 
of this slide or earlier in the slide,   I would say, what is an axis aligned 
partition. So let me just say that now.   So a partition is when you have a bunch of 
mutually exclusive and exhaustive sets. That is to   say the sets are all separate, they don't overlap, 
and when you take their union, they all become the   whole space. And so that's what we're seeing here 
is that we've taken up the space of x_1 and x_2,   our feature space, and we divided it up. 
That's all you're doing, the partition:   you're dividing it up into little subspaces and 
so we have basically four here is what we've done   and each one of those we're going to predict 
something different. Now two things that are   worth noting about this one, this is why we end 
up getting this step function because we're just,   in this case, predicting a constant value. Now 
you don't have to do that in regression trees,   you could do something else at the nodes, but 
here we're just going to do a constant value.   The other thing to notice is that these are axis 
aligned by our choice of splits. Our splits are   we take one dimension and we split it a particular 
value and so we're always going to get these lines   as our dividers that are parallel to one of 
our axes like x_1 or x_2 in this case. Again,   that's something that you could try to go beyond, 
but that one's a lot harder to go beyond in a   reasonable way that actually gives you useful 
predictions but it is something that you can do. Great okay. So now, we've talked about two 
different types of decision trees: we've talked   about classification trees, we've talked about 
regression trees and, in a way, you can think of   what we've done so far as describing the forward 
mode of these. We talked, when we were talking   about neural nets, about the forward run of the 
model and then going backwards through it and so   here what we're doing is we're saying, “hey, if 
I have a decision tree and I have a data point,   I can tell you the prediction, I know how to 
do that and we've talked through that now.” Okay and that's step one of our familiar pattern. 
So at this point, hopefully this is becoming so   old as to be boring (you totally know the familiar 
pattern and you're really familiar with it),   so step one: we have to choose how to predict a 
label. So if we're given a set of features and a   predictor, which is defined by parameters, and 
we'll say in a moment what is a natural way to   think about parameters here, but given features 
and parameters, we know how to predict a label. So   we just saw that with decision trees: we looked 
through an example and the medical example of   “suppose I have a data point, what'll I do 
to predict whether this person is high risk   or low risk?” We could do the same thing here: 
if I have a day that is 20 degrees Celsius and   there's zero precipitation, I can go to the top. 
I say there's zero precipitation, so I go to no.   It's 20 degrees celsius so I'll go all the way 
down to the bottom and I'll say I'm gonna run   five kilometers. Okay so we know how to predict 
a label given a set of features and parameters.   The next thing that we do is we choose a loss, a 
loss between our guess and our actual label. So   we'll just do that in a moment of course. 
That will depend on whether we're doing   regression or classification, that will 
depend on what type of thing we're doing   with regression and classification, but 
it's something that we'll choose and   then finally we somehow choose parameters 
by trying to minimize the training loss. Okay so what are the parameters here? Well the 
parameters are typically the thing that describes   what is our predictor and so, in this 
case, we have a bunch of unknowns.   We want to say, for each internal 
node, what is the split dimension.   We want to say for each internal node what is 
the split value. So again, for x_2 >= 0.3, the   split dimension is 2, the split value is 0.3, so 
those together will define what's going on at that   internal node, what am I going to do, how am I 
going to make my decision. But I also need to say:   what are the two children notes? Is there one leaf 
and one internal node? Is there one internal node   and one leaf in the other direction? Are there two 
leafs? Are there two internal nodes? These are all   possibilities and we need to say which it is and 
then for each leaf node we have to say the label.   So what we're gonna do in a moment is we're going 
to come up with a waym once we've chosen a loss,   to choose all of these parameters by 
trying to minimize the training loss.   Now something that's worth noting here that's a 
bit different from things we've done in the past,   hopefully immediately different if you think 
back to linear regression or logistic regression,   but actually really different from 
most of the things we've done,   is that the parameters here don't have a fixed 
dimension. So as soon as I make an internal node,   if that has two children that are internal nodes, 
now I suddenly have a bunch more parameters.   And if they have internal nodes as children, now 
I have a bunch more parameters. In fact, you can   get more and more parameters this way so that's 
a bit different than what we've done in the past. Okay so let's suppose that I'm going to do 
regression, I'm going to choose to do regression   and I'm going to choose squared error loss because 
that's a really natural thing that I might do   for regression. Well hey, squared error loss, 
we've talked before about how if I were doing   something with squared error loss, I might 
try to set up everything in my problem so   that's all differentiable and run gradient 
descent or stochastic gradient descent.   And that's just not going to happen 
here. So a couple of things to note.   One, we certainly haven't talked about how you 
would do gradient descent or stochastic gradient   descent if you don't have a fixed dimension 
parameter. That sounds like it might be hard.   Two, this is definitely a super not 
differentiable structure. So if I'm   trying to decide whether to add an internal 
node or not, that's at least, on the face of it,   not differentiable, that's just a very discreet 
decision and so it's really not clear that I   could apply something like gradient descent or 
stochastic gradient descent which really relies on   there being differentiability in all of my 
parameters, that my loss, my overall objective is   differentiable in all my parameters. And so we're 
going to have to think about what could we do.   Well, we could go back to our roots. Way 
back at the very beginning of this class,   we used a heuristic, the perceptron. We don't 
always have to use gradient descent and stochastic   gradient descent, I mean even when they're 
available, we don't always have to use them.   And so in this case, we're 
going to look at a heuristic.   It's going to have two parts: we're going to 
build up a tree, we're going to think of it   as growing the tree, and then we're going to prune 
it back, so take away parts of the tree. This is a   particular way, it's a very classical way to build 
the decision tree. I kind of want to emphasize   that just like many things we've talked about in 
this class, this is not the only way to make one,   there are definitely other ways you can make a 
decision tree and I think, in some ways, what's   really important to understand is the simplicity 
of the ultimate decision tree that you get   is very useful, it's very understandable, 
it's the kind of thing that you could present   to somebody who's an expert in another area who 
isn't an expert in machine learning. You could   have an extremely smart medical practitioner and 
that doesn't mean that they know about neural   nets so this is something you can easily talk 
about with them. If you're serving on a jury   and you're participating in your civic duty on a 
jury, a tree is something that you could explain   to your fellow jury members. And so that 
simplicity is separate from what we choose   to learn it and now we're going to talk about 
here's an algorithm that you might use to learn it   but again not the only thing you could do. Okay so 
again we're going to talk about these two steps:   building it up and putting it back. Okay so let's 
first start by talking about building a decision   tree and as just a motivating example, let's 
imagine that we have some toy data over here   and it is running times based on the, 
again, temperature and precipitation. So x_1   is the temperature. It's very cold, maybe 
I'm not running. When it's very warm, I'm   not running. And then, x_2 is the precipitation. 
When it's precipitating a lot, I'm not running,   but when you get that nice temperature 
and there's not a lot of precipitation,   then I'm running and maybe it's something like 
five kilometers. And we're gonna think of this   as a regression problem. Again, if I put a little 
more time into this, these could be numbers like,   realistically, I might run like 5.2 kilometers 
or 4.9 kilometers. So you can think of these as   being values that really do vary continuously 
and we're trying to learn a regression tree.   Okay so if we're trying to learn a regression, 
again a really natural loss is squared error   loss. So let's just go ahead with squared 
error loss for the moment. It depends on   what you're trying to do in general and it's 
not always the right choice for regression but   for here it's certainly convenient and we might 
just go ahead with it. So let's suppose we're   building a regression tree with squared error 
loss. Okay so we're going to build our tree   and it's going to take two inputs: one, it's 
going to take a collection of data indices.   What do I mean by that? Well the first thing that 
we're always going to do when we run BuildTree   is we're going to run it on our training 
data. And so here “I”, the collection of   indices that we put in, will be the indices of our 
training data: training data point 1, 2 up to n,   assuming we have n training data points. Now why 
would we bother making that an input to BuildTree?   Well the problem is we're going to be doing it 
recursively later with just a subset of the data   and so that's why we want to make that an input. 
We'll see that that's important as we go along. Okay next we have k. So k is going to 
be a hyper parameter of this algorithm.   It's going to be the minimum number of 
points that we allow in a tree branch   or in a leaf. So basically, as soon as we get 
down to two points, we're gonna stop, we're   gonna say we're done. We're not gonna go down to 
allowing a leaf with just, say, one point in it. According to the algorithm we're about to build 
up, I think it's actually the maximum number   that we're gonna allow. I take that back yeah. 
Okay the point is we're not going beyond two.   Okay so our first question: because we're 
not going beyond two points, we're not   letting ourselves have more than two points 
in a branch, we're going to ask ourselves,   “okay, is the number of data points that we're 
looking at right now greater than or equal to 2?” Okay, wellm let's start asking that. This is 
a real question for the chat. I'm looking at   the number of data points when I run this. 
I'm gonna run exactly BuildTree 1 to n on   the dataset that I have right here. So this is a 
dataset that I'm illustrating with the 0s and 5s.   I'm gonna run this BuildTree. What is 
the number of data indices? So that   absolute value “I” means the number of things 
in the set. So I'm asking, for the chat,   what is the absolute value of “I” here? What 
are the number of things in the set of indices? Okay I'm getting great answers from both 
directions. So some people are answering   in full generality, which is great. In that 
case, the answer is n: there are clearly n   things in this index set. And if you did answer in 
full generality, I'll ask you to also think about   what is the number n for this particular 
dataset that we're illustrating here?   Great, so just to recap: in this dataset, just 
to be really clear about how I'm labeling things,   so I'm at a particular x_1 and x_2 point. 
If there's a point, I'll put a number   and that number is the regression number. So it's 
like, oh, at this data point I ran it's the label:   I ran zero kilometers at this data point, I 
ran five kilometers and so the answer which   many of you are getting correctly is 11 because 
there are 11 data points that I'm plotting here.   And so I'm going to check, in this case, is 
11 <= 2 and it is not and so I would skip   this if statement and the first time I run 
this BuildTree for this dataset but let's ask   what would happen if I did happen to have 
actually only 2 points and we'll fill it in.   Okay so if I only had two points, if I were 
making myself a leaf, well what happens at a   leaf? At a leaf, we just make a label. We just say 
what's the label that I'm going to have? What's   the predictor that I'm going to return for this 
little area? It'll turn out to be like that little   partition element that we saw before, this 
little block of space in our feature set.   Okay so we're going to have to set y hat somehow. 
How are we going to set y hat, where y hat is our   label, it is what we predict in this little 
area? Well remember, our goal here—I mean this   is going to be a heuristic method—but our goal is 
to minimize the loss and, in particular, the loss   or the error, the thing that we want to minimize 
here, is going to be the sum of the losses over   the data points in this little set. So we have a 
sum over data indices in the tiny little set that   we're looking at right now. I mean by construction 
it's tiny because it had fewer data points than k   and then we're going to have 
the loss for that data point.   And so something that you should do is either 
immediately think that you have already solved   this problem earlier in the course or, 
even better, solve it again right now   and by right now I mean if you don't immediately 
think of the solution, do this later on your own   time. But you should convince yourself that the y 
hat that minimizes this loss is the average of the   y's of the training data here. So this is a great 
exercise if maybe you're getting a little rusty on   this from earlier in the course, but make sure you 
convince yourself that I'm trying to minimize the   squared error loss on this subset of data, then 
I'm going to take the average of that subset and   that'll be if I have to do a constant prediction 
that that'll be the best thing I can do. Okay so now, I know what I'm going 
to use for my label in this leaf:   it's going to be the average of the 
data points that fall into this leaf. Okay so now let's think about what 
happens if I didn't get down to,   say, two data points or if I 
didn't get down to, in general,   k data points which is certainly what happens 
the first time I start building this tree. Okay in that case, I'm gonna have to make some 
kind of split, that's what happens in our internal   nodes. I've decided I'm not making a leap so 
I'm gonna have to make an internal node and   that internal node does a split and so that split, 
remember, has a dimension and it has a value and   so, for instance, the dimension in this case, in 
this toy example we have here, could either be 1   or it could be 2, there's the only dimensions 
that I have. I'm basically go through all my   feature dimensions and consider each one of them 
in turn. Okay well let's suppose that I choose x_1   and I choose a split value that's maybe slightly 
negative. This is what my split would look like.   Or maybe I could choose a different split 
value or maybe I could choose one over here.   Any of these would be totally fine. Okay 
so here's an issue: if I have a for loop   over every data dimension, that's fine because 
there are usually d data dimensions, that's a   finite number, we're all good. If I have a for 
loop over every possible split value and these   are continuous dimensions, I have an uncountable 
for loop and that's bad, that's not something   that I could do on my computer and so that seems 
like a problem that we're going to have to resolve   and basically I'm just going to come back to in 
a moment but hopefully you can think to yourself   why is this bad. We couldn't have a for loop over 
an uncountable infinity or any infinity really.   Okay so let's suppose for the moment that somebody 
just gave us our split dimension j and our split   value s and go forward with that and then we'll 
come back and resolve this issue of how could we   deal with the fact that we have, seemingly, 
an uncountable infinity of split values.   Okay so somebody just gave us, they 
said “hey, we're going to take x_1   and we're going to split on a particular value s.” Okay what does that do? Well it divides 
my data into two parts. So for instance,   if this were my… So if I chose x_1, so if 
my j was 1 and my split value, s, was where   this line intersects the x_1 axis, then I would 
have two sides: I have the side where x_j >= s,   so I'm going to call all the data indices 
on that side I+ and I have the side where   x_j < s and so I'm going to call 
all the data indices on that side   I-. Okay so here's a question for you, again, for 
the chat: suppose I do this absolute value around   I-, that is to say I say how many data indices 
are in I- for this split? How many are there? Okay it's looking mostly pretty good. 
Let's just walk through what we do here.   So remember I- is the collection of data indices 
for data points where x_j < s so that's what we're   seeing on the left side of the split here in that 
orange rectangle and if we look at the number of   data points in that orange rectangle, it's 2, 
and so we're gonna have two data indices in I-.   If we look at I+, we're gonna say how 
many data indices are in I+ and you can   count them up. In this case, it'll be the 
things where x_j >= s, in this case it's   9 data points and as should be the case, 9 
+ 2 = 11, our total number of data points.   Okay so that's what I+ and I- are doing: they're 
just saying, what are the data indices on one   side or the other? And now what we're 
going to do is we're going to consider,   instead of just having one constant value 
that we predict for this entire space,   breaking into a prediction on one side, which 
we'll call y+ and a prediction on the other side,   which we'll call y-. So on the I- data, we'll 
predict y-. On the I+ data, we'll predict y+.   And then what we want to do is we want to minimize 
the error. So this looks, I think, complicated at   first but it is literally just the error that we 
wrote before which is for every data point in I,   we take the squared error loss and we add it up. 
It just so happens that every data point and I   either is in I+ or it's in I-, it's only one 
or the other, and so we can break that sum over   everything and I into a sum over I+ and a sum over 
I- and then we just have the squared error loss.   Okay again, just like we did above, you can 
actually convince yourself that we don't need   to do anything fancy to solve for y+ and y- 
to minimize this error because, remember,   we want to minimize the loss, we want to minimize 
the error and once we have j and once we have s,   we can just solve for the y+ and the y- that 
minimize them. You can convince yourself that,   again, those will just be the averages. So this 
is much like what we saw in the part above. Now   we're just saying what we're going to do is over 
I+, we're going to predict the average of the   training data in I+ and over I-, we're going to 
predict the average of the training data and I-. Okay so that means if we have a split 
j, so if we have a split dimension j,   and we have a value s, then we know 
how to get the best predictions.   Now we have to choose the split dimension j and 
the value s and so what we can do is we can say,   “hey, for this split dimension j and this split 
value s, what's the error? What's the loss?” And   then we'll choose, over all the different j's and 
s's, what is the error or what is the minimizing   error? How do we get the smallest loss? And we can 
do that by just calculating E_j,s for each j and s   and then looking at which one is minimizing. Now 
remember, we had this issue though where we said,   “ah, there's technically an infinity of values, 
in fact, an uncountable infinity of values   where we could split and so what are we going 
to do?” Okay, so let's look up at our plot again   up here and remember this line, this yellow 
line, is a possible split. So here's a different   possible split, here's a possible split. And with 
this possible split, we're saying, “hey, we're   going to put everything on the left-hand side into 
one prediction bin and and predict the average of   those things, we're going to put everything on 
the right-hand side into another prediction bin   and predict the average of those things.” So if I 
change my split from this to this, between these   two options, so oops here's even three options, 
if I change my split between these two options,   do my predictions change on the two sides?” 
This is a question for the chat: yes or no?   If I change between these two splits or even 
these three splits, do my predictions change?   Great, everybody's saying no. Fantastic. We're all 
on the same page. The observation here is that,   while technically my predictions that a new 
data point might change with these splits,   my predictions that the training 
data do not: they don't change at all   because these splits are between two 
training data points and so I'm not   changing anything about how the training data 
gets categorized and I might think to myself, well   in that sense, those splits are completely 
equivalent to me because all I know about   is the training data. I'm just going to say, 
“hey, those don't have any real difference.”   Now there's a whole bunch of splits that are 
between two data points. In fact, again, an   uncountable infinity of things. So all of these 
splits are between two data points so I might ask   myself, “well, I gotta choose something. How do I 
choose between them?” One option is to choose the   most central of them. Say, I'm gonna average 
the two x_1 values of the two data points and   just allow a split straight between them. That's 
one choice. But the basic observation that we   want to make is that, because all these splits 
are somehow equivalent on the training data,   I don't really have a way to choose between 
them so I'm just going to try one of them,   I'm not going to try every single one of them and 
try to compute the error every time because I know   I'm going to get the same error, the same training 
error in this case. And so instead of going over   every split dimension and every split value 
s which would be literally impossible to do,   what we're going to do is we're going 
to go over every split dimension j   and then maybe something like one split value 
roughly per data point. So we might say, maybe,   we'll do a split value everywhere 
that it could make a difference,   everywhere that there is a unique training 
data loss and that'll be about on the order   of the number of data points. I say 
about on the order because you might do   different things at the edges, you might have two 
data points that have exactly the same x_1 value,   but roughly you only have to do something 
roughly on the order of the data.   Okay so now, we've reduced this to a 
problem that you can actually solve,   we're going to try maybe something like the 
central values between every two data points. And   so finally we have a finite 
number of split dimension values j   and split values s that we're going to try 
and because we have a finite number of them,   we can look at the error or the loss 
on every one of them and we can say   which choice minimized that error or that 
loss and that's where we're going to split. Okay so finally, so what we did up top was we said 
if we had a small enough number of data points,   then we were going to make ourselves a node, 
it happened to be a leaf node, and that leaf   node's going to have a label. Here we said, okay, 
we didn't have a very small number of points,   so we're going to make ourselves an internal 
node, not a leaf node but an internal node. And   so let's think back what defines an internal node. 
We've said this a couple of times already but now   we're going to start getting really precise. One 
internal node is defined by a split dimension,   a split value, and it's two children. And so 
here, what we're going to do once we found the   best split dimension and the best split value, is 
we're going to return that best split dimension,   we're going to return that best split value, 
and then we're going to build the children.   So the left child, let's think how would you 
build a left child. Well, you're basically   going to recursively do the same thing on the 
data points in the left side of the split.   So in particular, we're going to BuildTree on 
a set of indices that are now smaller than the   original indices we put in. We put in our indices 
I and then what we did, was we found a split   described by j* and s*. We're gonna take all 
the indices on the left side of that split   and build a tree with them and then we're gonna, 
on the other side, take all the indices on the   right side of the split and we're gonna build the 
tree with them. And the reason now that this is   I_j*,s* is because we chose a particular split, 
described by j* and s* and so we technically,   in some sense, computed these I’s for every 
single j and s, but now we're saying, “ah,   this is the particular j* and s* that we chose. 
It's the particular split dimension and the   particular value and so that's what we're going to 
use to split going forward.” And so you can see,   in this sense, this is a greedy algorithm 
because it only uses the information that   we have right now and it tries to minimize the 
loss immediately even if that might not be the   best thing in a long-term sense and by long term, 
I mean over multiple splittings into the future. Okay so let's run through 
an example of doing this.   Let's build a tree for this data that we have 
on the slide. So we start by saying we're going   to BuildTree with all the data indices, 1 
to n. As you observed earlier, and in fact,   here is 11. And so we're going to build over the 
indices 1, 2, 3, 4, 5 up to 11. And let's suppose   we chose our k to be 2, that's that's how many 
data points that we're going to allow in a leaf. Okay, so first thing we're going to do is 
we're going to walk into this BuildTree   algorithm and we're going to start 
making ourselves some kind of node. We first decide whether it's going to be a leaf 
node, if we finally made it to a leaf node, or   if this is an internal node. As we observed, 11 is 
not less than or equal to 2. And so in this case,   we will not be building a leaf node, we'll be 
building an internal node. So we'll go to the   internal node building center. Okay so over here, 
we're going to look at every possible dimension,   again that's every dimension of our feature space, 
in this case x_1 and x_2, and we're gonna look at   every possible split value, every possible split 
value that could even give us a different loss,   a different error, and so really we're 
just going to look at something like,   maybe, between each two data points. 
Okay so we do that, we find maybe the   split value that is particularly good here, 
and we're going to start building our node.   So what makes our node? Well the first thing 
is what was the split dimension and value that   we found? Maybe it was this one. So here, maybe 
we found x_2 >= 0.28 is a good split. And so on   one hand, we have x_2 < 0.28 (so when it's not 
too much precipitation) and on the other hand,   we have x_2 >= 0.28 (when there is a lot of 
precipitation). So that's the split that we've   done and if you kind of look at it, this seems 
like probably a reasonable first split to have   done for this data: it chops off the most things 
that you can that are unique in an axis-lined way. Okay so we made this internal node and 
so it's going to have two children,   it's going to have a left tree and 
it's going to have a right tree. And the thing to notice here is we're 
going to start by making the left tree,   just in the way that this pseudo code runs.   Okay so let's talk about how would we build 
this. Well, we BuildTree on the indices   that fell into that region. So in this case, 
hopefully, you can see there are going to be eight   indices that fell into that region, there 
are eight data points that are in that region   and so we're going to ask: is 8 <= 2? It is not, 
so we're going to build ourselves an internal node   and it's going to have a cut off, maybe in this 
case, it happens to be 7.2 and so that might look   like the following: so we have our left hand 
side and our right hand side from that split   and now what we have to do, again, is start 
by building tree for the left hand side. Okay what is that going to look like? Well again,   we go up to BuildTree and we ask: is the 
number of indices less than or equal to   two? This is a question for you in the chat” is 
the number of indices less than or equal to two? Awesome, a lot of great things here: yes! It 
equals two. And so technically, 2 <= 2 and that's   like the relevant thing that we're interested in 
here. And so, in fact, we will make a leaf node   at this point. And so let's go ahead and do 
that. What does our leaf node look like? Well,   we're going to take the average of the data points 
at this point. That's going to be our prediction.   Another question for the chat: what is the 
average of the data points in this region? Great. Yes. As you've observed, this should be 
a particularly straightforward average because   both data points are zero, the answer is zero. 
In reality, probably, they would be very slightly   different from zero, but this is one that's 
easier to do some math with and so the answer is   zero. So we'll make that the label at this leaf. 
Okay, so now, we go to our right hand BuildTree   and maybe we find that a particularly good 
split is between all these 5s and all these 0s.   So on the right hand side, we have the 5s, 
on the left-hand side we have these 0s.   Again this looks like our “we don't run when 
it's too hot, we run when it's a reasonable   temperature” kind of split. Okay, so now we're 
going to build our left hand tree. And something   you'll notice is that even though everything is 
5, even though we have this great regressor for   everything, that's not our decision about whether 
to make a set of leaves. The set of leaves,   whether to make a leaf, our decision is the number 
of things in this partition element or “is the   number of indices less than or equal to two” and 
it is not and so we're gonna do another split   okay. So maybe we happen to split here into 
some 5s on one side and 5s on the other side.   In this case, all the splits are equivalent. 
There's nothing that's better, and so you have   to have some kind of tie breaker and maybe 
this is just where the tie breaker fell   and so we'll predict 5 on this side, we'll predict 
5 on this side because that's going to be the   average and then we'll go back up to making our 
left hand leaf up here and maybe we'll predict 0   over here and then we finally get to our original 
left hand leaf, you can see that we're doing a   depth first kind of thing here, and in this case, 
again, we're going to have to divide because there   were three data points and so maybe we'll have one 
side with one data point and one side with two.   Okay so we have created a tree, we have 
built a decision tree by doing this.   We kept going until we got down to a very 
small number of data points in each league. Okay so now, let's take a 
look at this decision tree.   Again, what it's doing is it's creating a 
predictor, it's creating a function of the   features and you can see that, you can imagine 
that it's popping out of the slide at you in   the y-axis and it's 0 over a lot of this 
area and then it's 5 in certain areas and   it's basically a step function: it's a 
step function that we have predicted. Now an observation that we could make here is, 
well, we didn't have to keep splitting. I mean,   at some point, we actually had like zero error and 
so why would we keep splitting? Maybe we should   stop splitting at zero error. Maybe even 
better ideas for regularization purposes,   we could do something like stop splitting after 
our error gets quite low. Maybe we should just   check if our error is too high or not. A related 
question here is: is overfitting an issue?   And if you think about it, what we're doing here 
is we're getting down until we have two data   points and then we're fitting them as perfectly 
as possible. You can imagine if k is one, then you   get down until you have one data point and then 
you just predict exactly that data point’s value   because the average of one data point is one and 
so it certainly seems like it would be easy to   overfit in this case and so, again, how about 
we regularize by stopping splitting after our   error gets low? What if we do that? Well just 
an observation about what could go wrong there.   So we're going to pursue this idea of stopping 
splitting when there's no or a small change in   loss. Just an observation of what could go wrong 
is let's look at this example: suppose, instead,   that I run when it's really warm and there's 
rain or I run when it's really cold and there's   no rain. This doesn't seem entirely implausible. 
This could be a way that somebody decides to run   and suppose that I wanted to use a decision 
tree, a regression tree, to learn that.   Well, at least if my data were perfectly arranged 
like this and actually we did see some data   back in the features lecture, back in lecture 
three, that actually could be arranged like this   just due to the way that we encode 
features, you're going to see that   let's try a first split with this. 
Suppose we're building a tree.   We go to our first split. We say, “hey, we have 
our set of indices, there are four indices, and   we're going to try every possible split and there 
is literally no split that improves the error   in this case, everything is just as good as 
if you had just this whole region and you were   looking at that.” And so if we stopped when there 
was no change in loss or a small change in loss,   we'd stop right here. And yet, if we made a 
split, pretty much any split that was between   data points here, and then we made another 
split, we'd get a perfect predictor and it   would be fantastic and it would do really well. 
Now if these things are moved around a little bit,   the idea changes slightly, but the idea is still 
there that, somehow, there's this issue of if you   stop splitting when your error is very low, it 
can be very short-sighted. Kind of the issue is   that we're doing these axis align splits and so it 
might take a few to actually get to a good point   rather than just a single one, like a single one 
might not represent the thing that you care about. So from that perspective, we were 
greedy when we were building this tree   but maybe we don't want to be greedy in stopping 
building the tree which is what would happen   if we stopped when there was no change in 
loss or small change in loss and instead,   an idea that was pretty influential when it was 
introduced and continues to be used to this day,   is to build the tree, to grow the tree with 
this algorithm, but then prune it back. And so   I'm just going to briefly go over what that 
might look like. Again, I wouldn't get too   obsessed with any particular way of building 
or learning a tree, but I think there are some   general ideas here that are important to keep 
in mind about what can you get from a tree,   what kinds of structures can you look at, and how 
you have to be aware about how you're learning it. Okay so let's talk about how to regularize. Now 
you can think about this as being encased in   some kind of objective. Certainly this 
is something that we've talked about   in other cases throughout our course that we have 
some objective that we're interested in optimizing   and typically it looks like the following: 
it looks like a loss over the training data   plus some kind of constant times a regularizer 
and usually the regularizer is a penalty,   it penalizes having some more complicated model. 
It makes you pay for that more complicated model,   it makes you say that the loss was really worth 
it and so we've seen this before for logistic   regression, for linear regression. A lot of times, 
our regularizer has been this squared error loss   or this squared error or sorry I should say just a 
squared size thing, like if we have a theta, we're   taking this squared penalty on the theta. And 
here, we're going to look at something a little   bit different. So here we want to think: what are 
our choices for our tree? Well, training loss,   that's pretty straightforward. We already 
said that we cared about squared error loss   for our training loss and so we can just write 
squared error loss for the training loss,   so that'll be straightforward, 
the concept’s easy to put in,   but what do we want to penalize? And I think, at 
least based on our discussions that we've had so   far just now, a natural thing to penalize is 
bigger trees, bigger more complex trees and so   we might say our penalty is going to be on trees 
that have more leaves. If you have more leaves,   you have to pay for that somehow with training 
loss, you have to have a better training loss.   Okay so here's a way to express that: let's 
just walk through the elements of this formula.   So first, we're just going to say what is the name 
of our objective? In this case, we'll call it c,   it'll be a function of T. So T is the tree that 
we learn, it's the predictor that we learn,   and alpha will be the constant that trades off   the penalty and the training loss. 
Often in the past we call this gamma. Okay so first, we have the training 
loss. So there's nothing new here:   we're just summing over all the data points that 
we have, we're looking at the loss between our   guess which is the predictor T applied to the 
features for this data point, the i-th feature,   and our actual which is the actual 
label for these data points, the y^(i),   and we have our alpha that trades off our 
penalty and our training loss. And here,   we're saying the thing that we're penalizing 
is the number of leaves so we're preferring,   we're trying to express our preference, 
for trees that have fewer number of leaves.   Okay so what would happen if 
we looked at our tree over here   and we did this trade off? Let's start... Oh and 
let's call this the cost complexity of a tree   T. So this is often what it's called, it's again 
very similar to the objectives we've seen before.   Okay so let's look at our tree over here. In 
particular, if we had our whole tree, it describes   this whole space with a bunch of partitions in 
it, but let's look specifically at this little   subtree down here at the bottom, so the sub tree 
that's defined by the x_1 >= 18.3 split and its   two children, the 5s. This seems like a useless 
subtree: it's not doing anything. If we didn't   have this subtree, if we just had a single leaf 
that predicted 5, it would do exactly as well   and so if you look at this cost complexity 
and you look at alpha = 0, then there's no   way to decide between this full tree and the tree 
where this subtree is replaced by a single leaf.   But as soon as you increase alpha just a tiny 
bit, it could literally be any slightly positive   value of alpha, anything whatsoever that is 
slightly positive, suddenly it's not worth   it to have this tree because you are taking a 
hit of an extra leaf and you could reduce that   by getting rid of this subtree and so as soon 
as you have this very slightly positive alpha,   it is better to replace this subtree with just 
a single node, with just a single leaf, because   that'll be one fewer leaves and so you'll reduce 
the cost complexity at no hit to the training loss   and you'll notice that this is the same over here: 
so here we have a case where there's a subtree,   this internal node and it's two children, 
where there's no change in the loss,   the training loss, if we reduce 
this to just a single node   and so if all we cared about was training loss, 
then these would be completely equivalent trees,   but as soon as we have this penalty and as 
soon as we have a tiny little bit of alpha,   then it's better to just get rid of 
that subtree and replace it with a leaf. Okay so the idea of pruning is basically to just 
keep doing this. So with pruning, what we do is   that you can imagine slowly increasing alpha. 
So here we increase it a little bit beyond zero,   we can slowly increase it, and as you increase 
it, eventually, certain sub trees are not worth it   and you can get rid of those subtrees one at 
a time as they become not worth it and you'll   end up having a finite sequence of trees until 
you get to the root. Eventually nothing will be   worth it and it would be better to just have 
a single constant thing that you predict. And   once you have this sequence of trees you can run 
cross-validation on it to find the best tree. Okay but the idea here, I think 
the really key idea to get,   is that this can perform a lot better having 
the ability to have a little bit of a longer,   less greedy procedure to create these trees 
can perform a lot better than if you just   stop making the tree when it seems like you're not 
getting any benefit from it in a particular step. Okay so at this point, we know a way to create 
a decision tree. Again, I want to emphasize   that it's not the only way. In fact, I think 
there's some really interesting work going on   in terms of trying to have the interpretability 
of decision trees with the flexibility of other   methods so you could try to have your very 
powerful method that performs really great   in terms of predictive performance and then 
what you can do is you can try to have a tree   that represents the information in that 
even if it takes a little bit of a hit   in terms of understanding and so there are lots 
of things that we can do here, trees are really   interesting nonetheless. Oh before I move on 
I'll just get this question from Discourse:   how do we choose the value k or should we just 
make it pretty small to get a large tree and then   prune? Basically these are both great ideas so a 
typical thing that people might do is that they   might get their very large tree and then prune 
and then the pruning aspect, you don't have to   care too much about k. You might still care about 
k just from a computational perspective. So if you   think about your k as being 1, you're gonna have 
to make a ton of splits in a very large dataset   to put everything into its own little bucket and 
so it might be just easier and faster to use a   larger k, so that's a consideration as well. So 
basically there are multiple considerations even   beyond our usual ones like prediction error but 
also just speed and ease of use but absolutely,   yeah, you can just make a relatively small k 
and then prune back to do the regularization.   Okay, another thing that you can do (you 
can think about this as regularization,   you can think about this as just doing better)   is you can ensemble. So just as we just talked 
about, maybe, we could choose a really small k   and then we could make this really big tree and 
then prune back, you could also truly choose a   really small k and then ensemble and that'll do 
some regularization for you. It also just will   turn out that this is a good idea, period, 
to increase the performance of your method.   Okay so we, in particular, again, just when we 
started off, we said we're going to talk about   decision trees in terms of interpretation. Now 
we're going to talk about ensembling in terms of   predictive performance. The general idea here is 
that using multiple machine learning predictors   even if they don't seem individually really great 
will typically make a much better predictor.   So you see this everywhere. So if you 
look at the Netflix prize back in the day,   not only did all the top teams use ensembles 
but literally one of the team names was “The   Ensemble” so good advertising for ensembles there. 
If you look at basically any other competition,   this tends to be the case. So the Makridakis 
competition is a competition that happens   regularly on time series data. They looked at 
100,000 time series in their competition and   no individual method ever did that well, but when 
you have these big ensembles of lots of methods,   some of which included neural nets but that 
wasn't the only method, you got these great   performances and, of course, it's not all just 
about competitions. I mean, in some sense,   the point of competitions is to prepare us to do 
things that really matter in the real world and   if you look at the forecast that people are using 
for both cases and deaths in COVID-19 right now,   the CDC, for instance, is reporting an ensemble 
forecast and so there's just this, generally,   sense that ensembles are giving us really the 
best predictive performance when we care about   predictive performance. Okay, so there are a lot 
of ways to do ensembling. We're going to talk   about one of them. So, in fact, I mentioned in 
the beginning, these Kaggle competitions and the   things that people are doing there. It turns out 
that's actually a different type of ensembling,   boosting, and I encourage you to check that out 
if you're interested in that direction. We're   going to talk about bagging, and from this, we're 
going to get a sense of what an ensemble does.   Okay, so this is just one of multiple ways to 
make and use an ensemble. Bagging stands for   bootstrap aggregating, which doesn't really 
make much sense right now, but hopefully   we'll make a bit more sense at the end of this 
slide because then I'll say what that means.   Okay so what we're going to do is 
we're going to take our training data   and we don't have to be doing trees right 
now, we could be doing just about anything,   and we're going to create a bunch 
of fake datasets from our original   dataset. In particular, B fake datasets. And the 
way that we're going to make these fake datasets   is, for every b, we're going to draw a new 
dataset by sampling with replacement from   our original dataset. So what does this look like? 
So in our original dataset (you can think of our   original dataset as a bunch of color billiard 
balls in a bowl, so there's a yellow ball,   there's a green ball, there's an orange ball, 
there's a blue ball, and there's a purple ball)   and then what we're going to do, is we're 
going to stick our hand into the bowl,   pull out a billiard ball and that'll be 
the first data point in our new dataset.   So we choose each one of these five data 
points, in this case, with equal probability.   Now the width replacement part is where we take 
the ball that we're holding and we put it back in   and then we shuffle it up again, we choose from 
every possible point again with equal probability.   If we were doing without replacement, every time 
we took one of these balls out, we would put it   over here outside of the ball and then we would 
draw a new ball and there would be fewer balls   every time. In this case, there's the same number 
of billiard balls and so we can keep doing this:   we can draw our third data point in our new 
dataset, our fourth data point. Of course we're   gonna get some, probably, it's very likely, that 
we're gonna get some repeats because we're just   putting it back in and we're drawing from the 
same distribution. In this case, it turns out   we got three repeats and so we'll say that this 
is our new dataset and it has its own indexing.   It just so happens that a lot of its data points 
are the same as the other one and so here, what   I'm going to do is I'm going to change this to be 
our new dataset and indicated by (x tilde)_1, (y   tilde)_1. So we have the first data point, our new 
dataset, the second data point, our new dataset,   the third data point, our new dataset, and so 
far. But you'll notice, the first, fourth, and   fifth data points are the same and two and three 
are different. Okay and so what we're gonna do   is we're gonna make this new 
dataset B different times. And the trade-off with B is that the more B, you 
get better in some sense, but eventually it's just   computationally difficult and you stop getting 
as many benefits, so that's the trade-off for B.   We're going to make all these different datasets 
and for every one of these datasets, we're going   to train a new predictor and because the datasets 
are random, because they're different every time,   we imagine that our predictor is 
going to be slightly different   and what do I mean by predictor? This is 
like we're going to make some regressor.   Maybe in regression, we'll have a different set 
of predictions for new, this is a function of   the features, what is the prediction? We make it 
a new feature, so that's what that function is. Okay and then finally, what we do is we combine 
the wisdom of all of these different predictors.   We imagine each of these different predictors is 
pretty good at something and we're going to return   some combination of all of their information. So 
for regression, a typical thing that we might do   is we might average over all of them. So 
we're going to return a final predictor,   so this (f hat)_bag is like what we actually 
return at the end of the day and it's the   average of all those individual predictors 
on the little different datasets that we did. For classification, you can do something similar: 
you can take the predictor at a point as being the   class with the highest vote count with some way 
to deal with ties, but basically at every point,   for every one of your B classifiers, 
you're going to have some   classification, it's going to say what 
class it thinks it is and so you can do   a voting across them and say which 
which one's getting the most votes. Okay this is a super simple idea. All we 
did was we randomized our data slightly   and then we got this way better predictor 
almost for free. I mean you have to run some   extra things, but you just have a bunch 
of these predictors and you already had   some way to make predictors and so now you're 
just running a bunch of these fake datasets   and so this is bootstrap aggregating and it is 
surprisingly effective. You can have some pretty   bad predictors as long as they're just slightly 
good, they're better than chance. This can be   really, really effective. Okay so why 
is it called bootstrap aggregating?   The bootstrap part is making all these little fake 
datasets by sampling with replacement B times.   This is an idea that is not at all specific 
to trees. It's not even specific to bagging.   It's actually an extremely general and 
surprisingly powerful idea for how simple it is.   I mean maybe I shouldn't say surprisingly: some of 
the simplest ideas are some of the most powerful,   but that's the bootstrap part and then the 
aggregating part is when we aggregate it all   together at the end and we take all of these 
different predictors and we make them into one   super predictor. It's like if you took the power 
rangers together and you made them into Megazord,   this is what's happening right here. Okay so that's bootstrap aggregating. There's 
nothing that is specific about trees here:   you could do this with any favorite predictor. 
In fact, there's reason to think that neural nets   actually, individually, are kind of unstable. Like 
you can get really different results if you just   change things slightly about your setup or change 
things slightly about the data and so you could   put them all together if you wanted to in this, 
you could do a lot of things with bagging. But of   course, one of the things you can do with bagging 
or wouldn't fit quite so well into this lecture,   is bag trees, bag decision trees, and so that 
is so popular that it's given a special name,   a name that you are likely to encounter in life 
which is random forests. At least if your life   is full of machine learning, you're likely 
to encounter this. Okay, so random forest   is basically bagging with decision trees 
with a little bit of extra randomness.   It's not exactly bagging decision trees and I'll 
point out where the extra randomness comes in. Okay so let's just briefly 
talk through a random forest. So random forest, you do the same thing: it's 
just bagging and so what we're going to do,   is we're going to go through all these B 
different bags basically. In each one of them,   we draw a new dataset with replacement from 
our original data. So that's just like bagging,   nothing has changed from bagging. Okay here's the 
part where we make this about trees instead of a   general regressor. So when we're doing general 
bagging, this could be any predictor. Now,   we're just speaking specifically about 
trees. We're going to build a tree   on the random dataset by recursively repeating a 
number of steps until the node size k is reached.   Okay, so first... Now here's the part that's 
different from both bagging and decision trees:   we're going to select m features uniformly at 
random without replacement from the d features. So   we didn't do this when we were doing random forest 
before, when we were doing decision trees before.   If this were just bag decision trees, we wouldn't 
have this part. This is like extra randomness and   it turns out to be useful to decrease correlation 
between the different trees you're learning from.   Okay but you're going to select these m random 
features, the subset of your d features,   and now you're going to do the usual 
decision tree thing. So instead of   going over all the features like we did 
when we were making decision trees before,   you just go over the subset but you still pick 
the best split dimension and the best split value   among the m features instead 
of the total d features. And then you build two children. So everything 
here is basically bagging and then we do trees   and there's just this little extra randomness 
here. Okay and finally, just as in bagging,   we return the average for regression or 
some kind of top vote for classification. So why is it called random forest? 
Well it's clearly random because we   have multiple sources of randomness: so there's 
the randomness in making the different datasets,   there's also the randomness in choosing the 
feature subsets. It's a forest because you make B   different trees and so you have a whole bunch of 
trees, B of them. Each tree defines a predictor   and then you average over all of those predictors 
to get your final predictor, your final regressor   that you're going to return, and so the forest 
is just because it's literally a bunch of trees. Okay and so to recap: the bagging part was 
that we drew all these different datasets   and then we averaged them over for regression 
or voted for classification at the end,   the trees part was that we 
chose trees as our predictor.   So we could have chosen something else 
with bagging, it's not specific to trees,   but here we chose to use trees and then the 
extra randomness was that we didn't just   use any features as we would normally for 
trees where we consider all the features,   we just concern ourselves with these m uniformly 
at random features. Okay so at this point,   we have accomplished some of our goals. Our goals 
were to try out, to see: could we come up with   methods that were useful for interpretability that 
weren't purely focused on predictive performance?   And decision trees, I think, are certainly that. 
If we go back to our tree that we saw at the   beginning, again, the hardest part I think, in 
some sense, about this tree is understanding the   medical terminology, it's not understanding how 
to use this machine learning predictor. This is   something that, I think, you could tell 
family members who don't have machine   learning background about, this is something 
you can tell juries about, this is something   you can tell medical experts about, this is 
something that you can talk to people about,   whereas I think it's a lot harder to explain 
what's the output of neural nets, for instance.   Conversely, with decision trees, we're not 
going to have as much predictive power. I mean,   they're just not as flexible as some of the models 
that we've seen elsewhere. With random forest,   we can get that predictive power. Even though 
it's built from these tiny decision tree blocks,   you actually get a huge amount of flexibility 
by having this ability to average over a lot of   decision trees but you lose, basically, 
the interpretability of decision trees   because you're not just making this 
simple series of decisions anymore,   you're making tons of them and what does it 
mean to average over tons of these decisions?   It's not as clear and so you get a real trade-off 
and it depends on what you're trying to do. Okay   great. Then I hope you all have a great break. 
I think we won't see each other for a couple   of weeks but I hope you enjoy the Thanksgiving 
break and I'll catch you at the next lecture. 

Okay it's about that time, let's go ahead and 
get started. So far in this class, we've spent   a lot of time talking about supervised learning. 
We've seen a couple of examples of this: things   like classification, regression. You’ve seen them 
a lot of times and so today, we're actually gonna   explore this other part of machine learning that 
we've mentioned before but not really dug deeply   into, unsupervised learning, and we're going to 
focus, in particular, on probably the most popular   form of unsupervised learning, clustering, and 
we're probably going to focus on the most popular   form of clustering which is k-means clustering. 
Okay, so let's dive in. As usual, we'll start   with a motivating example. So it happens 
that today is Giving Tuesday. I think that's   an international phenomenon now but I'm going to 
talk about just a couple of charities in America:   Feeding America and Meals on Wheels America. I 
think both are charities that distribute food to   people who have food insecurity and we're going 
to imagine, for the moment, that a new charity   has approached us. This new charity is called 
Yes Free Lunch and they are interested in also   distributing food to people with food insecurity 
and, in particular, their model is that they have   these food trucks that they're going to place in 
various places around America and they want to   place them in such a way that people can walk to 
the truck, pick up some food, and head back home   and so they've approached us, machine learning 
experts, to help them place their trucks in some   optimal way for this food distribution. So we're 
gonna think about that problem today. Okay so they   have, let's say a very fantastic existing model 
and existing trucks, but now they have a new area   of the US and they're interested in placing their 
k food trucks and asking where they should park.   And so what we're looking at here is perhaps the 
area that they're thinking of placing these k food   trucks and each dot here represents an individual 
that they think might use their service, that   might pick up some food from their place and so 
from one of their trucks. And so, in particular,   the horizontal axis here, we can think of as 
longitude, the vertical axis we can think of   as latitude. So this is really like we're just 
looking at some dots of where people live on a map   and we want to ask: how can we minimize the 
loss of the people that this charity is serving?   And so we have to think about, well, what does it 
mean to have a loss here? On what are they losing?   It might be some aspect of, like, “how much of 
their time do they have to give up” or “what   do they get by walking to these trucks.” Okay so 
let's establish a little notation to just be able   to write this kind of thing down. So first of all, 
let's say that there are n different people that   we’re interested in serving here and we'll say 
that the i-th person is at location x^(i), so this   is where the i-th person lives. So for instance, 
this little dot is where a particular person lives   and this will be described by a feature vector. 
That feature vector is the longitude and then   the latitude of that person's location. Similarly, 
we can talk about our j food trucks. So our food   trucks will be taking values from 1 to k or be 
indexed by values from 1 to k and they'll be   at some location too. So here let's suppose I have 
five food trucks, so k = 5, and I am interested in   locating them in an optimal way so I want to write 
down their location. So let's say that for the   j-th food truck, its location, again, is going 
to be given by a feature vector, a longitude   and a latitude. Okay, so now I want to say, 
let's assume that each person walks to a   single food truck and maybe we communicate with 
them beforehand on the phone and we say “hey,   here's your food truck. You can expect your meal 
is going to be at this particular food truck.”   and that food truck we need to identify to 
them and so let's call the food truck for the   i-th person y^(i). And so for instance, if this 
person, if we call them up and we say “hey,   you should go to this food truck,” then we're 
going to want to denote that with y^(i). Okay so finally, we have the notation to be 
able to say something like “what's the loss   of the i-th person walking to the jth truck?” 
Now it could be anything. I mean really,   you could choose whatever loss is most appropriate 
to you. I'm going to make a choice because we have   to make a choice to proceed but you could always 
make a different choice since we're thinking about   what the choices you make imply. So one choice 
that we could use, we've been talking in this   class a lot about some kind of squared error, and 
so here we have a notion of a squared error loss:   we're taking, from this particular point, from 
wherever this person lives, to the food truck,   we're taking the Euclidean distance and then we're 
squaring it. So one reason that we might consider   squaring the Euclidean distance in a loss here is 
that people really, really don't like to walk very   long distances. Having 10 people walk 0.5 miles 
is not the same as one person walking five miles.   Five miles is prohibitive, they might just not 
even bother to go to the food truck at that point   and so the squared loss penalizes far away 
food trucks even more than we might expect   from just maybe a street distance or some other 
type of loss. It's a choice but it's a choice   we're going to make here: we're going to use 
this squared Euclidean distance as this loss.   Okay, so now we can ask: well, what's the loss 
across all of the people that this charity is   serving based on this choice? What is the loss for 
a particular arrangement of the food trucks and   the assignments of people to food trucks? Okay, 
let's think about what's going on in this equation   here. So first, we're saying, for every person, 
there's a component of the loss (so that's the sum   from i = 1 to n) and now, for all of those people, 
we're going to say “hey, this is the loss that we   assigned to them.” The j index here is replaced 
by y^(i) because that is the food truck that this   person is going to go to. Now what I'm going to do 
is I'm going to write this exact same formula in a   slightly different way. So exactly the same thing, 
just adding up the loss over all the people,   we're going to write it in a slightly different 
way. So here's a slightly different way,   let me just talk through this. 
So again, we're just summing up   the loss for every individual that this 
charity is serving, so i = 1 up to n.   Now what we're doing is we're 
saying one way that we can represent   this loss that we said was the squared euclidean 
distance between an individual and the food truck   that they're going to is let's consider every 
possible food truck that they could have gone to   from j = 1 to k. Only one of these values will 
be non-zero. So remember, this indicator notation   that we've used in a few lectures before, since 
indicator notation is 1 if what's inside of it is   true and 0 if what's inside of it is false, and so 
all but one of the food trucks are going to have 0   here. The one that the person is assigned to is 
going to have 1 and in that case we're going to   say “what's the square of Euclidean distance to 
that food truck?” So hopefully you can convince   yourself that this is just exactly the same 
formula as summing up over all the individuals   and saying what is the the loss between them 
and the food truck that they're going to.   Okay, so now we have this loss and what we've 
done in the past in this class whenever we've had   a loss is we've said, “hey, let's try to use that 
to optimize some parameters that we're interested,   some values that we're interested in” and 
so we can think about what are we interested   in doing here. Well, we're interested in placing 
the food trucks and assigning people to the food   trucks and so we can do that here, we can say 
okay… Oh by the way, and I'll just mention really   briefly before I do that, this summation order 
actually doesn't matter and hopefully you can   convince yourself that I could just switch them, 
I could have the sum of the k before the n and   that wouldn't change anything. So totally 
the same, doesn't matter which one I use.   Here's just a different one. Okay so now that's 
the loss and I want to minimize it. So here mu   is going to be the collection of all the food 
truck locations, y is going to be the collection   of all the assignments of individuals to food 
trucks, and I want to find the mu and the y   that are going to minimize the loss, that are 
going to make it so people don't have to walk   very far to get their food trucks, food from these 
food trucks. So that's going to be my goal and   trying to satisfy this goal is called k-means 
clustering and this thing that we're minimizing   here is called the k-means objective. Why is 
it called the k-means objective? Well, one,   we've seen before that the thing that we're 
optimizing when we have an optimization problem,   we typically call that an objective, so that's 
why this is called an objective. Why is it called   k-means? Well we have these k food trucks that 
we're trying to place. So that's the k in k-means   (we have these k centers that we're trying 
to place) and the means are going to make a   little bit more sense in a moment. So that part 
isn't yet fully explained but we'll get there.   Okay so I have my food truck problem here. I know 
what I want to do: I want to try to place these   food trucks optimally and I want to assign people 
to food trucks optimally, and now I have to think   about how am I going to do that. And, again, this 
has somewhat of a similar flavor to what we've   done in the past in this course that we start 
by saying “here's an optimization problem that   represents some values that we want to learn, 
some values that we want to optimize on these   parameters of our problem, and then we think about 
how can we actually do that, how can we actually   optimize that problem, what's an algorithm 
that might help us do that.” Now, as with any   optimization problem, there are a lot of different 
ways that we can approach this, it doesn't have   to be one particular way, but it turns out 
that there's a really venerable algorithm   that just about everybody uses. If you ever do 
any clustering in your life, there's a really   good chance that you'll be doing it with this 
algorithm and it is called the k-means algorithm   so we're going to walk through what that does. 
Okay. So our k-means algorithm is going to take   two inputs. In this particular case, it's going to 
be k (essentially the number of food trucks from   what we just discussed) and tau which is basically 
a maximum number of iterations. Something we've   seen in the past a lot is that it's always good to 
have some kind of maximum iterations, if nothing   else has a fail safe, so that you don't overdo 
it and run your algorithm forever if there's a   bug or something like that or even if there's 
not a bug, some of these might just keep going.   Okay now we've kind of vacillated on whether the 
data is an input to this algorithm and so like   implicitly it certainly is here and you can make 
it explicit too and in fact the notes do that.   So let me just say: you will be inputting the data 
into this problem and let's say what is the data.   The data is the collection of feature vectors, 
of these individuals, all the individuals that   we're thinking about saying where should 
they get their meal from a food truck   and so, in particular, it's a bunch of 
feature vectors and I want to emphasize   that it is just feature vectors. So in 
the past, when we were doing regression,   when we were doing classification, the input 
to our algorithm was always a set of pairs:   a feature vector together with a label. Maybe 
it was a feature vector together with a class   label if it was classification. If I was 
doing regression, it would be a feature   vector together with some kind of real value 
label and now we're totally changing that:   we just have feature vectors. There is no label 
on those feature vectors, we just have these,   in this case, x_1, x_2. In general, it 
could be a more general feature vector. Okay so the first thing that we're going to 
do is we're going to drive our trucks into   town. So we initialize their location somehow. 
Essentially that's bringing them into the picture   so here's our trucks. They're appearing, they're 
now initialized. It's worth asking: how would we   do this in general? How would we initialize the 
food truck locations? We could say randomly.   I feel like random is one of those words that like 
always really sounds good until you actually have   to code it up and then you have to think 
to yourself what does random actually mean   and so here are two examples of what you might 
mean by random in this problem that would be very   concrete and that you could actually code up: one, 
you could choose randomly from the data points you   have. So one thing that's really important if you 
do that is to initialize without replacement. It's   worth asking yourself, once you've gone through 
this algorithm, once you've seen everything,   all the details in the algorithm: what would go 
wrong if you initialize two food trucks at the   same location? It's like a good exercise to go 
through later. Another option is you could find   the span of the data in every dimension and you 
could say uniformly at random from that span,   you could initialize the location of each truck. 
That's another perfectly fine option and that   seems to be what I've done here because these 
trucks are not on top of particular data points. Okay, so now what we're going to do is we're going 
to do our iterations for t = 1 to tau. Tau, again,   is the maximum number of iterations. 
We're going to take a few steps.   What are those steps? Well the first thing we're 
going to do is we're going to go to every person   and we're going to say what's the best food truck 
for you? What's the best choice of food truck   given the current locations of the food structure? 
One way to think about the k-means algorithm   is that it's essentially a sort of greedy 
algorithm. We're going to say, and it's a   coordinate descent algorithm if we're minimizing, 
so what we're going to do is we're going to say,   for all the people, what's the best option for 
them given the current location of the food trucks   and then, for all the food trucks, what's the best 
option for them given the current assignments of   the people and we'll go back and forth and back 
and forth doing that. Okay, so first let's go to   all the people and let's say what's the best food 
truck for them to go to? So how can we write that?   We can say that, remember y^(i) is the assignment 
of the i-th person to a food truck so that food   truck ranges over the indices j = 1 to k, and 
so we want to say what's the closest food truck   essentially? And so that's what we're doing here, 
we're just saying what is the closest food truck? And so if these are our food trucks, which I've 
now colored to hopefully encode them a little bit,   we can color all of the points that are closest 
to a particular food truck with the same color.   That's us assigning them to the same food 
truck. Now sometimes, it's a little bit hard   to distinguish different colors and so I'm also 
going to draw some lines to hopefully just help   with the visuals. So all the points that are near 
a particular truck, again, are being assigned to   that truck. You don't explicitly get lines like 
this (technically what we're constructing here is   a Voronoi diagram), you don't explicitly get them 
from k-means but you can construct them as soon as   you have the food truck locations. But again I'm 
just using that as an extra visual aspect here. Okay so now, we have an assignment of every 
person to a food truck, that's the colors   or it's distinguished by the lines, and we're 
going to take the next step in our algorithm. So the next step in our algorithm is we're 
going to say, “okay, for every food truck now   (so we just assigned every person to a 
food truck), now given those assignments,   we're going to say: what's the 
best place to put the food truck?” Okay let's think about what is the 
best place to put the food truck. I propose that it is the mean of the 
individuals assigned to that food truck.   You can actually work this out for yourself 
to show that that's true, but let me just   convince you that what I have 
written here is in fact the mean.   So what are we seeing here? So we're saying 
mu^(j) is the location of the j-th food truck.   So this should be some vector of size equal to our 
feature vector because this food truck exists in   the same space that our x's exist. What am I going 
to do here? I'm going to sum over every person   but I'm only going to count the people 
who are assigned to this food truck.   So another way you can think of this sum is 
it's actually just a sum over people who are   assigned to the food truck because we're 
zeroing out everything else and then for   everybody who is assigned to this food truck, 
we're summing up their x value, their location. And then finally, in the denominator, we're 
summing up the number of people assigned to   this food truck. We make a 1 for every person 
who is assigned to this food truck and a 0 for   every person who is not. So essentially, this is 
the number of people assigned to the food truck   and so this is the sum of locations 
of people assigned to the food truck   divided by the number of people assigned 
to the food truck and so that's just going   to be an average. And, in particular, it's 
going to be an average in every dimension   of the feature vector because the x's 
are our vectors and the mu’s are vectors. Okay so let's see what that would look like. So 
here's my current location of the food trucks.   Now I'm going to take this formula and update the 
location of the food trucks with the average of   the people who are assigned to them and so 
I'll have the food trucks move over there. And now what we're going to do is 
we're going to keep iterating on this.   So we have our iteration t and, 
again, it goes from 1 to tau. Oh there's a question I see: is the j loop 
within the 1 to n loop? And it is not and   this is an important point that we have our 
overarching t = 1 to tau and so within that,   we have two separate steps: the first step is 
assigning people to food trucks conditional on   knowing the food truck locations, then we assign 
food trucks to people conditional on knowing   the people assignments and those are just two 
separate steps and they're not within each other. Okay so now we're gonna go back to that first 
step, the assignment of people to food trucks,   because now we have new food truck locations and 
so we want to see what are the new assignments of   people to food trucks and so I'll just update 
that. A lot of these food truck locations were   somewhat similar. We're really seeing that 
the blue and the green moved quite a bit   and so we see a lot of change in 
those assignments down at the bottom.   Okay so the next thing that we're 
going to do, again, is we're going to,   with these updated assignments of people to food 
trucks, we're going to move the food trucks again.   So we'll go down here, we'll move the food trucks 
to the mean of their currently assigned people,   and we just keep doing this. So now, 
we're on the next iteration of t,   go back to the assignments of people 
to food trucks and we update that. Now we update the food truck locations. You can 
see that it was really just the blue and the green   that really moved this time down there at the 
bottom and now we update the assignments again   and now we update the food truck locations   and we update the assignments and we update the 
food truck locations and it turns out that nothing   changed this iteration: none of the assignments 
of people to food trucks changed at all. So I propose that they will never change 
again. Now that they have not changed once,   they won't change again. Do you agree with that?   How can I know? This is a question for 
the chat: does anybody have any thoughts? Okay great. So first of all, people are observing 
in the chat that once these assignments have not   changed on one iteration, they will never change 
in the future no matter how many iterations I run.   The second observation that people 
are making is that, remember,   we're choosing the assignments based on the 
means and so once the assignments are fixed,   the means are fixed, the mu^(j)’s are 
fixed, and if the mu^(j)’s don't change,   the assignments won't change because we're just 
changing them based on those means and so you   want to convince yourself that this is true 
but hopefully you'll at least go along with me   for the moment that once we have no new 
assignments, it's not going to happen again,   we're not going to get a change again. Okay and 
so for that reason, maybe we set tau to be a   100,000 just to be super safe, but maybe that's 
a big waste of our time and so we might want to   cut out early if we're not making any more changes 
and so let's change this algorithm to reflect that   and to take advantage of that. Okay so the 
first thing that we're going to do is we're   going to record what were the old assignments. So 
just before we go into changing the assignments,   let's say what were the assignments 
that we had before we made any changes.   So we're going to say y_old is the set 
of assignments of people to food trucks   before we make any changes, before 
we do that for loop over the y's,   and then we'll ask ourselves: did we make any 
changes here? All we're saying is that if y didn't   change in any way when we had our chance to update 
assignments, then let's not bother keeping going,   let's just break out and be done. Okay now 
technically speaking, there's something missing   from this right now which is there is no y to 
assign to y_old at the beginning of that for   loop and so we need to initialize y somehow. 
Now since, in this particular instantiation,   the literal only reason that you have 
an initialization of y in the beginning   is to check if anything has changed. Your first 
initialization of y should be something that would   detect that there's a change so you might, for 
instance, assign all the data points to the same   truck or something like that. You wouldn't want 
to accidentally not have a change in this case. Okay and then, finally, what we're going to do 
after we've done all of this, is we're going   to return our food truck locations, the mu^(j), 
and our assignments of individuals to food truck,   the y^(i), because that was the point: we went 
into this algorithm trying to optimize our,   in particular, what we want to do is we wanted 
to figure out where to put our food trucks   and where to tell people to go, which food truck 
should they get food from, and so we want to make   sure that we return those things. Okay so 
this is the k-means algorithm. Let's think   about what it's doing in relation to things 
that we've talked about earlier in the class. So, in particular, there are a lot of 
things I think that sound pretty similar   between k-means and classification.   Let's ask ourselves: was this classification? Did 
we need a new algorithm? Could we have just called   this classification and used one of our existing 
algorithms? Certainly something that is true   is that we assigned a label y^(i). y^(i) 
takes k different values as the k food trucks   to each feature vector x^(i). Okay so my 
questions for you, for the chat are: one,   is this classification and, two, why or why not? Okay great so we're getting some great 
observations that a big difference here   is that we didn't have any labeled data, that 
we were learning from… If you look at this   picture it looks like there are labels on all 
of our data but remember, we came up with them:   we came up with not only the assignments of 
data points to labels but the labels themselves.   Where were these trucks even located? This is 
the data that we got in to begin with. This data   has no labels and that is the difference 
between clustering and classification.   In classification, you start with labeled 
data and you learn how to label new data   based on the old labeled data. You start with 
labeled training data and then you consider,   for new test data, what are you going to do with 
it, how are you going to come up with labels,   and you use that label training data. In 
clustering, you start with no label data,   there are no labels, and you just find a pattern 
in that data, you just find these groups. Okay so big difference here between what we did 
and classification is that we didn't use any   labeled data and there's a more subtle difference 
too which is that the labels here don't have   meaning. So if I were doing classification, 
if I were doing something like saying “hey,   I'm trying to decide whether I should run outside 
and I run when it's really warm and I don't run   when it's really cold,” if you told 
me to run when it's really cold and   not run when it's really warm, I would 
have a bad time: my life would be really   annoying, I would be very uncomfortable, like my 
life would fundamentally change. In this case,   if I took my labels, my trucks, and I just 
permuted them, like I moved them around like   here I've just put different colored trucks in 
the same location, nothing has changed: I still   am feeding everybody just as well as I was 
with the previous locations of the trucks.   I could totally change all the labels, I could 
permute them, and I'd have the same result   and that's because what we're doing in this 
problem is fundamentally very different   than classification. Classification, again, 
we're trying to provide labels based on our   old labels. Here, we're just partitioning 
the data. Now a partition is a grouping   into mutually exclusive and 
exhaustive sets. That is to say,   every person is assigned to one truck and only one 
truck and so all we're trying to do here is group   the data together. We're just trying to say “Here 
are data points that go together. Here are data   points that go together. Here are data points that 
go together.” and the label is beside the point.   So this is a more subtle point but if you're 
ever trying to decide “am I doing clustering   or classification” it's helpful to think: did 
I have labeled data that I was learning from? Okay, now that all being said, I've 
kind of said a few times now what   we're doing. Let's make that formal. 
What are we actually doing here because   it's not classification? Hopefully 
we agree and have established that. So what did we do? Well, it's 
going to be something that needs   a new name because we didn't do 
it before: we clustered the data,   we did what's known as clustering. Okay what is 
clustering? It's grouping data by similarity.   So that is to say the data, we start with, a 
bunch of unlabeled data, just a bunch of feature   vectors. In this particular case, it was latitude 
and longitude of particular individuals but,   in general, it could be any feature vector: it 
doesn't have to be two-dimensional, it doesn't   have to be latitude and longitude, it doesn't 
have to have this particular interpretation. It   could be as high dimensional as you want but it's 
a bunch of feature vectors and they're unlabeled. Now we have some notion of similarity. In this 
particular case, we use something like Euclidean   distance. In other cases, we can use other notions 
of similarity. It doesn't have to be Euclidean   distance on the feature vectors or could be 
Euclidean distance on different feature vectors,   but we have some notion of similarity and we use 
that to group the data, again, to form a partition   of the data where we have a bunch of groups of 
data points where all the data points belong   together when they're in a group together. 
So that's what we did: we did clustering. And this is what we got. These 
are the clusters that we got out.   Now it's worth noting, again, we don't have to 
have this food truck interpretation when we do   clustering. In general, we could use the k-means 
algorithm or we could do k-means clustering and   come up with a set of cluster centers for more 
general feature vectors. So if I have a general   set of feature vectors that are my data, then I'm 
doing clustering on them. I can use this k-means   algorithm or some other form of k-means clustering 
to come up with a bunch of cluster centers and the   assignments of data points to clusters and, in 
fact, I don't have to have cluster centers to do   clustering: all I have to have to do clustering is 
grouping data points together. In this particular   case, I use cluster centers to help me do that but 
I don't have to to do clustering. I just have to   say, “here's the way that I'm grouping data points 
together.” Okay now let's talk a little bit about:   why bother with k-means clustering or the k-means 
algorithm? Why don't I just plot the data and see   that there are these clumps? I mean, in this case, 
it seems really natural in our example that we're   looking at where people live. They probably 
tend to live in cities. If I looked at a map,   I could probably find out where those cities 
are. Like why can't I just plot the data?   Well my first point to you is you should always 
plot your data. You absolutely should plot your   data. You should plot your data before you start 
doing any kind of machine learning algorithm and   then when you're done with your machine learning 
algorithm and your analysis, you should plot it   again to make sure it makes any sense. You always 
plot your data. Never don't plot your data. Man if   there's almost one thing that you get from this 
class, it should be to plot your data. It's just   such a useful, useful thing to do to understand 
what's going on. It'll help you understand so   many bugs. The point that I'm making now is not 
that you should not plot your data—you should   definitely plot your data—it's that sometimes 
plotting the data isn't enough on its own,   that you do still want to run some kind of 
algorithm. Okay so why would that be? One   is precision. I mean we wanted to park these 
cars, these trucks in a particular location   that really helped people. I don't know that you 
could eyeball this data and find the absolute best   location to park your trucks. You could 
probably get a reasonable approximation but   if we actually care about exactly parking 
them and really saving people's time,   then we probably want to run an algorithm 
that will find the best place to do that. Something that's worth thinking about is that if 
you have 10 million data points and you try to   plot them on one plot, you will fail: it will 
just be a big blob. It'll be a big blob that   means nothing. Now that's not insurmountable. You 
could do a heat map, you could do a density map,   there are things you can do to get around that, 
but you will have to do those things. A bigger   issue in terms of just plotting your data is high 
dimensions. So remember, we've talked about this   before in this class: as soon as you get above 
two dimensions (I mean, we as humans can only see   two dimensions. It's an unfortunate limitation 
that we have but we have to deal with it),   you should still try to plot aspects of your 
data, but when you get into high dimensions   it'll be helpful to have ways to deal with 
high dimensions to be able to use them and   you can still do that with an algorithm, in 
particular, with the algorithm we just covered.   Another big reason that we care about having 
these algorithms is high volume. Let's say that   you're Google and you want to do some clustering 
on every one of the search pages that appears to   individuals who are using your search. There is 
no way that you're going to have an individual   like an actual person look at every one of 
those and say something about it. You want an   algorithm that's going to be quick and automated 
and fast and actually get you results. Okay   so even when we're not trying to exactly place 
our trucks, we're just trying to find these   clusters in our data, it might be useful to 
have an algorithm like k-means clustering.   Okay so what we saw here was an example of what 
is known as unsupervised learning. So unsupervised   learning is any case where we don't have labeled 
data and we're trying to find patterns and this   is really distinct from, again, the types of 
supervised learning cases that we've seen before   where we do have labels and we're trying 
to do something with those labels. We're   trying to predict those labels on future 
data like classification and regression. So before I go on, I'm just going to make a little 
Venn diagram to emphasize these relationships.   So in particular, something that we just covered 
was using the k-means algorithm. I might have   a problem where I use the k-means algorithm but 
that is not the only way to do k-means clustering.   Anytime I have the k-means clustering objective 
and I'm trying to optimize it and I can optimize   it in other ways, it doesn't have to be with the 
k-means algorithm, I would still be doing k-means   clustering. Now likewise, I can do clustering 
without using k-means clustering. So for instance,   maybe I have different forms of the loss. Maybe 
it's not exactly what I talked about before like,   for instance, the reality is that people walk 
along blocks and that's actually the Manhattan   distance, that's not the Euclidean distance. 
Maybe I want to encode that somehow. If I do some   other type of loss or if I do something totally 
different (it doesn't even have to be exactly this   optimization objective that we talked about but if 
it's some way that brings my data points together   in groups), that's still clustering but it's not 
k-means cluster and it's worth noting that if I do   change the loss a little bit, it's not clear 
that you can use something like the k-means   algorithm to optimize that still. Finally, you 
can have forms of unsupervised learning that   are not clustering. For instance, suppose I have a 
bunch of data points and my data points represent   documents. Maybe they're all the documents in 
Wikipedia, maybe they're all the documents on   the web, and I want to cluster them to find the 
topics or the themes in those documents. So I   could do clustering but maybe I think that some 
of my documents have multiple topics in them,   like maybe if I look at an article from the New 
York Times and it's about the movie “Moneyball,”   well it's about arts because it's about a movie, 
it's about sports because it's about baseball,   but it's also about economics because it's 
about trading players and making money   and so in that case, I might not want clustering 
where each data point is assigned to one and only   one group. I might want something else where 
I can belong to multiple groups and so, again,   there are many forms of unsupervised 
learning that are not just clustering.   Okay so this gives us a little bit of 
a hierarchy how is this all related,   what's the the bigger picture, but now let's delve 
deeper into understanding the k-means algorithm   and what it’s doing because it is a particular 
clustering algorithm that's very popular, it's   a particular k-means clustering algorithm that's 
very popular. Tamara can I ask a quick question?   Yeah there's a super interesting question related 
to this: is there a version of k-means that   doesn't take in k as a parameter but rather learns 
it instead? Oh yes we will talk about choosing k   very shortly but it's a very natural question: 
how do you learn the number of clusters   and, unfortunately, there is not a fast 
answer to it. It turns out it really   depends on what you're trying to do but we'll 
add a little nuance to that shortly. Great. Okay before we get to that, let's talk about 
initialization of the k-means algorithm. Okay so first of all, why are we doing the 
k-means algorithm at all? Does it even work?   Does it do anything useful? So here is a result 
that actually you can prove for yourself if   you think about it for a bit. If we run the 
k-means algorithm for enough outer iterations,   so I'm talking about that big t iteration 
that's on the outside, the biggest for loop,   then this k-means algorithm will converge 
to a local minimum of the k-means objective.   So what I mean here in this particular case 
by converge is just that it will stop changing   and what I mean by local minimum is that 
if I change the mu or the y a little bit,   I'm not going to get something that's better. 
Okay so let's think about this. That sounds really   good. We get to this local optimum and, in fact, 
that's what we saw if we initialized like this.   Then we get to this optimum and, in fact, this 
is a global optimum. It turns out it looks pretty   good, right? I mean each of the trucks is in the 
middle of one of what seems to be a small town   or city and so that feels like a good assignment 
of trucks to individuals. Now the horrible truth   that I want to tell you here is that this is 
not the first random initialization that I ran.   I actually ran two other ones first before 
getting this one and so maybe we should take   a look at those and see what happens. It will 
turn out that the local minimum can be bad.   Okay so let's look at the first random 
initialization of trucks that I tried. So this   was our original one. This is the actual first 
one that I ran, just a random initialization.   What happens if we run k-means from these trucks, 
from this initialization? This is what we get. Question is: is this good? Is this bad? What 
do you think about this alternative clustering   of our data, of our people?   Is it better? Is it worse than the one that we 
saw before? This is a question for the chat. Okay there are a lot of really good responses 
here. Okay so the first one that I'm going to   highlight, the first type of response, is people 
saying it's worse and so let's notice a couple   of things here. So one, there's not really like 
a truck per cluster and so if we really think   that “hey I can kind of identify visually that 
there are roughly five clusters here, I think   that maybe a truck should be on each one and so it 
seems intuitively that we're not picking that up”   but even more than that, and I think really 
to the point, people are observing that,   one, it depends what you're trying to do which is 
always a good observation in machine learning and,   two, what we're trying to do here is we're trying 
to optimize the k-means objective. We really care   about minimizing this loss to the people who 
are using our food pantry service in this case   and so what we really care is: did the loss go 
up or down? Did that total k-means objective go   up or down and I can tell you and you can think to 
yourself why you might find this intuitive. Then,   in fact, the k-means objective is worse in this 
case. It is not as good as in the previous case   that we saw. Overall, the trucks are farther 
from the people who are using the service   and so, in that sense, and that's a very 
specific sense, this is a local optimum   not a global optimum and it is bad because 
it's not optimizing the objective as well. Okay let's try the second random initialization 
that I actually tried. So we just answered   this question, let's try the second 
initialization that I randomly tried.   So here, it is: there's 
another random initialization.   Now we're going to run k-means clustering on this 
random initialization and this is what we get. Again, I'll just tell you but you might want 
to convince yourself that this seems intuitive,   that this is a worse value 
of the objective function   because it's a higher value of the objective 
function than the first example that we saw,   the one that we saw when we were first 
introducing the k-means algorithm. Okay so the first observation here is the 
initialization can make a big difference.   If I have different initializations, I'm getting 
really different clusterings out and remember   that's all I can do with one run of the k-means 
algorithm. Once I start running it, I run it,   it stops moving eventually and then that's just 
it: it's never going to change again and that's   how I'm getting to these points. I'm getting to 
the point where it just isn't changing anymore and   it's going to get stuck in that local optimum, 
it's not going to move to a global optimum.   And so we have to think about, okay, well is 
there any way that we can deal with this? Is   there any way that we can maybe get out of this 
local optimum? Now there's a venerable idea in   machine learning that comes up anytime that you 
might get stuck in a local optimum and that is   random restarts. So the idea of random restarts, 
and it's essentially what I've done here, is I   randomly initialize. I run my algorithm (in this 
case, the k-means algorithm), I get out my output,   and then I do it again: I randomly initialize, I 
run my algorithm, I get my output and then I do   it again. I do this for some number of times. In 
this particular case, it was three: I have three   different random initializations, I run my output, 
I get my final output, and this is something   that's inherently massively parallelizable: 
you could just do this on a bunch of different   computers in parallel and so you could take 
advantage of that and now let's be concrete.   Suppose that I randomly initialized three 
separate times, I ran from every one of those   initializations, I got out for every one of those 
initializations an assignment of trucks of where   the trucks are and an assignment of individuals 
to trucks. What do I do with that? This is another   question for the chat. Suppose I did those 
three random restarts and I got the output   of the k-means algorithm every time. What do I do 
with that? What do I do with these three outputs? Okay great so lots of people are 
observing I should calculate the loss   or the objective for every one of those random 
restarts and then I should take the one with   the lowest objective because, again, our goal 
is minimizing this objective or at least that's   our stated goal and so if that is our goal, 
then what we want to do is we want to take   all of these different outputs, these different 
random restarts, we want to say, “hey, which one   is really minimizing the objective, at least 
across those random restarts?” and keep them. And at the very least, we'll know that we 
have the minimized objective across all of   the different assignments of mu and y that we had 
explored. That's not a guarantee that we'll get   the global optimum, we can still only guarantee 
a local optimum, but at least it should be better   than any particular run or at least not any worse 
than any particular run to be very precise. Now it   turns out that there are other things that you can 
do that are beyond the scope of our lecture today.   I'll just mention one of them. Oh okay so we 
just did these three random restarts, what do   I return? That's what we just talked about: we're 
going to return the one with the lowest objective.   I mean it turns out there's lots of work in 
this initialization area. One particularly   cool idea is what's known as k-means++, it's 
an initialization scheme for k-means and so   if you're looking for something cool to read 
up on, I'll definitely recommend k-means++. Okay so we've talked about initialization, 
we talked about how it can really matter,   can really change things, and you need 
to be aware of that and potentially   try to cope with that in some way. One idea 
is random restarts, one idea is k-means++. Let's notice, as in fact our question asker has 
already noticed, that k has an effect as well.   So we just saw that the initialization really has 
an effect and of course, intuitively, the choice   of k must have an effect. If we have five trucks 
at our disposal, if we have five trucks that are   available to us, then we can get them this close 
to the people who are getting the food from the   trucks but, of course, if I have a different 
k, that will get different results and so,   for instance, if I had four trucks, I just can't 
possibly get the same clustering that I did with   five trucks in this case. I, in fact, if you 
think about it, we'll just not be able to get   the trucks as close to the individuals who 
need the food and, in general, this is what   we can expect. I mean hopefully, again, this 
is intuitive that if I have fewer trucks,   I can't get the trucks as close to individuals. 
If I have more trucks, I can get them closer to   the individuals who need the food. This is why 
we would like to, if we were this agency, if we   had more money, we'd like to spend it on more 
trucks so that we can reach out to more people. So we see that the larger k, having more trucks, 
gets them closer to people or, in general,   if we're not thinking specifically about trucks 
and food service, that having more k, having more   cluster centers, having more clusters, allows 
the clusters to be smaller, to be more compact.   Okay so here's a question for you: can 
you identify, can you describe in words,   in the chat, the optimal clustering if the 
number of clusters is n? Oh sorry this should be   little n, the number of data points. I'll 
correct that in the slides afterwards,   but if we have k = n, the number of data points. 
So, in particular, in this particular case,   if I had the number of trucks equal to number of 
people, can you describe the optimal clustering? Great. So the observation is every person gets 
a truck, there is a food truck that is right   outside your home, you just pop outside, 
get your food, and head back in. It's the   best possible clustering: everything is zero 
feet or zero degrees of latitude and longitude   away from your house, so everybody gets their own 
cluster, everybody's in their own cluster, and the   objective, we'll have the distance from every 
person to every truck, is just gonna be zero.   It's hard to do better than that. Okay so now 
that we've observed that we get these different   clusterings when we have different k, let's think 
about choosing k. Now the first thing to notice   is that sometimes we just know k: we're given k by 
somebody else and so we don't need to worry about   choosing it in the sense that somebody told it to 
us. So an example was if we are working with our   food truck friends with this charity and 
the charity tells us I have five trucks,   they have five trucks, so k is 5, and that's 
the optimization problem that we're solving and   there's no more work to do in that sense. But this 
could happen in a lot of other cases. So suppose   that I am working for a web search engine and that 
search engine is interested in clustering search   results to display just a few similar results 
and maybe they only have space for four results   to be displayed on their page and so they tell 
me, “hey you gotta come up with four clusters.”   So for instance, suppose that I search for the 
word “drake.” Did I mean Drake the musician?   Did I mean drake the male duck? Did I mean Sir 
Francis Drake? Did I mean Drake University?   Who knows and so the search engine might display a 
cluster for each of these types of articles. There   would be a cluster for articles about Drake 
the musician which are all somewhat similar   on the order of articles about “drake”, there is 
a cluster of articles about Sir Francis Drake,   a cluster of articles about drake the male duck 
and a cluster of articles about Drake University   and because they only have four spaces to display 
something on a page, I mean there's only a limited   amount of screen space that you have, I know 
that k is 4. Similarly, I might be clustering   customers into groups and I'm going to assign 
a staff member in my business to each one of my   clusters and so maybe I have k staff members 
or the budget for k staff members. Maybe I'm   clustering books on k bookshelves and so I know 
that I have k. Okay but the observation that the   previous question made which is that sometimes 
we'd like to choose or learn k and so, maybe,   actually what happened is I was looking at a 
bunch of topics, again, a bunch of documents   that are on the internet, and I want to know how 
many topics are in those documents. Maybe I'm   looking at a bunch of animals and I want to know, 
I have a bunch of observations of animals, and I   want to know how many little groupings of those 
animals are there. Maybe I'm working with this   food truck service and they want to know how many 
trucks they should buy, how many trucks that they   should deploy and so, in all those cases, there's 
some sense that we'd like to choose or learn k but   it's worth noting that those are actually pretty 
different use cases that I've just described. So the first observation is that 
in none of these cases can we just   minimize the k-means objective over k. So let 
me first be explicit about what I mean by that.   So here is our k-means objective from before. 
Again, it's just the sum over all the individuals,   of their loss, so it, in general, it's this sum of 
square Euclidean distances. In our truck example,   it's the sum of the squared Euclidean distances of 
individuals to their nearest truck or to a truck   in this case but we want to find the nearest. 
What we had previously done is we tried to   minimize this over the assignments of individuals 
to trucks or, in general, the assignments of data   points to clusters and the cluster centers or, in 
our particular example, the truck locations and so   now what I'm proposing is: why don't we also just 
minimize over k? And this is a real question for   you in the chat again: why don't I just minimize 
over k? Why don't I just add that as a parameter   in my optimization problem or should I add 
that as a parameter in my optimization problem? Okay great, so lots of people are observing 
that if I do this, I can already tell you   the k that's going to optimize this 
objective, it's k = n, because remember,   on the previous slide, we observed that when we 
choose k = n where n is the number of data points,   we can set y and mu such that this objective 
is zero. Notice that this objective is   always non-negative and so zero is a global 
optimum and so this is optimized at k = n.   But k = n is not interesting, like that's not why 
we did clustering right? I mean all that does is   it says, “hey look here are my data points” but 
we want to find actual structure in the data,   we want to find something that's useful that could 
tell people where to put their trucks. Nobody's   gonna buy one truck for every single person 
and so we're going to need to rethink this,   we're going to need to think of a 
better way to approach choosing k. And of course, as is true for everything in 
machine learning, it really depends what you   want to do. So let me at least name two different 
types of problems that we've talked about here.   So one type of problem there's a cost 
benefit tradeoff. Let's say I'm working   for, again, this charity that distributes 
food trucks and I want to decide: how many   food trucks should we have? Well, in some sense, 
there's a cost to having fewer food trucks which   is that people have to walk farther, it's harder 
for them to get food, but there's also a cost to   having more food trucks. There is literally a cost 
for the truck and for maintaining it and so on   and so, very roughly, something that we could 
do in this case is, if we could quantify   the cost of having more food trucks, we could 
actually put that into our objectives. So that's   what I've done here: I've said, “hey, there's a 
cost to not having enough food trucks and there's   a cost to having more food trucks, maybe we 
can trade that off explicitly an objective   or something like this.” Similarly, with the 
web search example presumably there's a cost   to having a larger page of search results and 
that people tend to not want to scroll too far   and so maybe we could trade that off explicitly. 
If I'm a bookstore owner, there's a cost to having   more space in my bookstore and more bookshelves. 
If I'm a customer service representative and I   want to group my customers in order to assign 
staff to them, there's a cost to having more   staff. So in this case, if we can, we'd like to 
explicitly represent that cost benefit trade-off.   Now in other cases like we're trying to 
find the topics in a set of documents,   we're trying to find a grouping of animals, 
that's not a case where there's a clear   cost-benefit trade-off but it's also worth noting 
that there isn't necessarily a single right answer   to the number of clusters. Like let's say that I 
made a bunch of observations of animals in some   environment and I want to cluster them. Well 
if I cluster on one level, I might get the   species. If I cluster on another level, I might 
get the genuses. If I cluster on another level,   I might get the kingdoms and so it's not clear 
that there's this one right answer to clustering   and so is there any reason that we should expect 
that an algorithm would find that? So this isn't   a clear cut and dry answer to how do we choose 
k in part because there is no clear cut and dry   answer how do we choose k, it really depends on 
what you're trying to do and at least this gives   you a little bit of a flavor of the pitfalls 
that you could encounter in trying to choose k. Okay speaking of pitfalls, let's talk about some 
other ones that can arise with k-means clustering. So k-means, I'm going to observe 
and then I'm going to show you,   k-means works well if I have well-separated 
circular clusters of the same size. So I'm   going to break down each of the 
elements in this observation. So first, let's just notice that these are well 
separated clusters. So I'm just using our visual   intuition here that we can kind of identify 
the clusters in cases like this and so here   we think that there are probably two clusters. 
It's unlabeled data but I think you can kind   of identify what they would be just intuitively. 
They're well separated: they're not right on top   of each other. They're circular roughly, the data 
is roughly within some kind of radius in each case   and they're roughly the same size both in 
terms of radius and, I'll tell you right now,   they have the exact same number of data points in 
each cluster. If I run k-means on this, at least   I run it maybe with multiple random restarts and I 
do all this stuff, this is the optimum assignment   of the cluster centers and the assignments of data 
points to clusters. This is what I'm going to get.   That seems pretty much in line with what we were 
talking about earlier with our truck example,   with our food trucks, where we were trying to 
assign the food trucks, we're trying to assign   people to food trucks. We ended up seeing 
that we got, our cluster centers, were in   the center of the clusters and the groupings that 
we got made sense. When we looked at that picture   with all the data, maybe it was a bunch of 
where people live and in some square-shaped   state like Colorado or Wyoming, and we saw that 
roughly we picked up the cities or the towns,   the groupings of people, and it seems like we 
have an intuition that that's what we'd like to do   but let's notice that our intuition about what 
clustering should do is not necessarily what   k-means is going to do and it's worth separating 
those two things out. k-means is always going to   assign your cluster centers as though they were 
trucks and trying to find the optimal placement of   trucks for serving individuals who need food. That 
is not necessarily the same as something that kind   of looks like a cluster to us and so that's what 
we're going to spend just a moment on now. Okay,   first let's talk about “same size”. Now there are 
two different ways, at least two different ways,   that clusters might not be the same size. So the 
first is radius, so let's think about radius. So here, again, I have two what I 
might intuitively call clusters. Maybe,   before we were thinking about our two features 
that we were plotting as latitude and longitude,   maybe now let's think about them as we gathered 
a bunch of animals from the environment and   these are height and weight. So maybe maybe the 
horizontal axis is height and the vertical axis   is weight and if I gather this information, you 
might think “hey maybe there are two species here:   there's one that has a lot of variety 
within the species, it's pretty spread out,   and then there's one that's pretty concentrated 
and maybe I would like to pick up those two   species.” So we can ask ourselves: that's maybe 
what I would like to do, but what does k-means do?   So I want you to think for a second 
to yourself. You don't have to write   it in the chat but I want you to to think to 
yourself what you think is going to happen here. Okay now what I'm going to do is I'm going 
to show you the results of running k-means.   Probably the best thing that you could ever do 
is play around with k-means later: try it out,   see what happens, try out some examples 
that you're not sure what's going to happen,   and run them. k-means is basically programmed 
just about everywhere, it's very easy to get code,   it's very easy to write your own code. I mean 
we basically just wrote the pseudocode before   and so just like everything it's 
probably best to just try it out,   but here's what we're going to get. Okay. 
So the observation here, first of all,   is that there's a lot of data in each cluster 
and so the centers are roughly in the center   of each cluster as we expect, they are the means, 
the k-means, the two means. In this case, k = 2. Now remember: we assign data points to 
clusters based on which is the closest cluster.   So there's basically going to be a line down 
the center here between the two cluster means.   Everything on the left goes to the left 
cluster, it means everything on the right   goes to a right cluster and so because this 
right what we might think of as a cluster   overlaps with that dividing line, some 
of it gets assigned to the left cluster.   Again, whether this is good or bad, and I 
think people have rightly observed this in   some of our Q&A throughout this lecture, 
it depends on what you're trying to do:   if you're trying to assign trucks so that 
they are as close as possible according to   the objective we described earlier to the people 
that they are giving food to, this really is the   optimal thing to do, it optimizes the objective. 
If you're trying to discover that there were two   species here and you wanted everything in one 
clump to go together and everything in the other   clump to go together, this is not doing that 
and it's worth being aware that that's true. Okay now this was one notion of "same size". Let's 
talk about another notion of "same size". So here,   the two clusters were not the same size because 
they had different radiuses: one was bigger than   the other. Here, I've made two clusters where 
one has way more data points than the other.   So it turns out I generated these both from what's 
known as a Gaussian distribution. So they have   the same generation scheme, it just turns out 
when you have so many more data points in one,   it also gets a little bit bigger but hopefully 
you'll see that the point that I'm about to make   really depends on the number of data points more 
so than the exact shape. Okay so, again, I want   you to just take a moment and think to yourself. 
Again, you don't have to tell me, but you should   tell yourself, commit to yourself, what do you 
think might happen when we run k-means here. Okay now I'm going to tell you. Incidentally, 
this is a great exercise to do on your own   later when you're running k-means: ask 
yourself beforehand:what do you think   is going to happen and then run. Here's what 
we get. Now again, if what I'm trying to do   is optimize my food trucks so that they serve 
people in the best way possible and I minimize   the loss of people, this is the optimal thing to 
do: I reach more people more closely by having two   food trucks in the absolutely giant cluster, 
by putting two food trucks in the big city,   rather than one in the very small town. If what 
I was trying to do though was to observe that   there are two different species here and I 
massively over sampled one of the species,   maybe without knowing it, I mean, because I 
didn't have the labels to begin with, but maybe   one of them is just really, really easy to find 
in the environment and one's relatively rare,   I'm not going to find those clusters with 
k-means, I'm going to find this instead.   So again, is this good or bad? It 
depends on what you're trying to do. Okay so that's two different 
notions of “same size”.   Let's talk about “circular”. Okay so 
now, something that we talked about   way back in lecture three, we were talking 
about features, is that feature encoding matters   and, in particular, the scale of your features 
matters. So one way that you could create   the plot that I have right here is that I could 
take the original height and weight measurements   from way back when I had those two perfect round 
clusters that were the same size with the same   radius and the same number of data points and I 
could just change the scale of the vertical axis,   like maybe I measure length in feet instead of 
meters or maybe I measure weight (if it's weight),   I measure it in kilograms instead of grams or 
kilograms instead of pounds. This is a very easy   thing to change the scale without thinking about 
it and this is what I would get. I mean these are   clusters that would be circular 
if I had a different scale   but here they don't look circular, they're very 
spread out because I have a spread out scale. Okay again, it's worth thinking what might 
k-means do here. Just take a second for that.   Again, just to yourself, no need to tell 
me but think about it for just a second.   Well I'll tell you that this one is worth playing 
around with yourself because it really depends how   stretched out the clusters are in the vertical 
direction what you're going to get, but in this   particular case, these are the means returned by 
k-means. Again, if I'm trying to optimize my food   trucks, this is great, it turns out to be the best 
thing to do, but if I was trying to discover that   these were these two totally separated different 
species, I would not be discovering that here.   Now an interesting metapoint, remember one of 
the things that we talked about to deal with   different scales and different features, is to do 
some kind of standardization and that might help   you quite a bit here but it also might not totally 
solve this problem. If you're just saying, “hey,   I should standardize my features in such a way 
that maybe I have a particular range of my data   and I always want to make that between 0 and 1,” I 
could actually end up getting something like this   and so it's just worth keeping in mind. Okay so 
this is a case where we get away from circular   clusters. What about well separated? So here, 
I'm not just showing you the data to begin with,   I'm showing you what if we had secret 
knowledge that, in fact, there are two species   that we're about to observe, it just so happens 
that if we only look at their height and weight,   they actually have pretty similar values and 
so they're really going to overlap quite a lot   and so if we were to run k-means, what we 
would see is not our secret information   about having two species, we would see 
this data where there's a lot of overlap   and then, of course, if we run k-means it 
can't distinguish that overlap. Some of the   things we've talked about these examples with the 
circular clusters and the same size are somewhat   specific to k-means. We're making this specific 
assumption about what objective we care about,   we're making a specific assumption about how 
we're going to optimize that objective, but   really these are specific to the objective. This 
one is more a point about clustering in general   that sometimes what you would like to distinguish 
maybe isn't distinguishable in your data. If there   is no separation in your data, you can't expect 
that an algorithm is going to get around that,   that an algorithm is going to find that. It's 
similar to what we talked about previously with   classification: if you try a linear classifier and 
there's just no good linear classifier, there's   no way to distinguish classes in your data, you 
can't expect that an algorithm is going to solve   that for you. It may be that you just need to 
go and you need to get better data, you need to   go back to the drawing board, you need to think 
about what information you have at your disposal. Okay so at this point, we've talked about   k-means clustering in some depth: we've seen 
that there are things that can do really well,   there are definitely limitations, and I think 
that this is not specific to k-means clustering,   it's not specific to clustering, it's not specific 
to supervised learning, it's a very general   set of observations about machine learning that 
this is true and hopefully it's something that   we've really conveyed in this class that these 
can be very useful and powerful tools but they   also have limitations and it's worth knowing those 
limitations when you're using them in practice. Okay so I'd like to revisit my very first 
slide in this class, it was “Machine learning:   why & what.” So remember, way back in lecture one, 
we talked about machine learning is in the news   all the time. At the time I just looked up some of 
recent articles and I found a bunch. I'm sure that   if we just did this exercise again, if we looked 
in Google News, we would find all kinds of new   machine learning articles. I happen to know 
there are some big advances on a biological   side that happened recently, there's just all 
kinds of things in the news. And so my hope   is that in taking this class and then going over 
everything that we've done in this class that you   are better able to read this news, to engage 
with this, and also to make the news yourself   by using machine learning tools. We talked about, 
in this class, what is machine learning and in the   very beginning we had this big idea that it was 
a set of methods for making decisions from data.   We said see the rest of the course. Hopefully you 
have a better sense of what that means now that   we've gone into so much more detail than we had in 
the first lecture. We've covered so much in some   sense in this course: remember we talked about 
classification with a ton of different models,   logistic regression was one of them but there 
were many others, we talked about regression,   we talked about linear regression, and again 
many other models. For both classification and   regression, we talked about neural network style 
models: we talked about convolutional neural nets,   recurrent neural nets, also totally different 
models that are also very useful like decision   trees, random forests, and now, on top of all 
of that supervised learning work, we've added in   unsupervised learning. We've seen clustering as 
an example, we've gotten a sense that there's   a lot more beyond that, beyond just clustering. 
We've talked about not just all these different   models and things that we can do but also how to 
learn within them, what kinds of algorithms can   we use. We talked about greedy algorithms like 
perceptron way back in the beginning. We didn't   stop, we didn't say, “hey, we'll never touch a 
greedy algorithm again.” We use them to build   trees. In some sense, we've even done k-means 
clustering now that has a greedy type flavor.   We also talked about gradient descent, 
stochastic gradient descent. We talked   about local optima in all of these cases being 
an issue. In general, this is a real challenge   in optimization and how sometimes we have these 
nice convex problems but sometimes we don't and   that's a challenge and how can we overcome that. 
We've even talked about that today but we've also   talked about that in other contexts before 
today. Even beyond all this, we've also talked   about interacting with the world: we've talked 
about state machines, Markov decision processes,   reinforcement learning, and we've also talked 
about, and I think this is really important,   that machine learning isn't just a set of 
algorithms. It's really a whole pipeline that   you need to be really careful about: we've talked 
about choosing features, we had a little lecture   on choosing features and hopefully emphasizing 
that it's pretty non-trivial in practice. Like   those pre-processing steps before you run your 
algorithm really, really can matter. We've talked   about initializing a lot. Here we talked about 
it today but we've also talked about it in other   cases in this class and plotting your data. 
We've done a lot of plotting your data but I   want to call that out as something that is 
really useful for you to do and practice,   you can catch so many issues that way. You can 
catch not only so many issues beforehand, but also   after the fact to try to understand what are you 
getting out from your output, what does it mean,   what's going on there. It's really worth asking 
the tough questions about what you're doing. We've   seen other aspects of the pipeline like ensembling 
methods to help predictive performance, to get   gains in that way, things that you might want 
to do that aren't purely predictive performance   like interpret what's going on with your machine 
learning algorithm. We've talked about evaluation,   cross validation is a big example. We talked 
about cross validation versus using pure test or   validation data and even though we spent so much 
time in this class on all of these topics, there's   a very real sense in which they're all the tip of 
the iceberg. There's so much to machine learning   that we've covered and there's so much that we 
haven't covered, there's so much else to learn,   and I think there are various ways that you can go 
about doing that. You can go and take more machine   learning courses. You can go and take courses that 
use machine learning and apply them to different   ideas, to different application areas. It's really 
interesting the things that people do in different   application areas. Certain methods tend to 
be relevant to different applications and it   really depends what area you're working on, which 
methods might be particularly relevant to you,   but also you can learn so much just by using 
these methods, by carefully interrogating them, by   plotting what's going on, by trying them out. It's 
not all necessarily just about taking classes but   actually getting some real world feel by working 
on problems, by getting a sense of what happens. I   mean you really learn by doing and I think that's 
just a great idea to engage in that kind of thing.   So remember, we said in the beginning: why study 
machine learning? This is like just the exact   same slide that I had when I started. To apply 
it. Of course that's going to be a big issue is   that you want to apply it and you want to apply 
it carefully. I think a really important thing   that we've spent a lot of time on this course is 
not just what machine learning can do but what   it can't do: what are the limitations? Where are 
the places where it might be inappropriate, where   you might come up against some kind of barrier 
or maybe machine learning just isn't the right   tool for your problem or maybe there are things 
that you have to do before you can apply machine   learning successfully and so I hope that you have 
a better sense of all of those things. But I also   think it's really important to understand machine 
learning for the purposes of understanding it   because, again, it's influencing so much of 
our lives these days. It's used, increasingly,   in ways that affect all of our lives: in cyber 
security and personal finance and health and   all these different areas and so it's really 
important to be able to engage in a way that   has a deep understanding of what it can and can't 
do because I think if you don't have that, it's   easy to exaggerate in both directions, to think 
that it can be magic, that it can do everything,   or to think that it's horrible and doesn't do 
anything well and it's evil and so you want   to have that nuanced understanding to know when 
is it doing something good and what is it doing   something bad and to evaluate, to understand when 
people have a discourse about machine learning,   what are they talking about, what does it 
really mean. It is entirely plausible that   some of you could be applying this not just in 
your regular lives but that it could come up   maybe in a jury trial or that you're interfacing 
with the medical establishment in some way and it   comes up somehow and you want to be able to engage 
with that and to be able to say something about   it. I think that matters and hopefully at this 
point, it's much more clear, not just because   we're telling you, but because you've experienced 
it yourself that machine learning is not magic.   It's not going to find a signal where you can't 
find a signal, it's not going to solve all of   your problems, but it might solve some of your 
problems and hopefully you have a sense of how you   can do that. Also it is very much built on math. 
Hopefully you've gotten the taste of the math   behind it and I think if you're mathematically 
inclined, there's a lot of really great math   courses that you can take to get to a point where 
you can understand even more machine learning   and maybe innovate in different ways yourself 
in machine learning. So before we wrap up on   my part of the lectures, I just want to give 
a huge shout out to the amazing staff in this   course. It's just been a huge pleasure working 
with our absolutely fantastic instructors TAs   and LAs. I think, hopefully, all of you have had 
this experience as well, they're just so wonderful   and I've really enjoyed working with all of 
them and I want to give a huge shout out to you:   I think it's just been really fantastic working 
with you all this semester. I've so much enjoyed   how much all of you have participated in the 
Q&A, how much you've been very engaged and your   questions on Discourse and then your questions, 
in general, in the class have been just fantastic,   I'm so glad that you're asking them. I hope 
that you realize that this lecture and the   questions on Discourse and all these 
things: there is no penalty to asking,   to asking about anything, to trying questions 
and trying answers that maybe aren't just right,   but we learn by doing that and I think it's 
wonderful that you've engaged in that way   and then finally I want to plug. Next week we're 
going to have a guest lecture by the fantastic   professor, David Sontag. He's one of the 
instructors in this course and he's going to   be talking about machine learning for healthcare 
which I think is just absolutely fascinating and   ties into all the things that we've been 
talking about and so I hope that you will   will be there next week. Thank you very 
much and I hope you have a great week. 

Great okay. So welcome to lecture 14. I am David Sontag. I am one of the faculty instructors for this
course and for this last lecture of the semester I'll be the one who will be presenting and
it'll be a lecture about machine learning in healthcare. As in previous lectures, you may ask questions
either via the chat or via Discourse. We highly recommend Discourse because I myself
won't be able to monitor the chat other than at a couple of choice moments when I ask you
specific questions and I will be monitoring the chat at those instances to see what your
responses are. Otherwise, please ask in Discourse and one
of the core staff will unmute themselves and ask me those questions. Before I kick off the actual material for
today's lecture, I want to just remind us of a couple of announcements. First, the subject evaluations are now open
and we have posted a link to the evaluations on the course website. Please complete them by December 14th and
this course will be offered again in a virtual format next semester and given that this is
still a very new format for us, your feedback is going to be extremely valuable for the
next semester's course staff. Secondly, we'd like to emphasize that all
course extensions must be completed by Wednesday the 9th. That's less than one week from the normal
due date but that's important because MIT rules do not allow us to have anything later
than the last day of class be due. So I'm a computer scientist and over the last
10 years, I have pivoted my research from pure machine learning research to machine
learning research which looks at questions motivated by healthcare. And the reason why it's so exciting to be
a computer scientist working in the health space now is that there's a wealth of digital
health data available. That data ranges from unstructured clinical
notes to imaging data, laboratory tests, vital signs, more recently, includes data like proteomics
and genomics and even things that we don't traditionally think about as health data but
which give us a lens on a patient's health data like mobile activity information or data
from a Fitbit type device and the fact that this data is now in digital form means that
it gives us an opportunity to use machine learning algorithms both to learn from the
retrospective data and to deploy interventions such as clinical decision support on top of
that digital data at that point in care. Health data lives in a variety of different
settings and it's really important as we think about how we're going to tackle a few different
scenarios that we'll discuss in this lecture today: where that data might come from. For example, one major source of health data
is in providers or hospital systems. So for example, many of you students go to
MIT Medical. MIT Medical has an electronic medical record
and across the United States, the adoption of electronic medical records has increased
dramatically since 2008 from 9.4% to well over 85% today. That data consists of some of the aspects
that I mentioned earlier such as clinical notes and imaging data. Another major source of health data consists
of data that's being collected by health insurance companies and you can see in this diagram
here where I show you the three major players in healthcare infrastructure (providers, payers,
and patients) that their relationships between them data passes back and forth. So for example, your doctor, when they want
to get paid for a service performed for a patient, they send a bill to the health insurance
company. So for example, if you're a MIT student, your
health insurance company is probably Blue Cross Blue Shield if you're covered by the
MIT plan, otherwise it might be by whatever your parent's plan is. Now that bill actually has a lot of valuable
information in it about what happened during that visit. So for example, here I'm showing a concrete
instantiation of one such bill and you see that.... I'm trying to see how I can get my mouse to
show. I need to change the settings. One moment. [TA]: You can click on the pencil icon in
the bottom left and then choose laser. It's the gray icon. [DS]: Thank you. Can folks see my mouse now? [TA]: Yes we can see your mouse. If you use the laser pointer, it's a bright
red dot and it will be a little better yeah but the mouse is fine. [DS]: I can't find the laser pointer option
unfortunately. Oh I see it now. [TA]: A laser up there, not that one. [DS]: Yes, great, thank you very much. All right so over here, this is an example
of a bill that's sent from your doctor to your health insurance company to get paid
for the services they provide. You see that there are what are called diagnosis
codes that refer to the reasons for performing procedures. For example, here there's a code that denotes
diabetes, another code that denotes a heart condition called coronary artery disease. In addition, there are several different lines
corresponding to different procedures that were performed. Each procedure has a number associated to
it, so this is standardized, and, of course, a cost and on the right hand side here, you
have an ID for the provider that performed that service. The fact that there's been such a wealth of
digital health data available has been taken note of by industry and we see all of the
major tech players now (places like Apple, Google, Amazon) all getting involved in the
healthcare industry. So for example, Apple recently launched, as
part of their Apple Watch, a machine learning based product which can read a patient's EKG. So it's measuring a patient's heart through
a sensor on the watch and can diagnose a number of heart conditions. Google has launched a number of products that
are based on computer vision. So for example, they have one which can help
diagnose diabetic retinopathy which is a condition caused by diabetes and which goes undiagnosed
in many parts of the world due to not having sufficient number of doctors who can read
those images. Amazon has released products on their cloud
platform which allows hospital systems to upload their clinical notes and it'll perform
a number of natural language processing on top of those clinical notes to help with,
for example, that type of billing that I showed you earlier: which diagnosis codes to submit
to health insurance companies. There are also a number of startups working
in this area. So for example, a couple of my favorite startups
are looking at how one can change pathology or medical imaging by developing devices that
can be used by less trained physicians (for example, nurses instead of trained technicians)
to do, for example, imaging of the heart and this is going to be very impactful for starting
to move medical care away from hospital settings and closer to patients and furthermore, so
really starting to fill in the gap in many places in the world where there simply aren't
enough trained technicians. And these startups are ranging from a number
of different fields such as those imaging fields that I mentioned but also areas like
mental health and improving communication between patients and providers and improving
quality management and so on. So in today's lecture, I'm going to be doing
a number of brief case studies where we're going to dive into a few of these application
areas of machine learning healthcare from the lens of the material that you learned
from lectures 1 through 13. I'll start out with an example of supervised
machine learning for risk stratification. Then, and that'll be the vast majority of
the lecture, then I'll talk about a use of unsupervised learning, in particular k-means
clustering that you saw in last week's lecture, and I'll show how one can use that to discover
subtypes from asthma which you might recall was one of the optional questions in the lab
last week. Finally, I'll give an example of reinforcement
learning in healthcare and show how one can use that to better manage patients with septic
shock in hospital environments and I'll close with a couple of comments about what makes
healthcare different from many other application areas of machine learning and also some suggestions
for future courses you could take to further learn about machine learning here at MIT. So let's start with the following scenario:
as I imagine many of you have been tracking, we have been getting very close to having
a vaccine approved in the United States for coronavirus and over the next couple of months,
there is going to be an enormous effort to get our population vaccinated. But in the first couple of months, there won't
be enough vaccines for everyone to be vaccinated and so a number of ongoing efforts are trying
to think through what population should one vaccinate. Questions about: should one vaccinate first
health care workers? Frontline workers? Elderly? And so on. And I'm not going to focus on the question
here of who should get vaccinated but I want us to be thinking about: once that decision
has been made, what can we best do to achieve that goal? So in particular, let's think about an elderly
population. Almost all of the current guidelines are going
to prioritize individuals above the age of 65 very soon. In the next couple of months, they will be
eligible for getting a vaccine. But not all of them will know or prioritize
getting the vaccine themselves despite the fact that they're eligible to get it. So we can imagine that the government might
start a service where they are going to have a team of call workers that are going to be
picking up the phone and calling elderly individuals who are most at risk of having complications
due to Covid and working with those individuals to try to get them to a local clinic where
they could be vaccinated. You could also imagine having nurses sent
to patients homes to do vaccinations at home in some cases but one of the major challenges... Give me one moment. One of the main major challenges in doing
such outreach is that it's very costly and one might only have funding in order to, for
example, send nurses to a certain number of individuals homes or facilities and that leads
to a really important question: how should one prioritize those resources? So for example, one question, one way of trying
to frame this where machine learning might be able to provide some light is that we could
ask: who are the individuals who are most likely to have poor outcomes were they to
be infected with Covid, with the coronavirus? Now it's very difficult to try to think through
the question of who's likely to have poor outcomes from a machine learning perspective
because we have poor data on the base denominator: who actually was infected. However, we do have very good data on who
was hospitalized and a slight reformulation of this question in order to try to tackle
it with machine learning is to ask not about who has poor outcomes of those infected but,
of those who are hospitalized of which only a small fraction of them end up having poor
outcomes, which of those end up having poor outcomes such as admission to intensive care
unit or death? And this, one can start to try to tackle with
data that we have because there are very good records of who's been hospitalized and who
has poor outcomes among those who have been hospitalized. So this question has been studied by a number
of different authors already and, for example, recent work from New York City, from Korea,
from California, have looked at questions such as, of patients who've been hospitalized,
what factors, whether it be past medical history or factors that patients present with when
they're hospitalized, lead to those poor outcomes? And a number of different factors were surfaced. For example, older age, and that's one of
the reasons why individuals over the age of 65 are being prioritized, past conditions
such as hypertension which means high blood pressure, obesity, diabetes, and so on. So one approach that one might take is we're
going to use machine learning algorithm to try to predict these poor outcomes from electronic
medical records and then prioritize individuals based on those predictions. But the challenge with using that approach
is that electronic medical records are very siloed and so such a machine learning algorithm
would have to be deployed de novo at every single institution which is going to be very
slow, expensive, and unlikely to be possibly rolled out in a short period of time and so
this is where, in this hypothetical scenario, one of the most important machine learning
questions comes to mind, which is: what data do you have available and how can you make
use of that data despite the fact that it might be very imperfect? And so what I'm going to do is I'm going to
walk you through a hypothetical scenario where we think through what data to use, how we
would process it, and how we would evaluate the result or try to tackle this problem. So I'm going to propose that what we should
be using for this is data that's available at a national scale. In particular, health insurance claims. Remember I told you that providers have to
submit bills to their health insurance company in order to get paid and I'll show you an
example of what those bills look like. Well the biggest health insurer in the United
States (in particular, the one which is most relevant to elderly individuals) is the Center
for Medicare and Medicaid Services and CMS has data on most individuals, most Americans
over the age of 65. Whether you're poor, whether you're rich,
almost all individuals in the United States are covered by Medicare once they reach a
certain age and, thus, if one thinks about how one could very quickly both train and
deploy a machine learning algorithm that could stratify at a national scale that data is
perhaps where we should be looking. So here's the hypothetical study design which
I want us to think through as part of this. Let's take all data from patients hospitalized
with COVID-19 from March to July in the United States, data that's available from Center
for Medicare and Medicaid services. We're going to extract features from the data
pre-hospitalization and we're going to attempt to predict whether our patients have poor
outcomes during hospitalizations. In particular, we'll design a binary classification
task where we have a label y which is 1 if the individual is admitted to the intensive
care unit or expires (dies) during the hospitalization and 0 if they are discharged alive without
having had to be admitted to an intensive care unit. And again, as an important disclaimer, I have
not done the study that I'm proposing here. This is just a thought exercise for us thinking
about how we would try to tackle an important problem today using machine learning. So let's now start thinking about how one
would try to derive features from this data set. Now I just said that the labels are going
to be derived from that hospitalization and we're going to be focusing on just patients
who have been hospitalized with Covid-19. That should say 19 not 10. Well we could go back in time to pre-hospitalization
and look at all the types of data that would be available to the health insurance company. In particular, as I mentioned, every single
visit with a health provider is going to have some records to it. So if the patient has gone to their primary
care physician and they have high blood pressure noted as a diagnosis code, we will have a
record of that. If the patient goes to CVS pharmacy and fills
a prescription for high blood pressure, we will have a record of that. If the patient has a telemedicine visit with
their dermatologist, we will have a record of that. So the first question that I want us to be
thinking about now is: how do we take this time series data and try to construct a feature
vector from it? And notably, you can see here that some of
the factors which have been discovered in earlier work in, for example, using electronic
medical records do show up here. We mentioned how hypertension is a factor
which seems to be predictive of poor outcomes and we see that hypertension which is high
blood pressure is recorded in previous patient visits so that's something that we might be
able to distill from the data. But if you dig deeper into the literature,
you'll see that it's not just the patient having hypertension which is predictive. In fact, it's having complicated or uncontrolled
hypertension meaning the patient is not being well managed for their hypertension. If their blood pressure is high despite the
fact that they are on medications or if they're not taking the right medications, those are
the patients who tend to do poorly and, importantly, you have that information here. So you have records of whether medical patients
are on medications, you have records of how often they're seeing physicians, and although
we don't have direct measurements of whether the patient's hypertension is being controlled
appropriately or not, one might be able to try to pull that out of this data via a algorithm
that can detect very subtle signal and high dimensional data such as a machine learning
algorithm. So to dig a little bit more into the data,
here's what it actually looks like and what I'm showing you is a patient timeline and
I'm showing you, with each bar here, when a data element is observed in the data for
this hypothetical patient. So I told you that we have medical claims,
with those medical claims we have these diagnosis codes. Diagnosis codes are encoded in an ontology
known as ICD-10, this is a hierarchy where the top-level hierarchy denotes, for example,
neoplasms for cancers. There are another set of codes corresponding
to diseases of the blood, another set of codes corresponding to endocrine disorders like
diabetes, another set of codes corresponding to mental health issues and if you dive deep
into this hierarchy, you'll see that it is extremely detailed: we're getting really rich
information about the diagnosis, the past diagnosis of patients. For example, there's even a diagnosis code
for “bitten by a turtle” or “bitten by a sea lion”, “struck by a macaw”. These are some of the more rare codes but
it gives you a sense of the richness of the data that could be encoded in these health
insurance claims. We also know for every visit what specialist
the patient visited: was it a primary care doctor? Was it an endocrinologist? A dermatologist? And we know where the visit was. Now I talked about data being derived from
pharmacy records and these medications will have records of a code for the specific medication
(that's called the national drug code or NDC code). If you take a medication off of your medication
cabinet and you look at the box, you'll see that NDC code listed in the box and so every
medication has a unique identifier which allows us to think about how to construct features
from the presence of a medication in a patient's medical record. We know the number of days supplied and we
know the provider who prescribed it and when. So a traditional approach to feature construction
might take some of the factors that earlier work using electronic medical records had
shown already to be predictive. You might take the patient's age, their gender,
you might try to derive from all of the data that I showed you whether the patient has
hypertension, for example, looking at a set of hypertensive codes, looking at medications
that are used to treat hypertension, and saying if any of this set of codes or any of the
set of medications are present, then the patient has hypertension, yes or no, 1 or 0. You could do the same for other conditions
like diabetes, cancer, and so on and construct a feature vector of 20 or 30 or 40 different
features which are risk factors that were conjectured to be predictive of patients having
poor outcomes when hospitalized with COVID-19. So this would be a traditional approach to
feature construction and one of the major challenges with these types of traditional
approaches is, first, that they're very time consuming to create. So that notion of coming up with the list
of things and then seeing how does one derive whether a patient actually has hypertension
by “what is the set of diagnoses supposed to look at”, “what is the set of medications
to look at.” That takes a lot of time from a data scientist
or from a domain expert. The second challenge with these more traditional
approaches is that they miss a lot of the subtle detail around the patient's condition. So I spoke about how it's not just whether
the patient has hypertension or high blood pressure that's predictive, but whether it's
not being managed appropriately and that is something which is really, really hard to
derive from the patient's data manually. There's simply no good characterization that
works well for large numbers of patients. So the approach that we're going to take in
this hypothetical example is one which is more of a black box machine learning approach. And although this is a hypothetical example,
this is one which I have used very similar approaches on exactly the same sort of data
for a number of other non-COVID related conditions and so I know that this type of approach works. So the analogy that we're going to be drawing
is just something that we've seen throughout the course so far when we've talked about,
for example, text classification. We said, “how would we take a text document
which has a number of words and, for example, learn a classifier to label an email as spam
or not spam?” And we said that a very simple approach to
try to do that might be to create one feature for every word in the vocabulary with an indicator
of whether that feature is 1 or 0 which tells us “did that word appear in the email or
not?” Now for this type of data, I'm going to recommend
a very similar approach to extracting features from the patient's longitudinal health records:
we're going to first create some features just from some basic demographics like the
patient's age, gender, and so on. Then we're going to look at a number of other
features that look at their health insurance coverage and I'll come back to why that’s
relevant in just a couple of moments. Those two are are sort of the more standard
set and everything else they'll be referring to now are what I'll call as the bag of words
features. So for example, we'll create one feature for
the service place where patients are going. So, have they been to a primary care doctor? Have they been to an urgent care clinic? Have they been to a hospital? Have they been to a regular doctor's office? And for each of these service place categories,
we're going to just create a single binary indicator for “has the patient been to that
service place ever?” One or zero. Now we'll do the same thing for these other
categories. For example, of all of the medications, we
might look at the thousand most popular medications. We're going to create a single binary feature
for “was this medication ever filled?” Yes or no. We’re then going to look at all possible
procedures that could have been performed or let's say the top 1,000 most common procedures
and we'll say “has this procedure ever been performed for the patient?” Yes or no. And we'll do the same for the diagnosis codes
so we'll look at all of the different, let's say top 10,000 ICD-10 diagnosis codes, and
we'll look at “has this diagnosis code ever been recorded for a patient?” Yes or no. Now you might be wondering whether the code
has been recorded or not only tells you part of the story. For example, whether you fill this medication
in the last three months might be very different from whether you fill the medication three
years ago and so the way that we're going to bring in the temporal information here
in this hypothesized approach is by redoing this whole procedure I just mentioned, now
once for every time window. So we will create one set of features by performing
that procedure I just showed using all the patients past medical history. We'll then repeat the process now restricted
to just data from the past 24 months and we'll repeat the process using data from just the
past six months. We will then concatenate each of those feature
vectors together and we get the overall set of features that we'll use to predict the
patient's outcomes. Overall we might have tens or hundreds of
thousands of features and so we're going to need a machine learning approach that can
deal with the fact that we're in a very high dimensional setting and can prevent us from
overfitting. I'll just pause briefly to see if we have
any questions. [TA]: I'm done so far yeah. [DS]: And I saw a comment about disabling
annotate on the screen, has that been done? [TA]: I think that's for you. So if you go to view options on the top, you
should just like where you're sharing the screen just get rid of the annotate option. [DS]: Great. Okay hopefully that's done. [TA]: I can still see the option to annotate
your screen. [DS]: On the bottom or the top because I don't
see it? [TA]: On the top. It should say like you're sharing your screen
and then on the side it should say view options and then if you click down it should just
disable the annotate, just like uncheck it. [DS]: Hide names of annotators. Did that work? [TA]: I don't think so. I think you're okay without that option. Okay. [DS]: Okay so the technique that we're going
to propose to use here is one that you've already seen in a recent laboratory where
we asked you about how you would tackle these different scenarios of machine learning and
one of the options that we had suggested in that lab was doing logistic regression with
what was called L1 regularization. So I'll briefly walk you through what I mean
by this just to remind us. So as Tamara presented in the lecture on logistic
regression, logistic regression could be formulated by the following objective function: we're
going to sum over the n data points, our loss function is going to be a negative log likelihood. We're going to use as our predictor, which
will give us the probability of an individual having a poor outcome from COVID-19, we're
going to use for that a linear model parameterized by a weight vector theta. That linear model is going to feed into a
sigmoid which will give us the probability of 1, which is going to be a number between
0 and 1, and we're going to evaluate the likelihood using that probability of the true label,
whether the patient truly had a poor outcome when hospitalized with COVID-19 or not, and
that label is given to you by y^(i) for the i-th patient. So that is the unregularized logistic regression
objective function and what we saw in class is that one way to try to prevent ourselves
from overfitting would be to use L2 regularization where we add on to the objective function
a penalty for the squared norm of the weight factor theta. And just to remind you, by the squared norm
what we mean is we sum over the different dimensions of the weight vector of that weight,
the d-th weight, squared. Instead of that L2 regularization, what we
often do using this type of data is L1 regularization and L1 regularization has the property that
it tends to result in very sparse solutions and the reason why that makes a lot of sense
as a type of method for controlling for the number of high-dimensional features is because
in this data set, by the way that we constructed the feature vector, most of the features are
irrelevant or not useful and that we expect that they're only going to be a small number
of features that are actually predictive of this poor outcome. So by encouraging the learning algorithm to
find a sparse weight vector, it's using some prior knowledge we have that the hypothesis
space that we want to be searching within is one of sparse weight vectors and as a result,
we're going to be able to prevent ourselves from overfitting to the large weight vector
even if we have a very small number of labeled data points. So this L1 regularization, if we look on the
right hand side, is defined as summing over the features of the absolute value of the
d-th weight, theta_d, so you should be comparing this absolute value to the theta squared term
shown on the bottom. We can then use one of a number of different
optimization algorithms, for example, variance of stochastic gradient descent, in order to
minimize this overall objective function. Now intuitively, the reason why using this
L1 norm results in a sparse weight vector is because when you're minimizing that objective
function, you can think about the first term in the objective function over here, this
L negative log likelihood, as some convex function: it's a convex function when we're
using a linear model as we are here and we could think about what are the solutions that
it could reach if we were restricting the weight vector to, for example, have any constant
value according to the L2 norm or any constant value according to the L1 norm. Intuitively, you could think about trading
off. If you were to trade off this value being
10 or this value being 10, what solutions do that enable you to find in terms of the
original objective function? So on the squared norms case, the picture
you should be having in mind if you had just two weights, theta_1 and theta_2, would be
something like this: so if that squared norm were equal to 10, then the weights would have
to be something along this circle. On the other hand, if the L1 norm were equal
to 10, then the weights would have to be something along this diamond shape. So again, this first axis we'll call theta_1,
the second axis theta_2. Now if you look at the left hand side, the
solution that minimizes the objective function that lives along the circle is sort of this
solution over here, whereas in the right hand side, the solution which minimizes this objective
function according to that constraint is actually at this corner point and that's one of the
reasons intuitively why using this L1 regulation tends to result in sparse weight vectors as
a solution. Now once one has learned the model, the next
thing that we typically do is start looking at the model and trying to introspect: does
it make sense? And the reason why this is so important is
as a sanity check for whether we set up our problem appropriately. Often we will find by inspecting the weight
vectors errors in our machine learning formulation. For example, we might we might easily detect
that that there was label leakage by recognizing that some weight which really doesn't make
sense is much higher weighted and this type of introspection by looking at what features
have non-zero weight is much easier to do in the setting where you use a L1 regularization
as opposed to L2 regularization because there are many fewer weights than features that
have non-zero weights, you tend to just look at all of them. Now the second thing which is important in
thinking about how we would actually use the results for solving the original problem that
we set out to solve, which recall was prioritizing outreach to patients who are eligible for
having a vaccine. We want to figure out who we should be contacting
in order to help them, for example, or get them to their local clinic. We need to understand, well, as a function
of the number of people who we can actually outreach to, how many of them might actually
have poor outcomes according to the predictions made by this algorithm? And to character that, we use what's known
as the receiver-operator characteristic curve. You'll see this often in the machine learning
literature under the acronym ROC curve. Now the roc curve takes your predictive model. Remember, here we use logistic regression
so we get a probability out and it's going to look at if we were to threshold that probability
at a variety of different values, what true positive and false positive rates are achievable
by that threshold. Here I'm showing you just two hypothetical
models. For example, a model which is trained using
just those traditional risk factors that I showed you originally and then the second
model that I proposed where I said we're just going to include this bag of words feature
vector which I'll call the full model. So the roc curve will plot, for every possible
choice of threshold, we're going to look to see, okay, if we were to make those predictions
according to that threshold. So let's say if we put a threshold at 0.5
and we said everyone with a probability higher than 0.5, we're going to predict to be 1,
everyone with a probability less than 0.5, we're going to be going to predict to be 0
and we ask: well of the ones we predicted, of the ones who were truly 1, what fraction
of them, of the ones who were predicted to be 1, what fraction of them truly went on
to have poor outcomes? That's what's known as the true positive rate
and if we look at of the ones who are predicted to be 0, what fraction of them went on to
not have poor outcomes, that's the false positive rate, and that gives you one point along this
curve and if you're to change that threshold from 0.5 to 0.6, you get another point along
this curve. Now one could try to summarize how good one
algorithm versus another algorithm is by comparing these two different curves and one statistic
that could be useful for trying to compare one curve versus another is the area under
that curve and that is often reported in the literature as the area under the ROC curve
or AUC. So for example, a random predictor would be
shown by this dotted line here and it would have an AUC of 0.5 whereas this one that I'm
showing you here would have an AUC closer to, let's say, 0.7. Now a different way of interpreting the area
under the ROC curve, although I won't derive this for you here, is that it's the probability
that if you were to take two patients, one who had a label of 1 and one who had a label
of 0 and ask “if you're to take two patients who have a label of 1 and patient who has
label of 0, does your algorithm rank them correctly? Does the patient who has a label of one, is
their probability according to the output of your logistic regression higher than the
probability given to you by the patient who has a label of 0?” If you look at the fraction of times that
their ranking is correct, that turns out to be the area under the ROC curve. And where you want to be is up here in the
top left corner. So you would like to have zero false positives
and all true positives and that would have an area under the ROC curve of 1. Now you can see that a random model has an
area under the ROC curve of 0.5, A perfect model has an area under the ROC curve of 1. Now if we think about the actual application
that we set out to try to solve, we're not going to have resources to reach out to a
100,000… Sorry, we may not have resources to reach
out to five million individuals because there might not be budget for that. Instead, we might only have resources to do
this intensive reach out to, let's say, 2 million or 1 million individuals, and so we
don't really care so much about this right hand side of the curve, we care about what's
going on over here on this side of the curve and there's a different statistic which is
often used to try to characterize that and it's known as the positive predicted value. So the positive predictive value is a measure
of, if you were to take your sorted ranking, so we're now going to take the model that
was trained, we're going to apply it to patient data from December 2020, and we're going to
get a ranked list of individuals who, if hospitalized, who is most likely to have poor outcomes,
and then we're just we're going to say “okay we have money to perform outreach to a certain
number of these individuals to try to help them get vaccinated.” For example, we might have enough, we might
be able to reach out to 100 individuals, we might be able to reach out to 1,000 individuals,
we might be able to reach out to 10,000 individuals. And so what's relevant is, of those 100 individuals,
1,000 individuals or 10,000 individuals, what fraction of them actually went on to have
poor outcomes if they were hospitalized with COVID-19? And that's what the positive predictive value
measures: it's the fraction of individuals of a predicted set that actually have the
1 label versus 0 label and you want that number, of course, to be as high as possible. When you then go to a decision maker (for
example, you might go to Congress and ask for more budget), you might describe your
approach for your return on investment, in some sense, of how many more people you will
be able to reach out to if you were given this much more budget. Now I've given you a very hypothetical example
and, as I'm sure you have been thinking through, there are number of subtleties to how one
would try to use this machine learning approach and the real world really is messy and imperfect. So for example, I'd like to hear from your
thoughts: will model trained using data from July be useful in December? Has anything changed? So if you could take out your chat and send
me a private message, I'd like to see your thoughts about whether there are any concerns
with using a model which is trained using data from March to July to learn who is likely
to have poor outcomes from a COVID-19 hospitalization and then taking that model and deploying it
now in December or January using data up until now. Where might using that approach go wrong? I'd like to hear some responses. You can send me private chats too and I'll
just wait a moment to read, I'm getting a couple but I'll wait for a few more responses. I'm getting some really interesting responses. I'll wait one more minute to see what else
we get. All right, so some of the responses we heard
from our fellow students are that, well, first of all, the state of the world might be different
for a number of reasons. If you apply the model in December, for example,
it's now winter time, and so both of the data might look different. For example, patients visits to doctors might
be different in the winter time versus in the summer. Also very importantly, the COVID-19 strain
might have changed from July to December: there may have been mutations and so, as a
result of that, perhaps a different set of people might be at highest risk of having
poor outcomes. Another very important problem 
is that there may have been new treatments as well that have been discovered from July
to December and so that might also have affected who is likely to have poor outcomes. So I'll just mention a couple of these here,
but you mentioned many others, and these are all really good points. So these are things that we're going to have
to take into account when we want to use such an algorithm. So for example, you might think that the data
looks very different because of some of the, let's say, weather changes. So we might say, “okay, well one way to
try to address that problem is to only use data up until, let's say, March 2019.” So we're going to take only the patient's
data up until March 2019 to make the prediction even if that prediction we're making in January
or February of 2021. I meant to say up to 2020 instead of 2019. And so there you might be using less information
(you might be missing out on some of the most recent data from patients), but the upside
of that is that you have a consistent set of features that don't change much as we go
into the future. Now the second problem that you mentioned
is that there are, let's say, new treatments and that is, of course, also a big problem
and so one thing that one can do is one could take the model that was trained using data
up until July and you could evaluate how is it doing at various points in the future. For example, you could look at what is the
area under the ROC curve when you take that model, that same model, and you apply it to
patients in August, September, October, November, December, and you can look to see, well, despite
the fact that treatments are changing across time, is the relative ranking among patients
still relatively consistent? So if the area under the ROC curve is relatively
the same across time, it might be that some patients' risks go down. It could be that the overall probability of
everyone is shifted down a little bit due to the treatments, but so long as the relative
risk across patients is the same, then the model might still be useful. But that's an example of a sanity check that
one might want to do and, of course, introspection into the model can also help with that. Okay so that's all I want to say about that
example and now I want to move on to thinking about clustering and a use of unsupervised
learning for healthcare. So recall back to lecture 13, last lecture,
where Tamara presented the k-means algorithm. She gave this example where there are two
features x_1 and x_2, longitude and latitude, there are a number of data points, n data
points, and critically, in the k-means algorithm, there are no labels: our goal is to discover
some structure in the data. The first step of the k-means algorithm was
to initialize some means. Here we chose 1, 2, 3, 4, 5 different means
so there will be five clusters we get out. Then the second step of the k-means algorithm
assigned every data point to their closest mean and so that's what you're seeing here
by the colors. We're then going to average the data points
assigned to each mean in order to get a new mean for that cluster and then we're going
to repeat this procedure over and over again until it converges and you get a final clustering
algorithm out. So what I'm going to present next is an application
of the k-means algorithm to solve a problem that had been presented to me, actually, originally
a few years ago by a pharmaceutical company that was really interested in studying asthma
and this is a problem, by the way, that some of you might have noticed was the last optional
question of the lab last week. So the problem is that 5% to 10% of people
with severe asthma remain poorly controlled despite using the best therapy that's currently
available and so we'd like to think about: can we better understand asthma and can we
think through how we could design and evaluate new treatments for asthma? In this paper published in a medical journal,
The Lancet, a few years ago called “New targets for drug development in asthma,”
they asked the following questions: what are the processes, genetic or environmental, that
underlie different subtypes of asthma? What are markers of disease progression or
treatment response? And why is it that some patients are less
responsive to conventional therapies than others? So we're going to try to answer these questions
or some of these questions by using a k-means clustering algorithm on data from asthmatic
patients and the output of the algorithm is going to be a new set of clusters which... You don't have to understand this slide, we're
going to dive into in just a moment, but this is just a slide, a picture from this paper
published in 2008 which characterized three different families of new subtypes of asthma
and that's what we're going to aim to discover through this analysis. So they're going to be three data sets used
in this analysis. All three are from a non-smoking population
and the first data set is of 184 patients recruited from primary care practices in the
United Kingdom. The second data set is 187 patients who were
treated in an asthma clinic, so these are patients who potentially have more severe
asthma because they're not being managed just by their primary care doctor but by a specialist. And then finally, we're going to have a third
data set of 68 patients who had undergone a randomized control trial comparing two different
treatments. That third data set we're not going to use
for a few minutes. We're going to start by using those first
two data sets to try to do clustering to see: can we discover any interesting structure
in the data, see how patients might differentiate across different asthma conditions. Here's just a very quick analysis of those
three cohorts. So each column here is describing one of those
three data sets. Focus on the first two columns. In the primary care cohort, there were 184
patients and a secondary care cohorts, that is patients who are managed at an asthma clinic,
there were 187 patients. There were a little bit more female patients
in the second cohort, so 65.8% in the second, in the asthma clinic cohort. The age distributions were relatively similar:
an average of 49 years old in the primary care cohort, an average of 43 years old in
the secondary care cohort and so on. And so this gives us some intuition about
who are the individuals but there's been no clustering yet. The first thing we're going to do is we're
going to take the primary care cohort, these 184 patients, we’re going to run a k-means
algorithm and I won't walk through how we choose the number of clusters, but just believe
me, we're choosing three clusters here. So we initialize with k = 3 means, we run
the k-means algorithm, and we're now going to analyze the results. Now the first thing we're going to do here
is we're going to recognize that here, we're using a relatively small feature vector. In particular, every row here corresponds
to one of the features that went into the k-means algorithm. We have the gender of the individual, we have
their age, the age when they had an onset of asthma, their body mass index, a measure
of their lung function (that's measured by this biomarker called FEV_1 change), how much
corticosteroids they inhaled as part of their typical therapy, how often they were admitted
to a hospital or emergency room, and how many severe asthma exasperations they had in the
past 12 months. So each one of those is one of the features
and, again, we're doing this clustering just in the primary care cohort and in this first
column here, I'm just giving you the average of the feature values across the whole entire
cohort. Now the way that we're going to understand
or visualize the output of the clustering algorithm is by looking at the mu_1, mu_2,
mu_3, meaning the cluster centers, cluster means, of the three different clusters. We're also going to look at the variation
or variance of each of the features among the data points assigned to that cluster and
that's going to be a number presented in the parentheses here. So, we get three clusters out: the first cluster
has 61 patients, the second cluster is 27 patients, and third cluster has 96 patients. Let's start to try to analyze these clusters
by looking at these features. The first thing that is immediately apparent
is that in cluster 1, the age of onset of asthma is substantially smaller than in other
clusters. In cluster 1, patients tended to develop asthma
at roughly 14 years old with a standard deviation of 15 years compared to clusters 2 in clusters
3, where patients tend to develop asthma in their 30s. We also notice that in cluster 1, that on
average patients have had one previous hospital admission and almost two severe asthma exasperations
in the past 12 months and that should be compared to the averages in the whole cohort shown
in the first column. And so we're going to call cluster 1, the
early onset cluster, patients who have early onset asthma, and these patients actually
have pretty exasperated asthma. You can see that by their large number of
hospital admissions. These names you see in the very top here are
names that the analyst has given to the cluster by analyzing these means and comparing it
to the overall population. Cluster 2, we see, is overwhelmingly female. So 81 of individuals in cluster 2 are female
and they also tend to have a much higher body mass index, so this is a bit more of an obese
population. And cluster three is generally healthy. These are patients who have asthma but if
you look at the bottom three features here, on average they've basically not been to the
hospital at all and they have had very few asthma exasperations. So these are three clusters that we found
through k-means clustering on this one data set and the next thing we're going to do is
see how robust it is. So we're going to take the second data set
of patients who are treated in the asthma care clinics and we're going to repeat that
clustering, the same clustering procedure: initialize means randomly from scratch and
re-run clustering. The set of features are very similar but not
identical to the previous data set which is why we don't attempt to, for example, throw
together all the data and cluster them jointly, it's because we have a slightly different
set of features. So we run clustering here. For this data set, the authors chose to use
four clusters. I'm not going to comment on that but we saw
in lab and in the notes for the clustering how there are a number of different methods
for trying to choose number of clusters such as, for example, looking at k-means objective
as a function of cluster size increasing. So here, despite the fact that we redid the
clustering from scratch, what these authors found is that very similar clusters seem to
arise. So when they analyzed these four different
clusters, they found that three of the clusters ( what they're labeling as cluster 1, 2, and
3) corresponded almost identically to the clusters that were found in the previous cohort
but now there is a fourth new cluster, what they're calling information predominant, which
are our patients who have substantially worse asthma and that makes sense as well because
this is a slightly different population as the previous population: these patients are
patients who are being managed by experts in asthma and so there tend to be sicker patients
and so this fourth new clusters is the sicker set of patients. Okay, so now we're starting to see something
really interesting: we're seeing how these first three clusters were pretty robustly
discovered across two very different data sets. So that starts to give us some confidence
that there's some significance to those clusters, but now we're going to try to take that the
next level and see: can we try to see if those clusters are actually meaningful in changing
something about treatment or seeing how effective different treatment might be for patients
within those clusters? And that's what we're going to start to use
now, the third data set. Now the third data set only has 68 patients. So it's substantially smaller than the first
two and if we were to do clustering, which we'll do on these patients, we should really
question the robustness of those clustering results because of the small number of patients
and that's where the alignment with the previous two results is going to end up being important. I'll come back to that in a moment. Now for these 68 patients, these patients
were followed over 12 months and half of these patients received standard clinical care,
but the other half of the patients received a new type of treatment. In particular, what we're going to call the
sputum treatment and this treatment strategy involves regular monitoring of the airway
inflammation using the patient's phlegm and changing the amount of steroid therapy in
order to try to maintain a certain level of white blood cell count from the patients as
observed in the patient's phlegm. So that's a new treatment strategy and the
question is how well that treatment strategy works. Now the original study, the original randomized
control trial involving those 68 patients, found no statistically significant difference
between these two treatment strategies meaning if you look at outcomes such as hospitalizations
or number of exasperations, there was not a statistically significant difference across
the two different sets of patients who got one treatment versus the other. However, what you ought to wonder about is:
there might have been variation in the treatment response. Maybe, some patients perform, do best, under
that first treatment strategy and other patients do best under that second treatment strategy
and so what we're now going to do is we're going to re-analyze these results, trying
to separate out patients according to those three different clusters. So the first thing that was done is taking
the baseline data, so the data pre-treatment, and assigning patients to these three different
clusters and we now have a pretty rigorous definition for what these three different
categories of patients should be based on the clustering done on the first two data
sets. So we're going to categorize every patient
into one of these three categories and then we're going to look at what happened according
to those two different treatment strategies. So what you see here is that along the three
different sets of patients, the first category of patients (what the authors called obese
female cluster), if you look at the outcomes, for example, severe exaspirations and number
of times that they had to start oral cortical storage which is which is a measure of severity,
both of these numbers are very low. So these are patients who were not really
affected by either of the treatment strategy, there's no statistically significant difference. The other two clusters is where the interesting
things happen. So in particular, if you look at that third
outcome, you see that in the first of these clusters, the inflammation predominant cluster,
in one of the treatment arms, so the regular clinical practice, you see that under the
first cluster, patients do well under the typical clinical practice but on the second
cluster, they do poorly under typical clinical practice and you see the exact opposite in
terms of the patients who got the other strategy. So if you were to average these two numbers,
it looks like the treatment doesn't make much too much of a difference. Right if you compare the sum of 9 + 0 to the
sum of 2 + 6 which is 8, 9 versus 8 doesn't look very different. On the other hand now, if you break down the
patient according to these two different categories, you see that for one of these subpopulations,
it looks like this typical clinical practice does much better than sputum because we want
this number to be lower. Sorry I think it's the opposite: we want it
to be higher so the sputum category, the sputum arm, does much better for this cohort whereas
for this cohort, the typical clinical practice does much better as a treatment and so this
now suggests that maybe if one were to approve this new treatment strategy, one should approve
it only for these sub-cohorts of patients for whom it looks like they respond the best
to that treatment and this is just one illustration of how one can use k-means clustering to try
to guide who should who might benefit the best from treatment. So now I'm going to move on to the second
to last section of this lecture which is about reinforcement learning of treatment policies. So think back now to lecture nine where Tamara
introduced market precision processes and then lecture 10 where she introduced reinforcement
learning. So Markov processes involved, first, a specification
of a state space and in the simple example that she gave in lecture nine, the state space
was very simple. This is a farming example and the state space
was either there's poor soil or rich soil and so every farm is in one of these two states. Then, one has to specify a transition distribution. So according to what action is taken, there's
some probability of transitioning to either staying in the same state, which is denoted
by these self loops, or transitioning to a new state. So for example, the transition distribution
for the plant action if you were to take the action of planting. Then if you started out in a rich soil state
then if you were to take the plant action from the rich soil state, with probability
0.9 you're going to end up in a poor soil state. On the other hand, if you take the plant action
starting in the poor soil state, there's an extremely high (so probably 0.99 now) of staying
in a poor soil state, very unlikely that by planting you turn a poor soil into a rich
soil. And one can talk about the transition distribution
for different sets of actions such as a fallow action. So that's the state space, that's the action
space. We talked about the transition distribution. The transition distribution is given to you
by a starting state, an action, and then asking about the probability of ending up in a new
state and so you have a probability or a real number between 0 and 1 for every cross product
of starting state, action, and ending state. Now a Markov decision process also has a reward
function which tells you for every state and every state that you end up in and every action
that was taken, what is the reward for ending up in that state having taken some action? So for example, in lecture, Tamara hypothesized
a reward of being in a rich soil state and taking the plant action of 100 whereas the
reward of being in the poor soil state and taking the action of plant was 10. She also specified a discount factor but for
health care settings, the discount factor which is used to discount the effect of rewards
that were long in the past is irrelevant. In some sense we care about the full set of
rewards for patients and so we're going to think about the discount factor as just being
1. So I won't be talking about discount factor
in any of the subsequent slides. Okay so this is just a quick reminder of Markov
decision processes. Now let's think about how we might use Markov
decision processes in healthcare to try to provide decision support to clinicians as
they manage patients who have cancer or they try to manage patients who have a very critical
illness. And the goal, what we'll be describing, is
to think through how can we learn a policy pie which takes a state and proposes an action
and when we try to implement this in a healthcare setting, you think about this as taking data,
learning these policies, and then using these policies to provide decision support for clinicians. So if our policies says, “we think that
prescribing the patient this medication which would be an action would result in the best
future set of rewards,” then one might suggest to the clinician: maybe you should be prescribing
this treatment because we think the patient will respond best in the long term to this
treatment. So that's an example of what a policy might
be and how one would use it. So the case study that I'll be giving to you
here is from that of managing what's known as septic shock. Septic shock is a condition, it’s the second
leading cause of death in hospitals, and it's caused by infection and that infection the
patient's immune system attempts to fight the infection but at the same time as the
immune system being activated to fight infection, often \ the immune system also ends up affecting
the patient's organs. That can lead to organ failure and ultimately
death and so when one wants to try to manage sepsis, of course the first thing to do because
it's caused by an infection is to try to prescribe an antibiotic. Then, however, antibiotics alone tend to not
be sufficient for managing sepsis and there's a number of other things that one has to try
to do to try to keep the patient alive once they get into this state. So for example, a patient might be put on
mechanical ventilation in order to help with their breathing, patients might need sedation
because they might be very uncomfortable due to the mechanical ventilation which they were
just put on, giving them sedation might have reduced the patient's blood pressure and as
a result of that, one might have to give vassal pressures which is the machine you hook them
up to in order to try to artificially increase their blood pressure and so on. And so if one thinks about these series of
actions that would need to be taken in order to optimally manage a patient with sepsis,
it corresponds to a large number of actions across time and for any one patient, you only
observe one set of actions. So for this patient that I'm showing in the
orange line here, they receive mechanical ventilation, let's say at time three hours,
then they did not receive sedation and then at, let's say, four hours they received vasopressors
and so on. And had something else happened to them, had
they taken a different trajectory through this path of decisions, then they would have
a different set of outcomes but we only, of course, observe one actual patient trajectory
in the data set because doctors did one series of actions to them. Had doctors done a different series of actions,
then we might have gotten a different set of outcomes. And one of the challenges in trying to learn
an optimal policy for managing sepsis is that there are a large number of possible actions
but only some of them are observed in the data. I'll get back to that in just a minute. So in order to try to tackle this, the first
thing that we're going to try to do is define the state space, the set of actions, and the
reward function and whereas in the earlier part of the lecture I was giving you a hypothetical
scenario around COVID-19, what I'm describing here is a paper that had been published two
years ago in Nature Medicine. I have the reference in the very bottom here. So for the state vectors, what they did is
they derived a feature vector for each point in time which looks at both: what are the
most recent and past actions that were taken (so for example, path medications), looks
at the patient's vital signs (for example, what is their current heart rate? What is the current blood pressure? What's the current oxygen saturation? And so on). And you get this one high dimensional feature
vector describing the patient at that one point in time. Now, in order to apply the algorithms that
you learned in this class which were for discrete Markov decision processes, we want to get
a finite set of states out and so what these authors did is they took that feature vector
describing the continuous valued feature of the state of the patient at every point in
time, they're taking the union of all those different feature vectors from a single patient
at different points of time across patients. So now, one data point might correspond to
patient three at time step ten, for example, and we're going to do now k means clustering
of all of those different continuous valued future vectors and what we would get out are,
let's say, 500 different clusters and we're now going to define a set of states corresponding
to 500 discrete states. Every patient at their point in time is assigned
a number corresponding to which cluster that data point went to in k-means clustering and
so now we have a state space of 500 different states that we'll use in defining this Markov
position process. Now the actions were the actions that I described
earlier. So for example: do you put the patient on
mechanical ventilation? Yes or no. Do you give the patient fluids? If so, what amount of fluids? So you might imagine three different levels
of fluids and if you look at the combination of these two different types of actions, that
itself defines the action space. So one action corresponds to a few of these
different decisions. I can't remember the exact number of actions
but let's say there are 20 or 50 different actions. Each action, again, tells you “start or
stop ventilation?” plus a level of amount of fluids to give the
patient. Now for the reward function, one wants to,
of course, keep this patient healthy and so “how does one characterize healthy?” is
one of the key design choices in setting this up. And different choices of reward functions
will result in very different policies that are learned. So for example, one might just use a very
simple reward function which ignores the reward at every intermediate state and just looks
at the reward at the termination states where, in this case, a termination state corresponds
to either a patient dying while they're in the hospital or then being discharged and
then surviving at least 30 days or dying within 30 days. Each one of those will correspond to some
reward function where, of course, we're going to put a very high negative reward on the
poor outcomes. Now one might also be interested in having
more intermediate rewards along the way and this is one thing that you saw could be very
helpful in learning a good policy is this type of reward shaping and so one example
of an intermediate reward might be blood pressure control. So we might know that if we keep the patient's
blood pressure within some range, it’s a good thing and so if you ever see a patient's
blood pressure go outside of that range, you have a negative reward associated to that. So that's how you define the reward function. Again the reward function is a function of
these states. Here we're going to ignore the actions when
defining the rewards. So we have the states, we have the actions,
we have the rewards, we have everything we need to define a Markov decision process and
now the next step is: how do we learn it? So in lecture 10, Tamara spoke about exploration
versus exploitation and the key question there is: you start from no data and you start exploring
in the world by performing some actions, seeing what happens, what rewards you get, what states
you get to, and as you do more and more exploring, you get more and more data and you use that
data to try to learn a policy that gets better and better over time. And there's this question of: do you exploit,
meaning do you use the best policy that you have for all future individuals or do you
explore, meaning do you keep trying different types of treatments? And the exploration strategy that we spoke
about in this class is called the epsilon greedy strategy where, with probability epsilon,
you choose an action uniformly at random. Now do you think that strategy would make
sense in a healthcare setting? Any concerns with that? You can enter your response in the chat. Any concerns with using an explore and exploit
strategy? So one concern that I see people are entering
is that mistakes might have a higher impact. There might be really big ethical issues with
deciding who to use exploratory treatments on and these are all really good points and
one can't use this epsilon greedy strategy in the healthcare setting and that's what
one of the major challenges is in using reinforcement learning techniques that we saw in class in
the healthcare setting. And so I'll get back to what we do in just
a second but the tldr of this is that there are a couple of different algorithms that
we learn in lecture 10 for learning. The first approach was, let's say, just explore
and then learn your transition distribution and then once you've learned your transition
distribution using all the data you have, then you just do infinite horizon value iteration
in order to learn a good policy. The second approach that was suggested was
to estimate the Q function directly from the data that one has. So as we just discussed, one can't typically
explore in a healthcare setting and instead what one does is called offline reinforcement
learning. It turns out that the two algorithms I just
showed you from lecture 10 could be used in an offline setting where all one has are those
observed trajectories of state action reward pairs for patients in the retrospective data. So for example, you could use that retrospective
data to estimate the transition distribution and then run value iteration or you could
also just take pairs of these tuples (s, a, s’, and r), keep presenting them to your
Q learning algorithm .You're getting these pairs, instead of from doing epsilon greedy,
you're getting them from your observed data and you update your Q values based on those
observed batch data. So this is what's known as offline reinforcement
learning and that's what we would use in this healthcare context. Now recent reinforcement learning successes
in the news, for example, the AlphaGo algorithm which had succeeded at beating the world's
best go player led the healthcare community to be really excited about using reinforcement
learning and healthcare for problems like the one I just mentioned but it turns out
that healthcare is substantially harder than those settings I just described, those game
related settings. So for example, one key design choice is:
what is in that state space? I showed you one way to try to derive the
state space by this k-means clustering approach based on features like the patient's blood
pressure, oxygen saturation, and so on but if we had accidentally left out important
information there, then the policies that we get out could be really biased and similarly,
we just talked about how we can't do epsilon greedy algorithms to learn here, instead one
has to only take retrospective data that are available, and so thus we might have very
limited data available whereas in these games scenario they had simulators so they could
really think about any policy and just run that policy and see how well it does and so
those algorithms were substantially more successful as a result. Okay so I already just gave some hints about
what makes healthcare different so I'm going to skip these last few slides and I'm going
to get to my closing slide. Congratulations, this is the end of 6.036! You've had 13 lectures that walked you through
everything from supervised learning to reinforcement learning to unsupervised learning with the
k-means algorithm and along the way, we've hinted at how we're really just at the tip
of the iceberg. We hope that you now have some of the skills
you would need to start using machine learning in an industry position, an internship, and
research but one might also want to dive in deeper to learn much more about machine learning
and data science. So in terms of where to go to next, I'd like
to point out a few courses that you could consider taking subsequent semesters here
at MIT. These first two courses, 6.401 and 6.419,
are very introductory courses but focus on more data science aspects of machine learning
than what we focused on in this course. The next set of four courses, 6.806 up to
6.802, are courses that are at a more advanced level but many of these courses have both
undergraduate and graduate versions and they look at applications of machine learning in
different scenarios. So for example, 6.871 is (of course, I'm teaching
next semester) on machine learning for healthcare which goes into much, much more depth on some
of the things you learned in this lecture and it requires 6.036 as a prerequisite. Then there are two more courses I'd like to
point out that aren't offered into the fall semester: 6.867 which is the graduate machine
learning course and 6.860 which is another graduate course in machine learning which
is joint between course 6 and the brain and cognitive sciences department. Both of those courses would be very appropriate
courses if you want to do a much deeper dive into the algorithms that you learned about
in this course and learn some of the theoretical properties. All right, with that, we're out of time and
I want to thank you for joining us for 6.036 this semester and wish you a very good winter
break! Bye. 

