{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Okay it's 9:35 let's go ahead and get started....</td>\n",
       "      <td>['', 'Machine learning course introduction', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Okay, I think it's MIT time so let's go ahead ...</td>\n",
       "      <td>['', 'machine learning', 'classifier', 'hypoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Okay good morning and welcome to lecture three...</td>\n",
       "      <td>['', '', 'machine learning analysis', 'gatheri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Okay, good morning, it's MIT time \\nso let's g...</td>\n",
       "      <td>['', 'perceptron algorithm', 'linear separabil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Okay it's MIT time, so let's go ahead and get ...</td>\n",
       "      <td>['', 'Machine learning', 'Classification', 'Su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Transcript  \\\n",
       "0  Okay it's 9:35 let's go ahead and get started....   \n",
       "1  Okay, I think it's MIT time so let's go ahead ...   \n",
       "2  Okay good morning and welcome to lecture three...   \n",
       "3  Okay, good morning, it's MIT time \\nso let's g...   \n",
       "4  Okay it's MIT time, so let's go ahead and get ...   \n",
       "\n",
       "                                            Keywords  \n",
       "0  ['', 'Machine learning course introduction', '...  \n",
       "1  ['', 'machine learning', 'classifier', 'hypoth...  \n",
       "2  ['', '', 'machine learning analysis', 'gatheri...  \n",
       "3  ['', 'perceptron algorithm', 'linear separabil...  \n",
       "4  ['', 'Machine learning', 'Classification', 'Su...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link=\"merged_data.csv\"\n",
    "data =pd.read_csv(link)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/subrat_roy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/subrat_roy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = {'to', 'were', 'going', 'Is', 'are', 'am', 'this', 'that', 'me', 'let', 'was', 'were', 'these', 'those', 'so', 'the', 'do', 'etc', 'why', 'from', 'we', 'us','question','okay','lets','mit','umm','before','earlier','go','us','were','started','get','started','today','tomorrow','yesterday','wednesday','um','like','uh','word','soil','worksheet','workshop','food'}\n",
    "stop_words = set(stopwords.words('english')).union(additional_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text), re.I | re.A)  # Ensure text is converted to string\n",
    "    text = text.lower()\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    return ' '.join(text)\n",
    "# Handle missing values in 'Transcript' column\n",
    "data['Transcript'] = data['Transcript'].fillna('')  # Replace NaN values with empty string\n",
    "\n",
    "data['transcript_cleaned'] = data['Transcript'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(data['transcript_cleaned'])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Calculate dispersion and local span\n",
    "dispersion = tfidf_matrix.max(0).toarray()[0]\n",
    "local_span = (tfidf_matrix != 0).sum(0).A1\n",
    "\n",
    "# Create DataFrame to store the results\n",
    "tfidf_df = pd.DataFrame({'token': feature_names, 'tf-idf': dispersion, 'dispersion': dispersion, 'local_span': local_span})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 50 Tokens:\n",
      "               token    tf-idf  dispersion  local_span\n",
      "10285       networks  5.744359    0.179511          88\n",
      "6366      frameworks  2.450681    0.076584          17\n",
      "17289          works  1.214242    0.037945         141\n",
      "7364       homeworks  0.972050    0.030377          23\n",
      "17290     worksheets  0.723843    0.022620           1\n",
      "11657         policy  0.646018    0.646018          42\n",
      "9562          memory  0.640494    0.160123          50\n",
      "16105         trucks  0.605352    0.605352           4\n",
      "13276          right  0.604627    0.604627         156\n",
      "8526          kernel  0.597771    0.597771          26\n",
      "15646          theta  0.585231    0.585231          57\n",
      "667      adversarial  0.580323    0.580323          22\n",
      "10766       opponent  0.566272    0.566272          10\n",
      "3693            cost  0.566148    0.566148          65\n",
      "5409          entity  0.561356    0.561356          14\n",
      "2951         cluster  0.543669    0.543669          23\n",
      "7623           image  0.534962    0.534962          64\n",
      "10284        network  0.532024    0.266012          90\n",
      "8608            know  0.530508    0.530508         157\n",
      "15526         tensor  0.513305    0.513305          14\n",
      "5519             eta  0.494902    0.494902           9\n",
      "1219             arm  0.487298    0.487298           7\n",
      "2977            coat  0.481815    0.481815           6\n",
      "11585         player  0.467484    0.467484          17\n",
      "3624            core  0.458516    0.458516          37\n",
      "14447           sort  0.456668    0.456668         110\n",
      "12165  propositional  0.433986    0.433986           3\n",
      "10221        nearest  0.418237    0.418237          37\n",
      "16001    translation  0.418128    0.418128          39\n",
      "7508      hyperplane  0.415354    0.415354          30\n",
      "9116            loss  0.414364    0.414364         101\n",
      "11194       patients  0.406025    0.406025          19\n",
      "1263        artworks  0.404640    0.012645           2\n",
      "15814      tokenizer  0.403486    0.403486           3\n",
      "8610       knowledge  0.397313    0.397313          66\n",
      "8721        language  0.395082    0.395082          68\n",
      "5483           error  0.394730    0.394730          94\n",
      "15528     tensorflow  0.384360    0.384360          14\n",
      "10061     multimodal  0.382725    0.382725           2\n",
      "16820         vision  0.381634    0.381634          43\n",
      "9064           logic  0.381030    0.381030          22\n",
      "13338         robert  0.379287    0.379287           4\n",
      "16104          truck  0.377536    0.377536          12\n",
      "3530            conv  0.376439    0.376439           6\n",
      "1631             bar  0.373728    0.373728          20\n",
      "14741          state  0.368647    0.368647          88\n",
      "8812        learning  0.365783    0.365783         151\n",
      "11140      particles  0.365666    0.365666           2\n",
      "15442          teach  0.362601    0.362601          37\n",
      "4319      dependency  0.359089    0.359089          23\n"
     ]
    }
   ],
   "source": [
    "with open(\"www.txt\", \"r\") as file:\n",
    "    keywords = file.read().splitlines()\n",
    "found_keywords = set()\n",
    "for keyword in keywords:\n",
    "    for idx, row in data.iterrows():\n",
    "        if keyword.lower() in row['transcript_cleaned']:\n",
    "            found_keywords.add(keyword)\n",
    "            break\n",
    "\n",
    "# # Print the found keywords\n",
    "# print(\"Found Keywords:\")\n",
    "# for keyword in found_keywords:\n",
    "#     print(keyword)\n",
    "# Adjust the TF-IDF value for the keywords\n",
    "for keyword in keywords:\n",
    "    if keyword in tfidf_df['token'].values:\n",
    "        indices = tfidf_df[tfidf_df['token'].str.contains(keyword)].index\n",
    "        for index in indices:\n",
    "            tfidf_df.at[index, 'tf-idf'] *= 2  # Double the TF-IDF value\n",
    "\n",
    "\n",
    "\n",
    "# Display the top 50 tokens\n",
    "top_50 = tfidf_df.nlargest(50, 'tf-idf')\n",
    "print(\"\\nTop 50 Tokens:\")\n",
    "print(top_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"www.txt\", \"r\") as file:\n",
    "    keywords = file.read().splitlines()\n",
    "found_keywords = set()\n",
    "for keyword in keywords:\n",
    "    for idx, row in data.iterrows():\n",
    "        if keyword.lower() in row['transcript_cleaned']:\n",
    "            found_keywords.add(keyword)\n",
    "            break\n",
    "\n",
    "# Print the found keywords\n",
    "print(\"Found Keywords:\")\n",
    "for keyword in found_keywords:\n",
    "    print(keyword)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read keywords from the file\n",
    "with open(\"www.txt\", \"r\") as file:\n",
    "    keywords = file.read().splitlines()\n",
    "\n",
    "# # Adjust the TF-IDF value for the keywords\n",
    "# for keyword in keywords:\n",
    "#     if keyword in tfidf_df['token'].values:\n",
    "#         indices = tfidf_df[tfidf_df['token'].str.contains(keyword)].index\n",
    "#         for index in indices:\n",
    "#             tfidf_df.at[index, 'tf-idf'] *= 10  # Adjust TF-IDF value\n",
    "#             tfidf_df.at[index, 'dispersion'] *= 10  # Adjust dispersion value\n",
    "#             tfidf_df.at[index, 'local_span'] *= 10  # Adjust local span value\n",
    "# # Search and add the keywords in the transcript to a set\n",
    "# found_keywords = set()\n",
    "# for keyword in keywords:\n",
    "#     for idx, row in data.iterrows():\n",
    "#         if keyword.lower() in row['transcript_cleaned']:\n",
    "#             found_keywords.add(keyword)\n",
    "#             break\n",
    "# # Calculate the average of TF-IDF, dispersion, and local span\n",
    "# tfidf_df['average'] = (tfidf_df['tf-idf'] + tfidf_df['dispersion'] + tfidf_df['local_span']) / 3\n",
    "\n",
    "# # Display the top 50 tokens\n",
    "# top_50 = tfidf_df.nlargest(50, 'average')\n",
    "# print(\"\\nTop 50 Tokens:\")\n",
    "# print(top_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wwwe.txt\", \"r\") as file:\n",
    "    keywords = file.read().splitlines()\n",
    "# Search and add the keywords in the transcript to a set\n",
    "found_keywords = set()\n",
    "for keyword in keywords:\n",
    "    for idx, row in data.iterrows():\n",
    "        if keyword.lower() in row['transcript_cleaned']:\n",
    "            found_keywords.add(keyword)\n",
    "            break\n",
    "\n",
    "# Write the found keywords to a text file\n",
    "with open(\"found_keywords.txt\", \"w\") as file:\n",
    "    for keyword in found_keywords:\n",
    "        file.write(f\"{keyword}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords from the new transcript that coincide with www.txt saved to xxxx.txt\n"
     ]
    }
   ],
   "source": [
    "# New transcript file\n",
    "new_transcript_file = \"tt.txt\"\n",
    "output_file=\"xxxx.txt\"\n",
    "min_keyword_length=3\n",
    "\n",
    "# Read the new transcript from the file\n",
    "with open(new_transcript_file, \"r\") as file:\n",
    "    new_transcript = file.read()\n",
    "\n",
    "# Preprocess the new transcript\n",
    "new_transcript_cleaned = preprocess_text(new_transcript)\n",
    "\n",
    "# Find and print keywords from the new transcript that coincide with www.txt\n",
    "found_keywords = set()\n",
    "for keyword in keywords:\n",
    "    if keyword.lower() in new_transcript_cleaned and len(keyword)>min_keyword_length:\n",
    "        found_keywords.add(keyword)\n",
    "\n",
    "# Save found keywords to a file\n",
    "with open(output_file, \"w\") as file:\n",
    "    for keyword in found_keywords:\n",
    "        file.write(keyword + \"\\n\")\n",
    "\n",
    "print(\"Keywords from the new transcript that coincide with www.txt saved to\", output_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main keyword extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords from the new transcript that coincide with www.txt saved to zzzz.txt\n",
      "Accuracy: 276.19%\n",
      "58\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/subrat_roy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/subrat_roy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# New transcript file\n",
    "new_transcript_file = \"tt.txt\"\n",
    "# File to save the found keywords\n",
    "output_file = \"zzzz.txt\"\n",
    "# # Minimum length of the keyword to consider\n",
    "# min_keyword_length = 5\n",
    "# dataset_keywords_file = \"kw.txt\"\n",
    "\n",
    "# Function to lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Function to revert lemmatized words back to their original form\n",
    "def revert_to_original(keyword, lemmatized_keyword):\n",
    "    # Tokenize both the original and lemmatized keywords\n",
    "    original_tokens = word_tokenize(keyword.lower())\n",
    "    lemmatized_tokens = word_tokenize(lemmatized_keyword.lower())\n",
    "    # Create a mapping from lemmatized tokens to original tokens\n",
    "    mapping = dict(zip(lemmatized_tokens, original_tokens))\n",
    "    # Use the mapping to revert lemmatized tokens back to their original form\n",
    "    original_keyword_tokens = [mapping[token] for token in word_tokenize(lemmatized_keyword.lower())]\n",
    "    # Reconstruct the original keyword\n",
    "    original_keyword = \" \".join(original_keyword_tokens)\n",
    "    return original_keyword\n",
    "\n",
    "# Read the new transcript from the file\n",
    "with open(new_transcript_file, \"r\") as file:\n",
    "    new_transcript = file.read()\n",
    "\n",
    "# # Read keywords from the dataset\n",
    "# with open(dataset_keywords_file, \"r\") as file:\n",
    "#     dataset_keywords = [line.strip() for line in file.readlines()]    \n",
    "\n",
    "# Preprocess the new transcript\n",
    "new_transcript_cleaned = lemmatize_text(new_transcript)\n",
    "\n",
    "\n",
    "# Lemmatize the keywords\n",
    "keywords_lemmatized = [lemmatize_text(keyword) for keyword in keywords]\n",
    "\n",
    "# # Lemmatize the dataset keywords\n",
    "# dataset_keywords_lemmatized = [lemmatize_text(keyword) for keyword in dataset_keywords]\n",
    "\n",
    "\n",
    "# Find and save keywords from the new transcript that coincide with www.txt\n",
    "found_keywords = set()\n",
    "for keyword, lemmatized_keyword in zip(keywords, keywords_lemmatized):\n",
    "    if lemmatized_keyword.lower() in new_transcript_cleaned and len(lemmatized_keyword) >= min_keyword_length:\n",
    "        found_keywords.add(revert_to_original(keyword, lemmatized_keyword))\n",
    "\n",
    "# Save found keywords to a file\n",
    "with open(output_file, \"w\") as file:\n",
    "    for keyword in found_keywords:\n",
    "        file.write(keyword + \"\\n\")\n",
    "\n",
    "print(\"Keywords from the new transcript that coincide with www.txt saved to\", output_file)\n",
    "# Calculate accuracy\n",
    "# accuracy = len(found_keywords) / len(dataset_keywords) * 100\n",
    "# print(\"Accuracy: {:.2f}%\".format(accuracy))\n",
    "# print (len(found_keywords))\n",
    "# print(len(dataset_keywords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords from the new transcript that coincide with dataset saved to found_keywords.txt\n",
      "Accuracy: 25.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/subrat_roy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/subrat_roy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove non-alphabetic characters\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    # Join the words back into text\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# Function to lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Function to revert lemmatized words back to their original form\n",
    "def revert_to_original(keyword, lemmatized_keyword):\n",
    "    # Tokenize both the original and lemmatized keywords\n",
    "    original_tokens = word_tokenize(keyword.lower())\n",
    "    lemmatized_tokens = word_tokenize(lemmatized_keyword.lower())\n",
    "    # Create a mapping from lemmatized tokens to original tokens\n",
    "    mapping = dict(zip(lemmatized_tokens, original_tokens))\n",
    "    # Use the mapping to revert lemmatized tokens back to their original form\n",
    "    original_keyword_tokens = [mapping[token] for token in word_tokenize(lemmatized_keyword.lower())]\n",
    "    # Reconstruct the original keyword\n",
    "    original_keyword = \" \".join(original_keyword_tokens)\n",
    "    return original_keyword\n",
    "\n",
    "# New transcript file\n",
    "new_transcript_file = \"tt.txt\"\n",
    "# Dataset keywords file\n",
    "dataset_keywords_file = \"kw.txt\"\n",
    "# File to save the found keywords\n",
    "output_file = \"found_keywords.txt\"\n",
    "# Minimum length of the keyword to consider\n",
    "min_keyword_length = 3\n",
    "\n",
    "with open(\"www.txt\", \"r\") as file:\n",
    "    keywords = file.read().splitlines()\n",
    "    \n",
    "    \n",
    "# Read keywords from the dataset\n",
    "with open(dataset_keywords_file, \"r\") as file:\n",
    "    dataset_keywords = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Read the new transcript from the file\n",
    "with open(new_transcript_file, \"r\") as file:\n",
    "    new_transcript = file.read()\n",
    "\n",
    "# Preprocess the new transcript\n",
    "new_transcript_cleaned = preprocess_text(new_transcript)\n",
    "\n",
    "# Lemmatize the dataset keywords\n",
    "dataset_keywords_lemmatized = [lemmatize_text(keyword) for keyword in dataset_keywords]\n",
    "\n",
    "# Lemmatize the keywords\n",
    "keywords_lemmatized = [lemmatize_text(keyword) for keyword in dataset_keywords]\n",
    "\n",
    "# Find and save keywords from the new transcript that coincide with the dataset keywords\n",
    "found_keywords = set()\n",
    "for keyword, lemmatized_keyword in zip(dataset_keywords, dataset_keywords_lemmatized):\n",
    "    if lemmatized_keyword.lower() in new_transcript_cleaned and len(lemmatized_keyword) > min_keyword_length:\n",
    "        found_keywords.add(revert_to_original(keyword, lemmatized_keyword))\n",
    "\n",
    "# # Save found keywords to a file\n",
    "# with open(output_file, \"w\") as file:\n",
    "#     for keyword in found_keywords:\n",
    "#         file.write(keyword + \"\\n\")\n",
    "\n",
    "print(\"Keywords from the new transcript that coincide with dataset saved to\", output_file)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = len(found_keywords) / len(dataset_keywords) * 100\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single inverted commas and commas removed from the file and words formatted to the left side: kw.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the file name\n",
    "file_name = \"kw.txt\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_name, \"r\") as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "# Remove single inverted commas and commas, and format words to the left side\n",
    "formatted_content = \"\"\n",
    "for line in content:\n",
    "    line = line.replace(\"'\", \"\").replace(\",\", \"\")\n",
    "    formatted_content += line.lstrip() + \"\\n\"\n",
    "\n",
    "# Write back to the file\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(formatted_content)\n",
    "\n",
    "print(\"Single inverted commas and commas removed from the file and words formatted to the left side:\", file_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard similarity between found_keywords and given keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords from the new transcript that coincide with dataset keywords saved to zzzz.txt\n",
      "Jaccard Similarity: 84.21%\n",
      "Found Keywords: 16\n",
      "Dataset Keywords: 20\n",
      "{'structured prediction', 'linear predictors', 'regression', 'learning algorithm', 'reflex models', 'machine learning', 'stochastic gradient descent', 'binary classification', 'predictor', 'logic models', 'ranking', 'feature extraction', 'loss minimization', 'variable based models', 'multi-class classification', 'classification'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/subrat_roy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/subrat_roy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet if not already downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# New transcript file\n",
    "new_transcript_file = \"tt.txt\"\n",
    "# File to save the found keywords\n",
    "output_file = \"zzzz.txt\"\n",
    "# Minimum length of the keyword to consider\n",
    "min_keyword_length = 3\n",
    "dataset_keywords_file = \"kw.txt\"\n",
    "\n",
    "# Function to lemmatize the text\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Function to revert lemmatized words back to their original form\n",
    "def revert_to_original(keyword, lemmatized_keyword):\n",
    "    # Tokenize both the original and lemmatized keywords\n",
    "    original_tokens = word_tokenize(keyword.lower())\n",
    "    lemmatized_tokens = word_tokenize(lemmatized_keyword.lower())\n",
    "    # Create a mapping from lemmatized tokens to original tokens\n",
    "    mapping = dict(zip(lemmatized_tokens, original_tokens))\n",
    "    # Use the mapping to revert lemmatized tokens back to their original form\n",
    "    original_keyword_tokens = [mapping[token] for token in word_tokenize(lemmatized_keyword.lower())]\n",
    "    # Reconstruct the original keyword\n",
    "    original_keyword = \" \".join(original_keyword_tokens)\n",
    "    return original_keyword\n",
    "\n",
    "# Read the new transcript from the file\n",
    "with open(new_transcript_file, \"r\") as file:\n",
    "    new_transcript = file.read()\n",
    "    \n",
    "with open(\"wwwe.txt\", \"r\") as file:\n",
    "    keywords = file.read().splitlines()\n",
    "        \n",
    "\n",
    "# Read keywords from the dataset\n",
    "with open(dataset_keywords_file, \"r\") as file:\n",
    "    dataset_keywords = [line.strip() for line in file.readlines()]    \n",
    "\n",
    "# Preprocess the new transcript\n",
    "new_transcript_cleaned = lemmatize_text(new_transcript)\n",
    "\n",
    "# Lemmatize the dataset keywords\n",
    "dataset_keywords_lemmatized = [lemmatize_text(keyword) for keyword in dataset_keywords]\n",
    "\n",
    "# Lemmatize the keywords\n",
    "keywords_lemmatized = [lemmatize_text(keyword) for keyword in dataset_keywords]\n",
    "\n",
    "# Find and save keywords from the new transcript that coincide with dataset keywords\n",
    "found_keywords = set()\n",
    "for keyword, lemmatized_keyword in zip(dataset_keywords, keywords_lemmatized):\n",
    "    if lemmatized_keyword.lower() in new_transcript_cleaned and len(lemmatized_keyword) >= min_keyword_length:\n",
    "        found_keywords.add(revert_to_original(keyword, lemmatized_keyword))\n",
    "\n",
    "# Save found keywords to a file\n",
    "with open(output_file, \"w\") as file:\n",
    "    for keyword in found_keywords:\n",
    "        file.write(keyword + \"\\n\")\n",
    "\n",
    "print(\"Keywords from the new transcript that coincide with dataset keywords saved to\", output_file)\n",
    "\n",
    "# Calculate Jaccard similarity\n",
    "intersection = len(set(found_keywords).intersection(dataset_keywords))\n",
    "union = len(set(found_keywords).union(dataset_keywords))\n",
    "jaccard_similarity = intersection / union\n",
    "print(\"Jaccard Similarity: {:.2f}%\".format(jaccard_similarity * 100))\n",
    "print(\"Found Keywords:\", len(found_keywords))\n",
    "print(\"Dataset Keywords:\", len(dataset_keywords))\n",
    "print (found_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 100.00%\n",
      "Recall: 80.00%\n",
      "F1 Score: 88.89%\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision, Recall, and F1 Score\n",
    "true_positives = len(set(found_keywords).intersection(dataset_keywords))\n",
    "false_positives = len(found_keywords) - true_positives\n",
    "false_negatives = len(dataset_keywords) - true_positives\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "print(\"F1 Score: {:.2f}%\".format(f1_score * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single inverted commas and commas removed from the file and words formatted to the left side without space after the left margin: www.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the file name\n",
    "file_name = \"www.txt\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_name, \"r\") as file:\n",
    "    content = file.readlines()\n",
    "\n",
    "# Remove single inverted commas and commas, and format words to the left side without any space after the left margin\n",
    "formatted_content = \"\"\n",
    "for line in content:\n",
    "    line = line.replace(\"'\", \"\").replace(\",\", \"\")\n",
    "    formatted_content += line.strip() + \"\\n\"\n",
    "\n",
    "# Write back to the file\n",
    "with open(file_name, \"w\") as file:\n",
    "    file.write(formatted_content)\n",
    "\n",
    "print(\"Single inverted commas and commas removed from the file and words formatted to the left side without space after the left margin:\", file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the huge list of keywords from the file\n",
    "with open(\"dulla.txt\", \"r\") as file:\n",
    "    keywords = file.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from transcripts\n",
    "def extract_features(transcript):\n",
    "    vectorizer = CountVectorizer(vocabulary=keywords, binary=True)\n",
    "    X = vectorizer.fit_transform([transcript]).toarray()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "def train_classifier(X_train, y_train):\n",
    "    # Flatten the X_train array to make it two-dimensional\n",
    "    X_train_flat = [sample.flatten() for sample in X_train]\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_classifier.fit(X_train_flat, y_train)\n",
    "    return rf_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(input_file, output_file):\n",
    "    # Read the input file and create a set of unique lines\n",
    "    unique_lines = set()\n",
    "    with open(input_file, \"r\") as file:\n",
    "        for line in file:\n",
    "            unique_lines.add(line.strip())\n",
    "\n",
    "    # Write unique lines to the output file\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for line in unique_lines:\n",
    "            file.write(line + \"\\n\")\n",
    "\n",
    "# Example usage:\n",
    "input_file = \"wwwe.txt\"  # Replace with your input file path\n",
    "output_file = \"dulla.txt\"  # Replace with your output file path\n",
    "remove_duplicates(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transcripts from three text files\n",
    "transcript_files = [\"cs221lecture1.txt\", \"cs221lecture2.txt\", \"cs221lecture3.txt\",\"cs221lecture4.txt\",\"cs221lecture5.txt\",\"cs221lecture6.txt\",\"cs221lecture7.txt\",\"cs221lecture8.txt\",\"cs221lecture9.txt\",\"cs221lecture10.txt\",\"cs221lecture11.txt\",\"cs221lecture12.txt\",\"cs221lecture13.txt\",\"cs221lecture14.txt\",\"cs221lecture15.txt\",\"cs221lecture16.txt\"]\n",
    "X_train = []\n",
    "y_train = []\n",
    "for file in transcript_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        transcript = f.read()\n",
    "        for word in transcript.split():\n",
    "            if word.lower() in keywords:\n",
    "                X_train.append(extract_features(transcript))\n",
    "                y_train.append(1)  # Keyword\n",
    "            else:\n",
    "                X_train.append(extract_features(transcript))\n",
    "                y_train.append(0)  # Normal word\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "rf_classifier = train_classifier(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new transcript from the text file\n",
    "with open(\"new_tt.txt\", \"r\") as file:\n",
    "    new_transcript = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1381: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Extract features from the new transcript\n",
    "X_new = extract_features(new_transcript)\n",
    "\n",
    "# Flatten the X_new array to make it two-dimensional\n",
    "X_new_flat = [sample.flatten() for sample in X_new]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Predict using the trained classifier\n",
    "predictions = rf_classifier.predict(X_new_flat)\n",
    "\n",
    "# Print the predicted labels for each word\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"merged_data.csv\")\n",
    "transcripts = data.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store features and labels\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Iterate over each transcript\n",
    "for transcript in transcripts:\n",
    "    # Check for NaN values\n",
    "    if pd.notnull(transcript):\n",
    "        # Extract features from the current transcript\n",
    "        X_train.append(extract_features(transcript))\n",
    "        \n",
    "        # Label each word in the transcript\n",
    "        for word in transcript.split():\n",
    "            if word.lower() in keywords:\n",
    "                y_train.append(1)  # Keyword\n",
    "            else:\n",
    "                y_train.append(0)  # Normal word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [157, 1736165]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the classifier\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rf_classifier \u001b[39m=\u001b[39m train_classifier(X_train, y_train)\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(X_train, y_train)\u001b[0m\n\u001b[1;32m      4\u001b[0m X_train_flat \u001b[39m=\u001b[39m [sample\u001b[39m.\u001b[39mflatten() \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m X_train]\n\u001b[1;32m      5\u001b[0m rf_classifier \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m rf_classifier\u001b[39m.\u001b[39;49mfit(X_train_flat, y_train)\n\u001b[1;32m      7\u001b[0m \u001b[39mreturn\u001b[39;00m rf_classifier\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m issparse(y):\n\u001b[1;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    364\u001b[0m     X,\n\u001b[1;32m    365\u001b[0m     y,\n\u001b[1;32m    366\u001b[0m     multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    367\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    368\u001b[0m     dtype\u001b[39m=\u001b[39;49mDTYPE,\n\u001b[1;32m    369\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    371\u001b[0m \u001b[39m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[39m# missing values.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m estimator \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator)(criterion\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    651\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:1281\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1263\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1264\u001b[0m     X,\n\u001b[1;32m   1265\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1277\u001b[0m )\n\u001b[1;32m   1279\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m-> 1281\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1283\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    455\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    456\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    460\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [157, 1736165]"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "rf_classifier = train_classifier(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for reading\n",
    "with open(\"dulla.txt\", \"r\") as file:\n",
    "    # Read the contents of the file\n",
    "    contents = file.read()\n",
    "\n",
    "# Convert the contents to lowercase\n",
    "contents_lower = contents.lower()\n",
    "\n",
    "# Open the file for writing (this will overwrite the existing file)\n",
    "with open(\"allah.txt\", \"w\") as file:\n",
    "    # Write the lowercase contents back to the file\n",
    "    file.write(contents_lower)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
